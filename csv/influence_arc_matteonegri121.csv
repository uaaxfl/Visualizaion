2014.amta-workshop.2,W13-2241,0,0.0761008,"mpare the two learning paradigms in the MT QE field and within the same experimental setting. The analysis of the results achieved with the two methods yields interesting findings that suggest, as a promising research avenue, the possibility to exploit their complementarity. 2 Related Work State-of-the-art in QE explores different supervised linear or non-linear learning methods for regression or classification such as, among others, support vector machines (SVM), different types of decision trees, neural networks, elastic-net, gaussian processes, naive bayes (Specia et al., 2009; Buck, 2012; Beck et al., 2013; C. de Souza et al., 2014a). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the high-dimensionality of the feature space (Soricut et al., 2012; C. de Souza et al., 2013; Beck et al., 2013). Despite constant improvements, such learning methods have limitations. The main one is that they assume that both training and test data are independently and identically distributed. As a consequence, when they are applied to data from a different distribution or domain they show poor performance (C."
2014.amta-workshop.2,W12-3109,0,0.0117684,"ttempt to compare the two learning paradigms in the MT QE field and within the same experimental setting. The analysis of the results achieved with the two methods yields interesting findings that suggest, as a promising research avenue, the possibility to exploit their complementarity. 2 Related Work State-of-the-art in QE explores different supervised linear or non-linear learning methods for regression or classification such as, among others, support vector machines (SVM), different types of decision trees, neural networks, elastic-net, gaussian processes, naive bayes (Specia et al., 2009; Buck, 2012; Beck et al., 2013; C. de Souza et al., 2014a). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the high-dimensionality of the feature space (Soricut et al., 2012; C. de Souza et al., 2013; Beck et al., 2013). Despite constant improvements, such learning methods have limitations. The main one is that they assume that both training and test data are independently and identically distributed. As a consequence, when they are applied to data from a different distribution or domain they show po"
2014.amta-workshop.2,W13-2243,1,0.894785,"Missing"
2014.amta-workshop.2,W14-3340,1,0.880731,"Missing"
2014.amta-workshop.2,C14-1040,1,0.832257,"Missing"
2014.amta-workshop.2,P13-1004,0,0.153453,"ies (TMs). In such framework, the compelling need to speed up the translation process and reduce its costs by presenting human translators with good-quality suggestions raises interesting research challenges for the QE community. In such environments, translation jobs come from different domains that might be translated by different MT systems and are routed to professional translators with different idiolect, background and quality standards (Turchi et al., 2013). Such variability calls for flexible and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour (Cohn and Specia, 2013; Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data (C. de Souza et al., 2014b). In this study we experiment with the approaches proposed to address directions (i) and (ii) under the same conditions and evaluate their performance. We use the best learning algorithm presented by C. de Souza et al. (2014b) and the online learning protocol for QE presented in Turchi et al. (2014) and compare their results. In our experiments we use more data than both studies to perform our experiments (1000 data points) for three different domains and compare both metho"
2014.amta-workshop.2,C14-2028,1,0.838239,"T system and a commercial rule-based system. Furthermore, the translations were post-edited by up to four different translators, as described in 2 http://hlt.fbk.eu/technologies/aqet 3 http://sourceforge.net/projects/tercpp/ 4 http://anrtrace.limsi.fr/trace_postedit.tar.bz2 12 (Wisniewski et al., 2013). The IT texts come from a software user manual translated by a statistical MT system based on the state-of-the-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences. The post-editions were collected from one professional translator operating on the Matecat5 (Federico et al., 2014) CAT tool in real working conditions. Features. For all the experiments we use the same feature set composed of 17 features proposed in Specia et al. (2009) and extracted with the QuEst feature extractor (Specia et al., 2013; Shah et al., 2014). The set is formed by features that model the complexity of translating the source sentence (e.g. the average source token length or the number of tokens in the source sentence), and the fluency of the translated sentence produced by the MT system (e.g. the language model probability of the translation). The decision to use this feature set is motivated"
2014.amta-workshop.2,J82-2005,0,0.760624,"Missing"
2014.amta-workshop.2,2013.mtsummit-posters.13,1,0.784973,"Missing"
2014.amta-workshop.2,shah-etal-2014-efficient,1,0.732967,".fr/trace_postedit.tar.bz2 12 (Wisniewski et al., 2013). The IT texts come from a software user manual translated by a statistical MT system based on the state-of-the-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences. The post-editions were collected from one professional translator operating on the Matecat5 (Federico et al., 2014) CAT tool in real working conditions. Features. For all the experiments we use the same feature set composed of 17 features proposed in Specia et al. (2009) and extracted with the QuEst feature extractor (Specia et al., 2013; Shah et al., 2014). The set is formed by features that model the complexity of translating the source sentence (e.g. the average source token length or the number of tokens in the source sentence), and the fluency of the translated sentence produced by the MT system (e.g. the language model probability of the translation). The decision to use this feature set is motivated by the fact that it demonstrated to be robust across language pairs, MT systems and text domains (Specia et al., 2009). Baselines. As a term of comparison, in our experiments we consider two baselines. A simple to implement but difficult to be"
2014.amta-workshop.2,2006.amta-papers.25,0,0.046794,"r or not they can rely on a translation, filter out sentences that are not good enough for post-editing by professional translators, or select the best translation among options from multiple MT or translation memory systems. So far, despite its many possible applications, QE research has been mainly conducted in controlled laboratory testing scenarios that disregard some of the possible challenges posed by real working conditions. Indeed, the large body of research resulting from three editions of the shared QE task organized within the yearly Workshop on Machine Translation (WMT 1 The HTER (Snover et al., 2006) measures the minimum edit distance between the MT output and its manually post-edited version. Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the reference. Lower HTER values indicate better translations. 9 Proceedings of the Workshop on Interactive and Adaptive Machine Translation, pages 9–19 AMTA Workshop. Vancouver, Canada. September 22, 2014 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014)) has relied on simplistic assumptions that do not always hold in real life. These assumptions include"
2014.amta-workshop.2,W12-3118,0,0.0453475,"xploit their complementarity. 2 Related Work State-of-the-art in QE explores different supervised linear or non-linear learning methods for regression or classification such as, among others, support vector machines (SVM), different types of decision trees, neural networks, elastic-net, gaussian processes, naive bayes (Specia et al., 2009; Buck, 2012; Beck et al., 2013; C. de Souza et al., 2014a). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the high-dimensionality of the feature space (Soricut et al., 2012; C. de Souza et al., 2013; Beck et al., 2013). Despite constant improvements, such learning methods have limitations. The main one is that they assume that both training and test data are independently and identically distributed. As a consequence, when they are applied to data from a different distribution or domain they show poor performance (C. de Souza et al., 2014b). This limitation harms the performance of QE systems for several real-world applications, such as computer-assisted translation (CAT) environments. Advanced CAT systems currently integrate suggestions obtained from MT engines"
2014.amta-workshop.2,2009.eamt-1.5,1,0.95108,"-task learning, measuring their capability to overcome the limitations of current batch methods. The results of our experiments, which are carried out in the same experimental setting, demonstrate the effectiveness of the two methods and suggest their complementarity. This indicates, as a promising research avenue, the possibility to combine their strengths into an online multi-task approach to the problem. 1 Introduction Quality estimation (QE) for machine translation (MT) is the task of estimating the quality of a translated sentence at run-time and without access to reference translations (Specia et al., 2009). As a quality indicator, in a typical QE setting, automatic systems have to predict either the time or the number of editing operations (e.g. in terms of HTER1 ) required by a human to transform the machine-translated sentence into an adequate and fluent translation. In recent years, QE gained increasing interest in the MT community as a possible way to: decide whether a given translation is good enough for publishing as is, inform readers of the target language only whether or not they can rely on a translation, filter out sentences that are not good enough for post-editing by professional t"
2014.amta-workshop.2,P13-4014,1,0.893372,"Missing"
2014.amta-workshop.2,P14-1067,1,0.877153,"Missing"
2014.amta-workshop.2,W13-2231,1,0.834344,"translation (CAT) environments. Advanced CAT systems currently integrate suggestions obtained from MT engines with those derived from translation memories (TMs). In such framework, the compelling need to speed up the translation process and reduce its costs by presenting human translators with good-quality suggestions raises interesting research challenges for the QE community. In such environments, translation jobs come from different domains that might be translated by different MT systems and are routed to professional translators with different idiolect, background and quality standards (Turchi et al., 2013). Such variability calls for flexible and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour (Cohn and Specia, 2013; Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data (C. de Souza et al., 2014b). In this study we experiment with the approaches proposed to address directions (i) and (ii) under the same conditions and evaluate their performance. We use the best learning algorithm presented by C. de Souza et al. (2014b) and the online learning protocol for QE presented in Turchi et al. (2014) and compare their result"
2014.amta-workshop.2,2013.mtsummit-papers.15,0,0.232499,"anguage Translation (IWSLT). The News domain is formed by newswire text used in WMT translation campaigns and covers different topics. The sentence tuples for TED and News domains are taken from the Trace corpus4 . The translations were generated by two different MT systems, a state-of-the-art phrase-based statistical MT system and a commercial rule-based system. Furthermore, the translations were post-edited by up to four different translators, as described in 2 http://hlt.fbk.eu/technologies/aqet 3 http://sourceforge.net/projects/tercpp/ 4 http://anrtrace.limsi.fr/trace_postedit.tar.bz2 12 (Wisniewski et al., 2013). The IT texts come from a software user manual translated by a statistical MT system based on the state-of-the-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences. The post-editions were collected from one professional translator operating on the Matecat5 (Federico et al., 2014) CAT tool in real working conditions. Features. For all the experiments we use the same feature set composed of 17 features proposed in Specia et al. (2009) and extracted with the QuEst feature extractor (Specia et al., 2013; Shah et al., 2014). The set is formed by features that"
2016.amta-researchers.1,2011.mtsummit-papers.35,0,0.0720755,"Missing"
2016.amta-researchers.1,2013.mtsummit-papers.5,0,0.142336,"atedly: i) the system receives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting"
2016.amta-researchers.1,W15-3001,1,0.887759,"Missing"
2016.amta-researchers.1,2011.mtsummit-papers.1,0,0.0211613,"e data selected by the instance selection module are randomly split in training and development sets three times. A minimum number of selected sentence pairs is required to trigger the parameter optimisation process. If this minimum value is not reached, the optimization step is skipped because having few sentences might not yield to reliable weights. In this case, the weights computed on the previous input segment are used. In our experiments, we observed that this solution is more reliable and efﬁcient than the feature weights obtained with a single tuning, as it was previously proposed in (Cettolo et al., 2011). We believe this procedure to optimize the feature weights over a development set that closely resembles the test segment can help to obtain weights more suitable to the segment to be post-edited. Decode Test Segment. To decode the input segments, all the local models (language, translation, reordering) are built with all the selected instances. The log-linear feature weights are computed by taking the arithmetic mean of the tuned weights for the three data splits. The decoding process is performed with the Moses toolkit recalling that the input segment is kept untouched when no reliable info"
2016.amta-researchers.1,W16-2377,1,0.844916,"Missing"
2016.amta-researchers.1,W15-3025,1,0.897722,"parallel data consisting of MT output on one side and its corrected version on the other side. This data can be leveraged to develop automatic post-editing (APE) systems capable not only to spot recurring MT errors, but also to correct them. Thus, integrating an APE system inside the CAT framework can further improve the 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S quality of the suggested segments, reduce the workload of human post-editors and increase the productivity of the translation industry. As pointed out in (Parton et al., 2012) and (Chatterjee et al., 2015b), from the application point of view APE components would make it possible to: • Improve the MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of"
2016.amta-researchers.1,P15-2026,1,0.871001,"parallel data consisting of MT output on one side and its corrected version on the other side. This data can be leveraged to develop automatic post-editing (APE) systems capable not only to spot recurring MT errors, but also to correct them. Thus, integrating an APE system inside the CAT framework can further improve the 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S quality of the suggested segments, reduce the workload of human post-editors and increase the productivity of the translation industry. As pointed out in (Parton et al., 2012) and (Chatterjee et al., 2015b), from the application point of view APE components would make it possible to: • Improve the MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of"
2016.amta-researchers.1,P11-2031,0,0.0597909,"Missing"
2016.amta-researchers.1,E14-1042,0,0.24108,"ost-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting with online learning mechanism. To perform post-edit propagation, this system was trained incrementally us"
2016.amta-researchers.1,W07-0732,0,0.02438,"ailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned parameters. Although the"
2016.amta-researchers.1,W08-0509,0,0.015528,"tain a jointrepresentation that links each source word with a MT word (mt#src). This representation has been proposed in the context-aware APE approach by B´echara et al. (2011) and leverages the source information to disambiguate post-editing rules. Recently, Chatterjee et al. (2015b) also conﬁrmed this approach to work better than translating from raw MT segments over multiple language pairs. The joint-representation is used as a source corpus to train all the APE systems reported in this paper and it is obtained by ﬁrst aligning the words of source (src) and MT (mt) segments using MGIZA++ (Gao and Vogel, 2008), and then each mt word is concatenated with its corresponding src words. The Autodesk training, development, and test sets consist of 12,238, 1,948, and 1,956 segments respectively, while the WMT2016 data contains 12,000, 1,000, and 2,000 segments. Table 1 provides some additional statistics of the source (mt#src) and target (pe) training corpus, the repetition rate (RR) to measure the repetitiveness inside a text (Bertoldi et al., 2013), and the average TER score for both the data sets (computed between MT and PE). It is interesting to note that the Autodesk data set has on average shorter s"
2016.amta-researchers.1,2010.amta-papers.21,0,0.0339723,"the following steps repeatedly: i) the system receives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system,"
2016.amta-researchers.1,2005.eamt-1.19,0,0.0357856,"one proposed in real-time cdec (Denkowski et al., 2014), which uses sufﬁx-arrays to select the top k instances. In our approach the sample size is in fact dynamically set in order to select only the most similar ones. This allows us to build more reliable models (since the underlying data better resembles the test segment), and to gain speed 2 https://lucene.apache.org/ 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S when the sample size is small. The use of a tf-idf similarity measure was proposed before in the context of machine translation by Hildebrand et al. (2005) to create a pseudo in-domain corpus from a big out-of-domain corpus. Our work is the ﬁrst to investigate it for the APE task in an online learning scenario. Model Creation. From the selected instances we build several local models. The ﬁrst is the language model: A tri-gram local language model is built over the target side of the training corpus with the IRSTLM toolkit (Federico et al., 2008). Since the selected training data closely resembles the input segment, we believe that the local LM can capture the peculiarities of the domain to which the input segment belongs. Along with the local L"
2016.amta-researchers.1,W04-3250,0,0.0393172,"Missing"
2016.amta-researchers.1,P07-2045,0,0.00402914,"lieve that the local LM can capture the peculiarities of the domain to which the input segment belongs. Along with the local LM we always use a trigram global LM, which is updated whenever a human post-edition (pe) is received. The other local models are the translation and the reordering models: these local models are built over the training instances retrieved from the knowledge base. Since the training instances are very similar to the input segment, the post-editing rules learned from these local models are more reliable for the test segment. These models are build with the Moses toolkit (Koehn et al., 2007) and the word alignment of each sentence pair is computed using the incremental GIZA++ software.3 Parameter Optimization. The parameters are optimized over a section of the selected instances (development set). The size of this development set is critical: if it is too large, then the parameter optimization will be expensive. On the other hand, if it is too small the tuned weights might not be reliable. To achieve fast optimization with reliably-tuned weights, multiple instances of MIRA are run in parallel on several small development sets and all the resulting weights are then averaged. For t"
2016.amta-researchers.1,W13-2237,0,0.0186495,"eceives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting with online learning"
2016.amta-researchers.1,E14-2012,0,0.0200912,"thin the context of a single document. For every new document the APE system begins with an “empty” model. Since the post-editing rules are learned for a given document they can be more precise and useful for that document, but the limitation is that knowledge gained after processing one document is not utilized for other similar documents. This limitation can be addressed by our system (Section 3), in which we maintain one global knowledge base to store all the processed documents, still being able to retrieve post-editing rules speciﬁc to a document to be translated. Thot: The Thot toolkit (Ortiz-Martınez and Casacuberta, 2014) is developed to support fully automatic and interactive statistical machine translation.1 It was also used by Lagarda et al. (2015) in an online setting for the APE task, to perform large-scale experiments with several 1 https://github.com/daormar/thot 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S data sets for multiple language pairs, with base MT systems built using different technologies (rule-based MT, statistical MT). In the majority of their experiments online APE successfully improved the quality of the translations obtained from the bas"
2016.amta-researchers.1,P02-1040,0,0.098317,"Missing"
2016.amta-researchers.1,2012.eamt-1.34,0,0.0529399,"Missing"
2016.amta-researchers.1,2013.mtsummit-papers.24,0,0.245164,"he ability to incorporate human feedback in a real-time translation workﬂow. This led to the development of online learning algorithms that can leverage the continuous streams of data arriving in the form of human post-editing feedback to dynamically update the models and tune the parameters on-the-ﬂy within the CAT framework. In recent years, several online systems have been proposed in MT (see Section 2 for more details) to address the problem of incremental training of the models or on-the-ﬂy optimization of feature weights. Few online MT systems have also been applied to the APE scenario (Simard and Foster, 2013; Lagarda et al., 2015) in a controlled working environment in which the systems are trained and evaluated on homogeneous/coherent data where the training and test sets share similar characteristics. Moving from this controlled lab environment to real-world translation workﬂow, where training and test data can be produced by different MT systems, post-edited by various translators and belong to several text genres, makes the task more challenging, because the APE systems have to adapt to all these diversities in real-time. We deﬁne this scenario as a multi-domain translation environment (MDTE)"
2016.amta-researchers.1,N07-1064,0,0.222063,"iting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned pa"
2016.amta-researchers.1,W07-0728,0,0.21759,"iting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned pa"
2016.amta-researchers.1,2006.amta-papers.25,0,0.0327463,"Missing"
2016.amta-researchers.1,2009.mtsummit-posters.20,0,0.0160345,"dits with signiﬁcantly better performance than the existing online APE systems. 2 Online Translation Systems Online translation systems aim to incorporate human post-editing feedback (or the corrected version of the MT output) into their models in real-time, as soon as it becomes available. This feedback helps the system to learn from the mistakes made in the past translations and to avoid repeating them in future translations. This continuous learning capability will eventually improve the quality of the translations and consequently increase the productivity of the translators/post-editors (Tatsumi, 2009) working with MT suggestions in a CAT environment. The basic workﬂow of an online translation system goes through the following steps repeatedly: i) the system receives an input segment; ii) the input segment is translated and provided to the post-editor to ﬁx any errors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx er"
2016.amta-researchers.1,2007.mtsummit-wpt.4,0,0.0535562,"r, or by performing deeper text analysis that is too expensive at decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) PE effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. In the last decade several works have shown that the quality of the machine translated text can be improved signiﬁcantly by post-processing the translations with an APE system (Simard et al., 2007a; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015b, 2016). These systems mainly follow the phrase-based machine translation approach where the MT outputs (with optionally the source sentence) are used as the source language corpus and the post-edits are used as the target language corpus. A common trait of all these APE systems is that they were developed in a batch mode, which consists of training the models over a batch of parallel sentences, optimizing the parameters over a development set, and then decoding the test data with the tuned parameters. Although these standard appr"
2016.amta-researchers.1,D15-1123,0,0.0194726,"ors in it; and iii) the human post-edited version of the translation is incorporated back into the system, by stepwise updating the underlying models and parameters. In the APE context, the input is a machine-translated segment (optionally with its corresponding source segment), which is processed by the online APE system to ﬁx errors, and then veriﬁed by the post-editors. Several online translation systems have been proposed over the years (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; OrtizMartınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015, inter alia). In this section, we describe two online systems that have been used in the APE task (PEPr, and Thot), and one in the MT scenario which is similar to our proposed system (Realtime cdec): PEPr: Post-Edit Propagation: Simard and Foster (2013) proposed a method for post-edit propagation (PEPr), which learns post-editors’ corrections and applies them on-the-ﬂy to further MT output. Their proposal is based on a phrase-based SMT system, used in an APE setting with online learning mechanism. To perform post-edit propagation, this system was trained incrementally using pairs of machine-t"
2016.amta-researchers.1,2005.eamt-1.39,0,0.0218908,"Gaussian distributions. However, the feature weights of the log-linear model are static throughout the online learning process, as opposed to our method that updates the weights on-the-ﬂy. Also, this method learns post-editing rules from all the data processed in real-time, whereas, our approach learns from the most relevant data points. Realtime cdec: Denkowski et al. (2014) proposed an online model adaptation method to leverage human post-edited feedback to improve the quality of an MT system in a real-time translation workﬂow. To build the translation models they use a static sufﬁx array (Zhang and Vogel, 2005) to index initial data (or a seed corpus), and a dynamic lookup table to store information from the post-edited feedback. To decode a sentence, the statistics of the translation options are computed both from the sufﬁx array and from the lookup table. An incremental language model is maintained and updated with each incoming human post-edit. To update the feature weights they used an extended version of the margin-infused relaxed algorithm (MIRA) (Chiang, 2012). The decoding is treated as simply the next iteration of MIRA, where a segment is ﬁrst translated and then its corresponding reference"
2020.acl-main.619,E06-1032,0,0.285666,"Missing"
2020.acl-main.619,W19-3824,0,0.276167,"counting for MT systems’ strengths and weaknesses in the translation of gender shed light on the problem but, at the same time, have limitations. On one hand, the existing evaluations focused on gender bias were largely conducted on challenge datasets, which are controlled 6923 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6923–6933 c July 5 - 10, 2020. 2020 Association for Computational Linguistics artificial benchmarks that provide a limited perspective on the extent of the phenomenon and may force unreliable conclusions (Prates et al., 2018; Cho et al., 2019; Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). On the other hand, the natural corpora built on conversational language that were used in few studies (Elaraby et al., 2018; Vanmassenhove et al., 2018) include only a restricted quantity of not isolated gender-expressing forms, thus not permitting either extensive or targeted evaluations. Moreover, no attempt has yet been made to assess if and how speech translation (ST) systems are affected by this particular problem. As such, whether ST technologies that leverage audio inputs can retrieve useful clues for translating gender in"
2020.acl-main.619,N19-1202,1,0.898237,"Missing"
2020.acl-main.619,S18-2005,0,0.14022,"ias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. e"
2020.acl-main.619,L18-1001,0,0.119832,"bias in ST, which, as explained in §4, is used for a targeted gender-sensitive evaluation approach. 3 The MuST-SHE benchmark We built MuST-SHE on naturally occurring data retrieved from MuST-C (Di Gangi et al., 2019a), the largest freely available multilingual corpus for ST, which comprises (audio, transcript, translation) triplets extracted from TED talks data. Besides being multilingual, MuST-C is characterized by highquality speech and a variety of different speakers that adequately represent women, two aspects that determined its selection among other existing corpora (Post et al., 2013; Kocabiyikoglu et al., 2018; Sanabria et al., 2018). As such, MuST-SHE was compiled by targeting in the original dataset linguistic phenomena that entail a gender identification from English into Italian and French, two Romance languages that extensively express gender via feminine or masculine morphological markers on nouns, adjectives, verbs and other functional words (e.g. articles and demonstratives). 3.1 Categorization of gender phenomena MuST-SHE is compiled with segments that require the translation of at least one English genderneutral word into the corresponding masculine or feminine target word(s), where such"
2020.acl-main.619,W19-3807,0,0.171844,"male test sets containing first person singular pronouns. This strategy increases the chances to isolate speakerdependent gendered expressions, but still, the employed BLEU metric does not pointedly grasp the effect of gender translation on the output, as the overall performance is also impacted by other factors. Analogously, Elaraby et al. (2018) design a set of agreement rules to automatically recover 300 gender-affected sentences in their corpus, but the evaluation relies on global BLEU scores computed on a bigger set (1,300 sentences) and does not consider male-female related differences. Moryossef et al. (2019) use a parser to detect morphological realizations of speakers’ gender on a single femalespeaker corpus that does not permit inter-gender comparisons. In light of above, an ideal test set should consist of naturally occurring data exhibiting a diversified assortment of gender phenomena so to avoid forced predictions with over-controlled procedures. Also, a consistent amount of equally distributed feminine and masculine gender realizations need to be identified to disentangle the accuracy of gender translation from the overall model’s performance. Accordingly, in §3 we present MuST-SHE, a multi"
2020.acl-main.619,W19-3821,0,0.0629603,"Missing"
2020.acl-main.619,J86-2003,0,0.123168,"data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduces gender stereotypes discriminating women. In translation-related tasks, gender bias arises from the extent through which each language formally expresses the female or male gender of a referred human entity. Languages with a grammatical system of gender, such as Romance languages, rely on a copious set of morphological (inflection) and syntactic (gender agreement) devices applying to numerous parts of speech (Hockett, 1958). Differently, English is a natural gender language that only reflects distinction of sex via pronouns, inherently gendered words (boy, girl) and exceptionally with marked nouns (actor, actress). For all the other indistinct neutral words, the gender of the referred entity – if available – is inferred from contextual information present in the discourse, e.g. he/she is a friend. Nascent inquiries on machine translation (MT) pointed out that machines tend to reproduce the linguistic asymmetries present in the real-world data they are trained on. In the case of gender inequality, this is made ap"
2020.acl-main.619,P02-1040,0,0.111543,"Missing"
2020.acl-main.619,2013.iwslt-papers.14,0,0.0283928,"stigation of gender bias in ST, which, as explained in §4, is used for a targeted gender-sensitive evaluation approach. 3 The MuST-SHE benchmark We built MuST-SHE on naturally occurring data retrieved from MuST-C (Di Gangi et al., 2019a), the largest freely available multilingual corpus for ST, which comprises (audio, transcript, translation) triplets extracted from TED talks data. Besides being multilingual, MuST-C is characterized by highquality speech and a variety of different speakers that adequately represent women, two aspects that determined its selection among other existing corpora (Post et al., 2013; Kocabiyikoglu et al., 2018; Sanabria et al., 2018). As such, MuST-SHE was compiled by targeting in the original dataset linguistic phenomena that entail a gender identification from English into Italian and French, two Romance languages that extensively express gender via feminine or masculine morphological markers on nouns, adjectives, verbs and other functional words (e.g. articles and demonstratives). 3.1 Categorization of gender phenomena MuST-SHE is compiled with segments that require the translation of at least one English genderneutral word into the corresponding masculine or feminine"
2020.acl-main.619,N18-2002,0,0.0661732,"Missing"
2020.acl-main.619,2006.amta-papers.25,0,0.105408,"ion Testset (Sun et al., 2019), and represents posite gender form (containing feminine-marked the very first of its kind for ST and MT created on words “une”, “grandes” and “innovatrices”). The natural data. result is a new set of references that, compared to the correct ones, are “wrong” only with respect to 4 Experimental Setting the formal expression of gender. 4.1 Evaluation Method The underlying idea is that, as the two reference sets differ only for the swapped gendered forms, MT evaluation metrics like BLEU (Papineni et al., results’ differences for the same set of hypothe2002) or TER (Snover et al., 2006) provide a global ses produced by a given system can measure its score about translation “quality” as a whole. Used as-is, their holistic nature hinders the precise eval- capability to handle gender phenomena. In particuation of systems’ performance on an individual 5 Still the de facto standard in MT evaluation in spite of phenomenon as gender translation, since the vari- constant research efforts towards metrics that better correlate ations of BLEU score are only a coarse and indi- with human judgements. 6927 ular, we argue that higher values on the wrong set can signal a potentially gender-"
2020.acl-main.619,P19-1115,0,0.0259698,"Missing"
2020.acl-main.619,P19-1164,0,0.384908,"e translation of gender shed light on the problem but, at the same time, have limitations. On one hand, the existing evaluations focused on gender bias were largely conducted on challenge datasets, which are controlled 6923 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6923–6933 c July 5 - 10, 2020. 2020 Association for Computational Linguistics artificial benchmarks that provide a limited perspective on the extent of the phenomenon and may force unreliable conclusions (Prates et al., 2018; Cho et al., 2019; Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). On the other hand, the natural corpora built on conversational language that were used in few studies (Elaraby et al., 2018; Vanmassenhove et al., 2018) include only a restricted quantity of not isolated gender-expressing forms, thus not permitting either extensive or targeted evaluations. Moreover, no attempt has yet been made to assess if and how speech translation (ST) systems are affected by this particular problem. As such, whether ST technologies that leverage audio inputs can retrieve useful clues for translating gender in addition to contextual information present in the discourse, o"
2020.acl-main.619,P19-1159,0,0.193689,"from standard test sets, as it gender agreement. In particular, for each genderis precisely designed to: i) equally distribute genneutral English word in the source utterance (e.g. der references as well as speakers, and ii) allow “one”, “great” and “innovators” in the 4th examfor a sound and focused evaluation on the accuracy ple of Table 1), the correct translation (containing of gender translation. As such, it satisfies the pathe French words with masculine inflection “un”, rameters to be qualified as a GBET, Gender Bias “grands” and “innovateurs”) is swapped into its opEvaluation Testset (Sun et al., 2019), and represents posite gender form (containing feminine-marked the very first of its kind for ST and MT created on words “une”, “grandes” and “innovatrices”). The natural data. result is a new set of references that, compared to the correct ones, are “wrong” only with respect to 4 Experimental Setting the formal expression of gender. 4.1 Evaluation Method The underlying idea is that, as the two reference sets differ only for the swapped gendered forms, MT evaluation metrics like BLEU (Papineni et al., results’ differences for the same set of hypothe2002) or TER (Snover et al., 2006) provide a"
2020.acl-main.619,D18-1334,0,0.366988,"s were largely conducted on challenge datasets, which are controlled 6923 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6923–6933 c July 5 - 10, 2020. 2020 Association for Computational Linguistics artificial benchmarks that provide a limited perspective on the extent of the phenomenon and may force unreliable conclusions (Prates et al., 2018; Cho et al., 2019; Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). On the other hand, the natural corpora built on conversational language that were used in few studies (Elaraby et al., 2018; Vanmassenhove et al., 2018) include only a restricted quantity of not isolated gender-expressing forms, thus not permitting either extensive or targeted evaluations. Moreover, no attempt has yet been made to assess if and how speech translation (ST) systems are affected by this particular problem. As such, whether ST technologies that leverage audio inputs can retrieve useful clues for translating gender in addition to contextual information present in the discourse, or supply for their lack, remains a largely unexplored question. In the light of above, the contributions of this paper are: (1) We present the first syste"
2020.acl-main.619,Q18-1042,0,0.0272845,"ful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduce"
2020.acl-main.619,D17-1323,0,0.0216682,"(cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduces gender stereotypes discriminating women. In translation-related tasks"
2020.acl-main.619,N18-2003,0,0.135351,", and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French). 1 Introduction With the exponential popularity of deep learning approaches for a great range of natural language processing (NLP) tasks being integrated in our daily life, the need to address the issues of gender fairness1 and gender bias has become a growing interdisciplinary concern. Present-day studies on a variety of NLP-related tasks, such as sentiment analysis (Kiritchenko and Mohammad, 2018) coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Zhao et al., 2018), visual semantic-role labeling (Zhao et al., 2017) or language modeling (Lu ∗ ∗These authors contributed equally. The work by Beatrice Savoldi was carried out during an internship at Fondazione Bruno Kessler. 1 We acknowledge that gender is a multifaceted notion, not necessarily constrained within binary assumptions. However, since speech translation is hindered by the scarcity of available data, we rely on the female/male distinction of gender, as it is linguistically reflected in existing natural data. et al., 2019), attest the existence of a systemic bias that reproduces gender stereotypes"
2020.amta-research.13,N18-1008,0,0.0154015,"tion quality (B´erard et al., 2016). However, the small data condition of this task appeared to be the main obstacle to overcome, which has been tackled with techniques Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 146 like transfer learning or multitask learning (Weiss et al., 2017; B´erard et al., 2018) in order to leverage knowledge from the tasks of ASR and MT. This techniques resulted to be useful, but the improvements were limited and really effective only in very small data conditions (Anastasopoulos and Chiang, 2018; Bansal et al., 2018). Furthermore, Sperber et al. (2019) argued that current sequence-to-sequence architectures are not effective in leveraging the additional data, and an evolution of the two-stage-decoding model (Kano et al., 2017) is more data-efficient. In an attempt to transfer knowledge from MT to ST, Liu et al. (2019) used knowledge distillation, but the real game-changing approach appeared to be the use of synthetic parallel data generated with TTS and MT systems (Jia et al., 2019). Pino et al. (2019) showed that using data with the target side generated by MT outperforms model pretr"
2020.amta-research.13,D19-5619,1,0.88515,"Missing"
2020.amta-research.13,D18-1461,0,0.0368866,"rds in the target sentences. Some studies followed the setting of Listen, Attend and Spell (Chan et al., 2016) for automatic speech recognition (ASR) and segmented the target text at character level (Weiss et al., 2017; B´erard et al., 2018; Di Gangi et al., 2019b). Other studies followed Jia et al. (2019) in using subword-level segmentation (Pino et al., 2019; Bahar et al., 2019a; Liu et al., 2019), motivated by the impractical training time when using a character-level segmentation on large datasets due to the increased sequence length. A research strand in neural machine translation (NMT) (Cherry et al., 2018; Kreutzer and Sokolov, 2018; Ataman et al., 2019), based on recurrent models, showed that character-level ∗ The first author performed this work while he was a Ph.D. student at FBK. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 137 models can outperform subword-level models, but need architectural changes to make the computation more efficient and increase the capacity of the models. Indeed, while the subword-level models can store, to some extent, syntactic and semantic information of words"
2020.amta-research.13,N19-1202,1,0.876919,"Missing"
2020.amta-research.13,Q17-1024,0,0.0599349,"Missing"
2020.amta-research.13,L18-1001,0,0.156448,"ms built for the two settings are not directly comparable with each other as they use different data and training scheme. In particular, they use data augmentation for subword-level models but not for character-level models. By itself, data augmentation can be sufficient to explain the higher scores in the former case, hiding the contribution of the different segmentation method. In this paper, we aim to assess the effect of target-side segmentation by directly comparing character- and subword-level models on some of the most popular ST benchmarks. In particular we use: Augmented Librispeech (Kocabiyikoglu et al., 2018), the most used benchmark so far; MuST-C (Di Gangi et al., 2019a), which represents the largest training set for 8 language directions; How2 (Sanabria et al., 2018), which is a quite large dataset and has been used in a recent IWSLT shared task (Niehues et al., 2019). In order to shed light over this contrasting claims we perform our experiments both in a classical oneto-one translation and in a one-to-many multilingual translation settings. Our results show that the subword-level segmentation, in our setting, is always preferable and it outperforms the character-level segmentation by up to 3"
2020.amta-research.13,P07-2045,0,0.011025,"Missing"
2020.amta-research.13,2020.acl-main.615,0,0.0233524,"Missing"
2020.amta-research.13,P02-1040,0,0.112368,"ed benchmark so far; MuST-C (Di Gangi et al., 2019a), which represents the largest training set for 8 language directions; How2 (Sanabria et al., 2018), which is a quite large dataset and has been used in a recent IWSLT shared task (Niehues et al., 2019). In order to shed light over this contrasting claims we perform our experiments both in a classical oneto-one translation and in a one-to-many multilingual translation settings. Our results show that the subword-level segmentation, in our setting, is always preferable and it outperforms the character-level segmentation by up to 3 BLEU points (Papineni et al., 2002). The gap is confirmed in the multilingual setup. A final analysis shows that subword-level models are superior independently from the sequence length, and their increased capacity allows them to choose among more options in the generation phase. 2 Direct Speech Translation Spoken language translation (hereby speech translation) has been traditionally performed using a cascade of statistical models involving at least ASR and MT (Casacuberta et al., 2008), which benefits from the possibility of using state-of-the-art models for all the components, but the translations may be negatively impacted"
2020.amta-research.13,P18-2051,0,0.0313941,"Missing"
2020.amta-research.13,P16-1162,0,0.787132,"models, but need architectural changes to make the computation more efficient and increase the capacity of the models. Indeed, while the subword-level models can store, to some extent, syntactic and semantic information of words in their embedding layers, character embeddings cannot do the same as the function of a word is not obtained by character composition. The idea of the superiority of the subword level is suggested for speech translation by Bahar et al. (2019b), who claim that their baseline is stronger than the ones used in previous studies because of their use of BPE-segmented words (Sennrich et al., 2016) on the target side instead of characters. In contrast, two recent works in multilingual speech translation claim better results when using characters, and consequently do not show the weaker results when using BPE (Di Gangi et al., 2019c; Inaguma et al., 2019). To the best of our knowledge, the only previous study that uses both segmentation strategies was proposed by Indurthi et al. (2019). However, the systems built for the two settings are not directly comparable with each other as they use different data and training scheme. In particular, they use data augmentation for subword-level mode"
2020.amta-research.13,2006.amta-papers.25,0,0.0703548,"haracter-level models is to have a larger BLEU score in the second bin than in the first one, then a smooth transition to the third bin, and finally a dramatic degradation in the last two bins. The degradation in the last two bins is less evident for the BPE-level models. Similar patterns are found in the other language directions. This first analysis shows that BPE-level models achieve a higher quality in all length bins and can better manage long translations. Figure 1: BLEU score by reference length (number of characters) for character- and BPE-level models in English-French. TER. The TER (Snover et al., 2006) scores translations at sentence level as a function of errors (insertions, deletions, substitutions, shifts) with respect to a reference. We compute TER with a focus on MuST-C French translations to obtain more details about the score difference between the two systems. We use the sentence-level scores to divide the translations in three groups according to the system that produced the best translation (or a tie). We say that the Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 144 Figure 2: BLE"
2020.amta-research.13,Q19-1020,0,0.0135843,"on of this task appeared to be the main obstacle to overcome, which has been tackled with techniques Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 146 like transfer learning or multitask learning (Weiss et al., 2017; B´erard et al., 2018) in order to leverage knowledge from the tasks of ASR and MT. This techniques resulted to be useful, but the improvements were limited and really effective only in very small data conditions (Anastasopoulos and Chiang, 2018; Bansal et al., 2018). Furthermore, Sperber et al. (2019) argued that current sequence-to-sequence architectures are not effective in leveraging the additional data, and an evolution of the two-stage-decoding model (Kano et al., 2017) is more data-efficient. In an attempt to transfer knowledge from MT to ST, Liu et al. (2019) used knowledge distillation, but the real game-changing approach appeared to be the use of synthetic parallel data generated with TTS and MT systems (Jia et al., 2019). Pino et al. (2019) showed that using data with the target side generated by MT outperforms model pretraining. Separately, Bahar and colleagues studied the contr"
2020.coling-main.350,P14-2134,0,0.0272858,"nslation from English into Italian and French. To this aim, we manually annotated large datasets with speakers’ gender information and used them for experiments reflecting different possible real-world scenarios. Our results show that gender-aware direct ST solutions can significantly outperform strong – but gender-unaware – direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality. 1 Introduction Language use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be u"
2020.coling-main.350,N19-1006,0,0.0279948,"ely on data augmentation and knowledge transfer techniques that were shown to yield competitive models at the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2020). In particular, we use three data augmentation methods – SpecAugment (Park et al., 2019), time stretch (Nguyen et al., 2020), and synthetic data generation (Jia et al., 2019) – and we transfer knowledge both from ASR and MT through component initialization and knowledge distillation (Hinton et al., 2015). The ST model’s encoder is initialized with the encoder of an English ASR model (Bansal et al., 2019) with a lower number of encoder layers (the missing layers are initialized randomly, as well as the decoder). This ASR model is trained on Librispeech (Panayotov et al., 2015), Mozilla Common Voice,5 How2 (Sanabria et al., 2018), TEDLIUM-v3 (Hernandez et al., 2018), and the utterance-transcript pairs of the ST corpora – Europarl-ST (Iranzo-S´anchez et al., 2020) and MuST-C. These datasets are either gender unbalanced or do not provide speaker’s gender information apart from Librispeech, which is balanced in terms of female/male speakers (Garnerin et al., 2020). However, since these speakers ar"
2020.coling-main.350,Q18-1041,0,0.0116889,"erform strong – but gender-unaware – direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality. 1 Introduction Language use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of fe"
2020.coling-main.350,2020.acl-main.619,1,0.851509,"Missing"
2020.coling-main.350,2020.acl-main.418,0,0.0734234,"Missing"
2020.coling-main.350,N19-1202,1,0.887446,"Missing"
2020.coling-main.350,W19-6603,1,0.668742,"Missing"
2020.coling-main.350,W19-3821,0,0.0520695,"Missing"
2020.coling-main.350,2020.iwslt-1.8,1,0.829653,"Missing"
2020.coling-main.350,2020.lrec-1.813,0,0.0165881,"he encoder of an English ASR model (Bansal et al., 2019) with a lower number of encoder layers (the missing layers are initialized randomly, as well as the decoder). This ASR model is trained on Librispeech (Panayotov et al., 2015), Mozilla Common Voice,5 How2 (Sanabria et al., 2018), TEDLIUM-v3 (Hernandez et al., 2018), and the utterance-transcript pairs of the ST corpora – Europarl-ST (Iranzo-S´anchez et al., 2020) and MuST-C. These datasets are either gender unbalanced or do not provide speaker’s gender information apart from Librispeech, which is balanced in terms of female/male speakers (Garnerin et al., 2020). However, since these speakers are just book narrators, first-person sentences do not really refer to the speakers themselves. Knowledge distillation (KD) is performed from a teacher MT model by optimizing the cross entropy between the distribution produced by the teacher and by the student ST model being trained (Liu et al., 2019). For both en-it and en-fr, the MT model is trained on the OPUS datasets (Tiedemann, 2016). The ST model is trained in three consecutive steps. In the first step, we use the synthetic data obtained by pairing ASR audio samples with the automatic translations of the"
2020.coling-main.350,J86-2003,0,0.172817,"uages that do not convey such information. Indeed, languages with grammatical gender, such as French and † The authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 For a comprehensive overview on such societal issue see (Criado-Perez, 2019). License details: http:// 3951 Proceedings of the 28th International Conference on Computational Linguistics, pages 3951–3964 Barcelona, Spain (Online), December 8-13, 2020 Italian, display a complex morphosyntactic and semantic system of gender agreement (Hockett, 1958; Corbett, 1991), relying on feminine/masculine markings that reflect speakers’ gender on numerous parts of speech whenever they are talking about themselves (e.g. En: I’ve never been there – It: Non ci sono mai stata/stato). Differently, English is a natural gender language (Hellinger and Bußman, 2001) that mostly conveys gender via its pronoun system, but only for third-person pronouns (he/she), thus to refer to an entity other than the speaker. As the example shows, in absence of contextual information (e.g As a woman, I have never been there) correctly translating gender can be prohibitive"
2020.coling-main.350,P15-2079,0,0.0126195,"resentation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both from text (Vanmassenhove et al., 2018) and from audio (Bentivogli et al., 2020). These studies – focused on the translation of spoken language – revealed a systemic gender bias whenever systems are required to overtly and formally express speaker’s gender in the target languages, while translating from langua"
2020.coling-main.350,P16-2096,0,0.0643539,". 1 Introduction Language use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automat"
2020.coling-main.350,W17-1601,0,0.423292,"ot equally accurate across demographic groups (e.g. women and children are hard to distinguish as their pitch is typically high (Levitan et al., 2016)), manual assignment prevents from incorporating gender misclassifications in our training data. Second, biological essentialist frameworks that categorize gender based on acoustic cues (Zimman, 2020) are especially problematic for transgender individuals, whose gender identity is not aligned with the sex they have been assigned at birth based on designated anatomical/biological criteria (Stryker, 2008). Differently, following the guidelines in (Larson, 2017), we do not want to run the risk of making assumptions about speakers’ gender identity and introducing additional bias within an environment that has been specifically designed to inspect gender bias. By looking at the personal pronouns used by the speakers to describe themselves, our manual assignment instead is meant to account for the gender linguistic forms by which the speakers accept to be referred to in English (GLAAD, 2007), and would want their translations to conform to. We stress that gendered linguistic expressions do not directly map to speakers’ self-determined gender identity (C"
2020.coling-main.350,W19-3807,0,0.218546,"– although unintended – can affect users’ self-esteem (Bourguignon et al., 2015), especially when the linguistic bias is shaped as a perpetuation of stereotypical gender roles and associations (Levesque, 2011). Additionally, as the system does not perform equally well across gender groups, such tools may not be suitable for women, excluding them from benefiting from new technological resources. To date, few attempts have been made towards developing gender-aware translation models, and surprisingly, almost exclusively within the MT community (Vanmassenhove et al., 2018; Elaraby et al., 2018; Moryossef et al., 2019). The only work on gender bias in ST (Bentivogli et al., 2020) proved that direct ST has an advantage when it comes to speaker-dependent gender translation (as in I’ve never been there uttered by a woman), since it can leverage acoustic properties from the audio input (e.g. speaker’s fundamental frequency). However, relying on perceptual markers of speakers’ gender is not the best solution for all kinds of users (e.g. transgenders, children, vocally-impaired people). Moreover, although their conclusions remark that direct ST is nonetheless affected by gender bias, no attempt has yet been made"
2020.coling-main.350,2020.iwslt-1.9,0,0.290027,"on, 2017)). For the sake of simplicity, in our study we use female/male to respectively indicate those speakers whose personal pronouns are she/he. 3954 4.1 Base ST Model We are interested in evaluating and improving gender translation on strong ST models that can be used in real-world contexts. As such, our base, gender-unaware model is trained with the goal of achieving state-of-the-art performance on the ST task. To this aim, we rely on data augmentation and knowledge transfer techniques that were shown to yield competitive models at the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2020). In particular, we use three data augmentation methods – SpecAugment (Park et al., 2019), time stretch (Nguyen et al., 2020), and synthetic data generation (Jia et al., 2019) – and we transfer knowledge both from ASR and MT through component initialization and knowledge distillation (Hinton et al., 2015). The ST model’s encoder is initialized with the encoder of an English ASR model (Bansal et al., 2019) with a lower number of encoder layers (the missing layers are initialized randomly, as well as the decoder). This ASR model is trained on Librispeech (Panayotov et al., 2"
2020.coling-main.350,2020.acl-main.690,0,0.197261,"inine/masculine gender forms regardless of the perceptual features received from the audio signal, offering a solution for cases where relying on speakers’ vocal characteristics is detrimental to a proper gender translation. 3952 2 Background Besides the abundant work carried out for English monolingual NLP tasks (Sun et al., 2019), a consistent amount of studies have now inspected how MT is affected by the problem of gender bias. Most of them, however, do not focus on speaker-dependent gender agreement. Rather, a number of studies (Stanovsky et al., 2019; Escud´e Font and Costa-juss`a, 2019; Saunders and Byrne, 2020) evaluate whether MT is able to associate prononimal coreference with an occupational noun to produce the correct masculine/feminine forms in the target gender-inflected languages (En: I’ve known her for a long time, my friend is a cook. Es: La conozco desde hace mucho tiempo, mi amiga es cocinera). Notably, few approaches have been employed to make neural MT systems speaker-aware by controlling gender realization in their output. Elaraby et al. (2018) enrich their data with a set of genderagreement rules so to force the system to account for them in the prediction step. In (Vanmassenhove et a"
2020.coling-main.350,2020.acl-main.468,0,0.115074,"ge use is intrinsically social and situated as it varies across groups and even individuals (Bamman et al., 2014). As a result, the language data that are collected to build the corpora on which natural language processing models are trained are often far from being homogeneous and rarely offer a fair representation of different demographic groups and their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both"
2020.coling-main.350,P19-1164,0,0.0595861,"tion quality. Moreover, our best systems learn to produce feminine/masculine gender forms regardless of the perceptual features received from the audio signal, offering a solution for cases where relying on speakers’ vocal characteristics is detrimental to a proper gender translation. 3952 2 Background Besides the abundant work carried out for English monolingual NLP tasks (Sun et al., 2019), a consistent amount of studies have now inspected how MT is affected by the problem of gender bias. Most of them, however, do not focus on speaker-dependent gender agreement. Rather, a number of studies (Stanovsky et al., 2019; Escud´e Font and Costa-juss`a, 2019; Saunders and Byrne, 2020) evaluate whether MT is able to associate prononimal coreference with an occupational noun to produce the correct masculine/feminine forms in the target gender-inflected languages (En: I’ve known her for a long time, my friend is a cook. Es: La conozco desde hace mucho tiempo, mi amiga es cocinera). Notably, few approaches have been employed to make neural MT systems speaker-aware by controlling gender realization in their output. Elaraby et al. (2018) enrich their data with a set of genderagreement rules so to force the system to"
2020.coling-main.350,P19-1159,0,0.129883,"Missing"
2020.coling-main.350,W17-1606,0,0.0231593,"their linguistic behaviours (Bender and Friedman, 2018). Consequently, as predictive models learn from the data distribution they have seen, they tend to favor the demographic group most represented in their training data (Hovy and Spruit, 2016; Shah et al., 2020). This brings serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both from text (Vanmassenhove et al., 2018) and from audio (Bentivogli et al., 2020). These studies – focused on the translation of spoken language – revealed a systemic gender bias whenever systems are required to overtly and formally express speaker’s gender in the target languages, while translating from languages that do not convey such informatio"
2020.coling-main.350,2016.eamt-2.8,0,0.0451076,"C. These datasets are either gender unbalanced or do not provide speaker’s gender information apart from Librispeech, which is balanced in terms of female/male speakers (Garnerin et al., 2020). However, since these speakers are just book narrators, first-person sentences do not really refer to the speakers themselves. Knowledge distillation (KD) is performed from a teacher MT model by optimizing the cross entropy between the distribution produced by the teacher and by the student ST model being trained (Liu et al., 2019). For both en-it and en-fr, the MT model is trained on the OPUS datasets (Tiedemann, 2016). The ST model is trained in three consecutive steps. In the first step, we use the synthetic data obtained by pairing ASR audio samples with the automatic translations of the corresponding transcripts. In the second step, the model is trained on the ST corpora. In these first two steps, we use the KD loss function. Finally, in the third step, the model is fine-tuned on the same ST corpora using label-smoothed cross entropy (Szegedy et al., 2016). SpecAugment and time stretch are used in all steps. 4.2 Multi-gender Systems The idea of “multi-gender” models, i.e. models informed about the speak"
2020.coling-main.350,D18-1334,0,0.434685,"s serious social consequences as well, since the people who are more likely to be underrepresented within datasets are those whose representation is often less accounted for within our society. A case in point regards the gender data gap.1 In fact, studies on speech taggers (Hovy and Søgaard, 2015) and speech recognition (Tatman, 2017) showed that the underrepresentation of female speakers in the training data leads to significantly lower accuracy in modeling that demographic group. The problem of gender-related differences has also been inspected within automatic translation, both from text (Vanmassenhove et al., 2018) and from audio (Bentivogli et al., 2020). These studies – focused on the translation of spoken language – revealed a systemic gender bias whenever systems are required to overtly and formally express speaker’s gender in the target languages, while translating from languages that do not convey such information. Indeed, languages with grammatical gender, such as French and † The authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 For a comprehensive overview on such societal issue see (Criado-"
2020.coling-main.350,1991.mtsummit-papers.18,0,0.272103,"Missing"
2020.coling-main.382,2012.eamt-1.60,0,0.0330591,"Missing"
2020.coling-main.382,N19-1202,1,0.876496,"Missing"
2020.coling-main.382,P13-2121,0,0.0183956,"Missing"
2020.coling-main.382,2005.mtsummit-papers.11,0,0.155286,"Missing"
2020.coling-main.382,D18-2012,0,0.0582479,"Missing"
2020.coling-main.382,L16-1147,0,0.0475097,"Missing"
2020.coling-main.382,D17-1299,0,0.0400556,"Missing"
2020.coling-main.382,N19-4009,0,0.0223573,"Missing"
2020.coling-main.382,W19-5210,1,0.834329,"isochrony (synchronisation with body movements) which would require a multimodal analysis of the actions of the characters on screen and is therefore outside the scope of this paper. 4327 Proceedings of the 28th International Conference on Computational Linguistics, pages 4327–4333 Barcelona, Spain (Online), December 8-13, 2020 mouth is visible on screen or not (off screen). While on-screen dubbing is bound by synchronisation constraints, off-screen dubbing should simply be representative of orality. Very few works have attempted to automatise dubbing through Neural Machine Translation (NMT). Saboo and Baumann (2019) attempt to integrate the constraint of isochrony by selecting the translation ¨ which has a similar number of syllables to the source. Oktem et al. (2019) use the NMT attention mechanism to segment the translation into prosodic phrases in order to improve a Text-to-Speech system for dubbing. Federico et al. (2020) adapt an NMT system to generate translations of the same length as the source, although in terms of characters, which does not necessarily reflect duration of utterance. However, none of these works has taken into consideration the on/off-screen dichotomy. In this work, we address,"
2020.coling-main.382,N16-1005,0,0.0553904,"Missing"
2020.eamt-1.25,S19-2007,0,0.0157321,"ocument, which usually provides enough information about the general content of the document. We use the whole English training set to build our downstream classifiers. To simulate an under-resourced setting, we randomly sample 100 documents for each class from the Spanish and Italian training sets. We use these samples to adapt the generic NMT system for the downstream task, while for development and test we use the whole sets. Hate Speech Detection. For this binary task, we use the English and Spanish datasets published for the multilingual hate speech detection shared task at SemEval 2019 (Basile et al., 2019). We train the downstream classifier on the whole English training set, including 3,783 hateful and 5,217 nonhateful Twitter messages. We randomly sample 400 tweets for each class from the Spanish training set in order to simulate the under-resourced setting. Since the test set is not publicly available, we use the development set as final evaluation benchmark, and we sample 500 tweets for each class from the rest of the training set as the development set. Sentiment Classification. For this binary task, we use a collection of annotated tweets released for the Italian sentiment analysis task a"
2020.eamt-1.25,D18-1269,0,0.0325988,"erformance NLP components are available, ii) run a classifier on the translated text and, finally, iii) project the results back to the original language. This approach represents a straightforward solution in low/medium-resource1 language settings 1 Jain et al. (2019) consider as “medium-resource” languages those for which, although annotated training corpora do not exist, off-the-shelf (MT) systems like Google Translate are available. where reliable NLP components for specific tasks are not available, and represents a strong baseline in a variety of multilingual and cross-lingual NLP tasks (Conneau et al., 2018). However, the NMT systems normally used are still optimized by pursuing human-oriented adequacy and fluency objectives, which are not necessarily the optimal ones for this pipelined solution. These models can indeed produce translations in which some properties of the input text are altered or even lost. For instance, as shown in (Mohammad et al., 2016), this happens in sentiment classification, where automatic translations can fail to properly project core traits of the input text into the target language. When this happens, the downstream linguistic processor will likely produce results of"
2020.eamt-1.25,N19-1423,0,0.0204816,"we continue the training for 50 epochs and choose the best performing checkpoint based on the average F1 score measured on the development set of each task. We set K (i.e. the number of sampled translation candidates at each time step) to 5, and used the development set to evaluate different values of α (i.e. the constant value added to prevent zero rewards – see Section 3.1). The bestperforming value of 0.1 was then used in all the experiments. For developing the classifiers (both the downstream English ones and the languagespecific ones used as baseline), we fine-tune the multilingual BERT (Devlin et al., 2019). 5 Results and Discussion Our experimental results are shown in Table 3, which reports the classification performance (F1) obtained on each downstream task by: • Feeding the English classifiers with translations from different NMT models (i.e. Generic, Single-task MO-Reinforce and different variants of Multi-task MO-reinforce); • Running language-specific classifiers on the original untranslated data (Source). The F1 scores obtained by the Generic NMT systems in document classification (MLDoc) show that the simplest translation-based approach produces competitive results compared to those ach"
2020.eamt-1.25,P15-1166,0,0.0273394,"rget downstream application for which the input text has to be translated. This idea is drawn from multilingual NMT, in which an effective solution is to prepend to the input sentences a token defining the desired target language (Johnson et al., 2017). To avoid under/over-fitting when training the NMT model on mixed datasets that can have different sizes, we need to schedule the sampling from these datasets. In multilingual NMT, two fixed sampling schedules have been proposed, namely: i) proportionally with respect to the dataset size (Luong et al., 2015), or ii) uniformly from each dataset (Dong et al., 2015). However, these fixed scheduling approaches are not optimal solutions. The first one gives higher importance to tasks with larger datasets, so that those with less training material might remain under-fitted. The second one gives equal importance to all the tasks, which implies that larger datasets for some tasks will not be fully exploited, reducing systems’ performance on those tasks. To overcome these limitations, adaptive scheduling strategies can be adopted to update the importance of each task in the course of training. The idea is that, when the performance of the model is low on one t"
2020.eamt-1.25,D19-1100,0,0.0146294,"exploiting translation-based solutions. In tasks like sentiment classification, hate speech detection or document classification (the three application scenarios addressed in this paper) a translation-based approach would allow: i) translating the input text data from an underresourced language into a resource-rich target language for which high-performance NLP components are available, ii) run a classifier on the translated text and, finally, iii) project the results back to the original language. This approach represents a straightforward solution in low/medium-resource1 language settings 1 Jain et al. (2019) consider as “medium-resource” languages those for which, although annotated training corpora do not exist, off-the-shelf (MT) systems like Google Translate are available. where reliable NLP components for specific tasks are not available, and represents a strong baseline in a variety of multilingual and cross-lingual NLP tasks (Conneau et al., 2018). However, the NMT systems normally used are still optimized by pursuing human-oriented adequacy and fluency objectives, which are not necessarily the optimal ones for this pipelined solution. These models can indeed produce translations in which s"
2020.eamt-1.25,Q17-1024,0,0.0286738,"Section 3.2). 3.1 Normalized Reward To serve multiple downstream classifiers with a single NMT system, the model has to be trained on a mixture of the labeled datasets available for the different tasks. To define the target task, we prepend a task-specific token to each input sample within the corresponding dataset. In this way, the NMT model is informed about the target downstream application for which the input text has to be translated. This idea is drawn from multilingual NMT, in which an effective solution is to prepend to the input sentences a token defining the desired target language (Johnson et al., 2017). To avoid under/over-fitting when training the NMT model on mixed datasets that can have different sizes, we need to schedule the sampling from these datasets. In multilingual NMT, two fixed sampling schedules have been proposed, namely: i) proportionally with respect to the dataset size (Luong et al., 2015), or ii) uniformly from each dataset (Dong et al., 2015). However, these fixed scheduling approaches are not optimal solutions. The first one gives higher importance to tasks with larger datasets, so that those with less training material might remain under-fitted. The second one gives equ"
2020.eamt-1.25,P17-1138,0,0.176847,"Missing"
2020.eamt-1.25,D17-1153,0,0.24265,"Missing"
2020.eamt-1.25,P02-1040,0,0.11162,"end-user: L= = S X s=1 S X Eyˆ ∼P (.|xs ) ∆(ˆ y) (3) X P (ˆ y|xs )∆(ˆ y) s=1 y ˆ ∈Y where ∆(ˆ y) is the reward of the sampled translaˆ , and Y is the set of all the possible tion candidate y translation candidates. Since the size of this set Y is exponentially large, Equation 3 is estimated by sampling one translation candidate out of this set using multinomial sampling or beam search: Since collecting human rewards is costly, the process can be simulated by comparing the sampled translation candidates with the corresponding reference translations using automatic evaluation metrics like BLUE (Papineni et al., 2002). The two learning strategies (supervised and reinforcement) have two main commonalities: i) the learning objectives are human-oriented, and ii) they both need parallel data, respectively for maximizing the probability of the translation pair in supervised learning and for simulating the human reward in reinforcement learning. 2.2 To pursue machine-oriented objectives and to bypass the need for parallel corpora, in the MOReinforce algorithm proposed by (Tebbifakhr et al., 2019), the human reward is replaced by the reward from a downstream classifier (in that case, a polarity detector predictin"
2020.eamt-1.25,L18-1560,0,0.0446632,"e-specific classifiers trained on few data points. Ideally, thanks to the solutions proposed in Section 3, it should also compete with the single-task (machine-oriented) models. This would indicate the viability of a single-model approach to simultaneously address multiple tasks. In the following, we describe the task-specific data used for model adaptation and evaluation, as well as the parallel corpora used for training the generic NMT system. Their statistics are respectively reported in Tables 1 and 2. Document Classification. For this multi-class labelling task, we use the MLDoc corpora (Schwenk and Li, 2018), which cover 8 languages, including English, Spanish and Italian. They comprise news stories labeled with 4 different categories: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). For each language, the training, development and test sets respectively contain 10K, 1K, and 4K documents uniformly distributed into the 4 classes. Following (Bell, 1991), for training and evaluation we only consider the first sentence of each document, which usually provides enough information about the general content of the document. We use the whole English training set"
2020.eamt-1.25,P16-1162,0,0.0754586,"Generic NMT system, ii) directly processing the untranslated data (Source), iii) translating with separate Single-task MO-Rinforce models, iv) one Multi-task MO-Reinforce model with different sampling strategies, v) one Multi-task MO-Reinforce model with reward normalization and noisy sampling. we train the downstream system using a balanced set of 1.6M negative and positive tweets (Go et al., 2009). Generic NMT systems We train the generic NMT system using the parallel corpora reported in Table 2. After filtering out long and imbalanced pairs, we encode the corpora using 32K byte-pair codes (Sennrich et al., 2016). Our NMT model uses Transformer with parameters set as in the original paper (Vaswani et al., 2017). In all the settings, we start the training by initializing the NMT model with the trained generic NMT systems. Then, we continue the training for 50 epochs and choose the best performing checkpoint based on the average F1 score measured on the development set of each task. We set K (i.e. the number of sampled translation candidates at each time step) to 5, and used the development set to evaluate different values of α (i.e. the constant value added to prevent zero rewards – see Section 3.1). T"
2020.eamt-1.25,P16-1159,0,0.0648954,"ns 3.0 licence, no derivative works, attribution, CCBY-ND. Marco Turchi FBK, Trento, Italy turchi@fbk.eu end-users, and are hence optimized pursuing human-oriented requirements about the output quality. To meet these requirements, supervised NMT models are trained to maximize the probability of the given parallel corpora (Bahdanau et al., 2015; Sutskever et al., 2014), which embed the adequacy and fluency criteria essential for the human comprehension of a translated sentence. In another line of research, these objectives are directly addressed in Reinforcement Learning (Ranzato et al., 2016; Shen et al., 2016) and Bandit Learning (Kreutzer et al., 2017; Nguyen et al., 2017), where model optimization is driven by the human feedback obtained for each translation hypothesis. However, humans are not the only possible consumers of MT output. In a variety of application scenarios, MT can in fact act as a pre-processor to perform other natural language processing (NLP) tasks. For instance, this is the case of text classification tasks for which, in low-resource conditions, the paucity of training data provides a strong motivation for exploiting translation-based solutions. In tasks like sentiment classifi"
2020.eamt-1.25,D18-1397,0,0.0464344,"here, at each time step, a word is generated by sampling from the model’s distribution output. The generation is terminated when the EOS token is generated. For a given application, the choice between the two sampling strategies depends on the known trade-off between exploration and exploitation in reinforcement learning. Indeed, while beam search exploits more the model’s knowledge, multinomial sampling is more oriented to exploring the probability search space. In light of this difference, in MO-Reinforce the sampling is done using multinomial sampling, which achieves better results in NMT (Wu et al., 2018). This is needed, since the parameters of the model are initialized by a generic NMT system, which is trained on parallel data pursuing human-oriented objectives. Pushing for the exploration of the probability space instead of exploiting the original model’s knowledge will promote the generation of more diverse candidates and eventually increase the chance to influence system’s behaviour towards our machineoriented objectives. Although for these reasons multinomial sampling represents a better choice compared to beam search, in MO-Reinforce the exploration of the probability space does not alw"
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.iwslt-1.26,2012.eamt-1.33,0,0.111033,"Missing"
2020.iwslt-1.26,N19-1006,0,0.0193797,"s. We remove the 2D self-attention layers and increase the size of the encoder to 11 layers, while for the decoder we use 4 layers. This choice was motivated by preliminary experiments, where we noted that replacing the 2D self-attention layers with normal self-attention layers and adding more layers in the encoder increased the final score, while removing a few decoder layers did not negatively affect the performance. As distance penalty, we choose the logarithmic distance penalty. We use the encoder of the ASR model to initialise the weights of the ST encoder and achieve faster convergence (Bansal et al., 2019). Since the E2E-small system, trained only on MuST-Cinema, is disadvantaged in terms of the amount of training data compared to the cascade, we utilise synthetic data to boost the performance of the ST system (E2E). To this aim, we automatically translate into German and into French the English transcriptions of the data available for the IWSLT2020 offline speech translation task4 (whenever the translation is not available in the respective target language). To this aim, we use an MT Transformer model achieving 43.2 BLEU points on the WMT’14 test set (Ott et al., 2018) for EN→FR. 3 Must-Cinema"
2020.iwslt-1.26,N19-1202,1,0.877915,"Missing"
2020.iwslt-1.26,2015.iwslt-papers.12,0,0.0366543,"her drawback of the cascaded approach, particularly relevant for the task of subtitling, is that any transcript, no matter how accurate, is subject to information loss in the semiotic shift from the richer audio representation to the poorer text representation. This limitation has been addressed in the past in speech-to-speech translation cascades chiefly for improving the naturalness of the synthesised speech and for resolving ambiguities. This has been performed through acoustic feature vectors related to different prosodic elements, such as duration and power (Kano et al., 2013), emphasis (Do et al., 2015, 2016) and intonation (Aguero et al., 2006; Anumanchipalli et al., 2012). By avoiding intermediate textual representations, end-to-end speech translation (B´erard et al., 2016) can cope with the above limitations. However, its performance and suitability for reliable applications has been impeded by the limited amount of training data available. In spite of this data scarcity problem, it has been recently shown that the gap between the two approaches is closing (Niehues et al., 2018, 2019), especially with specially-tailored architectures (Di Gangi et al., 2019c; Dong et al., 2018) and via ef"
2020.iwslt-1.26,P17-1012,0,0.0198917,"the special symbols, respecting the subtitling constraints and with proper segmentation. This will allow us to have an upper-bound of the performance that NMT can achieve when provided with input already in the form of subtitles. We pre-train large models with the OPUS data used in the cascade but without lowercasing or removing punctuation and then fine-tune them on the full training set of MuST-Cinema. It should be noted that only the MuST-Cinema data contain break symbols. We use the same Transformer architecture as in the cascade system. For all the experiments we use the fairseq toolkit (Gehring et al., 2017). Models are trained until convergence. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is set to 8K operations for the E2E-small and E2E systems while to 50K joint operations for the Cascade and the Template systems. 3.3 Evaluation To evaluate translation quality we use BLEU (Papineni et al., 2002) against the MuST-Cinema test set, both with the break symbols and after removing them (BLEU-nob). For BLEU, an incorrect break symbol would account for an extra n-gram error in the score computation, while BLEU-nob allows us to evaluate only the translation quality without taking into account the"
2020.iwslt-1.26,2020.lrec-1.460,1,0.64024,"t al., 2016), which in this setting represents the length ratio between source and target, and the second inserts length information in the positional encoding of the Transformer. 211 The application of MT (either SMT or NMT) in the works described above is possible only because of the presence of a “perfect” source language transcript, either for the translation itself or for computing the length ratio. To our knowledge today, no work so far has experimented with direct end-to-end ST in the domain of subtitling. 3 Experimental Setup 3.1 Data For the experiments we use the MuST-Cinema corpus (Karakanta et al., 2020),3 which contains (audio, transcription, translation) triplets where the breaks between subtitles have been annotated with special symbols. The symbol &lt;eol> corresponds to a line break inside a subtitle block, while the symbol &lt;eob> to a subtitle block break (the next subtitle comes on a different screen), as seen in the following example from the MuST-Cinema test set: This kind of harassment keeps women &lt;eol> from accessing the internet – &lt;eob> essentially, knowledge. &lt;eob> We experiment with 2 language pairs, English→French and English→German, as languages with different syntax and word orde"
2020.iwslt-1.26,P02-1040,0,0.108957,"ithout lowercasing or removing punctuation and then fine-tune them on the full training set of MuST-Cinema. It should be noted that only the MuST-Cinema data contain break symbols. We use the same Transformer architecture as in the cascade system. For all the experiments we use the fairseq toolkit (Gehring et al., 2017). Models are trained until convergence. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is set to 8K operations for the E2E-small and E2E systems while to 50K joint operations for the Cascade and the Template systems. 3.3 Evaluation To evaluate translation quality we use BLEU (Papineni et al., 2002) against the MuST-Cinema test set, both with the break symbols and after removing them (BLEU-nob). For BLEU, an incorrect break symbol would account for an extra n-gram error in the score computation, while BLEU-nob allows us to evaluate only the translation quality without taking into account the subtitle segmentation. For evaluating the conformity to the constraint of length, we calculate the percentage of subtitles with a maximum length of 42 characters per line (CPL), while for reading speed the percentage of sentences with maximum 21 characters per second (CPS). Since the MuST-Cinema data"
2020.iwslt-1.26,W19-5209,0,0.26414,"typical multilingual subtitling workflow, a subtitler first creates a subtitle template (Georgakopoulou, 2019) by transcribing the source language audio, timing and adapting the text to create proper subtitles in the source language. These source language subtitles (also called captions) are already compressed and segmented to respect the subtitling constraints of length, reading speed and proper segmentation (Cintas and Remael, 2007; Karakanta et al., 2019). In this way, the work of an NMT system is already simplified, since it only needs to translate matching the length of the source text (Matusov et al., 2019; Lakew et al., 2019). However, the essence of a good subtitle goes beyond matching a predetermined length (as, for instance, 42 characters per line in the case of TED talks). Apart from this spatial dimension, subtitling relies heavily on the temporal dimension, which is incorporated in the subtitle templates in the form of timestamps. However, templates are expensive and slow to create and as so, not a viable solution for short turn-around times and individual content creators. Therefore, skipping the template creation process would greatly extend the application of NMT in the subtitling pro"
2020.iwslt-1.26,P16-1162,0,0.0149759,". This will allow us to have an upper-bound of the performance that NMT can achieve when provided with input already in the form of subtitles. We pre-train large models with the OPUS data used in the cascade but without lowercasing or removing punctuation and then fine-tune them on the full training set of MuST-Cinema. It should be noted that only the MuST-Cinema data contain break symbols. We use the same Transformer architecture as in the cascade system. For all the experiments we use the fairseq toolkit (Gehring et al., 2017). Models are trained until convergence. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is set to 8K operations for the E2E-small and E2E systems while to 50K joint operations for the Cascade and the Template systems. 3.3 Evaluation To evaluate translation quality we use BLEU (Papineni et al., 2002) against the MuST-Cinema test set, both with the break symbols and after removing them (BLEU-nob). For BLEU, an incorrect break symbol would account for an extra n-gram error in the score computation, while BLEU-nob allows us to evaluate only the translation quality without taking into account the subtitle segmentation. For evaluating the conformity to the constraint of length, we cal"
2020.iwslt-1.26,2006.amta-papers.25,0,0.0585368,"into account the subtitle segmentation. For evaluating the conformity to the constraint of length, we calculate the percentage of subtitles with a maximum length of 42 characters per line (CPL), while for reading speed the percentage of sentences with maximum 21 characters per second (CPS). Since the MuST-Cinema data come from TED talks, these values were chosen according to the TED subtitling guidelines5 . Finally, for judging the goodness of the segmentation, i.e. the position of the breaks in the translation, we mask all words except for the break symbols and compute Translation Edit Rate (Snover et al., 2006) only for the breaks against the reference translation (TER-br). This will allow us to determine the effort required by a human subtitler to manually correct the segmentation. 4 4.1 Results Translation quality The results are shown Table 1. As far as translation quality is concerned, the best performance is reached, as expected, in the Template setting, where the MT system is provided with “perfect” source language transcriptions. On the MuSTCinema test set, this leads to BLEU scores of 30.62 and 22.08 respectively for French and German. The Cascade setting follows with a BLEU score reduction"
2020.iwslt-1.26,W18-6301,0,0.0164918,"e faster convergence (Bansal et al., 2019). Since the E2E-small system, trained only on MuST-Cinema, is disadvantaged in terms of the amount of training data compared to the cascade, we utilise synthetic data to boost the performance of the ST system (E2E). To this aim, we automatically translate into German and into French the English transcriptions of the data available for the IWSLT2020 offline speech translation task4 (whenever the translation is not available in the respective target language). To this aim, we use an MT Transformer model achieving 43.2 BLEU points on the WMT’14 test set (Ott et al., 2018) for EN→FR. 3 Must-Cinema has been derived from the MuST-C corpus (Di Gangi et al., 2019a), which currently represents the largest multilingual corpus for ST. 4 http://iwslt.org/doku.php?id=offline_ speech_translation 212 Our EN→DE Transformer model using similar settings achieves 25.3 BLEU points on the WMT’14 test set. The resulting training data (both real and synthetic) amount to 1.5M sentences on the target side. We use a different tag to separate the real from the synthetic data. We further use SpecAugment (Park et al., 2019), a technique for online data augmentation, with augment rate o"
2020.iwslt-1.26,2010.jec-1.7,0,0.797463,"Missing"
2020.iwslt-1.8,N19-1006,0,0.0659195,"training an E2E ST system is more difficult because the task is more complex, since it deals with understanding the content of the input audio and translating it into a different language directly and without recurring to intermediate representations. The above-mentioned observations have led researchers to focus on transferring knowledge from MT and ASR systems to improve the ST models. A traditional approach consists in pretraining components: the ST encoder is initialized with the ASR encoder and the ST decoder with the MT decoder. The encoder pretraining has indeed proved to be effective (Bansal et al., 2019), while the decoder pretraining has not demonstrated to be as effective, unless with the addition of adaptation layers (Bahar et al., 2019a). A more promising way to transfer knowledge from an MT model is to use the MT as This paper describes FBK’s participation in the IWSLT 2020 offline speech translation (ST) task. The task evaluates systems’ ability to translate English TED talks audio into German texts. The test talks are provided in two versions: one contains the data already segmented with automatic tools and the other is the raw data without any segmentation. Participants can decide whe"
2020.iwslt-1.8,W19-5206,0,0.0179441,"gmentation are complementary , i.e. they cooperate to produce a better model; ASR. For this task, we used both pure ASR and ST available corpora. They include TED-LIUM 3 (Hernandez et al., 2018), Librispeech (Panayotov et al., 2015), Mozilla Common Voice3 , How2 (Sanabria et al., 2018), the En-De section of MuSTC (Di Gangi et al., 2019a), the Speech-Translation TED corpus provided by the task organizers1 and the En-De section of Europarl-ST (Iranzo-S´anchez et al., 2020). All data was lowercased and punctuation was removed. • combining synthetic and real data marking them with different tags (Caswell et al., 2019) leads to a model which generalizes better; • fine-tuning a model trained with word-level knowledge distillation using the more classical label smoothed cross entropy (Szegedy et al., 2016) significantly improves the results; ST. In addition to the allowed ST corpora (MuSTC, Europarl-ST and the Speech-Translation TED • there is a huge performance gap between data segmented in sentences and data segmented with VAD. Indeed, on the same test set, the score on VAD-segmented data is lower by 5.5 BLEU. 1 http://iwslt.org/doku.php?id=offline_ speech_translation 2 We run the CleaningPipelineMain class"
2020.iwslt-1.8,N19-1202,1,0.859024,"Missing"
2020.iwslt-1.8,D16-1139,0,0.330286,"ask learning has been proposed for speech recognition and has demonstrated to be useful in that scenario, we also include the CTC loss in ASR pretraining. Another topic that received considerable attention is data augmentation. Many techniques have been proposed: in this work we focus on SpecAugment (Park et al., 2019), time stretch and subsequence sampling (Nguyen et al., 2020). Moreover, we used synthetic data generated by automatically translating the ASR datasets with our MT model. This process can also be considered as a sequence-level knowledge distillation technique, named Sequence KD (Kim and Rush, 2016). In this paper, we explore different ways to combine synthetic and real data. We also check if the benefits of the techniques mentioned above are orthogonal and joining them leads to better results. Our experiments show that: To summarize, our submission is characterized by tagged synthetic data, multi-task with CTC loss on the transcriptions, data augmentation and wordlevel knowledge distillation. 2 Training data This section describes the data used to build our models. They include: i) MT corpora (EnglishGerman sentence pairs), for the model used in knowledge distillation; ii) ASR corpora ("
2020.iwslt-1.8,P07-2045,0,0.0138613,"5.5 BLEU. 1 http://iwslt.org/doku.php?id=offline_ speech_translation 2 We run the CleaningPipelineMain class of MMT. 3 https://voice.mozilla.org/ 81 corpus), we generated synthetic data using Sequence KD (see Section 3.2) for all the ASR datasets missing the German reference. Moreover, we generated synthetic data for the En-Fr section of MuST-C. Overall, the combination of real and generated data resulted in a ST training set of 1.5 million samples. All texts were preprocessed by tokenizing them, de-escaping special characters and normalizing punctuation with the scripts in the Moses toolkit (Koehn et al., 2007). The words in both languages were segmented using BPE with 8,000 merge rules learned jointly on the two languages of the MT training data (Sennrich et al., 2016). The audio was converted into 40 log Mel-filter banks with speaker normalization using XNMT (Neubig et al., 2018). We discarded samples with more than 2,000 filter-banks in order to prevent memory issues. 3 3.1 use logarithmic distance penalty. Both our ASR and ST models have 8 attention heads, 512 features for the attention layers and 2,048 hidden units in FFN layers. The ASR model has 8 encoder layers and 6 decoder layers, while th"
2020.iwslt-1.8,L16-1147,0,0.099045,"Missing"
2020.iwslt-1.8,W18-1818,0,0.0130819,". Moreover, we generated synthetic data for the En-Fr section of MuST-C. Overall, the combination of real and generated data resulted in a ST training set of 1.5 million samples. All texts were preprocessed by tokenizing them, de-escaping special characters and normalizing punctuation with the scripts in the Moses toolkit (Koehn et al., 2007). The words in both languages were segmented using BPE with 8,000 merge rules learned jointly on the two languages of the MT training data (Sennrich et al., 2016). The audio was converted into 40 log Mel-filter banks with speaker normalization using XNMT (Neubig et al., 2018). We discarded samples with more than 2,000 filter-banks in order to prevent memory issues. 3 3.1 use logarithmic distance penalty. Both our ASR and ST models have 8 attention heads, 512 features for the attention layers and 2,048 hidden units in FFN layers. The ASR model has 8 encoder layers and 6 decoder layers, while the ST model has 11 encoder layers and 4 decoder layers. The ST encoder is initialized with the ASR encoder (except for the additional 3 layers that are initialized with random values). The decision of having a different number of encoder layers in the two encoders is motivated"
2020.iwslt-1.8,N19-4009,0,0.0343628,"a factor of 2. During the ASR training, we added the CTC loss on the output of the last layer of the encoder. During the ST training, instead, the CTC loss was computed using the output of the last layer pretrained with the ASR encoder, ie. the 8th layer. In this way, the ST encoder has three additional layers which can transform the representation into features which are more convenient for the ST task, as Bahar et al. (2019a) did introducing an adaptation layer. 4 All experiments were executed on a single machine with 8 Tesla K80 with 11GB RAM. Our implementation is built on top of fairseq (Ott et al., 2019), an open source tool based on PyTorch (Paszke et al., 2019). Experimental settings For our experiments, we used the described training sets and we picked the best model according to the perplexity on MuST-C En-De validation set. We evaluated our models on three benchmarks: i) the MuST-C En-De test set segmented at sentence level, ii) the same test set segmented with a VAD (Meignier and Merlin, 2010), and iii) the IWSLT 2015 test set (Cettolo et al., 2015). We trained with Adam (Kingma and Ba, 2015) (betas (0.9, 0.98)). Unless stated otherwise, the learning rate was set to increase linearly fr"
2020.iwslt-1.8,P16-1162,0,0.109465,"e generated synthetic data using Sequence KD (see Section 3.2) for all the ASR datasets missing the German reference. Moreover, we generated synthetic data for the En-Fr section of MuST-C. Overall, the combination of real and generated data resulted in a ST training set of 1.5 million samples. All texts were preprocessed by tokenizing them, de-escaping special characters and normalizing punctuation with the scripts in the Moses toolkit (Koehn et al., 2007). The words in both languages were segmented using BPE with 8,000 merge rules learned jointly on the two languages of the MT training data (Sennrich et al., 2016). The audio was converted into 40 log Mel-filter banks with speaker normalization using XNMT (Neubig et al., 2018). We discarded samples with more than 2,000 filter-banks in order to prevent memory issues. 3 3.1 use logarithmic distance penalty. Both our ASR and ST models have 8 attention heads, 512 features for the attention layers and 2,048 hidden units in FFN layers. The ASR model has 8 encoder layers and 6 decoder layers, while the ST model has 11 encoder layers and 4 decoder layers. The ST encoder is initialized with the ASR encoder (except for the additional 3 layers that are initialized"
2020.lrec-1.460,W18-6402,0,0.0132951,"e laborious and costly. Therefore, there is ample ground for developing automatic solutions for efficiently providing subtitles in multiple languages, reducing human workload and the overall costs of spreading audiovisual content across cultures via subtitling. Recent developments in Neural Machine Translation (NMT) have opened up possibilities for processing inputs other than text within a single model component. This is particularly relevant for subtitling, where the translation depends not only on the source text, but also on acoustic and visual information. For example, in Multimodal NMT (Barrault et al., 2018) the input can be both text and image, and Spoken Language NMT directly receives audio as input (Niehues et al., 2019). These developments can be leveraged in order to reduce the effort involved in subtitling. Training NMT systems, however, requires large amounts of high-quality parallel data, representative of the task. A recent study (Karakanta et al., 2019) questioned the conformity of existing subtitling corpora to subtitling constraints. The authors suggested that subtitling corpora are insufficient for developing end-to-end NMT solutions for at least two reasons; first, because they do n"
2020.lrec-1.460,2012.eamt-1.60,0,0.0203296,"e to take advantage of a larger context. Apart from films and TV series, another source for obtaining multilingual subtitles is TED Talks. TED has been hosting talks (mostly in English) from different speakers and on different topics since 2007. The talks are transcribed and then translated by volunteers into more than 100 languages, and they are submitted to multiple reviewing and approval steps to ensure their quality. Therefore TED Talks provide an excellent source for creating multilingual corpora on a large variety of topics. The Web Inventory of Transcribed and Translated Talks (WIT3 ) (Cettolo et al., 2012) is a multilingual collection of transcriptions and translations of TED talks. Responding to the need for sizeable resources for training end-to-end speech translation systems, MuST-C (Di Gangi et al., 2019) is to date the largest multilingual corpus for speech translation. Like WIT3 , it is also built from TED talks (published between 2007 and April 2019). It contains (audio, transcription, translation) triplets, aligned at sentence level. Based on MuST-C, the International Workshop on Spoken Language Translation (IWSLT) (Niehues et al., 2019) has been releasing data for its campaigns on the"
2020.lrec-1.460,N19-1202,1,0.911292,"Missing"
2020.lrec-1.460,L16-1147,0,0.0332368,"automatic method that provides a translation adapted to the subtitle format would greatly simplify the work of subtitlers, significantly speeding up the process and cutting down related costs. 3. Related work In the following sections we describe several corpora that have been used in Machine Translation (MT) research for subtitling and attempts to create efficient subtitling MT systems. 3.1. Subtitling Corpora The increasing availability of subtitles in multiple languages has led to several attempts of compiling parallel corpora from subtitles. The largest subtitling corpus is OpenSubtitles (Lison and Tiedemann, 2016), which is built from freely available subtitles5 in 60 languages. The subtitles 5 3728 http://www.opensubtitles.org/ come from different sources, hence converting, normalising and splitting the subtitles into sentences was a major task. The corpus contains both professional and amateur subtitles, therefore the quality of the translations can vary. A challenge related to creating such a large subtitle corpus is the parallel sentence alignment. In order to create parallel sentences, the subtitles are merged and information about the subtitle breaks is removed. Even though the monolingual data,"
2020.lrec-1.460,W19-5209,0,0.222454,"guage pairs, but there are only limited project findings available (Bywood et al., 2013; Bywood et al., 2017). The systems involved in the above mentioned initiatives were built with proprietary data, which accentuates the need for offering freely-available subtitling resources to promote research in this direction. Still using SMT approaches, Aziz et al. (2012) modeled temporal and spatial constraints as part of the generation process in order to compress the subtitles only in the language pair English into Portuguese. The only approach utilising NMT for translating subtitles is described in Matusov et al. (2019) for the language pair English into Spanish. The authors proposed a complex pipeline of several elements to customise NMT to subtitle translation. Among those is a subtitle segmentation algorithm that predicts the end of a subtitle line using a recurrent neural network learned from human segmentation decisions, combined with subtitle length constraints established in the subtitling industry. Although they showed reductions in post-editing effort, it is not clear whether the improvements come from the segmentation algorithm or from fine-tuning the system to a domain which is very close to the t"
2020.lrec-1.460,N19-4009,0,0.0315314,"nce and the output is the same sentence annotated with <eol&gt; and <eob&gt; symbols. 6.1. Experimental Setting We create two portions of training data; one containing all the parallel sentences available, and a smaller one containing only the sentences where <eol&gt; symbols are present (top parts in Figure 5). We segment the data into subword units with SentencePiece9 with a different model for each language and 8K vocabulary size. The break symbols are maintained as a single word-piece. We train a sequence-to-sequence model based on the Transformer architecture (Vaswani et al., 2017) using fairseq (Ott et al., 2019). The model is first trained on all the available data (All) and then fine-tuned on the sentences containing the <eol&gt; symbols (ft_eol). This two-step procedure was applied to force the NMT system to take into consideration also the <eol&gt; symbols. This was necessary considering the mismatch in the number between sentences with and without <eol&gt; symbols. For optimisation, we use Adam (Kingma and Ba, 2015) (betas 0.9, 0.98), and dropout (Srivastava et al., 2014) is set to 0.3. We train the initial model for 12 epochs and fine-tune for 6 epochs. As baseline, we use an algorithm based on counting"
2020.lrec-1.460,P02-1040,0,0.107825,"haracters (Count Char). The algorithm consumes characters until the maximum length of 42 is reached and then inserts a break after the last consumed space. If the previous break is an <eob&gt;, it randomly selects between <eob&gt; and <eol&gt;, while if the previous break is an <eol&gt;, it inserts an <eob&gt;. This choice is motivated by the fact that, due to the constraint of having only two lines per block, we cannot have a segmentation containing consecutive <eol&gt; symbols. The last break is always an <eob&gt;. 6.2. Evaluation We evaluate our models using 3 categories of metrics. First, we compute the BLEU (Papineni et al., 2002) between the human reference and the output of the system. We report 9 3731 https://github.com/google/sentencepiece BLEU score 1) on the raw output, with the breaks (general BLEU) and 2) on the output without the breaks. The first computes the n-gram overlap between reference and output. Higher values indicate a high similarity between the system’s output and the desired output. The second ensures that no changes are performed to the actual text, since the task is only about inserting the break symbols, without changing the content. Higher values indicate less changes between the original text"
2020.lrec-1.460,2010.jec-1.7,0,0.64262,"been releasing data for its campaigns on the task of Spoken Language Translation (SLT). MuST-C is a promising corpus for building end-to-end systems which translate from an audio stream directly into subtitles. However, as in OpenSubtitles, the subtitles were merged to create full sentences and the information about the subtitle breaks was removed. In this work, we attempt to overcome this limitation by annotating MuST-C with the missing subtitle breaks in order to provide MuST-Cinema, the largest subtitling corpus available aligned to the audio. 3.2. tion to large-scale production projects. Volk et al. (2010) built SMT systems for translating TV subtitles for Danish, English, Norwegian and Swedish. The SUMAT project, an EU-funded project which ran from 2011 to 2014, aimed at offering an online service for MT subtitling. The scope was to collect subtitling resources, build and evaluate viable SMT solutions for the subtitling industry in nine language pairs, but there are only limited project findings available (Bywood et al., 2013; Bywood et al., 2017). The systems involved in the above mentioned initiatives were built with proprietary data, which accentuates the need for offering freely-available"
2020.lrec-1.460,vondricka-2014-aligning,0,0.0307552,"which we obtained the subtitles offered through Amara.8 We selected the subtitle files from Amara, because, compared to ted2srt, they contain the .srt files that are actually used in the TED videos, therefore the subtitle breaks in these files are accurate. For the test set, we manually selected 5 talks with subtitles available in all 7 languages, which were published after April 2019, in order to avoid any overlap with the training data. Hence, we obtained a common test set for all the languages. Each language was separately and manually aligned to the English transcription using InterText (Vondřička, 2014) in order to obtain parallel sentences. The same steps were performed for the development set, with the difference that we manually selected talks for each 8 Corpus structure and statistics https://amara.org/en/teams/ted/videos/ Train Dev Test Tgt sents tgt w sents tgt w sents tgt w De Es Fr It Nl Pt Ro 229K 265K 275K 253K 248K 206K 236K 4.7M 6.1M 6.3M 5.4M 4.8M 4.2M 4.7M 1088 1095 1079 1049 1023 975 494 19K 21K 22K 20K 20K 18K 9K 542 536 544 545 548 542 543 9.3K 10K 10K 9.6K 10K 10K 10K Table 1: Number of sentences and target side words for each language of MuST-Cinema for the training, devel"
2020.vardial-1.4,D19-5611,0,0.0622913,"ns: i) Masked Language Model (MLM) and ii) Next Sentence Prediction (NPS). This pre-trained model showed outstanding performance when fine-tuned for a variety of NLP tasks. There are many variants of BERT proposed after, among them: Conneau and Lample (2019) add cross-lingual data and only use MLM loss function, and Yang et al. (2019) train the model on the permuted data. Specifically for NMT, different attempts have been done to integrate pre-trained LMs in the sequenceto-sequence model. Among others, ELMo was used for initializing the embedding layer in the NMT system (Edunov et al., 2019). Clinchant et al. (2019) used BERT for initializing the encoder or embedding layer of the NMT systems. Then the BERT models are fixed or fine-tuned along with other variables of the model. In (Zhu et al., 2020), a method was proposed to fuse the representations obtained from BERT with each layer of the encoder and decoder in the NMT model through attention mechanisms. We take a similar approach to (Clinchant et al., 2019) in initializing source embedding or the encoder of the NMT system while training a generic NMT systems. Here, this is done for the first time in a machine-oriented setting and in zero-shot condition"
2020.vardial-1.4,D18-1269,0,0.0118571,"blicly available and, for many tasks, is limited to high-resource languages like English. A possible solution to leverage these services in low-resource settings is using Neural Machine Translation (NMT) in the so-called “translation-based” approach, where a text in the low-resource language is first translated into a high-resource one for which dedicated NLP tools exist. Then, the translated text is processed by these downstream tools and, finally, the results are propagated back to the source language. Although the translation-based approach shows promising results in low-resource settings (Conneau et al., 2018), it still has drawbacks. First, the output quality of current NMT models is not perfect yet (Koehn and Knowles, 2017). Second, even a good translation can alter some traits in the text, which are essential for the downstream NLP tool. This, for instance, is typical for sentiment traits, whose loss can result in final performance drops in sentiment classification tasks (Mohammad et al., 2016). Finally, state-of-the-art NMT models are trained considering humans as end-users and hence optimized to maximize human-oriented objectives like fluency and semantic equivalence of the translation with re"
2020.vardial-1.4,N19-1423,0,0.128582,"ges into a resource-rich one, the MO-Reinforce algorithm is run in three different conditions by using source data in: i) the same language of the test set (tuning on Italian - testing on Italian); ii) a different language, but closely related to the one of the test set, (Italian - Spanish), and iii) a different and distant language (German - Spanish). The main goal of these experiments is to show that MO-Reinforce leveraging a multilingual NMT and data in a closely-related language is able to overcome the lack of source labelled data in a specific task. Moreover, recently, multilingual BERT (Devlin et al., 2019) has shown good performance when finetuned for downstream tasks. Multilingual BERT is a pre-trained model built on the union of unlabeled data for more than 100 languages. The availability of unlabeled text in significant quantities in different languages helps this model to extract valuable knowledge about the languages resulting in good performance for different tasks. To strengthen the capability of the NMT system to represent the source sentence, we try different approaches to incorporate multilingual BERT in the NMT system’s encoder. Our goal, in this case, is to show that BERT-based NMT"
2020.vardial-1.4,N19-1409,0,0.0113032,"using two loss functions: i) Masked Language Model (MLM) and ii) Next Sentence Prediction (NPS). This pre-trained model showed outstanding performance when fine-tuned for a variety of NLP tasks. There are many variants of BERT proposed after, among them: Conneau and Lample (2019) add cross-lingual data and only use MLM loss function, and Yang et al. (2019) train the model on the permuted data. Specifically for NMT, different attempts have been done to integrate pre-trained LMs in the sequenceto-sequence model. Among others, ELMo was used for initializing the embedding layer in the NMT system (Edunov et al., 2019). Clinchant et al. (2019) used BERT for initializing the encoder or embedding layer of the NMT systems. Then the BERT models are fixed or fine-tuned along with other variables of the model. In (Zhu et al., 2020), a method was proposed to fuse the representations obtained from BERT with each layer of the encoder and decoder in the NMT model through attention mechanisms. We take a similar approach to (Clinchant et al., 2019) in initializing source embedding or the encoder of the NMT system while training a generic NMT systems. Here, this is done for the first time in a machine-oriented setting a"
2020.vardial-1.4,W17-3204,0,0.0145661,"verage these services in low-resource settings is using Neural Machine Translation (NMT) in the so-called “translation-based” approach, where a text in the low-resource language is first translated into a high-resource one for which dedicated NLP tools exist. Then, the translated text is processed by these downstream tools and, finally, the results are propagated back to the source language. Although the translation-based approach shows promising results in low-resource settings (Conneau et al., 2018), it still has drawbacks. First, the output quality of current NMT models is not perfect yet (Koehn and Knowles, 2017). Second, even a good translation can alter some traits in the text, which are essential for the downstream NLP tool. This, for instance, is typical for sentiment traits, whose loss can result in final performance drops in sentiment classification tasks (Mohammad et al., 2016). Finally, state-of-the-art NMT models are trained considering humans as end-users and hence optimized to maximize human-oriented objectives like fluency and semantic equivalence of the translation with respect to the source sentence. However, these objectives are not necessarily the optimal ones to exploit an NLP tool at"
2020.vardial-1.4,P17-1138,0,0.0726552,"tions, at the risk of cumulative errors at each step. In (Ranzato et al., 2016), the authors proposed a gradual 37 shifting from token-level maximum likelihood to sentence-level BLEU score to expose the model to its prediction instead of the reference translation. Shen et al. (2016) extended this idea by adopting minimum risk training (Goel and Byrne, 2000) to directly optimize task-specific metrics like BLEU or TER in NMT. Bahdanau et al. (2017) optimized the policy using the actor-critic algorithm. In another line of research, in situations where the reference translation is not available, (Kreutzer et al., 2017) proposed bandit structured prediction, which describes a stochastic optimization framework to leverage “weak” feedbacks collected from the user (e.g. Likert scores about output quality). A common trait of all the above-mentioned works is that they all consider humans as the end-users of NMT system’s output, which should hence adhere to the human criteria of fluency and adequacy. Tebbifakhr et al. (2019) recently proposed a paradigm shift by considering machines as the final consumers on machine-translated text, which should hence maximize “fitness for purpose” criteria (i.e. providing easy-to"
2020.vardial-1.4,P02-1040,0,0.113448,"ally distributed in the 4 classes. For the English downstream classifier we use whole 10K documents while, to simulate the low-resource setting, we sample 100 documents for each class from the Italian training set. In addition, we collect the same amount of data also for German. This data is used to fine-tune the Spanish system on a distant language and compare downstream performance results achieved by our approach in the two fine-tuning conditions (close – Es-It – vs distant – Es-De – languages). Evaluation metrics We evaluate the translation performance of the NMT systems using BLEU Score (Papineni et al., 2002) and the classification performance with macro average F1 Score. 5 5.1 Results The NMT systems’ translation performance We start the evaluation by comparing the translation performance of the three different NMT systems. The performance of the NMT systems in terms of BLEU score is reported in Table 2. As shown, BERT 41 Es-En It-En De-En Europarl JRC Wikipedia ECB TED KDE News11 News Total 2M 2M 2M 0.8M 0.8M 0.7M 1.8M 1M 2.5M 0.1M 0.2M 0.1M 0.2M 0.2M 0.1M 0.2M 0.3M 0.3M 0.3M 0.04M 0.2M 0.2M 0.02M 0.2M 5.6M 4.56M 6.1M Table 1: Number of sentences in the parallel corpora used for training the gen"
2020.vardial-1.4,D14-1162,0,0.0840295,"tput translations that are easier to be classified by the downstream tool. None of them, however, explored the application of the approach in zero-shot settings, nor focused on how language closeness/distance affects final performance as done in this paper. Pre-training a neural network or parts of it with existing models is a common approach in several NLP tasks and it allows developers to speed up the training, to leverage different types of training data and to improve the overall performance of the learning system. Among various solutions, word2vec (Mikolov et al., 2013) and its variants (Pennington et al., 2014; Levy and Goldberg, 2014) have been the first resources used to pre-training the embeddings in an NMT system. They provide embedded vectors of individual words and have been widely used in NLP. Recently, pre-trained Language Models (LM) showed better performance when fine-tuned for downstream tasks. ELMo (Peters et al., 2018) is among the first pre-trained LMs, which is based on BiLSTM architecture trained on monolingual data. The authors showed that combining the representations from different layers obtains contextual-aware word representations that can be used for other NLP tasks. Right af"
2020.vardial-1.4,N18-1202,0,0.0184938,"mmon approach in several NLP tasks and it allows developers to speed up the training, to leverage different types of training data and to improve the overall performance of the learning system. Among various solutions, word2vec (Mikolov et al., 2013) and its variants (Pennington et al., 2014; Levy and Goldberg, 2014) have been the first resources used to pre-training the embeddings in an NMT system. They provide embedded vectors of individual words and have been widely used in NLP. Recently, pre-trained Language Models (LM) showed better performance when fine-tuned for downstream tasks. ELMo (Peters et al., 2018) is among the first pre-trained LMs, which is based on BiLSTM architecture trained on monolingual data. The authors showed that combining the representations from different layers obtains contextual-aware word representations that can be used for other NLP tasks. Right after ELMo, BERT (Devlin et al., 2019) was proposed based on the encoder of Transformer (Vaswani et al., 2017). This model was trained on unlabeled data using two loss functions: i) Masked Language Model (MLM) and ii) Next Sentence Prediction (NPS). This pre-trained model showed outstanding performance when fine-tuned for a vari"
2020.vardial-1.4,L18-1560,0,0.0178347,"l, we tokenize and encode each side of the parallel corpora with 32K byte-pair encoding rules. For the other Bert-based NMT systems, on the source side, we use the BERT encoder setting to split the sentences to the tokens. We evaluate our translation-based classification approaches on a multilingual document classification task where Spanish and Italian news documents have to be automatically annotated with domain labels. Our classification data consists of the first sentence of each document that, according to (Bell, 1991) is a good proxy to determine the domain of news texts. The data used (Schwenk and Li, 2018) cover 4 domains: Corporate/Industrial, Economics, Government/Social, and Markets. The training, development, and test sets for each language respectively contain 10K, 1K, and 4K documents, equally distributed in the 4 classes. For the English downstream classifier we use whole 10K documents while, to simulate the low-resource setting, we sample 100 documents for each class from the Italian training set. In addition, we collect the same amount of data also for German. This data is used to fine-tune the Spanish system on a distant language and compare downstream performance results achieved by"
2020.vardial-1.4,P16-1162,0,0.0185414,"orating the multilingual BERT in our NMT system to take advantage of its embedded knowledge. In our setting, where the NMT system serves the downstream tool having a better representation of the input can be beneficial to generate a better and more useful translation, in particular in zero-shot languages. We employ two different approaches to incorporating BERT in our NMT system based on Transformer (Vaswani et al., 2017). In the standard Transformer, all the variables of the model are randomly initialized and then trained. In our implementation of this model, we use Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to extract the vocabulary from the source and target side of the parallel data. The following paragraphs explains the details of each BERT-based NMT implementations. BERT Encoder The first approach to incorporating BERT in our NMT system is initializing the encoder of the NMT system using the weights of the multilingual BERT. In this approach instead of using BPE, we tokenize the input sentence to sub-words using the BERT tokenizer and add special tokens [CLS] and [SEP] to the beginning and the end of the sentence. Then the tokenized sentence is encoded with multilingual BERT and the encoded"
2020.vardial-1.4,P16-1159,0,0.0274879,"ess the exposure bias problem inside sequence-to-sequence models, which refers to the discrepancy between training and inference time in NMT systems. During training, in fact, the model is exposed to the reference translations, while at inference time the model generates the translation based on its own (typically sub-optimal) predictions, at the risk of cumulative errors at each step. In (Ranzato et al., 2016), the authors proposed a gradual 37 shifting from token-level maximum likelihood to sentence-level BLEU score to expose the model to its prediction instead of the reference translation. Shen et al. (2016) extended this idea by adopting minimum risk training (Goel and Byrne, 2000) to directly optimize task-specific metrics like BLEU or TER in NMT. Bahdanau et al. (2017) optimized the policy using the actor-critic algorithm. In another line of research, in situations where the reference translation is not available, (Kreutzer et al., 2017) proposed bandit structured prediction, which describes a stochastic optimization framework to leverage “weak” feedbacks collected from the user (e.g. Likert scores about output quality). A common trait of all the above-mentioned works is that they all consider"
2020.vardial-1.4,D19-1140,1,0.842974,"This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 36 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 36–46 Barcelona, Spain (Online), December 13, 2020 of NMT. Rather, models should be adapted in a machine-oriented way that is optimal for automatic processing of their output. Traditionally, NMT models are trained using parallel corpora, consisting of sentences in the source language and their human translations in the target language. Recently, Tebbifakhr et al. (2019) proposed Machine-Oriented Reinforce (MO-Reinforce), a method based on Reinforcement Learning to pursue machine-oriented objectives for sentence-level classification tasks. In a nutshell: given the output of the downstream classifier (i.e. a probability distribution over the labels), MO-Reinforce considers the probability given to the true class as the collected reward from the downstream classifier. By maximizing the expected value of the collected reward, MO-Reinforce adapts the NMT model’s behavior to generate outputs that are easier to label by the downstream classifier. Although NMT model"
2020.vardial-1.4,2020.eamt-1.25,1,0.861287,"uman criteria of fluency and adequacy. Tebbifakhr et al. (2019) recently proposed a paradigm shift by considering machines as the final consumers on machine-translated text, which should hence maximize “fitness for purpose” criteria (i.e. providing easy-to-process input to downstream NLP components). To this aim, they adopted the REINFORCE (Williams, 1992) approach to leverage the feedback from a downstream task (e.g. classification accuracy in a polarity detection task) to update the agent’s policy (the probability of taking a certain action α when in state s). This approach was extended in (Tebbifakhr et al., 2020) to address different NLP tasks in parallel, with a single NMT engine using the same policy. Both works showed that leveraging the downstream classifier’s feedback adapts the NMT system to output translations that are easier to be classified by the downstream tool. None of them, however, explored the application of the approach in zero-shot settings, nor focused on how language closeness/distance affects final performance as done in this paper. Pre-training a neural network or parts of it with existing models is a common approach in several NLP tasks and it allows developers to speed up the tr"
2020.vardial-1.4,tiedemann-2012-parallel,0,0.0197209,"g MO-Reinforce by disabling and enabling the dropout while generating the translation candidates. We keep the parameter K in MO-Reinforce equal to 5, and adapt each NMT system for 50 epochs and choose the best checkpoint based on the performance on the development set. For simulating the downstream classifier we use English BERT fine-tuned for the downstream task using the English labeled data. Data For pre-training the NMT systems, we use the parallel corpora reported in Table 1, and we evaluate the Spanish and Italian translation performance of the NMT systems on the Ubuntu parallel corpus (Tiedemann, 2012). For the Transformer model, we tokenize and encode each side of the parallel corpora with 32K byte-pair encoding rules. For the other Bert-based NMT systems, on the source side, we use the BERT encoder setting to split the sentences to the tokens. We evaluate our translation-based classification approaches on a multilingual document classification task where Spanish and Italian news documents have to be automatically annotated with domain labels. Our classification data consists of the first sentence of each document that, according to (Bell, 1991) is a good proxy to determine the domain of n"
2020.wmt-1.75,2004.iwslt-evaluation.1,0,0.18357,"Missing"
2020.wmt-1.75,W18-1804,1,0.593388,"better than the baseline. This also happens for the two primary submissions to the English-Chinese subtask which, however, are both significantly worse than human post-edits. All in all, the improvements observed on both the language pairs can be most likely ascribed to the lower quality of the initial translations to be corrected. On English-German, the baseline (31.56 TER, 50.21 BLEU) was indeed much lower “narrow” IT domain allowed to test APE technology on the challenging scenario represented by the generic domain of Wikipedia articles. Indeed, as shown in the previous rounds of the task (Chatterjee et al., 2018a, 2019), the high level of repetitiveness of IT data makes this domain easier to model compared to a generic and less repetitive domain, both for MT and APE technology. Moreover, fixing the output of generic NMT models that are not domain-adapted allowed to test APE on lower-quality initial data and verify its potential as a downstream domain adaptation component. On the other side, the disadvantage of changing domain is the reduced possibility to compare results and measure progress across years. Specifically, the lower quality of the original sentences to be corrected (and, in turn, the lar"
2020.wmt-1.75,W16-2378,0,0.01779,"n of the target, which was produced by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances are left apart to measure system performance. For the English-German subtask, the training, development and test sets respectively contain 7,000, 1,000 and 1,000 triplets. Participants were also provided with two additional training resources, which were widely used in the previous rounds. One is the corpus of 4.5 million artificially-generated post-editing triplets described in (Junczys-Dowmunt and Grundkiewicz, 2016). The other resource is the English-German section of the eSCAPE corpus (Negri et al., 2018). It comprises 14.5 million instances, which were artificially generated both via phrase-based and neural translation (7.25 millions each) of the same source sentences. Also for the English-Chinese subtask, the training, development and test sets respectively contain 7,000, 1,000 and 1,000 triplets. For this language pair, however, no additional training resources were provided. Task description In continuity with all the previous rounds of the APE task, participants were provided with training and deve"
2020.wmt-1.75,W04-3250,0,0.0712656,"wei Translation Services Center & East China Normal University, China (Yang et al., 2020) Korea Advanced Institute of Science & Technology, Republic of Korea Pohang University of Science and Technology, Republic of Korea (Lee et al., 2020b) Pohang University & Electronics and Telecomm. Res. Inst., Republic of Korea (Lee et al., 2020a) Table 2: Participants in the WMT20 Automatic Post-Editing task. 4 for comparison with participants’ submissions.7 For each submitted run, the statistical significance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). items being perfect. In the light of previous years’ observations, both the subtasks hence seem to be easier to handle. As discussed in Section 4, also this year’s evaluation results confirm the strict correlation between the quality of the initial translations, the distribution of TER scores across the test items, and the actual potential of APE. 2.2 3 Six teams submitted a total of 11 runs for the English-German subtask. Two of them participated also in the English-Chinese subtask by submitting 2 runs each. Participants are listed in Table 2, and a short description of their systems is pro"
2020.wmt-1.75,P15-2026,1,0.842187,"domain of the data changed from Information Technology (IT) to Wikipedia articles. The third major novelty factor consists in the type of MT systems used to generate the translations to be corrected. Although for the third year in a row the task focused on translations produced by neural MT (NMT) systems, this year these models were not adapted to the target domain. These radical changes have advantages and disadvantages. On one side, moving away from the Introduction MT Automatic Post-Editing (APE) is the task of automatically correcting errors in a machinetranslated text. As pointed out by (Chatterjee et al., 2015), from the application point of view, the task is motivated by its possible uses to: 646 Proceedings of the 5th Conference on Machine Translation (WMT), pages 646–659 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics The overall evaluation results show significant improvements over the baseline on both the language directions. On English-German, where the “do-nothing” baseline (see Section 2.3) was 31.56 TER (Snover et al., 2006) and 50.21 BLEU (Papineni et al., 2002), the top-ranked system (20.21 TER, 66.89 BLEU) shows an impressive -11.35 TER reduction, which cor"
2020.wmt-1.75,2005.mtsummit-papers.11,0,0.221944,"h Wikipedia pages were the same, the source sentences eventually used to build the datasets for the two language pairs are different as they were randomly selected. The released training and development sets consist of (source, target, human post-edit) triplets in which: • The source (SRC) is a tokenized English sentence; 2 Both the NMT systems are based on the standard Transformer architecture (Vaswani et al., 2017) and follow the implementation details described in (Ott et al., 2018). They were trained on publicly available MT datasets including Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005), summing up to 23.7M parallel sentences for English-German and 22.6M for English-Chinese. • The target (TGT) is a tokenized German/Chinese translation of the source, which was produced by a generic, black-box system unknown to participants. For both the languages, translations were obtained from neu648 Language Domain MT type Rep. Rate SRC Rep. Rate TGT Rep. Rate PE Baseline TER Baseline BLEU δ TER 2015 En-Es News PBSMT 2.905 3.312 3.085 23.84 n/a +0.31 2016 En-De IT PBSMT 6.616 8.845 8.245 24.76 62.11 -3.24 2017 En-De IT PBSMT 7.216 9.531 8.946 24.48 62.49 -4,88 2017 De-En Medical PBSMT 5.22"
2020.wmt-1.75,2020.wmt-1.81,0,0.0702564,"Missing"
2020.wmt-1.75,2020.acl-main.747,0,0.060906,"Missing"
2020.wmt-1.75,2020.wmt-1.82,0,0.270305,"Missing"
2020.wmt-1.75,2020.wmt-1.83,0,0.0597609,"Missing"
2020.wmt-1.75,W19-5412,0,0.0123013,"ing module implants the simulated errors into the target text of the parallel corpora, so to exploit a synthetic MT output during the training phase. The quantity of noise is determined by using the TER distribution of the official training set. They then applied the same generation method proposed in (Negri et al., 2018), so to create a synthetic APE corpus to be used as additional training data. For this data construction process, they used the parallel corpora and the NMT model released for the WMT20 Quality Estimation shared task. As APE model, they chose the sequential model proposed in (Lee et al., 2019), applying some minor modifications to increase the training efficiency. They submitted two ensemble models. Their primary submission (TERNoise-Ops-Ens8) is an ensemble of eight runs. It was obtained by first selecting the top-5 runs having the lowest TER on the development set, for three individual weight initializations. Out of them, they then selected the top-2 runs showing most frequent corrections for each of the four edit operations to form the ensemble. The contrastive submission (TERNoise-nFold-Ens8) is an ensemble of eight runs obtained from models trained/validated in a 4-fold settin"
2020.wmt-1.75,W19-6721,0,0.0377166,"Missing"
2020.wmt-1.75,W19-5413,0,0.0432373,"Missing"
2020.wmt-1.75,W19-5204,1,0.839102,"inguishable to our evaluators. This interesting finding can be motivated by a number of reasons (the type/quality/quantity of data, the size of the sample, the number of collected judgements) that suggest to avoid exaggerated claims about a reached human parity. Nonetheless, we take it as indicator of a steady progress of APE research. Interestingly, 5 out of 6 APE submissions perform significantly better than the original MT output test.mt, demonstrating that APE can be used to improve machine translation output even for high-resource language settings like EnglishGerman, as already shown by Freitag et al. (2019). These findings are different from last year’s APE task (Chatterjee et al., 2019) where none of the English-German APE submissions was significantly better than the raw MT output. Human evaluation In order to complement the automatic evaluation of APE submissions, manual evaluation of the primary systems submitted (seven for EnglishGerman, three for English-Chinese) was conducted. In this section, we present the evaluation procedure, as well as the results obtained. 6.1 Evaluation procedure We evaluated the overall quality of the MT and PE output using source-based direct assessment (Graham e"
2020.wmt-1.75,N19-4009,0,0.0141616,"unseen data. Then, the top-2 runs for each fold were selected to form the ensemble. Huawei (HW-TSC). Huawei participated both in the English-German and English-Chinese subtasks. Their system basically follows the architecture of last year’s winning system (Lopes et al., 2019), where src and mt sentences are concatenated as input to the encoder, and the decoder is used for decoding the pe sentence. However, there are several differences with respect to (Lopes et al., 2019). First, instead of using a pretrained BERT model, the system relies on a Transformer NMT model (implemented with fairseq (Ott et al., 2019)), pre-trained on the WMT19 news translation corpora. Second, the model integrates bottleneck adapter layers to prevent from over-fitting. Third, external MT candidates (from Google Translate) are exploited as a source of auxiliary information. This results in a longer input sequence composed of (src, mt, auxiliary mt) triplets. Due to the domain change introduced this year, system’s training does not exploit the supplied additional corpora for data augmentation. Finally, the system does not include methods to prevent over-correction, such as the penalty mentioned in (Lopes et al., 2019). POST"
2020.wmt-1.75,2020.wmt-1.85,0,0.054402,"Missing"
2020.wmt-1.75,W18-6301,0,0.02079,"ted from English Wikipedia articles and then automatically translated in the two target languages. Although the original English Wikipedia pages were the same, the source sentences eventually used to build the datasets for the two language pairs are different as they were randomly selected. The released training and development sets consist of (source, target, human post-edit) triplets in which: • The source (SRC) is a tokenized English sentence; 2 Both the NMT systems are based on the standard Transformer architecture (Vaswani et al., 2017) and follow the implementation details described in (Ott et al., 2018). They were trained on publicly available MT datasets including Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005), summing up to 23.7M parallel sentences for English-German and 22.6M for English-Chinese. • The target (TGT) is a tokenized German/Chinese translation of the source, which was produced by a generic, black-box system unknown to participants. For both the languages, translations were obtained from neu648 Language Domain MT type Rep. Rate SRC Rep. Rate TGT Rep. Rate PE Baseline TER Baseline BLEU δ TER 2015 En-Es News PBSMT 2.905 3.312 3.085 23.84 n/a +0.31 2016 En-De IT PBSMT"
2020.wmt-1.75,N16-1004,0,0.0671598,"each. Similar to last year, all teams developed their systems based on neural technology, which confirms to be the state-of-the-art approach to APE. In most of the cases (see Section 3), participants experimented with the Transformer architecture (Vaswani et al., 2017), either directly or by adapting it to the task. As in previous rounds, their systems exploit information both from the MT output to be corrected and from the corresponding source sentence. This was done either by concatenating the two, as in last year’s winning system (Lopes et al., 2019), or by means of multi-source solutions (Zoph and Knight, 2016) successfully explored in the past (Libovick´y et al., 2016; Chatterjee et al., 2017). Following the recent trends in other NLP areas, the integration of pre-trained BERT-like language models was also considered. Model ensembling and the integration of word/sentence-level quality estimation techniques geared to APE (similar to (Chatterjee et al., 2018b)) were also explored. Finally, also this year participants took advantage of data augmentation techniques, either by creating their own eSCAPE-like corpora (Negri et al., 2018), or by generating synthetic data by adding artificial noise to simul"
2020.wmt-1.75,P02-1040,0,0.107928,") is the task of automatically correcting errors in a machinetranslated text. As pointed out by (Chatterjee et al., 2015), from the application point of view, the task is motivated by its possible uses to: 646 Proceedings of the 5th Conference on Machine Translation (WMT), pages 646–659 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics The overall evaluation results show significant improvements over the baseline on both the language directions. On English-German, where the “do-nothing” baseline (see Section 2.3) was 31.56 TER (Snover et al., 2006) and 50.21 BLEU (Papineni et al., 2002), the top-ranked system (20.21 TER, 66.89 BLEU) shows an impressive -11.35 TER reduction, which corresponds to a +16.68 gain in terms of BLEU score. Considering all the submissions, the average gain is -4.89 TER and +6.5 BLEU points, with only one system performing slightly worse than the baseline. Different from last year, where the differences between the top four submissions were not statistically significant, this year we have a clear winner, whose best submission is 6.78 TER points (and 11.12 BLEU points) above the second ranked team. Nevertheless, though possibly favoured by the relative"
2020.wmt-1.75,N07-1064,0,0.0679104,"lty strategies inspired by (Lopes et al., 2019). Baseline In continuity with the previous rounds, the official baseline results were the TER and BLEU scores calculated by comparing the raw MT output with human post-edits. In practice, the baseline APE system is a “do-nothing” system that leaves all the test targets unmodified. Baseline results, the same shown in Table 1, are also reported in Tables 3 and 7 In addition to the do-nothing baseline, in the first three rounds of the task we also compared systems’ performance with a re-implementation of the phrase-based approach firstly proposed by Simard et al. (2007), which represented the common backbone of APE systems before the spread of neural solutions. As shown in (Bojar et al., 2016, 2017), the steady progress of neural APE technology has made the phrasebased solution not competitive with current methods reducing the importance of having it as an additional term of comparison. Since 2018, we hence opted for considering only one baseline. 5 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl 6 651 Bering Lab (BerlingLab). Bering Lab participated only in the English-German subt"
2020.wmt-1.75,2006.amta-papers.25,0,0.732529,"uction MT Automatic Post-Editing (APE) is the task of automatically correcting errors in a machinetranslated text. As pointed out by (Chatterjee et al., 2015), from the application point of view, the task is motivated by its possible uses to: 646 Proceedings of the 5th Conference on Machine Translation (WMT), pages 646–659 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics The overall evaluation results show significant improvements over the baseline on both the language directions. On English-German, where the “do-nothing” baseline (see Section 2.3) was 31.56 TER (Snover et al., 2006) and 50.21 BLEU (Papineni et al., 2002), the top-ranked system (20.21 TER, 66.89 BLEU) shows an impressive -11.35 TER reduction, which corresponds to a +16.68 gain in terms of BLEU score. Considering all the submissions, the average gain is -4.89 TER and +6.5 BLEU points, with only one system performing slightly worse than the baseline. Different from last year, where the differences between the top four submissions were not statistically significant, this year we have a clear winner, whose best submission is 6.78 TER points (and 11.12 BLEU points) above the second ranked team. Nevertheless, t"
2020.wmt-1.75,2020.eamt-1.20,0,0.0220628,"well as the results obtained. 6.1 Evaluation procedure We evaluated the overall quality of the MT and PE output using source-based direct assessment (Graham et al., 2013; Cettolo et al., 2017; Bojar et al., 2018). We used the same instructions that are used in the News Translation track of WMT2020. We hired 25 professional linguists for EnglishGerman and 25 professional linguists for EnglishChinese. All involved linguists were either native speaker in German or Chinese. We acquired only a single rating per sentence as we found that professional linguists were more reliable than crowd workers (Toral, 2020). For adequacy, we asked annotators to assess the semantic similarity between the source and a candidate text, labelled as “source text” and “candidate translation”, respectively. The annotation interface implements a slider widget to encode perceived similarity as a value between 0 and 100. Note that the exact value is hidden from the human, and can only be guessed based on the positioning of the slider. Candidates are displayed in random order, so to prevent biased assessments. For our human evaluation campaign, we also include the human post-edits (test.pe) and the unedited, MT output (test"
2020.wmt-1.75,W13-2231,1,0.8362,"Missing"
2021.acl-long.224,D11-1033,0,0.0217744,"Missing"
2021.acl-long.224,2020.iwslt-1.3,0,0.734347,"ith ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce input length, and ii) penalties bias"
2021.acl-long.224,N19-1006,0,0.0282767,"The problem has been mainly tackled with data augmentation and knowledge transfer techniques. Data augmentation consists in producing artificial training corpora by altering existing datasets or by generating (audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019). Knowledge transfer (Gutstein et al., 2008) consists in passing (here to ST) the knowledge learnt by a neural network trained on closely related tasks (here, ASR and MT). Existing ASR models have been used for encoder pre-training (B´erard et al., 2018; Bansal et al., 2019; Bahar et al., 2019a) and multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Indurthi et al., 2020). Existing neural MT models have been used for decoder pre-training (Bahar et al., 2019a; Inaguma et al., 2020), joint learning (Indurthi et al., 2020; Liu et al., 2020) and knowledge distillation (Liu et al., 2019). Previous comparisons. Most of the works on direct ST also evaluate the proposed solutions against a cascade counterpart. The conclusions, however, are discordant. Looking at recent works, Pino et al. (2019) show similar scores, Indurthi et al. (2020) report hi"
2021.acl-long.224,D19-5304,0,0.0221619,"ffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first dire"
2021.acl-long.224,W04-3250,0,0.391428,"ww.cs.umd.edu/˜snover/tercom 6 BLEU8 (Post, 2018) and TER scores computed only on the official MuST-C Common references. C D C es D C it D de HTER 28.65 30.22 29.96 28.19∗ 25.69 26.14 PE Set mTER BLEU 24.41 28.96 25.60 28.46 25.30 34.05∗ 24.02∗ 32.17 23.29 30.04∗ 23.26 28.81 TER 53.23 52.56 50.75 51.08 54.01 54.06 M. Common BLEU TER 28.86 53.93 29.05 52.77∗ 32.93∗ 53.21∗ 31.98 54.00 28.56 56.29 28.56 55.35∗ Table 1: Performance of (C)ascade and (D)irect systems on the PE-sets and MuST-C Common test sets. Statistically significant differences (∗ ) are computed with Paired Bootstrap Resampling (Koehn, 2004). A bird’s-eye view of the results shows that, in more than half of the cases, performance differences between cascade and direct systems are not statistically significant. When they are, the raw count of wins for the two approaches is the same (4), attesting their substantial parity. Looking at our primary metrics (HTER and mTER), systems are on par on en-it and en-de, while for en-es the direct approach significantly outperforms the cascade one. This difference, however, does not emerge with the other metrics. Indeed, BLEU and TER scores computed against the official references are less cohe"
2021.acl-long.224,C96-1075,0,0.130421,"bility across languages and domains. At the same time, however, they suffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models,"
2021.acl-long.224,W18-1818,0,0.0340255,"Missing"
2021.acl-long.224,N19-4009,0,0.01971,"Missing"
2021.acl-long.224,2012.iwslt-papers.18,0,0.0208779,"hey suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and P"
2021.acl-long.224,2006.amta-papers.25,0,0.267854,", we manually checked the selected samples and kept only those segments for which the audio-transcripttranslation alignment was correct. Each of the three resulting test sets – henceforth PE-sets – is composed of 550 segments, corresponding to about 10,000 English source words. Post-editing. A key element of our multi-faceted analysis is human post-editing (PE), which consists in manually correcting systems’ output according to the input (the source audio in our case). In PEbased evaluation, the original output is compared against its post-edited version using distance-based metrics like TER (Snover et al., 2006). This allows for counting only the true errors made by a system, without penalising differences due to linguistic variation as it happens when exploiting independent references. This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b). To collect the post-edits for our study, we strictly followed the methodology of the IWSLT 20132017 evaluation campaigns (Cettolo et al., 2013), which offered us a c"
2021.acl-long.224,W09-0441,0,0.0747003,"Missing"
2021.acl-long.224,P19-1115,0,0.0127146,"n problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing"
2021.acl-long.224,W18-6319,0,0.0184861,"based7 metrics: i) human-targeted TER (HTER) computed between the automatic translation and its human post-edited version, and ii) multi-reference TER (mTER) computed against the closest reference among the three available ones (two post-edits and the official reference from MuST-C). The latter metric better accounts for post-editors’ variability, making the evaluation more reliable and informative. For the sake of completeness, in Table 1 we also report Sacre5 www.matecat.com The ad-hoc ST PE guidelines given to translators are included in Appendix B. 7 www.cs.umd.edu/˜snover/tercom 6 BLEU8 (Post, 2018) and TER scores computed only on the official MuST-C Common references. C D C es D C it D de HTER 28.65 30.22 29.96 28.19∗ 25.69 26.14 PE Set mTER BLEU 24.41 28.96 25.60 28.46 25.30 34.05∗ 24.02∗ 32.17 23.29 30.04∗ 23.26 28.81 TER 53.23 52.56 50.75 51.08 54.01 54.06 M. Common BLEU TER 28.86 53.93 29.05 52.77∗ 32.93∗ 53.21∗ 31.98 54.00 28.56 56.29 28.56 55.35∗ Table 1: Performance of (C)ascade and (D)irect systems on the PE-sets and MuST-C Common test sets. Statistically significant differences (∗ ) are computed with Paired Bootstrap Resampling (Koehn, 2004). A bird’s-eye view of the results sh"
2021.acl-long.224,2020.iwslt-1.9,0,0.44219,"tz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce input length, and ii) penalties biasing attention to local context in the encoder self-attention layers (Povey et al., 2018; Sperber et al., 2018; Di Gangi et al., 2019b). Though effective, these architectures have to confront with training data paucity, a critical bottleneck for neural solutions. The problem has been mainly tackled with data augmentation and knowledge transfer techniques. Data augmentation consists in p"
2021.acl-long.224,2020.acl-main.661,0,0.0171621,"ences 5 240 191 45 74 215 234 54 47 231 212 55 52 6.15 6.00 6.68 2.71 5.92 6.28 6.47 3.09 6.03 6.06 6.93 2.96 14.43 14.52 14.31 15.53 12.20 12.09 12.01 13.14 12.31 12.21 11.94 12.68 19.75 18.88 22.07 9.64 19.52 20.39 20.26 10.23 19.41 19.33 21.73 10.33 16.30 44.85 40.74 0 16.09 46.45 40.22 0 14.82 37.65 35.39 0 40.53 17.89 40.18 0 38.76 21.14 40.37 0 36.40 15.80 35.37 0 Table 2: Comparison of (C)ascade and (D)irect performance based on different audio properties. In particular, although suffering from the wellknown scarcity of sizeable training corpora, direct solutions come with the promise (Sperber and Paulik, 2020) of: i) higher robustness to error propagation, and ii) reduced loss of speech information (e.g. prosody). Our next qualitative analysis tries to delve into these aspects by looking at audio understanding and prosody issues. Audio understanding. Errors due to wrong audio understanding are easy to identify for cascade systems – since they are evident in the intermediate ASR transcripts – but harder to spot for direct systems, whose internal representations are by far less accessible. In this case, errors can still be identified in mistranslations corresponding to words which are phonetically si"
2021.acl-long.224,2016.eamt-2.8,0,0.113294,"Missing"
2021.acl-long.224,1991.mtsummit-papers.18,0,0.578059,"ng errors (§6). We finally explore whether, due to latent characteristics overlooked by all previous investigations, the output of cascade and direct systems can be distinguished either by a human or by an automatic classifier (§7). Together with a comparative study attesting the parity of the two paradigms on our test data, another contribution of this paper is the release of the manual post-edits that rendered our investigation possible. The data is available at: https://ict.fbk.eu/mustc-post-edits. 2 Background Cascade ST. By concatenating ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991), cascade ST architectures represent an intuitive solution to achieve reasonable performance and high adaptability across languages and domains. At the same time, however, they suffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data stru"
2021.eacl-main.57,2020.amta-research.13,1,0.844364,"Missing"
2021.eacl-main.57,W19-6603,1,0.853997,"Missing"
2021.eacl-main.57,N19-4009,0,0.0221702,"ennrich et al., 2016). As having more encoder layers than decoder layers has been shown to be beneficial (Potapczyk and Przybysz, 2020; Gaido et al., 2020), we use 8 Transformer encoder layers and 6 decoder layers for ASR and 11 encoder and 4 decoder layers for ST unless stated otherwise. We train until the 692 1 2 https://lowerquality.com/gentle/ http://www.voxforge.org/home model does not improve on the validation set for 5 epochs and we average the last 5 checkpoints. Trainings were performed on K80 GPUs and lasted ~48 hours (~50 minutes per epoch). Our implementation3 is based on Fairseq (Ott et al., 2019). We evaluate performance with WER for ASR and with BLEU (Papineni et al., 2002)4 and SacreBLEU (Post, 2018)5 for ST. Baseline - 8L EN 8L PH 2L PH AVG 4L PH AVG 8L PH AVG 8L PH W/O POS. AVG 8L EN AVG WER (↓) 16.0 15.6 21.2 17.5 16.3 16.4 16.3 RAM (MB) 6929 (1.00) 6661 (0.96) 3375 (0.49) 4542 (0.66) 6286 (0.91) 6565 (0.95) 6068 (0.88) Table 1: Results on ASR using the CTC loss with transcripts and phones as target. AVG indicates that sequence is compressed averaging the vectors. 5 5.1 Results ASR We first tested whether ASR benefits from the usage of phones and sequence compression. Table 1 sho"
2021.eacl-main.57,2020.iwslt-1.8,1,0.895243,"Missing"
2021.eacl-main.57,2020.conll-1.22,0,0.0350439,"input sequence assume that samples carry the same amount of information. This does not necessarily hold true, as phonetic features vary at a different speed in time and frequency in the audio signals. Consequently, researchers have studied how to reduce the input length according to dynamic criteria based on the audio content. Salesky et al. (2019) demonstrated that a phoneme-based compression of the input frames yields significant gains compared to fixed length reduction. Phone-based and linguistically-informed compression also proved to be useful in the context of visually grounded speech (Havard et al., 2020). However, Salesky and Black (2020) questioned the approach, claiming that the addition of phone features without segmentation and compression of the input is more effective. None of these works is a direct ST solution, as they all require a separate model for phone recog690 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 690–696 April 19 - 23, 2021. ©2021 Association for Computational Linguistics nition and intermediate representations. So, they: i) are affected by error propagation (Salesky and Black 2020 show in fact that lo"
2021.eacl-main.57,2020.acl-demos.34,0,0.142221,"Missing"
2021.eacl-main.57,P02-1040,0,0.113243,"s been shown to be beneficial (Potapczyk and Przybysz, 2020; Gaido et al., 2020), we use 8 Transformer encoder layers and 6 decoder layers for ASR and 11 encoder and 4 decoder layers for ST unless stated otherwise. We train until the 692 1 2 https://lowerquality.com/gentle/ http://www.voxforge.org/home model does not improve on the validation set for 5 epochs and we average the last 5 checkpoints. Trainings were performed on K80 GPUs and lasted ~48 hours (~50 minutes per epoch). Our implementation3 is based on Fairseq (Ott et al., 2019). We evaluate performance with WER for ASR and with BLEU (Papineni et al., 2002)4 and SacreBLEU (Post, 2018)5 for ST. Baseline - 8L EN 8L PH 2L PH AVG 4L PH AVG 8L PH AVG 8L PH W/O POS. AVG 8L EN AVG WER (↓) 16.0 15.6 21.2 17.5 16.3 16.4 16.3 RAM (MB) 6929 (1.00) 6661 (0.96) 3375 (0.49) 4542 (0.66) 6286 (0.91) 6565 (0.95) 6068 (0.88) Table 1: Results on ASR using the CTC loss with transcripts and phones as target. AVG indicates that sequence is compressed averaging the vectors. 5 5.1 Results ASR We first tested whether ASR benefits from the usage of phones and sequence compression. Table 1 shows that having phones instead of English transcripts (Baseline - 8L EN) as targe"
2021.eacl-main.57,W18-6319,0,0.0241895,"k and Przybysz, 2020; Gaido et al., 2020), we use 8 Transformer encoder layers and 6 decoder layers for ASR and 11 encoder and 4 decoder layers for ST unless stated otherwise. We train until the 692 1 2 https://lowerquality.com/gentle/ http://www.voxforge.org/home model does not improve on the validation set for 5 epochs and we average the last 5 checkpoints. Trainings were performed on K80 GPUs and lasted ~48 hours (~50 minutes per epoch). Our implementation3 is based on Fairseq (Ott et al., 2019). We evaluate performance with WER for ASR and with BLEU (Papineni et al., 2002)4 and SacreBLEU (Post, 2018)5 for ST. Baseline - 8L EN 8L PH 2L PH AVG 4L PH AVG 8L PH AVG 8L PH W/O POS. AVG 8L EN AVG WER (↓) 16.0 15.6 21.2 17.5 16.3 16.4 16.3 RAM (MB) 6929 (1.00) 6661 (0.96) 3375 (0.49) 4542 (0.66) 6286 (0.91) 6565 (0.95) 6068 (0.88) Table 1: Results on ASR using the CTC loss with transcripts and phones as target. AVG indicates that sequence is compressed averaging the vectors. 5 5.1 Results ASR We first tested whether ASR benefits from the usage of phones and sequence compression. Table 1 shows that having phones instead of English transcripts (Baseline - 8L EN) as target of the CTC loss (8L PH) wi"
2021.eacl-main.57,2020.iwslt-1.9,0,0.0991299,"Missing"
2021.eacl-main.57,2020.acl-main.217,0,0.027471,"ples carry the same amount of information. This does not necessarily hold true, as phonetic features vary at a different speed in time and frequency in the audio signals. Consequently, researchers have studied how to reduce the input length according to dynamic criteria based on the audio content. Salesky et al. (2019) demonstrated that a phoneme-based compression of the input frames yields significant gains compared to fixed length reduction. Phone-based and linguistically-informed compression also proved to be useful in the context of visually grounded speech (Havard et al., 2020). However, Salesky and Black (2020) questioned the approach, claiming that the addition of phone features without segmentation and compression of the input is more effective. None of these works is a direct ST solution, as they all require a separate model for phone recog690 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 690–696 April 19 - 23, 2021. ©2021 Association for Computational Linguistics nition and intermediate representations. So, they: i) are affected by error propagation (Salesky and Black 2020 show in fact that lower quality in phone recognition si"
2021.eacl-main.57,P19-1179,0,0.0969688,"Missing"
2021.eacl-main.57,W04-3250,0,0.668023,"Missing"
2021.eacl-main.57,1991.mtsummit-papers.18,0,0.666171,"Missing"
2021.eacl-tutorials.3,N19-1006,0,0.0287765,"from speech and MT needed for this interdisciplinary research. The topic has not been previously covered in *CL tutorials. 5 3 Current state (high level overview) Input representation Architecture modifications Output representation Outline Reading list • Survey paper (Sperber and Paulik, 2020) • Introduction (30 min) • The first papers on end-to-end ST (B´erard et al., 2016; Weiss et al., 2017) – Task definition – Challenges/differences in translating speech rather than text – Traditional cascade approach to ST • Data for end-to-end ST (Di Gangi et al., 2019b) • Integrating additional data (Bansal et al., 2019; Jia et al., 2019; Sperber et al., 2019) • End-to-End (45 min) 11 • Data representation (Salesky and Black, 2020) • Adapting the Transformer for ST (Di Gangi et al., 2019a) • Multilingual models (Inaguma et al., 2019) 6 Presenters Jan Niehues, Maastricht University Email: jan.niehues@maastrichtuniversity.nl Website: https://dke.maastrichtuniversity. nl/jan.niehues/ Jan Niehues is an assistant professor at Maastricht University. He received his doctoral degree from Karlsruhe Institute of Technology in 2014 on the topic of “Domain Adaptation in Machine Translation.” He has conducted research at"
2021.eacl-tutorials.3,2020.acl-main.217,1,0.789214,"*CL tutorials. 5 3 Current state (high level overview) Input representation Architecture modifications Output representation Outline Reading list • Survey paper (Sperber and Paulik, 2020) • Introduction (30 min) • The first papers on end-to-end ST (B´erard et al., 2016; Weiss et al., 2017) – Task definition – Challenges/differences in translating speech rather than text – Traditional cascade approach to ST • Data for end-to-end ST (Di Gangi et al., 2019b) • Integrating additional data (Bansal et al., 2019; Jia et al., 2019; Sperber et al., 2019) • End-to-End (45 min) 11 • Data representation (Salesky and Black, 2020) • Adapting the Transformer for ST (Di Gangi et al., 2019a) • Multilingual models (Inaguma et al., 2019) 6 Presenters Jan Niehues, Maastricht University Email: jan.niehues@maastrichtuniversity.nl Website: https://dke.maastrichtuniversity. nl/jan.niehues/ Jan Niehues is an assistant professor at Maastricht University. He received his doctoral degree from Karlsruhe Institute of Technology in 2014 on the topic of “Domain Adaptation in Machine Translation.” He has conducted research at Carnegie Mellon University and LIMSI/CNRS, Paris. His research has covered different aspects of Machine Translati"
2021.eacl-tutorials.3,2020.acl-main.619,1,0.833372,"Missing"
2021.eacl-tutorials.3,W16-2323,0,0.0802601,"Missing"
2021.eacl-tutorials.3,Q19-1020,1,0.901654,"he successful application of deep learning methods to speech and language processing has 1 For example, is use of pretrained models end-to-end? Is use of additional steps to create auxiliary target tasks like phoneme recognition? When do these distinctions matter? 10 Proceedings of EACL: Tutorials, pages 10–13 April 19 - 20, 2021. ©2020 Association for Computational Linguistics – – – – rently demonstrated through evaluation campaigns like IWSLT. A particular focus point of the tutorial will be the current data landscape, as well as techniques to exploit different resources (Kano et al., 2020; Sperber et al., 2019) to enable speech translation not just for the few high-resource languages for which multi-parallel speech, transcripts, and translations exist. After the survey of current state-of-the-art methods, we will present evaluation and analysis methods, and challenges when bringing these models from the lab to real-world environments. For example, one challenge of end-to-end models is their ‘opaqueness’; with one joint system, it is more difficult to isolate causes of particular model behaviors and perhaps intervene, to avoid situations where key terms are translated in unexpected ways. Further, mos"
2021.eacl-tutorials.3,2020.acl-main.661,0,0.0187525,"uaintance with basic knowledge of machine learning and sequence-tosequence models for machine translation, such as are covered in most introductory NLP courses. Any programming examples will be shown in Python. This tutorial will cover cutting-edge research in the emerging field of end-to-end speech translation, and the aspects from speech and MT needed for this interdisciplinary research. The topic has not been previously covered in *CL tutorials. 5 3 Current state (high level overview) Input representation Architecture modifications Output representation Outline Reading list • Survey paper (Sperber and Paulik, 2020) • Introduction (30 min) • The first papers on end-to-end ST (B´erard et al., 2016; Weiss et al., 2017) – Task definition – Challenges/differences in translating speech rather than text – Traditional cascade approach to ST • Data for end-to-end ST (Di Gangi et al., 2019b) • Integrating additional data (Bansal et al., 2019; Jia et al., 2019; Sperber et al., 2019) • End-to-End (45 min) 11 • Data representation (Salesky and Black, 2020) • Adapting the Transformer for ST (Di Gangi et al., 2019a) • Multilingual models (Inaguma et al., 2019) 6 Presenters Jan Niehues, Maastricht University Email: jan"
2021.eacl-tutorials.3,N19-1202,1,0.870523,"Missing"
2021.eacl-tutorials.3,1991.mtsummit-papers.18,0,0.432142,"endto-end speech translation for both high- and low-resource languages. In addition, we will discuss methods to evaluate analyze the proposed solutions, as well as the challenges faced when applying speech translation models for real-world applications. 1 Description Machine translation (MT) and automatic speech recognition (ASR) have been mainstays of the speech and natural language processing communities for decades. Speech translation (ST), the combination of both tasks to translate from speech in one language typically to text in another, has existed for nearly as long as either of these (Waibel et al., 1991), attracting interest from both academia and industry. Until very recently, however, research in this area involved a cascade of separately trained speech recognition and machine translation models, with main questions pertaining to intermediate representations and processing steps to best connect these models. The successful application of deep learning methods to speech and language processing has 1 For example, is use of pretrained models end-to-end? Is use of additional steps to create auxiliary target tasks like phoneme recognition? When do these distinctions matter? 10 Proceedings of EAC"
2021.emnlp-main.127,W19-6603,1,0.89187,"Missing"
2021.emnlp-main.127,2021.eacl-main.57,1,0.779642,"these approaches respectively require adding during the translation phase, ii) it has lower la- a model that performs phoneme classification and tency, and iii) it is not affected by error propagation. a pre-trained adaptive feature selection layer on top of an ASR encoder, losing the compactness of The authors contributed equally. direct solutions at the risk of error propagation. 1698 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1698–1706 c November 7–11, 2021. 2021 Association for Computational Linguistics In direct ST, Liu et al. (2020) and Gaido et al. (2021) addressed the problem with a transcript/phoneme-based compression leveraging Connectionist Temporal Classification (CTC – Graves et al. 2006). However, since these methods are applied to the representation encoded by Transformer layers, the initial content-unaware downsampling of the input is still required for memory reasons, at the risk of losing important information. To avoid initial fixed compression, we propose Speechformer: the first Transformer-based architecture that processes full audio content maintaining the original dimensions of the input sequence. Inspired by recent work on red"
2021.emnlp-main.127,2020.acl-demos.34,0,0.0322944,"Missing"
2021.emnlp-main.127,W04-3250,0,0.135464,"Missing"
2021.emnlp-main.127,P18-1007,0,0.0358879,"Missing"
2021.emnlp-main.127,D18-2012,0,0.0403629,"Missing"
2021.emnlp-main.127,N19-4009,0,0.0187823,"ings steps. The resulting sequence is compared with We experimented on three languages of MuST-C the reference, which is the sequence of subwords representing the transcript of the input utterance. (Cattoni et al., 2021): English-German, EnglishCTC compression, similarly to the loss computa- Spanish, and English-Dutch. To ensure the reproducibility of our work, all training details are protion, collapses consecutive features corresponding vided in the Appendix and the code – based on to the same predictions, averaging them. After this operation, the sequence is reduced to a represen- fairseq (Ott et al., 2019) – is released open source.1 Following (Wang et al., 2020b), we share the tation dimensionally closer to its textual content, convolution parameters of the ConvAttention laywhich can be processed by the original attention ers both among K and V and among the attention mechanism without the need of approximations. heads. We select the compression factor and the Speechformer (see Figure 2), is composed of EL ConvAttention layers up to a CTC compres- 1D convolution kernel size with a set of preliminary experiments on the en-de validation set. The sion layer, after which there are ET Transformer c"
2021.emnlp-main.127,W18-6319,0,0.0494399,"Missing"
2021.emnlp-main.127,P19-1179,0,0.0599141,"Missing"
2021.emnlp-main.127,1991.mtsummit-papers.18,0,0.477062,"Missing"
2021.emnlp-main.127,2020.aacl-demo.6,0,0.13267,"the input length. sue, we propose Speechformer, an architecture that, thanks to a reduced memory usage in the State-of-the-art architectures tackle the problem by attention layers, avoids the initial lossy comcollapsing adjacent vectors in a fixed way, i.e. by pression and aggregates information only at a mapping a predefined number of vectors (usually higher level according to more informed lin4) into a single one, either using strided convoluguistic criteria. Experiments on three language tional layers (Bérard et al., 2018; Di Gangi et al., pairs (en→de/es/nl) show the efficacy of our 2019; Wang et al., 2020a) or by stacking them (Sak solution, with gains of up to 0.8 BLEU on et al., 2015). As a positive side effect, these length the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario. reduction solutions lower input redundancy. As a negative side effect, they disregard the variability 1 Introduction over time of the amount of linguistic and phonetic information in audio signals (e.g. due to pauses and Speech-to-text translation (ST) has been traditionspeaking rate variations) by giving equal weight ally approached with cascade architectures consistto all features. In doing th"
2021.emnlp-main.127,2020.findings-emnlp.230,0,0.0760591,"Missing"
2021.emnlp-main.128,W17-4716,1,0.883528,"Missing"
2021.emnlp-main.128,W19-6603,1,0.877107,"Missing"
2021.emnlp-main.128,P19-1294,0,0.013632,"xisting studies (Ghannay challenges for neural machine translation (NMT) et al., 2018; Caubrière et al., 2020) are all limited models (Sennrich et al., 2016; Koehn and Knowles, to ASR, for which two benchmarks are available 2017). Among rare words, named entities (NEs) (Galibert et al., 2014; Yadav et al., 2020), while and terminology are particularly critical: not only suitable benchmarks do not even exist for ST. The are they important to understand the meaning of situation is similar for terminology: few annotated a sentence (Li et al., 2013), but they are also dif- test sets exist for MT (Dinu et al., 2019; Scansani ficult to handle due to the small number of valid et al., 2019; Bergmanis and Pinnis, 2021), but none translation options. While common words can be for ST, which so far has remained unexplored. rendered in the target language with synonyms or In light of the above, the contribution of this paraphrases, NEs and terminology offer less ex- work is twofold: (1) we present the first investipressive freedom, which is typically limited to one gation on the behavior of state-of-the-art ST sysvalid option. Under these conditions, translation tems in translating NEs and terms, discussing the"
2021.emnlp-main.128,2020.lrec-1.593,0,0.0549842,"Missing"
2021.emnlp-main.128,2020.iwslt-1.8,1,0.842204,"Missing"
2021.emnlp-main.128,galibert-etal-2014-etape,0,0.0540402,"Missing"
2021.emnlp-main.128,N18-2081,0,0.054873,"Missing"
2021.emnlp-main.128,W04-3250,0,0.512284,"Missing"
2021.emnlp-main.128,W17-3204,0,0.025107,"Missing"
2021.emnlp-main.128,P13-1059,0,0.0273085,"anslation of rare words is one of the main regards NEs, the few existing studies (Ghannay challenges for neural machine translation (NMT) et al., 2018; Caubrière et al., 2020) are all limited models (Sennrich et al., 2016; Koehn and Knowles, to ASR, for which two benchmarks are available 2017). Among rare words, named entities (NEs) (Galibert et al., 2014; Yadav et al., 2020), while and terminology are particularly critical: not only suitable benchmarks do not even exist for ST. The are they important to understand the meaning of situation is similar for terminology: few annotated a sentence (Li et al., 2013), but they are also dif- test sets exist for MT (Dinu et al., 2019; Scansani ficult to handle due to the small number of valid et al., 2019; Bergmanis and Pinnis, 2021), but none translation options. While common words can be for ST, which so far has remained unexplored. rendered in the target language with synonyms or In light of the above, the contribution of this paraphrases, NEs and terminology offer less ex- work is twofold: (1) we present the first investipressive freedom, which is typically limited to one gation on the behavior of state-of-the-art ST sysvalid option. Under these conditi"
2021.emnlp-main.128,W18-6319,0,0.0464177,"Missing"
2021.emnlp-main.128,2020.iwslt-1.9,0,0.0354527,"(Wang et al., 2020) ST corpora. ASR outputs are post-processed to add true-casing and punctuation. The MT model is trained on data collected from the OPUS repository,3 amounting to about 19M, 28M, and 45M parallel sentence pairs respectively for en-es, en-fr, and en-it. Our direct model has the same Transformer2 3 http://commonvoice.mozilla.org/en/ http://opus.nlpl.eu based architecture of the ASR component used in the cascade system. It exploits data augmentation and knowledge transfer techniques successfully applied by participants in the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2020a) and it is trained on MuST-C, Europarl-ST and synthetic data (∼1.5M pairs for each language direction). Systems’ performance is shown in Table 2 and discussed in Section 4. Complete details about their implementation and training procedures are provided in the Appendix. All the related code is available at https://github.com/mgaido91/ FBK-fairseq-ST/tree/emnlp2021. 3 Evaluation Data: NEuRoparl-ST To the best of our knowledge, freely available NE/term-labelled ST benchmarks suitable for our analysis do not exist. The required resource should contain i) the audio correspond"
2021.emnlp-main.128,W95-0107,0,0.10514,"ut, being polysemic, can be technical terms in different contexts (e.g. the word “board” 4 http://docs.deeppavlov.ai/en/master/ features/models/ner.html 5 Note that Dice coefficient has the same value of the F1 measure computed considering either annotator as reference. 6 http://iate.europa.eua 7 Preprocessing made with Spacy: http://spacy.io/ can refer to a tool or to a committee). Checking the presence of the corresponding translation in the target language disambiguates these cases, leading to a more accurate annotation. NE and term annotations were merged into a single test set using BIO (Ramshaw and Marcus, 1995) as span labeling format. Had a word been tagged both as term and NE, the latter was chosen favoring the more reliable manual annotation. Table 1 presents the total number of NEs and terms for the three language pairs, together with their corresponding number of tokens.8 These numbers differ between source and target texts and across pairs due to the peculiarities of the Europarl-ST data. Specifically, i) sometimes translations are not literal and NEs are omitted in the translation (e.g. when a NE is repeated in the source, one of the occurrences may be replaced by a pronoun in the target text"
2021.emnlp-main.128,W19-6608,1,0.879777,"Missing"
2021.emnlp-main.128,P16-1162,0,0.116616,"Missing"
2021.emnlp-main.128,2020.acl-main.661,0,0.0208585,"nology present in an utterance. To this aim, we compare instances of the two main approaches. One is the traditional cascade approach (Stentiford and Steer, 1988; Waibel et al., 1991), which consists of a pipeline where an ASR model produces a transcript of the input audio and an MT model generates its translation. The other is the so-called direct approach (Bérard et al., 2016; Weiss et al., 2017), which relies on a single neural network that maps the audio into target language text bypassing any intermediate symbolic representation. The two approaches have inherent strengths and weaknesses (Sperber and Paulik, 2020). Cascade solutions can exploit sizeable datasets for the ASR and MT subcomponents, but rely on a complex architecture prone to error propagation. Direct models suffer from the paucity of training data, but avoid error propagation and can take advantage of unmediated access to audio information (e.g. prosody) during the translation phase. In recent years, after a long dominance of the cascade paradigm, the initially huge performance gap between the two approaches has gradually closed (Ansari et al., 2020). Our cascade system integrates competitive Transformer-based (Vaswani et al., 2017) ASR a"
2021.emnlp-main.128,1991.mtsummit-papers.18,0,0.514023,"ion for Computational Linguistics riches their textual portions with NE and terminology annotation. Besides being the first benchmark of this type for ST, it can also be used for the evaluation of NE/terminology recognition (ASR) and translation (MT). The dataset is available at: ict.fbk.eu/neuroparl-st/. 2 Speech Translation Models Our goal is to assess the capability of state-of-theart ST systems to properly translate NEs and terminology present in an utterance. To this aim, we compare instances of the two main approaches. One is the traditional cascade approach (Stentiford and Steer, 1988; Waibel et al., 1991), which consists of a pipeline where an ASR model produces a transcript of the input audio and an MT model generates its translation. The other is the so-called direct approach (Bérard et al., 2016; Weiss et al., 2017), which relies on a single neural network that maps the audio into target language text bypassing any intermediate symbolic representation. The two approaches have inherent strengths and weaknesses (Sperber and Paulik, 2020). Cascade solutions can exploit sizeable datasets for the ASR and MT subcomponents, but rely on a complex architecture prone to error propagation. Direct mode"
2021.emnlp-main.128,2020.lrec-1.517,0,0.0203461,"n recent years, after a long dominance of the cascade paradigm, the initially huge performance gap between the two approaches has gradually closed (Ansari et al., 2020). Our cascade system integrates competitive Transformer-based (Vaswani et al., 2017) ASR and MT components built from large training corpora. Specifically, the ASR model is trained on LibriSpeech (Panayotov et al., 2015), TEDLIUM v3 (Hernandez et al., 2018) and Mozilla Common Voice,2 together with (utterance, transcript) pairs extracted from MuST-C (Cattoni et al., 2021), Europarl-ST (Iranzo-Sánchez et al., 2020), and CoVoST 2 (Wang et al., 2020) ST corpora. ASR outputs are post-processed to add true-casing and punctuation. The MT model is trained on data collected from the OPUS repository,3 amounting to about 19M, 28M, and 45M parallel sentence pairs respectively for en-es, en-fr, and en-it. Our direct model has the same Transformer2 3 http://commonvoice.mozilla.org/en/ http://opus.nlpl.eu based architecture of the ASR component used in the cascade system. It exploits data augmentation and knowledge transfer techniques successfully applied by participants in the IWSLT-2020 evaluation campaign (Ansari et al., 2020; Potapczyk and Przyb"
2021.findings-acl.313,P16-2058,0,0.0452662,"Missing"
2021.findings-acl.313,2020.gebnlp-1.3,0,0.0431789,"Missing"
2021.findings-acl.313,W19-6603,1,0.88023,"Missing"
2021.findings-acl.313,W19-4636,0,0.0191748,"utputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enriching data with additional gender information (Moryossef et al., 2019; Vanmassenhove et al., 2018; Elaraby and Zahran, 2019; Saunders et al., 2020; Stafanoviˇcs et al., 2020). Comparatively, very little work has tried to identify concurring factors to gender bias going beyond data. Among those, Vanmassenhove et al. (2019) ascribes to an algorithmic bias the loss of less frequent feminine forms in both phrase-based and neural MT. Closer to our intent, two recent works pinpoint the impact of models’ components and inner mechanisms. Costa-juss`a et al. (2020b) investigate the role of different architectural de3577 signs in multilingual MT, showing that languagespecific encoder-decoders (Escolano et al., 2019) better"
2021.findings-acl.313,P19-2033,0,0.020981,"Missing"
2021.findings-acl.313,2020.coling-main.350,1,0.720637,"Avg. 21.4 21.8 21.6 21.3 20.7 21.0 21.9 21.7 21.8 21.7 21.4 21.6 22.0 21.5 21.8 Table 2: SacreBLEU scores on MuST-C tst-COMMON (M-C) and MuST-SHE (M-SHE) for en-fr and en-it. 4.2 Evaluation We are interested in measuring both i) the overall translation quality obtained by different segmentation techniques, and ii) the correct generation of gender forms. We evaluate translation quality on both the MuST-C tst-COMMON set (2,574 sentences for en-it and 2,632 for en-fr) and MuST-SHE (§3.2), using SacreBLEU (Post, 2018).10 For fine-grained analysis on gender translation, we rely on gender accuracy (Gaido et al., 2020).11 We differentiate between two categories of phenomena represented in MuST-SHE. Category (1) contains first-person references (e.g. I’m a student) to be translated according to the speakers’ preferred linguistic expression of gender. In this context, ST models can leverage speakers’ vocal characteristics as a gender cue to infer gender translation.12 Gender phenomena of Category (2), instead, shall be translated in concordance with other gender information in the sentence (e.g. she/he is a student). 5 Comparison of Segmentation Methods Table 1: Resulting dictionary sizes. For fair comparison"
2021.findings-acl.313,2020.gebnlp-1.8,0,0.0607853,"Missing"
2021.findings-acl.313,2020.findings-emnlp.180,0,0.0128546,"is indeed an important factor for models’ gender bias. Our experiments consistently show that BPE leads to the highest BLEU scores, while character-based models are the best at translating gender. Preliminary analyses suggests that the isolation of the morphemes encoding gender can be a key factor for gender translation. (3) Finally, we propose a multi-decoder architecture able to combine BPE overall translation quality and the higher ability to translate gender of character-based segmentation. 2 Background Gender bias. Recent years have seen a surge of studies dedicated to gender bias in MT (Gonen and Webster, 2020; Rescigno et al., 2020) and ST (Costa-juss`a et al., 2020a). The primary source of such gender imbalance and adverse outputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorg"
2021.findings-acl.313,2020.amta-research.13,1,0.729514,"Missing"
2021.findings-acl.313,2020.acl-main.275,0,0.0176651,"chnique proves simple and particularly effective at generalizing over unseen words. On the other hand, the length of the resulting sequences increases the memory footprint, and slows both the training and inference phases. We perform our segmentation by appending “@@ ” to all characters but the last of each word. Statistical Segmentation. This family comprises data-driven algorithms that generate statistically significant subwords units. The most popular one is BPE (Sennrich et al., 2016),6 which proceeds by merging the most frequently co-occurring characters or character sequences. Recently, He et al. (2020) introduced the Dynamic Programming Encoding (DPE) algorithm, which performs 5 Source code available at https://github.com/ mgaido91/FBK-fairseq-ST/tree/acl_2021. 6 We use SentencePiece (Kudo and Richardson, 2018): https://github.com/google/sentencepiece. 3579 competitively and was claimed to accidentally produce more linguistically-plausible subwords with respect to BPE. DPE is obtained by training a mixed character-subword model. As such, the computational cost of a DPE-based ST model is around twice that of a BPE-based one. We trained the DPE segmentation on the transcripts and the target t"
2021.findings-acl.313,2020.acl-main.154,0,0.0152175,"ir Encoding (BPE) (Sennrich et al., 2016) represents the de-facto standard and has recently shown to yield better results compared to character-based segmentation in ST (Di Gangi et al., 2020). But does this hold true for gender translation as well? If not, why? Languages like French and Italian often exhibit comparatively complex feminine forms, derived from the masculine ones by means of an additional suffix (e.g. en: professor, fr: professeur M vs. professeure F). Additionally, women and their referential linguistic expressions of gender are typically under-represented in existing corpora (Hovy et al., 2020). In light of the above, purely statistical segmentation methods could be unfavourable for gender translation, as they can break the morphological structure of words and thus lose relevant linguistic information (Ataman et al., 2017). Indeed, as BPE merges the character sequences that co-occur more frequently, rarer or more complex feminine-marked words may result in less compact sequences of tokens (e.g. en: described, it: des@@critto M vs. des@@crit@@ta F). Due to such typological and distributive conditions, may certain splitting methods render feminine gender less probable and hinder its p"
2021.findings-acl.313,P16-2096,0,0.0303994,"verall translation quality and gender representation: our proposal of a model that combines two segmentation techniques is a step towards this goal. Note that technical mitigation approaches should be integrated with the long-term multidisciplinary commitment (Criado-Perez, 2019; Benjamin, 2019; D’Ignazio and Klein, 2020) necessary to radically address bias in our community. Also, we recognize the limits of working on binary gender, as we further discuss in the ethic section (§8). 1 Introduction The widespread use of language technologies has motivated growing interest on their social impact (Hovy and Spruit, 2016; Blodgett et al., 2020), with gender bias representing a major cause of concern (Costa-juss`a, 2019; Sun et al., 2019). As regards translation tools, focused evaluations have exposed that speech translation (ST) – and machine translation (MT) – models do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and ste"
2021.findings-acl.313,Q17-1024,0,0.0210177,"Missing"
2021.findings-acl.313,D18-2012,0,0.0156258,"d inference phases. We perform our segmentation by appending “@@ ” to all characters but the last of each word. Statistical Segmentation. This family comprises data-driven algorithms that generate statistically significant subwords units. The most popular one is BPE (Sennrich et al., 2016),6 which proceeds by merging the most frequently co-occurring characters or character sequences. Recently, He et al. (2020) introduced the Dynamic Programming Encoding (DPE) algorithm, which performs 5 Source code available at https://github.com/ mgaido91/FBK-fairseq-ST/tree/acl_2021. 6 We use SentencePiece (Kudo and Richardson, 2018): https://github.com/google/sentencepiece. 3579 competitively and was claimed to accidentally produce more linguistically-plausible subwords with respect to BPE. DPE is obtained by training a mixed character-subword model. As such, the computational cost of a DPE-based ST model is around twice that of a BPE-based one. We trained the DPE segmentation on the transcripts and the target translations of the MuST-C training set, using the same settings of the original paper.7 Morphological Segmentation. A third possibility is linguistically-guided tokenization that follows morpheme boundaries. Among"
2021.findings-acl.313,W19-3807,0,0.0281782,"rimary source of such gender imbalance and adverse outputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enriching data with additional gender information (Moryossef et al., 2019; Vanmassenhove et al., 2018; Elaraby and Zahran, 2019; Saunders et al., 2020; Stafanoviˇcs et al., 2020). Comparatively, very little work has tried to identify concurring factors to gender bias going beyond data. Among those, Vanmassenhove et al. (2019) ascribes to an algorithmic bias the loss of less frequent feminine forms in both phrase-based and neural MT. Closer to our intent, two recent works pinpoint the impact of models’ components and inner mechanisms. Costa-juss`a et al. (2020b) investigate the role of different architectural de3577 signs in multilingual MT, showing that languagespe"
2021.findings-acl.313,W18-1818,0,0.0449539,"Missing"
2021.findings-acl.313,N19-4009,0,0.0202664,"Missing"
2021.findings-acl.313,J19-3005,0,0.0204504,"Missing"
2021.findings-acl.313,W18-6319,0,0.0144413,"5 24.2 26.9 DPE 29.8 25.3 27.6 25.7 27.7 Morfessor 29.7 LMVR 30.3 26.0 28.2 en-it M-C M-SHE Avg. 21.4 21.8 21.6 21.3 20.7 21.0 21.9 21.7 21.8 21.7 21.4 21.6 22.0 21.5 21.8 Table 2: SacreBLEU scores on MuST-C tst-COMMON (M-C) and MuST-SHE (M-SHE) for en-fr and en-it. 4.2 Evaluation We are interested in measuring both i) the overall translation quality obtained by different segmentation techniques, and ii) the correct generation of gender forms. We evaluate translation quality on both the MuST-C tst-COMMON set (2,574 sentences for en-it and 2,632 for en-fr) and MuST-SHE (§3.2), using SacreBLEU (Post, 2018).10 For fine-grained analysis on gender translation, we rely on gender accuracy (Gaido et al., 2020).11 We differentiate between two categories of phenomena represented in MuST-SHE. Category (1) contains first-person references (e.g. I’m a student) to be translated according to the speakers’ preferred linguistic expression of gender. In this context, ST models can leverage speakers’ vocal characteristics as a gender cue to infer gender translation.12 Gender phenomena of Category (2), instead, shall be translated in concordance with other gender information in the sentence (e.g. she/he is a stu"
2021.findings-acl.313,W17-1601,0,0.043847,"Missing"
2021.findings-acl.313,2020.iwslt-1.9,0,0.0768968,"Missing"
2021.findings-acl.313,2020.amta-impact.4,0,0.0174404,"ctor for models’ gender bias. Our experiments consistently show that BPE leads to the highest BLEU scores, while character-based models are the best at translating gender. Preliminary analyses suggests that the isolation of the morphemes encoding gender can be a key factor for gender translation. (3) Finally, we propose a multi-decoder architecture able to combine BPE overall translation quality and the higher ability to translate gender of character-based segmentation. 2 Background Gender bias. Recent years have seen a surge of studies dedicated to gender bias in MT (Gonen and Webster, 2020; Rescigno et al., 2020) and ST (Costa-juss`a et al., 2020a). The primary source of such gender imbalance and adverse outputs has been identified in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enrichin"
2021.findings-acl.313,2020.acl-main.690,0,0.0523287,"r feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus"
2021.findings-acl.313,2020.gebnlp-1.4,0,0.015649,"in the training data, which reflect the under-participation of women – e.g. in the media (Madaan et al., 2018), sexist language and gender categories overgeneralization (Devinney et al., 2020). Hence, preventive initiatives concerning data documentation have emerged (Bender and Friedman, 2018), and several mitigating strategies have been proposed by training models on ad-hoc gender-balanced datasets (Saunders and Byrne, 2020; Costa-juss`a and de Jorge, 2020), or by enriching data with additional gender information (Moryossef et al., 2019; Vanmassenhove et al., 2018; Elaraby and Zahran, 2019; Saunders et al., 2020; Stafanoviˇcs et al., 2020). Comparatively, very little work has tried to identify concurring factors to gender bias going beyond data. Among those, Vanmassenhove et al. (2019) ascribes to an algorithmic bias the loss of less frequent feminine forms in both phrase-based and neural MT. Closer to our intent, two recent works pinpoint the impact of models’ components and inner mechanisms. Costa-juss`a et al. (2020b) investigate the role of different architectural de3577 signs in multilingual MT, showing that languagespecific encoder-decoders (Escolano et al., 2019) better translate gender than s"
2021.findings-acl.313,E17-2060,0,0.0121917,"taken into account, BPE greedy procedures can be suboptimal. By breaking the surface of words into plausible semantic units, linguistically motivated segmentations (Smit et al., 2014; Ataman et al., 2017) were proven more effective for low-resource and morphologically-rich languages (e.g. agglutinative languages like Turkish), which often have a high level of sparsity in the lexical distribution due to their numerous derivational and inflectional variants. Moreover, fine-grained analyses comparing the grammaticality of character, morpheme and BPE-based models exhibited different capabilities. Sennrich (2017) and Ataman et al. (2019) show the syntactic advantage of BPE in managing several agreement phenomena in German, a language that requires resolving long range dependencies. In contrast, Belinkov et al. (2020) demonstrate that while subword units better capture semantic information, character-level representations perform best at generalizing morphology, thus being more robust in handling unknown and lowfrequency words. Indeed, using different atomic units does affect models’ ability to handle specific linguistic phenomena. However, whether low gender translation accuracy can be to a certain ex"
2021.findings-acl.313,P16-1162,0,0.597551,"., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus on ST systems and inspect a core aspect of neural models: word segmentation. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) represents the de-facto standard and has recently shown to yield better results compared to character-based segmentation in ST (Di Gangi et al., 2020). But does this hold true for gender translation as well? If not, why? Languages like French and Italian often exhibit comparatively complex feminine forms, derived from the masculine ones by means of an additional suffix (e.g. en: professor, fr: professeur M vs. professeure F). Additionally, women and their referential linguistic expressions of gender are typically under-represented in existing corpora (Hovy et al., 2020). In light of the above"
2021.findings-acl.313,2020.acl-main.468,0,0.0187644,"pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus on ST systems and inspect a core aspect of neural models: word segmentation. Byte-Pair E"
2021.findings-acl.313,E14-2006,0,0.0640264,"Missing"
2021.findings-acl.313,2020.wmt-1.73,0,0.0348449,"Missing"
2021.findings-acl.313,P19-1164,0,0.0261457,"gender bias representing a major cause of concern (Costa-juss`a, 2019; Sun et al., 2019). As regards translation tools, focused evaluations have exposed that speech translation (ST) – and machine translation (MT) – models do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural chan"
2021.findings-acl.313,P19-1159,0,0.0170259,"a step towards this goal. Note that technical mitigation approaches should be integrated with the long-term multidisciplinary commitment (Criado-Perez, 2019; Benjamin, 2019; D’Ignazio and Klein, 2020) necessary to radically address bias in our community. Also, we recognize the limits of working on binary gender, as we further discuss in the ethic section (§8). 1 Introduction The widespread use of language technologies has motivated growing interest on their social impact (Hovy and Spruit, 2016; Blodgett et al., 2020), with gender bias representing a major cause of concern (Costa-juss`a, 2019; Sun et al., 2019). As regards translation tools, focused evaluations have exposed that speech translation (ST) – and machine translation (MT) – models do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary sour"
2021.findings-acl.313,D18-1334,0,0.111261,"ls do in fact overproduce masculine references in their outputs (Cho et al., 2019; Bentivogli et al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation qu"
2021.findings-acl.313,W19-6622,0,0.118919,"otypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias. Along this line, we focus on ST systems and inspect a core aspect of neural models: word segmentation. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) represents the de-facto standard and has recently shown to yield better results compared to character-based se"
2021.findings-acl.313,P19-1161,0,0.0281112,"al., 2020), except for feminine asso3576 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576–3589 August 1–6, 2021. ©2021 Association for Computational Linguistics ciations perpetuating traditional gender roles and stereotypes (Prates et al., 2020; Stanovsky et al., 2019). In this context, most works identified data as the primary source of gender asymmetries. Accordingly, many pointed out the misrepresentation of gender groups in datasets (Garnerin et al., 2019; Vanmassenhove et al., 2018), focusing on the development of data-centred mitigating techniques (Zmigrod et al., 2019; Saunders and Byrne, 2020). Although data are not the only factor contributing to generate bias (Shah et al., 2020; Savoldi et al., 2021), only few inquiries devoted attention to other technical components that exacerbate the problem (Vanmassenhove et al., 2019) or to architectural changes that can contribute to its mitigation (Costa-juss`a et al., 2020b). From an algorithmic perspective, Roberts et al. (2020) additionally expose how “taken-for-granted” approaches may come with high overall translation quality in terms of BLEU scores, but are actually detrimental when it comes to gender bias."
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.iwslt-1.26,N13-1073,0,0.0963066,"not be separated from the noun it describes. We consider as plausible only those breaks following punctuation marks or those between a content word (chunk) and a function word (chink). We obtain Universal Dependencies8 PoS-tags using the Stanza toolkit (Qi et al., 2020) and calculate the percentage of break symbols falling either in the punctuation or the content-function groups as plausible segmentation. Lastly, we evaluate structural and lexical consistency between the generated captions and corresponding subtitles, as described in Section 3.2. Word alignments are obtained using fast align (Dyer et al., 2013) on the concatenation of MuSTCinema training data and the system outputs. Text is tokenised using Moses tokeniser and the consistency percentage is computed on tokenised text. 219 En→Fr WER SacreBLEU Length Read speed Segment. Struc. Lex. Cas DirInd DirMu 2ST Tri En→De 19.69 19.69 17.73 19.05 18.93 26.9 24.0 25.2 25.6 25.3 .94 / .93 .94 / .94 .95 / .93 .95 / .94 .93 / .91 .85 / .70 .85 / .73 .85 / .73 .85 / .71 .85 / .72 .86 / .82 .86 / .84 .87 / .80 .87 / .82 .87 / .82 .98 .75 .77 .83 .82 .99 .86 .87 .84 .92 Cas DirInd DirMu 2ST Tri 18.52 18.52 16.95 18.93 19.10 19.9 18.1 18.7 19.6 19.8 .94 /"
2021.iwslt-1.26,2020.iwslt-1.8,1,0.844298,"Missing"
2021.iwslt-1.26,2020.iwslt-1.26,1,0.923201,"ccessibility needs. In those cases, captions and subtitles should not only be consistent with the visual and acoustic dimension of the audiovisual material but also between each other, for example in the number of blocks (pieces of time-aligned text) they occupy, their length and segmentation. Consistency is vital for user experience, for example in order to elicit the same reaction among multilingual audiences, or to facilitate the quality assurance process in the localisation industry. Previous work in ST for subtitling has focused on generating interlingual subtitles (Matusov et al., 2019; Karakanta et al., 2020a), a) without considering the necessity of obtaining captions consistent with the target subtitles, and b) without examining whether the joint generation leads to improvements in quality. We hypothesise that knowledge sharing between the tasks of transcription and translation could lead to such improvements. Moreover, joint generation with a single system can avoid the maintenance of two different models, increase efficiency, and in turn speed up the localisation process. Lastly, if joint generation improves consistency, joint models could increase automation in subtitling applications where"
2021.iwslt-1.26,2020.lrec-1.460,1,0.907226,"ccessibility needs. In those cases, captions and subtitles should not only be consistent with the visual and acoustic dimension of the audiovisual material but also between each other, for example in the number of blocks (pieces of time-aligned text) they occupy, their length and segmentation. Consistency is vital for user experience, for example in order to elicit the same reaction among multilingual audiences, or to facilitate the quality assurance process in the localisation industry. Previous work in ST for subtitling has focused on generating interlingual subtitles (Matusov et al., 2019; Karakanta et al., 2020a), a) without considering the necessity of obtaining captions consistent with the target subtitles, and b) without examining whether the joint generation leads to improvements in quality. We hypothesise that knowledge sharing between the tasks of transcription and translation could lead to such improvements. Moreover, joint generation with a single system can avoid the maintenance of two different models, increase efficiency, and in turn speed up the localisation process. Lastly, if joint generation improves consistency, joint models could increase automation in subtitling applications where"
2021.iwslt-1.26,W04-3250,0,0.044218,".10 19.9 18.1 18.7 19.6 19.8 .94 / .90 .94 / .92 .95 / .92 .94 / .92 .93 / .92 .62 / .58 .62 / .59 .62 / .59 .62 / .60 .62 / .60 .86 / .76 .86 / .78 .87 / .73 .86 / .76 .87 / .76 .95 .73 .75 .82 .78 .96 .86 .82 .81 .91 Table 1: Results for quality (WER and BLEU), subtitling conformity (Length, Reading speed and Segmentation), and subtitling consistency (Structural and Lexical) for model outputs for French and German. Conformity scores are reported for captions / subtitles. Bold denotes the best score. Results that are not statistically significant – according to pairwise bootstrap resampling (Koehn, 2004), p&lt;0.05 – than the best score are reported in italics. 5 Results 5.1 Transcription/Translation quality We first examine the quality of the systems’ outputs. The first two columns of Table 1 show the WER and SacreBLEU score for the examined models. In terms of transcription quality, DirMu (Multitask Direct – see Section 3.1) obtains the lowest WER for both languages (17.73 for French and 16.95 for German). As far as the rest of the models are concerned, there is a different tendency for French and German. Tri (Triangle) and 2ST (Two-Stage) perform equally better than the Cas/DirInd for French,"
2021.iwslt-1.26,2020.eamt-1.13,0,0.0233474,"hegoyhen et al., 2014) or even before, with example-based approaches (Melero et al., 2006; Armstrong et al., 2006; Nyberg and Mitamura, 1997; Popowich et al., 2000; Piperidis et al., 2005). With the neural era, the interest in automatic approaches to subtitling revived. Neural Machine Translation (NMT) led to higher performance and efficiency and opened new paths and opportunities. Matusov et al. (2019) customised a cascade of ASR and NMT for subtitling, using domain adaptation with fine-tuning and improving subtitle segmentation with a specific segmentation module. Similarly, using cascades, Koponen et al. (2020) explored sentence- and document-level NMT for subtitling and showed productivity gains for some language pairs. However, bypassing the need to train and maintain separate components for transcription, translation and segmentation, direct end-to-end ST systems are now being considered as a valid and potentially more promising alternative (Karakanta et al., 2020a). Indeed, besides the architectural advantages, they come with the promise to avoid error propagation (a well known issue of cascade solutions), reduce latency, and better exploit speech information (e.g. prosody) without loss of infor"
2021.iwslt-1.26,D18-2012,0,0.0251848,"and English→German. The breaks between subtitles are marked with special symbols, &lt;eob> for breaks between blocks of subtitles and &lt;eol> for new lines inside the same block. The training data contain 408 and 492 hours of pre-segmented audio (229K and 275K sentences) for German and French respectively. For tuning and evaluation we use the official development and test sets. We expect the captions and subtitles of TED Talks to have high consistency, since the captions serve as the basis for translating the speech in target subtitles. The text data is segmented into sub-words with Sentencepiece (Kudo and Richardson, 2018) with the unigram setting. In line with recent works in ST, we found that a small vocabulary size is beneficial for the performance of ST models. Therefore, 218 we set a shared vocabulary of 1024 for all models except the MT component of the cascade, where vocabulary size is set to 24k. The special symbols &lt;eob> and &lt;eol> are kept as a single token. For the audio input, we use 40-dimensional log Mel filterbank speech features. The ASR encoder was pretrained on the IWSLT 2020 data, i.e. Europarl-ST (Iranzo-S´anchez et al., 2020), Librispeech (Panayotov et al., 2015), How2 (Sanabria et al., 2018"
2021.iwslt-1.26,2020.coling-main.314,0,0.152843,"ce depends on the flexibility versus conformity requirements of the application scenario. The findings of this work have provided initial insights related to the joint generation of captions and subtitles. One future research direction is towards improving the quality of generation by using more recent, higher-performing ST architectures. For example, Liu et al. (2020) extended the notion of the dual decoder by adding an interactive attention mechanism which allows the two decoders to exchange information and learn from each other, while synchronously generating transcription and translation. Le et al. (2020) proposed two variants of the dual decoder of Liu et al. (2020), the cross and parallel dual decoder, and experimented with multilingual ST. While neither of these works reported results on consistency, we expect that they are relevant to our scenario and have the potential of jointly generating multiple language/accessibility versions with high consistency. Moving beyond generic architectures, in the future we are planning to experiment with tailored architectures for improving consistency between automatically generated captions and subtitles. One important insight emerging from this work is"
2021.iwslt-1.26,W19-5209,0,0.12518,"ences with different accessibility needs. In those cases, captions and subtitles should not only be consistent with the visual and acoustic dimension of the audiovisual material but also between each other, for example in the number of blocks (pieces of time-aligned text) they occupy, their length and segmentation. Consistency is vital for user experience, for example in order to elicit the same reaction among multilingual audiences, or to facilitate the quality assurance process in the localisation industry. Previous work in ST for subtitling has focused on generating interlingual subtitles (Matusov et al., 2019; Karakanta et al., 2020a), a) without considering the necessity of obtaining captions consistent with the target subtitles, and b) without examining whether the joint generation leads to improvements in quality. We hypothesise that knowledge sharing between the tasks of transcription and translation could lead to such improvements. Moreover, joint generation with a single system can avoid the maintenance of two different models, increase efficiency, and in turn speed up the localisation process. Lastly, if joint generation improves consistency, joint models could increase automation in subtit"
2021.iwslt-1.26,2006.tc-1.10,0,0.171322,"a template for multilingual localisation. This paper is a first step towards maximising automation for the generation of high-quality multiple language/accessibility subtitle versions. 216 1 https://www.dualsub.xyz/ https://www.watch-listen-read.com/ 3 https://amara.org/en/teams/ted/ 2 2.2 MT and ST for subtitling Subtitling has long sparked the interest of the Machine Translation (MT) community as a challenging type of translation. Most works employing MT for subtitling stem from the statistical era (Volk et al., 2010; Etchegoyhen et al., 2014) or even before, with example-based approaches (Melero et al., 2006; Armstrong et al., 2006; Nyberg and Mitamura, 1997; Popowich et al., 2000; Piperidis et al., 2005). With the neural era, the interest in automatic approaches to subtitling revived. Neural Machine Translation (NMT) led to higher performance and efficiency and opened new paths and opportunities. Matusov et al. (2019) customised a cascade of ASR and NMT for subtitling, using domain adaptation with fine-tuning and improving subtitle segmentation with a specific segmentation module. Similarly, using cascades, Koponen et al. (2020) explored sentence- and document-level NMT for subtitling and showed"
2021.iwslt-1.26,1997.mtsummit-papers.2,0,0.679673,"is paper is a first step towards maximising automation for the generation of high-quality multiple language/accessibility subtitle versions. 216 1 https://www.dualsub.xyz/ https://www.watch-listen-read.com/ 3 https://amara.org/en/teams/ted/ 2 2.2 MT and ST for subtitling Subtitling has long sparked the interest of the Machine Translation (MT) community as a challenging type of translation. Most works employing MT for subtitling stem from the statistical era (Volk et al., 2010; Etchegoyhen et al., 2014) or even before, with example-based approaches (Melero et al., 2006; Armstrong et al., 2006; Nyberg and Mitamura, 1997; Popowich et al., 2000; Piperidis et al., 2005). With the neural era, the interest in automatic approaches to subtitling revived. Neural Machine Translation (NMT) led to higher performance and efficiency and opened new paths and opportunities. Matusov et al. (2019) customised a cascade of ASR and NMT for subtitling, using domain adaptation with fine-tuning and improving subtitle segmentation with a specific segmentation module. Similarly, using cascades, Koponen et al. (2020) explored sentence- and document-level NMT for subtitling and showed productivity gains for some language pairs. Howeve"
2021.iwslt-1.26,N19-4009,0,0.0304594,"., 2019), a technique for online data augmentation, with augment rate of 0.5. Training is completed when the validation perplexity does not improve for 3 consecutive epochs. The MT component is based on the Transformer architecture (big) (Vaswani et al., 2017) with similar settings to the original paper. Since the ASR component outputs punctuation, no other pre-processing (except for BPE) is applied to the training data. In order to ensure a fair comparison with the direct and joint models, the MT component is trained only on MuST-Cinema data. All experiments are run with the fairseq toolkit (Ott et al., 2019). Training is performed on 4 two K80 GPUs with 11 GB memory and models converged in about five days. Our implementation of the DirMu, Tri and 2ST models is publicly available at: https://github.com/ We evaluate three aspects of the automatically generated captions and subtitles: 1) quality, 2) form, and 3) consistency. For quality of transcription we compute WER on unpunctuated, lowercased output, while for quality of translation we use SacreBLEU (Post, 2018).6 We report scores computed at the level of utterances, where the output sentences contain subtitle breaks. A break symbol is considered"
2021.iwslt-1.26,W00-0506,0,0.395212,"wards maximising automation for the generation of high-quality multiple language/accessibility subtitle versions. 216 1 https://www.dualsub.xyz/ https://www.watch-listen-read.com/ 3 https://amara.org/en/teams/ted/ 2 2.2 MT and ST for subtitling Subtitling has long sparked the interest of the Machine Translation (MT) community as a challenging type of translation. Most works employing MT for subtitling stem from the statistical era (Volk et al., 2010; Etchegoyhen et al., 2014) or even before, with example-based approaches (Melero et al., 2006; Armstrong et al., 2006; Nyberg and Mitamura, 1997; Popowich et al., 2000; Piperidis et al., 2005). With the neural era, the interest in automatic approaches to subtitling revived. Neural Machine Translation (NMT) led to higher performance and efficiency and opened new paths and opportunities. Matusov et al. (2019) customised a cascade of ASR and NMT for subtitling, using domain adaptation with fine-tuning and improving subtitle segmentation with a specific segmentation module. Similarly, using cascades, Koponen et al. (2020) explored sentence- and document-level NMT for subtitling and showed productivity gains for some language pairs. However, bypassing the need t"
2021.iwslt-1.26,W18-6319,0,0.0250678,"e direct and joint models, the MT component is trained only on MuST-Cinema data. All experiments are run with the fairseq toolkit (Ott et al., 2019). Training is performed on 4 two K80 GPUs with 11 GB memory and models converged in about five days. Our implementation of the DirMu, Tri and 2ST models is publicly available at: https://github.com/ We evaluate three aspects of the automatically generated captions and subtitles: 1) quality, 2) form, and 3) consistency. For quality of transcription we compute WER on unpunctuated, lowercased output, while for quality of translation we use SacreBLEU (Post, 2018).6 We report scores computed at the level of utterances, where the output sentences contain subtitle breaks. A break symbol is considered as another token contributing to the score. For evaluating the form of the subtitles, we focus on the conformity to the subtitling constraints of length and reading speed, as well as proper segmentation, as proposed in (Karakanta et al., 2019). We compute the percentage of subtitles conforming to a maximum length of 42 characters/line and a maximum reading speed of 21 characters/second.7 The plausibility of segmentation is evaluated based on syntactic proper"
2021.iwslt-1.26,2020.iwslt-1.9,0,0.0260214,"., 2020), Librispeech (Panayotov et al., 2015), How2 (Sanabria et al., 2018), Mozilla Common-Voice,4 MuST-C (Cattoni et al., 2020), and the ST TED corpus.5 4.2 https://voice.mozilla.org/ http://iwslt.org/doku.php?id=offline_ speech_translation 5 mgaido91/FBK-fairseq-ST/tree/acl_2021 4.3 Evaluation Model training The ASR and ST models are trained using the same settings. The architecture used is S-Transformer, (Di Gangi et al., 2019), an ST adaptation of Transformer, which has been shown to achieve high performance on different speech translation benchmarks. Following state-of-the-art systems (Potapczyk and Przybysz, 2020; Gaido et al., 2020), we do not add 2D self-attentions. The size of the encoder is set to 11 layers, and to 4 layers for the decoder. The ASR model used to pretrain the encoder, instead, has 8 encoder and 6 decoder layers. The additional 3 encoder layers are initialised randomly, similarly to the adaptation layer proposed by Bahar et al. (2019). As distance penalty, we choose the logarithmic distance penalty. We optimise using Adam (Kingma and Ba, 2015) (betas 0.9, 0.98), 4000 warm-up steps with initial learning rate of 0.0003, and learning rate decay with the inverse square root of the itera"
2021.iwslt-1.26,2020.acl-demos.14,0,0.0247899,"ute the percentage of subtitles conforming to a maximum length of 42 characters/line and a maximum reading speed of 21 characters/second.7 The plausibility of segmentation is evaluated based on syntactic properties. Subtitle breaks should be placed in such a way that keeps syntactic and semantic units together. For example, an adjective should not be separated from the noun it describes. We consider as plausible only those breaks following punctuation marks or those between a content word (chunk) and a function word (chink). We obtain Universal Dependencies8 PoS-tags using the Stanza toolkit (Qi et al., 2020) and calculate the percentage of break symbols falling either in the punctuation or the content-function groups as plausible segmentation. Lastly, we evaluate structural and lexical consistency between the generated captions and corresponding subtitles, as described in Section 3.2. Word alignments are obtained using fast align (Dyer et al., 2013) on the concatenation of MuSTCinema training data and the system outputs. Text is tokenised using Moses tokeniser and the consistency percentage is computed on tokenised text. 219 En→Fr WER SacreBLEU Length Read speed Segment. Struc. Lex. Cas DirInd Di"
2021.iwslt-1.26,2020.tacl-1.45,0,0.525244,"ation decoder which attends to encoded speech inputs. We compare these models with the established approaches in ST for subtitling: an independent direct ST model and a cascade (ASR+MT) model. Moreover, we extend the evaluation beyond the usual metrics used to assess transcription and 215 Proceedings of the 18th International Conference on Spoken Language Translation, pages 215–225 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics translation quality (respectively WER and BLEU), by also evaluating the form and consistency of the generated subtitles. Sperber et al. (2020) introduced several lexical and surface metrics to measure consistency of ST outputs, but they were only applied to standard, non-subtitle, texts. Subtitles, however, are a particular type of text structured in blocks which accompany the action on screen. Therefore, we propose to measure their consistency by taking advantage of this structure and introduce metrics able to reward subtitles that share similar structure and content. Our contributions can be summarised as follows: • We employ ST to directly generate both captions and subtitles without the need for human pre-processing (transcripti"
2021.iwslt-1.26,2010.jec-1.7,0,0.0603514,"Missing"
2021.iwslt-1.8,N19-1006,0,0.0223461,"translation step. Direct models, although being penalized by the paucity of training data, have two theoretical competitive advantages, namely: i) the absence of error propagation as there are no intermediate processing steps, and ii) a less mediated access to the source utterance, which allows them to better exploit speech information (e.g. prosody) without loss of information. The paucity of parallel (audio, translation) data for direct ST has been previously addressed in different ways, ranging from model pre-training to exploit knowledge transfer from ASR and/or MT (B´erard et al., 2018; Bansal et al., 2019; Alinejad and Sarkar, 2020), knowledge distillation (Liu et al., 2019; Gaido et al., 2021a), data augmentation (Jia et al., 2019; Bahar et al., 2019b; Nguyen et al., 2020), and multi-task learning (Weiss et al., This paper describes FBK’s system submission to the IWSLT 2021 Offline Speech Translation task. We participated with a direct model, which is a Transformer-based architecture trained to translate English speech audio data into German texts. The training pipeline is characterized by knowledge distillation and a two-step fine-tuning procedure. Both knowledge distillation and the first f"
2021.iwslt-1.8,2020.emnlp-main.644,0,0.0170466,"rect models, although being penalized by the paucity of training data, have two theoretical competitive advantages, namely: i) the absence of error propagation as there are no intermediate processing steps, and ii) a less mediated access to the source utterance, which allows them to better exploit speech information (e.g. prosody) without loss of information. The paucity of parallel (audio, translation) data for direct ST has been previously addressed in different ways, ranging from model pre-training to exploit knowledge transfer from ASR and/or MT (B´erard et al., 2018; Bansal et al., 2019; Alinejad and Sarkar, 2020), knowledge distillation (Liu et al., 2019; Gaido et al., 2021a), data augmentation (Jia et al., 2019; Bahar et al., 2019b; Nguyen et al., 2020), and multi-task learning (Weiss et al., This paper describes FBK’s system submission to the IWSLT 2021 Offline Speech Translation task. We participated with a direct model, which is a Transformer-based architecture trained to translate English speech audio data into German texts. The training pipeline is characterized by knowledge distillation and a two-step fine-tuning procedure. Both knowledge distillation and the first fine-tuning step are carried"
2021.iwslt-1.8,N13-1073,0,0.0327521,"ata. For the second fine-tuning step, we re-segmented the MuST-C v2 En-De training set following the procedure described in (Gaido et al., 2020a). The method consists in choosing a random word in the transcript of each sample, and using it as sentence boundary instead of the linguistically-motivated (sentence-level) splits provided in the original data. The corresponding audio segments are then obtained by means of audiotext alignments performed with Gentle.3 Similarly, the German translation of each re-segmented transcript is extracted with cross-lingual alignments generated by a fast align (Dyer et al., 2013) model trained on all the MT data available for the task and on MuST-C v2. In case either of the alignments is Fine-tuning step #1: using native and synthetic data. Once the KD training phase was concluded, we performed a multi-domain fine-tuning where 3 https://github.com/lowerquality/ gentle/ 87 model 1-FT LSCE 2-FT LSCE 1-FT LSCE+CTC 2-FT LSCE+CTC MuST-C2 manual 27.6 27.7 - MuST-C2 VAD (WebRTC) 20.8 23.4 (+2.6) 19.9 23.7 (+3.8) MuST-C2 hybrid 24.8 26.4 (+1.6) 25.3 26.3 (+1.0) IWSLT2015 VAD (LIUM) 16.1 20.7 (+4.6) 14.0 20.9 (+6.9) IWSLT2015 hybrid 21.9 22.7 (+0.8) 21.7 23.1 (+1.4) Table 3: R"
2021.iwslt-1.8,D16-1139,0,0.0253015,"inference time, we applied an automatic segmentation procedure to the test set in order to feed the model with input resembling, as much as possible, the gold manual segmentation. These two solutions, which characterize our final submission, are explained in the following. the 8th encoder layer. Accordingly, we used this architecture to perform all the successive training phases. Training with knowledge distillation. Two ST models, one with 12 and one with 15 encoder layers, were trained by loading the pre-trained ASR encoder weights and applying word-level Knowledge Distillation (KD) as in (Kim and Rush, 2016). In KD, a student model is trained with the goal of learning how to produce the same output distribution as a teacher model, and this is obtained by computing the KL divergence between the two output distributions. In our setting, the student and the teacher are respectively the ST system and an MT system that we trained on the MT data described in Section 2. It consists in a plain Transformer model with 6 layers for both the encoder and the decoder, 16 attention heads, 1,024 features for the attention layers and 4,096 hidden units in the feed-forward layers. Evaluated on the MuST-C v2 En-De"
2021.iwslt-1.8,2020.iwslt-1.8,1,0.855839,"Missing"
2021.iwslt-1.8,L16-1147,0,0.0328293,"Missing"
2021.iwslt-1.8,1991.mtsummit-papers.18,0,0.537816,"ning and test segmentation mismatch: FBK@IWSLT2021 Sara Papi1,2 , Marco Gaido1,2 , Matteo Negri1 , Marco Turchi1 1 Fondazione Bruno Kessler, Trento, Italy 2 University of Trento, Italy {spapi|mgaido|negri|turchi}@fbk.eu Abstract et al., 2020), the IWSLT2021 Offline Speech Translation task (Anastasopoulos et al., 2021) focused on the translation into German of English audio data extracted from TED talks. Participants could approach the task either with a cascade architecture or with a direct end-to-end system. The former represents the traditional pipeline approach (Stentiford and Steer, 1988; Waibel et al., 1991) comprising an automatic speech recognition (ASR) followed by a machine translation (MT) component. The latter (B´erard et al., 2016; Weiss et al., 2017) relies on a single neural network trained to translate the input audio into target language text bypassing any intermediate symbolic representation steps. The two paradigms have advantages and disadvantages. Cascade architectures have historically guaranteed higher translation quality (Niehues et al., 2018, 2019) thanks to the large corpora available to train their ASR and MT sub-components. However, a well-known drawback of pipelined solutio"
2021.iwslt-1.8,2020.lrec-1.517,0,0.200128,"its both solutions (see Section 4): at training time, by fine-tuning the model with a random segmentation of the available in-domain data; ASR. ASR corpora, together with the ST ones described below, were collected for the ASR training. In detail, the allowed native ASR datasets are: 1 85 https://iwslt.org/2021/offline LibriSpeech (Panayotov et al., 2015), TEDLIUM v3 (Hernandez et al., 2018) and Mozilla Common Voice.2 In all of them, English texts were lowercased and punctuation was removed. units in the feed-forward layers. The ASR and ST models are based on a custom version of the model by (Wang et al., 2020b), which is a Transformer whose encoder has two initial 1D convolutional layers with gelu activation functions (Hendrycks and Gimpel, 2020). Also, the encoder self-attentions were biased using a logarithmic distance penalty in favor of the local context as per (Di Gangi et al., 2019). A Connectionist Temporal Classification (CTC) scoring function was applied as described in (Gaido et al., 2020b). This was done by adding a linear layer to either the 6th, 8th or 10th encoder layer to map the encoder states to the vocabulary size and compute the CTC loss. The choice of the final architecture, de"
2021.iwslt-1.8,N19-4009,0,0.0126127,"n of the test data. At inference time, the test set was segmented with an hybrid approach that considers both the audio content and the length of the resulting segment (Gaido et al., 2021b). Specifically, every segment is ensured to be at least 17s and at most 20s long, but the exact splitting position is determined by the longest pause detected within this interval. Pauses are identified with the WebRTC VAD tool (Johnston and Burnett, 2012), using 20ms as frame duration and 2 as aggressivity level. 5 Results Experimental settings Our implementation is built on top of fairseq Pytorch library (Ott et al., 2019). All our models were trained using the Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98. During training, the learning rate was set to increase linearly from 0 to 2e-3 for the first 10,000 warm-up steps and then to decay with an inverse square root policy. Differently, the learning rate was kept constant for model fine-tuning, with a value of 1e-3 for the first fine-tuning step and 1e-4 for the second one. All the trainings were performed on 2 Tesla V100 GPUs with 32GB RAM. We set the maximum number of tokens to 10k per batch and 8 as update frequency. For generation, the maximum"
2021.iwslt-1.8,2020.aacl-demo.6,0,0.104858,"its both solutions (see Section 4): at training time, by fine-tuning the model with a random segmentation of the available in-domain data; ASR. ASR corpora, together with the ST ones described below, were collected for the ASR training. In detail, the allowed native ASR datasets are: 1 85 https://iwslt.org/2021/offline LibriSpeech (Panayotov et al., 2015), TEDLIUM v3 (Hernandez et al., 2018) and Mozilla Common Voice.2 In all of them, English texts were lowercased and punctuation was removed. units in the feed-forward layers. The ASR and ST models are based on a custom version of the model by (Wang et al., 2020b), which is a Transformer whose encoder has two initial 1D convolutional layers with gelu activation functions (Hendrycks and Gimpel, 2020). Also, the encoder self-attentions were biased using a logarithmic distance penalty in favor of the local context as per (Di Gangi et al., 2019). A Connectionist Temporal Classification (CTC) scoring function was applied as described in (Gaido et al., 2020b). This was done by adding a linear layer to either the 6th, 8th or 10th encoder layer to map the encoder states to the vocabulary size and compute the CTC loss. The choice of the final architecture, de"
2021.iwslt-1.8,W18-6319,0,0.0151171,"ers with gelu activation functions (Hendrycks and Gimpel, 2020). Also, the encoder self-attentions were biased using a logarithmic distance penalty in favor of the local context as per (Di Gangi et al., 2019). A Connectionist Temporal Classification (CTC) scoring function was applied as described in (Gaido et al., 2020b). This was done by adding a linear layer to either the 6th, 8th or 10th encoder layer to map the encoder states to the vocabulary size and compute the CTC loss. The choice of the final architecture, depending on where the CTC loss is applied, was made based on sacreBLEU score (Post, 2018) after training the models on MuST-C v1 En-De (Cattoni et al., 2021). ST results computed on the test set are reported on Table 1. As it can be seen from the table, two models obtained the highest, identical BLEU score (21.21): they both use logarithmic distance penalty but apply CTC loss to the 6th or the 8th encoder layer. ST. The ST benchmarks we used are essentially three: i) Europarl-ST (obtained from European Parliament debates – Iranzo-S´anchez et al. 2020), ii) MuST-C v2 (built from TED talks – Cattoni et al. 2021), and iii) CoVoST 2 (containing the translations of a portion of the Moz"
2021.iwslt-1.8,2020.iwslt-1.9,0,0.0412642,"aspect characterizing our participation to this year’s round of the offline ST task together with our custom automatic segmentation of the test set (see Section 4). Our experimental results proved the effectiveness of our solutions: compared to a standard ST model and a baseline VAD-based method, on the MuST-C v2 EnglishGerman test set (Cattoni et al., 2021), the gap with optimal manual segmentation is reduced from 8.3 to 1.4 BLEU. Another interesting finding from last year’s campaign concerns the sensitivity of ST models to different segmentations of the input audio. The 2020 winning system (Potapczyk and Przybysz, 2020) shows that, with a custom segmentation of the test data, the same model improved by 3.81 BLEU points the score achieved when using the basic segmentation provided by the task organizers. This noticeable difference is due to a well-known problem in MT, ST and in machine learning at large: any mismatch between training and test data (in terms of domain, text style or a variety of other aspects) can cause unpredictable, often large, performance drops at test time. In ST, this is a critical issue, inherent to the nature of the available resources: while systems are usually trained on corpora that"
2021.iwslt-1.8,P16-1162,0,0.0125096,"on of the Mozilla Common Voice dataset – Wang et al. 2020a). To cope with the scarcity of ST data, we complemented these native ST corpora with synthetic data. To this aim, we used the MT system trained on the available MT data to translate into German the English transcripts of the aforementioned ASR datasets. The resulting texts were used as reference material during the ST model training. The combination of native and generated data resulted in a total of about 1.26 million samples. The transcription-translation pairs were tokenized using, respectively, source/targetlanguage SentencePiece (Sennrich et al., 2016) unigram models trained on the MT corpora with a vocabulary size of 32k tokens. Similar to our last year’s IWSLT submission (Gaido et al., 2020b), the entire dataset was used for training in a multidomain fashion, where the two domains were native (original ST data) and generated (synthetic data). Prior to the extraction of the speech features, the audio was pre-processed with the SpecAugment (Park et al., 2019) data augmentation technique, which masks consecutive portions of the input both in frequency and in time dimensions. From all the audio files, 80 log Mel-filter banks features were ext"
2021.mtsummit-asltrw.4,D18-1337,0,0.0176626,"e tediousness ∗ Equal contribution Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st Workshop on Automatic Spoken Language Translation in Real-World Settings Page 35 of the task and the scarcity of highly-skilled professionals for live subtitling call for a more pronounced role of technology for providing real-time access to information. These growing needs for access to multilingual spoken content have motivated researchers to develop fully automatic solutions for real-time spoken language translation (Grissom II et al., 2014; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Ma et al., 2019). The new possibilities opened up by neural machine translation have led to improvements in automatic simultaneous speech-to-text translation (SimulST). In SimulST (Ma et al., 2020; Ren et al., 2020), the generation of the translation starts before the entire audio input is received, which is an indispensable characteristic for achieving low latency (translation delay) between speech and text in live events. The translation becomes available at consecutive steps, usually one word at a time. However, a display mode based on the word-for-word rate of g"
2021.mtsummit-asltrw.4,P19-1126,0,0.0330071,"Missing"
2021.mtsummit-asltrw.4,2020.iwslt-1.27,0,0.0279738,"Missing"
2021.mtsummit-asltrw.4,2021.eacl-demos.32,0,0.0289429,"Missing"
2021.mtsummit-asltrw.4,D14-1140,0,0.0772248,"Missing"
2021.mtsummit-asltrw.4,E17-1099,0,0.13855,"reting. Still, the tediousness ∗ Equal contribution Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st Workshop on Automatic Spoken Language Translation in Real-World Settings Page 35 of the task and the scarcity of highly-skilled professionals for live subtitling call for a more pronounced role of technology for providing real-time access to information. These growing needs for access to multilingual spoken content have motivated researchers to develop fully automatic solutions for real-time spoken language translation (Grissom II et al., 2014; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Ma et al., 2019). The new possibilities opened up by neural machine translation have led to improvements in automatic simultaneous speech-to-text translation (SimulST). In SimulST (Ma et al., 2020; Ren et al., 2020), the generation of the translation starts before the entire audio input is received, which is an indispensable characteristic for achieving low latency (translation delay) between speech and text in live events. The translation becomes available at consecutive steps, usually one word at a time. However, a display mode based on the"
2021.mtsummit-asltrw.4,2020.iwslt-1.26,1,0.874515,"els The SimulST systems used in this work are based on direct ST models (B´erard et al., 2016; Weiss et al., 2017), which are composed of an audio encoder and a text decoder. The encoder starts from the audio features extracted from the input signal and computes a hidden representation, while the decoder transforms this representation into the target text. These systems have been shown to have lower latency (Ren et al., 2020) – an important factor in simultaneous systems – compared to cascade systems, which perform two generation steps, one for transcription and one for translation. Moreover, Karakanta et al. (2020a) suggested that direct ST systems, having access to the audio source, make better subtitle segmentation decisions by taking advantage of the pauses in the audio. In order to adapt SimulST systems for the task of live interlingual subtitling, we force the system to learn from human subtitle segmentation decisions by training on data annotated with break symbols which correspond to subtitle breaks (&lt;eob> for end of a subtitle block and &lt;eol> for end of line inside a subtitle block). These break symbols, if positioned properly, are the key element which allows us to experiment with different di"
2021.mtsummit-asltrw.4,P18-1007,0,0.0180162,"orld Settings Page 41 4 Experimental setting Data For our experiments we use MuST-Cinema (Karakanta et al., 2020b), an ST corpus compiled from TED Talk subtitles. This corpus is ideal for exploring display modes other than word-for-word because it contains subtitle breaks as special symbols. We conduct experiments on three language pairs, English→Italian (442 hours), English→German (408 hours) and English→French (492 hours). For tuning and evaluation we use the MuST-Cinema dev and test sets. The text data were tokenized using SentencePiece (Kudo and Richardson, 2018) with the unigram setting (Kudo, 2018), trained on the training data with a 10k-token vocabulary size. The source audio was pre-processed with the SpecAugment data augmentation technique (Park et al., 2019), then the speech features (80 log Mel-filter banks) were extracted and Cepstral Mean and Variance Normalization was applied at global level. Samples with a length above 30s were filtered out. The configuration parameters are the default ones set by Ma et al. (2020). Training settings Our SimulST systems are Transformer-based models (Vaswani et al., 2017), composed by 12 encoder layers, 6 decoder layers, 256 features for the att"
2021.mtsummit-asltrw.4,D18-2012,0,0.0195606,"op on Automatic Spoken Language Translation in Real-World Settings Page 41 4 Experimental setting Data For our experiments we use MuST-Cinema (Karakanta et al., 2020b), an ST corpus compiled from TED Talk subtitles. This corpus is ideal for exploring display modes other than word-for-word because it contains subtitle breaks as special symbols. We conduct experiments on three language pairs, English→Italian (442 hours), English→German (408 hours) and English→French (492 hours). For tuning and evaluation we use the MuST-Cinema dev and test sets. The text data were tokenized using SentencePiece (Kudo and Richardson, 2018) with the unigram setting (Kudo, 2018), trained on the training data with a 10k-token vocabulary size. The source audio was pre-processed with the SpecAugment data augmentation technique (Park et al., 2019), then the speech features (80 log Mel-filter banks) were extracted and Cepstral Mean and Variance Normalization was applied at global level. Samples with a length above 30s were filtered out. The configuration parameters are the default ones set by Ma et al. (2020). Training settings Our SimulST systems are Transformer-based models (Vaswani et al., 2017), composed by 12 encoder layers, 6 de"
2021.mtsummit-asltrw.4,P19-1289,0,0.231033,"the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st Workshop on Automatic Spoken Language Translation in Real-World Settings Page 35 of the task and the scarcity of highly-skilled professionals for live subtitling call for a more pronounced role of technology for providing real-time access to information. These growing needs for access to multilingual spoken content have motivated researchers to develop fully automatic solutions for real-time spoken language translation (Grissom II et al., 2014; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Ma et al., 2019). The new possibilities opened up by neural machine translation have led to improvements in automatic simultaneous speech-to-text translation (SimulST). In SimulST (Ma et al., 2020; Ren et al., 2020), the generation of the translation starts before the entire audio input is received, which is an indispensable characteristic for achieving low latency (translation delay) between speech and text in live events. The translation becomes available at consecutive steps, usually one word at a time. However, a display mode based on the word-for-word rate of generation of SimulST systems may not be opti"
2021.mtsummit-asltrw.4,2020.aacl-main.58,0,0.241618,"the scarcity of highly-skilled professionals for live subtitling call for a more pronounced role of technology for providing real-time access to information. These growing needs for access to multilingual spoken content have motivated researchers to develop fully automatic solutions for real-time spoken language translation (Grissom II et al., 2014; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Ma et al., 2019). The new possibilities opened up by neural machine translation have led to improvements in automatic simultaneous speech-to-text translation (SimulST). In SimulST (Ma et al., 2020; Ren et al., 2020), the generation of the translation starts before the entire audio input is received, which is an indispensable characteristic for achieving low latency (translation delay) between speech and text in live events. The translation becomes available at consecutive steps, usually one word at a time. However, a display mode based on the word-for-word rate of generation of SimulST systems may not be optimal for displaying readable subtitles. Studies in intralingual subtitling have shown that a word-for-word display increases the number of saccadic crossovers between text and scene"
2021.mtsummit-asltrw.4,2020.acl-main.350,0,0.54983,"highly-skilled professionals for live subtitling call for a more pronounced role of technology for providing real-time access to information. These growing needs for access to multilingual spoken content have motivated researchers to develop fully automatic solutions for real-time spoken language translation (Grissom II et al., 2014; Gu et al., 2017; Alinejad et al., 2018; Arivazhagan et al., 2019; Ma et al., 2019). The new possibilities opened up by neural machine translation have led to improvements in automatic simultaneous speech-to-text translation (SimulST). In SimulST (Ma et al., 2020; Ren et al., 2020), the generation of the translation starts before the entire audio input is received, which is an indispensable characteristic for achieving low latency (translation delay) between speech and text in live events. The translation becomes available at consecutive steps, usually one word at a time. However, a display mode based on the word-for-word rate of generation of SimulST systems may not be optimal for displaying readable subtitles. Studies in intralingual subtitling have shown that a word-for-word display increases the number of saccadic crossovers between text and scene (Rajendran et al.,"
2021.mtsummit-asltrw.4,2020.aacl-demo.6,0,0.0152716,"SpecAugment data augmentation technique (Park et al., 2019), then the speech features (80 log Mel-filter banks) were extracted and Cepstral Mean and Variance Normalization was applied at global level. Samples with a length above 30s were filtered out. The configuration parameters are the default ones set by Ma et al. (2020). Training settings Our SimulST systems are Transformer-based models (Vaswani et al., 2017), composed by 12 encoder layers, 6 decoder layers, 256 features for the attention layers and 2,048 hidden units in the feed-forward layers. All models are based on a custom version of Wang et al. (2020), having two initial 1D convolutional layers with gelu activation functions (Hendrycks and Gimpel, 2020), but adapted to the simultaneous scenario as per Ma et al. (2020). Moreover, the encoder self-attentions are biased using a logarithmic distance penalty (Di Gangi et al., 2019), leveraging the local context. Training was performed with cross entropy loss, Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-4 with an inverse square-root scheduler and 4,000 warm-up updates. We set mini-batches of 5,000 max tokens and update the gradients every 16 mini-batches. The best checkpoint"
2021.mtsummit-asltrw.4,2020.acl-main.254,0,0.0210371,"Missing"
2021.mtsummit-asltrw.4,D19-1137,0,0.0132134,"or a write action, that is, whether to receive new information from the input or to write using the information received until that step. Consequently, a SimulST system needs a policy which decides the next action. Decision policies can be divided into: fixed, when the decision is taken based on the elapsed time, and adaptive, when the decision is taken by looking also at the contextual information extracted from the input. Even if the adoption of a fixed policy disregards the input context leading to a sub-optimal solution, little research has been done on adaptive policies (Gu et al., 2017; Zheng et al., 2019a, 2020) because they are hard and time-consuming to train (Zheng et al., 2019b; Arivazhagan et al., 2019). Among the fixed policies, the most popular and recently studied is the wait-k strategy, which was first proposed by Ma et al. (2019) for simultaneous Machine Translation (SimulMT). The SimulMT wait-k policy is based on waiting for k source words before starting to generate the target sentence. This simple yet effective approach was then employed in SimulST, as in Ma et al. (2020) and Ren et al. (2020), by using direct models i.e. models that, given an audio source, generate a textual tar"
2021.mtsummit-loresmt.10,N19-1388,0,0.274347,"al machine translation, by testing our models in an in-domain, out-of-domain, and source to target domain mismatch scenarios. 2 Zero-Shot Translation From a broad perspective, ZST research is moving in three directions, (i) improving translation quality by employing ZST specific objectives (Chen et al., 2017; Lu et al., 2018; Blackwood et al., 2018; Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019a; Pham et al., 2019; Ji et al., 2019; Siddhant et al., 2020), (ii) training favorable large scale multilingual models for the ZST languages with lexically and linguistically similar languages (Aharoni et al., 2019; Arivazhagan et al., 2019b), and (iii) incrementally learning better model for the ZST directions with selflearning objectives (Lakew et al., 2017; Gu et al., 2019; Zhang et al., 2020). The common way of employing self-supervised learning in ZST modeling is iterative back-translation that generates the source from the monolingual target to construct a new parallel sentence pair. In terms of performance, while (i) and (ii) fall behind, (iii) either approaches or even outperforms the two-step pivot translation approach (S → P → T ). Proceedings of the 18th Biennial Machine Translation Summit, V"
2021.mtsummit-loresmt.10,N19-1121,0,0.0234107,"ata involving U . • We empirically evaluate our approach on diverse language directions and in a real-world zero-resource scenario, a testing condition disregarded in previous literature. • We provide a rigorous comparison against unsupervised neural machine translation, by testing our models in an in-domain, out-of-domain, and source to target domain mismatch scenarios. 2 Zero-Shot Translation From a broad perspective, ZST research is moving in three directions, (i) improving translation quality by employing ZST specific objectives (Chen et al., 2017; Lu et al., 2018; Blackwood et al., 2018; Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019a; Pham et al., 2019; Ji et al., 2019; Siddhant et al., 2020), (ii) training favorable large scale multilingual models for the ZST languages with lexically and linguistically similar languages (Aharoni et al., 2019; Arivazhagan et al., 2019b), and (iii) incrementally learning better model for the ZST directions with selflearning objectives (Lakew et al., 2017; Gu et al., 2019; Zhang et al., 2020). The common way of employing self-supervised learning in ZST modeling is iterative back-translation that generates the source from the monolingual target to construct a new p"
2021.mtsummit-loresmt.10,D18-1549,0,0.0177966,"the semi-supervised learning with backtranslation (Sennrich et al., 2015), particularly if the initial supervised model is good enough for augmenting quality pseudo-bitext (Poncelas et al., 2018; Ott et al., 2018; Caswell et al., 2019). Moreover, back-translation showed to be a core element of new monolingual based approaches. These include zero-shot NMT (Lakew et al., 2017; Gu et al., 2019; Currey and Heafield, 2019), which relies on a multilingual model (Johnson et al., 2017; Ha et al., 2016) (Fig. 1b) and unsupervised NMT, which initializes from pre-trained embeddings (Lample et al., 2018; Artetxe et al., 2018) or cross-lingual language model (Lample and Conneau, 2019) (Fig. 1d). At least two observations can be made on the approaches that leverage monolingual data: i) they require high-quality and comparable monolingual examples, and ii) they show poor performance on real-world zero-resource language pairs (ZRPs)1 (Neubig and Hu, 2018; Guzm´an et al., 2019). To overcome these problems, in this work we propose a zero-shot modeling approach (Fig. 1c) to translate from an unseen source language U to a target language T that has only been † Work 1 ZRP : conducted when the author was at FBK. a language"
2021.mtsummit-loresmt.10,2020.acl-main.658,0,0.0520646,"Missing"
2021.mtsummit-loresmt.10,W09-0432,0,0.122291,"Missing"
2021.mtsummit-loresmt.10,C18-1263,0,0.0277442,"e-training on parallel data involving U . • We empirically evaluate our approach on diverse language directions and in a real-world zero-resource scenario, a testing condition disregarded in previous literature. • We provide a rigorous comparison against unsupervised neural machine translation, by testing our models in an in-domain, out-of-domain, and source to target domain mismatch scenarios. 2 Zero-Shot Translation From a broad perspective, ZST research is moving in three directions, (i) improving translation quality by employing ZST specific objectives (Chen et al., 2017; Lu et al., 2018; Blackwood et al., 2018; Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019a; Pham et al., 2019; Ji et al., 2019; Siddhant et al., 2020), (ii) training favorable large scale multilingual models for the ZST languages with lexically and linguistically similar languages (Aharoni et al., 2019; Arivazhagan et al., 2019b), and (iii) incrementally learning better model for the ZST directions with selflearning objectives (Lakew et al., 2017; Gu et al., 2019; Zhang et al., 2020). The common way of employing self-supervised learning in ZST modeling is iterative back-translation that generates the source from the monolingu"
2021.mtsummit-loresmt.10,W11-2138,0,0.0448973,"Missing"
2021.mtsummit-loresmt.10,W19-5206,0,0.013749,"l., 2014; Bahdanau et al., 2014), model learning using unlabeled (monolingual) data is increasingly gaining ground. Undoubtedly, the main motivating factor to explore beyond supervised learning is the lack of enough (parallel) examples, a performance bottleneck regardless of the underlying architecture (Koehn and Knowles, 2017). A fairly successful approach using monolingual data is the semi-supervised learning with backtranslation (Sennrich et al., 2015), particularly if the initial supervised model is good enough for augmenting quality pseudo-bitext (Poncelas et al., 2018; Ott et al., 2018; Caswell et al., 2019). Moreover, back-translation showed to be a core element of new monolingual based approaches. These include zero-shot NMT (Lakew et al., 2017; Gu et al., 2019; Currey and Heafield, 2019), which relies on a multilingual model (Johnson et al., 2017; Ha et al., 2016) (Fig. 1b) and unsupervised NMT, which initializes from pre-trained embeddings (Lample et al., 2018; Artetxe et al., 2018) or cross-lingual language model (Lample and Conneau, 2019) (Fig. 1d). At least two observations can be made on the approaches that leverage monolingual data: i) they require high-quality and comparable monolingual"
2021.mtsummit-loresmt.10,2012.eamt-1.60,0,0.00897226,"und that defines zero-shot translation without the pivot language, we selected a real-world low-resource languages benchmark. In other words, we considered the data to incorporate multiple and diverse languages, including parallel data for building strong baselines and monolingual data to evaluate Z NMT. Moreover, our choice is motivated by the findings of Neubig and Hu (2018) and Guzm´an et al. (2019), showing that monolingual-based approaches under-perform when assessed with real-world zero-shot pairs (ZSPs). 3.1 Languages and Dataset Due to their low-resource nature, we use Ted talks data (Cettolo et al., 2012; Qi et al., 2018) for Azerbaijani (Az), Belarussian (Be), Galician (Gl), and Slovak (Sk) paired with English (En). The four pairs come with train, dev, and test sets, with a max of 61k and as few as 4.5k examples, creating an ideal scenario of low-resource pair (LRP). The parallel data of the LRP is used to build baseline models in isolation and in a multilingual settings. The same dataset has been also used in recent works in an extremely low-resource scenario (Neubig and Hu, 2018; Xia et al., 2019; Lakew et al., 2019). For the approaches utilizing monolingual data, we take the non–En side f"
2021.mtsummit-loresmt.10,P17-1176,0,0.0128986,"urce language U , with no need of pre-training on parallel data involving U . • We empirically evaluate our approach on diverse language directions and in a real-world zero-resource scenario, a testing condition disregarded in previous literature. • We provide a rigorous comparison against unsupervised neural machine translation, by testing our models in an in-domain, out-of-domain, and source to target domain mismatch scenarios. 2 Zero-Shot Translation From a broad perspective, ZST research is moving in three directions, (i) improving translation quality by employing ZST specific objectives (Chen et al., 2017; Lu et al., 2018; Blackwood et al., 2018; Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019a; Pham et al., 2019; Ji et al., 2019; Siddhant et al., 2020), (ii) training favorable large scale multilingual models for the ZST languages with lexically and linguistically similar languages (Aharoni et al., 2019; Arivazhagan et al., 2019b), and (iii) incrementally learning better model for the ZST directions with selflearning objectives (Lakew et al., 2017; Gu et al., 2019; Zhang et al., 2020). The common way of employing self-supervised learning in ZST modeling is iterative back-translation tha"
2021.mtsummit-loresmt.10,D14-1179,0,0.0161358,"Missing"
2021.mtsummit-loresmt.10,D19-5610,0,0.0164102,"sed learning is the lack of enough (parallel) examples, a performance bottleneck regardless of the underlying architecture (Koehn and Knowles, 2017). A fairly successful approach using monolingual data is the semi-supervised learning with backtranslation (Sennrich et al., 2015), particularly if the initial supervised model is good enough for augmenting quality pseudo-bitext (Poncelas et al., 2018; Ott et al., 2018; Caswell et al., 2019). Moreover, back-translation showed to be a core element of new monolingual based approaches. These include zero-shot NMT (Lakew et al., 2017; Gu et al., 2019; Currey and Heafield, 2019), which relies on a multilingual model (Johnson et al., 2017; Ha et al., 2016) (Fig. 1b) and unsupervised NMT, which initializes from pre-trained embeddings (Lample et al., 2018; Artetxe et al., 2018) or cross-lingual language model (Lample and Conneau, 2019) (Fig. 1d). At least two observations can be made on the approaches that leverage monolingual data: i) they require high-quality and comparable monolingual examples, and ii) they show poor performance on real-world zero-resource language pairs (ZRPs)1 (Neubig and Hu, 2018; Guzm´an et al., 2019). To overcome these problems, in this work we"
2021.mtsummit-loresmt.10,2020.acl-main.252,0,0.0184748,"n a real-world zero-resource scenario, a testing condition disregarded in previous literature. • We provide a rigorous comparison against unsupervised neural machine translation, by testing our models in an in-domain, out-of-domain, and source to target domain mismatch scenarios. 2 Zero-Shot Translation From a broad perspective, ZST research is moving in three directions, (i) improving translation quality by employing ZST specific objectives (Chen et al., 2017; Lu et al., 2018; Blackwood et al., 2018; Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019a; Pham et al., 2019; Ji et al., 2019; Siddhant et al., 2020), (ii) training favorable large scale multilingual models for the ZST languages with lexically and linguistically similar languages (Aharoni et al., 2019; Arivazhagan et al., 2019b), and (iii) incrementally learning better model for the ZST directions with selflearning objectives (Lakew et al., 2017; Gu et al., 2019; Zhang et al., 2020). The common way of employing self-supervised learning in ZST modeling is iterative back-translation that generates the source from the monolingual target to construct a new parallel sentence pair. In terms of performance, while (i) and (ii) fall behind, (iii) e"
2021.mtsummit-loresmt.10,P19-1579,0,0.223303,"(ZSPs). 3.1 Languages and Dataset Due to their low-resource nature, we use Ted talks data (Cettolo et al., 2012; Qi et al., 2018) for Azerbaijani (Az), Belarussian (Be), Galician (Gl), and Slovak (Sk) paired with English (En). The four pairs come with train, dev, and test sets, with a max of 61k and as few as 4.5k examples, creating an ideal scenario of low-resource pair (LRP). The parallel data of the LRP is used to build baseline models in isolation and in a multilingual settings. The same dataset has been also used in recent works in an extremely low-resource scenario (Neubig and Hu, 2018; Xia et al., 2019; Lakew et al., 2019). For the approaches utilizing monolingual data, we take the non–En side for each of the four LRP languages as in-domain (IND) monolingual examples. For the En monolingual data, segments are collected from the target side of the respective S−T (En) pairs. However, to avoid the presence of comparable sentences in the U and T sides of the ZSP, we discard segments of monolingual En if the T (En) side of the U − T and S − T are overlapping. For out-of-domain (OOD) monolingual data we extract segments from Wikipedia dumps, similar to Xia et al. (2019).2 The collected data are d"
2021.mtsummit-loresmt.10,2020.acl-main.148,0,0.0195096,"ch is moving in three directions, (i) improving translation quality by employing ZST specific objectives (Chen et al., 2017; Lu et al., 2018; Blackwood et al., 2018; Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019a; Pham et al., 2019; Ji et al., 2019; Siddhant et al., 2020), (ii) training favorable large scale multilingual models for the ZST languages with lexically and linguistically similar languages (Aharoni et al., 2019; Arivazhagan et al., 2019b), and (iii) incrementally learning better model for the ZST directions with selflearning objectives (Lakew et al., 2017; Gu et al., 2019; Zhang et al., 2020). The common way of employing self-supervised learning in ZST modeling is iterative back-translation that generates the source from the monolingual target to construct a new parallel sentence pair. In terms of performance, while (i) and (ii) fall behind, (iii) either approaches or even outperforms the two-step pivot translation approach (S → P → T ). Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 97 Despite these progresses, current approaches make identical assumptions, namely: i"
2021.mtsummit-loresmt.10,D16-1163,0,0.0226311,"Missing"
C08-3008,ou-etal-2008-development,1,0.449167,"Missing"
C14-1040,W13-2241,0,0.154309,"enerated translation, to linguistically motivated measures that estimate how adequate the translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a). State-of-the-art QE explores different supervised linear or non-linear learning methods for regression or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck, 2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the highdimensionality of the feature space (Soricut et al., 2012; de Souza et al., 2013a; Beck et al., 2013; de Souza et al., 2013b). Despite constant improvements, such learning methods have limitations. The main one is that they assume that both training and test data are independently and identically distributed. As a consequence, when they are applied to data from a different distribution or domain they show poor pe"
C14-1040,W06-1615,0,0.0764529,"related tasks to improve model generalization (Caruana, 1997). Although it was not originally thought for transferring knowledge to a new task, MTL can be used to achieve this objective due to its capability to capture task relatedness, which is important knowledge that can be applied to a new task (Jiang, 2009). Domain adaptation is a kind of transfer learning in which source and target domains (i.e. training and test) are different but the tasks are the same (Pan and Yang, 2010). The domain adaptation techniques that inspire our work have been successfully applied to a variety of NLP tasks (Blitzer et al., 2006; Jiang and Zhai, 2007). For instance, an effective solution for supervised domain adaptation, EasyAdapt (SVR FEDA henceforth), was proposed in (Daum´e III, 2007) and applied to named entity recognition, part-of-speech tagging and shallow parsing. The approach transforms the domain adaptation problem into a standard learning problem by augmenting the source and target feature set. The feature space is transformed to be a cross-product of the features of the source and target domains augmented with the original target domain features. In supervised domain adaptation one has access to out-of-dom"
C14-1040,W12-3109,0,0.0662774,"omatically generated translation, to linguistically motivated measures that estimate how adequate the translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a). State-of-the-art QE explores different supervised linear or non-linear learning methods for regression or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck, 2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the highdimensionality of the feature space (Soricut et al., 2012; de Souza et al., 2013a; Beck et al., 2013; de Souza et al., 2013b). Despite constant improvements, such learning methods have limitations. The main one is that they assume that both training and test data are independently and identically distributed. As a consequence, when they are applied to data from a different distribution or domai"
C14-1040,P13-1004,0,0.434925,"e and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour (Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data. The second research objective motivates our investigation on methods that allow the training and test domains and 410 the distributions to be different. Recent work in QE focused on aspects that are problematic even in the controlled WMT scenario, and are closely related to the flexibility/adaptability issue. Focusing on the first of the two aforementioned directions (i.e. modeling translators’ behaviour), Cohn and Specia (2013) propose a Multitask Gaussian Process method that jointly learns a series of annotator-specific models and that outperforms models trained for each annotator. Our work differs from theirs in that we are interested in the latter research direction (i.e. coping with domain and distribution diversity) and we use in and out-of-domain data to learn robust in-domain models. Our scenario represents a more challenging setting than the one tackled in (Cohn and Specia, 2013), which does not consider different domains. In transfer learning there are many techniques suitable to fulfill our requirements. T"
C14-1040,P07-1033,0,0.116019,"Missing"
C14-1040,W13-2243,1,0.876554,"Missing"
C14-1040,P13-2135,1,0.893593,"Missing"
C14-1040,W12-3112,0,0.0227192,"pects of the problem: feature engineering and machine learning methods. Feature engineering accounts for linguistically-based predictors that aim to model different perspectives of the quality estimation problem. The research ranges from identifying indicators that approximate the complexity of translating the source sentence and designing features that model the fluency of the automatically generated translation, to linguistically motivated measures that estimate how adequate the translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a). State-of-the-art QE explores different supervised linear or non-linear learning methods for regression or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck, 2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the highdimensionality of the fea"
C14-1040,P07-1034,0,0.0240129,"ve model generalization (Caruana, 1997). Although it was not originally thought for transferring knowledge to a new task, MTL can be used to achieve this objective due to its capability to capture task relatedness, which is important knowledge that can be applied to a new task (Jiang, 2009). Domain adaptation is a kind of transfer learning in which source and target domains (i.e. training and test) are different but the tasks are the same (Pan and Yang, 2010). The domain adaptation techniques that inspire our work have been successfully applied to a variety of NLP tasks (Blitzer et al., 2006; Jiang and Zhai, 2007). For instance, an effective solution for supervised domain adaptation, EasyAdapt (SVR FEDA henceforth), was proposed in (Daum´e III, 2007) and applied to named entity recognition, part-of-speech tagging and shallow parsing. The approach transforms the domain adaptation problem into a standard learning problem by augmenting the source and target feature set. The feature space is transformed to be a cross-product of the features of the source and target domains augmented with the original target domain features. In supervised domain adaptation one has access to out-of-domain labels and wants to"
C14-1040,P09-1114,0,0.0181837,"many techniques suitable to fulfill our requirements. The aim of transfer learning is to extract the knowledge from one or more source tasks and apply it to a target task (Pan and Yang, 2010). One type of transfer learning is multitask learning (MTL), which uses domain-specific training signals of related tasks to improve model generalization (Caruana, 1997). Although it was not originally thought for transferring knowledge to a new task, MTL can be used to achieve this objective due to its capability to capture task relatedness, which is important knowledge that can be applied to a new task (Jiang, 2009). Domain adaptation is a kind of transfer learning in which source and target domains (i.e. training and test) are different but the tasks are the same (Pan and Yang, 2010). The domain adaptation techniques that inspire our work have been successfully applied to a variety of NLP tasks (Blitzer et al., 2006; Jiang and Zhai, 2007). For instance, an effective solution for supervised domain adaptation, EasyAdapt (SVR FEDA henceforth), was proposed in (Daum´e III, 2007) and applied to named entity recognition, part-of-speech tagging and shallow parsing. The approach transforms the domain adaptation"
C14-1040,P07-2045,0,0.00522066,"length 19 19 9 8 21 23 Table 1: Datasets statistics for each domain. The TED talks domain is formed by subtitles of several talks in a range of topics presented in the TED conferences. The complete dataset has been used for MT and automatic speech recognition systems evaluation within the International Workshop on Spoken Language Translation (IWSLT). The News domain is formed by newswire text used in WMT translation campaigns and covers different topics. The IT texts come from a software user manual translated by a statistical MT system based on the state-ofthe-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences. The post-editions were collected from one professional translator operating on the Matecat5 CAT tool in real working conditions. Table 1 provides macro-indicators (number of tokens, vocabulary size, average sentence length) that evidence the large difference between the domains addressed by our experiments and give an idea of the difficulty of the task. A peculiarity of the TED domain is that it is formed by manual transcriptions of speech translated by different MT systems, configuring a different type of discourse than News and IT. In TED, the vocabul"
C14-1040,W12-3122,1,0.757817,"ular, on two major aspects of the problem: feature engineering and machine learning methods. Feature engineering accounts for linguistically-based predictors that aim to model different perspectives of the quality estimation problem. The research ranges from identifying indicators that approximate the complexity of translating the source sentence and designing features that model the fluency of the automatically generated translation, to linguistically motivated measures that estimate how adequate the translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a). State-of-the-art QE explores different supervised linear or non-linear learning methods for regression or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck, 2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the highd"
C14-1040,W12-3117,0,0.0186229,"ature engineering and machine learning methods. Feature engineering accounts for linguistically-based predictors that aim to model different perspectives of the quality estimation problem. The research ranges from identifying indicators that approximate the complexity of translating the source sentence and designing features that model the fluency of the automatically generated translation, to linguistically motivated measures that estimate how adequate the translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a). State-of-the-art QE explores different supervised linear or non-linear learning methods for regression or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck, 2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the highdimensionality of the feature space (Soricut e"
C14-1040,2013.mtsummit-posters.13,1,0.867328,"Missing"
C14-1040,2006.amta-papers.25,0,0.112566,". 1 Introduction Machine Translation (MT) Quality Estimation (QE) aims to automatically predict the quality of MT output without using reference translations (Blatz et al., 2003; Specia et al., 2009). QE systems usually employ supervised machine learning models that use different information extracted from (source, target) sentence pairs as features along with quality scores as labels. The notion of quality that these models measure can be indicated by different scores. Some examples are the average number of edits required to post-edit the MT output, i.e., human translation edit rate1 (HTER (Snover et al., 2006)), and the time (in seconds) required to post-edit a translation produced by an MT system (Specia, 2011). Research on QE has received a strong boost in recent years due to the increase in the usage of MT systems in real-world applications. Automatic and reference-free MT quality prediction demonstrated to be useful for different applications, such as: deciding whether the translation output can be published without post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions that should be rewritten from scratch (Specia et al., 2009), selecting the best translat"
C14-1040,P10-1063,0,0.0595759,"by different scores. Some examples are the average number of edits required to post-edit the MT output, i.e., human translation edit rate1 (HTER (Snover et al., 2006)), and the time (in seconds) required to post-edit a translation produced by an MT system (Specia, 2011). Research on QE has received a strong boost in recent years due to the increase in the usage of MT systems in real-world applications. Automatic and reference-free MT quality prediction demonstrated to be useful for different applications, such as: deciding whether the translation output can be published without post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions that should be rewritten from scratch (Specia et al., 2009), selecting the best translation output from a pool of MT systems (Specia et al., 2010), and informing readers of the translation whether it is reliable or not (Turchi et al., 2012). Another example is the computer-assisted translation (CAT) scenario, in which it might be necessary to predict the quality of translation suggestions generated by different MT systems to support the activity of post editors working with different genres of text. The dominant QE framework presents some cha"
C14-1040,W12-3118,0,0.0996593,"al., 2012; Specia et al., 2012; de Souza et al., 2013a). State-of-the-art QE explores different supervised linear or non-linear learning methods for regression or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck, 2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the highdimensionality of the feature space (Soricut et al., 2012; de Souza et al., 2013a; Beck et al., 2013; de Souza et al., 2013b). Despite constant improvements, such learning methods have limitations. The main one is that they assume that both training and test data are independently and identically distributed. As a consequence, when they are applied to data from a different distribution or domain they show poor performance. This limitation harms the performance of QE systems for several real-world applications, such as CAT environments. Advanced CAT systems currently integrate suggestions obtained from MT engines with those derived from translation m"
C14-1040,W14-3340,1,0.817737,"Missing"
C14-1040,2009.eamt-1.5,1,0.941952,"chieved in recent years, current MT QE systems are not capable of dealing with data coming from different train/test distributions or domains, and scenarios in which training data is scarce. We investigate different multitask learning methods that can cope with such limitations and show that they overcome current state-of-the-art methods in real-world conditions where training and test data come from different domains. 1 Introduction Machine Translation (MT) Quality Estimation (QE) aims to automatically predict the quality of MT output without using reference translations (Blatz et al., 2003; Specia et al., 2009). QE systems usually employ supervised machine learning models that use different information extracted from (source, target) sentence pairs as features along with quality scores as labels. The notion of quality that these models measure can be indicated by different scores. Some examples are the average number of edits required to post-edit the MT output, i.e., human translation edit rate1 (HTER (Snover et al., 2006)), and the time (in seconds) required to post-edit a translation produced by an MT system (Specia, 2011). Research on QE has received a strong boost in recent years due to the inc"
C14-1040,W12-3110,0,0.0181957,"machine learning methods. Feature engineering accounts for linguistically-based predictors that aim to model different perspectives of the quality estimation problem. The research ranges from identifying indicators that approximate the complexity of translating the source sentence and designing features that model the fluency of the automatically generated translation, to linguistically motivated measures that estimate how adequate the translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a). State-of-the-art QE explores different supervised linear or non-linear learning methods for regression or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck, 2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has received attention is the optimal selection of features in order to overcome issues related with the highdimensionality of the feature space (Soricut et al., 2012; de Souza"
C14-1040,2011.eamt-1.12,0,0.0270622,"MT output without using reference translations (Blatz et al., 2003; Specia et al., 2009). QE systems usually employ supervised machine learning models that use different information extracted from (source, target) sentence pairs as features along with quality scores as labels. The notion of quality that these models measure can be indicated by different scores. Some examples are the average number of edits required to post-edit the MT output, i.e., human translation edit rate1 (HTER (Snover et al., 2006)), and the time (in seconds) required to post-edit a translation produced by an MT system (Specia, 2011). Research on QE has received a strong boost in recent years due to the increase in the usage of MT systems in real-world applications. Automatic and reference-free MT quality prediction demonstrated to be useful for different applications, such as: deciding whether the translation output can be published without post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions that should be rewritten from scratch (Specia et al., 2009), selecting the best translation output from a pool of MT systems (Specia et al., 2010), and informing readers of the translation whe"
C14-1040,2012.eamt-1.39,1,0.847437,"d a strong boost in recent years due to the increase in the usage of MT systems in real-world applications. Automatic and reference-free MT quality prediction demonstrated to be useful for different applications, such as: deciding whether the translation output can be published without post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions that should be rewritten from scratch (Specia et al., 2009), selecting the best translation output from a pool of MT systems (Specia et al., 2010), and informing readers of the translation whether it is reliable or not (Turchi et al., 2012). Another example is the computer-assisted translation (CAT) scenario, in which it might be necessary to predict the quality of translation suggestions generated by different MT systems to support the activity of post editors working with different genres of text. The dominant QE framework presents some characteristics that can limit models’ applicability in such real-world scenarios. First, the scores used as training labels (HTER, time) are costly to obtain because they are derived from manual post-editions of MT output. Such requirement makes it difficult to develop models for domains in wh"
C14-1040,W13-2231,1,0.871667,"tions, such as CAT environments. Advanced CAT systems currently integrate suggestions obtained from MT engines with those derived from translation memories (TMs). In such framework, the compelling need to speed up the translation process and reduce its costs by presenting human translators with good-quality suggestions raises interesting research challenges for the QE community. In such environments, translation jobs come from different domains that might be translated by different MT systems and are routed to professional translators with different idiolect, background and quality standards (Turchi et al., 2013). Such variability calls for flexible and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour (Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data. The second research objective motivates our investigation on methods that allow the training and test domains and 410 the distributions to be different. Recent work in QE focused on aspects that are problematic even in the controlled WMT scenario, and are closely related to the flexibility/adaptability issue. Focusing on the first of the two aforementioned directions (i.e"
C14-1040,P14-1067,1,0.814947,"Missing"
C14-1040,2013.mtsummit-papers.15,0,0.239719,"instances for training contrasts with the 800 or more instances of the WMT evaluation campaigns and is closer to real-world applications where the availability of large and representative training sets is far from being guaranteed (e.g. the CAT scenario). The sentence tuples for the first two domains are randomly sampled from the Trace corpus4 . The translations were generated by two different MT systems, a state-of-the-art phrase-based statistical MT system and a commercial rule-based system. Furthermore, the translations were post-edited by up to four different translators, as described in (Wisniewski et al., 2013). Domain TED source TED target IT source IT target News source News target No. of tokens 6858 7016 3310 3134 7605 8230 Vocab. size 1659 1828 1004 1049 2273 2346 Avg. sent. length 19 19 9 8 21 23 Table 1: Datasets statistics for each domain. The TED talks domain is formed by subtitles of several talks in a range of topics presented in the TED conferences. The complete dataset has been used for MT and automatic speech recognition systems evaluation within the International Workshop on Spoken Language Translation (IWSLT). The News domain is formed by newswire text used in WMT translation campaign"
C14-1040,W12-3102,0,\N,Missing
C14-1040,C04-1046,0,\N,Missing
C14-1040,W13-2201,0,\N,Missing
C14-1171,P11-1022,0,0.0105512,"ions has motivated a large body of research.4 Quality estimation for MT and ASR have a number of commonalities. First, they both deal with a “source” (respectively a sentence in a language L and an acoustic utterance) and an “hypothesis” whose quality has to be estimated without references (respectively a translation in a language L1 and an automatic transcription of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality estimation is similar to its MT counterpart where research focused on quality predictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estima"
C14-1171,W13-2243,1,0.770971,"Missing"
C14-1171,W14-3340,1,0.674362,"Missing"
C14-1171,C14-1040,1,0.869419,"Missing"
C14-1171,P04-1044,0,0.0399342,"decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a supervised regression problem. Given a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance in a test set of unseen (signal, transcription) pairs. Features. As shown in Table 1, the features used in our experiments (68 in total) can be categorized in four main groups. The first group (ASR features) includes several glass-box features proposed in previous literature on ASR confidence estimation (Litman et al., 2000; Gabsdil and Lemon, 2004; Goldwater et al., 2010; Higgins et al., 2011). These features are suitable only for the ideal situation in which information about systems’ internal decoding strategies is available (as in the experiments discussed in §4.1). We use them as a term of comparison to evaluate the usefulness of the other three groups (signal, hybrid and textual), which belong to the black-box type. These features, which are totally uninformed about the decoding process, have wider applicability to the system-independent ASR quality estimation tasks that represent our target scenario (see Sections 4.2 and 4.3). Mo"
C14-1171,A00-2029,0,0.0337862,"he confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a supervised regression problem. Given a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance in a test set of unseen (signal, transcription) pairs. Features. As shown in Table 1, the features used in our experiments (68 in total) can be categorized in four main groups. The first group (ASR features) includes several glass-box features proposed in previous literature on ASR confidence estimation (Litman et al., 2000; Gabsdil and Lemon, 2004; Goldwater et al., 2010; Higgins et al., 2011). These features are suitable only for the ideal situation in which information about systems’ internal decoding strategies is available (as in the experiments discussed in §4.1). We use them as a term of comparison to evaluate the usefulness of the other three groups (signal, hybrid and textual), which belong to the black-box type. These features, which are totally uninformed about the decoding process, have wider applicability to the system-independent ASR quality estimation tasks that represent our target scenario (see"
C14-1171,J13-2002,0,0.0241361,"obabilities). Secondly, the domain addressed is constrained to responses to prompted utterances, while in this paper we address a large unconstrained domain, namely the automatic transcription of lectures (TED talks) covering different topics. Finally, (Yoon et al., 2010) is based on a rather simple model whose performance is not carefully analysed from the learning point of view (e.g. by comparing the contribution different state-of-the-art algorithms) as we do here. The problem of automating system evaluation without a gold standard has been addressed also in other NLP areas. For instance, (Louis and Nenkova, 2013) recently addressed the assessment of machinegenerated summaries without model summaries. The strongest parallelism with our work, however, can be found in the Machine Translation (MT) evaluation field, where the goal of bypassing the need of manually-created reference translations has motivated a large body of research.4 Quality estimation for MT and ASR have a number of commonalities. First, they both deal with a “source” (respectively a sentence in a language L and an acoustic utterance) and an “hypothesis” whose quality has to be estimated without references (respectively a translation in"
C14-1171,W12-3122,1,0.812518,"imation for MT and ASR have a number of commonalities. First, they both deal with a “source” (respectively a sentence in a language L and an acoustic utterance) and an “hypothesis” whose quality has to be estimated without references (respectively a translation in a language L1 and an automatic transcription of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality estimation is similar to its MT counterpart where research focused on quality predictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a supervised regression probl"
C14-1171,quirk-2004-training,0,0.00980323,"ality has to be estimated without references (respectively a translation in a language L1 and an automatic transcription of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality estimation is similar to its MT counterpart where research focused on quality predictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a supervised regression problem. Given a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance in a test set of unseen (signal, transcription) pairs. Features."
C14-1171,2013.mtsummit-posters.13,1,0.795286,"Missing"
C14-1171,W13-2249,0,0.113781,"edictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a supervised regression problem. Given a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance in a test set of unseen (signal, transcription) pairs. Features. As shown in Table 1, the features used in our experiments (68 in total) can be categorized in four main groups. The first group (ASR features) includes several glass-box features proposed in previous literature on ASR confidence estimation (Litman et al., 2000; Gabsdil and Lemon, 2004; Goldwater et al., 2010; Hi"
C14-1171,P10-1063,0,0.0201985,"ommonalities. First, they both deal with a “source” (respectively a sentence in a language L and an acoustic utterance) and an “hypothesis” whose quality has to be estimated without references (respectively a translation in a language L1 and an automatic transcription of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality estimation is similar to its MT counterpart where research focused on quality predictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a supervised regression problem. Given a training set of (signal, transcripti"
C14-1171,2009.eamt-1.5,1,0.827178,"esearch.4 Quality estimation for MT and ASR have a number of commonalities. First, they both deal with a “source” (respectively a sentence in a language L and an acoustic utterance) and an “hypothesis” whose quality has to be estimated without references (respectively a translation in a language L1 and an automatic transcription of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality estimation is similar to its MT counterpart where research focused on quality predictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a super"
C14-1171,2011.eamt-1.12,0,0.0149626,"ively a translation in a language L1 and an automatic transcription of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality estimation is similar to its MT counterpart where research focused on quality predictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach the automatic estimation of ASR output quality as a supervised regression problem. Given a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance in a test set of unseen (signal, transcription) pairs. Features. As shown in Table 1, the features used in our experime"
C14-1171,P14-1067,1,0.813858,"Missing"
C14-1171,J07-1003,0,0.0132225,"ated reference translations has motivated a large body of research.4 Quality estimation for MT and ASR have a number of commonalities. First, they both deal with a “source” (respectively a sentence in a language L and an acoustic utterance) and an “hypothesis” whose quality has to be estimated without references (respectively a translation in a language L1 and an automatic transcription of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality estimation is similar to its MT counterpart where research focused on quality predictions at word level (Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012) and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the confidence of the decoding process (Felice, 2012; Rubino et al., 2013b). 3 Approach We approach"
C14-1171,W12-3102,0,\N,Missing
C14-1171,C04-1046,0,\N,Missing
C14-1171,W13-2201,0,\N,Missing
C14-1171,2012.iwslt-evaluation.9,0,\N,Missing
C14-1171,2011.iwslt-evaluation.1,0,\N,Missing
C14-2028,2013.mtsummit-papers.5,1,0.880457,"Missing"
C14-2028,2013.mtsummit-wptp.13,1,0.900099,"Missing"
C14-2028,2012.amta-papers.22,1,0.865102,"Missing"
C14-2028,2013.mtsummit-wptp.10,0,0.135035,"Missing"
C14-2028,W13-2231,1,0.678754,"Missing"
C14-2028,P14-1067,1,0.806908,"Missing"
C14-2028,2013.mtsummit-wptp.7,1,\N,Missing
cabrio-etal-2008-qall,W03-2120,0,\N,Missing
cabrio-etal-2008-qall,W99-0310,0,\N,Missing
D11-1062,W10-0733,0,0.0314394,"Missing"
D11-1062,W10-0701,0,0.0128303,"ios of different complexity (monolingual TE, and CLTE between close or distant languages). We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment. 2 Related Works Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7 , have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces. As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers. Focusin"
D11-1062,N10-1045,1,0.400414,"ection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German. 1 Yashar Mehdad FBK-irst and University of Trento Trento, Italy mehdad@fbk.eu Introduction Cross-lingual Textual Entailment (CLTE) has been recently proposed by (Mehdad et al., 2010; Mehdad et al., 2011) as an extension of Textual Entailment (Dagan and Glickman, 2004). The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE, 1 http://www.nist.gov/tac/2011/RTE/ http://nlp.uned.es/clef-qa/ave/ 3 http://www.evalita.it/2009/tasks/te 4 For instance, in the first five RTE Challenges, the average effort needed to create 1,000 pairs featuring full agreement among 3 annotators was around 2.5 person-months. Typical"
D11-1062,P11-1134,1,0.520619,"Missing"
D11-1062,P09-2078,0,0.012216,"bs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways). In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check. Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010). The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010). Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g. “semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions. To tackle these issues the “divide and conquer” approach described in the next s"
D11-1062,P08-1051,0,0.0228368,"of content generation jobs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways). In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check. Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010). The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010). Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g. “semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions. To tackle these issues the “divide and conquer”"
D11-1062,W10-0734,1,0.928539,"ating from scratch aligned CLTE corpora for different language combinations. To this aim, we do not resort to already annotated data, nor languagespecific preprocessing tools. Second, their approach involves qualitative analysis of the collected data only a posteriori, after manual removal of invalid and trivial generated hypotheses. In contrast, our approach integrates quality control mechanisms at all stages of the data collection/annotation process, thus minimizing the recourse to experts to check the quality of the collected material. Related research in the CLTE direction is reported in (Negri and Mehdad, 2010), which describes the creation of an English-Spanish corpus obtained from the RTE-3 dataset by translating the English hypotheses into Spanish. Translations have been crowdsourced adopting a methodology based on translation-validation cycles, defined as separate HITs. Although simplifying the CLTE corpus creation problem, which is recast as the task of translating already available annotated data, this solution is relevant to our work for the idea of combining gold standard units and “validation HITS” as a way to control the quality of the collected data at runtime. 3 Quality Control of Crowds"
D11-1062,P10-1122,0,0.0213975,"ince the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time. Finally, as a by-product of our method, the acquired pairs are fully aligned for all language combinations, thus enabling meaningful comparisons between scenarios of different complexity (monolingual TE, and CLTE between close or distant languages). We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment. 2 Related Works Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7 , have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence T"
D11-1062,D08-1027,0,0.0603459,"Missing"
D11-1062,W10-0725,0,0.194152,"ounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces. As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers. Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities. Taking a step beyond the task of annotating exist5 The CLTE corpora described in this paper will be made freely available for research purposes through the website of the funding EU Project CoSyne (http://www.cosyne.eu/). 6 https://www.mturk.com/ 7 Although MTurk is directly accessible only to US citizens, the CrowdFlower service (http://crowdflower.com/) provides an interface to MTurk for non-US citizens. ing datasets, and showing the feasibili"
D11-1062,bentivogli-etal-2010-building,1,\N,Missing
D14-1172,W05-0909,0,0.0397333,". Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and ev"
D14-1172,2011.mtsummit-papers.17,0,0.0134317,"ems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post-editors). Another limitation of the"
D14-1172,2010.eamt-1.12,0,0.183772,"Missing"
D14-1172,C14-2026,1,0.831976,"e, Arabic, and Russian. An international organization provided us a set of English sentences together with their translation produced by two anonymous MT systems. For each evaluation item (source sentence and two MT outputs) three experts were asked to assign quality scores to the MT outputs, and a fourth expert was asked to annotate translation errors. The four experts, who were all professional translators native in the examined target languages, were carefully trained to get acquainted with the evaluation guidelines and the annotation tool specifically developed for these evaluation tasks (Girardi et al., 2014). The annotation process was carried out in parallel by all annotators over one week, resulting in a final dataset composed of 312 evaluation items for the ENZH direction, 393 for ENAR, and 437 for ENRU. 4.1 Quality Judgements Quality judgements were collected by asking the three experts to rate each automatic translation according to a 1-5 Likert scale, where 1 means “incomprehensible translation” and 5 means “perfect translation”. The distribution of the collected annotations with respect to each quality score is shown in Figure 1. As we can see, this distribution reflects different levels o"
D14-1172,N09-1057,0,0.0141677,"total error frequencies and automatic scores (Popovi´c and Ney, 2011; Farr´us et al., 2012). Using two different error taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we"
D14-1172,2013.mtsummit-wptp.10,0,0.0451853,"Missing"
D14-1172,C04-1072,0,0.0133562,"n the plot. The total number of errors amounts to 16,320 characters for ENZH, 4,926 words for ENAR, and 5,965 words for ENRU. This distribution highlights some differences between languages directions. For example, translations into Arabic and Russian present several morphology errors, while word reordering is the most frequent issue for translations into Chinese. As we will see in §5.1, error frequency does not give a direct indication of their impact on traslation quality judgements. 4.3 Automatic Metrics In our investigation we consider three popular automatic metrics: sentence-level BLEU (Lin and Och, 2004), TER (Snover et al., 2006), and GTM (Turian et al., 2003). We compute all automatic scores by relying on a single reference and by 1647 4000 3500 3000 2500 2000 1500 1000 500 0 LEX MISS MORPH REO ENZH ENAR ENRU Figure 3: Distribution of error types. means of standard packages. In particular, automatic scores on Chinese are computed at the character level. Moreover, as we use metrics as response variables for our regression models, we compute all metrics at the sentence level. The overall mean scores for all systems and languages are reported in Table 2. Differences in systems’ performance can"
D14-1172,2014.eamt-1.38,0,0.54091,"Missing"
D14-1172,2001.mtsummit-papers.68,0,0.0130816,"empirical observations are drawn. Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reas"
D14-1172,J11-4002,0,0.0770266,"Missing"
D14-1172,2013.mtsummit-posters.5,0,0.0997061,"few simpler methods proposed so far. Overall, our study has clear practical implications for MT systems’ development and evaluation. Indeed, the proposed statistical analysis framework represents an ideal instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic"
D14-1172,temnikova-2010-cognitive,0,0.0145572,"es produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post"
D14-1172,2003.mtsummit-papers.51,0,0.431724,"rs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automatic metrics provide a holist"
D14-1172,vilar-etal-2006-error,0,0.851647,"as a development and evaluation criterion, automatic metrics provide a holistic view of systems’ behaviour without identifying the specific issues of a translation. Indeed, the global scores returned by MT evaluation metrics depend on comparisons between translation hypotheses and reference translations, where the causes and the nature of the differences between them are not identified. To cope with these issues and define system improvement priorities, the focus of MT evaluation research is gradually shifting towards profiling systems’ behaviour with respect to various typologies of errors (Vilar et al., 2006; Popovi´c and Ney, 2011; Farr´us et al., 2012, inter alia). This shift has enriched the traditional MT evaluation framework with a new element, that is the actual errors done by a system. Until now, most of the research has focused on the relationship (i.e. the correlation) between two elements of the framework: humans and automatic evaluation metrics. As a new element of the framework, which becomes a sort of “evaluation triangle”, the analysis of error annotations opens interesting research problems related to the relationships between: i) error types and human perception of MT quality and"
D14-1172,W12-3106,0,0.019253,"rror taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been inv"
D14-1172,2014.amta-researchers.20,1,0.775141,"rror taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we employ mixed models to measure the influence of different MT error types - expressed as continuous fixed effects"
D14-1172,2006.amta-papers.25,0,0.280249,"t on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automat"
D14-1172,stymne-ahrenberg-2012-practice,0,0.266291,"al instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011)."
D14-1172,P02-1040,0,\N,Missing
D14-1172,W13-2201,0,\N,Missing
D19-1140,balahur-etal-2014-resource,1,0.907896,"Missing"
D19-1140,N19-1423,0,0.028239,"Missing"
D19-1140,W17-3204,0,0.0163088,"fferings provides a typical example of this situation: a variety of affordable high-performance NLP tools can be easily accessed via APIs but often they are available only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and e"
D19-1140,N18-3012,0,0.0133005,"impossible to exhaustively compute the expected reward, which is thus estimated by samˆ ∼ pθ (.|x(s) ) (2) pθ (ˆ y|x(s) )∆(ˆ y), y s=1 Background and Methodology During training, NMT systems based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) are optimized with maximum likelihood estimation (MLE), which aims to maximize the log-likelihood of the training data. In doing so, they indirectly model the human-oriented quality criteria (adequacy and fluency) expressed in the training corpus. A different strand of research (Ranzato et al., 2016; Shen et al., 2016; Kreutzer et al., 2018) focuses on optimizing the model parameters by maximizing an objective function that leverages either an evaluation metric like BLEU (Papineni et al., 2002) or an external human feedback. These methods are based on Reinforcement Learning (RL), in which the MT system parameters θ define a policy that chooses an action, i.e. generating the next word in ˆ , and gets a reward ∆(ˆ a translation candidate y y) according to that action. Given S training sentences {x(s) }Ss=1 , the RL training goal is to maximize the expected reward: S X 4: 5: 6: 7: 8: Input: x(s) s-th source sentence in training data"
D19-1140,P17-1138,0,0.0982028,"Missing"
D19-1140,D15-1130,0,0.0233668,"ccessed via APIs but often they are available only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense"
D19-1140,S13-2052,0,0.0283728,"increases the probability to sample a “useful” one, possibly by diverting from the initial model behaviour. On the other side, selecting the candidate with the highest reward will push the system towards translations emphasizing input traits that are relevant for the downstream task at hand. In the sentiment classification use case, these are expected to be sentiment-bearing terms that help the classifier to predict the correct class. • German and Italian classifiers on the original, untranslated tweets (Original). 3 Task-specific data. We experiment with a dataset based on Semeval 2013 data (Nakov et al., 2013), which contains polarity-labeled parallel German/Italian–English corpora (Balahur et al., 2014). For each language pair, the development and test sets respectively comprise 583 (197 negative and 386 positive) and 2,173 tweets (601 negative and 1,572 positive). To cope with the skewed data distribution, the negative tweets in the development sets are over-sampled, leading to new balanced sets of 772 tweets. NMT Systems. Our Generic models are based on Transformer (Vaswani et al., 2017), with parameters similar to those used in the original paper. Training data amount to 6.1M (De-En) and 4.56M"
D19-1140,D17-1153,0,0.119131,"Missing"
D19-1140,P02-1040,0,0.105066,"dology During training, NMT systems based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) are optimized with maximum likelihood estimation (MLE), which aims to maximize the log-likelihood of the training data. In doing so, they indirectly model the human-oriented quality criteria (adequacy and fluency) expressed in the training corpus. A different strand of research (Ranzato et al., 2016; Shen et al., 2016; Kreutzer et al., 2018) focuses on optimizing the model parameters by maximizing an objective function that leverages either an evaluation metric like BLEU (Papineni et al., 2002) or an external human feedback. These methods are based on Reinforcement Learning (RL), in which the MT system parameters θ define a policy that chooses an action, i.e. generating the next word in ˆ , and gets a reward ∆(ˆ a translation candidate y y) according to that action. Given S training sentences {x(s) }Ss=1 , the RL training goal is to maximize the expected reward: S X 4: 5: 6: 7: 8: Input: x(s) s-th source sentence in training data, l(s) the ground-truth label, K number of sampled candidates ˆ (s) Output: sampled candidate y C=∅ . Candidates set for k = 1,...,K do c ∼ pθ (.|x(s) ) f ="
D19-1140,E17-1101,0,0.0194126,"often they are available only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense of human-quality standar"
D19-1140,N15-1078,0,0.0245338,"sification task, in which Twitter data in German and Italian are to be classified according to their polarity by means of an English classifier. In this setting, a shortcoming of previous translation-based approaches (Denecke, 2008; Balahur et al., 2014) is that, similar to other traits, 1368 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1368–1374, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sentiment is often not preserved by MT (Salameh et al., 2015; Mohammad et al., 2016; Lohar et al., 2017). Although it represents a viable solution to leverage sentiment analysis to a wide number of languages (Araujo et al., 2016), the translationbased approach should hence be supported by advanced technology able to preserve the sentiment traits of the input. Along this direction, our experiments show that machine-oriented MT optimization makes the classifier’s task easier and eventually results in significant classification improvements. Our results outperform those obtained with translations produced by general-purpose NMT models as well as by an NMT"
D19-1140,P16-1162,0,0.0161613,"e) and 2,173 tweets (601 negative and 1,572 positive). To cope with the skewed data distribution, the negative tweets in the development sets are over-sampled, leading to new balanced sets of 772 tweets. NMT Systems. Our Generic models are based on Transformer (Vaswani et al., 2017), with parameters similar to those used in the original paper. Training data amount to 6.1M (De-En) and 4.56M (It-En) parallel sentences from freely-available corpora. The statistics of the parallel corpora are reported in Table 1. For each language pair, all data are merged and tokenized. Then, byte pair encoding (Sennrich et al., 2016) is applied to obtain 32K sub-word units. Experiments Our evaluation is done by feeding an English sentiment classifier with the translations of German and Italian tweets generated by: • A general-purpose NMT system (Generic); • The same system conditioned with REINFORCE (Reinforce); • The same system conditioned with our Machine-Oriented method (MO-Reinforce). As other terms of comparison, we calculate the results of: De-En It-En Europarl JRC Wikipedia ECB TED KDE News11 News 2M 0.7M 2.5M 0.1M 0.1M 0.3M 0.2M 0.2M 2M 0.8M 1M 0.2M 0.2M 0.3M 0.04M 0.02M Total 6.1M 4.56M Table 1: Statistics of th"
D19-1140,P16-1159,0,0.378266,"e, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense of human-quality standards. To this end, inspired by previous – human-oriented – MT approaches based on Reinforcement Learning (Ranzato et al., 2016; Shen et al., 2016) and Bandit Learning (Kreutzer et al., 2017; Nguyen et al., 2017), we explore a NMT optimization strategy that exploits the weak feedback from the downstream task to influence system’s behaviour towards the generation of optimal “machine-oriented” output. As a proof of concept, we test our approach on a sentiment classification task, in which Twitter data in German and Italian are to be classified according to their polarity by means of an English classifier. In this setting, a shortcoming of previous translation-based approaches (Denecke, 2008; Balahur et al., 2014) is that, similar to other"
D19-1140,D18-1334,0,0.0119267,"only for a few languages. Translating into one of these high-resource languages gives the possibility to address the downstream task by: i) using existing tools for that language to process the translated text, and ii) projecting their output back to the original language. However, using MT “as is” might not be optimal for different reasons. First, despite the qualitative leap brought by neural networks, MT is still not perfect (Koehn and Knowles, 2017). Second, previous literature shows that MT can alter some of the properties of the source text (Mirkin et al., 2015; Rabinovich et al., 2017; Vanmassenhove et al., 2018). Finally, even in the case of a perfect MT able to preserve all the traits of the source sentence, models are still trained on parallel data, which are created by humans and thus reflect quality criteria relevant for humans. In this work, we posit that these criteria might not be the optimal ones for a machine (i.e. a downstream NLP tool fed with MT output). In this scenario, MT should pursue the objective of preserving and emphasizing those properties of the source text that are crucial for the downstream task at hand, even at the expense of human-quality standards. To this end, inspired by"
D19-1140,D18-1397,0,0.0327418,"of obtaining less fluent and adequate output. However, as will be shown in Section 4, this 1369 type of reward induces highly polarized translations that are best suited for our downstream task. • The English classifier on the gold standard English tweets (English); Sampling Approach. A possible sampling strategy is to exploit beam search (Sutskever et al., 2014) to find, at each decoding step, the candidate with the highest probability. Another solution is to use multinomial sampling (Ranzato et al., 2016) which, at each decoding step, samples tokens over the model’s output distribution. In (Wu et al., 2018), the higher results achieved by multinomial sampling are ascribed to its capability to better explore the probability space by generating more diverse candidates. This finding is particularly relevant in the proposed “MT for machines” scenario, in which the emphasis on final performance in the downstream task admits radical (application-oriented) changes in the behaviour of the MT model, even at the expense of human quality standards. To increase the possibility of such changes, we propose a new sampling strategy. Instead of generating only one candidate token via multinomial sampling, K cand"
E17-1050,2011.mtsummit-papers.35,0,0.916424,"Missing"
E17-1050,E14-1042,0,0.107277,"are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been used also for the APE task. To overcome this limitation, we proceed incrementally. First, we propose an approach based on an instance selection strategy, which learns local, sentence-specific APE models from small amounts of relevant data for each translation to be post-edited. Then, on top of it, we add an improved way to estimate the parameters of the sentence-specific APE models. To this aim, we exploit a dynamic knowledge base that keeps track of global statistics computed over all the previous"
E17-1050,2013.mtsummit-papers.5,0,0.146711,"APE cast the problem as a phrase-based statistical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been used also for the APE task. To overcome this limitation, we proceed incrementally. First, we propose an approach based on an instance selection strategy, which learns local, sentence-specific APE models from small amounts of relevant data for each translation to be post-edited. Then, on top of it, we add an improved way to estimate the parameters of the sentence-specific APE models. To this a"
E17-1050,W07-0732,0,0.044522,"he-art online translation systems evaluated in the APE task in MDTE conditions. Thot (Ortiz-Martınez and Casacuberta, 2014), the online system used as term of comparison, shows in fact the inability to discern which of the learned correction rules is suitable for a specific context. In practice, all rules are created equal, for any given domain. 2 Related work Most of the previous works on APE cast the problem as a phrase-based statistical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been u"
E17-1050,W08-0509,0,0.0450666,"ure is applied to the development sets. links each MT word with its corresponding source word/s (mt#src). This representation, proposed by B´echara et al. (2011), leverages the source information to disambiguate post-editing rules and foster their application only in appropriate contexts (the matching condition is defined both on the source and on the target language). The joint representation is used as a source corpus to train all the APE systems compared in this paper and it is obtained by concatenating words in the source (src) and in the MT (mt) segments after aligning them with MGIZA++ (Gao and Vogel, 2008). Evaluation metrics. The performance of the different systems is evaluated in terms of Translation Error rate (TER) (Snover et al., 2006), BLEU (Papineni et al., 2002), and precision (Chatterjee et al., 2015a). TER and BLEU measure the similarity between the MT output and the corresponding references (in this case human post-edits) by looking at the word/n-gram overlaps. Precision is the ratio of the number of sentences an APE system improves (with respect to the MT output) over all the sentences it modifies.9 Higher precision indicates that the APE system is able to improve the quality of mo"
E17-1050,2014.amta-workshop.3,0,0.0126629,"s been modified is derived by string matching the target side of the rule in the final human post-edit. counts are stored apart. At decoding time, the IDs of the samples retrieved by instance selection and the mt sentences are used to query the dynamic knowledge base. The translation options that satisfy the query are retrieved and supplied to the decoder in the form of translation and reordering model information. All the feature scores (four for the translation model and six for the reordering model) are computed on-the-fly. Compared to the suffix arrays used to implement MT dynamic models (Germann, 2014; Denkowski et al., 2014), in which the whole sentence pairs are stored, our technique needs to save more information (all the translation options) but: i) the amount of data in APE is much less that in MT so it can be easily managed by ad hoc solutions, and ii) it allows us to collect global information at translation option level that can result in useful additional features for the model. This last aspect is explored in the next section, in which the reliability of the translation options is measured by looking at the behavior of the post-editors. 3.3 • F2. This rule penalizes the correctio"
E17-1050,2010.amta-papers.21,0,0.0511956,"of the previous works on APE cast the problem as a phrase-based statistical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been used also for the APE task. To overcome this limitation, we proceed incrementally. First, we propose an approach based on an instance selection strategy, which learns local, sentence-specific APE models from small amounts of relevant data for each translation to be post-edited. Then, on top of it, we add an improved way to estimate the parameters of the sentence-specifi"
E17-1050,W15-3025,1,0.946833,"fic correction rules show a relatively high precision, some of them might in fact be useful also in other contexts and should be retained. Our approach avoids this limitation by maintaining a global knowledge base to store all the processed documents, still being able to retrieve post-editing rules specific to a document to be translated.2 less, considering Thot as the state-of-the-art in online APE, we will use it as a term of comparison in our experiments. 3 Online APE system The backbone of our online APE system is similar to the state-of-the-art statistical batch APE approach proposed in (Chatterjee et al., 2015b). The system is trained on (src, mt, pe) triplets, and learns correction rules in the form of (mt#src, pe) pairs. The first element of each pair consists of MT phrases (single or multiple words) that are associated to their corresponding source words by using a word alignment model. This “joint representation” helps to restrict the applicability of each rule to the appropriate context, and was shown to perform better than using only the mt words as the left-hand side of the rules (B´echara et al., 2011). Our migration to the online scenario builds on incrementally extending this backbone wit"
E17-1050,2005.eamt-1.19,0,0.0472713,"sticSearch7 engine is used for this purpose. Once the post-edit is made available to our system, the word alignment between the mt and the pe is computed, the sentence pair is split in phrases and then added to the dynamic model. If a translation option is already present, then the phrase translation and the orientation counts are updated, otherwise it is inserted for the first time. This is run in multithreading, by also managing possible conflicts (i.e. the access to the same translation option by different threads). Word lexical information and phrase 4 In MT, tf-idf was previously used by Hildebrand et al. (2005) to create a pseudo in-domain corpus from a large outof-domain corpus. Our work is the first to investigate it for the APE task in an online learning scenario. 5 https://lucene.apache.org/ 6 https://code.google.com/archive/p/ inc-giza-pp/ 7 http://www.elastic.co/products/ elasticsearch 528 applied by the APE system is obtained from the Moses decoder trace option. The information about which of them has been modified is derived by string matching the target side of the rule in the final human post-edit. counts are stored apart. At decoding time, the IDs of the samples retrieved by instance sele"
E17-1050,P15-2026,1,0.894556,"fic correction rules show a relatively high precision, some of them might in fact be useful also in other contexts and should be retained. Our approach avoids this limitation by maintaining a global knowledge base to store all the processed documents, still being able to retrieve post-editing rules specific to a document to be translated.2 less, considering Thot as the state-of-the-art in online APE, we will use it as a term of comparison in our experiments. 3 Online APE system The backbone of our online APE system is similar to the state-of-the-art statistical batch APE approach proposed in (Chatterjee et al., 2015b). The system is trained on (src, mt, pe) triplets, and learns correction rules in the form of (mt#src, pe) pairs. The first element of each pair consists of MT phrases (single or multiple words) that are associated to their corresponding source words by using a word alignment model. This “joint representation” helps to restrict the applicability of each rule to the appropriate context, and was shown to perform better than using only the mt words as the left-hand side of the rules (B´echara et al., 2011). Our migration to the online scenario builds on incrementally extending this backbone wit"
E17-1050,W16-2378,0,0.468651,"ii) Adapt the output of a general-purpose MT system to the lexicon/style requested in a specific application domain. Similar to what is usually done in MT, APE components learn postediting rules from “parallel” corpora consisting of machine-translated text (mt, optionally with its corresponding source text – src) and its post-edits (pe) provided by human post-editors. The effectiveness of learning from relatively small amounts of post-edited data is evident from the impressive outcomes of the recently held APE shared task at WMT 2016 (Bojar et al., 2016). Different APE paradigms, like neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016) were all able to significantly improve MT output quality in the IT domain, with gains ranging from 2.0 to 5.5 BLEU points. Nevertheless, this success and the positive outcomes of previous work on automatic MT error correction build on a problem formulation that assumes to operate in a controlled lab environment, where the systems are trained and evaluated across a coherent/homogeneous data set. Moving from this controlled scenario to real-world translation workflows, where training and test data can be produced by differen"
E17-1050,P07-2045,0,0.0101176,"Missing"
E17-1050,P11-2031,0,0.0803895,"jee et al., 2015a). TER and BLEU measure the similarity between the MT output and the corresponding references (in this case human post-edits) by looking at the word/n-gram overlaps. Precision is the ratio of the number of sentences an APE system improves (with respect to the MT output) over all the sentences it modifies.9 Higher precision indicates that the APE system is able to improve the quality of most of the sentences it changed. The statistical significance of BLEU results is computed using paired bootstrap resampling (Koehn, 2004). For TER, we use stratified approximate randomization (Clark et al., 2011). The Autodesk training and development sets consist of 12,238, and 1,948 segments respectively, while the WMT16 data contains 12,000, and 1,000 segments. Table 1 provides additional statistics of the source (mt#src) and target (pe) training sets, the repetition rate (RR) to measure the repetitiveness inside a text (Bertoldi et al., 2013), and the average TER score for both the data sets (computed between MT and PE), as an indicator of the original translation quality. Looking at these statistics, there are several indicators that suggest that the WMT16 corpus provides a more difficult scenari"
E17-1050,W04-3250,0,0.0385325,"et al., 2006), BLEU (Papineni et al., 2002), and precision (Chatterjee et al., 2015a). TER and BLEU measure the similarity between the MT output and the corresponding references (in this case human post-edits) by looking at the word/n-gram overlaps. Precision is the ratio of the number of sentences an APE system improves (with respect to the MT output) over all the sentences it modifies.9 Higher precision indicates that the APE system is able to improve the quality of most of the sentences it changed. The statistical significance of BLEU results is computed using paired bootstrap resampling (Koehn, 2004). For TER, we use stratified approximate randomization (Clark et al., 2011). The Autodesk training and development sets consist of 12,238, and 1,948 segments respectively, while the WMT16 data contains 12,000, and 1,000 segments. Table 1 provides additional statistics of the source (mt#src) and target (pe) training sets, the repetition rate (RR) to measure the repetitiveness inside a text (Bertoldi et al., 2013), and the average TER score for both the data sets (computed between MT and PE), as an indicator of the original translation quality. Looking at these statistics, there are several indi"
E17-1050,2007.mtsummit-wpt.4,0,0.0668051,"tion systems evaluated in the APE task in MDTE conditions. Thot (Ortiz-Martınez and Casacuberta, 2014), the online system used as term of comparison, shows in fact the inability to discern which of the learned correction rules is suitable for a specific context. In practice, all rules are created equal, for any given domain. 2 Related work Most of the previous works on APE cast the problem as a phrase-based statistical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been used also for the"
E17-1050,W13-2237,0,0.0184206,"s a phrase-based statistical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been used also for the APE task. To overcome this limitation, we proceed incrementally. First, we propose an approach based on an instance selection strategy, which learns local, sentence-specific APE models from small amounts of relevant data for each translation to be post-edited. Then, on top of it, we add an improved way to estimate the parameters of the sentence-specific APE models. To this aim, we exploit a dyna"
E17-1050,D15-1123,0,0.0152514,"test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been used also for the APE task. To overcome this limitation, we proceed incrementally. First, we propose an approach based on an instance selection strategy, which learns local, sentence-specific APE models from small amounts of relevant data for each translation to be post-edited. Then, on top of it, we add an improved way to estimate the parameters of the sentence-specific APE models. To this aim, we exploit a dynamic knowledge base that keeps track of global statistics computed over all the previously seen data (i.e. it d"
E17-1050,E14-2012,0,0.131789,"ll unexplored. This paper represents a first step along this direction: although a fullfledged evaluation centered on human translation in a computer-assisted translation (CAT) framework is out of our reach, we provide a proof of concept in which we simulate the MDTE scenario by running different APE solutions on a stream of data coming from two different domains. By analysing alternative solutions, we discuss the limitations not only of batch APE methods (insensitive to domain shifts), but also of state-of-the-art online translation systems evaluated in the APE task in MDTE conditions. Thot (Ortiz-Martınez and Casacuberta, 2014), the online system used as term of comparison, shows in fact the inability to discern which of the learned correction rules is suitable for a specific context. In practice, all rules are created equal, for any given domain. 2 Related work Most of the previous works on APE cast the problem as a phrase-based statistical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however"
E17-1050,W16-2379,0,0.286437,"Missing"
E17-1050,P02-1040,0,0.106258,"erages the source information to disambiguate post-editing rules and foster their application only in appropriate contexts (the matching condition is defined both on the source and on the target language). The joint representation is used as a source corpus to train all the APE systems compared in this paper and it is obtained by concatenating words in the source (src) and in the MT (mt) segments after aligning them with MGIZA++ (Gao and Vogel, 2008). Evaluation metrics. The performance of the different systems is evaluated in terms of Translation Error rate (TER) (Snover et al., 2006), BLEU (Papineni et al., 2002), and precision (Chatterjee et al., 2015a). TER and BLEU measure the similarity between the MT output and the corresponding references (in this case human post-edits) by looking at the word/n-gram overlaps. Precision is the ratio of the number of sentences an APE system improves (with respect to the MT output) over all the sentences it modifies.9 Higher precision indicates that the APE system is able to improve the quality of most of the sentences it changed. The statistical significance of BLEU results is computed using paired bootstrap resampling (Koehn, 2004). For TER, we use stratified app"
E17-1050,2012.eamt-1.34,0,0.0383369,"Missing"
E17-1050,2013.mtsummit-papers.24,0,0.9241,"istical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the two that have been used also for the APE task. To overcome this limitation, we proceed incrementally. First, we propose an approach based on an instance selection strategy, which learns local, sentence-specific APE models from small amounts of relevant data for each translation to be post-edited. Then, on top of it, we add an improved way to estimate the parameters of the sentence-specific APE models. To this aim, we exploit a dynamic knowledge base that k"
E17-1050,N07-1064,0,0.53178,"ut also of state-of-the-art online translation systems evaluated in the APE task in MDTE conditions. Thot (Ortiz-Martınez and Casacuberta, 2014), the online system used as term of comparison, shows in fact the inability to discern which of the learned correction rules is suitable for a specific context. In practice, all rules are created equal, for any given domain. 2 Related work Most of the previous works on APE cast the problem as a phrase-based statistical MT task1 and operate in a batch framework where systems are evaluated on static test sets that are homogeneous with the training data (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2016). These systems, however, are not able to leverage the feedback of the post-editors in an online translation scenario. The capability to evolve by learning from human feedback has been addressed by several online translation systems but mainly focusing on the MT task (Hardt and Elming, 2010; Bertoldi et al., 2013; Mathur et al., 2013; Simard and Foster, 2013; Ortiz-Martınez and Casacuberta, 2014; Denkowski et al., 2014; Wuebker et al., 2015). From these several online MT systems, we discuss the"
E17-1050,2006.amta-papers.25,0,0.0924559,"B´echara et al. (2011), leverages the source information to disambiguate post-editing rules and foster their application only in appropriate contexts (the matching condition is defined both on the source and on the target language). The joint representation is used as a source corpus to train all the APE systems compared in this paper and it is obtained by concatenating words in the source (src) and in the MT (mt) segments after aligning them with MGIZA++ (Gao and Vogel, 2008). Evaluation metrics. The performance of the different systems is evaluated in terms of Translation Error rate (TER) (Snover et al., 2006), BLEU (Papineni et al., 2002), and precision (Chatterjee et al., 2015a). TER and BLEU measure the similarity between the MT output and the corresponding references (in this case human post-edits) by looking at the word/n-gram overlaps. Precision is the ratio of the number of sentences an APE system improves (with respect to the MT output) over all the sentences it modifies.9 Higher precision indicates that the APE system is able to improve the quality of most of the sentences it changed. The statistical significance of BLEU results is computed using paired bootstrap resampling (Koehn, 2004)."
E17-2045,Q17-1024,0,0.0617671,"Missing"
E17-2045,W14-3363,0,0.0202907,"-Domain Machine Translation Multi-domain machine translation is very wellstudied in the field of statistical phrase-based MT. The approaches proposed for this issue vary from learning a single model from pooled training data, 280 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 280–284, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics to more complicated (log-)linear interpolations of multiple models using mixture models (Foster and Kuhn, 2007) and linear mixture models (Carpuat et al., 2014). However, being a very new field of research, to the best of our knowledge, there is no work on developing multi-domain NMT systems. However, to the best of our knowledge, there is still no work on developing multi-domain systems (i.e. generic/multi-purpose systems trained with all the data available at a given time) within the stateof-the-art NMT framework. Indeed, though interesting and well motivated from an applicationoriented perspective (e.g. think about a translation company looking for a generic MT backbone usable for jobs coming from any domain), this issue is still unexplored. The c"
E17-2045,N12-1047,0,0.0332362,"ra, after pre-processing. Experimental Setup 3.1 Segments 147.7K 238.4K 689.2K 163.2K 34.5K 38.4K 9.0K 40.3K 2.6M 1.7M 3.2 Phrase-based SMT The experiments of the phrase-based SMT systems are carried out using the open source Moses All these corpora are available in http://opus.lingfil.uu.se 281 4 toolkit (Koehn et al., 2007). The word alignment models are trained using fast-align (Dyer et al., 2013). In our experiments we used 5-gram language models trained with modified Kneser-Ney smoothing using KenLM toolkit (Heafield et al., 2013). The weights of the parameters are tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the development set. Development set is a combination of all the development corpora of all the domains. 3.3 Table 3 presents the results of the generic systems (PBMT gen. and NMT gen.) and the NMT system adapted to the concatenation of all the eight specific domains (NMT-adp.jnt), as well as the NMT systems which are specifically adapted to each domain separately (NMT-adp.sep). In the case of NMT-adp.jnt and NMT-adp.sep we used the best model of the NMT gen. and adapted it to their corresponding training corpora by continuing the training for several epochs, using the tra"
E17-2045,P07-2045,1,0.014309,"ice (OOffice), PHP, Ubuntu, and translated UN documents (UNTM).1 Since the size of these corpora are relatively small for training robust data-driven MT systems, 1 Tokens 3.1M 1.7M 10.8M 1.0M 389.0K 259.0K 47.7K 913.8K 57.8M 39.6M Table 1: Statistics of the English side of the original corpora, after pre-processing. Experimental Setup 3.1 Segments 147.7K 238.4K 689.2K 163.2K 34.5K 38.4K 9.0K 40.3K 2.6M 1.7M 3.2 Phrase-based SMT The experiments of the phrase-based SMT systems are carried out using the open source Moses All these corpora are available in http://opus.lingfil.uu.se 281 4 toolkit (Koehn et al., 2007). The word alignment models are trained using fast-align (Dyer et al., 2013). In our experiments we used 5-gram language models trained with modified Kneser-Ney smoothing using KenLM toolkit (Heafield et al., 2013). The weights of the parameters are tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the development set. Development set is a combination of all the development corpora of all the domains. 3.3 Table 3 presents the results of the generic systems (PBMT gen. and NMT gen.) and the NMT system adapted to the concatenation of all the eight specific domains (NMT-adp.jnt),"
E17-2045,N13-1073,0,0.0200347,"ze of these corpora are relatively small for training robust data-driven MT systems, 1 Tokens 3.1M 1.7M 10.8M 1.0M 389.0K 259.0K 47.7K 913.8K 57.8M 39.6M Table 1: Statistics of the English side of the original corpora, after pre-processing. Experimental Setup 3.1 Segments 147.7K 238.4K 689.2K 163.2K 34.5K 38.4K 9.0K 40.3K 2.6M 1.7M 3.2 Phrase-based SMT The experiments of the phrase-based SMT systems are carried out using the open source Moses All these corpora are available in http://opus.lingfil.uu.se 281 4 toolkit (Koehn et al., 2007). The word alignment models are trained using fast-align (Dyer et al., 2013). In our experiments we used 5-gram language models trained with modified Kneser-Ney smoothing using KenLM toolkit (Heafield et al., 2013). The weights of the parameters are tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the development set. Development set is a combination of all the development corpora of all the domains. 3.3 Table 3 presents the results of the generic systems (PBMT gen. and NMT gen.) and the NMT system adapted to the concatenation of all the eight specific domains (NMT-adp.jnt), as well as the NMT systems which are specifically adapted to each domain se"
E17-2045,2015.iwslt-evaluation.11,0,0.698635,"g them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings? 1 Introduction Neural machine translation systems have recently outperformed their conventional statistical counterparts in the translation tasks in several domains such as news (Sennrich et al., 2016a), UN documents (Junczys-Dowmunt et al., 2016), and spoken language data (Luong and Manning, 2015). One common pattern in all these cases is that the target domain is always predefined, hence it is feasible to perform domain adaptation techniques in order to boost system performance for that particular application. However, in real-world applications it is very hard, if not impossible, to develop and maintain several specific MT systems for multiple domains. This is mostly due to the fact that usually: i) the target domain is not known in advance, and users might query different sen2 Multi-Domain Machine Translation Multi-domain machine translation is very wellstudied in the field of stati"
E17-2045,P16-5005,0,0.0509751,"Missing"
E17-2045,P02-1040,0,0.0980109,"to increase the consistency in segmenting the source and target text, the source and target side of the training set are combined and number of merge rules is set to 89,500, resulting in vocabularies of size 78K and 86K tokens for English and French languages, respectively. We use mini-batches of size 100, word embeddings of size 500, and hidden layers of size 1024. The maximum sentence length is set to 50 in our experiments. The models are trained using Adagrad (Duchi et al., 2011), reshuffling the training corpora for each epoch. The models are evaluated every 10,000 mini-batches via BLEU (Papineni et al., 2002). It is worth mentioning that with the same set-up we recently achieved state-of-theart performance in the International Workshop on Spoken Language Translation evaluation (Farajian et al., 2016). 2 Analysis and Discussion NMT vs. PBMT in Multi-domain scenario As the results show, the generic PBMT system outperforms its NMT counterpart in all the domains by a very large margin; and as the NMT system becomes more specific by observing more domain-specific data, the gap between the performances reduces until the NMT outperforms; which confirms the results of the previous works in this field (Luo"
E17-2045,N16-1101,0,0.0237093,"for jobs coming from any domain), this issue is still unexplored. The current state-of-theart research in NMT explored the effectiveness of domain adaptation, and the approaches for how to adapt existing NMT systems to a new domain (Luong and Manning, 2015). The assumption of these works, however, is that the new target domains are either known in advance or presented together after some sample data have been made available to fine-tune the system. There exist an active field of research that is trying to solve a quite different issue that has a similar motivation, which is multi-lingual NMT (Firat et al., 2016a; Firat et al., 2016b; Johnson et al., 2016). The motivations behind these works are very similar to the ones described in Section 1, which is mostly simplifying the deployment of MT engines in the production lines. So, the final goal is to reduce the number of final systems, trained with pooled multi-domain data sets, without degrading the final performance. As we will see in the remainder of this paper, this issue is still open, especially when we embrace the state-of-the-art NMT paradigm. 3 ECB Gnome JRC KDE4 OOffice PHP Ubuntu UN-TM CommonCrawl Europarl ECB Gnome JRC KDE4 OOffice PHP Ubun"
E17-2045,D16-1026,0,0.0380055,"Missing"
E17-2045,P16-1162,0,0.877463,"f a generic NMT system and phrase-based statistical machine translation (PBMT) system by training them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings? 1 Introduction Neural machine translation systems have recently outperformed their conventional statistical counterparts in the translation tasks in several domains such as news (Sennrich et al., 2016a), UN documents (Junczys-Dowmunt et al., 2016), and spoken language data (Luong and Manning, 2015). One common pattern in all these cases is that the target domain is always predefined, hence it is feasible to perform domain adaptation techniques in order to boost system performance for that particular application. However, in real-world applications it is very hard, if not impossible, to develop and maintain several specific MT systems for multiple domains. This is mostly due to the fact that usually: i) the target domain is not known in advance, and users might query different sen2 Multi-Do"
E17-2045,W07-0717,0,0.0623347,"dvance, and users might query different sen2 Multi-Domain Machine Translation Multi-domain machine translation is very wellstudied in the field of statistical phrase-based MT. The approaches proposed for this issue vary from learning a single model from pooled training data, 280 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 280–284, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics to more complicated (log-)linear interpolations of multiple models using mixture models (Foster and Kuhn, 2007) and linear mixture models (Carpuat et al., 2014). However, being a very new field of research, to the best of our knowledge, there is no work on developing multi-domain NMT systems. However, to the best of our knowledge, there is still no work on developing multi-domain systems (i.e. generic/multi-purpose systems trained with all the data available at a given time) within the stateof-the-art NMT framework. Indeed, though interesting and well motivated from an applicationoriented perspective (e.g. think about a translation company looking for a generic MT backbone usable for jobs coming from a"
kouylekov-etal-2010-mining,D07-1073,0,\N,Missing
kouylekov-etal-2010-mining,C02-1167,0,\N,Missing
kouylekov-etal-2010-mining,E09-1064,0,\N,Missing
kouylekov-etal-2010-mining,W07-1401,0,\N,Missing
kouylekov-etal-2010-mining,P98-2127,0,\N,Missing
kouylekov-etal-2010-mining,C98-2122,0,\N,Missing
kouylekov-etal-2010-mining,kouylekov-magnini-2006-building,1,\N,Missing
kouylekov-etal-2010-mining,W04-3205,0,\N,Missing
L18-1004,2011.mtsummit-papers.35,0,0.440459,"Missing"
L18-1004,P15-2026,1,0.953279,"ting, Machine Translation 1. Introduction inal MT output that has been left untouched (i.e. raw, non post-edited translations). Automatic post-editing (APE) for machine translation (MT) aims to fix recurrent errors made by the MT decoder by learning from correction examples. As a post-processing step, APE has several possible applications, especially in black-box scenarios (e.g. when working with a third-party translation engine) in which the MT system is used “as is” and is not directly accessible for retraining or for more radical internal modifications. In such scenarios, as pointed out by Chatterjee et al. (2015), APE systems can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing a deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) post-editing effort, and iii) adapt the output of a general-purpose MT system to the lexicon/style requested in a specific application domain. The training of APE systems usually relies on data sets comprising (source, MT, human post-edit) triplets, in which the source sentence in a given language has been automatically tran"
L18-1004,W17-4773,1,0.903438,"lthough the general monolingual translation approach to the problem is still the same, over the years the proposed solutions evolved in several ways, first by refining the decoding approach and then, in the last couple of years, by radically changing the core APE technology. Decoding refinements successfully explored, for instance, the integration of source information for enhanced (joint, context-aware) input representation, either in the standard phrase-based MT (PBMT) framework (B´echara et al., 2011) or in more elegant batch factored models(Chatterjee et al., 2016) and online PBMT models (Chatterjee et al., 2017b). More recently, radical paradigm changes followed the “neural revolution” witnessed in the MT field. The current state of the art is indeed represented by single/multisource neural APE systems, the former relying on the loglinear combination of monolingual and bilingual models (Junczys-Dowmunt and Grundkiewicz, 2016), and the latter learning from source and target information in a joint fashion (Chatterjee et al., 2017a). Recent works addressed the problem by also integrating external information such as word-level quality estimation scores (Chatterjee et al., 2017c) as a way to guide neura"
L18-1004,E17-1050,1,0.938153,"lthough the general monolingual translation approach to the problem is still the same, over the years the proposed solutions evolved in several ways, first by refining the decoding approach and then, in the last couple of years, by radically changing the core APE technology. Decoding refinements successfully explored, for instance, the integration of source information for enhanced (joint, context-aware) input representation, either in the standard phrase-based MT (PBMT) framework (B´echara et al., 2011) or in more elegant batch factored models(Chatterjee et al., 2016) and online PBMT models (Chatterjee et al., 2017b). More recently, radical paradigm changes followed the “neural revolution” witnessed in the MT field. The current state of the art is indeed represented by single/multisource neural APE systems, the former relying on the loglinear combination of monolingual and bilingual models (Junczys-Dowmunt and Grundkiewicz, 2016), and the latter learning from source and target information in a joint fashion (Chatterjee et al., 2017a). Recent works addressed the problem by also integrating external information such as word-level quality estimation scores (Chatterjee et al., 2017c) as a way to guide neura"
L18-1004,W17-4716,1,0.890869,"lthough the general monolingual translation approach to the problem is still the same, over the years the proposed solutions evolved in several ways, first by refining the decoding approach and then, in the last couple of years, by radically changing the core APE technology. Decoding refinements successfully explored, for instance, the integration of source information for enhanced (joint, context-aware) input representation, either in the standard phrase-based MT (PBMT) framework (B´echara et al., 2011) or in more elegant batch factored models(Chatterjee et al., 2016) and online PBMT models (Chatterjee et al., 2017b). More recently, radical paradigm changes followed the “neural revolution” witnessed in the MT field. The current state of the art is indeed represented by single/multisource neural APE systems, the former relying on the loglinear combination of monolingual and bilingual models (Junczys-Dowmunt and Grundkiewicz, 2016), and the latter learning from source and target information in a joint fashion (Chatterjee et al., 2017a). Recent works addressed the problem by also integrating external information such as word-level quality estimation scores (Chatterjee et al., 2017c) as a way to guide neura"
L18-1004,P16-5005,0,0.021919,"essional translators. The combination of domain specificity and higher post-editing quality resulted in significant gains over the baseline. Related Work: Existing APE Corpora The growing interest towards APE has to confront with the hard truth of data scarcity. Although nowadays post-edited data are a clear by-product of industrial translation workflows, the largest part of the daily work done by professional translators focuses on proprietary or copyright data that cannot be released. Though present in the industrial sector (as confirmed by recent works coming from big players like SYSTRAN (Crego et al., 2016) or eBay (Mathur et al., 2017)), APE technology is still more a matter of inhouse development rather than a framework motivating free data sharing. The few existing corpora that are usable for APE research can be classified into two types: i) the aforementioned “gold” data sets made of (source, MT, human post-edit) triplets, and ii) the synthetic ones, to which our eSCAPE corpus belongs, in which some elements of the triplets derive from automatic translation. The remainder of this section provides an inventory of the existing APE corpora. As also shown in Table 1, the global picture is quite"
L18-1004,E17-2045,1,0.847727,"les Gnome Ubuntu KDE4 OpenOffice PHP TOTAL Domain LEGAL LEGAL MIXED LEGAL NEWS MIXED MEDIC. MEDIC. MEDIC. IT IT IT IT IT En-De 1,920,209 11,317 2,399,123 719,372 242,770 143,836 1,108,752 1,848,303 10,406 28,439 13,245 224,035 42,391 39,707 En-It 1,909,115 193,047 810,979 40,009 181,874 1,081,134 319,141 21,014 175,058 35,538 8,853,762 4,128,128 memory, storing external user translation memories (TMs). When ModernMT receives a translation query, it quickly analyses its context, recalls from its memory the most related translation examples, and instantly adapts its neural network to the query (Farajian et al., 2017). Training and test of the neural models were run on one GPU (NVIDIA Tesla K80) for around three weeks. Final performance is 38.17 BLEU points for English–German and 41.01 for English–Italian. To give the possibility for experiments on domainadaptation for APE, each eSCAPE triplet is associated to a label indicating the name of the corpus from which the original (source, reference) pair was extracted. 4. To test the usefulness of the eSCAPE corpus, we run APE experiments for both the language pairs covered by the data set. En-De and En-It data were first tokenised and then split into dev (2,00"
L18-1004,W17-4775,0,0.0615828,"ssed the problem by also integrating external information such as word-level quality estimation scores (Chatterjee et al., 2017c) as a way to guide neural APE decoding towards better corrections. Unsurprisingly, the impressive gains achieved by the neu24 2.1. ral solutions come at the cost of a much higher data demand compared to the PBMT methods. To overcome this problem, the latest published results on neural APE have been obtained by exploiting synthetically-created data during training (Junczys-Dowmunt and Grundkiewicz, 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Variˇs and Bojar, 2017; Hokamp, 2017; Chatterjee et al., 2017a). These trends, which emerged after three rounds of the APE task organised within the Conference on Machine Translation (WMT) (Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017), clearly indicate that: i) information from the source text is definitely useful to train reliable APE models, and ii) the limited availability of “gold” training corpora made of (source, MT, human post-edit) triplets calls for workarounds to unleash the full potential of state-of-the art but data-demanding neural systems. The eSCAPE corpus presented in this paper meets such demand b"
L18-1004,W16-2378,0,0.357143,"ully explored, for instance, the integration of source information for enhanced (joint, context-aware) input representation, either in the standard phrase-based MT (PBMT) framework (B´echara et al., 2011) or in more elegant batch factored models(Chatterjee et al., 2016) and online PBMT models (Chatterjee et al., 2017b). More recently, radical paradigm changes followed the “neural revolution” witnessed in the MT field. The current state of the art is indeed represented by single/multisource neural APE systems, the former relying on the loglinear combination of monolingual and bilingual models (Junczys-Dowmunt and Grundkiewicz, 2016), and the latter learning from source and target information in a joint fashion (Chatterjee et al., 2017a). Recent works addressed the problem by also integrating external information such as word-level quality estimation scores (Chatterjee et al., 2017c) as a way to guide neural APE decoding towards better corrections. Unsurprisingly, the impressive gains achieved by the neu24 2.1. ral solutions come at the cost of a much higher data demand compared to the PBMT methods. To overcome this problem, the latest published results on neural APE have been obtained by exploiting synthetically-created"
L18-1004,I17-1013,0,0.0310568,"n a joint fashion (Chatterjee et al., 2017a). Recent works addressed the problem by also integrating external information such as word-level quality estimation scores (Chatterjee et al., 2017c) as a way to guide neural APE decoding towards better corrections. Unsurprisingly, the impressive gains achieved by the neu24 2.1. ral solutions come at the cost of a much higher data demand compared to the PBMT methods. To overcome this problem, the latest published results on neural APE have been obtained by exploiting synthetically-created data during training (Junczys-Dowmunt and Grundkiewicz, 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Variˇs and Bojar, 2017; Hokamp, 2017; Chatterjee et al., 2017a). These trends, which emerged after three rounds of the APE task organised within the Conference on Machine Translation (WMT) (Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017), clearly indicate that: i) information from the source text is definitely useful to train reliable APE models, and ii) the limited availability of “gold” training corpora made of (source, MT, human post-edit) triplets calls for workarounds to unleash the full potential of state-of-the art but data-demanding neural systems. The eSCAPE corpus prese"
L18-1004,W04-3250,0,0.226557,"ificantly outperform baseline results in both language directions, independently from the MT technology underlying the data generation process. The work reported in this paper is the initial step of a more ambitious roadmap aimed to extend the resource with more data covering a larger spectrum of domains and language combinations. The current version of eSCAPE can be freely downloaded from: http://hltshare.fbk. eu/QT21/eSCAPE.html. 38.08 39.80 41.01 42.15 Table 3: Neural APE results (BLEU score improvements are statistically significant with p &lt; 0.05 computed with paired bootstrap resampling (Koehn, 2004)). and tested on artificial data built from phrase-based models (+1.39 on En–De, +1.72 on En–It), and when training and test are performed on artificial data derived from neural models (+1.04 on En–De, +1.14 on En–It).12 The observed gains vary for the two language pairs (with highest results on En–It) and depending on the type of data used. Concerning this latter aspect, the higher quality of neural MT output results in lower gains on both language settings. This confirms previous outcomes from the WMT APE task: the higher the baseline (i.e. the BLEU score of the raw MT output), the lower the"
L18-1004,2005.mtsummit-papers.11,0,0.0938754,"iments and the reuse of the selected triplets, the authors released the scripts used for data extraction.2 Another useful resource is described in (Potet et al., 2012).3 It consists of 10,881 triplets in which a French source sentence taken from several news corpora is translated into English by a PBMT system. Post-edits were collected using Amazon Mechanical Turk following strict control reviewing procedures to guarantee correction quality. Two smaller corpora are respectively described in (Specia et al., 2010) and (Specia, 2011). The former consists of 4,000 English sentences from Europarl (Koehn, 2005), which were translated into Spanish by a PBMT system and manually post-edited by professional translators. The latter, which covers the news domain, includes 2,525 French–English PBMT translations and 1,000 English– Spanish translations with professional post-edits. Other useful data have been released by the organisers of the WMT APE task. The first round of the task (Bojar et al., 2015) presented participants with around 12,000 English– Spanish training data drawn from the news domain, with translations derived from a PBMT system. A peculiarity of this corpus is that post-edits were collect"
L18-1004,W17-3525,0,0.0134074,"bination of domain specificity and higher post-editing quality resulted in significant gains over the baseline. Related Work: Existing APE Corpora The growing interest towards APE has to confront with the hard truth of data scarcity. Although nowadays post-edited data are a clear by-product of industrial translation workflows, the largest part of the daily work done by professional translators focuses on proprietary or copyright data that cannot be released. Though present in the industrial sector (as confirmed by recent works coming from big players like SYSTRAN (Crego et al., 2016) or eBay (Mathur et al., 2017)), APE technology is still more a matter of inhouse development rather than a framework motivating free data sharing. The few existing corpora that are usable for APE research can be classified into two types: i) the aforementioned “gold” data sets made of (source, MT, human post-edit) triplets, and ii) the synthetic ones, to which our eSCAPE corpus belongs, in which some elements of the triplets derive from automatic translation. The remainder of this section provides an inventory of the existing APE corpora. As also shown in Table 1, the global picture is quite fragmented, with domain-specif"
L18-1004,W07-0704,0,0.0928191,". In both cases, translations were produced by a customised PBMT system and post-edited by professional translators. 2.2. Synthetic corpora The use of synthetic resources aims to overcome the aforementioned problem of “gold” data scarcity with approximate solutions. This can be done in different ways. Several previous works have shown the viability of mimicking the ideal scenario in which the training triplets include actual human post-edits of machine-translated text by learning, instead, from the weaker connection between the MT output and external references. Though with variable margins, (Oflazer and El-Kahlout, 2007; B´echara et al., 2011; Rubino et al., 2012) report translation quality improvements in the PBMT scenario with post-editing components trained on (source, MT, reference) triplets. To the best of our knowledge, though potentially useful to APE research, none of such previous works released reusable datasets. When moving to the data-demanding neural framework, data scarcity becomes a major problem that definitely calls for the external support of artificial corpora that are orders of magnitude larger than the current training sets. 3. The eSCAPE corpus7 consists in two datasets (En-De and En-It"
L18-1004,P02-1040,0,0.105377,"to the lexicon/style requested in a specific application domain. The training of APE systems usually relies on data sets comprising (source, MT, human post-edit) triplets, in which the source sentence in a given language has been automatically translated to produce the MT element that, in turn, has been manually corrected to produce the human post-edit. In this supervised learning setting, the goal is to learn from the training data (and possibly generalise) the appropriate corrections of systematic errors made by the MT system, and apply them at test stage on unseen (source, MT) pairs. BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) computed against reference human post-edits are the standard evaluation metrics for the task. Their respective improvements and reductions are usually compared against the baseline scores obtained by the origEarly works on this problem date back to (Allen and Hogan, 2000; Simard et al., 2007), which addressed the problem as a “monolingual translation” task in which raw MT output in the target language has to be translated, in the same language, into a fluent and adequate translation of the original source text. Although the general monolingual translation approac"
L18-1004,potet-etal-2012-collection,0,0.02422,"Missing"
L18-1004,2012.eamt-1.55,0,0.0431765,"Missing"
L18-1004,W16-2323,0,0.0653322,"Missing"
L18-1004,E17-3017,0,0.0730758,"Missing"
L18-1004,N07-1064,0,0.194401,"produce the human post-edit. In this supervised learning setting, the goal is to learn from the training data (and possibly generalise) the appropriate corrections of systematic errors made by the MT system, and apply them at test stage on unseen (source, MT) pairs. BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) computed against reference human post-edits are the standard evaluation metrics for the task. Their respective improvements and reductions are usually compared against the baseline scores obtained by the origEarly works on this problem date back to (Allen and Hogan, 2000; Simard et al., 2007), which addressed the problem as a “monolingual translation” task in which raw MT output in the target language has to be translated, in the same language, into a fluent and adequate translation of the original source text. Although the general monolingual translation approach to the problem is still the same, over the years the proposed solutions evolved in several ways, first by refining the decoding approach and then, in the last couple of years, by radically changing the core APE technology. Decoding refinements successfully explored, for instance, the integration of source information for"
L18-1004,2006.amta-papers.25,0,0.514833,"n a specific application domain. The training of APE systems usually relies on data sets comprising (source, MT, human post-edit) triplets, in which the source sentence in a given language has been automatically translated to produce the MT element that, in turn, has been manually corrected to produce the human post-edit. In this supervised learning setting, the goal is to learn from the training data (and possibly generalise) the appropriate corrections of systematic errors made by the MT system, and apply them at test stage on unseen (source, MT) pairs. BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) computed against reference human post-edits are the standard evaluation metrics for the task. Their respective improvements and reductions are usually compared against the baseline scores obtained by the origEarly works on this problem date back to (Allen and Hogan, 2000; Simard et al., 2007), which addressed the problem as a “monolingual translation” task in which raw MT output in the target language has to be translated, in the same language, into a fluent and adequate translation of the original source text. Although the general monolingual translation approach to the problem is still the"
L18-1004,specia-etal-2010-dataset,0,0.163984,"human post-edit) triplets that share the same English source. To ease the replicability of their experiments and the reuse of the selected triplets, the authors released the scripts used for data extraction.2 Another useful resource is described in (Potet et al., 2012).3 It consists of 10,881 triplets in which a French source sentence taken from several news corpora is translated into English by a PBMT system. Post-edits were collected using Amazon Mechanical Turk following strict control reviewing procedures to guarantee correction quality. Two smaller corpora are respectively described in (Specia et al., 2010) and (Specia, 2011). The former consists of 4,000 English sentences from Europarl (Koehn, 2005), which were translated into Spanish by a PBMT system and manually post-edited by professional translators. The latter, which covers the news domain, includes 2,525 French–English PBMT translations and 1,000 English– Spanish translations with professional post-edits. Other useful data have been released by the organisers of the WMT APE task. The first round of the task (Bojar et al., 2015) presented participants with around 12,000 English– Spanish training data drawn from the news domain, with transl"
L18-1004,2011.eamt-1.12,0,0.125555,"that share the same English source. To ease the replicability of their experiments and the reuse of the selected triplets, the authors released the scripts used for data extraction.2 Another useful resource is described in (Potet et al., 2012).3 It consists of 10,881 triplets in which a French source sentence taken from several news corpora is translated into English by a PBMT system. Post-edits were collected using Amazon Mechanical Turk following strict control reviewing procedures to guarantee correction quality. Two smaller corpora are respectively described in (Specia et al., 2010) and (Specia, 2011). The former consists of 4,000 English sentences from Europarl (Koehn, 2005), which were translated into Spanish by a PBMT system and manually post-edited by professional translators. The latter, which covers the news domain, includes 2,525 French–English PBMT translations and 1,000 English– Spanish translations with professional post-edits. Other useful data have been released by the organisers of the WMT APE task. The first round of the task (Bojar et al., 2015) presented participants with around 12,000 English– Spanish training data drawn from the news domain, with translations derived from"
L18-1004,W17-4777,0,0.183411,"Missing"
L18-1004,2012.amta-wptp.10,0,0.0454916,"Missing"
L18-1004,W16-2377,1,\N,Missing
L18-1004,W16-2301,1,\N,Missing
magnini-etal-2002-towards,W01-1204,0,\N,Missing
N10-1045,P06-1114,0,0.0515693,"ed as a generic framework for modeling language variability. Given two texts T and H, the task consists in deciding if the meaning of H can be inferred from the meaning of T. So far, TE has been only applied in a monolingual setting, where both texts are assumed to be written in the same language. In this work, we propose and investigate a cross-lingual extension of TE, where we assume that T and H are written in different languages. The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al., 2006), information extraction (Romano et al., 2006), and document summarization (Lloret et al., 2008). To the best of our knowledge, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. As a matter of fact, despite the great deal of attention that TE has received in recent years (also witnessed by five editions of the Recognizing Textual Entailment Challenge1 ), interest for cross-lingual extensions has not been in the mainstream of TE research, w"
N10-1045,P07-2045,1,0.00784892,"Missing"
N10-1045,P09-2073,1,0.304023,"tance Textual Entailment Suite). This system is an open source software package based on edit distance algorithms, which computes the T-H distance as the cost of the edit operations (i.e. insertion, deletion and substitution) that are necessary to transform T into H. By defining the edit distance algorithm and a cost scheme (i.e. which defines the costs of each edit operation), this package is able to learn a distance model over a set of training pairs, which is used to decide if an entailment relation holds over each test pair. In order to obtain a monolingual TE model, we trained and tuned (Mehdad, 2009) our model on the RTE3 test set, to reduce the overfitting bias, since 4 http://translate.google.com http://www.statmt.org/moses/ 6 http://www.statmt.org/europarl/ 7 http://www.ldc.upenn.edu 8 http://edits.fbk.eu/ our original data was created over the RTE3 development set. Moreover, we used a set of lexical entailment rules extracted from Wikipedia and WordNet, as described in (Mehdad et al., 2009). To begin with, we used this model to classify the created cross-lingual entailment corpus in three different settings: 1) hypotheses translated by Google, 2) hypotheses translated by Moses (1st be"
N10-1045,W09-0404,0,0.0583924,"Missing"
N10-1045,E06-1052,0,0.0313124,"sts in deciding if the meaning of H can be inferred from the meaning of T. So far, TE has been only applied in a monolingual setting, where both texts are assumed to be written in the same language. In this work, we propose and investigate a cross-lingual extension of TE, where we assume that T and H are written in different languages. The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al., 2006), information extraction (Romano et al., 2006), and document summarization (Lloret et al., 2008). To the best of our knowledge, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. As a matter of fact, despite the great deal of attention that TE has received in recent years (also witnessed by five editions of the Recognizing Textual Entailment Challenge1 ), interest for cross-lingual extensions has not been in the mainstream of TE research, which until now has been mainly focused on the English language. Nevertheless, the strong inter"
N15-1073,2013.iwslt-evaluation.1,0,0.230623,"Missing"
N15-1073,P13-1004,0,0.0374886,"Missing"
N15-1073,2007.mtsummit-papers.30,0,0.124359,"Missing"
N15-1073,W12-3122,1,0.80325,"Missing"
N15-1073,C14-1171,1,0.464342,"Missing"
N15-1073,2009.eamt-1.5,1,0.904071,"Missing"
N15-1073,P14-1067,1,0.908599,"Missing"
N15-1073,C14-1040,1,\N,Missing
N15-1073,W14-3340,1,\N,Missing
N19-1202,N19-1006,0,0.601701,"Missing"
N19-1202,C10-2010,0,0.0464972,"Missing"
N19-1202,N18-1008,0,0.232838,"Missing"
N19-1202,L18-1001,0,0.161824,"large size, speaker variety (male/female, native/non-native) and coverage in terms of topics and languages. To achieve these objectives, similar to (Niehues et al., 2018), we started from English TED Talks, in which a variety of speakers discuss topics spanning from business to science and entertainment. Most importantly, the fact that TED talks are often manually transcribed and translated sets ideal conditions for creating an SLT corpus from high-quality text material. Although the initial data are similar to those used to build the IWSLT18 corpus, our methodology is different. Inspired by Kocabiyikoglu et al. (2018), it exploits automatic alignment procedures, first at the text level (between transcriptions and translations) and then with the corresponding audio segments. More in detail, for each target language Li , the (English-Li ) section of MuST-C is created as follows. First, for all the English talks available from the TED website,3 we download the videos and the HTML files containing the manual transcriptions and their translation into Li .4 Then, the plain text transcription and the translation of each talk are split at the sentence level based on strong punctuation marks and aligned using the G"
N19-1202,D15-1166,0,0.0171243,"ture releases of the corpus. 2013 4 Tgt De Es Fr It Nl Pt Ro Ru #Talk 2,093 2,564 2,510 2,374 2,267 2,050 2,216 2,498 #Sent 234K 270K 280K 258K 253K 211K 240K 270K Hours 408 504 492 465 442 385 432 489 src w 4.3M 5.3M 5.2M 4.9M 4.7M 4.0M 4.6M 5.1M tgt w 4.0M 5.1M 5.4M 4.6M 4.3M 3.8M 4.3M 4.3M tional layers that reduce the sequence length. The output of the convolutions is then processed by three stacked LSTMs (Hochreiter and Schmidhuber, 1997). The decoder consists of a two-layered deep transition (Pascanu et al., 2014) LSTM with an attention network based on the general soft attention score (Luong et al., 2015). The final output of the decoder is a function of the concatenation of the LSTM output, the context vector and the previous-character embedding. Table 2: Statistics for each section of MuST-C. aligned audio using the XNMT tool (Neubig et al., 2018).7 Table 2 provides basic statistics for the 8 sections of the MuST-C corpus. Comparing the 4th column with the numbers reported in Table 1, it is worth noting that, in terms of hours of transcribed/translated speech, each section is larger than any existing publicly available SLT resource. 3 Experiments In this section we present two sets of experi"
N19-1202,W18-1818,0,0.0651267,"Missing"
N19-1202,P02-1040,0,0.107942,"on Metrics In our experiments, texts are tokenized and punctuation is normalized. Furthermore, the English texts are lowercased, while the target language texts are split into characters still preserving the word boundaries. For MT, we segment the English words with the BPE algorithm (Sennrich et al., 2015) using a maximum of 30K merge operations. The output generation of all models is performed using beam search with a beam size of 5. ASR performance is measured with word error rate (WER) computed on lower-cased, tokenized texts without punctuation. MT and SLT results are computed with BLEU (Papineni et al., 2002). 3.3 Experiment 1: Corpus Quality As observed in Section 2, each section of MuSTC is larger than any other existing publicly available SLT corpus. The usefulness of a resource, however, is not only a matter of size but also of quality (in this case, the quality of the audio-transcription-translation alignments). For an empirical verification of this aspect, we experimented with two comparable datasets. One is 8 github.com/neulab/xnmt 2014 www.modernmt.eu the TED-derived English-German IWSLT18 corpus (Niehues et al., 2018), which is built following a pipeline that performs segment extraction a"
N19-1202,2013.iwslt-papers.14,0,0.532837,"Missing"
N19-1202,shimizu-etal-2014-collection,0,0.0820902,"Missing"
N19-1202,stuker-etal-2012-kit,0,0.127078,"Missing"
N19-1202,W17-4608,0,0.0512685,"Missing"
negri-etal-2012-chinese,P02-1040,0,\N,Missing
negri-etal-2012-chinese,D11-1062,1,\N,Missing
negri-etal-2012-chinese,P01-1008,0,\N,Missing
negri-etal-2012-chinese,P08-1077,0,\N,Missing
negri-etal-2012-chinese,P11-1134,1,\N,Missing
negri-etal-2012-chinese,P08-1004,0,\N,Missing
negri-etal-2012-chinese,N06-1058,0,\N,Missing
negri-etal-2012-chinese,P05-1074,0,\N,Missing
negri-etal-2012-chinese,N06-1003,0,\N,Missing
negri-etal-2012-chinese,N10-1045,1,\N,Missing
negri-etal-2012-chinese,P11-1020,0,\N,Missing
negri-etal-2012-chinese,P02-1006,0,\N,Missing
negri-etal-2012-chinese,W10-0734,1,\N,Missing
negri-etal-2012-chinese,W04-3206,0,\N,Missing
P02-1054,breck-etal-2000-evaluate,0,0.030976,"Missing"
P02-1054,J93-1003,0,0.0542797,"Missing"
P02-1054,W01-1204,0,0.0395133,"Missing"
P02-1054,W01-1205,0,0.0377541,"Missing"
P10-4008,W07-1407,0,0.0369015,"orpus as input, the test procedure produces a file containing for each pair: i) the decision of the system (YES, NO), ii) the confidence of the decision, iii) the entailment score, iv) the sequence of edit operations made to calculate the entailment score. 4.4 scorecombination = n ! i=0 scorei ∗ weighti (1) In this formula, weighti is an ad-hoc weight parameter for each entailment engine. Optimal weight parameters can be determined using the same optimization strategy used to optimize the cost schemes, as described in Section 3.3. Classifier Combination is similar to the approach proposed in (Malakasiotis and Androutsopoulos, 2007), and is based on using the entailment scores returned by each engine as features to train a classifier (see Figure 4). To this aim, EDITS provides a plug-in that uses the Weka8 machine learning workbench as a core. By default the plug-in uses an SVM classifier, but other Weka algorithms can be specified as options in the configuration file. The following configuration file describes a combination of two engines (i.e. one based on Tree Edit Distance, the other based on Cosine Similarity), used to train a classifier with Weka9 . Combining Engines A relevant feature of EDITS is the possibility t"
P10-4008,P09-2073,0,0.0267192,"or other information to be used inside the cost scheme. 3.3 Figure 3: Example of XML Rule Repository 4 Using the System This section provides basic information about the use of EDITS, which can be run with commands in a Unix Shell. A complete guide to all the parameters of the main script is available as HTML documentation downloadable with the package. Cost Optimizer A cost optimizer is used to adapt cost schemes (either those provided with the system, or new ones defined by the user) to specific datasets. The optimizer is based on cost adaptation through genetic algorithms, as proposed in (Mehdad, 2009). To this aim, cost schemes can be parametrized by externalizing as parameters the edit operations costs. The optimizer iterates over training data using different values of these parameters until on optimal set is found (i.e. the one that best performs on the training set). 3.4 4.1 Input The input of the system is an entailment corpus represented in the EDITS Text Annotation Format (ETAF), a simple XML internal annotation format. ETAF is used to represent both the input T-H pairs, and the entailment and contradiction rules. ETAF allows to represent texts at two different levels: i) as sequenc"
P10-4008,R09-1056,1,0.850347,"Missing"
P11-1134,P98-1013,0,0.0179973,"g relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts a"
P11-1134,P05-1074,0,0.237919,"een texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works (Bannard and Callison-Burch, 2005; Zhao et al., 2009; Kouylekov et al., 2009) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words. Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources. As regards the first issue, it’s worth noting that in the monolingual scenario simple “bag of words” (or “bag of ngrams”) approaches are per se sufficient to achieve"
P11-1134,W04-3205,0,0.0289879,"chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the s"
P11-1134,N10-1031,0,0.0124482,"hrase table, and extract their equivalents in l1 ; 3. use the paraphrase table in l1 to find paraphrases of the extracted fragments in l1 ; phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. 4. map such paraphrases to phrases in T. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using t"
P11-1134,E09-1025,0,0.0260215,"alents in l1 ; 3. use the paraphrase table in l1 to find paraphrases of the extracted fragments in l1 ; phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. 4. map such paraphrases to phrases in T. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined."
P11-1134,N03-1017,0,0.0166869,"es. In particular, “YES” and “NO” judgements are assigned considering the proportion of words in the hypothesis that are found also in the text. This way to approximate entailment reflects the intuition that, as a directional relation between the text and the hypothesis, the full content of H has to be found in T. 3.1 Extracting Phrase and Paraphrase Tables Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses"
P11-1134,P07-2045,1,0.00976982,"re several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 3.2 In order to maximize the usage of lexical knowledge, our entailment decision criterion is based on similarity s"
P11-1134,kouylekov-etal-2010-mining,1,0.859106,"Missing"
P11-1134,N10-1146,1,0.754403,"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources 1336 Marcello Federico FBK - irst Povo (Trento), Italy federico@fbk.eu (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic"
P11-1134,N10-1045,1,0.567905,"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources 1336 Marcello Federico FBK - irst Povo (Trento), Italy federico@fbk.eu (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic"
P11-1134,C02-1167,0,0.0845279,"Missing"
P11-1134,W10-0734,1,0.473862,"s obtained at each n-gram level, and optimize their relative weights, we trained a Support Vector Machine classifier, SVMlight (Joachims, 1999), using each score as a feature. Scoren = 4 Experiments on CLTE To address the first two questions outlined in Section 1, we experimented with the phrase matching method previously described, contrasting the effectiveness of lexical information extracted from parallel corpora with the knowledge provided by other resources used in the same way. 4.1 Dataset the CrowdFlower3 channel to Amazon Mechanical Turk4 (MTurk), adopting the methodology proposed by (Negri and Mehdad, 2010). The method relies on translation-validation cycles, defined as separate jobs routed to MTurk’s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untru"
P11-1134,J03-1002,0,0.00279614,"ciation probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities"
P11-1134,H05-1047,0,0.00878721,"available knowledge sources. Sections 3 and 4 address the first three questions, giving motivations for the use of bilingual parallel corpora in CLTE, and showing the results of our experiments. Section 5 addresses the last question, reporting on our experiments with paraphrase tables extracted from phrase tables on the monolingual RTE datasets. Section 6 concludes the paper, and outlines the directions of our future research. 2 Lexical resources for TE and CLTE All current approaches to monolingual TE, either syntactically oriented (Rus et al., 2005), or applying logical inference (Tatu and Moldovan, 2005), or adopting transformation-based techniques (Kouleykov and Magnini, 2005; Bar-Haim et al., 2008), incorporate different types of lexical knowledge to support textual inference. Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each"
P11-1134,W09-0441,0,0.00557519,"n a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined. Moreover, in order to experiment with different paraphrase sets providing different degrees of coverage and precision, we pruned the main paraphrase table based on the probabilities, associated to its entries, of 0.1, 0.2 and 0.3. The number of phrase pairs extracted varies from 6 million to about 80000, with an average of 3.2 words per phrase. With the second method, phrasal matches b"
P11-1134,D09-1082,0,0.0358948,"Missing"
P11-1134,P01-1067,0,0.0367702,"scenario. Contrasting results with those obtained with the most widely used resources in TE, we demonstrated the effectiveness of paraphrase tables as a mean to overcome the bias towards single words featured by the existing resources. Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001). Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. Acknowledgments This work has been partially supported by the ECfunded project CoSyne (FP7-I"
P11-1134,C98-1013,0,\N,Missing
P12-2024,carreras-etal-2004-freeling,0,0.169167,"Missing"
P12-2024,de-marneffe-etal-2006-generating,0,0.0595464,"Missing"
P12-2024,P05-1045,0,0.0381264,"Missing"
P12-2024,P07-2045,1,0.00822022,"s, SPTs are extracted from parallel corpora. As a first step we annotate the parallel corpora with named-entity taggers for the source and target languages, replacing named entities with general semantic labels chosen from a coarse-grained taxonomy (person, location, organization, date and numeric expression). Then, we combine the sequences of unique labels into one single token of the same label, and we run Giza++ (Och and Ney, 2000) to align the resulting semantically augmented corpora. Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al., 2007). For the matching phase, we first annotate T and H in the same way we labeled our parallel corpora. Then, for each n-gram order (n=1 to 5) we use the SPT to calculate a matching score as the number of n-grams in H that match with phrases in T divided by the number of n-grams in H.1 Dependency Relation (DR) matching targets the increase of CLTE precision. Adding syntactic constraints to the matching process, DR features aim to reduce the amount of wrong matches often occurring with bag-of-words methods (both at the lexical level and with recall-oriented SPTs). For instance, the contradiction b"
P12-2024,P10-4008,1,0.231125,"Missing"
P12-2024,W11-2404,1,0.892041,"Missing"
P12-2024,N10-1045,1,0.424311,"pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach, however, still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, we propose a novel approach that improves the state-of-the-art in CLTE. 2 CLTE-based content synchronization CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment which consists of deci"
P12-2024,P11-1134,1,0.505816,"hine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach, however, still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, we propose a novel approach that improves the state-of-the-art in CLTE. 2 CLTE-based content synchronization CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment which consists of deciding, given a text T and an hypothesis H in different languages, if the meaning of H can be inferred from the meaning of T. The adoption of entai"
P12-2024,W12-3122,1,0.51127,"et such problem as an application-oriented, crosslingual variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Along this direction, we make two main contributions: (a) Experiments with multi-directional crosslingual textual entailment. So far, cross-lingual textual entailment (CLTE) has been only applied to: i) available TE datasets (uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the i"
P12-2024,W10-0734,1,0.400732,"e informative with respect to the content of the other page. In this paper we set such problem as an application-oriented, crosslingual variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Along this direction, we make two main contributions: (a) Experiments with multi-directional crosslingual textual entailment. So far, cross-lingual textual entailment (CLTE) has been only applied to: i) available TE datasets (uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual"
P12-2024,D11-1062,1,0.47246,"ed words can be either the same, or semantically equivalent terms in the two languages (e.g. according to a bilingual dictionary). Given the dependency tree representations of T and H, for each grammatical relation (r) we calculate a DR matching score as the number of matching occurrences of r in T and H, divided by the number of occurrences of r in H. Separate DR matching scores are calculated for each relation r appearing both in T and H. 4 Experiments and results 4.1 Content synchronization scenario In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. Each pair in the dataset is annotated with “Bidirectional”, “Forward”, or “Backward” entailment judgements. Although highly relevant for the content synchronization task, “Contradiction” and “Unknown” cases (i.e. “NO” entailment in both directions) are not present in the annotation. However, this is the only available dataset suitable to gather insights about the viability of our approach to multi-directional CLTE recognition.2 We chose the ENG-GER portion of the dataset since for such"
P12-2024,S12-1053,1,0.350796,"taset since for such language pair MT systems performance is often lower, making the adoption of simpler solutions based on pivoting more vulnerable. To build the English-German phrase tables we combined the Europarl, News Commentary and “denews”3 parallel corpora. After tokenization, Giza++ and Moses were respectively used to align the corpora and extract a lexical phrase table (PT). Similarly, the semantic phrase table (SPT) has been ex2 Recently, a new dataset including “Unknown” pairs has been used in the “Cross-Lingual Textual Entailment for Content Synchronization” task at SemEval-2012 (Negri et al., 2012). 3 http://homepages.inf.ed.ac.uk/pkoehn/ 122 tracted from the same corpora annotated with the Stanford NE tagger (Faruqui and Pad´o, 2010; Finkel et al., 2005). Dependency relations (DR) have been extracted running the Stanford parser (Rafferty and Manning, 2008; De Marneffe et al., 2006). The dictionary created during the alignment of the parallel corpora provided the lexical knowledge to perform matches when the connected words are different, but semantically equivalent in the two languages. To combine and weight features at different levels we used SVMlight (Joachims, 1999) with default pa"
P12-2024,P00-1056,0,0.0178874,"place of “out of vocabulary” terms (e.g. unseen person names) is an effective way to improve CLTE performance, even at the cost of some loss in precision. Like lexical phrase tables, SPTs are extracted from parallel corpora. As a first step we annotate the parallel corpora with named-entity taggers for the source and target languages, replacing named entities with general semantic labels chosen from a coarse-grained taxonomy (person, location, organization, date and numeric expression). Then, we combine the sequences of unique labels into one single token of the same label, and we run Giza++ (Och and Ney, 2000) to align the resulting semantically augmented corpora. Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al., 2007). For the matching phase, we first annotate T and H in the same way we labeled our parallel corpora. Then, for each n-gram order (n=1 to 5) we use the SPT to calculate a matching score as the number of n-grams in H that match with phrases in T divided by the number of n-grams in H.1 Dependency Relation (DR) matching targets the increase of CLTE precision. Adding syntactic constraints to the matching process, DR feat"
P12-2024,W08-1006,0,0.0178413,"Missing"
P13-2135,J93-2003,0,0.0369107,"(2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2 , merging their output to obtain the 4-class judgments (e.g. yes/ye"
P13-2135,W12-3102,0,0.194915,"Missing"
P13-2135,W08-0509,0,0.0470092,"odels we used the Europarl parallel corpus (Koehn, 2005), concatenated with the News Commentary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two bina"
P13-2135,S12-1053,1,0.80961,"used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2 , merging their output to obtain the 4-class judgments (e.g. yes/yes=bidirectional, yes/no=forward). puted separately for values of n = 1 . . . 5). The POS group considers the part of speech (PoS) of the words in T∗ as a source of qualitative information about their importance. To compute these features we use the TreeTagger (Schmid, 1995), manually mapping the fine-grained set of assigned PoS labels into a more general set of tags (P ) based on the universal PoS tag set by Petrov et al. (2012). POS features differentiate between aligned words (words in T1 that are aligned to one or more words in T2 ) and alignments (the edges connecting words in T1 and T2 ). Features considering the aligned words in T∗ are: 7. for each PoS tag p ∈ P , proportion of aligned words in T∗ tagged with p; 8. proportion of words in T1 aligned with words with the same PoS tag in T2 (and vice-versa); 9. for each PoS tag p ∈ P , proportion of words in T1 tagged as p which are aligned to words with the same tag in T2 (and vice-versa). Features considering the alignments are: 10. proportion of alignments conne"
P13-2135,S12-1102,0,0.0409073,"Missing"
P13-2135,S13-2005,1,0.883856,"Missing"
P13-2135,2005.iwslt-1.8,0,0.0366323,"tences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2 , merging their output to obtain the 4-class judgments (e.g. yes/yes=bidirectional, yes/no=forward). puted separately for values of n = 1 ."
P13-2135,J03-1002,0,0.0182223,"Missing"
P13-2135,2005.mtsummit-papers.11,0,0.00881447,"sequences; 5. position of a) the first unaligned word, and b) the last unaligned word, both normalized by the lenght of T∗ ; 6. proportion of word n-grams in T∗ containing only aligned words (the feature was com2 For instance, the fact that aligning all nouns and the most relevant terms in T1 and T2 is a good indicator of semantic equivalence. 1 A translation has to be semantically equivalent to the source sentence. 772 described in Negri et al. (2011), and consists of 1000 T1 –T2 pairs (500 for training, 500 for test). To train the word alignment models we used the Europarl parallel corpus (Koehn, 2005), concatenated with the News Commentary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out"
P13-2135,lenci-etal-2002-multilingual,0,0.0413395,"Missing"
P13-2135,petrov-etal-2012-universal,0,0.0528715,"Missing"
P13-2135,N10-1045,1,0.823396,"for a variety of cross-lingual applications. So far, despite the relevance of the problem, research on multilingual applications has either circumvented the issue, or proposed partial solutions. When possible, the typical approach builds on the reduction to a monolingual task, burdening the process with dependencies from machine translation (MT) components. For instance, in crosslingual question answering and cross-lingual textual entailment (CLTE), intermediate MT steps are respectively performed to ease answer retrieval/presentation (Parton, 2012; Tanev et al., 2006) and semantic inference (Mehdad et al., 2010). Direct solutions that avoid such pivoting strategies typically exploit similarity measures that rely on bag-of-words representations. As an 2 Objectives and Method We propose a supervised learning approach for identifying and classifying semantic relations between two sentences T1 and T2 written in different languages. Beyond semantic equivalence, which is relevant to applications such as MT quality es771 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 771–776, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (a"
P13-2135,P12-2024,1,0.819601,"1-L2 Combination CLTE model for L1-L2 Learning algorithm Unlabeled CLTE data for L1-L2 CLTE model for L1-L2 CLTE model for L5-L6 Unlabeled CLTE data for L3-L4 CLTE annotation CLTE model for L1-L2 CLTE annotation Unlabeled CLTE data for L3-L4 CLTE model for L7-L8 CLTE annotation Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and"
P13-2135,W12-3122,1,0.820082,"1-L2 Combination CLTE model for L1-L2 Learning algorithm Unlabeled CLTE data for L1-L2 CLTE model for L1-L2 CLTE model for L5-L6 Unlabeled CLTE data for L3-L4 CLTE annotation CLTE model for L1-L2 CLTE annotation Unlabeled CLTE data for L3-L4 CLTE model for L7-L8 CLTE annotation Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and"
P13-2135,C96-2141,0,0.0460042,"ary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2 , merging their output"
P13-2135,S12-1064,0,0.026082,"Missing"
P13-2135,D11-1062,1,0.830023,"ned words, and b) sequence of unaligned words, both normalized by the length of T∗ ; 4. average length of a) the aligned word sequences, and b) the unaligned word sequences; 5. position of a) the first unaligned word, and b) the last unaligned word, both normalized by the lenght of T∗ ; 6. proportion of word n-grams in T∗ containing only aligned words (the feature was com2 For instance, the fact that aligning all nouns and the most relevant terms in T1 and T2 is a good indicator of semantic equivalence. 1 A translation has to be semantically equivalent to the source sentence. 772 described in Negri et al. (2011), and consists of 1000 T1 –T2 pairs (500 for training, 500 for test). To train the word alignment models we used the Europarl parallel corpus (Koehn, 2005), concatenated with the News Commentary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value"
P13-2135,C04-1046,0,\N,Missing
P14-1067,W13-2241,0,0.0508695,"used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of"
P14-1067,2013.mtsummit-papers.5,0,0.0102042,"ranslated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Mart´ınez et al., 2010; Mart´ınez-G´omez et al., 2011; Mart´ınez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Each post-edition brings a wealth of dynamic knowledge about the whole translation process and the involved actors. For instance, adaptive QE components could exploit information about the distance between automatically assigned scores and the quality standards of individual translators (inferred from the amount of their corrections) to “profile” their behaviour. The online learning paradigm fits"
P14-1067,W12-3112,0,0.0061207,"e model to update its predictions for future instances. QE is generally cast as a supervised machine learning task, where a model trained from a collection of (source, target, label) instances is used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diver"
P14-1067,C12-1070,0,0.0195713,"to one of three sets (support, empty, error) maintaining the consistency of a set of conditions known as KarushKuhn Tucker (KKT) conditions. For each new point, OSVR starts a cycle where the samples are moved across the three sets until the KKT conditions are verified and the new point is assigned to one of the sets. If the point is identified as a support vector, the parameters of the model are updated. This allows OSVR to benefit from the prediction capability of -SVR in an online setting. From a practical point of view, providing the best trade off between accuracy and computational time (He and Wang, 2012), PA represents a good solution to meet the demand of efficiency posed by the CAT framework. For each instance i, after emitting a prediction and receiving the true label, PA computes the -insensitive hinge loss function. If its value is larger than the tolerance parameter (), the weights of the model are updated as much as the aggressiveness parameter C allows. In contrast with OSVR, which keeps track of the most important points seen in the past (support vectors), the update of the weights is done without considering the previously processed i-1 instances. Although it makes PA faster than"
P14-1067,W13-2206,0,0.00575961,") instances is used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thu"
P14-1067,2012.amta-wptp.2,0,0.00377347,") model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items. Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach. The evaluation is carried out by measuring the global error of each algorithm on test sets featuring different degrees of similarity with the data used for trai"
P14-1067,W12-3102,0,0.0375667,"Missing"
P14-1067,W12-3123,0,0.035032,"imation (QE) systems, additional complexity comes from the difficulty to model user and domain changes. Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach. 1 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tangible mutual benefit. On one side, SMT research brings to the industry improved output quality and a"
P14-1067,P13-1004,0,0.0213563,"ction from a source–target pair (i.e. one instance at a time instead of processing an entire training set); 2. Emit a prediction for the input instance; 3. Gather user feedback for the instance (i.e. calculating a “true label” based on the amount of user post-editions); 711 These interconnected issues are particularly relevant in the CAT framework, where translation jobs from different domains are routed to professional translators with different idiolect, background and quality standards. The first aspect, modelling annotators’ individual behaviour and interdependences, has been addressed by Cohn and Specia (2013), who explored multi-task Gaussian Processes as a way to jointly learn from the output of multiple annotations. This technique is suitable to cope with the unbalanced distribution of training instances and yields better models when heterogeneous training datasets are available. The second problem, the adaptability of QE models, has not been explored yet. A common trait of all current approaches, in fact, is the reliance on batch learning techniques, which assume a “static” nature of the world where new unseen instances that will be encountered will be similar to the training data.4 However, si"
P14-1067,W13-2243,1,0.645696,"Missing"
P14-1067,W13-2237,0,0.0127973,"incrementally store translated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Mart´ınez et al., 2010; Mart´ınez-G´omez et al., 2011; Mart´ınez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Each post-edition brings a wealth of dynamic knowledge about the whole translation process and the involved actors. For instance, adaptive QE components could exploit information about the distance between automatically assigned scores and the quality standards of individual translators (inferred from the amount of their corrections) to “profile” their behaviour. The onlin"
P14-1067,P13-2135,1,0.899634,"Missing"
P14-1067,W12-3122,1,0.413375,"of Athens, Greece {turchi,desouza,negri}@fbk.eu anastasopoulos.ant@gmail.com Abstract The possibility to speed up the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: The automatic estimation of machine translation (MT) output quality is a hard task in which the selec"
P14-1067,turchi-negri-2014-automatic,1,0.822394,"plexity comes from the difficulty to model user and domain changes. Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach. 1 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tangible mutual benefit. On one side, SMT research brings to the industry improved output quality and a number of appealing solutions useful to increa"
P14-1067,N10-1079,0,0.00997561,"Missing"
P14-1067,W13-2231,1,0.446827,"stems, additional complexity comes from the difficulty to model user and domain changes. Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach. 1 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tangible mutual benefit. On one side, SMT research brings to the industry improved output quality and a number of appealing s"
P14-1067,C00-2137,0,0.0374506,"2007) on parallel data from the two domains (about 2M sentences for IT and 1.5M for L). Post-editions were collected from eight professional translators (four for each document) operating with the CAT tool in real working conditions. According to the way they are created, the two datasets allow us to evaluate the adaptability of different QE models with respect to user changes 11 Results marked with the “∗ ” symbol are NOT statistically significant compared to the corresponding batch model. The others are always statistically significant at p≤0.005, calculated with approximate randomization (Yeh, 2000). 12 13 715 MateCat – http://www.matecat.com/ http://eur-lex.europa.eu/ user change Train Test rad cons sim1 sim2 cons rad sim2 sim1 Train Test cons rad sim2 sim1 rad cons sim1 sim2 ∆ HTER 20.5 19.4 3.3 3.2 ∆ HTER 12.8 9.6 3.3 1.1 Legal Domain Batch Adaptive MAE MAE Alg. 20.6 14.5 PA 21.3 16.1 PA 12.2 12.6∗ OSVR 13.3 13.9∗ OSVR IT Domain µ Batch Adaptive MAE MAE MAE Alg 19.2 19.8 17.5∗ OSVR 16.8 16.6 15.6 PA 14.7 14.4 15∗ OSVR 15 13.9 14.4∗ OSVR µ MAE 21.4 21.2 14.7 13.4 Empty MAE Alg. 12.5 OSVR 11.3 OSVR 12.9∗ OSVR 15.2∗ OSVR Empty MAE Alg 16.6 OSVR 15.5 OSVR 15.5∗ OSVR 16.1∗ OSVR Table 2: MA"
P14-1067,2013.mtsummit-posters.13,1,0.179685,"Missing"
P14-1067,W13-2227,0,0.0504453,"Missing"
P14-1067,shah-etal-2014-efficient,1,0.715171,"represented by (source, target) pairs; • The prediction p(xi ) is the automatically estimated HTER score; • The true label pˆ(xi ) is the actual HTER score calculated over the target and its post-edition. At each step of the process, the goal of the learner is to exploit user post-editions to reduce the difference between the predicted HTER values and the true labels for the following (source, target) pairs. As depicted in Figure 1, this is done as follows: 1. At step i, an unlabelled (source, target) pair xi is sent to a feature extraction component. To this aim, we used an adapted version (Shah et al., 2014) of the open-source QuEst6 tool (Specia et al., 2013). The tool, which implements a large number of features proposed by participants in the WMT QE shared tasks, has been modified to process one sentence at a time as requested for integration in a CAT environment; Online QE for CAT environments When operating with advanced CAT tools, translators are presented with suggestions (either matching fragments from a translation memory or automatic translations produced by an MT system) for each sentence of a source document. Before being approved and published, translation suggestions may require dif"
P14-1067,2006.amta-papers.25,0,0.104216,"changes in data distribution, learning from user feedback on new, unseen test items. Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach. The evaluation is carried out by measuring the global error of each algorithm on test sets featuring different degrees of similarity with the data used for training. Our results 1 Possible label types include post-editing effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values (Snover et al., 2006), and post-editing time (e.g. seconds per word). 2 http://www.statmt.org/wmt13/ 3 For a comprehensive overview of the QE approaches proposed so far we refer the reader to the WMT12 and WMT13 QE shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013). 1. Perform online feature extraction from a source–target pair (i.e. one instance at a time instead of processing an entire training set); 2. Emit a prediction for the input instance; 3. Gather user feedback for the instance (i.e. calculating a “true label” based on the amount of user post-editions); 711 These interconnected issues a"
P14-1067,W12-3118,0,0.0129618,"bels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherentl"
P14-1067,2009.eamt-1.5,1,0.289783,"Technical University of Athens, Greece {turchi,desouza,negri}@fbk.eu anastasopoulos.ant@gmail.com Abstract The possibility to speed up the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: The automatic estimation of machine translation (MT) output quality is a hard t"
P14-1067,P13-4014,1,0.049007,"Missing"
P14-1067,P07-2045,0,\N,Missing
P14-1067,C04-1046,0,\N,Missing
P14-1067,W13-2201,0,\N,Missing
P15-1022,W12-3102,0,0.141203,"Missing"
P15-1022,P13-1004,0,0.0466311,"paradigm to classification problems, but its use for HTER estimation requires real-valued predictions. To this aim, we propose a new regression algorithm that, at the same time, handles positive and negative transfer and performs online weight updates. • A comparison between online multitask and alternative, state-of-the-art online learning strategies. Our experiments, carried out in a realistic scenario involving a stream of data from four domains, lead to consistent results that prove the effectiveness of our approach. 2 On the other way round, task relatedness is successfully exploited by Cohn and Specia (2013), who apply multitask learning to jointly learn from data obtained from several annotators with different levels of expertise and reliability. A similar approach is adopted by de Souza et al. (2014a), who apply multitask learning to cope with situations in which a QE model has to be trained with scarce data from multiple domains/genres, different from the actual test domain. The two methods significantly outperform both individual single-task (indomain) models and single pooled models. However, operating in batch learning mode, none of them provides the continuous learning capabilities desirab"
P15-1022,C14-1040,1,0.877759,"Missing"
P15-1022,2014.amta-workshop.2,1,0.788544,"Missing"
P15-1022,P11-1022,0,0.0221012,"stract are extremely useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: We present a method for predicting machine translation output quality geared to the needs of computer-assisted translation. These include the capability to: i) continuously learn and self-adapt to a stream of data coming from multiple translation jobs, ii) react to data diversity by exploiting hu"
P15-1022,P10-1063,0,0.0215689,"i,eliricci,turchi}@fbk.eu Abstract are extremely useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: We present a method for predicting machine translation output quality geared to the needs of computer-assisted translation. These include the capability to: i) continuously learn and self-adapt to a stream of data coming from multiple translation jobs, ii) react to data diversi"
P15-1022,P14-1081,0,0.153381,"rent sources. Along this direction our contributions are: First, the data used are substantially homogeneous (usually they come from the same domain, and target translations are produced by the same MT system). Second, training and test are carried out as distinct, sequential phases. Instead, in the CAT environment, a QE component should ideally serve, adapt to and continuously learn from simultaneous translation jobs involving different MT engines, domains, genres and users (Turchi et al., 2013). These challenges have been separately addressed from different perspectives in few recent works. Huang et al. (2014) proposed a method to adaptively train a QE model for documentspecific MT post-editing. Adaptability, however, is achieved in a batch fashion, by re-training an ad hoc QE component for each document to be translated. The adaptive approach proposed by Turchi et al. (2014) overcomes the limitations of batch methods by applying an online learning protocol to continuously learn from a stream of (potentially heterogeneous) data. Experimental results suggest the effectiveness of online learning as a way to exploit user feedback to tailor QE predictions to their quality standards and to cope with the"
P15-1022,2009.eamt-1.5,1,0.948525,", Italy {desouza,negri,eliricci,turchi}@fbk.eu Abstract are extremely useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: We present a method for predicting machine translation output quality geared to the needs of computer-assisted translation. These include the capability to: i) continuously learn and self-adapt to a stream of data coming from multiple translation job"
P15-1022,P07-2045,0,0.00601708,"Missing"
P15-1022,W13-2231,1,0.887281,"erse data. The latter provides the model with the capability to exploit the similarities between data coming from different sources. Along this direction our contributions are: First, the data used are substantially homogeneous (usually they come from the same domain, and target translations are produced by the same MT system). Second, training and test are carried out as distinct, sequential phases. Instead, in the CAT environment, a QE component should ideally serve, adapt to and continuously learn from simultaneous translation jobs involving different MT engines, domains, genres and users (Turchi et al., 2013). These challenges have been separately addressed from different perspectives in few recent works. Huang et al. (2014) proposed a method to adaptively train a QE model for documentspecific MT post-editing. Adaptability, however, is achieved in a batch fashion, by re-training an ad hoc QE component for each document to be translated. The adaptive approach proposed by Turchi et al. (2014) overcomes the limitations of batch methods by applying an online learning protocol to continuously learn from a stream of (potentially heterogeneous) data. Experimental results suggest the effectiveness of onli"
P15-1022,P14-1067,1,0.8615,"Missing"
P15-1022,W12-3122,1,0.777086,"y useful to let the user decide whether to post-edit a given suggestion or ignore it and translate the source segment from scratch. However, while scoring TM matches relies on standard methods based on fuzzy matching, predicting the quality of MT suggestions at run-time and without references is still an open issue. This is the goal of MT quality estimation (QE), which aims to predict the quality of an automatic translation as a function of the estimated number of editing operations or the time required for manual correction (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Mehdad et al., 2012). So far, QE has been mainly approached in controlled settings where homogeneous training and test data is used to learn and evaluate static predictors. Cast in this way, however, it does not fully reflect (nor exploit) the working conditions posed by the CAT framework, in which: We present a method for predicting machine translation output quality geared to the needs of computer-assisted translation. These include the capability to: i) continuously learn and self-adapt to a stream of data coming from multiple translation jobs, ii) react to data diversity by exploiting human feedback, and iii)"
P15-1022,2013.mtsummit-papers.15,0,0.0110473,"om Technology Entertainment Design talks (TED), Information Technology manuals (IT) and Education Material (EM). All datasets provide a set of tuples composed by (source, translation and post-edited translation). The TED dataset is distributed in the Trace corpus4 and includes, as source sentences, the subtitles of several talks spanning a range of topics presented in the TED conferences. Translations were generated by two different MT systems: a phrase-based statistical MT system and a commercial rule-based system. Post-editions were collected from four different translators, as described by Wisniewski et al. (2013). The IT manuals data come from two language service providers, henceforth LSP 1 and LSP 2. The ITLSP 1 tuples belong to a software manual translated by an SMT system trained using the Moses toolkit (Koehn et al., 2007). The posteditions were produced by one professional trans5 https://autodesk.app.box.com/ Autodesk-PostEditing 6 http://sourceforge.net/projects/ tercpp/ 4 http://anrtrace.limsi.fr/trace_ postedit.tar.bz2 223 Figure 1: Validation curves for the R parameter. Figure 2: Learning curves for all the domains, computed by calculating the mean MAE (↓) of the four domains. ity between IT"
P15-1022,2013.mtsummit-posters.13,1,0.866283,"Missing"
P15-1022,shah-etal-2014-efficient,1,0.869567,"TLin results are obtained by separately training one model for each domain. These represent two alternative strategies for the integration of QE in the CAT framework. The former would allow a single model to simultaneously support multiple translation jobs in different domains, without any notion about their relations. The latter would lead to a more complex architecture, organized as a pool of independent, specialized QE modules. Features. Our models are trained using the 17 baseline features proposed in (Specia et al., 2009), extracted with the online version of the QuEst feature extractor (Shah et al., 2014). These features take into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the translation (e.g. language model probabilities). Their description is available in (Callison-Burch et al., 2012). The results of previous WMT QE shared tasks have shown that these features are particularly competitive in the HTER prediction task. Evaluation metric. The performance of our regression models is evaluated in terms of mean absolute error (MAE), a standard error measure for regression problems commonly used also for QE (Calli"
P15-1022,C14-2028,1,\N,Missing
P15-1022,W13-2201,0,\N,Missing
P15-1106,C14-1040,1,0.893522,"Missing"
P15-1106,N15-1073,1,0.642373,"Missing"
P15-1106,2013.iwslt-evaluation.1,0,0.0757383,"Missing"
P15-1106,C02-1025,0,0.175123,"Missing"
P15-1106,W13-2243,1,0.862965,"Missing"
P15-1106,N07-2017,0,0.201705,"variant and, if available, by exploiting confidence features. 2 Related work This paper gathers three main research strands together: ASR system combination, ASR quality estimation and machine-learned ranking. Fiscus (1997) proposed ROVER as an approach to produce a composite ASR output. The basic approach has been extended in several ways. N-Best ROVER (Stolcke et al., 2000) improves the original method by combining multiple alternatives from each combined system. Schwenk and Gauvain (2000) exploit a secondary language model to rescore the final n-best hypotheses generated by ROVER. iROVER (Hillard et al., 2007) exploits a classifier to choose the system that is most likely to be correct at each word location. cROVER (Abida et al., 2011) integrates a semantic pre-filtering step in which the word transition network is scanned to flag and eliminate erroneous words to facilitate the voting. Other approaches to ASR system combination make use of word lattices or confusion networks (Mangu, 2000; Li et al., 2002; Evermann and Woodland, 2000; Hoffmeister et al., 2006; Bougares et al., 2013, inter alia). Note that all these combination methods require to have access to the inner structure of the ASR decoder,"
P15-1106,W12-3122,1,0.809438,"ility of a confidence-independent method to predict the quality of ASR transcriptions at segment level. This “quality estimation” (QE) task has been recently addressed in (Negri et al., 2014; C. de Souza et al., 2015) as a supervised regression problem in which transcriptions’ WER is predicted without having access to reference transcripts.3 Different feature sets have been evaluated, showing that even with those extracted only 2 Details about this dataset will be provided in Section 6.1. This formulation is very similar to the machine translation counterpart of the task (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014; C. de Souza et al., 2014). 3 from the signal and the transcription (i.e. disregarding information about the decoding process) the prediction error is sufficiently low to open to real applications. However, though promising, experimental results stem from an intrinsic evaluation in which QE is only addressed in isolation. By applying it to inform ROVER, we propose for the first time an application-oriented extrinsic evaluation of ASR QE (our first contribution). To this aim, we extend previous ASR QE methods with new features (second contribution), and report significant"
P15-1106,2009.eamt-1.5,1,0.801667,"s goal is the availability of a confidence-independent method to predict the quality of ASR transcriptions at segment level. This “quality estimation” (QE) task has been recently addressed in (Negri et al., 2014; C. de Souza et al., 2015) as a supervised regression problem in which transcriptions’ WER is predicted without having access to reference transcripts.3 Different feature sets have been evaluated, showing that even with those extracted only 2 Details about this dataset will be provided in Section 6.1. This formulation is very similar to the machine translation counterpart of the task (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014; C. de Souza et al., 2014). 3 from the signal and the transcription (i.e. disregarding information about the decoding process) the prediction error is sufficiently low to open to real applications. However, though promising, experimental results stem from an intrinsic evaluation in which QE is only addressed in isolation. By applying it to inform ROVER, we propose for the first time an application-oriented extrinsic evaluation of ASR QE (our first contribution). To this aim, we extend previous ASR QE methods with new features (second contribution), an"
P15-1106,P14-1067,1,0.889561,"Missing"
P15-1106,C14-1171,1,0.757729,"Missing"
P15-2026,2011.mtsummit-papers.35,0,0.769683,"Missing"
P15-2026,C12-1014,0,0.408062,"Missing"
P15-2026,W07-0732,0,0.15215,"i}@fbk.eu {wellermn@ims.uni-stuttgart.de} Abstract The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that the reported results"
P15-2026,W09-0419,0,0.0148512,"ms.uni-stuttgart.de} Abstract The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that the reported results are difficult to gen"
P15-2026,W08-0509,0,0.103509,"methods To obtain the statistical APE pipeline that represents the backbone of both methods we used a phrase-based Moses system (Koehn et al., 2007). Our training data (see Section 3) consists of (source, MT output, post-edition) triplets for six language pairs having English as source. While Method 1 uses only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. The APE system for each target language was tuned on comparable development sets (see below), optimizing TER with Minimum Error Rate Training (Och, 2003) using the post-edited sentences as references. 3 No. tokens 210,491 202,475 211,149 252,020 263,690 239,912 206,016 Vocab. Size 10,727 16,716 17,563 11,075 10,928 10,703 17,027 No. Lemmas 8,260 10,137 14,368 6,683 7,213 6,549 10,430 Table 1: Data statistics for each language. sentences are translated"
P15-2026,W12-3146,0,0.193653,"Missing"
P15-2026,W11-2123,0,0.00813414,"s we used a phrase-based Moses system (Koehn et al., 2007). Our training data (see Section 3) consists of (source, MT output, post-edition) triplets for six language pairs having English as source. While Method 1 uses only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. The APE system for each target language was tuned on comparable development sets (see below), optimizing TER with Minimum Error Rate Training (Och, 2003) using the post-edited sentences as references. 3 No. tokens 210,491 202,475 211,149 252,020 263,690 239,912 206,016 Vocab. Size 10,727 16,716 17,563 11,075 10,928 10,703 17,027 No. Lemmas 8,260 10,137 14,368 6,683 7,213 6,549 10,430 Table 1: Data statistics for each language. sentences are translated into several languages (30K to 410K translations per language) with Autodesk’s in-hous"
P15-2026,P13-3025,0,0.0455629,"ection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that the reported results are difficult to generalise. Indeed, most of the works focus on evaluating a specific method,1 which is typically applied to one"
P15-2026,2007.mtsummit-papers.34,0,0.122265,"{chatterjee,negri,turchi}@fbk.eu {wellermn@ims.uni-stuttgart.de} Abstract The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that"
P15-2026,2012.eamt-1.55,0,0.200898,"Missing"
P15-2026,P07-2045,0,0.0049239,"disregarding information about the source language (E): Correction 2 Here, f 0 and f respectively stand for the rough MT output and its correct version in the foreign language F. 3 This is done based on the description provided by the published works. Discrepancies with the actual methods are possible, due to our misinterpretation or to wrong guesses about details that are missing in the papers. 157 guage pairs. Lang. 2.3 En Cs De Es Fr It Pl Reimplementing the two methods To obtain the statistical APE pipeline that represents the backbone of both methods we used a phrase-based Moses system (Koehn et al., 2007). Our training data (see Section 3) consists of (source, MT output, post-edition) triplets for six language pairs having English as source. While Method 1 uses only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gra"
P15-2026,N07-1064,0,0.625979,"o (2) FBK - Fondazione Bruno Kessler (3) IMS, University of Stuttgart {chatterjee,negri,turchi}@fbk.eu {wellermn@ims.uni-stuttgart.de} Abstract The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from"
P15-2026,2006.amta-papers.25,0,0.716643,"Missing"
P15-2026,W04-3250,0,0.336997,"ence, baseline scores for each language pair correspond to the TER computed between the original MT output (produced by the “black-box” Autodesk inhouse system) and the human post-edits. 4 Results Table 2 lists our results, with language pairs ordered according to the respective baseline TER. The positive answer to Q1 (“Does APE yield consistent improvements to MT output?”) is evident: both APE methods consistently improve MT quality on all language pairs. TER reductions range from 3.06 to 5.27 points. Quality improvements are statistically significant at p < 0.05, measured by bootstrap test (Koehn, 2004). In answer to Q2 (“What is the relation between original MT quality and APE results?”), our controlled experiments evidence for the first time in APE research that the higher the MT quality, the higher is the improvement, i.e. percentage of error reduction, yielded by the APE methods. On one side, this interesting result may seem counterintuitive because a larger room for improvement 6 Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the correct translation. Lower TER/HTER values indicate better MT qua"
P15-2026,2005.mtsummit-papers.11,0,0.0449242,"a fair comparative study between different methods in controlled conditions. The key enabling factor is the availability, for the first time, of data consisting of the same source sentences, machinetranslated in several languages and post-edited by professional translators. To guarantee similar experimental conditions in the six language settings, we also train comparable target language models from external data (indeed, the 12.2K post-edits would not be enough to train reliable LMs). We build our LMs from approximately 2.5M translations of the same English sentences collected from Europarl (Koehn, 2005), DGT-Translation Memory (Steinberger et al., 2012), JRC Acquis (Steinberger et al., 2006), OPUS IT (Tiedemann, ) and other Autodesk data common to all languages. Evaluation metric. We evaluate the APE methods based on their capability to reduce the distance between the MT output and a correct (fluent and adequate) translation. As a measure of the amount of the editing operations needed for the correction, TER and HTER (Snover et al., 2006) fit for our purpose. TER and HTER measure the minimum edit distance between the MT output and its corData. We experiment with the Autodesk PostEditing Data"
P15-2026,N09-2055,0,0.121042,"Missing"
P15-2026,steinberger-etal-2012-dgt,0,0.0279034,"Missing"
P15-2026,tiedemann-2012-parallel,0,0.0662439,"Missing"
P15-2026,P03-1021,0,0.0340665,"s only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. The APE system for each target language was tuned on comparable development sets (see below), optimizing TER with Minimum Error Rate Training (Och, 2003) using the post-edited sentences as references. 3 No. tokens 210,491 202,475 211,149 252,020 263,690 239,912 206,016 Vocab. Size 10,727 16,716 17,563 11,075 10,928 10,703 17,027 No. Lemmas 8,260 10,137 14,368 6,683 7,213 6,549 10,430 Table 1: Data statistics for each language. sentences are translated into several languages (30K to 410K translations per language) with Autodesk’s in-house MT system (Zhechev, 2012) and post-edited by professional translators. Our experiments are run on six language pairs having English as source and Czech, German, Spanish, French, Italian and Polish as target. T"
P15-2026,2012.amta-wptp.10,0,0.0672469,"n-gram modeling with an n-gram length of 5. The APE system for each target language was tuned on comparable development sets (see below), optimizing TER with Minimum Error Rate Training (Och, 2003) using the post-edited sentences as references. 3 No. tokens 210,491 202,475 211,149 252,020 263,690 239,912 206,016 Vocab. Size 10,727 16,716 17,563 11,075 10,928 10,703 17,027 No. Lemmas 8,260 10,137 14,368 6,683 7,213 6,549 10,430 Table 1: Data statistics for each language. sentences are translated into several languages (30K to 410K translations per language) with Autodesk’s in-house MT system (Zhechev, 2012) and post-edited by professional translators. Our experiments are run on six language pairs having English as source and Czech, German, Spanish, French, Italian and Polish as target. To set up our controlled environment, we extract all the (source, MT output, post-edition) triplets sharing the same source (En) sentences across all language pairs. Table 1 provides some statistics about the resulting tri-parallel corpora. After random shuffling the triplets, we create training (12.2K triplets), development (2K) and test data (2K) sharing exactly the same source sentences across languages. Traini"
P15-2026,steinberger-etal-2006-jrc,0,\N,Missing
P15-2087,P15-1022,1,0.749626,"Missing"
P15-2087,C14-2028,1,0.898342,"Missing"
P15-2087,P14-1081,0,0.0138533,"impact of QE predictions on translators’ productivity is analysed by measuring the number of words that can be post-edited in a fixed amount of time. The evaluation, however, only concentrates on the use of QE to rank MT outputs, and the gains in translation speed are measured against the contrastive condition in which no QE-based ranking mechanism is used. In this artificial scenario, the analysis disregards the relation The usefulness of translation quality estimation (QE) to increase productivity in a computer-assisted translation (CAT) framework is a widely held assumption (Specia, 2011; Huang et al., 2014). So far, however, the validity of this assumption has not been yet demonstrated through sound evaluations in realistic settings. To this aim, we report on an evaluation involving professional translators operating with a CAT tool in controlled but natural conditions. Contrastive experiments are carried out by measuring post-editing time differences when: i) translation suggestions are presented together with binary quality estimates, and ii) the same suggestions are presented without quality indicators. Translators’ productivity in the two conditions is analysed in a principled way, accountin"
P15-2087,W12-3122,1,0.85332,"counting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions. 1 Introduction Machine translation (MT) quality estimation aims to automatically predict the expected time (e.g. in seconds) or effort (e.g. number of editing operations) required to correct machine-translated sentences into publishable translations (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014a; C. de Souza et al., 2015). In principle, the task has a number of practical applications. An intuitive one is speeding-up the work of human translators operating with a CAT tool, a software de1 Notice that the same sentence cannot be post-edited twice (e.g. with/without quality labels) by the same translator without introducing a bias in the time measurements. 530 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 530–535, c Beijing, China"
P15-2087,shah-etal-2014-efficient,1,0.808839,"parate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The features are extracted from the data available at prediction time (source text and raw MT output) by using an adapted version (Shah et al., 2014) of the open-source QuEst software (Specia et al., 2013). The SVM parameters are optimized by cross-validation on the training set. With these classifiers, we finally assign quality flags to the raw segment translations in the test • Does QE really help in the CAT scenario? • If yes, under what conditions? 2 Experimental Setup One of the key questions in utilising QE in the CAT scenario is how to relay QE information to the user. In our experiments, we evaluate a way of visualising MT quality estimates that is based on a color-coded binary classification (green vs. red) as an alternative to re"
P15-2087,2006.amta-papers.25,0,0.122715,"up for a between-subject comparison on a single long document as follows. First, the document is split in two parts. The first part serves as the training portion for a binary quality estimator; the second part is reserved for evaluation. The training portion is machine-translated with a state-of-the-art, phrasebased Moses system (Koehn et al., 2007)2 and post-edited under standard conditions (i.e. without visualising QE information) by the same users involved in the testing phase. Based on their postedits, the raw MT output samples are then labeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER> 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline featu"
P15-2087,2009.eamt-1.5,1,0.923217,"a principled way, accounting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions. 1 Introduction Machine translation (MT) quality estimation aims to automatically predict the expected time (e.g. in seconds) or effort (e.g. number of editing operations) required to correct machine-translated sentences into publishable translations (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014a; C. de Souza et al., 2015). In principle, the task has a number of practical applications. An intuitive one is speeding-up the work of human translators operating with a CAT tool, a software de1 Notice that the same sentence cannot be post-edited twice (e.g. with/without quality labels) by the same translator without introducing a bias in the time measurements. 530 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 530–"
P15-2087,P13-4014,0,0.0522816,"Missing"
P15-2087,2011.eamt-1.12,0,0.0408152,"al., 2013; Bojar et al., 2014). On-field evaluation is indeed a complex task, as it requires: i) the availability of a CAT tool capable to integrate MT QE functionalities, ii) professional translators used to MT post-editing, iii) a sound evaluation protocol to perform betweensubject comparisons,1 and iv) robust analysis techniques to measure statistical significance under variable conditions (e.g. differences in users’ postediting behavior). To bypass these issues, the works more closely related to our investigation resort to controlled and simplified evaluation protocols. For instance, in (Specia, 2011) the impact of QE predictions on translators’ productivity is analysed by measuring the number of words that can be post-edited in a fixed amount of time. The evaluation, however, only concentrates on the use of QE to rank MT outputs, and the gains in translation speed are measured against the contrastive condition in which no QE-based ranking mechanism is used. In this artificial scenario, the analysis disregards the relation The usefulness of translation quality estimation (QE) to increase productivity in a computer-assisted translation (CAT) framework is a widely held assumption (Specia, 20"
P15-2087,W13-2231,1,0.887438,"the training portion for a binary quality estimator; the second part is reserved for evaluation. The training portion is machine-translated with a state-of-the-art, phrasebased Moses system (Koehn et al., 2007)2 and post-edited under standard conditions (i.e. without visualising QE information) by the same users involved in the testing phase. Based on their postedits, the raw MT output samples are then labeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER> 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens"
P15-2087,P14-1067,1,0.894194,"Missing"
P15-2087,W12-3102,0,\N,Missing
P15-2087,P07-2045,1,\N,Missing
P15-2087,W05-0908,0,\N,Missing
P15-2087,W13-2201,0,\N,Missing
P16-2047,W12-3122,1,0.865891,"istration ... (... 4 weeks after somministarzione ...) 5. e 5. ensure the organization of ... 5. Per lo smaltimento leggere il foglio illustrativo f Read package leaflet For disposal read the package leaflet chuck carne assada g beef chuck roast ?chuck meat ?assada risultato della stagione h is an integral part of the contract (result of the season) Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory. ENGLISH tic textual similarity,1 cross-lingual textual entailment (Negri et al., 2013), and quality estimation (QE) for MT (Specia et al., 2009; Mehdad et al., 2012; C. de Souza et al., 2014; Turchi et al., 2014; C. de Souza et al., 2015). Also most of the previous approaches to bilingual data mining/cleaning for statistical MT rely on supervised learning (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Jiang et al., 2009). Unsupervised solutions, like the one proposed by Cui et al. (2013) usually rely on redundancy-based approaches that reward parallel segments containing phrase pairs that are frequent in a training corpus. This idea is wellmotivated in the SMT framework but scarcely applicable in the CAT scenario, in which it is crucial to manage and"
P16-2047,W15-5202,1,0.44854,"speed-up and simplify the daily work of human translators. Focusing on TM maintenance, we explore an automatic method to clean a large-scale TM by identifying the TUs in which the target is a poor translation of the source. Its main strength is the reliance on a fully unsupervised approach, which makes it independent from the availability of human-labelled data. As it allows us to avoid the burden of acquiring a (possibly large) set of annotated TUs, our method is cost-effective and highly portable across languages and TMs. This contrasts with supervised strategies like the one presented in (Barbu, 2015) or those applied in closely-related tasks such as cross-lingual semanWe address the problem of automatically cleaning a large-scale Translation Memory (TM) in a fully unsupervised fashion, i.e. without human-labelled data. We approach the task by: i) designing a set of features that capture the similarity between two text segments in different languages, ii) use them to induce reliable training labels for a subset of the translation units (TUs) contained in the TM, and iii) use the automatically labelled data to train an ensemble of binary classifiers. We apply our method to clean a test set"
P16-2047,J05-4003,0,0.0664214,"k roast ?chuck meat ?assada risultato della stagione h is an integral part of the contract (result of the season) Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory. ENGLISH tic textual similarity,1 cross-lingual textual entailment (Negri et al., 2013), and quality estimation (QE) for MT (Specia et al., 2009; Mehdad et al., 2012; C. de Souza et al., 2014; Turchi et al., 2014; C. de Souza et al., 2015). Also most of the previous approaches to bilingual data mining/cleaning for statistical MT rely on supervised learning (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Jiang et al., 2009). Unsupervised solutions, like the one proposed by Cui et al. (2013) usually rely on redundancy-based approaches that reward parallel segments containing phrase pairs that are frequent in a training corpus. This idea is wellmotivated in the SMT framework but scarcely applicable in the CAT scenario, in which it is crucial to manage and reward rare phrases as a source of useful suggestions for difficult translations. 2 and (f), 4. situations in which the translation is awkward (due to mistranslations and/or untranslated terms) like in (g) or it is completely unrelated to the"
P16-2047,S13-2005,1,0.830529,"eason) Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory. ENGLISH tic textual similarity,1 cross-lingual textual entailment (Negri et al., 2013), and quality estimation (QE) for MT (Specia et al., 2009; Mehdad et al., 2012; C. de Souza et al., 2014; Turchi et al., 2014; C. de Souza et al., 2015). Also most of the previous approaches to bilingual data mining/cleaning for statistical MT rely on supervised learning (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Jiang et al., 2009). Unsupervised solutions, like the one proposed by Cui et al. (2013) usually rely on redundancy-based approaches that reward parallel segments containing phrase pairs that are frequent in a training corpus. This idea is wellmotivated in the SMT framework but scarcely applicable in the CAT scenario, in which it is crucial to manage and reward rare phrases as a source of useful suggestions for difficult translations. 2 and (f), 4. situations in which the translation is awkward (due to mistranslations and/or untranslated terms) like in (g) or it is completely unrelated to the source sentence like in (h). Especially in the case of collaboratively-created public TM"
P16-2047,W14-3340,1,0.904003,"Missing"
P16-2047,P15-1022,1,0.894192,"Missing"
P16-2047,J03-3002,0,0.0969194,"carne assada g beef chuck roast ?chuck meat ?assada risultato della stagione h is an integral part of the contract (result of the season) Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory. ENGLISH tic textual similarity,1 cross-lingual textual entailment (Negri et al., 2013), and quality estimation (QE) for MT (Specia et al., 2009; Mehdad et al., 2012; C. de Souza et al., 2014; Turchi et al., 2014; C. de Souza et al., 2015). Also most of the previous approaches to bilingual data mining/cleaning for statistical MT rely on supervised learning (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Jiang et al., 2009). Unsupervised solutions, like the one proposed by Cui et al. (2013) usually rely on redundancy-based approaches that reward parallel segments containing phrase pairs that are frequent in a training corpus. This idea is wellmotivated in the SMT framework but scarcely applicable in the CAT scenario, in which it is crucial to manage and reward rare phrases as a source of useful suggestions for difficult translations. 2 and (f), 4. situations in which the translation is awkward (due to mistranslations and/or untranslated terms) like in (g) or it is c"
P16-2047,2006.amta-papers.25,0,0.175245,"Missing"
P16-2047,W13-2243,1,0.91907,"Missing"
P16-2047,P15-1165,0,0.165344,"Missing"
P16-2047,P13-2061,0,0.0185158,"lt of the season) Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory. ENGLISH tic textual similarity,1 cross-lingual textual entailment (Negri et al., 2013), and quality estimation (QE) for MT (Specia et al., 2009; Mehdad et al., 2012; C. de Souza et al., 2014; Turchi et al., 2014; C. de Souza et al., 2015). Also most of the previous approaches to bilingual data mining/cleaning for statistical MT rely on supervised learning (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Jiang et al., 2009). Unsupervised solutions, like the one proposed by Cui et al. (2013) usually rely on redundancy-based approaches that reward parallel segments containing phrase pairs that are frequent in a training corpus. This idea is wellmotivated in the SMT framework but scarcely applicable in the CAT scenario, in which it is crucial to manage and reward rare phrases as a source of useful suggestions for difficult translations. 2 and (f), 4. situations in which the translation is awkward (due to mistranslations and/or untranslated terms) like in (g) or it is completely unrelated to the source sentence like in (h). Especially in the case of collaboratively-created public TM"
P16-2047,2009.eamt-1.5,1,0.796013,". 4 weeks after administration ... (... 4 weeks after somministarzione ...) 5. e 5. ensure the organization of ... 5. Per lo smaltimento leggere il foglio illustrativo f Read package leaflet For disposal read the package leaflet chuck carne assada g beef chuck roast ?chuck meat ?assada risultato della stagione h is an integral part of the contract (result of the season) Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory. ENGLISH tic textual similarity,1 cross-lingual textual entailment (Negri et al., 2013), and quality estimation (QE) for MT (Specia et al., 2009; Mehdad et al., 2012; C. de Souza et al., 2014; Turchi et al., 2014; C. de Souza et al., 2015). Also most of the previous approaches to bilingual data mining/cleaning for statistical MT rely on supervised learning (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Jiang et al., 2009). Unsupervised solutions, like the one proposed by Cui et al. (2013) usually rely on redundancy-based approaches that reward parallel segments containing phrase pairs that are frequent in a training corpus. This idea is wellmotivated in the SMT framework but scarcely applicable in the CAT scenario, in which it is"
P16-2047,W08-0509,0,0.0304461,"They use word alignment information to link source and target words and capture the quantity of meaning preserved by the translation. For each segment of a TU, word alignment information is used to calculate: i) the proportion of aligned and unaligned word n-grams (n=1,2), ii) the ratio between the longest aligned/unaligned word sequence and the length of the segment, iii) the average length of the aligned/unaligned word sequences, and iv) the position of the first/last unaligned word, normalized by the length of the segment. Word alignment models were trained on the whole TM, using MGIZA++ (Gao and Vogel, 2008). Training of the base classifiers. Each of the three inferred annotations of Z (say z 1 , z 2 , z 3 ) reflects the specific view of the two groups of features used to obtain it (i.e. AB for z 1 , AC for z 2 , BC for z 3 ). Based on each view, we train a binary classifier using the third group of features (i.e. C for z 1 , B for z 2 , A for z 3 ). This results in three ˆ B ˆ and Cˆ that, in spite of the base classifiers: A, same shared purpose, are by construction different from each other. This allows us to create an ensemble of base classifiers and to minimize the risk of overfitting, in whi"
P16-2047,W13-2231,1,0.821448,"Missing"
P16-2047,P09-1098,0,0.0229891,"a risultato della stagione h is an integral part of the contract (result of the season) Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory. ENGLISH tic textual similarity,1 cross-lingual textual entailment (Negri et al., 2013), and quality estimation (QE) for MT (Specia et al., 2009; Mehdad et al., 2012; C. de Souza et al., 2014; Turchi et al., 2014; C. de Souza et al., 2015). Also most of the previous approaches to bilingual data mining/cleaning for statistical MT rely on supervised learning (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Jiang et al., 2009). Unsupervised solutions, like the one proposed by Cui et al. (2013) usually rely on redundancy-based approaches that reward parallel segments containing phrase pairs that are frequent in a training corpus. This idea is wellmotivated in the SMT framework but scarcely applicable in the CAT scenario, in which it is crucial to manage and reward rare phrases as a source of useful suggestions for difficult translations. 2 and (f), 4. situations in which the translation is awkward (due to mistranslations and/or untranslated terms) like in (g) or it is completely unrelated to the source sentence like"
P16-2047,P14-1067,1,0.899817,"Missing"
P16-2047,P12-3005,0,0.0794616,"ose to 1, indicating a high similarity between source and target) to the worst (avg. close to 0). This is done separately for each feature combination, so that the independent views they provide will produce three different ranked lists for the TUs in Z. Finally, the three ranked lists are processed to obtain different sets of positive/negative examples, whose variable size depends on the amount of TUs taken from the top and the bottom of the lists. the actual source and target languages and those indicated in the TM is also verified. Language identification, carried out with the Langid tool (Lui and Baldwin, 2012), is a highly predictive feature since sometimes the two languages are inverted or even completely different. Other features model the similarity between source and target by computing the direct and inverse ratio between the number of characters and words, as well as the average word length in the two segments. Finally, two features look at the presence of uncommon character or word repetitions. QE-derived features (18). This group contains features borrowed from the closely-related task of MT quality estimation, in which the complexity of the source, the fluency of the target and the adequac"
P16-4008,W12-3122,1,0.795935,"ut quality at run-time. Often, indeed, the nature of such applications (consider for instance spoken dialog systems) requires quick re1 More than 157 millions in 10 languages, as announced by Google already in 2012 (source: http://goo.gl/ 5Wlkjl). 43 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 43–48, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics dependencies, TranscRater provides the first offthe-shelf solution to approach ASR QE and extend its application to new scenarios. lations (Mehdad et al., 2012; Camargo de Souza et al., 2013; C. de Souza et al., 2014). Indeed, the two tasks deal with similar issues. In both cases, we have an input “source” (a written sentence and a recorded signal) and an output text (a translation and a transcription) that has to be assessed without any pre-created term of comparison. They can also be approached with similar supervised classification (C. de Souza et al., 2015) or regression strategies (Negri et al., 2014; C. de Souza et al., 2015). Finally, they have similar applications like: 2 ASR QE The basic ASR QE task consists in training a model from (signal"
P16-4008,W14-3340,1,0.89367,"Missing"
P16-4008,C14-1171,1,0.863104,"Missing"
P16-4008,N15-1073,1,0.863867,"Missing"
P16-4008,W13-2243,1,0.884618,"Missing"
P16-4008,2015.iwslt-papers.11,0,0.0552118,"essible and, in this frequent case, the sole elements available for quality prediction are the signal and its transcription (consider, for instance, the increasing amount of captioned Youtube videos generated by a “black-box” ASR system1 ). These issues call for a method to predict ASR output quality that is also confidence-independent. TranscRater (Transcription Rater) provides a unified ASR QE framework designed to meet the three aforementioned requirements. Its development was inspired by software previously released for the machine translation (MT) (Specia et al., 2013; Shah et al., 2014; Servan et al., 2015) equivalent of ASR QE, in which MT quality has to be estimated at run-time and without reference transWe present TranscRater, an open-source tool for automatic speech recognition (ASR) quality estimation (QE). The tool allows users to perform ASR evaluation bypassing the need of reference transcripts and confidence information, which is common to current assessment protocols. TranscRater includes: i) methods to extract a variety of quality indicators from (signal, transcription) pairs and ii) machine learning algorithms which make possible to build ASR QE models exploiting the extracted featur"
P16-4008,shah-etal-2014-efficient,1,0.834601,"t is not always accessible and, in this frequent case, the sole elements available for quality prediction are the signal and its transcription (consider, for instance, the increasing amount of captioned Youtube videos generated by a “black-box” ASR system1 ). These issues call for a method to predict ASR output quality that is also confidence-independent. TranscRater (Transcription Rater) provides a unified ASR QE framework designed to meet the three aforementioned requirements. Its development was inspired by software previously released for the machine translation (MT) (Specia et al., 2013; Shah et al., 2014; Servan et al., 2015) equivalent of ASR QE, in which MT quality has to be estimated at run-time and without reference transWe present TranscRater, an open-source tool for automatic speech recognition (ASR) quality estimation (QE). The tool allows users to perform ASR evaluation bypassing the need of reference transcripts and confidence information, which is common to current assessment protocols. TranscRater includes: i) methods to extract a variety of quality indicators from (signal, transcription) pairs and ii) machine learning algorithms which make possible to build ASR QE models exploitin"
P16-4008,P13-4014,1,0.892243,"Missing"
P16-4008,P15-1106,1,0.780131,"quality indicators from (signal, transcription) pairs and ii) machine learning algorithms which make possible to build ASR QE models exploiting the extracted features. Confirming the positive results of previous evaluations, new experiments with TranscRater indicate its effectiveness both in WER prediction and transcription ranking tasks. 1 Introduction How to determine the quality of an automatic transcription without reference transcripts and without confidence information? This is the key problem addressed by research on ASR quality estimation (Negri et al., 2014; C. de Souza et al., 2015; Jalalvand et al., 2015b), and the task for which TranscRater, the tool described in this paper, has been designed. The work on ASR quality estimation (ASR QE) has several motivations. First, the steady increase of applications involving automatic speech recognition (e.g. video/TV programs subtitling, voice search engines, voice question answering, spoken dialog systems, meeting and broadcast news transcriptions) calls for an accurate method to estimate ASR output quality at run-time. Often, indeed, the nature of such applications (consider for instance spoken dialog systems) requires quick re1 More than 157 million"
P16-4009,W13-2243,1,0.90459,"Missing"
P16-4009,W14-3340,1,0.905877,"Missing"
P16-4009,W15-5204,0,0.0456436,"ies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classification task, this approach highly depends on the availability of"
P16-4009,W08-0509,0,0.102406,"d deviation values for a given indicator (e.g. sentence length ratio, proportion of aligned words), quantiles or std counts in case of normal value distributions will be used as decision boundaries. Then, in the decision step, each filter uses the gathered information to decide about each TU. At the end of this process, for each 3 The tool has been recently used also in the unsupervised approach by Jalili Sabet et al. (2016). 51 models can be trained on the whole TM with one of the many existing word aligners. For instance, the results of WE filters reported in §4 were obtained using MGIZA++ (Gao and Vogel, 2008). TU the policy manager collects all the decisions taken by the filters and applies the policy set by the user in the configuration file to assign an accept or reject judgment. The final labels, the TUs and the filters outputs are saved in different files. 3.2 Word embedding filters (5). Cross-lingual word embeddings provide a common vector representation for words in different languages and allow looking at the source and target segments at the same time. In TMop, they are computed using the method proposed in (Søgaard et al., 2015) but, instead of considering bilingual documents as atomic co"
P16-4009,2014.tc-1.10,0,0.0168299,"des. Advanced CAT tools currently integrate the strengths of two complementary technologies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as"
P16-4009,W15-4905,0,0.0648637,"Missing"
P16-4009,P16-2047,1,0.847076,"the learning step, each filter i iterates over the TM or a subset of it to gather the basic statistics needed to define its accept/reject criteria. For instance, by computing mean and standard deviation values for a given indicator (e.g. sentence length ratio, proportion of aligned words), quantiles or std counts in case of normal value distributions will be used as decision boundaries. Then, in the decision step, each filter uses the gathered information to decide about each TU. At the end of this process, for each 3 The tool has been recently used also in the unsupervised approach by Jalili Sabet et al. (2016). 51 models can be trained on the whole TM with one of the many existing word aligners. For instance, the results of WE filters reported in §4 were obtained using MGIZA++ (Gao and Vogel, 2008). TU the policy manager collects all the decisions taken by the filters and applies the policy set by the user in the configuration file to assign an accept or reject judgment. The final labels, the TUs and the filters outputs are saved in different files. 3.2 Word embedding filters (5). Cross-lingual word embeddings provide a common vector representation for words in different languages and allow looking"
P16-4009,P12-3005,0,0.0385588,"offered by commercial TM cleaning tools. They capture translation quality by looking at surface aspects, such as the possible mismatches in the number of dates, numbers, URLs, XML tags, ref and image tags present in the source and target segments. Other filters model the similarity between source and target by computing the direct and inverse ratio between the number of characters and words, as well as the average word length in the two segments. Finally, two filters look for uncommon character or word repetitions. Language identification filter (1). This filter (LI) exploits the Langid tool (Lui and Baldwin, 2012) to verify the consistency between the source and target languages of a TU and those indicated in the TM. Though simple, it is quite effective since often the two languages are inverted or even completely different from the expected ones. 3.3 Policies Decision policies allow TMop combining the output of the active filters into a final decision for each TU. Simple decision-making strategies can consider the number of accept and reject judgments, but more complex methods can be easily implemented by the user (both filters and policy managers can be easily modified and extended by exploiting well"
P16-4009,P12-3000,0,0.28566,"Missing"
P16-4009,W15-5202,0,0.349279,"ion for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classification task, this approach highly depends on the availability of labelled training data. Our work goes beyond the initial effort of Barbu (2015) in two ways. First, we propose a configurable and extensible open source framework for TM cleaning. In this way, we address the demand of easy-to-use TM management tools whose development is out of the reach of individual translators and translation companies. Such demand is not onl"
P16-4009,W12-3122,1,0.840455,"ning methods (e.g. Apsic X-Bench1 ) only implement very simple syntactic checks (e.g. repetitions, opening/closing tags consistency). These are insufficient to capture the variety of errors that can be encountered in a TM (especially in the public ones). Second, our approach to TM cleaning is fully unsupervised. This is to cope with the lack of labelled training data which, due to the high acquisition costs, represents a bottleneck rendering supervised solutions unpractical. It is worth remarking that also current approaches to tasks closely related to TM cleaning (e.g. MT quality estimation (Mehdad et al., 2012; C. de Souza et al., 2014)) suffer from the same problem. Besides not being customised for the specificities of the TM cleaning scenario (their usefulness for the task should be demonstrated), their dependence on labelled 1 training data is a strong requirement from the TM cleaning application perspective. 2 The TM cleaning task The identification of “bad” TUs is a multifaceted problem. First, it deals with the recognition of a variety of errors. These include: • Surface errors, such as opening/closing tags inconsistencies and empty or suspiciously long/short translations; • Language inconsis"
P16-4009,E14-1022,0,0.0268135,"ols currently integrate the strengths of two complementary technologies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classific"
P16-4009,P15-1165,0,0.122512,"Missing"
P16-4009,W15-4920,0,0.0347765,"trengths of two complementary technologies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classification task, this approach highly depend"
P16-4009,2015.eamt-1.6,0,\N,Missing
R09-1056,W07-1401,0,0.160138,"Missing"
R09-1056,P06-1114,0,0.0247151,"he use of TE as a mechanism for approximating the types of inference needed for QA. However, the QA subtasks addressed up to date (answer validation and ranking) differ completely from the problem discussed in the present work (question analysis). For instance, both in the Pascal-RTE Challenge, and in the CLEFAVE task [10], the QA problem is modeled considering a question Q turned into an affirmative sentence as the hypothesis, and a text passage containing a candidate answer A as the text (i.e. systems have to decide whether A supports, or entails, Q). The same perspective is also adopted in [7], where TE is applied to filter and rank candidate answers returned by a QA system. While the application of TE for extracting relations in a given question is not documented in the QA field, similarities with our approach can be found in the RE area. Relation Extraction. The most similar approach based on TE is described in [12], which reports experiments on a dataset of protein interactions. In spite of the similarities (i.e. the use of entailing templates for RE, and a syntax-based entailment checking), this approach differs from ours in several aspects. First, while [12] deals with a singl"
R09-1056,E06-1052,1,0.822024,"QA problem is modeled considering a question Q turned into an affirmative sentence as the hypothesis, and a text passage containing a candidate answer A as the text (i.e. systems have to decide whether A supports, or entails, Q). The same perspective is also adopted in [7], where TE is applied to filter and rank candidate answers returned by a QA system. While the application of TE for extracting relations in a given question is not documented in the QA field, similarities with our approach can be found in the RE area. Relation Extraction. The most similar approach based on TE is described in [12], which reports experiments on a dataset of protein interactions. In spite of the similarities (i.e. the use of entailing templates for RE, and a syntax-based entailment checking), this approach differs from ours in several aspects. First, while [12] deals with a single relation, we consider a large number of possible target relations (i.e. 75), assuming that more than one relation can appear in a given question at the same time. Second, while [12] 8 The Pascal-RTE3 dataset consists of 800 T-H pairs (the development set, which was used for training), and 800 T-H pairs (the test set, which was"
S10-1044,S07-1003,0,0.0719701,"Missing"
S10-1044,H05-1112,0,0.0718394,"Missing"
S10-1044,W09-2415,0,\N,Missing
S12-1053,W05-0909,0,0.0301332,"Missing"
S12-1053,S12-1065,0,0.0529572,"Missing"
S12-1053,S12-1102,0,0.0711613,"glish pairs. Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics. Binary entailment de7 http://www.microsofttranslator.com/ 405 cisions are then heuristically combined into single decisions. Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier. A CLTE corpus derived from the RTE-3 dataset is also used as a source of additional training material. SoftCard [pivoting, multi-class] (Jimenez et al., 2012) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity (computed with edit-distance) UAlacant [pivoting, multi-class] (Espl`a-Gomis et al., 2012) exploits translations obtained from Google Translate, Microsoft Bing translator, and the Apertium open-source MT platform (Forcada et al., 2011).8 Then, a multi-class SVM classifier is used to take entailment decisi"
S12-1053,P10-4008,1,0.237359,"classification using SVMs. HDU [hybrid, compositional] (W¨aschle and Fendrich, 2012) uses a combination of binary classifiers for each entailment direction. The classifiers use both monolingual alignment features based on METEOR (Banerjee and Lavie, 2005) alignments (translations obtained from Google Translate), and cross-lingual alignment features based on GIZA++ (Och and Ney, 2000) (word alignments learned on Europarl). ICT [pivoting, compositional] (Meng et al., 2012) adopts a pivoting method (using Google Translate and an in-house hierarchical MT system), and the open source EDITS system (Kouylekov and Negri, 2010) to calculate similarity scores between monolingual English pairs. Separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid CLTE judgments. 3 http://translate.google.com/ http://extensions.services.openoffice. org/en/taxonomy/term/233 4 404 5 6 http://www.freedict.com/ http://www.wordreference.com/ SP-EN System name BUAP spa-eng run2 celi spa-eng run2 DirRelCond3 spa-eng run4 FBK spa-eng run3 HDU spa-eng run2 ICT spa-eng run1 JU-CSE-NLP spa-eng run1 Sagan spa-eng run3 SoftCard spa-eng run1 UAlacant spa-eng run1 LATE AVG. P 0,337"
S12-1053,S12-1104,0,0.0962891,"ignment tools, part-of-speech taggers, NP chunkers, named entity recognizers, stemmers, stopwords lists, and Wikipedia as an external multilingual corpus. More in detail: BUAP [pivoting, compositional] (Vilari˜no et al., 2012) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (Google Translate3 and the OpenOffice Thesaurus4 ). Similarity measures (e.g. Jaccard index) and rules are respectively used to annotate the two resulting sentence pairs with entailment judgments and combine them in a single decision. CELI [cross lingual, compositional & multiclass] (Kouylekov, 2012) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. Word overlap and similarity measures are then used in different approaches to the task. In one run (Run 1), they are used to train a classifier that assigns separate entailment judgments for each direction. Such judgments are finally composed into a single one for each pair. In the other runs, the same features are used for multi-class classification. DirRelCond3 [cross lingual, compositional] (Perini, 2012) uses bilingual dictionaries (Freedict5 and WordReference6 ) to translate conten"
S12-1053,N10-1045,1,0.464787,"erence over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved. 1 Yashar Mehdad FBK-irst Trento, Italy mehdad@fbk.eu Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated. The great potential for integrating monolingual TE recognition components into NLP architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization. However, mainly due to the absence of cross"
S12-1053,P11-1134,1,0.480581,"Missing"
S12-1053,S12-1105,1,0.885932,"Missing"
S12-1053,P12-2024,1,0.450645,"Missing"
S12-1053,W12-3122,1,0.873098,"Missing"
S12-1053,S12-1108,0,0.0711563,"Missing"
S12-1053,D11-1062,1,0.47997,"is task, both T1 and T2 are assumed to be true statements. Although contradiction is relevant from an application-oriented perspective, contradictory pairs are not present in the dataset created for the first round of the task. 3 Dataset description Four CLTE corpora have been created for the following language combinations: Spanish/English (SP-EN), Italian/English (IT-EN), French/English (FR-EN), German/English (DE-EN). The datasets are released in the XML format shown in Figure 1. 3.1 Data collection and annotation The dataset was created following the crowdsourcing methodology proposed in (Negri et al., 2011), which consists of the following steps: 1. First, English sentences were manually extracted from copyright-free sources (Wikipedia and Wikinews). The selected sentences represent one of the elements (T1) of each entailment pair; 2. Next, each T1 was modified through crowdsourcing in various ways in order to obtain a corresponding T2 (e.g. introducing meaning-preserving lexical and syntactic changes, adding and removing portions of text); 3. Each T2 was then paired to the original T1, and the resulting pairs were annotated with one of the four entailment judgments. In order to reduce the corre"
S12-1053,S12-1103,0,0.0191844,"1 0,076 0,201 0,568 0,332 F1 0,235 0,397 0,469 0,644 0,521 0,379 0,247 0,625 0,440 No entailment P R F1 0,344 0,688 0,459 0,339 0,312 0,325 0,367 0,320 0,342 0,540 0,488 0,513 0,390 0,512 0,443 0,315 0,560 0,403 0,405 0,600 0,484 0,521 0,488 0,504 0,403 0,496 0,434 Bidirectional P R F1 0,364 0,288 0,321 0,319 0,288 0,303 0,298 0,312 0,305 0,524 0,520 0,522 0,439 0,552 0,489 0,233 0,080 0,119 0,443 0,344 0,387 0,496 0,504 0,500 0,390 0,361 0,368 Table 4: precision, recall and F1 scores, calculated for each team’s best run for all the language combinations. JU-CSE-NLP [pivoting, compositional] (Neogi et al., 2012) uses Microsoft Bing translator7 to produce monolingual English pairs. Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics. Binary entailment de7 http://www.microsofttranslator.com/ 405 cisions are then heuristically combined into single decisions. Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier. A CLTE corpus derived from the RTE-3 dataset is also used as a source of additi"
S12-1053,P00-1056,0,0.151501,"Missing"
S12-1053,S12-1107,0,0.0689288,"Missing"
S12-1053,S12-1106,0,0.0276864,"Missing"
S12-1053,S12-1064,0,0.250345,"Missing"
S12-1092,W05-0909,0,0.141398,"ay with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was"
S12-1092,carreras-etal-2004-freeling,0,0.0242724,"Missing"
S12-1092,P05-1022,0,0.0149797,"cessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching (STM) metric proposed by (Liu and Gildea, 2005). Dependency trees were obtained using MINIPAR (Lin, 2003). Two types of metrics were used to calculate the similarity between two texts using dependency trees. In the first, different similarity measures were calculated taking into consideration three"
S12-1092,P04-1014,0,0.0233595,"sed: overlapping according to the part-of-speech; overlapping according to the chunk type; the accumulated NIST metric (Doddington, 2002) scores over different Figure 1: A summary of the class of features explored. sequences (lemmas, parts-of-speech, base phrase chunks and chunk IOB labels). 2.1.3 Semantic Level At the semantic level we aplored three different types of information, namely: discourse representations, named entities and semantic roles. Hereafter they are respectively referred to as dr, ne, and sr features. The discourse relations are automatically annotated using the C&C Tools (Clark and Curran, 2004). The following metrics using semantic tree representations were proposed by (Gim´enez, 2008). A metric similar to the STM in which semantic trees are used instead of constituency trees; the overlapping between discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type. Named entities metrics are calculated by comparing the entities that appear in each text. The named entities were annotated using the BIOS package (Surdeanu et al., 2005). Two types of metrics were used: the overlapping between"
S12-1092,I05-5003,0,0.311872,"Missing"
S12-1092,gimenez-marquez-2004-svmtool,0,0.0159207,"Missing"
S12-1092,J09-4007,0,0.0312577,"the available thesauri. In order to increase the coverage we extracted concepts from the YAGO2 semantic knowledge base (Hoffart et al., 2011) derived from Wikipedia, Wordnet (Miller, 1995) and Geonames3 . YAGO2 contains knowledge about 10 million entities and more than 120 million facts about these entities. In order to link the entities in the text to the entities in YAGO2 we have used “The Wiki Machine” (TWM) tool4 . The tool solves the linking problem by disambiguating each entity mention in the text (excluding pronouns) using Wikipedia to provide the sense inventory and the training data (Giuliano et al., 2009). After preprocessing the datasets with TWM the entities are annotated with their respective Wikipedia entries represented by their URLs. Using the entity’s URL it is possible to retrieve the Wordnet synsets related to the entity’s entry in YAGO2 and explore different knowledge-based metrics to compute word similarity between entities. In our experiments we selected three different algorithms to calculate word similarity using YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994), the Leacock-Chodorow (Leacock et al., 1998) and 2 http://hlt.fbk.eu/en/technology/jlsi http://www.geonames.org/ 4 http://the"
S12-1092,kouylekov-etal-2010-mining,1,0.843845,": overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As some previous work on semantic textual textual similarity (Mihalcea et al., 2006) and textual entailment (Kouylekov et al., 2010; Mehdad et al., 2010) have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexically different. We measure the word similarity computing a set of Latent Semantic Analysis (LSA) metrics over Wikipedia. The 200,000 most visited articles of Wikipedia were extracted and cleaned to build the term-by-document matrix using the jLSI tool2 . Using this model we designed three different similarity metrics that compute the similarity between all elements in one text with all elements in the other text. For two metrics we"
S12-1092,J98-1006,0,0.05943,"Missing"
S12-1092,P04-1077,0,0.123706,"shared by both texts. Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we he"
S12-1092,W05-0904,0,0.0225841,"a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching (STM) metric proposed by (Liu and Gildea, 2005). Dependency trees were obtained using MINIPAR (Lin, 2003). Two types of metrics were used to calculate the similarity between two texts using dependency trees. In the first, different similarity measures were calculated taking into consideration three different perspectives: overlap of words that hang in the same level or in a deeper level of the dependency tree; overlap between words that hang directly from terminal nodes given a specified partof-speech; and overlap between words that are ruled by non-terminal nodes given a specified grammatical relation (subject, object, relative clause, am"
S12-1092,N10-1146,1,0.850476,"e semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As some previous work on semantic textual textual similarity (Mihalcea et al., 2006) and textual entailment (Kouylekov et al., 2010; Mehdad et al., 2010) have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexically different. We measure the word similarity computing a set of Latent Semantic Analysis (LSA) metrics over Wikipedia. The 200,000 most visited articles of Wikipedia were extracted and cleaned to build the term-by-document matrix using the jLSI tool2 . Using this model we designed three different similarity metrics that compute the similarity between all elements in one text with all elements in the other text. For two metrics we calculate the similar"
S12-1092,N03-2021,0,0.0353569,"tching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Synt"
S12-1092,niessen-etal-2000-evaluation,0,0.0728532,"ty, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part"
S12-1092,P02-1040,0,0.106563,"f items inside the linguistic elements of a certain type shared by both texts. Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). Th"
S12-1092,2006.amta-papers.25,0,0.0383817,"ollowing n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency"
S12-1092,W05-0635,0,0.0326889,"tween discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type. Named entities metrics are calculated by comparing the entities that appear in each text. The named entities were annotated using the BIOS package (Surdeanu et al., 2005). Two types of metrics were used: the overlapping between the named entities in each sentence according to their type and the matching between the named entities in function of their type. Semantic roles were automatically annotated us626 ing the SwiRL package (Surdeanu and Turmo, 2005). The arguments and adjuncts annotated in each sentence are compared according to three different metrics: overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As s"
S12-1092,U06-1019,0,0.047547,"Missing"
S12-1092,P94-1019,0,0.286357,"Missing"
S12-1092,S12-1051,0,\N,Missing
S12-1105,carreras-etal-2004-freeling,0,0.0542349,"Missing"
S12-1105,P07-2045,0,0.00682366,"been identified as exact matches are not considered. 702 M atchn |H(n)| (1) In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 Shared Translation Task.2 We run the TreeTagger (Schmid, 1995) and Snowball stemmer (Porter, 2001) for preprocessing, and used the Giza++ (Och and Ney, 2000) toolkit to align the tokenized corpora at the word level. Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). 2. Dependency Relation (DR) matching targets the increase of CLTE precision. By adding syntactic constraints to the matching process, DR features aim to reduce wrong matches often occurring at the lexical level. For instance, the contradiction between “Yahoo acquired Overture” and “Overture compr´o Yahoo” is evident when syntax (in this case subject-object inversion) is taken into account, but can not be caught by bag-of-words methods. We define a dependency relation as a triple that connects pairs of words through a grammatical relation. For example, “nsubj (loves, John)” is a dependency re"
S12-1105,N10-1045,1,0.802174,"Missing"
S12-1105,P11-1134,1,0.779175,"nslation evaluation datasets (Mehdad et al., 2012b). The content synchronization task represents a challenging application scenario to test the capabilities of CLTE systems, by proposing a richer inventory of phenomena (i.e. “Bidirectional”/“Forward”/“Backward”/“No entailment” multi-directional entailment relations). The CLTE methods proposed so far adopt either a “pivoting approach” (translation of the two input texts into the same language, as in (Mehdad et al., 2010)), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, in our participation, we propose an approach that combines lexical, syntactic and semantic features within a machine learning framework (Mehdad et al., 2012a). Our submitted runs have been produced by training and optimizing multiclass and binary SVM classifiers, over the Spanish-English (Spa-Eng) development set. In both cases, our results were positive, showing signif"
S12-1105,P12-2024,1,0.753336,"Missing"
S12-1105,W12-3122,1,0.893173,"Missing"
S12-1105,W10-0734,1,0.902251,"Missing"
S12-1105,D11-1062,1,0.818954,"significant improvements over the median systems and average scores obtained by participants. The overall results confirm the difficulty of the task, and the potential of our approach in combining linguistically motivated features in a “pure” cross-lingual approach that avoids the recourse to external MT components. 701 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701–705, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Experiments In our experiment we used the Spa-Eng portion of the dataset described in (Negri et al., 2012; Negri et al., 2011), consisting of 500 multi-directional entailment pairs which was provided to train the systems and 500 pairs for the submission. Each pair in the dataset is annotated with “Bidirectional”, “Forward”, “Backward” or “No entailment” judgements. 2.1 Once the matching phase for each n-gram level has been concluded, the number of matches M atchn and the number of phrases in the hypothesis H(n) is used to estimate the portion of phrases in H that are matched at each level n (Equation 1).1 Since languages can express the same meaning with different amounts of words, a phrase with length n in H can mat"
S12-1105,S12-1053,1,0.839505,"e positive, showing significant improvements over the median systems and average scores obtained by participants. The overall results confirm the difficulty of the task, and the potential of our approach in combining linguistically motivated features in a “pure” cross-lingual approach that avoids the recourse to external MT components. 701 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701–705, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Experiments In our experiment we used the Spa-Eng portion of the dataset described in (Negri et al., 2012; Negri et al., 2011), consisting of 500 multi-directional entailment pairs which was provided to train the systems and 500 pairs for the submission. Each pair in the dataset is annotated with “Bidirectional”, “Forward”, “Backward” or “No entailment” judgements. 2.1 Once the matching phase for each n-gram level has been concluded, the number of matches M atchn and the number of phrases in the hypothesis H(n) is used to estimate the portion of phrases in H that are matched at each level n (Equation 1).1 Since languages can express the same meaning with different amounts of words, a phrase with"
S12-1105,P00-1056,0,0.664839,"of failure with exact matching, lexical matching is performed at the same three levels. To reduce redundant matches, the lexical matches between pairs of phrases which have already been identified as exact matches are not considered. 702 M atchn |H(n)| (1) In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 Shared Translation Task.2 We run the TreeTagger (Schmid, 1995) and Snowball stemmer (Porter, 2001) for preprocessing, and used the Giza++ (Och and Ney, 2000) toolkit to align the tokenized corpora at the word level. Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). 2. Dependency Relation (DR) matching targets the increase of CLTE precision. By adding syntactic constraints to the matching process, DR features aim to reduce wrong matches often occurring at the lexical level. For instance, the contradiction between “Yahoo acquired Overture” and “Overture compr´o Yahoo” is evident when syntax (in this case subject-object inversion) is taken into account, but can not be caught"
S13-2005,S13-2024,0,0.0219644,"Missing"
S13-2005,S13-2006,0,0.030069,"ta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measures, and syntactic information. SoftCard [pivoting, multi-class] (Jimenez et al., 2013) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity. Umelb [cross-lingual, pivoting, compositional] (Graham et al., 2013) adopts both pivoting and cross-lingual approaches. For the latter, GIZA++ was used to compute word alignments between the input sentences. Word alignment features are used to train binary SVM classifiers whose decisions are eventually c"
S13-2005,P10-4008,1,0.522467,"using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measure"
S13-2005,S13-2099,0,0.0105521,"ned into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two senten"
S13-2005,N10-1045,1,0.669554,"nce over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (six teams, 61 runs), the approaches adopted and the results achieved. 1 Yashar Mehdad UBC Vancouver, Canada mehdad@cs.ubc.ca Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Given two texts in different languages, the cross-lingual textual entailment (CLTE) task consists of deciding if the meaning of one text can be inferred from the meaning of the other text. Crosslinguality represents an interesting direction for research on recognizing textual entailment (RTE), especially due to its possible application in a variety of tasks. Among others (e.g. question answering, i"
S13-2005,P11-1134,1,0.635365,"about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other. The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task. In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as features contributing to the entailment decision), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE (Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics fragments entail each other (semantic equivalence); • forward (T1→T2 & T16←T2): unidirectional entailment from T1 to T2; • backward (T16→T2 & T1←T2): unidirectional entailment from T2 to T1; • no entailment (T16→T2 & T16←T2): there is no entailment between T1 and T2 in either direction; Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012"
S13-2005,P12-2024,1,0.83509,"Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics fragments entail each other (semantic equivalence); • forward (T1→T2 & T16←T2): unidirectional entailment from T1 to T2; • backward (T16→T2 & T1←T2): unidirectional entailment from T2 to T1; • no entailment (T16→T2 & T16←T2): there is no entailment between T1 and T2 in either direction; Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012). However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as “perro”→“animal”). Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional). Thanks to the contiguity between CLTE, TE and SMT,"
S13-2005,W10-0734,1,0.383734,"Missing"
S13-2005,D11-1062,1,0.7367,"n/English (DE-EN). Each corpus consists of 1,500 sentence pairs (1,000 for training and 500 for test), balanced across the four entailment judgements. In this year’s evaluation, as training set we used the CLTE-2012 corpus1 that was created for the SemEval-2012 evaluation exercise2 (including both training and test sets). The CLTE-2013 test set was created from scratch, following the methodology described in the next section. 3.1 To collect the entailment pairs for the 2013 test set we adopted a slightly modified version of the crowdsourcing methodology followed to create the CLTE2012 corpus (Negri et al., 2011). The main difference with last year’s procedure is that we did not take advantage of crowdsourcing for the whole data collection process, but only for part of it. As for CLTE-2012, the collection and annotation process consists of the following steps: 1. First, English sentences were manually extracted from Wikipedia and Wikinews. The selected sentences represent one of the elements (T1) of each entailment pair; 1 • bidirectional (T1→T2 & T1←T2): the two 26 Data collection and annotation 2 http://www.celct.it/resources.php?id page=CLTE http://www.cs.york.ac.uk/semeval-2012/task8/ 2. Next, eac"
S13-2005,S12-1053,1,0.323391,"Missing"
S13-2005,J03-1002,0,0.00871819,"a-classification strategies have been proposed. 31 Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers)"
S13-2005,S13-2023,1,0.84219,"ls. Regarding the latter dimension, in addition to compositional and multi-class strategies, also alternative solutions that leverage more sophisticated meta-classification strategies have been proposed. 31 Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or i"
S13-2005,S13-2022,0,0.0210187,"Missing"
S13-2005,negri-etal-2012-chinese,1,\N,Missing
S13-2023,J93-2003,0,0.025223,"ers is combined to form the four valid entailment decisions: • forward and backward classifier output true: “bidirectional” entailment; • forward is true and backward is false: “forward” entailment; • forward is false and backward is true: “backward” entailment; • both forward and backward output false: “no entailment” relation. Both binary classifiers were implemented using the SVM implementation of Weka (Hall et al., 2009). 3 Experiments In our submission we experimented with three standard word alignment algorithms: the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993). They are implemented in the MGIZA++ package (Gao and Vogel, 2008). Building on a probabilistic lexical model to establish mappings between words in two languages, these models compute alignments between the word positions in two input sentences S1 and S2 . The models are trained incrementally: HMM is the base for IBM model 3, which is the base for IBM model 4. To train our models, we used 5 iterations of HMM, and 3 iterations of IBM models 3 and 4. Word alignments produced by these models are asymmetric (S1 → S2 6= S2 → S1 ). To cope with this, different heuristics (Koehn et al., 2005) have"
S13-2023,S12-1065,0,0.0273764,"Missing"
S13-2023,W08-0509,0,0.0175316,"ward and backward classifier output true: “bidirectional” entailment; • forward is true and backward is false: “forward” entailment; • forward is false and backward is true: “backward” entailment; • both forward and backward output false: “no entailment” relation. Both binary classifiers were implemented using the SVM implementation of Weka (Hall et al., 2009). 3 Experiments In our submission we experimented with three standard word alignment algorithms: the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993). They are implemented in the MGIZA++ package (Gao and Vogel, 2008). Building on a probabilistic lexical model to establish mappings between words in two languages, these models compute alignments between the word positions in two input sentences S1 and S2 . The models are trained incrementally: HMM is the base for IBM model 3, which is the base for IBM model 4. To train our models, we used 5 iterations of HMM, and 3 iterations of IBM models 3 and 4. Word alignments produced by these models are asymmetric (S1 → S2 6= S2 → S1 ). To cope with this, different heuristics (Koehn et al., 2005) have been proposed to obtain symmetric alignments from two asymmetric se"
S13-2023,2005.iwslt-1.8,0,0.0334312,"nd 4 (Brown et al., 1993). They are implemented in the MGIZA++ package (Gao and Vogel, 2008). Building on a probabilistic lexical model to establish mappings between words in two languages, these models compute alignments between the word positions in two input sentences S1 and S2 . The models are trained incrementally: HMM is the base for IBM model 3, which is the base for IBM model 4. To train our models, we used 5 iterations of HMM, and 3 iterations of IBM models 3 and 4. Word alignments produced by these models are asymmetric (S1 → S2 6= S2 → S1 ). To cope with this, different heuristics (Koehn et al., 2005) have been proposed to obtain symmetric alignments from two asymmetric sets (S1 ↔ S2 ). We experimented with three symmetrization heuristics, namely: union, intersection, and grow-diag-finaland, a more complex symmetrization method which combines intersection with some alignments from the union. To train the word alignment models we used the Europarl parallel corpus (Koehn, 2005) concatenated with the News Commentary corpus2 for 2 http://www.statmt.org/wmt11/ translation-task.html#download 130 three language pairs: English-German (2,079,049 sentences), English-Spanish (2,123,036 sentences), En"
S13-2023,2005.mtsummit-papers.11,0,0.0442747,"To train our models, we used 5 iterations of HMM, and 3 iterations of IBM models 3 and 4. Word alignments produced by these models are asymmetric (S1 → S2 6= S2 → S1 ). To cope with this, different heuristics (Koehn et al., 2005) have been proposed to obtain symmetric alignments from two asymmetric sets (S1 ↔ S2 ). We experimented with three symmetrization heuristics, namely: union, intersection, and grow-diag-finaland, a more complex symmetrization method which combines intersection with some alignments from the union. To train the word alignment models we used the Europarl parallel corpus (Koehn, 2005) concatenated with the News Commentary corpus2 for 2 http://www.statmt.org/wmt11/ translation-task.html#download 130 three language pairs: English-German (2,079,049 sentences), English-Spanish (2,123,036 sentences), English-French (2,144,820 sentences). For EnglishItalian we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. For our submitted run the SVM classifiers were trained using the whole training set. Such dataset consists of 1,000 pairs for each of the four language combinations, resulting from"
S13-2023,P10-4008,1,0.831231,"ndrich, 2012), MT of subsegments (Espl`a-Gomis et al., 2012), or semantic Wordnets (Castillo, 2011). In this work we propose a CLTE detection method based on a new set of features using word alignment as a source of cross-lingual knowledge. This set, which is richer than the one by (W¨aschle and Fendrich, 2012), is aimed not only at grasping information about the proportion of aligned words, but also about the distribution of the alignments in both 1 In the first CLTE evaluation round at Semeval 2012, for instance, the system described in (Meng et al., 2012) used the open source EDITS system (Kouylekov and Negri, 2010; Negri et al., 2009) to calculate similarity scores between monolingual English pairs. 128 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 128–132, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics H and T . This set of features is later used by two support vector machine (SVM) classifiers for detecting CLTE separately in both directions (T → H and T ← H). We use the combined output of both classifiers for performing the CLTE detection. The paper is o"
S13-2023,S12-1104,0,0.0133377,"itation of CLTE components and the integration of semantics and machine translation (MT) technology (Mehdad et al., 2011; Mehdad et al., 2012b; Bronner et al., 2012; Monz et al., 2011). In the last few years, several methods have been proposed for CLTE. These can be roughly divided in two main groups (Negri et al., 2012): i) those using a pivoting strategy by translating H into the language of T and then using monolingual TE components1 , and those directly using cross-lingual strategies. Among this second group, several sources of cross-lingual knowledge have been used, such as dictionaries (Kouylekov et al., 2012; Perini, 2012), phrase and paraphrase tables (Mehdad et al., 2012a), GIZA++ (Och and Ney, 2003) word alignment models (W¨aschle and Fendrich, 2012), MT of subsegments (Espl`a-Gomis et al., 2012), or semantic Wordnets (Castillo, 2011). In this work we propose a CLTE detection method based on a new set of features using word alignment as a source of cross-lingual knowledge. This set, which is richer than the one by (W¨aschle and Fendrich, 2012), is aimed not only at grasping information about the proportion of aligned words, but also about the distribution of the alignments in both 1 In the fir"
S13-2023,N10-1045,1,0.911032,"Missing"
S13-2023,P11-1134,1,0.87568,"ted as a multi-class classification problem in which there are four possible relations between T and H: forward (T → H), backward (T ← H), bidirectional (T ↔ H) and “no entailment”. Targeting the identification of semantic equivalence and information disparity between topically related sentences, CLTE recognition can be seen as a core task for a number of cross-lingual applications. Among others, multilingual content synchronization has been recently proposed as an ideal framework for the exploitation of CLTE components and the integration of semantics and machine translation (MT) technology (Mehdad et al., 2011; Mehdad et al., 2012b; Bronner et al., 2012; Monz et al., 2011). In the last few years, several methods have been proposed for CLTE. These can be roughly divided in two main groups (Negri et al., 2012): i) those using a pivoting strategy by translating H into the language of T and then using monolingual TE components1 , and those directly using cross-lingual strategies. Among this second group, several sources of cross-lingual knowledge have been used, such as dictionaries (Kouylekov et al., 2012; Perini, 2012), phrase and paraphrase tables (Mehdad et al., 2012a), GIZA++ (Och and Ney, 2003) w"
S13-2023,S12-1105,1,0.868221,"Missing"
S13-2023,P12-2024,1,0.814384,"classification problem in which there are four possible relations between T and H: forward (T → H), backward (T ← H), bidirectional (T ↔ H) and “no entailment”. Targeting the identification of semantic equivalence and information disparity between topically related sentences, CLTE recognition can be seen as a core task for a number of cross-lingual applications. Among others, multilingual content synchronization has been recently proposed as an ideal framework for the exploitation of CLTE components and the integration of semantics and machine translation (MT) technology (Mehdad et al., 2011; Mehdad et al., 2012b; Bronner et al., 2012; Monz et al., 2011). In the last few years, several methods have been proposed for CLTE. These can be roughly divided in two main groups (Negri et al., 2012): i) those using a pivoting strategy by translating H into the language of T and then using monolingual TE components1 , and those directly using cross-lingual strategies. Among this second group, several sources of cross-lingual knowledge have been used, such as dictionaries (Kouylekov et al., 2012; Perini, 2012), phrase and paraphrase tables (Mehdad et al., 2012a), GIZA++ (Och and Ney, 2003) word alignment models"
S13-2023,S12-1108,0,0.0167191,"and Ney, 2003) word alignment models (W¨aschle and Fendrich, 2012), MT of subsegments (Espl`a-Gomis et al., 2012), or semantic Wordnets (Castillo, 2011). In this work we propose a CLTE detection method based on a new set of features using word alignment as a source of cross-lingual knowledge. This set, which is richer than the one by (W¨aschle and Fendrich, 2012), is aimed not only at grasping information about the proportion of aligned words, but also about the distribution of the alignments in both 1 In the first CLTE evaluation round at Semeval 2012, for instance, the system described in (Meng et al., 2012) used the open source EDITS system (Kouylekov and Negri, 2010; Negri et al., 2009) to calculate similarity scores between monolingual English pairs. 128 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 128–132, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics H and T . This set of features is later used by two support vector machine (SVM) classifiers for detecting CLTE separately in both directions (T → H and T ← H). We use the combined output of both"
S13-2023,D11-1062,1,0.871284,"language pairs: English-German (2,079,049 sentences), English-Spanish (2,123,036 sentences), English-French (2,144,820 sentences). For EnglishItalian we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. For our submitted run the SVM classifiers were trained using the whole training set. Such dataset consists of 1,000 pairs for each of the four language combinations, resulting from a concatenation of the training and test sets used for the first round of evaluation at SemEval 2012 (Negri et al., 2012; Negri et al., 2011). We have set a polynomial kernel with parameters empirically estimated on the training set: C = 2.0, and d = 1. After some preliminary experiments we have concluded that the HMM model in conjunction with the intersection symmetrization provides the best results. Our results, calculated over the 500 test pairs provided for each language combination, are presented in Table 3. As can be seen from the table, our system consistently outperforms the best average run of all participants and is the second best system for Spanish/English and Italian/English. For the other two languages, French/English"
S13-2023,S12-1053,1,0.848557,"Missing"
S13-2023,S13-2005,1,0.889491,"Missing"
S13-2023,J03-1002,0,0.0132356,"(Mehdad et al., 2011; Mehdad et al., 2012b; Bronner et al., 2012; Monz et al., 2011). In the last few years, several methods have been proposed for CLTE. These can be roughly divided in two main groups (Negri et al., 2012): i) those using a pivoting strategy by translating H into the language of T and then using monolingual TE components1 , and those directly using cross-lingual strategies. Among this second group, several sources of cross-lingual knowledge have been used, such as dictionaries (Kouylekov et al., 2012; Perini, 2012), phrase and paraphrase tables (Mehdad et al., 2012a), GIZA++ (Och and Ney, 2003) word alignment models (W¨aschle and Fendrich, 2012), MT of subsegments (Espl`a-Gomis et al., 2012), or semantic Wordnets (Castillo, 2011). In this work we propose a CLTE detection method based on a new set of features using word alignment as a source of cross-lingual knowledge. This set, which is richer than the one by (W¨aschle and Fendrich, 2012), is aimed not only at grasping information about the proportion of aligned words, but also about the distribution of the alignments in both 1 In the first CLTE evaluation round at Semeval 2012, for instance, the system described in (Meng et al., 20"
S13-2023,S12-1107,0,0.0128434,"ts and the integration of semantics and machine translation (MT) technology (Mehdad et al., 2011; Mehdad et al., 2012b; Bronner et al., 2012; Monz et al., 2011). In the last few years, several methods have been proposed for CLTE. These can be roughly divided in two main groups (Negri et al., 2012): i) those using a pivoting strategy by translating H into the language of T and then using monolingual TE components1 , and those directly using cross-lingual strategies. Among this second group, several sources of cross-lingual knowledge have been used, such as dictionaries (Kouylekov et al., 2012; Perini, 2012), phrase and paraphrase tables (Mehdad et al., 2012a), GIZA++ (Och and Ney, 2003) word alignment models (W¨aschle and Fendrich, 2012), MT of subsegments (Espl`a-Gomis et al., 2012), or semantic Wordnets (Castillo, 2011). In this work we propose a CLTE detection method based on a new set of features using word alignment as a source of cross-lingual knowledge. This set, which is richer than the one by (W¨aschle and Fendrich, 2012), is aimed not only at grasping information about the proportion of aligned words, but also about the distribution of the alignments in both 1 In the first CLTE evaluat"
S13-2023,C96-2141,0,0.408434,"ailment (T ← H). The output of both classifiers is combined to form the four valid entailment decisions: • forward and backward classifier output true: “bidirectional” entailment; • forward is true and backward is false: “forward” entailment; • forward is false and backward is true: “backward” entailment; • both forward and backward output false: “no entailment” relation. Both binary classifiers were implemented using the SVM implementation of Weka (Hall et al., 2009). 3 Experiments In our submission we experimented with three standard word alignment algorithms: the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993). They are implemented in the MGIZA++ package (Gao and Vogel, 2008). Building on a probabilistic lexical model to establish mappings between words in two languages, these models compute alignments between the word positions in two input sentences S1 and S2 . The models are trained incrementally: HMM is the base for IBM model 3, which is the base for IBM model 4. To train our models, we used 5 iterations of HMM, and 3 iterations of IBM models 3 and 4. Word alignments produced by these models are asymmetric (S1 → S2 6= S2 → S1 ). To cope with this, dif"
S13-2023,S12-1064,0,0.0917177,"Missing"
S16-1086,S12-1051,0,0.0463632,"ion An ET regressor is used as the learning method in the system. The ET regressor applies bagging to generate a number of random subsets of the training data and fits individual decision trees using different subsets of features and hyper parameters. The final prediction is produced by an ensemble average over all of the decision trees. 3 Experiments 3.1 Corpus For this first round of the CL-STS task, training data was not released by the organizers. Therefore, the data for training the regressor was generated using those from the monolingual STS tasks organized in 2012, 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). The data was translated to Spanish using the MateCat tool (Federico et al., 2014) to create a cross-lingual data set where one of the sentences is in English and the other is in Spanish. In order to compensate for the different characteristics of data which occur after translation, we generate three different sets as follows: 1. The first sentence in the data set is selected as the English sentence, the second sentence 573 is translated to Spanish, denoted as: s1 (en) s2 (es). 2. The second sentence in the data set is selected a"
S16-1086,S13-1004,0,0.0200914,"s used as the learning method in the system. The ET regressor applies bagging to generate a number of random subsets of the training data and fits individual decision trees using different subsets of features and hyper parameters. The final prediction is produced by an ensemble average over all of the decision trees. 3 Experiments 3.1 Corpus For this first round of the CL-STS task, training data was not released by the organizers. Therefore, the data for training the regressor was generated using those from the monolingual STS tasks organized in 2012, 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). The data was translated to Spanish using the MateCat tool (Federico et al., 2014) to create a cross-lingual data set where one of the sentences is in English and the other is in Spanish. In order to compensate for the different characteristics of data which occur after translation, we generate three different sets as follows: 1. The first sentence in the data set is selected as the English sentence, the second sentence 573 is translated to Spanish, denoted as: s1 (en) s2 (es). 2. The second sentence in the data set is selected as the English sentenc"
S16-1086,S14-2010,0,0.0208797,"g method in the system. The ET regressor applies bagging to generate a number of random subsets of the training data and fits individual decision trees using different subsets of features and hyper parameters. The final prediction is produced by an ensemble average over all of the decision trees. 3 Experiments 3.1 Corpus For this first round of the CL-STS task, training data was not released by the organizers. Therefore, the data for training the regressor was generated using those from the monolingual STS tasks organized in 2012, 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). The data was translated to Spanish using the MateCat tool (Federico et al., 2014) to create a cross-lingual data set where one of the sentences is in English and the other is in Spanish. In order to compensate for the different characteristics of data which occur after translation, we generate three different sets as follows: 1. The first sentence in the data set is selected as the English sentence, the second sentence 573 is translated to Spanish, denoted as: s1 (en) s2 (es). 2. The second sentence in the data set is selected as the English sentence, the first sentence"
S16-1086,S15-2045,0,0.133944,"TS task, the cross-lingual task requires the interpretation of the semantic similarity of two crosslingual sentences, one in English and another one in Spanish, with a score ranging from 0 to 5. CLSTS measurement could be extremely useful for achieving textual entailment, paraphrase identification, word-sense disambiguation or sentiment analysis at the cross-lingual level as well as providing new means for an adequacy-oriented evaluation of machine translation outputs. Introduction Semantic textual similarity (STS) measures the degree of equivalence between the meanings of two text sequences (Agirre et al., 2015). The similarity of the text pair can be represented as a continuous or discrete-time value ranging from irrelevance to exact semantic equivalence (Agirre et al., 2015). STS has been one of the official shared tasks in SemEval since 2013 and has attracted the participation of many researchers from the scientific community; enabling the evaluation of several different approaches in natural language processing with a common benchmark and the production of novel annotated data sets that can be used in future A related task in natural language processing is quality estimation. Quality estimation ("
S16-1086,D12-1050,0,0.0359848,"n, 2005), UN (Rafalovitch et al., 2009), data sets of the quality estimation shared task in WMT 2012 (Callison-Burch et al., 2012), as well as the training data of the monolingual STS task from previous years (See Subsection 3.1) and used this data to train our bilingual embeddings. Sentence embeddings are then generated by averaging the word embeddings in each sentence. Averaging is a simple and powerful composition method for monolingual word embeddings which has not been outperformed yet by much more sophisticated schemes, such as the recurrent neural networks and long short-term memories (Blacoe and Lapata, 2012; Wieting et al., 2015). Moreover, the latter often requires language-specific syntactic parsers which are not available in all languages, thus are not generally suitable for cross-lingual applications. In our system, we implement three different averaging strategies to see the influence of stop words or term frequencies in the final sentence embedding. Our approaches consist of: 1. Averaging with all the tokens in the sentence including punctuation; 2. Averaging after removing stop words and punctuation in the sentence; Figure 1: The schematic of the overall system for CL-STS 3. Averaging by"
S16-1086,W14-3340,1,0.901107,"Missing"
S16-1086,C14-1040,1,0.903319,"Missing"
S16-1086,P15-1022,1,0.881365,"Missing"
S16-1086,W12-3102,0,0.0835412,"Missing"
S16-1086,C14-2028,1,0.844,"training data and fits individual decision trees using different subsets of features and hyper parameters. The final prediction is produced by an ensemble average over all of the decision trees. 3 Experiments 3.1 Corpus For this first round of the CL-STS task, training data was not released by the organizers. Therefore, the data for training the regressor was generated using those from the monolingual STS tasks organized in 2012, 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). The data was translated to Spanish using the MateCat tool (Federico et al., 2014) to create a cross-lingual data set where one of the sentences is in English and the other is in Spanish. In order to compensate for the different characteristics of data which occur after translation, we generate three different sets as follows: 1. The first sentence in the data set is selected as the English sentence, the second sentence 573 is translated to Spanish, denoted as: s1 (en) s2 (es). 2. The second sentence in the data set is selected as the English sentence, the first sentence is translated to Spanish, denoted as: s2 (en) s1 (es). 3. The first two data sets are concatenated, deno"
S16-1086,S15-2046,0,0.0294543,"Missing"
S16-1086,C12-1089,0,0.0265331,"onal Linguistics geared to also capture the adequacy aspects of crosslingual comparison of the sentences. In order to improve the quality of the comparison, the features used in a QE system can be improved using distributional semantics. Neural language models, such as CBOW or Skipgram (Mikolov et al., 2013a) have proved to be useful in the monolingual STS task before (Agirre et al., 2015). Recent studies have extended these models to create bilingual word embeddings such that the embeddings are mapped to a common cross-lingual vector space by using a parallel training corpus or a dictionary (Klementiev et al., 2012; Mikolov et al., 2013b; Luong et al., 2015). 2.1 In light of these considerations, our submission to the first SemEval CL-STS task combines features derived from QE with distance features obtained by applying cross-lingual word embeddings. These features are used to feed an Extremely Randomized Trees (ET) regressor (Geurts et al., 2006) trained to predict the similarity score of the two sentences. 2.2 The rest of this paper is organized as follows. Section 2 describes the components of our CL-STS system. The details of the experimental analysis carried out on different composition approaches,"
S16-1086,P07-2045,0,0.0034511,"571 Pre-processing The data used in training and testing the system are processed before feature extraction in the following way. First of all, a language identification package developed by Lui et al. (2012) is used to detect the order of English and Spanish sentences in each line of the data set. This step is meant as a first sanity check since some of the next processing steps are language-dependent and hence sensitive to the order in which the two sentences are presented. The data is then tokenized and lowercased before the extraction of features using the text processing system of Moses (Koehn et al., 2007). Bilingual Embedding Features To obtain word embeddings, we use the bilingual Skipgram model by Luong et al. (2015). The embeddings are trained using the default parameters described by the authors and with a dimension of 200. We constructed an English-Spanish parallel corpus from Europarl (Koehn, 2005), UN (Rafalovitch et al., 2009), data sets of the quality estimation shared task in WMT 2012 (Callison-Burch et al., 2012), as well as the training data of the monolingual STS task from previous years (See Subsection 3.1) and used this data to train our bilingual embeddings. Sentence embeddings"
S16-1086,2005.mtsummit-papers.11,0,0.0106296,"meant as a first sanity check since some of the next processing steps are language-dependent and hence sensitive to the order in which the two sentences are presented. The data is then tokenized and lowercased before the extraction of features using the text processing system of Moses (Koehn et al., 2007). Bilingual Embedding Features To obtain word embeddings, we use the bilingual Skipgram model by Luong et al. (2015). The embeddings are trained using the default parameters described by the authors and with a dimension of 200. We constructed an English-Spanish parallel corpus from Europarl (Koehn, 2005), UN (Rafalovitch et al., 2009), data sets of the quality estimation shared task in WMT 2012 (Callison-Burch et al., 2012), as well as the training data of the monolingual STS task from previous years (See Subsection 3.1) and used this data to train our bilingual embeddings. Sentence embeddings are then generated by averaging the word embeddings in each sentence. Averaging is a simple and powerful composition method for monolingual word embeddings which has not been outperformed yet by much more sophisticated schemes, such as the recurrent neural networks and long short-term memories (Blacoe a"
S16-1086,P12-3005,0,0.0249268,"Missing"
S16-1086,W15-1521,0,0.17356,"uacy aspects of crosslingual comparison of the sentences. In order to improve the quality of the comparison, the features used in a QE system can be improved using distributional semantics. Neural language models, such as CBOW or Skipgram (Mikolov et al., 2013a) have proved to be useful in the monolingual STS task before (Agirre et al., 2015). Recent studies have extended these models to create bilingual word embeddings such that the embeddings are mapped to a common cross-lingual vector space by using a parallel training corpus or a dictionary (Klementiev et al., 2012; Mikolov et al., 2013b; Luong et al., 2015). 2.1 In light of these considerations, our submission to the first SemEval CL-STS task combines features derived from QE with distance features obtained by applying cross-lingual word embeddings. These features are used to feed an Extremely Randomized Trees (ET) regressor (Geurts et al., 2006) trained to predict the similarity score of the two sentences. 2.2 The rest of this paper is organized as follows. Section 2 describes the components of our CL-STS system. The details of the experimental analysis carried out on different composition approaches, the characteristics of the system under the"
S16-1086,W12-3122,1,0.85152,"(Agirre et al., 2015). STS has been one of the official shared tasks in SemEval since 2013 and has attracted the participation of many researchers from the scientific community; enabling the evaluation of several different approaches in natural language processing with a common benchmark and the production of novel annotated data sets that can be used in future A related task in natural language processing is quality estimation. Quality estimation (QE) is used for automatically predicting the quality of machine translation outputs with respect to the source sentences in the original language (Mehdad et al., 2012; Turchi et al., 2014; C. de Souza et al., 2014a; C. de Souza et al., 2014b; C. de Souza et al., 2015). One shortcoming of QE approaches is that the QE system may not capture all aspects of the semantic representations of sentences. For instance, from a QE perspective, under which the number of edit operations required to fix a translation is used as a proxy of quality, a fluent translation containing an unnecessary negation would likely be labelled as a “good” translation. Therefore, a better solution would be 570 Proceedings of SemEval-2016, pages 570–576, c San Diego, California, June 16-17"
S16-1086,2009.mtsummit-posters.15,0,0.0260609,"Missing"
S16-1086,P13-4014,1,0.919686,"Missing"
S16-1086,S15-2027,0,0.104451,"val-2016 Task 1: Cross-lingual Semantic Similarity Measurement Using Quality Estimation Features and Compositional Bilingual Word Embeddings Duygu Ataman1,2 , Jos´e G. C. de Souza1,2 , Marco Turchi1 , Matteo Negri1 1 HLT - MT Fondazione Bruno Kessler, Trento, Italy 2 Department of Information Engineering and Computer Science University of Trento, Italy {ataman,desouza,turchi,negri}@fbk.eu Abstract research. State-of-the-art monolingual STS methods make use of several approaches including word alignments and distributional semantics, which are typically employed in a machine learning scenario (Sultan et al., 2015; H¨anig et al., 2015). This paper describes the system by FBK HLTMT for cross-lingual semantic textual similarity measurement. Our approach is based on supervised regression with an ensemble decision tree. In order to assign a semantic similarity score to an input sentence pair, the model combines features collected by state-of-the-art methods in machine translation quality estimation and distance metrics between crosslingual embeddings of the two sentences. In our analysis, we compare different techniques for composing sentence vectors, several distance features and ways to produce training"
S16-1086,P14-1067,1,0.898331,"Missing"
tanev-etal-2004-multilingual,P02-1054,1,\N,Missing
tanev-etal-2004-multilingual,P02-1006,0,\N,Missing
turchi-negri-2014-automatic,steinberger-etal-2006-jrc,0,\N,Missing
turchi-negri-2014-automatic,W10-1723,0,\N,Missing
turchi-negri-2014-automatic,W09-0441,0,\N,Missing
turchi-negri-2014-automatic,W12-3102,0,\N,Missing
turchi-negri-2014-automatic,P02-1040,0,\N,Missing
turchi-negri-2014-automatic,P07-2045,0,\N,Missing
turchi-negri-2014-automatic,P11-1022,0,\N,Missing
turchi-negri-2014-automatic,W13-2231,1,\N,Missing
turchi-negri-2014-automatic,W04-1013,0,\N,Missing
turchi-negri-2014-automatic,2009.eamt-1.5,1,\N,Missing
turchi-negri-2014-automatic,2011.eamt-1.12,0,\N,Missing
turchi-negri-2014-automatic,2005.mtsummit-papers.11,0,\N,Missing
turchi-negri-2014-automatic,potet-etal-2012-collection,0,\N,Missing
turchi-negri-2014-automatic,W13-2201,0,\N,Missing
turchi-negri-2014-automatic,W12-3122,1,\N,Missing
turchi-negri-2014-automatic,P10-1063,0,\N,Missing
turchi-negri-2014-automatic,W12-3123,0,\N,Missing
turchi-negri-2014-automatic,P14-1067,1,\N,Missing
turchi-negri-2014-automatic,2012.iwslt-papers.12,0,\N,Missing
W02-1109,E99-1001,0,\N,Missing
W02-1109,C96-1071,0,\N,Missing
W02-1109,A97-1030,0,\N,Missing
W02-1109,P98-1045,0,\N,Missing
W02-1109,C98-1045,0,\N,Missing
W06-0905,magnini-etal-2006-cab,0,0.023429,"ion benchmark is necessary to test systems performances. For this purpose, the temporal annotations of the Italian Content Annotation Bank (I-CAB-temp7 ) have been selected. I-CAB consists of 525 news documents taken from the Italian newspaper L’Adige (http://www.adige.it), and contains around 182,500 words. Its 3,830 temporal expressions (2,393 in the training part of the corpus, and 1,437 in the test part) have been manually annotated following the TIMEX2 standard with some adaptations to the specific morpho-syntactic features of Italian, which has a far richer morphology than English (see (Magnini et al., 2006) for further details). 7 I-CAB is being developed as part of the three-year project ONTOTEXT funded by the Provincia Autonoma di Trento, Italy. See http://tcc.itc.it/projects/ontotext 31 3 The starting point: TERSEO As a starting point for our experiments we used TERSEO, a system originally developed for the automatic annotation of TEs appearing in a Spanish written text in compliance with the TIMEX2 standard (see (Saquete, 2005) for a thorough description of TERSEO’s main features and functionalities). Documental DataBase TEXT POS TAGGER RECOGNITION: PARSER Temporal Expression Grammar TEMPORA"
W06-0905,W06-2001,0,0.0304897,"or some tasks, ML techniques still fall short from providing effective solutions for others. This is confirmed by the outcomes of the TERN 2004 evaluation, which provide a clear picture of the situation. In spite of the good results obtained in the TE recognition task (Hacioglu et al., 2005), the normalization by means of ML techniques has not been tackled yet, and still remains an unresolved problem. Considering the inadequacy of ML techniques to deal with the normalization problem, and focusing on portability across languages, this paper extends and completes the previous work presented in (Saquete et al., 2006b) and (Saquete et al., 2006a). More specifically, we address the following crucial issue: how to minimize the costs of building a rule-based TE recognition system for a new language, given an already existing system for another language. Our goal is to experiment with different automatic porting procedures to build temporal models for new languages, starting from previously defined ones. Still adhering to the rule-based paradigm, we analyse different porting methodologies that automatically learn the TE recognition model used by the system in one language, adjusting the set of normalization r"
W10-0734,D08-1027,0,0.0148646,"Missing"
W10-0734,ambati-etal-2010-active,0,0.0205873,"advantage of an already available monolingual corpus, casting the problem as a translation one. The challenge consists in taking a publicly available RTE dataset of English T-H pairs (i.e. the PASCAL-RTE3 dataset1 ), and create its English-Spanish CLTE equivalent by translating the hypotheses into Spanish. To this aim non-expert workers have been hired through the CrowdFlower2 channel to Amazon Mechanical Turk3 (MTurk), a crowdsourcing marketplace recently used with success for a variety of NLP tasks (Snow et al., 2008; Callison-Burch, 2009; Mihalcea and Strapparava, 2009; Marge et al., 2010; Ambati et al., 2010). The following sections overview our experiments, carried out under strict time (10 days) and cost ($100) limitations. In particular, Section 2 describes our data acquisition process; Section 3 summarizes 1 Available at: http://www.nist.gov/tac/data/RTE/index.html http://crowdflower.com/ 3 https://www.mturk.com/mturk/ 2 212 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 212–216, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics the successive approximations that led to the definition of ou"
W10-0734,D09-1030,0,0.279741,"ems’ development/evaluation cycle. Our first step in this direction takes advantage of an already available monolingual corpus, casting the problem as a translation one. The challenge consists in taking a publicly available RTE dataset of English T-H pairs (i.e. the PASCAL-RTE3 dataset1 ), and create its English-Spanish CLTE equivalent by translating the hypotheses into Spanish. To this aim non-expert workers have been hired through the CrowdFlower2 channel to Amazon Mechanical Turk3 (MTurk), a crowdsourcing marketplace recently used with success for a variety of NLP tasks (Snow et al., 2008; Callison-Burch, 2009; Mihalcea and Strapparava, 2009; Marge et al., 2010; Ambati et al., 2010). The following sections overview our experiments, carried out under strict time (10 days) and cost ($100) limitations. In particular, Section 2 describes our data acquisition process; Section 3 summarizes 1 Available at: http://www.nist.gov/tac/data/RTE/index.html http://crowdflower.com/ 3 https://www.mturk.com/mturk/ 2 212 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 212–216, c Los Angeles, California, June 2010. 2010 Association for Computational"
W10-0734,N10-1045,1,0.180136,"s H, the task consists in deciding if the meaning of H can be inferred from the meaning of T. At the monolingual level, the great potential of integrating TE recognition (RTE) components into NLP architectures has been demonstrated in several areas, including question answering, information retrieval, information extraction, and document summarization. In contrast, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. Along such direction, focusing on feasibility and architectural issues, (Mehdad et al., 2010) recently proposed baseline results demonstrating the potential of a simple approach that integrates Machine Translation and monolingual TE components. As a complementary research problem, this paper addresses the data collection issue, focusing on the definition of a fast, cheap, and reliable methodology to create CLTE corpora. The main motivation is that, as in many other NLP areas, the availability of large quantities of annotated data represents a critical bottleneck in the systems’ development/evaluation cycle. Our first step in this direction takes advantage of an already available monol"
W10-0734,P09-2078,0,0.0254218,"ation cycle. Our first step in this direction takes advantage of an already available monolingual corpus, casting the problem as a translation one. The challenge consists in taking a publicly available RTE dataset of English T-H pairs (i.e. the PASCAL-RTE3 dataset1 ), and create its English-Spanish CLTE equivalent by translating the hypotheses into Spanish. To this aim non-expert workers have been hired through the CrowdFlower2 channel to Amazon Mechanical Turk3 (MTurk), a crowdsourcing marketplace recently used with success for a variety of NLP tasks (Snow et al., 2008; Callison-Burch, 2009; Mihalcea and Strapparava, 2009; Marge et al., 2010; Ambati et al., 2010). The following sections overview our experiments, carried out under strict time (10 days) and cost ($100) limitations. In particular, Section 2 describes our data acquisition process; Section 3 summarizes 1 Available at: http://www.nist.gov/tac/data/RTE/index.html http://crowdflower.com/ 3 https://www.mturk.com/mturk/ 2 212 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 212–216, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics the successive appro"
W11-2404,W07-1428,0,0.0238803,"to more complex and natural ones. However, in contrast with other more stable tasks in terms of evaluation settings and metrics (e.g. machine translation), such changes make it difficult to capitalize on the experience obtained by participants throughout the years. Third, looking at RTE-related literature and the outcomes of the six campaigns organised so far, the conclusions that can be drawn are often controversial. For instance, it is not clear whether the availability of larger amounts of training data correlates with better performance (Hickl et al., 2006) or not (Zanzotto et al., 2007; Hickl and Bensley, 2007), even within the same evaluation setting. In addition, ablation tests carried out in recent editions of the challenge do not allow for definite conclusions about the actual usefulness of tools and resources, even the most popular ones (Bentivogli et al., 2009). Finally, the best performing systems often have different natures from one year to another, showing alternations of deep (Hickl and Bensley, 2007; Tatu and Moldovan, 2007) and shallow approaches (Jia et al., 2010) ranked at the top positions. In light of these considerations, it would be useful for sys30 Proceedings of the TextInfer 20"
W11-2404,P10-4008,1,0.739727,"good performance on all the available RTE Challenge datasets, but also to improve the official results, achieved with the same system, through ad hoc configurations manually defined by the developers team. Our contribution is twofold. On one side, in the spirit of the collaborative nature of open source projects, we extend an existing tool with a useful functionality that was still missing. On the other side, we provide a good “sparring partner” for system developers, to be used as a fast and free term of comparison to position the results of their work. 2 “Coping” with configurability EDITS (Kouylekov and Negri, 2010) is an open source RTE package, which offers a modular, flexible, and adaptable working environment to experiment with the RTE task over different datasets. The package allows to: i) create an entailment engine by defining its basic components (i.e. algorithms, cost schemes, rules, and optimizers); ii) train such entailment engine over an annotated RTE corpus to learn a model; and iii) use the entailment engine and the model to assign an entailment judgement and a confidence score to each pair of an un-annotated test corpus. A key feature of EDITS is represented by its high configurability, al"
W11-2404,P09-2073,1,0.78213,"ine by defining its basic components (i.e. algorithms, cost schemes, rules, and optimizers); ii) train such entailment engine over an annotated RTE corpus to learn a model; and iii) use the entailment engine and the model to assign an entailment judgement and a confidence score to each pair of an un-annotated test corpus. A key feature of EDITS is represented by its high configurability, allowed by the availability of different algorithms, the possibility to integrate different sets of lexical entailment/contradiction rules, and the variety of parameters for performance optimization (see also Mehdad, 2009). Although configurability is per se an important aspect (especially for an open-source and general purpose system), there is another side of the coin. In principle, in order to select the most promising configuration over a given development set, one should exhaustively run a huge number of training/evaluation routines. Such num1 http://edits.fbk.eu/ 31 ber corresponds to the total number of configurations allowed by the system, which result from the possible combinations of parameter settings. When dealing with enlarging dataset sizes, and the tight time constraints usually posed by the eval"
W11-2404,W07-1412,0,\N,Missing
W11-2404,W07-1404,0,\N,Missing
W12-3122,W11-2104,0,0.0497598,"estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However, their method still includes many features which are not focused on ad"
W12-3122,P11-1022,0,0.0747271,"addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However, their method still includes many features wh"
W12-3122,W05-0909,0,0.029602,"frequency counts with meaning representations. In order to integrate semantics more deeply into MT technology, in this paper we focus on the evaluation dimension. Restricting our investigation to some of the more pressing issues emerging from this area of research, we provide two main contributions. 1. An automatic evaluation method that avoids the use of reference translations. Most current metrics are based on comparisons between automatic translations and human references, and reward lexical similarity at the n-gram level (e.g. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006)). Due to the variability of natural languages in terms of possible ways to express the same meaning, reliable lexical similarity metrics depend on the availability of multiple hand-crafted (costly) realizations of the same source sentence in the target language. Our approach aims to avoid this bottleneck by adapting cross-lingual semantic inference capabilities and judging a translation only given the source sentence. 2. A method for evaluating translation adequacy. Most current solutions do not consistently reward translation adequacy (semantic equivalence between"
W12-3122,C04-1046,0,0.18869,"from a rich set of variants at five different linguistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translati"
W12-3122,W07-0718,0,0.02155,"on. 4 Experiments and results 4.1 Datasets Datasets with manual evaluation of MT output have been made available through a number of shared evaluation tasks. However, most of these datasets are not specifically annotated for adequacy measurement purposes, and the available adequacy judgements are limited to few hundred sentences for some language pairs. Moreover, most datasets are created by comparing reference translations with MT systems’ output, disregarding the input sentences. Such judgements are hence biased towards the reference. Furthermore, the inter-annotator agreement is often low (Callison-Burch et al., 2007). In light of these limitations, most of the available datasets are per se not fully suitable for adequacy evaluation methods based on supervised learning, nor to provide stable and meaningful results. To partially cope with these problems, our experiments have been carried out over two different datasets: • 16K: 16.000 English-Spanish pairs, with Spanish translations produced by multiple MT systems, annotated by professional translators with quality scores in a 4-point scale (Specia et al., 2010a). • WMT07: 703 English-Spanish pairs derived from MT systems’ output, with explicit adequacy judg"
W12-3122,carreras-etal-2004-freeling,0,0.0213697,"Missing"
W12-3122,W07-0738,0,0.0366771,"Missing"
W12-3122,P07-2045,1,0.0115053,"rsers. Phrase Table (PT) matching features are calculated as in (Mehdad et al., 2011), with a phrasal matching algorithm that takes advantage of a lexical phrase table extracted from a bilingual parallel corpus. The algorithm determines the number of phrases in the source (1 to 5-grams, at the level of tokens, lemmas and stems) that can be mapped into target word sequences, and vice-versa. To build our English-Spanish phrase table, we used the Europarl, News Commentary and United Nations SpanishEnglish parallel corpora. After tokenization, the Giza++ (Och and Ney, 2000) and the Moses toolkit (Koehn et al., 2007) were respectively used to align the corpora and extract the phrase table. Although the phrase table was generated using MT technology, its use to compute our features is still compatible with a system-independent approach since the extraction is carried out without tuning the process towards any particular task. Moreover, our phrase matching algorithm integrates matches from overlapping n-grams of different size and nature (tokens, lemmas and stems) which current MT decoding algorithms cannot explore for complexity reasons. Dependency Relation (DR) matching features target the increase of CLT"
W12-3122,N10-1045,1,0.836751,"im, like (Xiong et al., 2010; Bach et al., 2011; Avramidis et al., 2011; Specia et al., 2010b; Specia et al., 2011) we rely on a large number of features, but focusing on source-target dependent ones, aiming at informed adequacy evaluation of a translation given the source instead of a more generic quality assessment based on surface features. 3 CLTE for adequacy evaluation We address adequacy evaluation by adapting crosslingual textual entailment recognition as a way to measure to what extent a source sentence and its automatic translation are semantically similar. CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment (Dagan and Glickman, 2004) that consists in deciding, given a text T and a hypothesis H in different languages, if the meaning of H can be inferred from the meaning of T. The main motivation in approaching adequacy evaluation using CLTE is that an adequate translation and the source text should convey the same meaning. In terms of entailment, this means that an adequate MT output and the source sentence should entail each other (bi-directional entailment). Losing or altering part of the meaning conveyed by the source sentence (i.e. having more, or differe"
W12-3122,P11-1134,1,0.925957,"will change the entailment direction and, consequently, the adequacy judgement. Framed in this way, CLTE-based adequacy evaluation methods can be designed to distinguish meaning-preserving variations from true divergence, regardless of reference translations. Similarly to many monolingual TE approaches, CLTE solutions proposed so far adopt supervised learning methods, with features that measure to what extent the hypotheses can be mapped into the texts. The underlying assumption is that the probability of entailment is proportional to the number of words in H that can be mapped to words in T (Mehdad et al., 2011). Such mapping can be carried out at different word representation levels (e.g. tokens, lemmas, stems), possibly with the support of lexical knowledge in order to cross the language barrier between T and H (e.g. dictionaries, phrase tables). Under the same assumption, since in the adequacy evaluation framework the entailment relation should hold in both directions, the mapping is performed both from the source to the target and vice-versa, building on features extracted from both sentences. Moreover, to improve over previous CLTE methods and boost MT adequacy evaluation performance, we explore"
W12-3122,P12-2024,1,0.497952,"okens, lemmas, stems), possibly with the support of lexical knowledge in order to cross the language barrier between T and H (e.g. dictionaries, phrase tables). Under the same assumption, since in the adequacy evaluation framework the entailment relation should hold in both directions, the mapping is performed both from the source to the target and vice-versa, building on features extracted from both sentences. Moreover, to improve over previous CLTE methods and boost MT adequacy evaluation performance, we explore the joint contribution of a number of lexical, syntactic and semantic features (Mehdad et al., 2012). Concerning the features used, it’s worth observing that the cost of implementing our approach (in terms of required resources and linguistic processors), and the need of reference translations are intrinsically different bottlenecks for MT. While the limited availability of processing tools for some language pairs is a “temporary” bottleneck, the acquisition of multiple references is a “permanent” one. The former cost is reducing over time due to the progress in NLP research; the latter represents a fixed cost that has to be eliminated. Similar considerations hold regarding the need of annot"
W12-3122,P00-1056,0,0.120809,"provides English and Spanish dependency parsers. Phrase Table (PT) matching features are calculated as in (Mehdad et al., 2011), with a phrasal matching algorithm that takes advantage of a lexical phrase table extracted from a bilingual parallel corpus. The algorithm determines the number of phrases in the source (1 to 5-grams, at the level of tokens, lemmas and stems) that can be mapped into target word sequences, and vice-versa. To build our English-Spanish phrase table, we used the Europarl, News Commentary and United Nations SpanishEnglish parallel corpora. After tokenization, the Giza++ (Och and Ney, 2000) and the Moses toolkit (Koehn et al., 2007) were respectively used to align the corpora and extract the phrase table. Although the phrase table was generated using MT technology, its use to compute our features is still compatible with a system-independent approach since the extraction is carried out without tuning the process towards any particular task. Moreover, our phrase matching algorithm integrates matches from overlapping n-grams of different size and nature (tokens, lemmas and stems) which current MT decoding algorithms cannot explore for complexity reasons. Dependency Relation (DR) m"
W12-3122,W09-0404,0,0.0341673,"Missing"
W12-3122,P02-1040,0,0.124085,"Missing"
W12-3122,quirk-2004-training,0,0.208721,"ariants at five different linguistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequa"
W12-3122,2006.amta-papers.25,0,0.0853718,"presentations. In order to integrate semantics more deeply into MT technology, in this paper we focus on the evaluation dimension. Restricting our investigation to some of the more pressing issues emerging from this area of research, we provide two main contributions. 1. An automatic evaluation method that avoids the use of reference translations. Most current metrics are based on comparisons between automatic translations and human references, and reward lexical similarity at the n-gram level (e.g. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006)). Due to the variability of natural languages in terms of possible ways to express the same meaning, reliable lexical similarity metrics depend on the availability of multiple hand-crafted (costly) realizations of the same source sentence in the target language. Our approach aims to avoid this bottleneck by adapting cross-lingual semantic inference capabilities and judging a translation only given the source sentence. 2. A method for evaluating translation adequacy. Most current solutions do not consistently reward translation adequacy (semantic equivalence between source sentence and target"
W12-3122,2010.jec-1.5,0,0.0357492,"l, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of"
W12-3122,specia-etal-2010-dataset,0,0.180153,"guistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-orien"
W12-3122,2011.mtsummit-papers.58,0,0.354932,"Missing"
W12-3122,2011.eamt-1.12,0,0.0381818,"tic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation ma"
W12-3122,P10-1062,0,0.0457323,"Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However,"
W12-3122,W07-0734,0,\N,Missing
W13-2231,P11-1022,0,0.029072,"racy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni e"
W13-2231,2005.mtsummit-papers.11,0,0.0453967,"Missing"
W13-2231,W12-3123,0,0.0636256,"Missing"
W13-2231,2012.iwslt-papers.12,0,0.0437826,"post-editings (lower HTER values) and pairs marked as rewritings (higher HTER values). Such separation corresponds to an HTER value around 0.4, which is significantly lower than the threshold of 0.7 proposed by the WMT-12 guidelines as a criterion to label sentences for which “a translation from scratch is necessary”. This confirms that our separation differs from those produced by partition methods based on human annotations or arbitrary HTER thresholds. Furthermore, our au8 Monolingual stem-to-stem exact matches between TGT and correct translation are inferred by computing the HTER, as in (Blain et al., 2012). 9 All ROUGE scores, described in (Lin, 2004), have been calculated using the software available at http://www. berouge.com. 10 Such partitions are: average effort scores = 3, human scores = 3-3-3, HTER score = 0.45. 245 optimizing a metric that takes into account the number of true and false positives (see below). Seventeen features proposed in (Specia et al., 2009) were extracted from each source-target pair. This feature set, fully described in (CallisonBurch et al., 2012), mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per so"
W13-2231,W04-1013,0,0.0441569,"rewritings (higher HTER values). Such separation corresponds to an HTER value around 0.4, which is significantly lower than the threshold of 0.7 proposed by the WMT-12 guidelines as a criterion to label sentences for which “a translation from scratch is necessary”. This confirms that our separation differs from those produced by partition methods based on human annotations or arbitrary HTER thresholds. Furthermore, our au8 Monolingual stem-to-stem exact matches between TGT and correct translation are inferred by computing the HTER, as in (Blain et al., 2012). 9 All ROUGE scores, described in (Lin, 2004), have been calculated using the software available at http://www. berouge.com. 10 Such partitions are: average effort scores = 3, human scores = 3-3-3, HTER score = 0.45. 245 optimizing a metric that takes into account the number of true and false positives (see below). Seventeen features proposed in (Specia et al., 2009) were extracted from each source-target pair. This feature set, fully described in (CallisonBurch et al., 2012), mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target trans"
W13-2231,P11-1124,0,0.0238105,"Missing"
W13-2231,W12-3102,0,0.137719,"Missing"
W13-2231,P12-2024,1,0.911436,"such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is"
W13-2231,W12-3122,1,0.542784,"such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is"
W13-2231,P13-2135,1,0.879764,"Missing"
W13-2231,P02-1040,0,0.117005,"al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is to rely on human annotations, which seem to lead to more accurate models (Quirk, 2004; Specia et al., 2009). Along this direction, the QE task consists in predicting scores that reflect human quality judgements, by learning from manually annotated datasets (e.g. collections of source-target pairs laSupervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications."
W13-2231,2012.amta-papers.6,0,0.0138318,"e Bruno Kessler, FBK-irst Trento , Italy {turchi|negri|federico}@fbk.eu Abstract instance, MT and TM translations can be automatically ranked to ease the selection of the most suitable one for post-editing (He et al., 2010), or the TM can be used to constrain and improve MT suggestions (Ma et al., 2011). In all cases, the effectiveness of the integration is conditioned by: i) the quality of MT, and ii) the accuracy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al"
W13-2231,P10-1064,0,0.0195728,"Missing"
W13-2231,P07-2045,1,0.0080684,"Missing"
W13-2231,quirk-2004-training,0,0.200826,"ulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is to rely on human annotations, which seem to lead to more accurate models (Quirk, 2004; Specia et al., 2009). Along this direction, the QE task consists in predicting scores that reflect human quality judgements, by learning from manually annotated datasets (e.g. collections of source-target pairs laSupervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using humanannotated data to train a bin"
W13-2231,W09-0441,0,0.0261665,", checking whether it can be partitioned in a way that reflects the distinction between good (useful for the translator, suitable for post editing) and bad translations (that need complete rewriting).2 To this aim we experiment with the QE data released within the 7th Workshop on Machine Translation (WMT-12). The corpus consists of source-target pairs annotated with manual QE labels (1-5 scores) indicating the post-editing needed to correct the translations. Besides explicit human judgements, the availability of post-edited translations makes also possible to calculate the actual HTER values (Snover et al., 2009), indicating the minimum edit distance between the machine translation and its manually post-edited version in the [0,1] interval. The second option is to automatically reannotate the same dataset, trying to produce labels that reflect an objective and more reliable binary distinction based on empirical observations. Our analysis aims to answer the following questions: and replicable way compared to current data annotation methods. By answering these questions, this paper provides the following main contributions: • We show that training a binary classifier on arbitrary partitions of an existi"
W13-2231,P10-1063,0,0.0542327,"lity of MT, and ii) the accuracy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics li"
W13-2231,W12-3118,0,0.115285,"such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is to rely on human annotations, which seem to lead to more accurate models (Quirk, 2004; Specia et al., 2009). Along this direction, the QE task consists in predicting scores that reflect human quality judgements, by learning from manually annotated datasets (e.g. col"
W13-2231,2009.eamt-1.5,1,0.915027,"tioned by: i) the quality of MT, and ii) the accuracy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automa"
W13-2231,2011.mtsummit-papers.58,0,0.0205377,"s with estimates of the expected quality of each MT suggestion. Such intuitive solution, however, disregards the fact that even precise QE scores would not alleviate translators from the effort of reading useless MT output (or at least the associated score). A more effective alternative is to use the estimated QE scores to filter out poor MT suggestions, presenting only those worth for post-editing. Binary classification, however, has to confront with the problem of setting reasonable cut-off criteria. The arbitrary thresholds, used in several previous works (Quirk, 2004; Specia et al., 2010; Specia et al., 2011) are in fact hard to justify, and even harder to learn from human-labelled training data. 1. Are human labels reliable and coherent enough to train accurate binary models? 2. Are arbitrarily-set thresholds useful to partition QE data for this task? 3. Is it possible to obtain reliable binary annotations from an automatic procedure? Negative answers to the first two questions would respectively call into question: i) the intuitive idea that human labels are the most reliable for a supervised approach to binary QE, and ii) the possibility that thresholds on a single metric (e.g. the HTER) can be"
W13-2231,2011.eamt-1.12,0,0.255609,"ly predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002),"
W13-2231,S13-2023,1,0.863131,"Missing"
W13-2231,C04-1046,0,\N,Missing
W13-2243,2005.iwslt-1.8,0,0.0333495,"aligned/unaligned sequences of words; • summation of the IDF scores of aligned words in src divided by the sum of IDF scores of the aligned words in tgt (and viceversa). • position of the first/last unaligned word normalized by the length of the sentence; • proportion of aligned n-grams in the sentence. Preliminary experiments have been executed to find the best word alignment algorithm for each task. We explored three different word alignment algorithms: the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993). We also tried three symmetrization models (Koehn et al., 2005): union, intersection, and grow-diag-final-and, a more complex symmetrization method which combines intersection with some alignments from the union. The best alignment and symmetrization combination found for Task 1.1 was IBM4 with intersection and for task 1.3 was HMM with intersection. These experiments were carried out in 10fold cross-validation on the training set and used only the alignment features. To compute the features of the POS group we use part-of-speech (PoS) information for each word in src and tgt. Training and test data for both tasks were preprocessed with the TreeTagger (Sc"
W13-2243,P07-2045,0,0.00427213,"rds tagged with p in the sentence (p ∈ P ). This feature is processed for both src and tgt; 353 of using the search graph directly such as the inability compute edit distance between hypotheses. Thus, to quantify the coherence of translation options we compute a (symmetrical) matrix of pairwise Levenshtein distances, either on token or character level, for n-best lists of size up to 100k1 using the baseline system and the systems we describe in Section 2.4. For this matrix the following features are produced: have been estimated using publicly available software (SRILM (Stolcke, 2002), Moses (Koehn et al., 2007)), and corpora (Europarl, News Commentary, MultiUN, Gigaword). Using the predictions of the English-Spanish systems as pseudoreferences and likewise the original source as reference for the back-translation system we computed a number of automatic metrics including BLEU (Papineni et al., 2002), GTM (Turian et al., 2003), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). 1. The index of the central hypothesis, i.e. the translation with the minimum average distance to all other entries. 3 2. The average edit distance between the central hypothesis and"
W13-2243,P12-2024,1,0.250924,"and 500 for test). We participated only in the scoring mode of this task. 2.1 Features Word Alignment Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms) under the assumption that features that explore what is aligned can bring improvements to tasks where sentence-level semantic relations need to be identified. Among the possible applications, Souza et al. (2013) recently investigated with success their application in Crosslingual Textual Entailment for content synchronization (Mehdad et al., 2012; Negri et al., 2013). For our experiments in both tasks we built word alignment models using the resources made available for the evaluation campaign. To train the word alignment models we used the MGIZA++ implementation (Gao and Vogel, 2008) of the IBM models (Brown et al., 1993) and the concatenation of Europarl, News Commentary, MultiUN, paral352 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352–358, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics • proportion of words in src aligned with words in tgt that share the same PoS"
W13-2243,S13-2005,1,0.830107,"dited version. The data set contains 2,754 English-Spanish sentence pairs post-edited by one translator (2,254 for training and 500 for test). We participated only in the scoring mode of this task. 2.1 Features Word Alignment Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms) under the assumption that features that explore what is aligned can bring improvements to tasks where sentence-level semantic relations need to be identified. Among the possible applications, Souza et al. (2013) recently investigated with success their application in Crosslingual Textual Entailment for content synchronization (Mehdad et al., 2012; Negri et al., 2013). For our experiments in both tasks we built word alignment models using the resources made available for the evaluation campaign. To train the word alignment models we used the MGIZA++ implementation (Gao and Vogel, 2008) of the IBM models (Brown et al., 1993) and the concatenation of Europarl, News Commentary, MultiUN, paral352 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352–358, c Sofia, Bulgaria, Augus"
W13-2243,P02-1040,0,0.107292,"wise Levenshtein distances, either on token or character level, for n-best lists of size up to 100k1 using the baseline system and the systems we describe in Section 2.4. For this matrix the following features are produced: have been estimated using publicly available software (SRILM (Stolcke, 2002), Moses (Koehn et al., 2007)), and corpora (Europarl, News Commentary, MultiUN, Gigaword). Using the predictions of the English-Spanish systems as pseudoreferences and likewise the original source as reference for the back-translation system we computed a number of automatic metrics including BLEU (Papineni et al., 2002), GTM (Turian et al., 2003), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). 1. The index of the central hypothesis, i.e. the translation with the minimum average distance to all other entries. 3 2. The average edit distance between the central hypothesis and all other entries normalized by the length of top scoring hypothesis. To build our models using the features presented in Section 2 we tried different learning algorithms. After some preliminary experiments for both tasks we decided to use mainly two: support vector machines (SVM) and extreme"
W13-2243,petrov-etal-2012-universal,0,0.00619189,"some alignments from the union. The best alignment and symmetrization combination found for Task 1.1 was IBM4 with intersection and for task 1.3 was HMM with intersection. These experiments were carried out in 10fold cross-validation on the training set and used only the alignment features. To compute the features of the POS group we use part-of-speech (PoS) information for each word in src and tgt. Training and test data for both tasks were preprocessed with the TreeTagger (Schmid, 1995) and mapped to a more coarsegrained set of part-of-speech tags (P ) based on the universal PoS tag set by Petrov et al. (2012). In this group there are two different types of features: one is computed for the alignments (the mapping between a word in src and a word in tgt) and the other is computed for aligned words (words in src that are aligned to one or more words in tgt and vice-versa). The features computed over the alignments are: • proportion of alignments connecting words with the same PoS tag; 2.2 N-best Diversity scores Our n-best diversity features are based on the intuition that a large number of possible choices generally leads to more errors. While a similar notion can be expressed locally by counting t"
W13-2243,2006.amta-papers.25,0,0.592967,"MT) is the task of evaluating the quality of the output of an MT system without relying on reference translations. The WMT 2013 QE Shared Task defined four different tasks covering both word and sentence level QE. In this work we describe the Fondazione Bruno Kessler (FBK) and University of Edinburgh approach and system setup of our participation to the shared task. We developed models for two sentence-level tasks: Task 1.1: Scoring and ranking for post-editing effort, and Task 1.3: Predicting post-editing time. The first task aims at predicting the Humanmediated Translation Edit Rate (HTER) (Snover et al., 2006) between a suggestion generated by a machine translation system and its manually post-edited version. The data set contains 2,754 English-Spanish sentence pairs post-edited by one translator (2,254 for training and 500 for test). We participated only in the scoring mode of this task. 2.1 Features Word Alignment Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms) under the assumption that features that explore what is aligned can bring improvements to tasks where sentence-level s"
W13-2243,W12-3118,0,0.032567,"Missing"
W13-2243,P13-2135,1,0.847402,"Missing"
W13-2243,2003.mtsummit-papers.51,0,0.0136173,"ither on token or character level, for n-best lists of size up to 100k1 using the baseline system and the systems we describe in Section 2.4. For this matrix the following features are produced: have been estimated using publicly available software (SRILM (Stolcke, 2002), Moses (Koehn et al., 2007)), and corpora (Europarl, News Commentary, MultiUN, Gigaword). Using the predictions of the English-Spanish systems as pseudoreferences and likewise the original source as reference for the back-translation system we computed a number of automatic metrics including BLEU (Papineni et al., 2002), GTM (Turian et al., 2003), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). 1. The index of the central hypothesis, i.e. the translation with the minimum average distance to all other entries. 3 2. The average edit distance between the central hypothesis and all other entries normalized by the length of top scoring hypothesis. To build our models using the features presented in Section 2 we tried different learning algorithms. After some preliminary experiments for both tasks we decided to use mainly two: support vector machines (SVM) and extremely randomized trees (Geurts"
W13-2243,2003.mtsummit-papers.52,0,0.0150373,"d search and it drastically reduced the time required to compute the best parameter combination. Finally, we trained an extremely randomized forest, i.e. an ensemble of extremely randomized trees. Each tree can be parameterized differently. The results of the individual trees are combined by averaging their predictions. When a tree is built, 3. Edit distance between top scoring and central hypothesis 4. Number of hypotheses with an edit distance to the top-scoring hypothesis below a set threshold. 2.3 Word Posterior Probabilities Following previous work on word posterior probabilities (WPPs) (Ueffing et al., 2003) we computed the sequence of edit operations needed to transform the MT suggestion into all entries of an n-best list in which we normalized the logarithmic model scores to resemble probabilities. Tokens are considered incorrect is the operation is either insert or substitute, otherwise the probability of the hypothesis counts towards the correctness of the word. These word-level features were then normalized by taking the geometric mean of the individual probabilities. We did this for all systems described in Section 2.4 and varying sizes of n between 10 and 100k. 2.4 Learning algorithms Pseu"
W13-2243,C96-2141,0,0.357043,"he longest sequence of aligned/unaligned words normalized by the length of the sentence; • average length of aligned/unaligned sequences of words; • summation of the IDF scores of aligned words in src divided by the sum of IDF scores of the aligned words in tgt (and viceversa). • position of the first/last unaligned word normalized by the length of the sentence; • proportion of aligned n-grams in the sentence. Preliminary experiments have been executed to find the best word alignment algorithm for each task. We explored three different word alignment algorithms: the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993). We also tried three symmetrization models (Koehn et al., 2005): union, intersection, and grow-diag-final-and, a more complex symmetrization method which combines intersection with some alignments from the union. The best alignment and symmetrization combination found for Task 1.1 was IBM4 with intersection and for task 1.3 was HMM with intersection. These experiments were carried out in 10fold cross-validation on the training set and used only the alignment features. To compute the features of the POS group we use part-of-speech (PoS) information f"
W13-2243,J93-2003,0,\N,Missing
W13-2243,W08-0509,0,\N,Missing
W13-2243,W11-2107,0,\N,Missing
W14-3340,C04-1046,0,0.887897,"of possible translations we re-translate the source using the system that is available for the 2013 WMT QE Shared Task (Bojar et al., 2013). Certainly, there is a mismatch between the original system and the one that we used but, since our system was trained using the same news domain as the QE data, we assume that both face similar ambiguous words or possible reorderings. Using this system we generate a 100k-best list which is the foundation of several features. We extract a set of word-level features based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Language Models (LM) As language model features we produced n-gram length/backoff behaviour and conditional probabilities for every word in the sentence. We employed both an interpolated LM taken from the MT system discussed 325 in Section 3 as well as a very large LM which we built on 62 billion tokens of monolingual data extracted from Common Crawl, a public web crawl. While generally following the procedure of Buck et al. (2014) we apply an additional lowercasing step before training the model. 3.2 We use bidirectional long short-term memory r"
W14-3340,2007.mtsummit-papers.54,0,0.0428409,"he source using the system that is available for the 2013 WMT QE Shared Task (Bojar et al., 2013). Certainly, there is a mismatch between the original system and the one that we used but, since our system was trained using the same news domain as the QE data, we assume that both face similar ambiguous words or possible reorderings. Using this system we generate a 100k-best list which is the foundation of several features. We extract a set of word-level features based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Language Models (LM) As language model features we produced n-gram length/backoff behaviour and conditional probabilities for every word in the sentence. We employed both an interpolated LM taken from the MT system discussed 325 in Section 3 as well as a very large LM which we built on 62 billion tokens of monolingual data extracted from Common Crawl, a public web crawl. While generally following the procedure of Buck et al. (2014) we apply an additional lowercasing step before training the model. 3.2 We use bidirectional long short-term memory recurrent neural networks (BLSTM-RNNs) as imple"
W14-3340,J93-2003,0,0.0664319,"at compose the translation sentence. In total, we compute 18 features: Word alignment (wla). Following our last year’s submission (de Souza et al., 2013a) we explore information about word alignments to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms). Our assumption is that features that explore what is aligned can bring improvements to tasks where sentence-level semantic relations need to be identified. We train the word alignment models with the MGIZA++ toolkit (Gao and Vogel, 2008) implementation of the IBM models (Brown et al., 1993). The models are built on the concatenation of Europarl, News Commentary, and MultiUN parallel corpora made available in the QE shared task of 2013, comprising about 12.8 million sentence pairs. A more detailed description of the 89 features extracted can be found in (de Souza et al., 2013a; de Souza et al., 2013b). • number of OK predictions divided by the no. of words in the translation sentence (1 feature); • number of OK function/content words predictions divided by the no. of function/content words in the translation (2 features); • number of OK nouns, verbs, proper-nouns, adjective, pron"
W14-3340,buck-etal-2014-n,1,0.39337,"Missing"
W14-3340,2006.amta-papers.25,0,0.145835,"Missing"
W14-3340,W13-2243,1,0.652141,"Missing"
W14-3340,P13-4014,1,0.184453,"Missing"
W14-3340,P13-2135,1,0.912362,"Missing"
W14-3340,J07-1003,0,0.224433,"tions we re-translate the source using the system that is available for the 2013 WMT QE Shared Task (Bojar et al., 2013). Certainly, there is a mismatch between the original system and the one that we used but, since our system was trained using the same news domain as the QE data, we assume that both face similar ambiguous words or possible reorderings. Using this system we generate a 100k-best list which is the foundation of several features. We extract a set of word-level features based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Language Models (LM) As language model features we produced n-gram length/backoff behaviour and conditional probabilities for every word in the sentence. We employed both an interpolated LM taken from the MT system discussed 325 in Section 3 as well as a very large LM which we built on 62 billion tokens of monolingual data extracted from Common Crawl, a public web crawl. While generally following the procedure of Buck et al. (2014) we apply an additional lowercasing step before training the model. 3.2 We use bidirectional long short-term memory recurrent neural network"
W14-3340,W06-3110,0,0.0272069,"n Task 1.2 the two distributions are more similar than in Task 1.3, which presents slightly different distributions between training and test data. Consider a target word ei belonging to a translation e = e1 . . . ei . . . e|e |generated from a source sentence f . Let N (f ) be the list of N-best translations for f . We compute features as the normalized sum of probabilities of those translations S(ei ) ⊆ N (f ) that “contain” word ei : X 1 P P(e0 |f ) (1) 00 |f ) P(e e00 ∈N (f ) 0 3 where P(e |f ) is the probability translation e given source sentence f according to the SMT model. We follow (Zens and Ney, 2006) and extract three different WPP features depending on the criteria chosen to compute S(ei ): e ∈S(ei ) Word-Level QE Task 2 is the word-level quality estimation of automatically translated news sentences without given reference translations. Participants are required to produce a label for each word in one or more of the following settings: S(ei ) = {e0 ∈ N (f ) |a = Le(e0 , e) ∧ e0ai = ei } S(ei ) contain those translations e0 for which the word Levenshtein-aligned (Levenshtein, 1966) to position i in e is equal to ei . Binary classification: a OK/BAD label, where BAD indicates the need for"
W14-3340,W08-0509,0,0.0166982,"ndicating the quality of distinct meaningful regions that compose the translation sentence. In total, we compute 18 features: Word alignment (wla). Following our last year’s submission (de Souza et al., 2013a) we explore information about word alignments to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms). Our assumption is that features that explore what is aligned can bring improvements to tasks where sentence-level semantic relations need to be identified. We train the word alignment models with the MGIZA++ toolkit (Gao and Vogel, 2008) implementation of the IBM models (Brown et al., 1993). The models are built on the concatenation of Europarl, News Commentary, and MultiUN parallel corpora made available in the QE shared task of 2013, comprising about 12.8 million sentence pairs. A more detailed description of the 89 features extracted can be found in (de Souza et al., 2013a; de Souza et al., 2013b). • number of OK predictions divided by the no. of words in the translation sentence (1 feature); • number of OK function/content words predictions divided by the no. of function/content words in the translation (2 features); • nu"
W14-3340,W13-2201,1,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W15-3025,2011.mtsummit-papers.35,0,0.805393,"Missing"
W15-3025,P15-2026,1,0.736824,"ine consist of various stages like language model selection, phrase table pruning, and feature designing as discussed in the following sections. Evaluation Metric: We select TER (Snover et al., 2006) as our evaluation metric because it mimics the human post-editing effort by measuring the edit operation needed to translate the MT output into its human-revised version. Apart from TER as an evaluation metric we also compute number of sentences being modified1 in the test set and then compute the precision as follow: N umberof SentencesImproved Precision = N umberof SentencesM odif ied Recently, Chatterjee et al. (2015) showed a fair systematic comparison of these two approaches over multiple language pairs and revealed that inclusion of source information in the form of context-aware variant is useful to improve translation quality over standard monolingual translation approach. They also showed that using monolingual translation alignment to build context-aware APE helps to mitigate the sparsity issue at the level of word alignment and for this reasons, we use this configuration to implement APE-2 method. 3 Baseline: Our baseline is the MT output asis. To evaluate, we use the corresponding human post-edite"
W15-3025,W08-0509,0,0.259289,"of a larger vocabulary size and, consequently, higher data sparseness that will eventually reduce the reliability of the translation rules being learned. Second, the joint representation f 0 #e may be infected by the word alignment errors which may mislead the learning of translation option. Experiment Settings: To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES(Koehn et al., 2007). For all the experiments mentioned in this paper we use “grow-diag-final-and” as alignment heuristic and “msd-bidirectional-fe” heuristic for reordering model. MGIZA++ (Gao and Vogel, 2008) is used for word alignment. The APE systems are tuned to optimize TER(Snover et al., 2006) with MERT(Och, 2003). We follow an incremental strategy to develop the APE systems, at each stage of the APE pipeline we find the best configuration of a component and then proceed to explore the next component. Our APE pipeline consist of various stages like language model selection, phrase table pruning, and feature designing as discussed in the following sections. Evaluation Metric: We select TER (Snover et al., 2006) as our evaluation metric because it mimics the human post-editing effort by measuri"
W15-3025,W14-1703,0,0.0388874,"Missing"
W15-3025,D07-1103,0,0.0664096,"Missing"
W15-3025,P07-2045,0,0.0506012,"ata used to train the statistical APE system. This “context-aware” variant seems to be more precise but faces two potential problems. First, preserving the source context comes at the cost of a larger vocabulary size and, consequently, higher data sparseness that will eventually reduce the reliability of the translation rules being learned. Second, the joint representation f 0 #e may be infected by the word alignment errors which may mislead the learning of translation option. Experiment Settings: To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES(Koehn et al., 2007). For all the experiments mentioned in this paper we use “grow-diag-final-and” as alignment heuristic and “msd-bidirectional-fe” heuristic for reordering model. MGIZA++ (Gao and Vogel, 2008) is used for word alignment. The APE systems are tuned to optimize TER(Snover et al., 2006) with MERT(Och, 2003). We follow an incremental strategy to develop the APE systems, at each stage of the APE pipeline we find the best configuration of a component and then proceed to explore the next component. Our APE pipeline consist of various stages like language model selection, phrase table pruning, and featur"
W15-3025,P03-1021,0,0.143611,"ranslation rules being learned. Second, the joint representation f 0 #e may be infected by the word alignment errors which may mislead the learning of translation option. Experiment Settings: To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES(Koehn et al., 2007). For all the experiments mentioned in this paper we use “grow-diag-final-and” as alignment heuristic and “msd-bidirectional-fe” heuristic for reordering model. MGIZA++ (Gao and Vogel, 2008) is used for word alignment. The APE systems are tuned to optimize TER(Snover et al., 2006) with MERT(Och, 2003). We follow an incremental strategy to develop the APE systems, at each stage of the APE pipeline we find the best configuration of a component and then proceed to explore the next component. Our APE pipeline consist of various stages like language model selection, phrase table pruning, and feature designing as discussed in the following sections. Evaluation Metric: We select TER (Snover et al., 2006) as our evaluation metric because it mimics the human post-editing effort by measuring the edit operation needed to translate the MT output into its human-revised version. Apart from TER as an eva"
W15-3025,2012.eamt-1.34,0,0.402315,"Missing"
W15-3025,N07-1064,0,0.899156,"prove MT quality. The problem is appealing for several reasons. On one side, as shown by Parton et al. (2012), APE systems can improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage. On the other side, APE represents the only way to rectify errors present in the “black-box” scenario where the MT system is unknown or its internal decoding information is not available. 2 Statistical APE Methods In this paper we examine the most widely used statistical phrase-based post-editing strategy proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). We describe the two methods and there pros and cons in the following subsections. 2.1 APE-1 (Simard et al., 2007) In this approach APE systems are trained in the same way as the statistical machine translation (SMT) system. But, as contrast to SMT which makes use of the source and target language parallel corpus, APE uses the MT output and its corresponding human post-edited data in the form of parallel corpus. One of the most important missing concepts in this “monolingual translation” is the inclusion of source information"
W15-3025,2006.amta-papers.25,0,0.628049,"reduce the reliability of the translation rules being learned. Second, the joint representation f 0 #e may be infected by the word alignment errors which may mislead the learning of translation option. Experiment Settings: To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES(Koehn et al., 2007). For all the experiments mentioned in this paper we use “grow-diag-final-and” as alignment heuristic and “msd-bidirectional-fe” heuristic for reordering model. MGIZA++ (Gao and Vogel, 2008) is used for word alignment. The APE systems are tuned to optimize TER(Snover et al., 2006) with MERT(Och, 2003). We follow an incremental strategy to develop the APE systems, at each stage of the APE pipeline we find the best configuration of a component and then proceed to explore the next component. Our APE pipeline consist of various stages like language model selection, phrase table pruning, and feature designing as discussed in the following sections. Evaluation Metric: We select TER (Snover et al., 2006) as our evaluation metric because it mimics the human post-editing effort by measuring the edit operation needed to translate the MT output into its human-revised version. Apa"
W15-3025,P02-1040,0,\N,Missing
W15-3025,D07-1091,0,\N,Missing
W15-3025,P13-4014,0,\N,Missing
W15-3025,W14-3340,1,\N,Missing
W15-3025,P15-1022,1,\N,Missing
W15-3025,W12-3122,1,\N,Missing
W15-3025,W13-2243,1,\N,Missing
W15-3025,D13-1140,0,\N,Missing
W15-3025,W11-2123,0,\N,Missing
W15-3025,P14-1067,1,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2377,2011.mtsummit-papers.35,0,0.251467,"Missing"
W16-2377,W12-3122,1,0.905252,"Missing"
W16-2377,W13-2243,1,0.923881,"Missing"
W16-2377,P03-1021,0,0.127714,"T|407 MP4-Videos|NN |41 .|$.|451 Table 2: Example of parallel corpus with factored representation. lations by scoring them with both part-ofspeech tag and class-based language models. mt pairs of the training data by using MGIZA++ (Gao and Vogel, 2008). To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES (Koehn et al., 2007) with alignment heuristic set to “grow-diag-final-and”, and reordering heuristic to “msd-bidirectional-fe”. For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008). For tuning the feature weights we use MERT (Och, 2003) optimizing TER (Snover et al., 2006). We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation metrics, we also measure precision of our APE system using sentence level TER score as defined in Chatterjee et al. (2015a) Number of Improved Sentences Precision = Number of Modified Sentences Source factors: The factor on the source side of the parallel corpus is obtained following the approach to obtain the joint representation (as described in Section 2) for co"
W16-2377,W14-3340,1,0.919172,"Missing"
W16-2377,P02-1040,0,0.113146,"rt-ofspeech tag and class-based language models. mt pairs of the training data by using MGIZA++ (Gao and Vogel, 2008). To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES (Koehn et al., 2007) with alignment heuristic set to “grow-diag-final-and”, and reordering heuristic to “msd-bidirectional-fe”. For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008). For tuning the feature weights we use MERT (Och, 2003) optimizing TER (Snover et al., 2006). We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation metrics, we also measure precision of our APE system using sentence level TER score as defined in Chatterjee et al. (2015a) Number of Improved Sentences Precision = Number of Modified Sentences Source factors: The factor on the source side of the parallel corpus is obtained following the approach to obtain the joint representation (as described in Section 2) for context-aware APE, the only difference is that instead of joint representation (mt#source) we now have factored representation (mt|source) suitable t"
W16-2377,P15-1022,1,0.898752,"Missing"
W16-2377,W15-3025,1,0.225401,"kit MOSES (Koehn et al., 2007) with alignment heuristic set to “grow-diag-final-and”, and reordering heuristic to “msd-bidirectional-fe”. For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008). For tuning the feature weights we use MERT (Och, 2003) optimizing TER (Snover et al., 2006). We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation metrics, we also measure precision of our APE system using sentence level TER score as defined in Chatterjee et al. (2015a) Number of Improved Sentences Precision = Number of Modified Sentences Source factors: The factor on the source side of the parallel corpus is obtained following the approach to obtain the joint representation (as described in Section 2) for context-aware APE, the only difference is that instead of joint representation (mt#source) we now have factored representation (mt|source) suitable to train factored models. Target factors: We introduce two target factors to measure fluency of the translations at syntactic and semantic levels, i) POS-tag (∼50 tags) obtained using the TreeTagger (Schmid,"
W16-2377,N07-1064,0,0.507731,"a more elegant approach that combines them into a factored model as described in the following section. Most of the current statistical APE systems follow the phrase-based machine translation approach. They mainly differ in the way the data is represented in the parallel corpus. Unlike MT systems where the parallel corpus is made up of source and target language texts, APE systems use either i) MT text or ii) MT text with source annotations on the source side, and post-edits on the target side of the parallel corpus. The former variant (to use only MT text on the source side) was proposed by Simard et al. (2007), also known as monolingual translation, and the latter variant was proposed by B´echara et al. (2011), which is known as contextaware translation. The monolingual translation approach is more robust, it better generalizes the post-editing rules, and is less prone to word alignment errors which eventually impact on the quality of the post-editing rules. However, since the post-editing rules are learned from (mt, pe) (mt: machine translated; pe: post-edited) pairs, it loses connection with the source sentence, which implies that information lost or distorted in the machine translation process a"
W16-2377,P15-2026,1,0.836829,"kit MOSES (Koehn et al., 2007) with alignment heuristic set to “grow-diag-final-and”, and reordering heuristic to “msd-bidirectional-fe”. For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008). For tuning the feature weights we use MERT (Och, 2003) optimizing TER (Snover et al., 2006). We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation metrics, we also measure precision of our APE system using sentence level TER score as defined in Chatterjee et al. (2015a) Number of Improved Sentences Precision = Number of Modified Sentences Source factors: The factor on the source side of the parallel corpus is obtained following the approach to obtain the joint representation (as described in Section 2) for context-aware APE, the only difference is that instead of joint representation (mt#source) we now have factored representation (mt|source) suitable to train factored models. Target factors: We introduce two target factors to measure fluency of the translations at syntactic and semantic levels, i) POS-tag (∼50 tags) obtained using the TreeTagger (Schmid,"
W16-2377,2006.amta-papers.25,0,0.129928,".|451 Table 2: Example of parallel corpus with factored representation. lations by scoring them with both part-ofspeech tag and class-based language models. mt pairs of the training data by using MGIZA++ (Gao and Vogel, 2008). To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES (Koehn et al., 2007) with alignment heuristic set to “grow-diag-final-and”, and reordering heuristic to “msd-bidirectional-fe”. For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008). For tuning the feature weights we use MERT (Och, 2003) optimizing TER (Snover et al., 2006). We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation metrics, we also measure precision of our APE system using sentence level TER score as defined in Chatterjee et al. (2015a) Number of Improved Sentences Precision = Number of Modified Sentences Source factors: The factor on the source side of the parallel corpus is obtained following the approach to obtain the joint representation (as described in Section 2) for context-aware APE, the only difference"
W16-2377,W08-0509,0,0.058532,"&gt;class-id. This allows us to improve the fluency of the trans746 Source (mt_word|source_word) Parallel Corpus Target (pe_word|pos-tag|class-id) Siehe|See Bemalen|Paint_on von|on 3DModellen|3D_models .|. Siehe|ADV|104 &quot;|$(|373 Bemalen|NN|40 von| APPR|382 3D-Modellen|NN|137 .|$.|451 &quot;|$(|373 Bildrate|Framerate des|of_the Videos|video MP4|MP4 .|. Bildrate|NN|339 des|ART|407 MP4-Videos|NN |41 .|$.|451 Table 2: Example of parallel corpus with factored representation. lations by scoring them with both part-ofspeech tag and class-based language models. mt pairs of the training data by using MGIZA++ (Gao and Vogel, 2008). To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES (Koehn et al., 2007) with alignment heuristic set to “grow-diag-final-and”, and reordering heuristic to “msd-bidirectional-fe”. For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008). For tuning the feature weights we use MERT (Och, 2003) optimizing TER (Snover et al., 2006). We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation metrics,"
W16-2377,P13-4014,1,0.894252,"Missing"
W16-2377,W11-2123,0,0.020343,"Modified Sentences” consists in all the APE outputs that have TER scores different from the TER of the corresponding MT output. Data set and Experimental setup As defined in the shared task, the training data (English-German) consist of 12K triplets of source (src), MT output (mt), and human post-edits (pe). We split the development data (consisting of 1K triplets) released in this shared task into 400 and 600 triplets (selected randomly) to tune and evaluate our APE systems. We use the pe from the training data to build a 5-gram word-based statistical language model using the KENLM toolkit (Heafield, 2011), and 8-gram POS-tag and classbased language model using both KENLM (statistical) and the NPLM (neural) (Vaswani et al., 2013) toolkit. To build the joint representation (mt#src) and to obtain source factors (mt|src), we use the word alignment model trained on src and 1 5 Experiments and Results Baseline: For internal evaluation we consider the MT system as one of the baselines (an APE system outputting the input sentence), and the two variants of phrase-based APE as described in Section 2. The monolingual variant is labeled as APE-1 and the context-aware as APE-2. The baseline results reporte"
W16-2377,P14-1067,1,0.900007,"Missing"
W16-2377,D07-1091,0,0.0831568,"ost-edited) pairs, it loses connection with the source sentence, which implies that information lost or distorted in the machine translation process are impossible to recover by the APE system. This issue was addressed by the context-aware variant that annotates each word in the machine translated text by the corresponding source word (obtained from word alignment information between the mt and source text) to form a joint representation (mt#source) that represents the new source side of the parallel corpus (as shown in Table 1). Source 3 The factored machine translation model was proposed by Koehn and Hoang (2007). It enables a straightforward integration of additional annotation (called factors) at the word-level. These factors can be linguistic markup or automatically generated word classes. To build our factored APE systems, we pre-process the training data to obtain the factored representation. A fragment of our parallel corpus with factored representation is shown in Table 2. The source side of the parallel corpus has 2 factors (mt word and source word, similar to the joint representation), and the target side contains 3 factors (pe word, pos-tag, and class-id). In this representation we can defin"
W16-2377,D13-1140,0,0.0195894,"output. Data set and Experimental setup As defined in the shared task, the training data (English-German) consist of 12K triplets of source (src), MT output (mt), and human post-edits (pe). We split the development data (consisting of 1K triplets) released in this shared task into 400 and 600 triplets (selected randomly) to tune and evaluate our APE systems. We use the pe from the training data to build a 5-gram word-based statistical language model using the KENLM toolkit (Heafield, 2011), and 8-gram POS-tag and classbased language model using both KENLM (statistical) and the NPLM (neural) (Vaswani et al., 2013) toolkit. To build the joint representation (mt#src) and to obtain source factors (mt|src), we use the word alignment model trained on src and 1 5 Experiments and Results Baseline: For internal evaluation we consider the MT system as one of the baselines (an APE system outputting the input sentence), and the two variants of phrase-based APE as described in Section 2. The monolingual variant is labeled as APE-1 and the context-aware as APE-2. The baseline results reported in Table 4 show that the naive monolingual APE system already outperforms the MT baseline by 1.5 BLEU score. However, the lo"
W16-2377,P07-2045,0,0.041382,"_word|pos-tag|class-id) Siehe|See Bemalen|Paint_on von|on 3DModellen|3D_models .|. Siehe|ADV|104 &quot;|$(|373 Bemalen|NN|40 von| APPR|382 3D-Modellen|NN|137 .|$.|451 &quot;|$(|373 Bildrate|Framerate des|of_the Videos|video MP4|MP4 .|. Bildrate|NN|339 des|ART|407 MP4-Videos|NN |41 .|$.|451 Table 2: Example of parallel corpus with factored representation. lations by scoring them with both part-ofspeech tag and class-based language models. mt pairs of the training data by using MGIZA++ (Gao and Vogel, 2008). To develop the APE systems we use the phrase-based statistical machine translation toolkit MOSES (Koehn et al., 2007) with alignment heuristic set to “grow-diag-final-and”, and reordering heuristic to “msd-bidirectional-fe”. For building the word alignment models we use MGIZA++ (Gao and Vogel, 2008). For tuning the feature weights we use MERT (Och, 2003) optimizing TER (Snover et al., 2006). We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation metrics, we also measure precision of our APE system using sentence level TER score as defined in Chatterjee et al. (2015a) Numb"
W17-4713,W14-3346,0,0.101337,"ssing steps. In this paper, we use Lucene (McCandless et al., 2010), an open-source information retrieval library that is highly optimized for text search purposes. However, since the similarity measure used in Lucene is based on tfidf counts (Baeza-Yates and Ribeiro-Neto, 2011), it does not consider the order of the words and ngrams in the query and in the retrieved sentences, which is an important aspect for MT model training. In order to take advantage also of this information, we first query Lucene to retrieve a large set of candidates and then re-score them using the sentence-level BLEU (Chen and Cherry, 2014), so that the sentences with higher BLEU score are ranked first. Finally, the top-n similar sentences are used to update the model. This approach is reasonably fast, since it takes advantage of Lucene in searching in a large set of data and then computes the BLEU scores on just few candidates. 5 Experimental Setup 5.1 Data Our experiments are carried out on an English to French translation task, where the training data is a collection of publicly available corpora from different domains: European Central Bank (ECB), Gnome, JRC-Acquis (JRC), KDE4, OpenOffice (OOffice), PHP, Ubuntu, and translat"
W17-4713,D14-1179,0,0.0100254,"Missing"
W17-4713,W07-0733,0,0.0106122,"use scenarios (language combinations, genres, domains). On the other side, the infrastructures required to reach this objective should be scalable 127 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 127–137 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics field mostly rely on the assumption of knowing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the"
W17-4713,L18-1146,0,0.207046,"Missing"
W17-4713,N13-1073,0,0.041585,"aximum sentence length is set to 50. The models are trained using Adagrad (Duchi et al., 2011) by reshuffling the training set at each epoch, and are evaluated every 10,000 mini-batches with BLEU (Papineni et al., 2002). 5.3 Terms of Comparison We compare our adaptive NMT system with a generic NMT and a strong PBMT system trained on the pool of all the training data. For training the PBMT system we used the open source Moses 4 131 https://github.com/rsennrich/nematus best sentence pair for updating the model. toolkit (Koehn et al., 2007). The word alignment models were trained with FastAlign (Dyer et al., 2013). We trained the 5-gram language models with the KenLM toolkit (Heafield et al., 2013) on the target side of the pooled corpora. Feature weights were tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the dev set. Details of the generic NMT system are described in Section 5.2. In Table 3, the results on the dev set are reported. Although trained on the same dataset, it is interesting to note that, the performance of the generic NMT system is by far lower than the PBMT system. A possible explanation is that the PBMT system can explicitly memorise and use translation options lea"
W17-4713,eck-etal-2004-language,0,0.149515,"Missing"
W17-4713,2015.iwslt-evaluation.11,0,0.36339,"proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report significant improvements by adapting the generic system to the target domain as an offline step (Luong and Manning, 2015). More recently, (Li et al., 2016)1 proposed an instance-based adaptation technique for NMT in which for each translation segment a set of similar sentence pairs is retrieved. This small training set is then used to update the model before translating the given test sentence. In order to reduce the cost of computing the similarity of the test segment and all the sentences in the pool, they suggest a three-step process in which, as the first step, all the sentence pairs containing at least one of the test words are retrieved. This large set is then filtered by measuring the similarity between t"
W17-4713,W07-0717,0,0.0167602,"domains). On the other side, the infrastructures required to reach this objective should be scalable 127 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 127–137 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics field mostly rely on the assumption of knowing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report signif"
W17-4713,P02-1040,0,0.120401,"ds. As recommended in (Sennrich et al., 2016), in order to increase the consistency in segmenting the source and target text, we combined both sides of the training data, and set the number of merge rules to 89,500, resulting in vocabularies of size 78K and 86K tokens respectively for English and French. We use mini-batches of size 100, word embeddings of size 500, and GRU layers of size 1,024. The maximum sentence length is set to 50. The models are trained using Adagrad (Duchi et al., 2011) by reshuffling the training set at each epoch, and are evaluated every 10,000 mini-batches with BLEU (Papineni et al., 2002). 5.3 Terms of Comparison We compare our adaptive NMT system with a generic NMT and a strong PBMT system trained on the pool of all the training data. For training the PBMT system we used the open source Moses 4 131 https://github.com/rsennrich/nematus best sentence pair for updating the model. toolkit (Koehn et al., 2007). The word alignment models were trained with FastAlign (Dyer et al., 2013). We trained the 5-gram language models with the KenLM toolkit (Heafield et al., 2013) on the target side of the pooled corpora. Feature weights were tuned with batch MIRA (Cherry and Foster, 2012) to"
W17-4713,P16-1162,0,0.0622272,"show, these corpora are extremely diverse in terms of average sentence length and word frequency, which are likely to correspond to different 5.2 Neural MT System All our experiments with NMT are conducted with an in-house developed and maintained branch of the Nematus toolkit4 which is an implementation of the attentional encoder-decoder architecture (Bahdanau et al., 2014). Since handling large vocabularies is one of the main bottlenecks for the existing NMT systems, state-of-the-art approaches are trained on corpora in which the less frequent words are segmented into their sub-word units (Sennrich et al., 2016) by applying a modified version of the byte pair encoding (BPE) compression algorithm (Gage, 1994). This makes the NMT systems capable of dealing with new and rare words. As recommended in (Sennrich et al., 2016), in order to increase the consistency in segmenting the source and target text, we combined both sides of the training data, and set the number of merge rules to 89,500, resulting in vocabularies of size 78K and 86K tokens respectively for English and French. We use mini-batches of size 100, word embeddings of size 500, and GRU layers of size 1,024. The maximum sentence length is set"
W17-4713,P13-2121,0,0.0167917,"Missing"
W17-4713,C04-1059,0,0.0395585,"lable 127 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 127–137 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics field mostly rely on the assumption of knowing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report significant improvements by adapting the generic system to the target domain as an offline step ("
W17-4713,2005.eamt-1.19,0,0.0514629,"wing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report significant improvements by adapting the generic system to the target domain as an offline step (Luong and Manning, 2015). More recently, (Li et al., 2016)1 proposed an instance-based adaptation technique for NMT in which for each translation segment a set of similar sentence pairs is retrieved. This small training set is then used to update the"
W17-4713,P07-2045,1,0.0170856,"f size 100, word embeddings of size 500, and GRU layers of size 1,024. The maximum sentence length is set to 50. The models are trained using Adagrad (Duchi et al., 2011) by reshuffling the training set at each epoch, and are evaluated every 10,000 mini-batches with BLEU (Papineni et al., 2002). 5.3 Terms of Comparison We compare our adaptive NMT system with a generic NMT and a strong PBMT system trained on the pool of all the training data. For training the PBMT system we used the open source Moses 4 131 https://github.com/rsennrich/nematus best sentence pair for updating the model. toolkit (Koehn et al., 2007). The word alignment models were trained with FastAlign (Dyer et al., 2013). We trained the 5-gram language models with the KenLM toolkit (Heafield et al., 2013) on the target side of the pooled corpora. Feature weights were tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the dev set. Details of the generic NMT system are described in Section 5.2. In Table 3, the results on the dev set are reported. Although trained on the same dataset, it is interesting to note that, the performance of the generic NMT system is by far lower than the PBMT system. A possible explanation is t"
W17-4713,N12-1047,0,\N,Missing
W17-4716,C04-1046,0,0.0634356,"words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-specific terms. The presence of few annotated terms poses fewer constraints to the decoder in generating the output sentence. The second scenario is an automatic post-editing (APE) task, in which the NMT model is trained to translate “monolingually” from draft machine-translated sentences into humanquality post-edits. The external guidance is provided by word-level quality judgements (Blatz et al., 2004) indicating the “good” words in the machine-translated sentence that should be kept in the final APE output. In this case, the large number of “good” words already present in the original MT output poses more constraints to the decoding process. In both scenarios, our guidance mechanism achieves significant performance gains over the original NMT decoder. 2 Related Work In PBSMT, the injection of external knowledge in the decoder is usually handled with the socalled XML markup, a technique used to guide the decoder by supplying the desired translation for some of the source phrases. The suppli"
W17-4716,D16-1162,0,0.0331893,"replacing entries that cover the specific source phrase, or adding the alternative phrase translations to it, so that they are in competition. This problem has only recently started to be explored in NMT and, in most of the cases, the proposed solutions integrate external knowledge at training stage. Time-consuming training routines, however, limit the suitability of this strategy for applications requiring real-time translations. In Gulcehre et al. (2015), monolingual data is used to train a neural language model that is integrated in the NMT decoder by concatenating their hidden states. In Arthur et al. (2016), the probability of the next target word in the NMT decoder is biased by using lexicon probabilities computed from a bilingual lexicon. When the external knowledge is The closest approach to ours is the one by (Hokamp and Liu, 2017). They explore all the possible constraints (or translation options) at each time step making sure not to generate a constraint that have already been generated in the previous timestep. Their approach generates all the constraints in the final output, thus implicitly it assumes that only one translation options is provided as constraint for a given source word/phr"
W17-4716,E17-1050,1,0.888712,"Missing"
W17-4716,P17-1141,0,0.0370356,"the proposed solutions integrate external knowledge at training stage. Time-consuming training routines, however, limit the suitability of this strategy for applications requiring real-time translations. In Gulcehre et al. (2015), monolingual data is used to train a neural language model that is integrated in the NMT decoder by concatenating their hidden states. In Arthur et al. (2016), the probability of the next target word in the NMT decoder is biased by using lexicon probabilities computed from a bilingual lexicon. When the external knowledge is The closest approach to ours is the one by (Hokamp and Liu, 2017). They explore all the possible constraints (or translation options) at each time step making sure not to generate a constraint that have already been generated in the previous timestep. Their approach generates all the constraints in the final output, thus implicitly it assumes that only one translation options is provided as constraint for a given source word/phrase. However, in a more realistic scenario (e.g. in presence of a termbase or when the target language is more inflected than the source language), a source word can have multiple translation options from which the decoder should dec"
W17-4716,P15-1001,0,0.166045,"lberg et al., 2016) proposed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015) and Luong and Manning (2015) replace the unknown words either with the most likely aligned source word or with the translation determined by another word alignment model. source words with the corresponding translation options. The guidance mechanism supervises the process, generating the final output with the expected translations, in the right place, including cases of external words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-speci"
W17-4716,W15-3025,1,0.843161,"signed&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al., 2014) to annotate the “g"
W17-4716,W16-2378,0,0.0509962,"on from the QE annotations. Base-APE improves the Base-MT up to 3.14 BLEU points. Similar to §5.2, the evaluation of our guided decoder is performed incrementally. GDec_base forces the “good” words in the automatic translation to appear in the output according to the mechanism described in §4.1. This basic guidance mechanism yields only marginal improvements over the Base-MT and is far behind the Base-APE. This can be explained by the large number of constraints (i.e. “good” words to be Experimental setting NMT models. We use the pre-trained model built for the best English-German submission (Junczys-Dowmunt and Grundkiewicz, 2016) at the WMT’16 APE task. This available model was trained with Nematus over a data set of ∼4M back-translated pairs, and then adapted to the taskspecific data segmented using the BPE technique. 164 Src: <n translation=&quot;durchsuchen||Durchsuchen&quot;&gt; browse </n&gt; all products Base: stöbern alle Produkte GDec: durchsuchen Sie alle Produkte Ref: durchsuchen Sie alle Produkte Src: <n translation=&quot;produkt||Produkt&quot;&gt; Product </n&gt; 4 - <n translation=&quot;plastics||Plastics&quot;&gt; Plastics </n&gt; <n translation=&quot;labs||Labs&quot;&gt; Labs </n&gt; Base: Produkt 4 - Kunststofflabore GDec: Produkt 4 - Plastics Labs Ref: Produkt 4 -"
W17-4716,P15-2026,1,0.665898,"signed&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al., 2014) to annotate the “g"
W17-4716,W04-3250,0,0.035147,"ed model built for the best EnglishGerman submission (Sennrich et al., 2016a) at the News Translation task at WMT’16 (Bojar et al., 2016). At test stage, it is supplied with terminology lists containing term recommendations in BPE format. In all the experiments we use a default beam size of 12. 5.2 Results and discussion Our results on the MT task are reported in Table 1, which shows system performance on the concatenation of the test sets from the two target domains. Performance is measured with BLEU (Papineni et al., 2002), and statistical significance is computed with bootstrap resampling (Koehn, 2004). The result of the word-level baseline system is computed after post-processing its output following the approach of Jean et al. (2015), which was customized to our scenario. This method (see §2) is driven by the attention model to replace the UNK tokens in the output with their corresponding recommendation supplied as external knowledge. This post-processing strategy is not used for the BPE-level baseline because it implicitly addresses the problem of OOVs. We evaluate our guided decoder incrementally, by adding one at a time the mechanisms described in §4. In the discussion, we do not compa"
W17-4716,W14-4012,0,0.0668892,"Missing"
W17-4716,2005.mtsummit-papers.11,0,0.126246,"r knowledge supplied as translation recommendations for domainspecific terms. The suggested terms (i.e. the constraints posed to the decoder) are usually few, thus leaving a large degree of freedom to the NMT decoder while generating the output. 5.1 Experimental setting NMT models. We evaluate guided decoding in its ability to improve the performance of two different English to German NMT models, both obtained with the Nematus toolkit (Sennrich et al., 2016a). The first system operates at word level and it is trained by using part of the JRC-Acquis corpus (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), which results in at total of about 1.8M parallel sentence pairs. The size of the vocabulary, word embedding, and hidden units is respectively set to 40K, 600, and 600, and parameters are optimised with Adagrad (Duchi et al., 2011) using a learning rate of 0.01. The batch size is set to 100, and the model is trained for 300K updates (∼17 epochs). At test stage, the word-level system is supplied with terminology lists containing term recommendations at the level of granularity of full words. The second system is trained on sub-word units by using the Byt"
W17-4716,W14-3340,1,0.91221,"Missing"
W17-4716,2015.iwslt-evaluation.11,0,0.0709613,"posed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015) and Luong and Manning (2015) replace the unknown words either with the most likely aligned source word or with the translation determined by another word alignment model. source words with the corresponding translation options. The guidance mechanism supervises the process, generating the final output with the expected translations, in the right place, including cases of external words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-specific terms. The presence of fe"
W17-4716,P02-1040,0,0.122482,"a successful way to reduce the OOV rate. The system used in our evaluation is the pre-trained model built for the best EnglishGerman submission (Sennrich et al., 2016a) at the News Translation task at WMT’16 (Bojar et al., 2016). At test stage, it is supplied with terminology lists containing term recommendations in BPE format. In all the experiments we use a default beam size of 12. 5.2 Results and discussion Our results on the MT task are reported in Table 1, which shows system performance on the concatenation of the test sets from the two target domains. Performance is measured with BLEU (Papineni et al., 2002), and statistical significance is computed with bootstrap resampling (Koehn, 2004). The result of the word-level baseline system is computed after post-processing its output following the approach of Jean et al. (2015), which was customized to our scenario. This method (see §2) is driven by the attention model to replace the UNK tokens in the output with their corresponding recommendation supplied as external knowledge. This post-processing strategy is not used for the BPE-level baseline because it implicitly addresses the problem of OOVs. We evaluate our guided decoder incrementally, by addin"
W17-4716,C16-1172,0,0.0268576,"Missing"
W17-4716,W08-0509,0,0.019382,"Missing"
W17-4716,W16-2209,0,0.0179184,"ations of specific words or phrases, are a typical example of external knowledge used to guide the process to meet such constraints. Meeting predefined constraints, however, does not represent the only case in which an external guidance can support decoding. In ensemble MT architectures, for example, the output of a translation system 157 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 157–168 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics in the form of linguistic information, such as POS tags or lemmas, Sennrich and Haddow (2016) propose to compute separate embedding vectors for each linguistic information and then concatenate them, without altering the decoder. Other solutions exploit the strengths of PBSMT systems to improve NMT by pre-translating the source sentence. In Niehues et al. (2016), the NMT model is fed with a concatenation of the source and its PBSMT translation. Some of these solutions lead to improvements in performance, but they all require time-intensive training of the NMT models to use an enriched input representation or to optimize the parameters of the model. (Stahlberg et al., 2016) proposed an"
W17-4716,P16-1162,0,0.317137,"l knowledge without retraining of paramount importance. To address this gap, we investigate problems arising from the fact that NMT operates on implicit word and sentence representations in a continuous space, which makes influencing the process with external knowledge more complex. In particular, we attempt to answer the following questions: i) How to enforce the presence of a given translation recommendation in the decoder’s output? ii) How to place these word(s) in the right position? iii) How to guide the translation of outof-vocabulary terms? Our solution extends an existing NMT decoder (Sennrich et al., 2016a) by introducing the possibility to guide the translation process with constraints provided as XML annotations of the Differently from the phrase-based paradigm, neural machine translation (NMT) operates on word and sentence representations in a continuous space. This makes the decoding process not only more difficult to interpret, but also harder to influence with external knowledge. For the latter problem, effective solutions like the XML-markup used by phrase-based models to inject fixed translation options as constraints at decoding time are not yet available. We propose a “guide” mechani"
W17-4716,N07-1064,0,0.0222422,"f the source word &quot;assigned&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al.,"
W17-4716,2006.amta-papers.25,0,0.147198,"Missing"
W17-4716,P16-2049,0,0.0166403,"r lemmas, Sennrich and Haddow (2016) propose to compute separate embedding vectors for each linguistic information and then concatenate them, without altering the decoder. Other solutions exploit the strengths of PBSMT systems to improve NMT by pre-translating the source sentence. In Niehues et al. (2016), the NMT model is fed with a concatenation of the source and its PBSMT translation. Some of these solutions lead to improvements in performance, but they all require time-intensive training of the NMT models to use an enriched input representation or to optimize the parameters of the model. (Stahlberg et al., 2016) proposed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015)"
W17-4716,C00-2137,0,0.0534122,"al., 2014) to annotate the “good” words that should be kept. Due to the relatively high quality of the MT outputs (62.11 BLEU), source sentences will usually contain many terms annotated as “good”. This, compared to the MT task, poses more constraints on the decoder. 6.1 6.2 Results and discussion Our results on the APE task are reported in Table 3. Performance is measured with the two WMT’16 APE task metrics, namely TER and BLEU (Bojar et al., 2016). The statistical significance for BLEU is computed using paired bootstrap resampling, while for TER we use stratified approximate randomization (Yeh, 2000). Our first baseline (Base-MT), the same used at WMT, corresponds to the original MT output left untouched. Our second baseline (Base-APE) is a neural APE system that was trained on (MT_output, MT_post-edit) pairs but ignores the information from the QE annotations. Base-APE improves the Base-MT up to 3.14 BLEU points. Similar to §5.2, the evaluation of our guided decoder is performed incrementally. GDec_base forces the “good” words in the automatic translation to appear in the output according to the mechanism described in §4.1. This basic guidance mechanism yields only marginal improvements"
W17-4716,steinberger-etal-2006-jrc,0,\N,Missing
W17-4716,P07-2045,1,\N,Missing
W17-4716,W16-2323,0,\N,Missing
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4773,2011.mtsummit-papers.35,0,0.590859,"Missing"
W17-4773,W16-2377,1,0.904407,"Missing"
W17-4773,E17-1050,1,0.854521,"can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) postediting effort and iii) adapt the output of a generalpurpose MT system to the lexicon/style requested in a specific application domain. Different APE paradigms based on statistical methods (Simard et al., 2007; Dugast et al., 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatterjee et al., 2015b) and neural (Libovick´y et al., 2016) also suggested the importance of jointly learning both from the source sent"
W17-4773,W15-3025,1,0.947772,", 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatterjee et al., 2015b) and neural (Libovick´y et al., 2016) also suggested the importance of jointly learning both from the source sentences and from the corresponding translations in order to take advantage of the strict dependency between translation errors and the original source sentences. Learning from these lessons, this year the FBK participation in the APE task is based on a multisource neural sequence-to-sequence architecture. We extend the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016a) to facilitate multi-source training and decoding. This year we participated in both transl"
W17-4773,N09-2055,0,0.040377,"Missing"
W17-4773,P15-2026,1,0.925704,", 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatterjee et al., 2015b) and neural (Libovick´y et al., 2016) also suggested the importance of jointly learning both from the source sentences and from the corresponding translations in order to take advantage of the strict dependency between translation errors and the original source sentences. Learning from these lessons, this year the FBK participation in the APE task is based on a multisource neural sequence-to-sequence architecture. We extend the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016a) to facilitate multi-source training and decoding. This year we participated in both transl"
W17-4773,N12-1047,0,0.0203607,"m achieves significant improvement over the MT baseline (-5.4 TER and 8.7 BLEU points) also on the 2016 test set. Systems MT Baseline APE Baseline Ens8+Re-rank-A Ensemble (Ens8) In order to leverage all the network architectures discussed above, we ensemble the two best models for each of them. Since the networks are very diverse in terms of information learned from the input representation we observed that weighing all the models equally does not improve over the single system. Therefore, we generate 50-best hypothesis from the ensemble system and then tune the model weights with Batch-MIRA (Cherry and Foster, 2012) on the development set to maximize the BLEU score. We observe that, after 3 cycles of decoding and tuning, the performance converges. The weighted ensemble of 8 models further improves the translation quality (-0.8 TER and +1.1 BLEU) over the best single multi-source model (MT+SRC PE). TER 24.76 24.64 19.32† BLEU 62.11 63.47 70.88† Table 3: Performance of the APE systems on the 2016 test set (en-de) (“†” indicates statistically significant differences wrt. MT Baseline with p&lt;0.05). Statistical (Re-rank-AB) This re-ranker is similar to the one used in (Pal et al., 2017). The feature set consis"
W17-4773,W16-2361,0,0.0977145,"Missing"
W17-4773,E17-2056,1,0.881001,"Missing"
W17-4773,W07-0732,0,0.0695889,"more radical internal modifications. As pointed out in (Bojar et al., 2015), from the application point of view an APE system can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) postediting effort and iii) adapt the output of a generalpurpose MT system to the lexicon/style requested in a specific application domain. Different APE paradigms based on statistical methods (Simard et al., 2007; Dugast et al., 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatte"
W17-4773,P16-2046,1,0.72219,"Missing"
W17-4773,W16-2379,1,0.873345,"Missing"
W17-4773,P02-1040,0,0.0985144,"n Tables 1 and 2. The performance trends among different networks are similar for both language directions. However, the variation are less visible in the case of de-en given the fact that the room of improvement is much lower due to higher MT quality (15.58 TER and 79.46 BLEU scores). Therefore, we base our discussion for each model below on the results achieved on the development data for the en-de direction, where the performance variations among different networks are much more visible. Evaluation Metric We run case-sensitive evaluation with TER, which is based on edit distance, and BLEU (Papineni et al., 2002), which is based on modified n-gram precision. In addition to the standard evaluation 2 https://github.com/amunmt/amunmt/ wiki/AmuNMT-for-Automatic-Post-Editing 3 http://www.statmt.org/wmt17/ape-task. html 633 SRC PE This system is similar to a NMT system used for bilingual translation from a source language to a target language. The parallel corpus consist of source text and post-edits of MT segments. We notice that the performance of this system is below the MT Baseline indicating that learning only from the source text is not enough to improve the translation quality. Most likely, this syst"
W17-4773,2012.iwslt-papers.19,0,0.035867,"al., 2015), from the application point of view an APE system can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) postediting effort and iii) adapt the output of a generalpurpose MT system to the lexicon/style requested in a specific application domain. Different APE paradigms based on statistical methods (Simard et al., 2007; Dugast et al., 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatterjee et al., 2015b) and neural (Libovick´y et al., 2016) also sug"
W17-4773,2007.mtsummit-papers.34,0,0.061602,"l modifications. As pointed out in (Bojar et al., 2015), from the application point of view an APE system can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) postediting effort and iii) adapt the output of a generalpurpose MT system to the lexicon/style requested in a specific application domain. Different APE paradigms based on statistical methods (Simard et al., 2007; Dugast et al., 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatterjee et al., 2015b) and"
W17-4773,P13-3025,0,0.0167829,"application point of view an APE system can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) postediting effort and iii) adapt the output of a generalpurpose MT system to the lexicon/style requested in a specific application domain. Different APE paradigms based on statistical methods (Simard et al., 2007; Dugast et al., 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatterjee et al., 2015b) and neural (Libovick´y et al., 2016) also suggested the importan"
W17-4773,E17-3017,0,0.0518769,"Missing"
W17-4773,P16-1162,0,0.426112,"e best in 2016. Some of the previous approaches, both phrase-based (B´echara et al., 2011; Chatterjee et al., 2015b) and neural (Libovick´y et al., 2016) also suggested the importance of jointly learning both from the source sentences and from the corresponding translations in order to take advantage of the strict dependency between translation errors and the original source sentences. Learning from these lessons, this year the FBK participation in the APE task is based on a multisource neural sequence-to-sequence architecture. We extend the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016a) to facilitate multi-source training and decoding. This year we participated in both translation directions Previous phrase-based approaches to Automatic Post-editing (APE) have shown that the dependency of MT errors from the source sentence can be exploited by jointly learning from source and target information. By integrating this notion in a neural approach to the problem, we present the multi-source neural machine translation (NMT) system submitted by FBK to the WMT 2017 APE shared task. Our system implements multi-source NMT in a weighted ensemble of 8 models. The n-best hypotheses prod"
W17-4773,N07-1064,0,0.428785,"for retraining or for more radical internal modifications. As pointed out in (Bojar et al., 2015), from the application point of view an APE system can help to: i) improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; ii) provide professional translators with improved MT output quality to reduce (human) postediting effort and iii) adapt the output of a generalpurpose MT system to the lexicon/style requested in a specific application domain. Different APE paradigms based on statistical methods (Simard et al., 2007; Dugast et al., 2007; Isabelle et al., 2007; Lagarda et al., 2009; Potet et al., 2012; Rosa et al., 2013; Lagarda et al., 2015; Chatterjee et al., 2017) have been proposed in the past showing the effectiveness of APE systems. In the previous round of the APE shared task (WMT16), neural (Junczys-Dowmunt and Grundkiewicz, 2016), hybrid (Chatterjee et al., 2016), and phrase-based (Pal et al., 2016b) solutions were all able to significantly improve MT output quality in domain-specific settings, with neural system being the best in 2016. Some of the previous approaches, both phrase-based (B´echara"
W17-4773,W15-3001,1,\N,Missing
W17-4773,W16-2378,0,\N,Missing
W17-4773,W16-2323,0,\N,Missing
W17-4773,W16-2301,1,\N,Missing
W18-1804,2011.mtsummit-papers.35,0,0.210498,"Missing"
W18-1804,W14-3302,1,0.872855,"Missing"
W18-1804,W15-3001,1,0.883251,"Missing"
W18-1804,W14-3340,1,0.893289,"Missing"
W18-1804,P15-1022,1,0.889823,"Missing"
W18-1804,W12-3102,1,0.867471,"Missing"
W18-1804,W17-4773,1,0.851701,"Missing"
W18-1804,W17-4716,1,0.771004,"stimation-task.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 28 In terms of methods, early works rely on a statistical phrase-based approach (Simard et al., 2007), possibly integrating source information to reduce systems’ tendency to over-correct and enhance their precision (B´echara et al., 2011; Chatterjee et al., 2015). More recently, large performance gains comparable to those achieved in MT by the adoption of neural approaches have also been observed in APE (Junczys-Dowmunt and Grundkiewicz, 2016), especially with multi-source neural systems (Chatterjee et al., 2017a) enhancing decoding with source language information. In order to evaluate the effects of combining QE with different APE technologies, the experiments discussed in Section 4.3 are performed with both phrase-based (Chatterjee et al., 2016) and neural-based (Junczys-Dowmunt and Grundkiewicz, 2016) architectures. The latter is the winning system at the WMT 2016 APE shared task.3 For a comprehensive overview of the evolution of the APE task and the proposed approaches, we refer the reader to the aforementioned WMT literature. 2.3 Combination of QE and APE So far, the interaction and the possibl"
W18-1804,P15-2026,1,0.764604,"tput are associated with a human-corrected version of the translation. Under this general formulation, previous work concentrated on several aspects, with outcomes that indicate a steady evolution of the approaches. 2 http://www.statmt.org/wmt17/quality-estimation-task.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 28 In terms of methods, early works rely on a statistical phrase-based approach (Simard et al., 2007), possibly integrating source information to reduce systems’ tendency to over-correct and enhance their precision (B´echara et al., 2011; Chatterjee et al., 2015). More recently, large performance gains comparable to those achieved in MT by the adoption of neural approaches have also been observed in APE (Junczys-Dowmunt and Grundkiewicz, 2016), especially with multi-source neural systems (Chatterjee et al., 2017a) enhancing decoding with source language information. In order to evaluate the effects of combining QE with different APE technologies, the experiments discussed in Section 4.3 are performed with both phrase-based (Chatterjee et al., 2016) and neural-based (Junczys-Dowmunt and Grundkiewicz, 2016) architectures. The latter is the winning syste"
W18-1804,W17-4775,0,0.212885,"output of an APE system as an extra feature to boost the performance of a neural QE architecture. The intuition is that word-level quality labels can be automatically obtained through TER (Snover et al., 2006) alignments between the translated and the post-edited sentence (used as “pseudo human post-edit”). The resulting APE-based QE system achieves state-of-the-art performance at the word and sentence-level QE tasks on the WMT 2015 and WMT 2016 data sets. Another line of research that is closer to our work has focused on improving APE performance by leveraging word-level QE predictions. In (Hokamp, 2017), this is done by incorporating word-level features as factors in the input (i.e. a concatenation of different word embedding representations) to a neural APE system. By taking a different approach, Chatterjee et al. (2017b) explore a “guided-decoding” mechanism to guide a neural APE system with word-level binary QE judgments. The idea of constraining the decoding process (i.e. forcing the system to keep “good” tokens in the APE output) is the basis for the integration approach described in Section 3.2: “QE as APE guidance”. A third solution for the integration of QE and APE is explored in (Ch"
W18-1804,W16-2378,0,0.778111,"indicate a steady evolution of the approaches. 2 http://www.statmt.org/wmt17/quality-estimation-task.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 28 In terms of methods, early works rely on a statistical phrase-based approach (Simard et al., 2007), possibly integrating source information to reduce systems’ tendency to over-correct and enhance their precision (B´echara et al., 2011; Chatterjee et al., 2015). More recently, large performance gains comparable to those achieved in MT by the adoption of neural approaches have also been observed in APE (Junczys-Dowmunt and Grundkiewicz, 2016), especially with multi-source neural systems (Chatterjee et al., 2017a) enhancing decoding with source language information. In order to evaluate the effects of combining QE with different APE technologies, the experiments discussed in Section 4.3 are performed with both phrase-based (Chatterjee et al., 2016) and neural-based (Junczys-Dowmunt and Grundkiewicz, 2016) architectures. The latter is the winning system at the WMT 2016 APE shared task.3 For a comprehensive overview of the evolution of the APE task and the proposed approaches, we refer the reader to the aforementioned WMT literature."
W18-1804,W17-4763,0,0.0128066,"(Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produced by a machine translation system, typi"
W18-1804,P07-2045,0,0.0124647,"Missing"
W18-1804,W16-2385,0,0.0172752,"each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produced by a machine translation system, typically by exploiting knowledge acquired from human post-edits. APE research dates back to the work of (Knight and Chander, 1994; Simard et al., 2007), in which the problem is approached as a “monolingual translation” task. Similar"
W18-1804,W15-3037,0,0.0221818,"tion of online and multitask learning methods (Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produ"
W18-1804,2013.tc-1.10,1,0.7845,"Missing"
W18-1804,W13-2248,0,0.0309467,"king. Together with the batch learning solutions that characterize the majority of the proposed approaches, recent work also explored the application of online and multitask learning methods (Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch"
W18-1804,W16-2387,0,0.113116,"itask learning methods (Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produced by a machine trans"
W18-1804,Q17-1015,0,0.0149624,"m at the WMT 2016 APE shared task.3 For a comprehensive overview of the evolution of the APE task and the proposed approaches, we refer the reader to the aforementioned WMT literature. 2.3 Combination of QE and APE So far, the interaction and the possible joint contribution of QE and APE technology has been scarcely explored. This is particularly surprising if we consider that both the tasks can rely on the type of same training data consisting of (source, MT, post edited MT) parallel triplets, which in principle allow for knowledge transfer and model sharing. Building on this consideration, (Martins et al., 2017) exploited the synergies between the two related tasks by using the output of an APE system as an extra feature to boost the performance of a neural QE architecture. The intuition is that word-level quality labels can be automatically obtained through TER (Snover et al., 2006) alignments between the translated and the post-edited sentence (used as “pseudo human post-edit”). The resulting APE-based QE system achieves state-of-the-art performance at the word and sentence-level QE tasks on the WMT 2015 and WMT 2016 data sets. Another line of research that is closer to our work has focused on impr"
W18-1804,P02-1040,0,0.10271,"Missing"
W18-1804,P16-1162,0,0.0627687,"thors of the winning system at the WMT 2016 APE shared task (Junczys-Dowmunt and Grundkiewicz, 2016) using the large “round-trip translation” dataset, and then adapted to taskspeciﬁc data. The network parameters are: word embedding dimensionality of 600, hidden unit size of 1,024, maximum sentence length of 50, batch size of 80, and vocabulary size of 40K. The network parameters are optimized with Adadelta (Zeiler, 2012). For the guided decoder, the best value step of the look-ahead mechanism is deﬁned on the development set. The data is segmented using the Byte-Pair Encoding (BPE) technique (Sennrich et al., 2016). Each QE word-level annotation is projected to all the subword units. We only use a single MT → pe model instead of an ensemble of models. 5 It is important to note that there are several perfectly valid translations of the same input text, so the gold QE predictions that we use are a subset of possible oracle labels generated based on the available reference sentence. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 32 It is important to note that both the APE systems are strong. In fact, they signiﬁcantly improve over the MT output (respectively +2.64 an"
W18-1804,N07-1064,0,0.864484,"tion, adaptation and evaluation, but also to aspects that are external to the core translation approach. Among them, quality estimation (QE) and automatic post-editing (APE) deal respectively with the possibility of predicting the quality of MT output and correcting it via downstream postprocessing. QE (Specia et al., 2010) is motivated by the need for estimating output quality at Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 26 run-time, that is when reference-based evaluation is unfeasible (the typical scenario when MT is deployed in production). APE (Simard et al., 2007) is motivated by the need of improving MT systems’ output in black-box conditions, in which the translation models are not accessible for internal modiﬁcation, retraining or adaptation (a typical situation for companies that rely on third-party MT systems). Both QE and APE have been successfully explored as standalone tasks in previous work, in particular within the well-established framework of the Conference on Machine Translation (WMT1 ). In six editions of the WMT QE shared task (2012-2017), the MT quality prediction problem has been formulated in different ways (e.g. ranking, scoring) and"
W18-1804,2006.amta-papers.25,0,0.0977762,"QE and APE technology has been scarcely explored. This is particularly surprising if we consider that both the tasks can rely on the type of same training data consisting of (source, MT, post edited MT) parallel triplets, which in principle allow for knowledge transfer and model sharing. Building on this consideration, (Martins et al., 2017) exploited the synergies between the two related tasks by using the output of an APE system as an extra feature to boost the performance of a neural QE architecture. The intuition is that word-level quality labels can be automatically obtained through TER (Snover et al., 2006) alignments between the translated and the post-edited sentence (used as “pseudo human post-edit”). The resulting APE-based QE system achieves state-of-the-art performance at the word and sentence-level QE tasks on the WMT 2015 and WMT 2016 data sets. Another line of research that is closer to our work has focused on improving APE performance by leveraging word-level QE predictions. In (Hokamp, 2017), this is done by incorporating word-level features as factors in the input (i.e. a concatenation of different word embedding representations) to a neural APE system. By taking a different approach"
W18-1804,P14-1067,1,0.894496,"Missing"
W18-6452,D16-1025,0,0.0415751,"Missing"
W18-6452,W17-4730,0,0.0210998,"es on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary size, the system applies ad hoc pre-processing for the German language by re-implementing the pipeline developed by the best system at the WMT‘17 Translation task (Huck et al., 2017). In addition to the data released for the task, training is performed by taking advantage of both the artificial data provided by (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus (Negri et al., 2018). The submitted runs, which rely on the same multi-source architecture and pre-processing step, differ in the loss function used, which is either minimum-risk training alone (“MRT”), or its linear combination with maximum likelihood estimation (“MRT+MLE”). Participants Five participating teams submitted a total of 11 runs for the PBSMT subtask and 10 runs for the NMT subtask. Partici"
W18-6452,W16-2378,0,0.519466,"previous rounds. The TER (↓) and BLEU (↑) scores reported in Table 2 are computed using the human post-edits as reference. As discussed in (Bojar et al., 2017), numeric evidence of a higher quality of the original translations can indicate a smaller room for improvement for APE systems (having, at the same time, less to learn during training and less to corParticipants were also provided with additional training material for both the subtasks. One resource (called “Artificial” in Table 1) is the corpus of 4.5 million artificially-generated postediting triplets used by the 2016 winning system (Junczys-Dowmunt and Grundkiewicz, 2016). This corpus was widely used by participants in the 2017 round of the APE task. The other resource is the English-German section of the eSCAPE corpus (Negri et al., 2018). It comprises 14.5 million instances, which were artificially generated both via phrase-based and neural translation (7.25 millions each) of the same source sentences. Table 1 provides basic statistics about the data, which was released by the European Project QT21 (Specia et al., 2017). In addition, Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the four rounds of the APE"
W18-6452,P15-2026,1,0.890076,"e technology progress over the past. On the other side, extending the evaluation to NMTderived data was meant to explore the effectiveness of APE techniques, which now migrated to the neural paradigm, to correct data obtained with the same paradigm. In terms of participants and submitted runs, 5 teams produced respectively 11 runs for the PBSMT subtask and 10 runs for the NMT subtask. Introduction The WMT shared task on MT Automatic PostEditing (APE), this year at its fourth round, aims to evaluate systems for the automatic correction of errors in a machine-translated text. As pointed out by (Chatterjee et al., 2015), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by per1 As opposed to the 2017 round, in which both EnglishGerman and German-English data were considered. 710 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 710–725 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64079 All submissions were produced by neural APE systems. All the teams experimented"
W18-6452,W17-4773,1,0.908777,"ntion layers (4 and 8 respectively). calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “do-nothing” system that leaves all the test targets unmodified. Baseline results, the same shown in Table 2, are also reported in Tables 4 and 5 for comparison with participants’ submissions.7 For each submitted run, the statistical significance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary s"
W18-6452,P18-4020,0,0.0259359,"Missing"
W18-6452,W18-1804,1,0.845642,"Missing"
W18-6452,P17-4012,0,0.0450369,"tly trained to handle PBNMT and NMT outputs. This was achieved by adding, at the beginning of every MT segment to be corrected, a specific token indicating which type of MT system was used to produce it and from which training corpus the segment pair was extracted. (i.e. the WMT training data, the artificial training data presented in (Junczys-Dowmunt and Grundkiewicz, 2016), or the eSCAPE corpus (Negri et al., 2018)). The submitted runs were obtained with two neural architectures. One (“LSTM”) is an attentional RNN with gated units based on (Bahdanau et al., 2014) and implemented in OpenNMT (Klein et al., 2017). The other is the multi-head attention-only network (Vaswani et al., 2017) implemented in Microsoft & University of Edinburgh. MS UEdin’s neural APE system is based on the dual-source Transformer models available in Marian (Junczys-Dowmunt et al., 2018). The models are trained with tied embeddings across all embeddings matrices and shared parameters for all the encoders. The dual-source Transformer model is implemented by stacking an additional target-source multi-head component on the previ7 In addition to the do-nothing baseline, in previous rounds we also compared systems’ performance with"
W18-6452,W04-3250,0,0.0838393,"(i.e. “Transf.base” and “Transf.large”) were trained with different configurations in terms of parallel attention layers (4 and 8 respectively). calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “do-nothing” system that leaves all the test targets unmodified. Baseline results, the same shown in Table 2, are also reported in Tables 4 and 5 for comparison with participants’ submissions.7 For each submitted run, the statistical significance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias a"
W18-6452,W13-2305,0,0.0160731,"score. Average standardized scores for individual segments belonging to a given system are then computed, before the final overall DA score for that system is computed as the average of its segment scores. Figure 5: Screenshot of the direct assessment user interface. the PBSMT subtask. 6 Human evaluation In order to complement the automatic evaluation of APE submissions, a manual evaluation of the primary systems submitted (five in total) was conducted. Similarly to the manual evaluation conducted for last year APE shared task, it was carried out following the direct assessment (DA) approach (Graham et al., 2013; Graham et al., 2016). In this Section, we present the evaluation procedure as well as the results obtained. 6.1 Evaluation procedure The manual evaluation carried out this year involved 12 native German speakers with full professional proficiency in English in the IT domain, with a third of the evaluators being students in translation technologies from Saarland University and the remaining ones researchers and engineers from DFKI. Each evaluator was introduced to the evaluation task through a set of slides and a testing phase of the evaluation platform in order to be familiar with the user i"
W18-6452,L18-1004,1,0.931029,"uality of the original translations can indicate a smaller room for improvement for APE systems (having, at the same time, less to learn during training and less to corParticipants were also provided with additional training material for both the subtasks. One resource (called “Artificial” in Table 1) is the corpus of 4.5 million artificially-generated postediting triplets used by the 2016 winning system (Junczys-Dowmunt and Grundkiewicz, 2016). This corpus was widely used by participants in the 2017 round of the APE task. The other resource is the English-German section of the eSCAPE corpus (Negri et al., 2018). It comprises 14.5 million instances, which were artificially generated both via phrase-based and neural translation (7.25 millions each) of the same source sentences. Table 1 provides basic statistics about the data, which was released by the European Project QT21 (Specia et al., 2017). In addition, Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the four rounds of the APE task, we report the repetition rate of SRC, TGT and PE elements, as well as the TER (Snover et al., 2006) and the BLEU score (Papineni et al., 2002) of the TGT elements ("
W18-6452,W18-6468,0,0.100227,"Missing"
W18-6452,W13-0805,0,0.0175914,"t set consists of 2,000 newly-released instances. For the NMT subtask, the training and development set respectively consist of 13,442 and 1,000 triplets, while the test set comprises 1,023 instances. Task description Similar to previous years, participants were provided with training and development data consisting of (source, target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data 3 We used a phrase-based MT system trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 4 The NMT system was trained with generic and indomain parallel training data using the attentional encoderdecoder architecture (Bahdanau et al., 2014) implemented in the Nematus toolkit (Sennrich et al., 2017). We used bytepair encoding (Sennrich et al., 2016) for vocabulary reduction, mini-batches of 100, word embeddings of 500 dimensions, and gated recurrent unit layers of 1,024 units. Optimization was done using Adam and by re-shuffling the training set at each epoch. For this year’s round, the APE task focused on one lang"
W18-6452,P02-1040,0,0.102334,"Missing"
W18-6452,W17-4775,0,0.0193748,"ficance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary size, the system applies ad hoc pre-processing for the German language by re-implementing the pipeline developed by the best system at the WMT‘17 Translation task (Huck et al., 2017). In addition to the data released for the task, training is performed by taking advantage of both the artificial data provided by (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus ("
W18-6452,W18-6469,1,0.876946,"Missing"
W18-6452,P16-1162,0,0.0670476,"target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data 3 We used a phrase-based MT system trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 4 The NMT system was trained with generic and indomain parallel training data using the attentional encoderdecoder architecture (Bahdanau et al., 2014) implemented in the Nematus toolkit (Sennrich et al., 2017). We used bytepair encoding (Sennrich et al., 2016) for vocabulary reduction, mini-batches of 100, word embeddings of 500 dimensions, and gated recurrent unit layers of 1,024 units. Optimization was done using Adam and by re-shuffling the training set at each epoch. For this year’s round, the APE task focused on one language pair, English-German, and on data coming from the Information Technology (IT) domain. As emerged from the previous evaluations, the selected target domain is specific and repetitive enough to allow supervised systems to 2 In addition to these small in-domain training sets, which were released by the organizers over the yea"
W18-6452,E17-3017,0,0.0615879,"Missing"
W18-6452,P16-1159,0,0.0236516,"lated with the bootstrap test (Koehn, 2004). 3 Fondazione Bruno Kessler. FBK’s system improves the multi-source neural approach adopted in (Chatterjee et al., 2017). The improvements target lower complexity of the architecture and, in turn, higher efficiency without loss in performance. To this aim, the proposed solution relies on the Transformer architecture (Vaswani et al., 2017), which was modified to incorporate multiple encoders, thereby leveraging information also from the source sentences. In addition, similar to (Hokamp, 2017), the system exploits minimumrisk training for fine-tuning (Shen et al., 2016) to avoid exposure bias and to be consistent with the automatic evaluation metrics used for the task. Finally, in order to reduce the vocabulary size, the system applies ad hoc pre-processing for the German language by re-implementing the pipeline developed by the best system at the WMT‘17 Translation task (Huck et al., 2017). In addition to the data released for the task, training is performed by taking advantage of both the artificial data provided by (Junczys-Dowmunt and Grundkiewicz, 2016) and the eSCAPE corpus (Negri et al., 2018). The submitted runs, which rely on the same multi-source a"
W18-6452,W18-6470,0,0.167966,"Missing"
W18-6452,N07-1064,0,0.0365222,"2017) implemented in Microsoft & University of Edinburgh. MS UEdin’s neural APE system is based on the dual-source Transformer models available in Marian (Junczys-Dowmunt et al., 2018). The models are trained with tied embeddings across all embeddings matrices and shared parameters for all the encoders. The dual-source Transformer model is implemented by stacking an additional target-source multi-head component on the previ7 In addition to the do-nothing baseline, in previous rounds we also compared systems’ performance with a reimplementation of the phrase-based approach firstly proposed by Simard et al. (2007), which represented the common backbone of APE systems before the spread of neural solutions. As shown in (Bojar et al., 2016; Bojar et al., 2017), the steady progress of neural APE technology has made the phrase-based solution not competitive with current methods reducing the importance of having it as an additional term of comparison. 714 ous multi-head component, one for each encoder. Each multi-head attention block is followed by a skip connection from the previous input and layer normalization. Each encoder corresponds exactly to the implementation from (Vaswani et al., 2017), but with co"
W18-6452,2006.amta-papers.25,0,0.232446,"Missing"
W18-6452,W18-6471,1,0.873456,"Missing"
W18-6471,W17-4717,1,0.895067,"Missing"
W18-6471,W16-2378,0,0.175913,"Missing"
W18-6471,W17-4774,0,0.0121867,"ed Losses for Automatic Post-Editing Amirhossein Tebbifakhr1,2 , Ruchit Agrawal1,2 , Matteo Negri1 , Marco Turchi1 1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento - Italy 2 University of Trento, Italy {atebbifakhr,ragrawal,negri,turchi}@fbk.eu Abstract In the last few years, the APE shared tasks at WMT (Bojar et al., 2015, 2016, 2017) have renewed the interests in this topic and boosted the technology around it. Moving from the phrase-based approaches used in the first editions of the task (Chatterjee et al., 2015), last year the multi-source neural models (Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Hokamp, 2017) have shown their capability to significantly improve the output of a PBSMT system. These APE systems shared several features and implementation choices, namely: 1) an RNN-based architecture, 2) the use of large artificial corpora for training, 3) model ensembling techniques, 4) parameter optimization based on Maximum Likelihood Estimation (MLE) and 5) vocabulary reduction using the Byte Pair Encoding (BPE) technique. Although they achieve good performance and impressive translation quality improvements, some of these techniques are not optimal for the actual deployment of APE t"
W18-6471,P17-4012,0,0.0514415,"nce to the sampled set of hypotheses, we found that adding the reference is harmful. Actually, by adding the reference to the sample, the other hypotheses are considered as poor alternatives, since they have a lower BLEU score. Nevertheless, these samples usually have good quality and a considerable overlap with the reference. Therefore, updating the model in the direction of decreasing the probability of these hypotheses is does not seem a promising direction. Hyperparameters We set the number of merging rules to 32K for applying BPE in the pre-processing steps. We employ OpenNMT-tf toolkit (Klein et al., 2017) for our implementation, by using 512 dimensions for word embeddings, 4 layers for both the encoders and the decoder with 512 units, and feedfroward dimension of 1,024. In order to avoid over-fitting, we use attention and residual dropout by setting the dropout probability to 0.1, along with the label-smoothing with parameter equal to 0.1. For training using MLE, we use the Adam optimizer (Kingma and Ba, 2014) with batch size of 8,192 tokens, learning rate of 2.0 and the warmup strategy introduced by (Vaswani et al., 2017) with the warm-up steps equals to 8,000. For training using MRT, we use"
W18-6471,E03-1076,0,0.0993863,"2018), which contains 7.2M English-German triplets for each MT paradigm (i.e. 7.2M phrase-based and neural translations of the same source sentences). It has been generated from a parallel English-German corpus, by taking the target sentences as artificial post-edits and the machine-translated source sentences as MT elements of each triplet. 1. Suffixes are separated from the word stems by using a modified version of snowball stemming, which separates and keeps the suffixes instead of stripping them; 2. The output of the previous step is passed to the empirical compound splitter described in (Koehn and Knight, 2003), which is run with the same parameters reported in (Huck et al., 2017); • The artificial corpus provided by JunczysDowmunt and Grundkiewicz (2016), which contains 4.0M English-German triplets generated by applying a round-trip translation protocol to German monolingual data. 3. The output of the previous step is segmented with Byte Pair Encoding (BPE) (Sennrich et al., 2016). Before applying the pre-processing described in Section 4 to the eSCAPE data, we performed the following two cleaning steps: 1 Although TER (Snover et al., 2006) is the primary evaluation metric for the task, we opted fo"
W18-6471,W17-4773,1,0.783277,"e Transformer with Combined Losses for Automatic Post-Editing Amirhossein Tebbifakhr1,2 , Ruchit Agrawal1,2 , Matteo Negri1 , Marco Turchi1 1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento - Italy 2 University of Trento, Italy {atebbifakhr,ragrawal,negri,turchi}@fbk.eu Abstract In the last few years, the APE shared tasks at WMT (Bojar et al., 2015, 2016, 2017) have renewed the interests in this topic and boosted the technology around it. Moving from the phrase-based approaches used in the first editions of the task (Chatterjee et al., 2015), last year the multi-source neural models (Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Hokamp, 2017) have shown their capability to significantly improve the output of a PBSMT system. These APE systems shared several features and implementation choices, namely: 1) an RNN-based architecture, 2) the use of large artificial corpora for training, 3) model ensembling techniques, 4) parameter optimization based on Maximum Likelihood Estimation (MLE) and 5) vocabulary reduction using the Byte Pair Encoding (BPE) technique. Although they achieve good performance and impressive translation quality improvements, some of these techniques are not op"
W18-6471,L18-1004,1,0.70557,"st system at WMT‘17 Translation task (Huck et 2017). the the the al., where dk is added as a scaling factor for improving the numerical stability, which is equal to the dimensionality of the key matrix. The multi-head attention receives h different representations of (Q, K, V ), which makes it possible to learn different relationships between information coming from different positions simultaneously. It is computed as follows: • In addition to the artificial data released by (Junczys-Dowmunt and Grundkiewicz, 2016), we make extensive use of a synthetic corpus of 7.2M English-German triplets (Negri et al., 2018), which was provided by the organizers as additional training material. MH(Q, K, V ) = Concat(head1 , ..., headh )W O where h is number of heads and W O is a parameter matrix with hdv *dmodel dimension. In Transformer, the multi-head attention is used in two different ways: encoder-decoder and self-attention. In the self-attention, in both the encoder and the decoder, the Q, K, and V matrices are coming from the previous layer, while in the encoderdecoder attention, Q matrix comes from the previous layer, and the K and V matrices come from the encoder. In order to encode the source sentence in"
W18-6471,W15-3025,1,0.901644,"Missing"
W18-6471,P02-1040,0,0.101623,", which is the largest one, comprises 28K triplets. The NMT training set, is smaller in size and contains 13K instances. From the two training corpora, we extracted 1K triplets to be used as development set to compare the performance of different models during training. P (y|x) ∆(y) 0 y0 ∈S(x) P (y |x) where S(x) is a set of sampled hypotheses from the model for the input sentence x, P (y|x) is the probability of the sampled hypothesis, and ∆(y) is a cost value for generating the sample y, e.g. ∆(y) = −BLEU(y). Following Sennrich et al. (2016), we employ negative smoothed sentence-level BLEU (Papineni et al., 2002; Chen and Cherry, 2014) for computing the cost function.1 4 Experimental Setting Synthetic data. Since building neural APE models heavily relies on the availability of large training data, we took advantage of the following two corpora: Data Pre-processing In order to reduce the vocabulary size, on the German MT output and post-edits we apply our reimplementation of the word segmentation method introduced by Huck et al. (2017). It consists of three different steps: • the eSCAPE corpus (Negri et al., 2018), which contains 7.2M English-German triplets for each MT paradigm (i.e. 7.2M phrase-base"
W18-6471,W14-3346,0,0.0329775,"one, comprises 28K triplets. The NMT training set, is smaller in size and contains 13K instances. From the two training corpora, we extracted 1K triplets to be used as development set to compare the performance of different models during training. P (y|x) ∆(y) 0 y0 ∈S(x) P (y |x) where S(x) is a set of sampled hypotheses from the model for the input sentence x, P (y|x) is the probability of the sampled hypothesis, and ∆(y) is a cost value for generating the sample y, e.g. ∆(y) = −BLEU(y). Following Sennrich et al. (2016), we employ negative smoothed sentence-level BLEU (Papineni et al., 2002; Chen and Cherry, 2014) for computing the cost function.1 4 Experimental Setting Synthetic data. Since building neural APE models heavily relies on the availability of large training data, we took advantage of the following two corpora: Data Pre-processing In order to reduce the vocabulary size, on the German MT output and post-edits we apply our reimplementation of the word segmentation method introduced by Huck et al. (2017). It consists of three different steps: • the eSCAPE corpus (Negri et al., 2018), which contains 7.2M English-German triplets for each MT paradigm (i.e. 7.2M phrase-based and neural translation"
W18-6471,W17-4775,0,0.0471016,"rhossein Tebbifakhr1,2 , Ruchit Agrawal1,2 , Matteo Negri1 , Marco Turchi1 1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento - Italy 2 University of Trento, Italy {atebbifakhr,ragrawal,negri,turchi}@fbk.eu Abstract In the last few years, the APE shared tasks at WMT (Bojar et al., 2015, 2016, 2017) have renewed the interests in this topic and boosted the technology around it. Moving from the phrase-based approaches used in the first editions of the task (Chatterjee et al., 2015), last year the multi-source neural models (Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Hokamp, 2017) have shown their capability to significantly improve the output of a PBSMT system. These APE systems shared several features and implementation choices, namely: 1) an RNN-based architecture, 2) the use of large artificial corpora for training, 3) model ensembling techniques, 4) parameter optimization based on Maximum Likelihood Estimation (MLE) and 5) vocabulary reduction using the Byte Pair Encoding (BPE) technique. Although they achieve good performance and impressive translation quality improvements, some of these techniques are not optimal for the actual deployment of APE technology in th"
W18-6471,P16-1162,0,0.118529,"n of the target made by professional post-editors. The PBSMT training set, which is the largest one, comprises 28K triplets. The NMT training set, is smaller in size and contains 13K instances. From the two training corpora, we extracted 1K triplets to be used as development set to compare the performance of different models during training. P (y|x) ∆(y) 0 y0 ∈S(x) P (y |x) where S(x) is a set of sampled hypotheses from the model for the input sentence x, P (y|x) is the probability of the sampled hypothesis, and ∆(y) is a cost value for generating the sample y, e.g. ∆(y) = −BLEU(y). Following Sennrich et al. (2016), we employ negative smoothed sentence-level BLEU (Papineni et al., 2002; Chen and Cherry, 2014) for computing the cost function.1 4 Experimental Setting Synthetic data. Since building neural APE models heavily relies on the availability of large training data, we took advantage of the following two corpora: Data Pre-processing In order to reduce the vocabulary size, on the German MT output and post-edits we apply our reimplementation of the word segmentation method introduced by Huck et al. (2017). It consists of three different steps: • the eSCAPE corpus (Negri et al., 2018), which contains"
W18-6471,W17-4730,0,0.0359438,"hesis, and ∆(y) is a cost value for generating the sample y, e.g. ∆(y) = −BLEU(y). Following Sennrich et al. (2016), we employ negative smoothed sentence-level BLEU (Papineni et al., 2002; Chen and Cherry, 2014) for computing the cost function.1 4 Experimental Setting Synthetic data. Since building neural APE models heavily relies on the availability of large training data, we took advantage of the following two corpora: Data Pre-processing In order to reduce the vocabulary size, on the German MT output and post-edits we apply our reimplementation of the word segmentation method introduced by Huck et al. (2017). It consists of three different steps: • the eSCAPE corpus (Negri et al., 2018), which contains 7.2M English-German triplets for each MT paradigm (i.e. 7.2M phrase-based and neural translations of the same source sentences). It has been generated from a parallel English-German corpus, by taking the target sentences as artificial post-edits and the machine-translated source sentences as MT elements of each triplet. 1. Suffixes are separated from the word stems by using a modified version of snowball stemming, which separates and keeps the suffixes instead of stripping them; 2. The output of th"
W18-6471,2006.amta-papers.25,0,0.733438,"passed to the empirical compound splitter described in (Koehn and Knight, 2003), which is run with the same parameters reported in (Huck et al., 2017); • The artificial corpus provided by JunczysDowmunt and Grundkiewicz (2016), which contains 4.0M English-German triplets generated by applying a round-trip translation protocol to German monolingual data. 3. The output of the previous step is segmented with Byte Pair Encoding (BPE) (Sennrich et al., 2016). Before applying the pre-processing described in Section 4 to the eSCAPE data, we performed the following two cleaning steps: 1 Although TER (Snover et al., 2006) is the primary evaluation metric for the task, we opted for BLEU since, according to (Shen et al., 2016), optimizing with this metric gives better results also when evaluation is done with TER. 1. We removed the triplets in which the length ratio between the source sentence and the 848 post-edit is too different from the average in the corpus; model as initial generic model in order to fine-tune it using the in-domain data. 2. We run a language identifier2 in order to remove the triplets having a non-English source sentence or a non-German post-edit. MLE. Using MLE, we fine-tune the generic m"
W18-6471,N16-1004,0,0.0224613,"erial. MH(Q, K, V ) = Concat(head1 , ..., headh )W O where h is number of heads and W O is a parameter matrix with hdv *dmodel dimension. In Transformer, the multi-head attention is used in two different ways: encoder-decoder and self-attention. In the self-attention, in both the encoder and the decoder, the Q, K, and V matrices are coming from the previous layer, while in the encoderdecoder attention, Q matrix comes from the previous layer, and the K and V matrices come from the encoder. In order to encode the source sentence in addition to the MT output, we employ the multi-source method by Zoph and Knight (2016). Our model consists of two encoders, one for the source sentence and one for the MT output. The outputs of these two encoders are concatenated and passed as the key in the attention. This helps to have a better representation, leading to a more effective attention at decoding time. We participated in both the APE‘18 subtasks with positive results. In the PBSMT subtask our top run improves the baseline up to -5.3 TER and +8.23 BLEU points (ranking second out of 11 submissions) while, in the NMT subtask, it achieves a -0.38 TER and +0.8 BLEU improvement (ranking first out of 10 submissions). 2"
W18-6530,W14-3346,0,0.012966,"ty predictor training data described above, analyzing the different dimensions of a title that can lead to poor product titles (laid out in Section 4 and finegrained in the quality prediction data description, above). 5.2 5.3 Parameter Settings The most important parameter of the title generation system is the beam size, which was set to 3 (the one giving the best performance in terms of BLEU score during the development of the model). We used sentence-level BLEU to compute the labels for training the regression model, set to 4-grams over cased titles and the smoothing mechanism described in (Chen and Cherry, 2014). The hyper-parameters of the LSTM-based and RF-based title quality prediction models were respectively optimized with 300 and 600 iterations of random search with an inner 3-fold crossvalidation over the training data. With RFs, we were able to explore the hyper-parameter search space more than with the neural-network-based models due to its faster training time. 6 Results and Discussion In this Section, we report and discuss the results obtained for each task. We start with the quality prediction problem. It can work as a method for selecting good candidate product titles that complements th"
W18-6530,2016.amta-researchers.10,1,0.856723,"edict the posterior probability p(y|x) (in our case, the posterior probability of a generated title y given the initial list of user-created titles x), which can also be viewed as a sequence quality score. Since quality scores are used to rank the partial sequences, an accurate scoring function would yield the highest quality outputs. Decoding can be seen in Minimum Bayes’ Risk Combination (Gonz´alez-Rubio et al., 2011; Gonz´alez-Rubio and Casacuberta, 2013), abstractive summarization (Rush et al., 2015; Chopra et al., 2016), and Neural Machine Translation (NMT) models (Bahdanau et al., 2014; Chen et al., 2016; Vaswani et al., 2017). State-ofthe-art approaches utilize encoder-decoder models that extract a feature representation of a variablelength input sentence before generating an output. However, the bottleneck of this approach is its dependence on the size of quality data; it often performs poorly when annotated data is noisy and/or insufficient (Koehn, 2017). 3 Title Generation This system’s purpose is to provide hypotheses of product titles. It receives as input a list of item titles for listings previously aggregated into one product (like the titles at the bottom of Figure 1) and product-re"
W18-6530,N16-1012,0,0.053356,"be robust to noises present in seller-created titles. generative process that learns to predict the posterior probability p(y|x) (in our case, the posterior probability of a generated title y given the initial list of user-created titles x), which can also be viewed as a sequence quality score. Since quality scores are used to rank the partial sequences, an accurate scoring function would yield the highest quality outputs. Decoding can be seen in Minimum Bayes’ Risk Combination (Gonz´alez-Rubio et al., 2011; Gonz´alez-Rubio and Casacuberta, 2013), abstractive summarization (Rush et al., 2015; Chopra et al., 2016), and Neural Machine Translation (NMT) models (Bahdanau et al., 2014; Chen et al., 2016; Vaswani et al., 2017). State-ofthe-art approaches utilize encoder-decoder models that extract a feature representation of a variablelength input sentence before generating an output. However, the bottleneck of this approach is its dependence on the size of quality data; it often performs poorly when annotated data is noisy and/or insufficient (Koehn, 2017). 3 Title Generation This system’s purpose is to provide hypotheses of product titles. It receives as input a list of item titles for listings previously"
W18-6530,D15-1122,0,0.0125857,". In this approach, a confusion network is generated by first selecting a listing title as backbone, and then by aligning it to all the other listings. The network is then traversed to obtain the product title with the highest consensus among the input hypotheses. This title can be decoded with decoding units that include either phrase-level (Feng et al., 2009; Du and Way, 2010) or word-level (Barrault, 2010; Rosti et al., 2007; Fiscus, 1997). Systems can choose between 1to-1 mappings (Barrault, 2010; Rosti et al., 2007; Du and Way, 2010) or many-to-many mappings (lattice) (Feng et al., 2009; Ma and McKeown, 2015; Matusov et al., 2006) in hypothesis alignment. The main drawbacks of these solutions are: (1) final output quality is highly dependent on the quality of the selected backbone (aligning hypotheses to a poor-quality listing can result in outputs that are far from being usable in real industrial settings), and (2) lattice creation becomes computationally expensive as the number of initial hypotheses grows, potentially O(n2 ). This makes approaches based on CN unsuitable for our working scenario where there is the need of generating titles for millions of products where each product consists of"
W18-6530,N12-1059,0,0.0265296,"marization, such as Maximal Marginal Relevance (MMR), to prune and select from the set of titles (Carbonell and Goldstein, 1998; Gillick, 2011). Alternatively, systems can also learn how to pick the candidate title that is closest to the reference title. This approach can rely on ranking scores produced by models trained on listing titles and the corresponding human-curated reference product titles, with automatic metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) as labels. Some of these techniques are employed in system combination (Rosti et al., 2007; Barrault, 2010; Devlin and Matsoukas, 2012; Suzuki, 2011). However, this approach limits the number of possible generated titles and can potentially introduce seller-biases when a single seller’s title is selected as the product title. Re-decoding Approaches. Re-decoding is a • An approach that generates product titles taking seller-provided listing titles as input and that scales to millions of products. The method is based on a stack decoder search algorithm that recombines frequent n-grams observed in the listing titles to form a product title hypothesis. • An approach to estimating the quality of titles based on supervised machine"
W18-6530,W17-3525,1,0.786816,"to assess whether the generated titles are good enough for publishing, in order to avoid bad user experience. For example, in Figure 1, if a generated title has a brand other than “ACME”, it would not be an appropriate title for that specific product. In this paper we present approaches to both problems: (i) generating e-commerce product titles and (ii) predicting their quality. Our main contributions are: 2 Related Work Prior work on title generation for e-commerce focused on browse pages and has only explored a hybrid approach combining rule-based and statistical machine translation models (Mathur et al., 2017). The input to this approach consists of structured information about products in terms of slot/value pairs (e.g. Watch Type: wrist watch). Although the task is similar to ours, hand-crafting and encoding product-specific rules is a time-consuming endeavour which does not scale to the hundreds of slot-value pairs and millions of products in the catalog. Below, we discuss three approaches that can either be directly applied or adapted to product title generation. The first approach is selection-based, while the last two are generation-based. Hypothesis Selection. The most intuitive approach is"
W18-6530,2010.amta-papers.9,0,0.0163469,"on computed in the first step. Hypothesis Fusion. An alternative approach is neither to select nor to generate, but to ‘fuse’ already generated hypotheses. This, for instance, can be done using Confusion Network (CN) decoding (Ma, 2014). In this approach, a confusion network is generated by first selecting a listing title as backbone, and then by aligning it to all the other listings. The network is then traversed to obtain the product title with the highest consensus among the input hypotheses. This title can be decoded with decoding units that include either phrase-level (Feng et al., 2009; Du and Way, 2010) or word-level (Barrault, 2010; Rosti et al., 2007; Fiscus, 1997). Systems can choose between 1to-1 mappings (Barrault, 2010; Rosti et al., 2007; Du and Way, 2010) or many-to-many mappings (lattice) (Feng et al., 2009; Ma and McKeown, 2015; Matusov et al., 2006) in hypothesis alignment. The main drawbacks of these solutions are: (1) final output quality is highly dependent on the quality of the selected backbone (aligning hypotheses to a poor-quality listing can result in outputs that are far from being usable in real industrial settings), and (2) lattice creation becomes computationally expen"
W18-6530,E06-1005,1,0.601487,"confusion network is generated by first selecting a listing title as backbone, and then by aligning it to all the other listings. The network is then traversed to obtain the product title with the highest consensus among the input hypotheses. This title can be decoded with decoding units that include either phrase-level (Feng et al., 2009; Du and Way, 2010) or word-level (Barrault, 2010; Rosti et al., 2007; Fiscus, 1997). Systems can choose between 1to-1 mappings (Barrault, 2010; Rosti et al., 2007; Du and Way, 2010) or many-to-many mappings (lattice) (Feng et al., 2009; Ma and McKeown, 2015; Matusov et al., 2006) in hypothesis alignment. The main drawbacks of these solutions are: (1) final output quality is highly dependent on the quality of the selected backbone (aligning hypotheses to a poor-quality listing can result in outputs that are far from being usable in real industrial settings), and (2) lattice creation becomes computationally expensive as the number of initial hypotheses grows, potentially O(n2 ). This makes approaches based on CN unsuitable for our working scenario where there is the need of generating titles for millions of products where each product consists of a potentially large num"
W18-6530,D09-1115,0,0.0584192,"Missing"
W18-6530,P02-1040,0,0.104004,"sting titles, the one that most “appropriately” describes the product. This can be achieved by applying diversitybased ranking techniques used in extractive summarization, such as Maximal Marginal Relevance (MMR), to prune and select from the set of titles (Carbonell and Goldstein, 1998; Gillick, 2011). Alternatively, systems can also learn how to pick the candidate title that is closest to the reference title. This approach can rely on ranking scores produced by models trained on listing titles and the corresponding human-curated reference product titles, with automatic metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) as labels. Some of these techniques are employed in system combination (Rosti et al., 2007; Barrault, 2010; Devlin and Matsoukas, 2012; Suzuki, 2011). However, this approach limits the number of possible generated titles and can potentially introduce seller-biases when a single seller’s title is selected as the product title. Re-decoding Approaches. Re-decoding is a • An approach that generates product titles taking seller-provided listing titles as input and that scales to millions of products. The method is based on a stack decoder search algorithm that recombin"
W18-6530,J18-3002,0,0.0250927,"wanted tokens (Bluetooth Lines on Display) Table 4: Output examples generated by the systems evaluated in the human evaluation. Third column lists the issues in each output. proach in order to improve the overall generation and quality prediction accuracy. tems outputs. In the first block of outputs, for example, some titles do not present the specification of the type of the product (what is the product). The observation that BLEU alone is not appropriate for evaluating natural language generation systems is not new and corroborates previous work on the field, most notably the recent work by Reiter (2018). Another important trend observed in Table 3 is that using the quality prediction system as a rescorer of the generated and seller titles does not improve over generation alone. We hypothesize this is due to the fact that both systems are trained separately and therefore do not leverage from the signals and features both systems explore. As future work we would like to experiment with joint training of the generation and quality prediction systems, in order to cope with this gap. 7 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learn"
W18-6530,P07-1040,0,0.169758,"ng techniques used in extractive summarization, such as Maximal Marginal Relevance (MMR), to prune and select from the set of titles (Carbonell and Goldstein, 1998; Gillick, 2011). Alternatively, systems can also learn how to pick the candidate title that is closest to the reference title. This approach can rely on ranking scores produced by models trained on listing titles and the corresponding human-curated reference product titles, with automatic metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) as labels. Some of these techniques are employed in system combination (Rosti et al., 2007; Barrault, 2010; Devlin and Matsoukas, 2012; Suzuki, 2011). However, this approach limits the number of possible generated titles and can potentially introduce seller-biases when a single seller’s title is selected as the product title. Re-decoding Approaches. Re-decoding is a • An approach that generates product titles taking seller-provided listing titles as input and that scales to millions of products. The method is based on a stack decoder search algorithm that recombines frequent n-grams observed in the listing titles to form a product title hypothesis. • An approach to estimating the q"
W18-6530,D15-1044,0,0.0666797,"near time and must be robust to noises present in seller-created titles. generative process that learns to predict the posterior probability p(y|x) (in our case, the posterior probability of a generated title y given the initial list of user-created titles x), which can also be viewed as a sequence quality score. Since quality scores are used to rank the partial sequences, an accurate scoring function would yield the highest quality outputs. Decoding can be seen in Minimum Bayes’ Risk Combination (Gonz´alez-Rubio et al., 2011; Gonz´alez-Rubio and Casacuberta, 2013), abstractive summarization (Rush et al., 2015; Chopra et al., 2016), and Neural Machine Translation (NMT) models (Bahdanau et al., 2014; Chen et al., 2016; Vaswani et al., 2017). State-ofthe-art approaches utilize encoder-decoder models that extract a feature representation of a variablelength input sentence before generating an output. However, the bottleneck of this approach is its dependence on the size of quality data; it often performs poorly when annotated data is noisy and/or insufficient (Koehn, 2017). 3 Title Generation This system’s purpose is to provide hypotheses of product titles. It receives as input a list of item titles f"
W18-6530,2013.iwslt-papers.4,0,0.0466445,"Missing"
W18-6530,P11-1127,0,0.042489,"Missing"
W18-6530,2006.amta-papers.25,0,0.0289261,"“appropriately” describes the product. This can be achieved by applying diversitybased ranking techniques used in extractive summarization, such as Maximal Marginal Relevance (MMR), to prune and select from the set of titles (Carbonell and Goldstein, 1998; Gillick, 2011). Alternatively, systems can also learn how to pick the candidate title that is closest to the reference title. This approach can rely on ranking scores produced by models trained on listing titles and the corresponding human-curated reference product titles, with automatic metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) as labels. Some of these techniques are employed in system combination (Rosti et al., 2007; Barrault, 2010; Devlin and Matsoukas, 2012; Suzuki, 2011). However, this approach limits the number of possible generated titles and can potentially introduce seller-biases when a single seller’s title is selected as the product title. Re-decoding Approaches. Re-decoding is a • An approach that generates product titles taking seller-provided listing titles as input and that scales to millions of products. The method is based on a stack decoder search algorithm that recombines frequent n-grams observed"
W18-6530,2011.mtsummit-papers.16,0,0.0181385,"Marginal Relevance (MMR), to prune and select from the set of titles (Carbonell and Goldstein, 1998; Gillick, 2011). Alternatively, systems can also learn how to pick the candidate title that is closest to the reference title. This approach can rely on ranking scores produced by models trained on listing titles and the corresponding human-curated reference product titles, with automatic metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) as labels. Some of these techniques are employed in system combination (Rosti et al., 2007; Barrault, 2010; Devlin and Matsoukas, 2012; Suzuki, 2011). However, this approach limits the number of possible generated titles and can potentially introduce seller-biases when a single seller’s title is selected as the product title. Re-decoding Approaches. Re-decoding is a • An approach that generates product titles taking seller-provided listing titles as input and that scales to millions of products. The method is based on a stack decoder search algorithm that recombines frequent n-grams observed in the listing titles to form a product title hypothesis. • An approach to estimating the quality of titles based on supervised machine learning metho"
W18-6530,W17-4123,0,0.0310258,"mum Bayes’ Risk Combination (Gonz´alez-Rubio et al., 2011; Gonz´alez-Rubio and Casacuberta, 2013), abstractive summarization (Rush et al., 2015; Chopra et al., 2016), and Neural Machine Translation (NMT) models (Bahdanau et al., 2014; Chen et al., 2016; Vaswani et al., 2017). State-ofthe-art approaches utilize encoder-decoder models that extract a feature representation of a variablelength input sentence before generating an output. However, the bottleneck of this approach is its dependence on the size of quality data; it often performs poorly when annotated data is noisy and/or insufficient (Koehn, 2017). 3 Title Generation This system’s purpose is to provide hypotheses of product titles. It receives as input a list of item titles for listings previously aggregated into one product (like the titles at the bottom of Figure 1) and product-related data in the form of slot-value pairs (as the name-value pairs shown under “Product Details” in Figure 1). In addition to these, a human-curated reference product title is required during training time. The process of generating titles can be roughly summarized into two steps. The first is computing different statistics about the item titles: n-gram cou"
W18-6530,P97-1047,0,0.444443,"gation of titles into products, which means some of the pairs can present noise. In order to filter out noisy slot-value pairs and to understand which pairs are important, we derive an importance score for each pair. This score is computed by dividing the number of times a value appears at least once in the listing titles by the number of listing titles aggregated to the product. The top-k pairs according to this score are kept. The second step consists of performing the recombination of n-grams found in the titles using an heuristic stack-based search algorithm, also known as stack decoding (Wang and Waibel, 1997) using all the information computed in the first step. Hypothesis Fusion. An alternative approach is neither to select nor to generate, but to ‘fuse’ already generated hypotheses. This, for instance, can be done using Confusion Network (CN) decoding (Ma, 2014). In this approach, a confusion network is generated by first selecting a listing title as backbone, and then by aligning it to all the other listings. The network is then traversed to obtain the product title with the highest consensus among the input hypotheses. This title can be decoded with decoding units that include either phrase-le"
W19-2305,W12-2910,0,0.0253778,"nglish Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) has to be covered. Neural text simplification has gained increasing attention in the NLP community thanks to recent advancements in deep sequence-tosequence learning. Most recent efforts with such a data-demanding paradigm have dealt with the English language, for which sizeable training datasets are currently available to deploy competi"
W19-2305,W14-1820,0,0.0657305,"Missing"
W19-2305,F12-2016,0,0.060497,"Missing"
W19-2305,P18-2113,0,0.0437516,"mplex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) has to be covered. Neural text simplification has gained increasing attention in the NLP community thanks to recent advancements in deep sequence-tosequence"
W19-2305,P17-1099,0,0.0370237,"uence of words is fed to the encoder, which maps it into a sequence of continuous representations (the hidden states of the encoder) providing increasing levels of abstraction. At each time step, based on these continuous representations and the generated word in the previous time step, the decoder generates the next word. This process continues until the decoder generates the end-of-sentence symbol. This sequence-to-sequence model is extended by adding a pointer-generator network that allows both copying words via pointing to the source sentence, and generating words from a fixed vocabulary (See et al., 2017). At each time step, the network estimates the probability of generating a word and uses this probability as a gate to decide whether to generate or copy the word. To apply this pointer-generator network, a shared vocabulary containing all the words in the complex and simple training sentences is used. This architecture is implemented in the OpenNMT platform (Klein et al., 2017). • We explore different approaches for augmenting training data for neural text simplification using weak supervision; • We test them in under-resourced conditions on Italian and Spanish. 2 Neural sentence simplificati"
W19-2305,P17-4012,0,0.0413134,"end-of-sentence symbol. This sequence-to-sequence model is extended by adding a pointer-generator network that allows both copying words via pointing to the source sentence, and generating words from a fixed vocabulary (See et al., 2017). At each time step, the network estimates the probability of generating a word and uses this probability as a gate to decide whether to generate or copy the word. To apply this pointer-generator network, a shared vocabulary containing all the words in the complex and simple training sentences is used. This architecture is implemented in the OpenNMT platform (Klein et al., 2017). • We explore different approaches for augmenting training data for neural text simplification using weak supervision; • We test them in under-resourced conditions on Italian and Spanish. 2 Neural sentence simplification system Related work The lack of data for training sequence-to-sequence models is a problem that has been addressed in several NLP tasks. In MT, for instance, synthetic parallel data for low-resource settings have been generated by automatically translating sentences from the target language into the source language (Sennrich et al., 2016b,a). In speech translation, recent wor"
W19-2305,P16-1009,0,0.507026,"lopment of neural solutions also for languages other than English, we explore data augmentation techniques for creating task-specific training data. Our experiments range from simple oversampling techniques to weakly supervised data augmentation methods inspired by recent 37 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 37–44 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics works in other NLP tasks (B´erard et al., 2016; Ding and Balog, 2018), in particular Machine Translation (MT) (Sennrich et al., 2016b). In a nutshell, taking an opposite direction to simplification, we proceed by i) automatically selecting simple sentences from a large pool of monolingual data, and ii) synthetically creating complex sentences. These artificially created sentences will be then used as the “source” side of new difficult– simple training pairs fed into an MT-like encoderdecoder architecture. Our hypothesis is that, though sub-optimal due to possible errors introduced in the automatic generation of complex sentences, these training pairs represent useful material for building our sequence-to-sequence text simp"
W19-2305,W14-0406,0,0.0865816,"Missing"
W19-2305,P18-1016,0,0.0396309,"or this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) has to be covered. Neural text simplification has gained increasing attention in"
W19-2305,P17-2014,0,0.10099,"e by automatic processing, large “silver” data provide a valuable additional complement to small “gold” training corpora. Regarding neural text simplification, we are not aware of previous work on extending small training corpora with synthetic data. Indeed, the lack of training instances has been a major issue in the development of such applications for languages other than English. 3 Our sentence simplification approach is based on the attentional encoder-decoder model (Bahdanau et al., 2014) initially proposed for MT. It takes as input a complex sentence and outputs its simplified version (Nisioi et al., 2017). Cast as a (monolingual) translation task, it provides a comprehensive solution to address both lexical and structural simplification, since the model does not only learn single term replacements, but also more complex structural changes. Initially, a sequence of words is fed to the encoder, which maps it into a sequence of continuous representations (the hidden states of the encoder) providing increasing levels of abstraction. At each time step, based on these continuous representations and the generated word in the previous time step, the decoder generates the next word. This process contin"
W19-2305,W08-2140,0,0.0441388,"sampling and the use of external word embeddings to be fed to the neural simplification system. Our approach is evaluated on Italian and Spanish, for which few thousand gold sentence pairs are available. The results show that these techniques yield performance improvements over a baseline sequence-to-sequence configuration. 1 Marco Turchi Fondazione Bruno Kessler Trento, Italy turchi@fbk.eu Introduction Text simplification aims at making a text more readable by reducing its lexical and structural complexity while preserving the meaning. (Chandrasekar and Bangalore, 1997; Carroll et al., 1998; Vickrey and Koller, 2008; Crossley et al., 2012; Shardlow, 2014). Neural approaches to the task have gained increasing attention in the NLP community thanks to recent advancements of deep, To alleviate the data bottleneck issue, enabling the development of neural solutions also for languages other than English, we explore data augmentation techniques for creating task-specific training data. Our experiments range from simple oversampling techniques to weakly supervised data augmentation methods inspired by recent 37 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (Neura"
W19-2305,L18-1615,0,0.0534213,"e Spanish Billion Word Corpus, that is widely used in NLP experiments on Spanish (Zea et al., 2016; Quir´os et al., 2016).9 To favour the extraction of word embeddings from simple texts, we increase the Spanish Billion Word Corpus by adding our extracted simple Spanish texts. In total, Spanish word embeddings are extracted from a corpus of nearly 1.5B words. Spanish The Spanish gold standard is obtained from the Spanish Newsela corpus,5 containing 1, 221 documents manually annotated by professionals for different proficiency levels. We align complex– ˇ simple pairs using the CATS-Align6 tool (Stajner et al., 2018) and discard the pairs coupled with an alignment accuracy below 0.5. The gold standard contains 55, 890 complex-to-simple pairs. The set of simple sentences used to create the synthetic pairs is extracted from a large monolin5.3 System configuration OpenNMT is run on a Nvidia Tesla K80 GPU using stochastic gradient descent (Robbins and Monro, 1951) optimization with learning rate 1. Each run is repeated three times with different seeds, then the average value is considered. Since the source and target languages are the same, in 2 www.opensubtitles.org www.gazzettaufficiale.it 4 simple.wikipedi"
W19-2305,N18-2013,0,0.0248981,"instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) h"
W19-2305,D11-1038,0,0.0689553,"ubtitles,2 the Pais`a corpus (Lyding et al., 2014), Wikipedia and the collection of Italian laws.3 This merging process results in around 1.3B words and 125M sentences. We rank all sentences by readability level according to the best features described in (Dell’Orletta et al., 2014) and keep the 500, 000 most readable (i.e. simplest) sentences to create the synthetic pairs. This process is needed due to the lack of an Italian equivalent of the Simple English Wikipedia,4 which is widely used as a source of simple monolingual data when dealing with English text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011). From the large corpus described above, before filtering only simple sentences, we also create word embeddings with 300 dimensions using word2vec (Mikolov et al., 2013). 5.2 gual corpus covering different domains, obtained from websites written in simple Spanish for language learners.7 The documents are then ranked based on the Flesch-Szigriszt readability score for Spanish (Szigriszt, 1993)8 and all sentences belonging to the most readable ones are included in the set of simple monolingual data (484, 325 simple sentences in total, from a set of about 1.2M sentences). For Spanish, we do not r"
W19-2305,Q15-1021,0,0.106446,"ne Bruno Kessler Trento, Italy negri@fbk.eu Mattia Di Gangi Fondazione Bruno Kessler Trento, Italy digangi@fbk.eu Abstract sequence-to-sequence approaches. However, all recent improvements have dealt with English. The main reason is that such data-hungry approaches require large training sets (in the order of hundred thousand instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solu"
W19-2305,W16-2705,0,0.0352302,"Missing"
W19-2305,D17-1062,0,0.0238801,"uire large training sets (in the order of hundred thousand instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new langu"
W19-2305,C10-1152,0,0.323913,"r Trento, Italy digangi@fbk.eu Abstract sequence-to-sequence approaches. However, all recent improvements have dealt with English. The main reason is that such data-hungry approaches require large training sets (in the order of hundred thousand instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and To"
W19-5402,W16-2301,1,0.88964,"Missing"
W19-5402,2004.iwslt-evaluation.1,0,0.144319,"Missing"
W19-5402,P15-2026,1,0.860832,"correction of phrase-based MT output, this year only neural MT (NMT) output has been considered. However, this year’s campaign allows both for a fair assessment of the progress in APE technology and for tests in more challenging conditions. On one side, reusing the same test English-German set used last year, the evaluation framework allows us for a direct comparison with the last year’s outcomes at least on one language. On the other side, dealing with a Introduction MT Automatic Post-Editing (APE) is the task of automatically correcting errors in a machinetranslated text. As pointed out by (Chatterjee et al., 2015), from the application point of view the 11 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 11–28 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to beat the baseline (16.16 TER, 76.20 BLEU). These results confirm one of the main findings of previous rounds (Bojar et al., 2017; Chatterjee et al., 2018a): improving high-quality MT output remains the biggest challenge for APE. This motivates further research on precise and conservative solutions able to mimic human behaviour by performing only the m"
W19-5402,W18-1804,1,0.783788,"t least on one language. On the other side, dealing with a Introduction MT Automatic Post-Editing (APE) is the task of automatically correcting errors in a machinetranslated text. As pointed out by (Chatterjee et al., 2015), from the application point of view the 11 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 11–28 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to beat the baseline (16.16 TER, 76.20 BLEU). These results confirm one of the main findings of previous rounds (Bojar et al., 2017; Chatterjee et al., 2018a): improving high-quality MT output remains the biggest challenge for APE. This motivates further research on precise and conservative solutions able to mimic human behaviour by performing only the minimum amount of edit operations needed. difficult language like Russian and only with highquality NMT output, also this round presented participants with an increased level of difficulty with respect to the past. Seven teams participated in the EnglishGerman task, submitting 18 runs in total. Two teams participated in the English-Russian task, submitting 2 runs each. Similar to last year, all the"
W19-5402,P07-2045,0,0.00969238,"Missing"
W19-5402,P19-1292,0,0.0959054,"llowed by ii) a Transformer decoder block, but without masking, for self-attention on the MT segment, which effectively acts as second encoder combining source and MT output, and iii) feeds this representation into a final decoder block generating the post-edit. The intuition behind the proposed architecture is to generate better representations via both self- and cross- attention and to further facilitate the learning capacity of the feedforward layer in the decoder block. Also in this case, model training takes advantage of the eSCAPE synthetic data (Negri et al., 2018). Unbabel. Following (Correia and Martins, 2019), Unbabel’s submission (English-German subtask) adapts BERT (Devlin et al., 2018) to the APE task with an encoder-decoder framework. The system consists in a BERT encoder initialised with the pretrained model’s weights and a BERT decoder initialised analogously, where the multi-head context attention is initialised with University of Sheffield & Imperial College London. IC USFD’s submission (English-German subtask) is based on the dual-source Transformer model (Junczys-Dowmunt and Grundkiewicz, 2018), which was re-implemented in the 17 Tensor2Tensor (Vaswani et al., 2017) toolkit. The model wa"
W19-5402,W04-3250,0,0.015291,"s are discussed in Section 6. 2.2 2.3 Baseline In continuity with the previous rounds, the official baseline results were the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “do-nothing” system that leaves all the test targets unmodified. Baseline results, the same shown in Table 2, are also reported in Tables 4 and 5 for comparison with participants’ submissions.8 For each submitted run, the statistical significance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). Evaluation metrics System performance was evaluated both by means of automatic metrics and manually. Automatic metrics were used to compute the distance between automatic and human post-edits of the machine-translated sentences present in the test sets. To this aim, TER and BLEU (case-sensitive) were respectively used as primary and secondary evaluation metrics. Systems were ranked based on the average TER calculated on the test set by using the TERcom6 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package7 available 3 Participants"
W19-5402,W17-4713,1,0.837945,"“light post-edit” (minimal edits are required), and “heavy post-edit” (a large number of edits are required. At training time, the instances are labelled based on the TER computed between the MT output and its post-edited version, with the boundary between light and heavy post-edit set to TER=0.4 based on the findings reported in (Turchi et al., 2013; Turchi et al., 2014). At test time, tokens are predicted with two approaches. One is based on a classifier obtained by fine-tuning BERT (Devlin et al., 2018) on the in-domain data. The other approach exploits a retrieval-based method similar to (Farajian et al., 2017): given a query containing the source and the MT output to be post-edited, it: i) retrieves similar triplets from the training data, ii) ranks them based on the sentence level BLEU score between the MT output and the post-edit, and iii) creates the token based on the TER computed between the MT output and the post-edit of the most similar triplet. The backbone architecture is the multi-source extension of Transformer (Vaswani et al., 2017) described in (Tebbifakhr et al., 2018), which is trained both on the task data and on the available artificial corpora. ADAPT Centre & Dublin City Universit"
W19-5402,W19-5412,0,0.115706,"Missing"
W19-5402,W13-2305,0,0.108674,"ny. Each evaluator had experience with the evaluation task through previous work using the same evaluation platform in order to be familiar with the user interface and its functionalities. A screenshot of the evaluation interface is presented in Figure 4. We measure post-editing quality using sourcebased direct assessment (src-DA), as implemented in Appraise (Federmann, 2012). Scores are collected as x ∈ [0, 100], focusing on adequacy (and not fluency, which previous WMT evaluation campaigns have found to be highly correlated with adequacy direct assessment results). The original DA approach (Graham et al., 2013; Graham et al., 2014) is reference-based and, thus, needs to be adapted for use in our paraphrase assessment and translation scoring scenarios. Of course, this makes translation evaluation more difficult, as we require bilingual annotators. Src-DA has previously been used, e.g., in (Cettolo et al., 2017; Bojar et al., 2018). Direct assessment initializes mental context for annotators by asking a priming question. The user interface shows two sentences: For both the subtasks, the differences in system’s behaviour are indeed barely visible, mainly due to the fact that, in most of the cases, the"
W19-5402,W19-5413,0,0.167547,"llowed by ii) a Transformer decoder block, but without masking, for self-attention on the MT segment, which effectively acts as second encoder combining source and MT output, and iii) feeds this representation into a final decoder block generating the post-edit. The intuition behind the proposed architecture is to generate better representations via both self- and cross- attention and to further facilitate the learning capacity of the feedforward layer in the decoder block. Also in this case, model training takes advantage of the eSCAPE synthetic data (Negri et al., 2018). Unbabel. Following (Correia and Martins, 2019), Unbabel’s submission (English-German subtask) adapts BERT (Devlin et al., 2018) to the APE task with an encoder-decoder framework. The system consists in a BERT encoder initialised with the pretrained model’s weights and a BERT decoder initialised analogously, where the multi-head context attention is initialised with University of Sheffield & Imperial College London. IC USFD’s submission (English-German subtask) is based on the dual-source Transformer model (Junczys-Dowmunt and Grundkiewicz, 2018), which was re-implemented in the 17 Tensor2Tensor (Vaswani et al., 2017) toolkit. The model wa"
W19-5402,E14-1047,0,0.0536777,"Missing"
W19-5402,L18-1004,1,0.577374,"a phrase-based APE system. In most of the cases, participants experimented with the Transformer architecture (Vaswani et al., 2017), either directly or by adapting it to the task (see Section 3). Another common trait of the submitted systems is the reliance on the consolidated multi-source approach (Zoph and Knight, 2016; Libovick´y et al., 2016), which is able to exploit information from both the MT output to be corrected and the corresponding source sentence. The third aspect common to all submissions is the exploitation of synthetic data, either those provided together with the task data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016) or similar, domain-specific resources created ad-hoc by participants. 2 Task description In continuity with all the previous rounds of the APE task, participants were provided with training and development data consisting of (source, target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data This year, the evaluation was performed on two language pairs, English-German and EnglishRussian. For both the subtasks, data were selected from the Information Technology (IT) domain."
W19-5402,W19-5414,0,0.144563,"Missing"
W19-5402,W16-2378,0,0.494675,"system. In most of the cases, participants experimented with the Transformer architecture (Vaswani et al., 2017), either directly or by adapting it to the task (see Section 3). Another common trait of the submitted systems is the reliance on the consolidated multi-source approach (Zoph and Knight, 2016; Libovick´y et al., 2016), which is able to exploit information from both the MT output to be corrected and the corresponding source sentence. The third aspect common to all submissions is the exploitation of synthetic data, either those provided together with the task data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016) or similar, domain-specific resources created ad-hoc by participants. 2 Task description In continuity with all the previous rounds of the APE task, participants were provided with training and development data consisting of (source, target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data This year, the evaluation was performed on two language pairs, English-German and EnglishRussian. For both the subtasks, data were selected from the Information Technology (IT) domain. As emerged from the previous evaluations,"
W19-5402,P02-1040,0,0.109642,"hort segments (menu commands, short messages, etc.), is shared with the English-Russian Quality Estimation shared task.3 The training and development set respectively contain 15,089 and 1,000 triplets, while the test set comprises 1,023 instances. For this language pair, the eSCAPE corpus has been extended to provide participants with additional Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the five rounds of the APE task, it shows the repetition rate of SRC, TGT and PE elements, as well as the TER (Snover et al., 2006) and the BLEU score (Papineni et al., 2002) of the TGT elements (i.e. the original target translations). The repetition rate measures the repetitiveness inside a text by looking at the rate of non-singleton n-gram types (n=1...4) and combining them using the geometric mean. Larger values indicate a higher text repetitiveness and, as discussed in (Bojar et al., 2016; Bojar et al., 2017; Chatterjee et al., 2018a), suggest a higher chance of learning from the training set correction patterns that are applicable also to the test set. In the previous rounds of the task, we considered the large differences in repetitiveness across the datase"
W19-5402,E17-2025,0,0.0173592,"der that computes the attenFondazione Bruno Kessler. Also FBK participated in both the subtasks. Their submissions focus on mitigating the “over-correction” problem in APE, that is the systems’ tendency to rephrase and correct MT output that is already acceptable, thus producing translations that will be penalized by evaluation against human post-edits. Following (Chatterjee et al., 2018b), the underlying idea is that over-correction can be prevented by inform9 https://marian-nmt.github.io/ 16 the self-attention weights. Additionally, source embeddings, target embeddings and projection layer (Press and Wolf, 2017) are shared, as well as the self-attention weights of the encoder and decoder. The system exploits BERT training schedule with streams A and B: the encoder receives as input both the source and the MT output separated by the special symbol “[SEP]”, assigning to the first “A” segment embeddings and to the latter “B” segment embeddings. Regarding the BERT decoder, they use just the post-edit with “B” segment embeddings. In addition, as the NMT system has a strong in-domain performance, a conservativeness factor to avoid over-correction is explored. Similarly to (Junczys-Dowmunt and Grundkiewicz,"
W19-5402,P17-4012,0,0.032833,"ens and the topic induced via LDA clustering (Blei et al., 2003). The statistical APE models, which are based on Moses (Koehn et al., 2007), were trained to explore the idea of interleaving different MT technologies to improve NMT output quality. All the models are built by taking advantage of both the released training material and the provided artificial data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). Pohang University of Science and Technology. POSTECH’s system (English-German subtask) is a multi-source model that extends the Transformer implementation of the OpenNMT-py (Klein et al., 2017) library. It includes: i) a joint encoder that is able to generate joint representations reflecting the relationship between two input sources (SRC, TGT) with optional future masking to mimic the general decoding process of machine translation systems, and ii) two types of multi-source attention layers in the decoder that computes the attenFondazione Bruno Kessler. Also FBK participated in both the subtasks. Their submissions focus on mitigating the “over-correction” problem in APE, that is the systems’ tendency to rephrase and correct MT output that is already acceptable, thus producing trans"
W19-5402,P16-1162,0,0.0407548,"e; • The target (TGT) is a tokenized German/Russian translation of the source, which was produced by a black-box system unknown to participants. For both the languages, translations were obtained from neural MT systems:1 this implies that their overall quality is generally high, making the task harder compared to previous rounds, which 1 For English-German, the NMT system was trained with generic and in-domain parallel training data using the attentional encoder-decoder architecture (Bahdanau et al., 2014) implemented in the Nematus toolkit (Sennrich et al., 2017). We used byte-pair encoding (Sennrich et al., 2016) for vocabulary reduction, mini-batches of 100, word embeddings of 500 dimensions, and gated recurrent unit layers of 1,024 units. Optimization was done using Adam and by re-shuffling the training set at each epoch. For English-Russian, the NMT system used was the Microsoft Translator production system, which was trained with both generic and in-domain parallel training data. The newly proposed English-Russian task represents a more challenging evaluation scenario, mainly due to the higher quality of the NMT output to be corrected. In this case, even the best submission (16.59 TER, 75.27 TER)"
W19-5402,E17-3017,0,0.0655502,"Missing"
W19-5402,W19-5415,0,0.0287686,"Missing"
W19-5402,W19-5417,0,0.230128,"Missing"
W19-5402,D18-1049,0,0.0590858,"Missing"
W19-5402,N07-1064,0,0.0460932,"metrics. Systems were ranked based on the average TER calculated on the test set by using the TERcom6 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package7 available 3 Participants Seven teams submitted a total of 18 runs for the English-German subtask. Two of them participated also in the English-Russian subtask by subgeneric/multi-bleu.perl 8 In addition to the do-nothing baseline, in the first three rounds of the task we also compared systems’ performance with a re-implementation of the phrase-based approach firstly proposed by Simard et al. (2007), which represented the common backbone of APE systems before the spread of neural solutions. As shown in (Bojar et al., 2016; Bojar et al., 2017), the steady progress of neural APE technology has made the phrase-based solution not competitive with current methods reducing the importance of having it as an additional term of comparison. In 2018, we hence opted for considering only one baseline. 6 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ 7 15 ID ADAPT DCU FBK POSTECH UDS UNBABEL USAAR DFKI IC USFD Participating team ADAPT Centre & Dub"
W19-5402,2006.amta-papers.25,0,0.23352,"This material, which mainly consists of short segments (menu commands, short messages, etc.), is shared with the English-Russian Quality Estimation shared task.3 The training and development set respectively contain 15,089 and 1,000 triplets, while the test set comprises 1,023 instances. For this language pair, the eSCAPE corpus has been extended to provide participants with additional Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the five rounds of the APE task, it shows the repetition rate of SRC, TGT and PE elements, as well as the TER (Snover et al., 2006) and the BLEU score (Papineni et al., 2002) of the TGT elements (i.e. the original target translations). The repetition rate measures the repetitiveness inside a text by looking at the rate of non-singleton n-gram types (n=1...4) and combining them using the geometric mean. Larger values indicate a higher text repetitiveness and, as discussed in (Bojar et al., 2016; Bojar et al., 2017; Chatterjee et al., 2018a), suggest a higher chance of learning from the training set correction patterns that are applicable also to the test set. In the previous rounds of the task, we considered the large diff"
W19-5402,W18-6471,1,0.90356,"ning BERT (Devlin et al., 2018) on the in-domain data. The other approach exploits a retrieval-based method similar to (Farajian et al., 2017): given a query containing the source and the MT output to be post-edited, it: i) retrieves similar triplets from the training data, ii) ranks them based on the sentence level BLEU score between the MT output and the post-edit, and iii) creates the token based on the TER computed between the MT output and the post-edit of the most similar triplet. The backbone architecture is the multi-source extension of Transformer (Vaswani et al., 2017) described in (Tebbifakhr et al., 2018), which is trained both on the task data and on the available artificial corpora. ADAPT Centre & Dublin City University. The ADAPT DCU team participated in both the subtasks proposed this year. Their submissions pursue two main objectives, namely: i) investigating the effect of adding extra information in the form of prefix tokens in a neural APE system; and ii) assessing whether an SMT-based approach can be effective for post-editing NMT output. The neural APE system exploits a multisource approach based on Marian-NMT.9 Training data were augmented with two types of extra context tokens that"
W19-5402,W19-5416,1,0.827672,"Missing"
W19-5402,W13-2231,1,0.834987,"ected amount of corrections needed. The proposed solution is based on prepending a special token to the source text and the MT output, so to indicate the required amount of post-editing. Three different tokens are used, namely “no post-edit” (no edits are required), “light post-edit” (minimal edits are required), and “heavy post-edit” (a large number of edits are required. At training time, the instances are labelled based on the TER computed between the MT output and its post-edited version, with the boundary between light and heavy post-edit set to TER=0.4 based on the findings reported in (Turchi et al., 2013; Turchi et al., 2014). At test time, tokens are predicted with two approaches. One is based on a classifier obtained by fine-tuning BERT (Devlin et al., 2018) on the in-domain data. The other approach exploits a retrieval-based method similar to (Farajian et al., 2017): given a query containing the source and the MT output to be post-edited, it: i) retrieves similar triplets from the training data, ii) ranks them based on the sentence level BLEU score between the MT output and the post-edit, and iii) creates the token based on the TER computed between the MT output and the post-edit of the mo"
W19-5416,Q17-1024,0,0.075499,"Missing"
W19-5416,W16-2378,0,0.0442796,"h the post-edited outputs are real human postedits. To overcome the lack of data and to train neural APE models, the organizers also provided a large amount of synthetic data. For the En-Ru subtask, they provided the eSCAPE dataset (Negri et al., 2018), which is produced from a parallel corpus by considering the target sentences as artificial human post-edits and machine-translated source sentences as MT output. For the En-De subtask, in addition to the eSCAPE dataset, another synthetic dataset was made available, which is created using round-trip translation from a German monolingual corpus (Junczys-Dowmunt and Grundkiewicz, 2016). We clean the English to German/Russian eSCAPE dataset by removing i) samples with a length ratio between source text and post-edited output which is too different than the average and ii) samples where the source text language is not English or post-edited output language is not German/Russian. In order to reduce the vocabulary size, we apply Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE merging rules on the union of the source text, MT output and post-edit output to obtain a shared vocabulary. 4 Results For both subtasks, we train our APE systems with and without prepen"
W19-5416,W18-6467,0,0.223765,"ri1 , Marco Turchi1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento - Italy 2 University of Trento, Italy {atebbifakhr,negri,turchi}@fbk.eu Abstract problem can be cast as a monolingual translation task and be addressed with different MT solutions (Simard et al., 2007; Pal et al., 2016). However, it has been proven that better performance can be obtained by not only using the raw output of the MT system but also by leveraging the source text (Chatterjee et al., 2017). In the last round of the APE shared task (Chatterjee et al., 2018a), the top-ranked systems (Tebbifakhr et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018) were based on Transformer (Vaswani et al., 2017), the state-of-the-art architecture in neural MT (NMT), with two encoders to encode both source text and MT output. Although using these systems to postedit the output of Phrase-Based Statistical Machine Translation (PBSMT) system resulted in a large boost in performance, smaller improvements were observed over neural MT outputs. Indeed, the good performance of the NMT systems leaves less room for improvement and poses the risk of over-correcting the MT output. Over-correction occurs when the APE system rephrases an already correct MT output. Al"
W19-5416,W17-4773,1,0.859247,"Missing"
W19-5416,P17-4012,0,0.0584243,"et and the query is quite high, but this is not always the case. So, to limit the risk of assigning a token obtained from the top triplet, but with a low similarity, a threshold (τ ) is set. If the average sentence-level BLEU of the top retrieved triplet is above τ , the relative token is associated to the query, otherwise the most frequent token in the training data is used. Once the token is obtained, it is added to the source and the sentence to be post-edited during inference. 3 3.1 Hyperparameters In our APE system, we use 32K merging rules for applying BPE. We employ OpenNMT-tf toolkit (Klein et al., 2017) to implement our system. We use 512 dimensions for the word embedding and 6 layers for both the encoders and the decoder, each containing 512 units and a feed-forward network with 1,024 dimensions. We set the attention and residual dropout probabilities, as well as the label-smoothing parameter to 0.1. For training the system, we use Adam optimizer (Kingma and Ba, 2014) with effective batch size of 8,192 tokens and the warm-up strategy introduced by (Vaswani et al., 2017) with warm-up steps equal to 8,000. We also employ beam search with beam width of 4. 3.3 Evaluation Metrics We use two diff"
W19-5416,L18-1004,1,0.860868,"for the task, computed based on the edit distance between the given hypothesis and the reference and ii) BLEU (Papineni et al., 2002), as the geometric average of n−gram precisions in the given hypothesis multiplied by the brevity penalty. Experimental Settings Data The official training data of the APE shared task contains a small amount of in-domain data, in which the post-edited outputs are real human postedits. To overcome the lack of data and to train neural APE models, the organizers also provided a large amount of synthetic data. For the En-Ru subtask, they provided the eSCAPE dataset (Negri et al., 2018), which is produced from a parallel corpus by considering the target sentences as artificial human post-edits and machine-translated source sentences as MT output. For the En-De subtask, in addition to the eSCAPE dataset, another synthetic dataset was made available, which is created using round-trip translation from a German monolingual corpus (Junczys-Dowmunt and Grundkiewicz, 2016). We clean the English to German/Russian eSCAPE dataset by removing i) samples with a length ratio between source text and post-edited output which is too different than the average and ii) samples where the sourc"
W19-5416,P16-2046,0,0.0378965,"Missing"
W19-5416,W18-1804,1,0.71585,"Aware Neural Automatic Post-Editing 1 Amirhossein Tebbifakhr1,2 , Matteo Negri1 , Marco Turchi1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento - Italy 2 University of Trento, Italy {atebbifakhr,negri,turchi}@fbk.eu Abstract problem can be cast as a monolingual translation task and be addressed with different MT solutions (Simard et al., 2007; Pal et al., 2016). However, it has been proven that better performance can be obtained by not only using the raw output of the MT system but also by leveraging the source text (Chatterjee et al., 2017). In the last round of the APE shared task (Chatterjee et al., 2018a), the top-ranked systems (Tebbifakhr et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018) were based on Transformer (Vaswani et al., 2017), the state-of-the-art architecture in neural MT (NMT), with two encoders to encode both source text and MT output. Although using these systems to postedit the output of Phrase-Based Statistical Machine Translation (PBSMT) system resulted in a large boost in performance, smaller improvements were observed over neural MT outputs. Indeed, the good performance of the NMT systems leaves less room for improvement and poses the risk of over-correcting the MT o"
W19-5416,P02-1040,0,0.103884,"dual dropout probabilities, as well as the label-smoothing parameter to 0.1. For training the system, we use Adam optimizer (Kingma and Ba, 2014) with effective batch size of 8,192 tokens and the warm-up strategy introduced by (Vaswani et al., 2017) with warm-up steps equal to 8,000. We also employ beam search with beam width of 4. 3.3 Evaluation Metrics We use two different evaluation metrics to assess the quality of our APE systems: i) TER (Snover et al., 2006), the official metric for the task, computed based on the edit distance between the given hypothesis and the reference and ii) BLEU (Papineni et al., 2002), as the geometric average of n−gram precisions in the given hypothesis multiplied by the brevity penalty. Experimental Settings Data The official training data of the APE shared task contains a small amount of in-domain data, in which the post-edited outputs are real human postedits. To overcome the lack of data and to train neural APE models, the organizers also provided a large amount of synthetic data. For the En-Ru subtask, they provided the eSCAPE dataset (Negri et al., 2018), which is produced from a parallel corpus by considering the target sentences as artificial human post-edits and"
W19-5416,W14-3346,0,0.0157356,"namely SIM, is an information retrieval approach, that, given a query containing the source and the MT sentence to be post-edited, retrieves the most similar triplet (source, MT sentence and post-edit) from the training data using an inverted index. Then, similarly to (Farajian et al., 2017), the retrieved triplets are ranked based on the averSystem Architecture The backbone architecture of our system is based on the state-of-the-art architecture in NMT i.e. Transformer (Vaswani et al., 2017). Like most NMT models, it follows the encoder-decoder 140 3.2 age of the sentence-level BLEU scores (Chen and Cherry, 2014) between a) the source segment in the query and the retrieved source sentence and b) the MT segment in the query and the retrieved MT sentence. For the most similar triplet, the TER between the MT sentence and the post-edit is computed and the token created. For highly repetitive and homogeneous corpora, the similarity between the top retrieved triplet and the query is quite high, but this is not always the case. So, to limit the risk of assigning a token obtained from the top triplet, but with a low similarity, a threshold (τ ) is set. If the average sentence-level BLEU of the top retrieved t"
W19-5416,P16-1162,0,0.0512668,"ut. For the En-De subtask, in addition to the eSCAPE dataset, another synthetic dataset was made available, which is created using round-trip translation from a German monolingual corpus (Junczys-Dowmunt and Grundkiewicz, 2016). We clean the English to German/Russian eSCAPE dataset by removing i) samples with a length ratio between source text and post-edited output which is too different than the average and ii) samples where the source text language is not English or post-edited output language is not German/Russian. In order to reduce the vocabulary size, we apply Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE merging rules on the union of the source text, MT output and post-edit output to obtain a shared vocabulary. 4 Results For both subtasks, we train our APE systems with and without prepending the token. We start the training of the APE systems on the union of the synthetic data and 20-times over-sampled indomain data. Then, we fine-tune the best performing checkpoint on the development set only on the in-domain data. The best performance on the development sets for En-De and En-Ru is reported in Tables 1 and 2 respectively. As shown in Table 1, both APE systems, with the orac"
W19-5416,W17-4713,1,0.873366,"le, we need to predict the proper token for the input sample. For predicting the proper token, we test two approaches. The first one, namely BERT, is based on a text classifier obtained by fine-tuning BERT (Devlin et al., 2018) on the in-domain data, which classifies the MT output into the three defined classes. The second one, namely SIM, is an information retrieval approach, that, given a query containing the source and the MT sentence to be post-edited, retrieves the most similar triplet (source, MT sentence and post-edit) from the training data using an inverted index. Then, similarly to (Farajian et al., 2017), the retrieved triplets are ranked based on the averSystem Architecture The backbone architecture of our system is based on the state-of-the-art architecture in NMT i.e. Transformer (Vaswani et al., 2017). Like most NMT models, it follows the encoder-decoder 140 3.2 age of the sentence-level BLEU scores (Chen and Cherry, 2014) between a) the source segment in the query and the retrieved source sentence and b) the MT segment in the query and the retrieved MT sentence. For the most similar triplet, the TER between the MT sentence and the post-edit is computed and the token created. For highly r"
W19-5416,W07-0728,0,0.138812,"Missing"
W19-5416,2006.amta-papers.25,0,0.388691,"Missing"
W19-5416,W18-6471,1,0.880601,"ebbifakhr1,2 , Matteo Negri1 , Marco Turchi1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento - Italy 2 University of Trento, Italy {atebbifakhr,negri,turchi}@fbk.eu Abstract problem can be cast as a monolingual translation task and be addressed with different MT solutions (Simard et al., 2007; Pal et al., 2016). However, it has been proven that better performance can be obtained by not only using the raw output of the MT system but also by leveraging the source text (Chatterjee et al., 2017). In the last round of the APE shared task (Chatterjee et al., 2018a), the top-ranked systems (Tebbifakhr et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2018) were based on Transformer (Vaswani et al., 2017), the state-of-the-art architecture in neural MT (NMT), with two encoders to encode both source text and MT output. Although using these systems to postedit the output of Phrase-Based Statistical Machine Translation (PBSMT) system resulted in a large boost in performance, smaller improvements were observed over neural MT outputs. Indeed, the good performance of the NMT systems leaves less room for improvement and poses the risk of over-correcting the MT output. Over-correction occurs when the APE system r"
W19-5416,W13-2231,1,0.856494,"” (minimal edits are required), and “heavy post-edit” (a large number of edits are required). However, the number of tokens can be increased/decreased to provide more fine/coarse-grained information to the APE system, but this is beyond the scope of this paper. Before training, we first compute the TER (Snover et al., 2006) score between the MT output and the post-edited output, then we add the no post-edit token to samples with zero TER score, light postedit to samples with non-zero TER score smaller than 40, and finally heavy post-edit to samples with TER score larger than 40. According to (Turchi et al., 2013, 2014), 40 TER is the level of quality above which a human translator tends to rewrite the post-edited sentence from scratch. At testing time, since the post-edited output is not available, we need to predict the proper token for the input sample. For predicting the proper token, we test two approaches. The first one, namely BERT, is based on a text classifier obtained by fine-tuning BERT (Devlin et al., 2018) on the in-domain data, which classifies the MT output into the three defined classes. The second one, namely SIM, is an information retrieval approach, that, given a query containing th"
W19-6603,N18-1008,0,0.253998,".com/mattiadg/ FBK-Fairseq-ST. 2 Related works Our work has been influenced by the recent works on end-to-end SLT, as well as the applications of SANs to the task of ASR. End-to-end SLT. The first encoder-decoder architecture based on LSTM was introduced for SLT by B´erard et al. (2016) showing the feasibility of 1 http://mustc.fbk.eu Dublin, Aug. 19-23, 2019 |p. 22 directly translating from the audio signal. Weiss et al. (2017) enhanced this approach by exploring settings with different numbers of layers in encoder and decoder and testing various multitask learning strategies. B´erard et al. (2018) trained a single model to translate English audiobooks into French and shown that pre-training the encoder on ASR data improves the final result. All these works showed that the input sequence length has to be reduced to work with recurrent models. To cope with the lack of end-to-end data, different directions have been evaluated. For instance, (Anastasopoulos and Chiang, 2018) and (Weiss et al., 2017) performed analyses of different multitask settings to leverage more data. Bansal et al. (2018) shown that the pre-training of the encoder is also helpful when performed on a different language,"
W19-6603,D16-1025,0,0.0312396,"ormance on six language directions. ∗ Work done during a summer internship at the Machine Translation Research Unit at Fondazione Bruno Kessler. ∗ c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 1 Introduction Neural encoder-decoder models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) is a general architecture that, by enabling to tackle sequence-to-sequence problems with a single endto-end model, achieved state-of-the-art results on machine translation (MT) (Bentivogli et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) and obtained increasingly good performance in automatic speech recognition (Chan et al., 2016; Chiu et al., 2018; Zhang et al., 2017; Zeyer et al., 2018; Dong et al., 2018). The advantages of end-to-end techniques, besides their conceptual simplicity, reside on the prevention of error propagation, and a reduced inference latency. Error propagation is particularly problematic for the SLT task (Ruiz et al., 2017), in which MT would be significantly penalized by errors resulting from the previous ASR processing step. For this reason"
W19-6603,C10-2010,0,0.092464,"Missing"
W19-6603,P18-1008,0,0.0316164,"nship at the Machine Translation Research Unit at Fondazione Bruno Kessler. ∗ c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 1 Introduction Neural encoder-decoder models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) is a general architecture that, by enabling to tackle sequence-to-sequence problems with a single endto-end model, achieved state-of-the-art results on machine translation (MT) (Bentivogli et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) and obtained increasingly good performance in automatic speech recognition (Chan et al., 2016; Chiu et al., 2018; Zhang et al., 2017; Zeyer et al., 2018; Dong et al., 2018). The advantages of end-to-end techniques, besides their conceptual simplicity, reside on the prevention of error propagation, and a reduced inference latency. Error propagation is particularly problematic for the SLT task (Ruiz et al., 2017), in which MT would be significantly penalized by errors resulting from the previous ASR processing step. For this reason, end-to-end solutions have been recently proposed (B´erard et a"
W19-6603,D16-1053,0,0.0875594,"Missing"
W19-6603,N19-1202,1,0.889644,"Missing"
W19-6603,L18-1001,0,0.27163,"nhancements to the Transformer architecture. To this aim, we proceed incrementally showing, through comparative experiments, that: 1. Sequence compression with CNNs and downsampling enables effective audio encoding while allowing to train the system even on single GPUs; 2. Modeling 2D dependencies produces more stable and better results; 3. Biasing the encoder self-attention with a distance penalty improves translation quality. Our experiments are run on different datasets covering different languages. First, we evaluate our architecture on two relatively small corpora: Augmented Librispeech (Kocabiyikoglu et al., 2018) for English→French and IWSLT 2018 for English→German. Then, we broaden the language coverage through experiments with MuSTC (Di Gangi et al., 2019),1 a large multilingual SLT dataset recently released. This allows to validate our findings on six language directions (EnDe/Es/Fr/Pt/Ro/Ru). Overall, our evaluation indicates that the proposed SLT-oriented adaptation of Transformer results in a model that significantly outperforms a strong end-to-end system both in translation quality and training speed. For the sake of results’ replicability the code developed for the experiments described in thi"
W19-6603,P02-1040,0,0.105781,"orpus and then we use it to initialize the weights of the SLT encoder. All the experiments are run on a single GPU Nvidia 1080 Ti with 12G of RAM, and the code used for all the experiments is based on Pytorch (Paszke et al., ). Data processing and evaluation. 40-dimensional MFCC filter-banks were extracted from the audio signals of each dataset using window size of 25 ms and step size 10 ms. The frame energy feature was additionally extracted from the LibriSpeech audio, similarly to (B´erard et al., 2018). All texts were tokenized and split into characters. Performance is evaluated with BLEU (Papineni et al., 2002) at token level after aggregating the output characters into words. Dublin, Aug. 19-23, 2019 |p. 26 Librispeech CNN+LSTM B-Transformer IWSLT LSTM B-Transformer Enc X X X X Dec X X X X X X X X 8.5 9.2 7.5 7.9 7.3 5.9 Table 2: Speech translation results for the Librispeech and IWSLT corpora wuth our two baseline models. A checkmark on Enc (Dec) means that the encoder (decoder) has been pretrainined. Enc / Dec LSTM Transformer LSTM 13.2 8.2 Transformer 11.9 9.0 Table 3: Mixed-architecture experiments on Librispeech. 6 Librispeech CNN+LSTM B-Transformer R-Transformer - Gauss penalty - log penalty"
W19-6625,2011.mtsummit-papers.35,0,0.110811,"Missing"
W19-6625,P15-2026,1,0.852334,"g from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approa"
W19-6625,W17-4773,1,0.870702,"ques (Isabel, 2017). In many cases, the state-of-the-art techniques are applied to improve translation proposals from a translation memory (TM) or directly produced by a mac 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 chine translation (MT) system. Post-editing techniques can be automated and seamlessly integrated into the typical translation pipeline for productivity gains. Two such techniques: fuzzy-match repair (FMR) (Ortega et al., 2016) and automatic post-editing (APE) (Chatterjee et al., 2017) have shown to be effective without the initial intervention of the translator by offering a repaired translation proposal from a TM in the case of FMR, and an improved MT output in the case of APE. FMR is an automatic post-editing technique typically used with TM-based computer-aided translation (CAT) tools. In TM-based CAT, the translator is offered a translation proposal that comes from a translation unit (a pair of parallel segments) whose source segment is similar to the segment to be translated. When the source segment in the translation unit and the segment to be translated are not iden"
W19-6625,W18-1804,1,0.873753,"echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximize results. The APE system used in this paper proposes a novel approach extending the original technology implemented by the best performing system at the WMT 2016 APE shared task (Chatterjee et al., 2017). 2.3 Combination of approaches We briefly describe a few combinations of approaches and systems that are usually used in different scenarios, such as FMR and APE, and that could be considered nov"
W19-6625,2011.eamt-1.28,0,0.0241052,"se should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed a comparison of the nature of MT systems for their use in FMR. In particular, they contrast the quality of FMR output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indomain data. Other novel works, like the work by Bult´e et al. (2018), include FMR as a primary part of a system integrating MT and TM. Lastly, Ortega et al. (2018) have found a statistical way to select the best MT system to use i"
W19-6625,2015.iwslt-evaluation.11,0,0.0142639,"els are optimized using Adagrad (Duchi et al., 2011) and every 10,000 mini-batches they are evaluated with BLEU on the 500-sentence-pairs development set. 4.3 APE settings The APE system is trained on the eSCAPE corpus (Negri et al., 2018), a collection of ∼7M triplets (source, MT output and reference), where the MT outputs have been created by a phrasebased MT system. It consists of datasets belonging to different domains and it is filtered by removing duplicates and too short (3 words) or too long (60 words) segments. To adapt the generic APE system to the FMR task, the model is fine-tuned (Luong and Manning, 2015) on 2,500 triplets (see Section 4.1), where the source input is paired with the repaired translation proposal produced by FMR. Similar to the neural MT system, the APE system is trained on sub-word units by using BPE. The APE vocabulary is created by selecting 50k most frequent sub-words. Word embedding and GRU hidden state size is set to 1024. Network parameters are optimized with Adagrad with a learning rate of 0.01. Source and target dropout is set to 6 All available at opus.lingfil.uu.se. www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 8 www.statmt.org/europarl/ 7 Dublin, Aug. 19-2"
W19-6625,W07-0732,0,0.0399822,"urring errors from an MT system by learning from human corrections. Starting from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seve"
W19-6625,L18-1004,1,0.86203,"Missing"
W19-6625,2005.eamt-1.18,0,0.0746382,"ld be “glued” together to form a new system that is added in a modular way to a traditional CAT pipeline. Third, in Section 4 we describe our exProceedings of MT Summit XVII, volume 1 Related work Fuzzy-match repair FMR aims to reduce the post-editing effort of translation proposals retrieved from a TM. To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed"
W19-6625,W17-4775,0,0.0132537,", volume 1 MT QE to choose between the original MT output and its post-edited version. Additionally, Tan et al. (2017) attempts to correct a common problem in APE known as “overcorrection” (i.e. systems’ tendency to completely re-translate the MT output, also rephrasing parts that are already correct). They do this by specifying two models (called neural post-editing models). Then, they use MT and QE to help select one of the models for the translation. This by no means is related to fuzzy-match repair; however, the idea of combining several systems around APE is similar to what we are doing. Hokamp (2017) includes word-level MT QE features as additional inputs to an APE system and trains several neural models using different input representations, but sharing the same output space. These models are finally ensembled together and tuned for APE and MT QE. 3 TM repairing through FMR and APE Our system is a two-step process that can be added to any TM-based CAT tool that has access to a source of bilingual information (SBI), such as a black-box MT system. The first step of our process is to use the translation unit whose source segment is most similar to the segment to be translated as input to FM"
W19-6625,W16-2378,0,0.013358,"n improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximi"
W19-6625,I17-1013,0,0.0157202,"e “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximize results. The APE system used in this paper proposes a novel ap"
W19-6625,2016.amta-researchers.3,1,0.873894,"Missing"
W19-6625,P16-2046,0,0.0336612,"Missing"
W19-6625,P02-1040,0,0.115866,"Missing"
W19-6625,W18-2108,1,0.888247,"ou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed a comparison of the nature of MT systems for their use in FMR. In particular, they contrast the quality of FMR output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indomain data. Other novel works, like the work by Bult´e et al. (2018), include FMR as a primary part of a system integrating MT and TM. Lastly, Ortega et al. (2018) have found a statistical way to select the best MT system to use in black-box FMR. Dublin, Aug. 19-23, 2019 |p. 257 2.2 Automatic post-editing Automatic post-editing is"
W19-6625,2012.eamt-1.34,0,0.0603209,"Missing"
W19-6625,2010.jec-1.4,0,0.124755,"anslation proposal that comes from a translation unit (a pair of parallel segments) whose source segment is similar to the segment to be translated. When the source segment in the translation unit and the segment to be translated are not identical, which happens very often, the translation proposal needs to be post-edited in order to create the final translation. FMR aims to provide repaired translation hypotheses to reduce the postediting effort of the original translation proposals by using another source of bilingual information such as an MT system. Some approaches to FMR, like the one by Koehn and Senellart (2010), heavily depend on the specific MT system type being used for repairing. Others, such as the one by Ortega et al. (2016) use an agnostic, black-box, MT system in such a way that the user would only choose from several repaired hypothesis proposals. APE aims to correct the errors present in a machine-translated text before showing it to the translator or post-editor. As motivated by Parton et al. (2012), an APE system can help to improve MT output by exploiting information that is not available during translation, or by performing a deeper text analysis, and by adapting the output of Dublin, A"
W19-6625,P07-2045,0,0.00759509,"Missing"
W19-6625,P16-1162,0,0.0261162,"0 sentences selected at random from the DGT TM, 2,500 are randomly selected and used to fine-tune the APE system (see Section 4.3), 500 are used for development, and 1,000 for testing. Altogether, about 350 sentences are not successfully repaired by FMR; in those cases, we used the output of Moses. 4.2 Machine translation systems We use the phrase-based statistical MT system Moses (Koehn et al., 2007) as a SBI for FMR; it has shown to perform well in previous experiments and in the black-box setting (Knowles et al., 2018). As a term of comparison we use Moses and the neural MT system Nematus (Sennrich et al., 2016) as baselines; we leave for future work the inclusion of a neural MT system as a SBI for FMR. It is worth noting that the phrase-based MT system performed better on the APE module than the neural MT system (see Table 2). With Moses we use pre-trained models downloaded from www.statmt.org/moses/ RELEASE-3.0/models/. By using pretrained models, we try to replicate what most users in a corporate setting would choose, at least as a first iteration, in absence of advanced knowledge to build the MT models by their own. Nematus is trained on a collection of datasets belonging to different domains. Th"
W19-6625,J10-4005,0,0.035949,"e words in s and s0 obtained as a by-product of the computation of the word-based edit distance (Levenshtein, 1966) between s and 1 Other SBIs that could be used are sub-segment translation memories, bilingual dictionaries or phrase tables. Dublin, Aug. 19-23, 2019 |p. 258 s0 : mismatched words are left unaligned. SBIs are then used to translate into the target language subsegment pairs of s and s0 containing mismatched words. The sub-segments pairs to be translated are obtained by using the phrase-pair extraction algorithm used in phrase-based statistical MT to obtain bilingual phrase pairs (Koehn, 2010, section 5.2.3). The translations obtained for the subsegments of s are used to identify the sub-segment in t that needs to be modified, and the translation of the sub-segments of s0 to identify the way they should be modified. In this way, a set of patching operators is built. Each patching operator consists of a sub-segment σ of s, a sub-segment σ 0 of s0 aligned with σ, a sub-segment τ of t to be repaired, and a sub-segment τ 0 , the translation of σ 0 , to be used for repairing. By combining these patching operators, a set of fuzzy-match repaired hypothesis is generated. For a detailed de"
W19-6625,kranias-samiotou-2004-automatic,0,0.0936348,"how how the two technologies could be “glued” together to form a new system that is added in a modular way to a traditional CAT pipeline. Third, in Section 4 we describe our exProceedings of MT Summit XVII, volume 1 Related work Fuzzy-match repair FMR aims to reduce the post-editing effort of translation proposals retrieved from a TM. To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles e"
W19-6625,2009.mtsummit-papers.14,0,0.0474348,"effort of translation proposals retrieved from a TM. To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed a comparison of the nature of MT systems for their use in FMR. In particular, they contrast the quality of FMR output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indo"
W19-6625,N07-1064,0,0.0532993,"output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indomain data. Other novel works, like the work by Bult´e et al. (2018), include FMR as a primary part of a system integrating MT and TM. Lastly, Ortega et al. (2018) have found a statistical way to select the best MT system to use in black-box FMR. Dublin, Aug. 19-23, 2019 |p. 257 2.2 Automatic post-editing Automatic post-editing is the task of correcting recurring errors from an MT system by learning from human corrections. Starting from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016"
W19-6625,2006.amta-papers.25,0,0.174736,"Missing"
W19-6625,W18-6471,1,0.828095,"PE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximize results. The APE system used in this paper proposes a novel approach extending the origi"
W19-6625,2007.mtsummit-wpt.4,0,0.0992745,"MT system by learning from human corrections. Starting from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (B"
