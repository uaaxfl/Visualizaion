2021.emnlp-main.376,Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes,2021,-1,-1,2,1,9477,tomasz limisiewicz,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mare{\v{c}}ek, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT{'}s contextual representations for nine diverse languages. We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing."
2021.blackboxnlp-1.20,Analyzing {BERT}{'}s Knowledge of Hypernymy via Prompting,2021,-1,-1,2,0,12103,michael hanna,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"The high performance of large pretrained language models (LLMs) such as BERT on NLP tasks has prompted questions about BERT{'}s linguistic capabilities, and how they differ from humans{'}. In this paper, we approach this question by examining BERT{'}s knowledge of lexical semantic relations. We focus on hypernymy, the {``}is-a{''} relation that relates a word to a superordinate category. We use a prompting methodology to simply ask BERT what the hypernym of a given word is. We find that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernyms with up to 57{\%} accuracy. Moreover, BERT with prompting outperforms other unsupervised models for hypernym discovery even in an unconstrained scenario. However, BERT{'}s predictions and performance on a dataset containing uncommon hyponyms and hypernyms indicate that its knowledge of hypernymy is still limited."
2021.acl-long.36,Introducing Orthogonal Constraint in Structural Probes,2021,-1,-1,2,1,9477,tomasz limisiewicz,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization."
2020.findings-emnlp.245,{U}niversal {D}ependencies {A}ccording to {BERT}: {B}oth {M}ore {S}pecific and {M}ore {G}eneral,2020,11,0,2,1,9477,tomasz limisiewicz,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"This work focuses on analyzing the form and extent of syntactic abstraction captured by BERT by extracting labeled dependency trees from self-attentions. Previous work showed that individual BERT heads tend to encode particular dependency relation types. We extend these findings by explicitly comparing BERT relations to Universal Dependencies (UD) annotations, showing that they often do not match one-to-one. We suggest a method for relation identification and syntactic tree construction. Our approach produces significantly more consistent dependency trees than previous work, showing that it better explains the syntactic abstractions in BERT. At the same time, it can be successfully applied with only a minimal amount of supervision and generalizes well across languages."
W19-4818,Derivational Morphological Relations in Word Embeddings,2019,9,0,3,0,3176,tomavs musil,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes. In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language. We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmas into derivational trees. For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors. Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation {`}bake{--}baker{'} belongs to category {`}actor{'}, and a correct clustering puts it into the same cluster as {`}govern{--}governor{'})."
W19-4827,From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions,2019,21,0,1,1,9478,david marevcek,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall."
W18-6326,Input Combination Strategies for Multi-Source Transformer Decoder,2018,17,0,3,0,13977,jindvrich libovicky,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"In multi-source sequence-to-sequence tasks, the attention mechanism can be modeled in several ways. This topic has been thoroughly studied on recurrent architectures. In this paper, we extend the previous work to the encoder-decoder attention in the Transformer architecture. We propose four different input combination strategies for the encoder-decoder attention: serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of multimodal translation and translation with multiple source languages. The experiments show that the models are able to use multiple sources and improve over single source baselines."
W18-5444,Extracting Syntactic Trees from Transformer Encoder Self-Attentions,2018,0,6,1,1,9478,david marevcek,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"This is a work in progress about extracting the sentence tree structures from the encoder{'}s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations."
K18-2019,{CUNI} x-ling: Parsing Under-Resourced Languages in {C}o{NLL} 2018 {UD} Shared Task,2018,0,4,2,0.655468,14784,rudolf rosa,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with no or little training data available. We employed a wide range of approaches, including simple word-based treebank translation, combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages. In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category."
W17-4720,{CUNI} submission in {WMT}17: Chimera goes neural,2017,0,1,2,0,27706,roman sudarikov,Proceedings of the Second Conference on Machine Translation,0,None
W17-4769,{CUNI} Experiments for {WMT}17 Metrics Task,2017,4,0,1,1,9478,david marevcek,Proceedings of the Second Conference on Machine Translation,0,None
W17-2806,Communication with Robots using Multilayer Recurrent Networks,2017,6,0,2,0,31851,bedvrich pivsl,Proceedings of the First Workshop on Language Grounding for Robotics,0,"In this paper, we describe an improvement on the task of giving instructions to robots in a simulated block world using unrestricted natural language commands."
W17-1226,"{S}lavic Forest, {N}orwegian Wood",2017,0,0,3,0.666302,14784,rudolf rosa,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"We once had a corp, or should we say, it once had us They showed us its tags, isn{'}t it great, unified tags They asked us to parse and they told us to use everything So we looked around and we noticed there was near nothing We took other langs, bitext aligned: words one-to-one We played for two weeks, and then they said, here is the test The parser kept training till morning, just until deadline So we had to wait and hope what we get would be just fine And, when we awoke, the results were done, we saw we{'}d won So, we wrote this paper, isn{'}t it good, Norwegian wood."
Y16-2018,Planting Trees in the Desert: Delexicalized Tagging and Parsing Combined,2016,14,0,2,0,5828,daniel zeman,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
W16-6401,{M}oses {\\&} Treex Hybrid {MT} Systems Bestiary,2016,19,0,4,0.685993,14784,rudolf rosa,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-2318,Merged bilingual trees based on {U}niversal {D}ependencies in Machine Translation,2016,7,1,1,1,9478,david marevcek,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"In this paper, we present our new experimental system of merging dependency representations of two parallel sentences into one dependency tree. All the inner nodes in dependency tree represent source-target pairs of words, the extra words are in form of leaf nodes. We use Universal Dependencies annotation style, in which the function words, whose usage often differs between languages, are annotated as leaves. The parallel treebank is parsed in minimally supervised way. Unaligned words are there automatically pushed to leaves. We present a simple translation system trained on such merged trees and evaluate it in WMT 2016 English-to-Czech and Czechto-English translation task. Even though the model is so far very simple and no language model and word-reordering model were used, the Czech-to-English variant reached similar BLEU score as another established tree-based system."
L16-1015,If You {E}ven Don{'}t Have a Bit of {B}ible: Learning Delexicalized {POS} Taggers,2016,17,2,2,0,13611,zhiwei yu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Various unsupervised and semi-supervised methods have been proposed to tag an unseen language. However, many of them require some partial understanding of the target language because they rely on dictionaries or parallel corpora such as the Bible. In this paper, we propose a different method named delexicalized tagging, for which we only need a raw corpus of the target language. We transfer tagging models trained on annotated corpora of one or more resource-rich languages. We employ language-independent features such as word length, frequency, neighborhood entropy, character classes (alphabetic vs. numeric vs. punctuation) etc. We demonstrate that such features can, to certain extent, serve as predictors of the part of speech, represented by the universal POS tag."
rosa-etal-2014-hamledt,{H}amle{DT} 2.0: Thirty Dependency Treebanks Stanfordized,2014,30,20,3,1,14784,rudolf rosa,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present HamleDT 2.0 (HArmonized Multi-LanguagE Dependency Treebank). HamleDT 2.0 is a collection of 30 existing treebanks harmonized into a common annotation style, the Prague Dependencies, and further transformed into Stanford Dependencies, a treebank annotation style that became popular in recent years. We use the newest basic Universal Stanford Dependencies, without added language-specific subtypes. We describe both of the annotation styles, including adjustments that were necessary to make, and provide details about the conversion process. We also discuss the differences between the two styles, evaluating their advantages and disadvantages, and note the effects of the differences on the conversion. We regard the stanfordization as generally successful, although we admit several shortcomings, especially in the distinction between direct and indirect objects, that have to be addressed in future. We release part of HamleDT 2.0 freely; we are not allowed to redistribute the whole dataset, but we do provide the conversion pipeline."
P13-3025,{D}eepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis,2013,20,7,2,1,14784,rudolf rosa,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge."
P13-1028,Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing,2013,28,22,1,1,9478,david marevcek,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direction), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks."
P13-1051,Coordination Structures in Dependency Treebanks,2013,31,22,2,1,227,martin popel,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Paratactic syntactic structures are notoriously difficult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical observations on convertibility between selected styles of representations are shown too."
W12-4205,Using Parallel Features in Parsing of Machine-Translated Sentences for Correction of Grammatical Errors,2012,32,6,3,1,14784,rudolf rosa,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In this paper, we present two dependency parser training methods appropriate for parsing outputs of statistical machine translation (SMT), which pose problems to standard parsers due to their frequent ungrammaticality. We adapt the MST parser by exploiting additional features from the source language, and by introducing artificial grammatical errors in the parser training data, so that the training sentences resemble SMT output.n n We evaluate the modified parser on DEPFIX, a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality."
W12-3132,Formemes in {E}nglish-{C}zech Deep Syntactic {MT},2012,20,10,6,0,2976,ondvrej duvsek,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"One of the most notable recent improvements of the TectoMT English-to-Czech translation is a systematic and theoretically supported revision of formemes---the annotation of morpho-syntactic features of content words in deep dependency syntactic structures based on the Prague tectogrammatics theory. Our modifications aim at reducing data sparsity, increasing consistency across languages and widening the usage area of this markup. Formemes can be used not only in MT, but in various other NLP tasks."
W12-3146,{DEPFIX}: A System for Automatic Correction of {C}zech {MT} Outputs,2012,15,27,2,1,14784,rudolf rosa,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We present an improved version of DEPFIX (Marecek et al., 2011), a system for automatic rule-based post-processing of English-to-Czech MT outputs designed to increase their fluency. We enhanced the rule set used by the original DEPFIX system and measured the performance of the individual rules.n n We also modified the dependency parser of McDonald et al. (2005) in two ways to adjust it for the parsing of MT outputs. We show that our system is able to improve the quality of the state-of-the-art MT systems."
W12-1911,Unsupervised Dependency Parsing using Reducibility and Fertility features,2012,4,1,1,1,9478,david marevcek,Proceedings of the {NAACL}-{HLT} Workshop on the Induction of Linguistic Structure,0,"This paper describes a system for unsupervised dependency parsing based on Gibbs sampling algorithm. The novel approach introduces a fertility model and reducibility model, which assumes that dependent words can be removed from a sentence without violating its syntactic correctness."
zeman-etal-2012-hamledt,{H}amle{DT}: To Parse or Not to Parse?,2012,24,34,2,0,5828,daniel zeman,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We propose HamleDT â HArmonized Multi-LanguagE Dependency Treebank. HamleDT is a compilation of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. While the license terms prevent us from directly redistributing the corpora, most of them are easily acquirable for research purposes. What we provide instead is the software that normalizes tree structures in the data obtained by the user from their original providers."
bojar-etal-2012-joy,The Joy of Parallelism with {C}z{E}ng 1.0,2012,15,34,6,0.308642,292,ondvrej bojar,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"CzEng 1.0 is an updated release of our Czech-English parallel corpus, freely available for non-commercial research or educational purposes. In this release, we approximately doubled the corpus size, reaching 15 million sentence pairs (about 200 million tokens per language). More importantly, we carefully filtered the data to reduce the amount of non-matching sentence pairs. CzEng 1.0 is automatically aligned at the level of sentences as well as words. We provide not only the plain text representation, but also automatic morphological tags, surface syntactic as well as deep syntactic dependency parse trees and automatic co-reference links in both English and Czech. This paper describes key properties of the released resource including the distribution of text domains, the corpus data formats, and a toolkit to handle the provided rich annotation. We also summarize the procedure of the rich annotation (incl. co-reference resolution) and of the automatic filtering. Finally, we provide some suggestions on exploiting such an automatically annotated sentence-parallel corpus."
D12-1028,Exploiting Reducibility in Unsupervised Dependency Parsing,2012,15,9,1,1,9478,david marevcek,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,The possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifestations of syntactic dependency. We introduce a novel unsupervised parsing approach that is based on a new n-gram reducibility measure. We perform experiments across 18 languages available in CoNLL data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results.
W11-3901,{G}ibbs Sampling with Treeness Constraint in Unsupervised Dependency Parsing,2011,14,0,1,1,9478,david marevcek,Proceedings of Workshop on Robust Unsupervised and Semisupervised Methods in Natural Language Processing,0,"This paper presents a work in progress on the task of unsupervised parsing, following the main stream approach of optimizing the overall probability of the corpus. We evaluate a sequence of experiments for Czech with various modifications of corpus initiation, of dependency edge probability model and of sampling procedure, stressing especially the treeness constraint. The best configuration is then applied to 19 languages from CoNLL-2006 and CoNLL-2007 shared tasks. Our best achieved results are comparable to the state of the art in dependency parsing and outperform the previously published results for many languages."
W11-2152,Two-step translation with grammatical post-processing,2011,11,17,1,1,9478,david marevcek,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,This paper describes an experiment in which we try to automatically correct mistakes in grammatical agreement in English to Czech MT outputs. We perform several rule-based corrections on sentences parsed to dependency trees. We prove that it is possible to improve the MT quality of majority of the systems participating in WMT shared task. We made both automatic (BLEU) and manual evaluations.
W11-2153,Influence of Parser Choice on Dependency-Based {MT},2011,18,9,2,1,227,martin popel,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,Accuracy of dependency parsers is one of the key factors limiting the quality of dependency-based machine translation. This paper deals with the influence of various dependency parsing approaches (and also different training data size) on the overall performance of an English-to-Czech dependency-based statistical translation system implemented in the Treex framework. We also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU.
W10-1730,Maximum Entropy Translation Model in Dependency-Based {MT} Framework,2010,18,13,3,0.315537,7153,zdenvek vzabokrtsky,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"Maximum Entropy Principle has been used successfully in various NLP tasks. In this paper we propose a forward translation model consisting of a set of maximum entropy classifiers: a separate classifier is trained for each (sufficiently frequent) source-side lemma. In this way the estimates of translation probabilities can be sensitive to a large number of features derived from the source sentence (including non-local features, features making use of sentence syntactic structure, etc.). When integrated into English-to-Czech dependency-based translation scenario implemented in the TectoMT framework, the new translation model significantly outperforms the baseline model (MLE) in terms of BLEU. The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model."
P10-2016,Tackling Sparse Data Issue in Machine Translation Evaluation,2010,9,16,3,0.555556,292,ondvrej bojar,Proceedings of the {ACL} 2010 Conference Short Papers,0,We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.
W09-4005,Converting {R}ussian Treebank {S}yn{T}ag{R}us into Praguian {PDT} Style,2009,8,2,1,1,9478,david marevcek,"Proceedings of the Workshop Multilingual resources, technologies and evaluation for central and Eastern {E}uropean languages",0,"In this paper, we report a work in progress on transforming syntactic structures from the SynTagRus corpus into tectogrammatical trees in the Prague Dependency Treebank (PDT) style. SynTagRus (Russian) and PDT (Czech) are both dependency treebanks sharing lots of common features and facing similar linguistic challenges due to the close relatedness of the two languages. While in PDT the tectogrammatical representation exists, sentences in SynTagRus are annotated on syntactic level only."
W09-0422,{E}nglish-{C}zech {MT} in 2008,2009,15,13,2,0.555556,292,ondvrej bojar,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,We describe two systems for English-to-Czech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach.
2008.eamt-1.16,Automatic alignment of {C}zech and {E}nglish deep syntactic dependency trees,2008,13,8,1,1,9478,david marevcek,Proceedings of the 12th Annual conference of the European Association for Machine Translation,0,"In this paper, we focus on alignment of Czech and English tectogrammatical dependency trees. The alignment of deep syntactic de- pendency trees can be used for training transfer models for machine translation systems based on analysis-transfer-synthesis architecture. The results of our experiments show that shifting the alignment task from the word layer to the tectogrammatical layer both (a) increases the inter- annotator agreement on the task and (b) allows to construct a feature- based algorithm which uses sentence structure and which outperforms the GIZA aligner in terms of f-measure on aligned tectogrammatical node pairs."
