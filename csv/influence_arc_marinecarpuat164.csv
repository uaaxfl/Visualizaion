2006.iwslt-evaluation.5,W02-2020,0,0.0420419,"classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. Our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. As demonstrated by Wu et al. (2002) and Carreras et al. (2002), boosting can be used to build language independent NER models that perform exceptionally well. Support Vector Machines: Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992). Sassano and Utsuro (2000) and McNamee and Mayfield (2002) have demonstrated that SVMs show promise when applied to named entity recognition, though performance appears • Lexical (words and lemmas) and syntactic (partof-speech) information within a window of 2 words surrounding the current word • Prefixes and suffixes of up to a length of 4 characters from the current word • Capitalization: whether the word starts with a capital letter and/or the entire word is capitalized • A small set of conjunctions of POS tags and words within a window of 2 words of the current word • Previous history: the chunk tags (gold standard during training; assigned for e"
2006.iwslt-evaluation.5,N01-1006,0,0.0178429,"8992 13337 13337 18992 quite sensitive to parameter choices. Transformation-based learning: Transformationbased learning (TBL) is a rule-based machine learning algorithm that was first introduced by Brill (1995) and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Our system uses the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. patterns: for instance, many location and person names can typically be transliterated, while some components of organization names should be translated with a standard bilexicon instead. After identifying NE boundaries and types, a rulebased translation approach based on name gazetteers and transliteration schemes is used to obtain one or more translations for each identified NE. The decoder integrates the NE translation candidates as additional translation candidates for the NE phr"
2006.iwslt-evaluation.5,P02-1038,0,0.0119,"be well suited to handling features that are highly interdependent. 2.1. Decoder For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation. While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes an appropriate point of reference. 2.2. Phrasal bilexicon The core phrasal bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2002). The intersection is augmented using growing heuristics proposed by Och and Ney (2002) in order to improve recall. Following Koehn (2003), each entry in the phrasal bilexicon is scored using phrase translation conditional probabilities for both translation directions, as well as lexical weights which combine word translation probabilities according to the word alignment observed within the phrase pair during training. 2.3. Language model The language model is a standard trigram model with Kneser-Ney smoothing trained using the SRI language modeling toolkit (Stolcke, 2002). 3.2. WSD features T"
2006.iwslt-evaluation.5,J95-4004,0,0.00644608,"translation disambiguation task faced by the SMT system. • Instead of using predefined senses drawn from 39 Table 3: IWSLT-06 Training data statistics computed for the 4 language pairs Training Data Statistics Chinese-English Arabic-English Italian-English Japanese-English Number of bisentences 39953 19972 19972 39953 Vocabulary size (input lang) 11178 25152 17917 12535 Vocabulary size (English) 18992 13337 13337 18992 quite sensitive to parameter choices. Transformation-based learning: Transformationbased learning (TBL) is a rule-based machine learning algorithm that was first introduced by Brill (1995) and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Our system uses the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. patterns: for instance, many location and person names can typically be tran"
2006.iwslt-evaluation.5,P02-1040,0,0.0769139,"): ïå ÷ Š ` ( å, „ 0@ ™ e } Output: Could you please write down the address in Japan, please. Input (read): ïå ÷î ¨ ( å, „ 0@ ™ e } Output: Could you please write down the address in Japan, please. Input (spontaneous): ïå ÷ ž Ù º „ 0@ ™ † } Output: May handle, deal with the address of the please. Example 2 Input (text): ` Z A åM Å{ {° Output: You must check in by ten o’clock in the evening. Input (read): ¨ ©?A åM Å{ {° Output: You must check in at ten before. Input (spontaneous): ¨ Z A åM Å{ {° Output: You must check in by ten o’clock in the evening. mon automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), as well as word error rate (WER) and position-independent word error rate (PER) (Tillmann et al., 1997). The HKUST system achieves reasonable performance, with evaluation scores situated in the middle range, compared to all systems evaluated on the open track. syntactic units, is the first step of processing in ASVMT. This is followed by lemmatization which, in ASVMT, refers to a normalization step where the tokens coming from stems that were modified when agglutinated are converted back to their original form. Italian: We preproces"
2006.iwslt-evaluation.5,I05-2021,1,0.860642,"ed and therefore not designed and tuned to tackle the challenges of speech translation. We also find that the system achieves reasonable results on a wide range of languages, by evaluating on read speech transcriptions from Arabic, Italian, and Japanese into English. 1. Introduction The role and usefulness of semantic processing for Statistical Machine Translation (SMT) has recently been much debated. In previous work, we reported surprisingly disappointing results when using the predictions of a Senseval word sense disambiguation (WSD) system in conjunction with SMT using an IBM-style model (Carpuat and Wu, 2005b). Nevertheless, error analysis leaves little doubt that the performance of SMT systems still suffers from inaccurate lexical choice. Other empirical studies have shown that SMT systems perform much more poorly than dedicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation"
2006.iwslt-evaluation.5,W05-0822,0,0.0114512,"oth 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation model. This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used. Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yie"
2006.iwslt-evaluation.5,P05-1048,1,0.935692,"ed and therefore not designed and tuned to tackle the challenges of speech translation. We also find that the system achieves reasonable results on a wide range of languages, by evaluating on read speech transcriptions from Arabic, Italian, and Japanese into English. 1. Introduction The role and usefulness of semantic processing for Statistical Machine Translation (SMT) has recently been much debated. In previous work, we reported surprisingly disappointing results when using the predictions of a Senseval word sense disambiguation (WSD) system in conjunction with SMT using an IBM-style model (Carpuat and Wu, 2005b). Nevertheless, error analysis leaves little doubt that the performance of SMT systems still suffers from inaccurate lexical choice. Other empirical studies have shown that SMT systems perform much more poorly than dedicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation"
2006.iwslt-evaluation.5,C00-2102,0,0.0176483,"e strong classifier. Each weak classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. Our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. As demonstrated by Wu et al. (2002) and Carreras et al. (2002), boosting can be used to build language independent NER models that perform exceptionally well. Support Vector Machines: Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992). Sassano and Utsuro (2000) and McNamee and Mayfield (2002) have demonstrated that SVMs show promise when applied to named entity recognition, though performance appears • Lexical (words and lemmas) and syntactic (partof-speech) information within a window of 2 words surrounding the current word • Prefixes and suffixes of up to a length of 4 characters from the current word • Capitalization: whether the word starts with a capital letter and/or the entire word is capitalized • A small set of conjunctions of POS tags and words within a window of 2 words of the current word • Previous history: the chunk tags (gold standard"
2006.iwslt-evaluation.5,W04-0822,1,0.916544,"tracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. We have showed that KPCAbased WSD models achieve close accuracies to the best individual WSD models, while having a significantly different bias (Carpuat et al., 2004). All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 2.1. Decoder For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation. While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes"
2006.iwslt-evaluation.5,W02-2004,0,0.08139,"Missing"
2006.iwslt-evaluation.5,C04-1190,1,0.836588,"ankings and are more discriminative than the baseline translation probabilities, yielding improved translations as can be seen in Table 2. Table 2: Translation examples with and without WSD for SMT Example 1 Input © ÜU } Ref. May I see the menu ? Baseline Let me see the menu ? + WSD May I see the menu ? Example 2 Input ý Š „ §M Ù Ref. Would you show me to my seat ? Baseline Can you change my seat finger for me ? + WSD Can you direct me to my seat ? 4. Named-entity translation cal sample disambiguation tasks both on Senseval-2 and Senseval-3 data (e.g., Carpuat et al. (2004), Wu et al. (2004), Su et al. (2004)). Recognizing, disambiguating, and translating entities is a special case of word sense disambiguation for translation lexical choice, where the words or phrases in question are entities of various sorts. Translating names correctly is particularly important to translation quality and usefulness, but does present some distinct challenges from regular phrase translation. First, the vast majority of names are rare and often never seen in training, and, with the exception of names of well-known persons or other entities, are typically not recorded in lexicons. Second, whether a phrase is a named"
2006.iwslt-evaluation.5,W02-2035,1,0.875051,"iers As NER can be framed as a classification task, we use an ensemble of three relatively high performing machine learning classifiers: Boosting: The main idea behind boosting algorithms is that a set of many weak classifiers can be effectively combined to yield a single strong classifier. Each weak classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. Our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. As demonstrated by Wu et al. (2002) and Carreras et al. (2002), boosting can be used to build language independent NER models that perform exceptionally well. Support Vector Machines: Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992). Sassano and Utsuro (2000) and McNamee and Mayfield (2002) have demonstrated that SVMs show promise when applied to named entity recognition, though performance appears • Lexical (words and lemmas) and syntactic (partof-speech) information within a window of 2 words surrounding the current word • Prefixes and suffixes of up to a length of 4 cha"
2006.iwslt-evaluation.5,W03-0433,1,0.809241,", and vice versa. Third, unlike European languages, Chinese allows an open vocabulary for proper names of persons, eliminating another major source of explicit clues used by European language NER models. Based on these observations, we use character-level features instead of word-level features; this prevents committing to a given word segmentation, which might be incorrect at NE boundaries. Several versions of this NER system were extensively evaluated on NER shared tasks for Chinese at SIGHAN 2006 (Yu et al., 2006) and for several European languages at CoNLL 2002 (Wu et al., 2002) and 2003 (Wu et al., 2003). English sentence. All training data was clean text, representing a mismatch to the test data used in the evaluation, which was noisy output from automatic speech recognition. In addition to recognition errors, automatic speech transcriptions do not contain punctuation, and use digits to represent numbers. Performance could be improved by eliminating the mismatch between training and test data. For each Chinese sentence, we are given correct speech transcriptions as well as automatic read speech transcriptions and automatic spontaneous speech transcriptions. For the other languages, we only t"
2006.iwslt-evaluation.5,W02-1002,0,0.0223543,". (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification, as described in Section 4.1.1. We also use the Adaboost.MH algorithm for WSD, just like for NER. The fourth voting model is a model based on Kernel PCA (Wu et al., 2004). Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets"
2006.iwslt-evaluation.5,P04-1081,1,0.795225,"on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification, as described in Section 4.1.1. We also use the Adaboost.MH algorithm for WSD, just like for NER. The fourth voting model is a model based on Kernel PCA (Wu et al., 2004). Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. We have showed that KPCAbased WSD models achieve close accuracies"
2006.iwslt-evaluation.5,2005.iwslt-1.8,0,0.0189194,"dicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation model. This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used. Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) foun"
2006.iwslt-evaluation.5,koen-2004-pharaoh,0,0.0488484,"pf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. We have showed that KPCAbased WSD models achieve close accuracies to the best individual WSD models, while having a significantly different bias (Carpuat et al., 2004). All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 2.1. Decoder For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation. While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes an appropriate point of reference. 2.2. Phrasal bilexicon The core phrasal bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2002). The intersection is augmented using growing heuristics proposed by Och and Ney (2002) in order to improve recall. Following Koeh"
2006.iwslt-evaluation.5,2005.iwslt-1.20,0,0.0338571,"more poorly than dedicated WSD models, both 2. Machine translation engine ∗ This work was supported in part by DARPA GALE contract HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine trans37 3.1. WSD classifiers lation model. This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used. Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)). All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method. The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1979), since Klein an"
2006.iwslt-evaluation.5,W06-0124,1,\N,Missing
2007.iwslt-1.12,2006.iwslt-evaluation.5,1,0.846504,"d on Chinese to English translation, but we also report results on the Arabic to English, Italian to English, and Japanese to English tasks. 1. Introduction We describe experiments conducted at HKUST during the IWSLT 2007 evaluation campaign on spoken language translation. For our second participation in the IWSLT evaluation, our focus was on experimenting with Moses (Koehn et al., 2007), the new open-source toolkit for phrase-based Statistical Machine Translation (SMT), and on comparing it against its closed-source predecessor Pharaoh (Koehn, 2004) which we used in our IWSLT 2006 submission (Carpuat et al., 2006). Our main focus was on the Chinese-English task, which, this year, used clean text as opposed to the other tasks where speech transcriptions were to be translated. We also report results on all the language pairs, although we did not do any tuning or any language-specific processing for the Arabic to English, Japanese to English and Italian to English tasks.  This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC625"
2007.iwslt-1.12,2006.iwslt-evaluation.19,0,0.0290995,"under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate richer linguistic i"
2007.iwslt-1.12,P07-2045,0,0.015333,"he HKUST experiments in the IWSLT 2007 evaluation campaign on spoken language translation. Our primary objective was to compare the open-source phrase-based statistical machine translation toolkit Moses against Pharaoh. We focused on Chinese to English translation, but we also report results on the Arabic to English, Italian to English, and Japanese to English tasks. 1. Introduction We describe experiments conducted at HKUST during the IWSLT 2007 evaluation campaign on spoken language translation. For our second participation in the IWSLT evaluation, our focus was on experimenting with Moses (Koehn et al., 2007), the new open-source toolkit for phrase-based Statistical Machine Translation (SMT), and on comparing it against its closed-source predecessor Pharaoh (Koehn, 2004) which we used in our IWSLT 2006 submission (Carpuat et al., 2006). Our main focus was on the Chinese-English task, which, this year, used clean text as opposed to the other tasks where speech transcriptions were to be translated. We also report results on all the language pairs, although we did not do any tuning or any language-specific processing for the Arabic to English, Japanese to English and Italian to English tasks.  This"
2007.iwslt-1.12,E06-1031,0,0.0938417,"(DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate"
2007.iwslt-1.12,P02-1038,0,0.0434894,"n decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate richer linguistic information. However, we do not use the factored representation in this first set of experiments, and use the surface form of words, just like in Pharaoh. 2.2. Phrasal bilexicon The core phrasal bilexicon is obtained by collecting phrase pairs that are consistent with the IBM model 4 alignments obtained with GIZA++ (Och and Ney, 2002). During phrase extraction, we tried two different methods to get the final word alignment from the bi-directional GIZA++ alignments: (1) intersect and (2) grow-diagfinal. Intersect uses the strict intersection of the bidirectional word alignments, while grow-diag-final expands the alignment by adding directly neighboring alignment points, and alignment points in the diagonal neighborhood. We found that using grow-diag-final improves IWSLT-07 data set CE devtest1 CE devtest2 CE devtest3 Table 1: Resegmenting test sentences improves BLEU score. # original # sentences after BLEU with original BL"
2007.iwslt-1.12,P02-1040,0,0.0935189,"not optimal, as Italian presents more morphological inflexions than English, as suggested by the larger vocabulary size on the Italian side of the training data than on the English side (Table 2.) Japanese: We used the provided word segmentation and did not perform any additional processing. 5. Experimental results The official BLEU scores for HKUST’s submitted runs, which were buggy due to accidental errors in combining the models and parameters used in the experiments, are shown in Table 3 for all four language pairs. The official results were only automatically evaluated using BLEU score (Papineni et al., 2002). We achieved a BLEU score of 34.26 on Chinese to English read speech translation. There were 9 primary submissions to that task, with BLEU scores ranging from 19.34 to 40.77. Our subsequent debugged runs yielded higher translation accuracy. Updated results for our debugged runs on the development sets are reported in Table 4 for the Chinese-English task. For running the submitted buggy model on the official IWSLT-07 test set, there is a slight difference between the official BLEU score of 34.26 and our own measurement of 34.04. This difference appears to be caused by slight differences betwee"
2007.iwslt-1.12,2006.amta-papers.25,0,0.0178306,"on the development sets are reported in Table 4 for the Chinese-English task. For running the submitted buggy model on the official IWSLT-07 test set, there is a slight difference between the official BLEU score of 34.26 and our own measurement of 34.04. This difference appears to be caused by slight differences between BLEU scoring tools and settings (the tool we are using appears to give lower scores).We also computed the other most commonly used automatic evaluation metrics for translation quality: NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and Translation Edit Rate (TER) (Snover et al., 2006), Word Error Rate (WER), Position-Independent Word Error Rate (PER) and CDER (Leusch et al., 2006). 6. Comparing Moses results with Pharaoh Using the same phrasal bilexicon and language model as with Moses, we performed several contrastive runs using Pharaoh, all other settings being identical. Results are reported in Table 5 for three different baseline experimental settings. We performed many experimental runs in which we vary the experimental settings and pre or post processing steps, e.g. phrase tables, language models, to compare the translation quality produced by Pharoah and Moses. The"
2007.iwslt-1.12,P96-1021,1,0.764787,"Missing"
2007.iwslt-1.12,J97-3002,1,0.656199,"Missing"
2007.iwslt-1.12,2005.iwslt-1.20,0,0.0197801,"vanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based m"
2007.iwslt-1.12,2006.iwslt-evaluation.20,0,0.0201845,"previous evaluations where manual and automatic transcriptions of speech had to be translated. The IWSLT 2007 test set therefore matches more closely with the training data and the first three development test data, as opposed to the more recent Chinese-English tasks where automatic transcriptions of read and spontaneous speech were used. 3.2. Training data preprocessing For the training data, we used the same basic preprocessing as in our IWSLT 2006 submission, which consists in performing tokenization and case normalization. The case normalization method is the same as the one described in Zollmann et al. (2006), where the first word of the sentence is normalized to its most frequent form. English: The English was simply tokenized and case-normalized in the same manner for all languages. Chinese: We use the LDC segmenter to re-segment the Chinese side of the corpus to get a better segmentation. 3.3. English text normalization For all language pairs, in addition to training data normalization, we use simple heuristics to normalize punctuation, capitalization and contractions in the English output. 3.4. Improved sentence segmentation Since the training data is drawn from clean text as opposed to speech"
2007.iwslt-1.12,koen-2004-pharaoh,0,\N,Missing
2007.iwslt-1.12,W05-0909,0,\N,Missing
2007.iwslt-1.12,D07-1007,1,\N,Missing
2007.iwslt-1.12,D08-1064,0,\N,Missing
2007.iwslt-1.12,C08-1141,0,\N,Missing
2007.iwslt-1.12,2005.iwslt-1.16,0,\N,Missing
2007.mtsummit-papers.11,W05-0909,0,0.036251,"xicon is still a pure WSD model. Just as in any Senseval/SemEval multilingual lexical sample task (e.g., Chklovski et al. (2004)), the task consists of disambiguating between semantic distinctions made by another language. 4 Evaluation on full-scale translation We have conducted a comprehensive evaluation on two standard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD models independently, as we cannot safely assume that higher WSD evaluation scores necessarily lead to"
2007.mtsummit-papers.11,P91-1034,0,0.0893691,"glish translation and allows for rich expressive context features, but this system was also only evaluated on WSD accuracy, and not integrated in a full-scale machine translation system. 2.2 Context-dependent SMT To the best of our knowledge, our model represents the first attempt at integrating a fully phrasal context-dependent translation lexicon into SMT, where evaluation is conducted by measuring the accuracy of the resulting SMT system on a translation task (as opposed to, for example, measures of word sense disambiguation accuracy as discussed in the preceding section). In contrast with Brown et al. (1991), our approach incorporates the predictions of state-of-the-art WSD models that generalize across rich contextual features for any phrase in the input vocabulary. In Brown et al.’s early study of contextual features on SMT performance, the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize t"
2007.mtsummit-papers.11,I05-2021,1,0.941639,"e that for languages that do not contain space characters, such as Chinese (as considered in this paper), it is not even clear what “single word” means. Any string of characters could be considered as either a word or a phrase, if we insist on forcing an analogy to European languages. 2.3 WSD vs. SMT In previous work, we have obtained seemingly conflicting empirical evidence on the usefulness of WSD in SMT. When we integrated the WSD predictions of Senseval-style WSD models into a word-based SMT system in a number of ways, for the first time, we surprisingly obtained a decrease in BLEU score (Carpuat and Wu, 2005b). However, we also showed that SMT systems alone perform much worse that WSD systems on a WSD task (Carpuat and Wu, 2005a), which suggests that WSD should have something to offer to SMT. Taken together, these results suggest that a better framework for integrating contextual evidence in SMT is needed. In this paper, we argue that such a framework is provided by context-dependent phrasal translation lexicons. 3 Context-dependent phrasal translation lexicons As mentioned earlier, there are two main open issues in moving toward context-dependent phrasal translation lexicons. First, which dynami"
2007.mtsummit-papers.11,W04-0822,1,0.927777,"tual evidence into a translation or sense prediction. In particular, the Senseval/SemEval series of workshops have extensively evaluated systems with different feature sets, as well as different machine learning models for combining contextual evidence. Recent work on WSD for SMT also provides interesting insights. Following this approach, we propose to exploit WSD insights to build context-dependent translation lexicons for SMT. We use context features and WSD models that were designed and evaluated on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002) and Senseval-3 tasks (Carpuat et al., 2004). On the one hand, these tasks included monolingual lexical choice tasks, where word senses are defined according to some manually built ontology or semantic network such as WordNet, HowNet, or the like. More relevant to the task at hand, however, are also the multilingual lexical choice tasks, where word senses are directly defined as the semantic distinctions made by another language (e.g., Chklovski et al. (2004)). In many ways, the multilingual lexical choice tasks of Senseval/SemEval embody a more empirically justifiable approach to defining the sense inventory for WSD than the monolingua"
2007.mtsummit-papers.11,2006.iwslt-evaluation.5,1,0.831432,"e NIST and METEOR metrics are slightly improved while the BLEU and WER score gets worse, which shows that the results are not consistent across the most widely used automated evaluation metrics. This confirms that the full sentential context and the syntactic features used by WSD models are necessary to translate long phrases as well as single words, and therefore that WSD is an appropriate framework for integrating contextual information into traditional phrase-based SMT. Note that we also reported small improvements in BLEU score by using single-word WSD predictions in a Pharaoh baseline in Carpuat et al. (2006). These small improvements were obtained on a slightly weaker SMT baseline. On the contrary, Table 5 shows that BLEU scores now actually slightly decrease with our stronger baseline. This restricted lexicon approach is similar to the proposal by Cabezas and Resnik (2005) who used the XML input scheme to provide word-based WSD predictions in the Pharaoh decoder. They obtained small gains in BLEU score on the Spanish-English Europarl task. However, their report does not check consistency of this improvement using other evaluation metrics and other data sets. 11 Conclusion We have described a new"
2007.mtsummit-papers.11,W07-0403,0,0.0152676,"aim was to study this approach for the broadest possible class of models, we chose one of the most widely used SMT models as the baseline, namely flat phrasebased SMT. In light of the encouraging results, dynamic context-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E,"
2007.mtsummit-papers.11,W04-0802,0,0.162256,"exicons for SMT. We use context features and WSD models that were designed and evaluated on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002) and Senseval-3 tasks (Carpuat et al., 2004). On the one hand, these tasks included monolingual lexical choice tasks, where word senses are defined according to some manually built ontology or semantic network such as WordNet, HowNet, or the like. More relevant to the task at hand, however, are also the multilingual lexical choice tasks, where word senses are directly defined as the semantic distinctions made by another language (e.g., Chklovski et al. (2004)). In many ways, the multilingual lexical choice tasks of Senseval/SemEval embody a more empirically justifiable approach to defining the sense inventory for WSD than the monolingual ontology-based lexical choice tasks. Sense inventories constructed on the basis of manually-built ontologies inherit an enormous variety of arbitrary choices made by the ontology builder, that can damage prediction and generalization accuracy. Since ontologies are not directly observable, as the saying goes, there are as many different ontologies as there are ontology builders. Unlike manually built ontologies, on"
2007.mtsummit-papers.11,P01-1027,0,0.0231678,"quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures and full phrasal translation lexicons. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based contextdependent lexical choice model, but not improved translation accuracy. Our evaluation in this paper is conducted on the decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used. Another problem in the context-dependent SMT models of Garcia Varea et al. is that their feature set is insufficiently rich to make much better predictions than the SMT model itself. In contrast, our dynamic con"
2007.mtsummit-papers.11,garcia-varea-etal-2002-efficient,0,0.0236878,"task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures and full phrasal translation lexicons. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based contextdependent lexical choice model, but not improved translation accuracy. Our evaluation in this paper is conducted on the decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used. Another problem in the context-dependent SMT models of Garcia Varea et al. is that their feature set is insufficiently rich to make much better predictions than the SMT model itself. In contrast, our dynamic context-dependent phrasal lexicons"
2007.mtsummit-papers.11,koen-2004-pharaoh,0,0.0337202,"re corpora, and therefore of a much wider domain than the IWSLT data set. The training set consists of about 1 million sentence pairs in the news domain. Basic preprocessing was applied to the corpus. The English side was simply tokenized and case-normalized. The Chinese side was word segmented using the LDC segmenter. 4.2 A standard baseline SMT system Our aim is to lay out an approach that can be expected to work in any reasonably common phrase-based SMT implementation. Since our focus is not on a specific SMT architecture, we chose the widely-used off-the-shelf phrasebased decoder Pharaoh (Koehn, 2004). Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used. Table 3: Translation examples with and without the WSDbased dynamic context-dependent phrasal translation lexicon, drawn from IWSLT data sets. Input ÷ l X -. ¿ Ref. Please transfer to the Central train line . SMT Please turn to the Central Line . +WSD Please transfer to Central Line . Input fh ( f p Ref. Do I pay on the bus ? SMT Please get on the bus ? +WSD I buy a ticket on the bus ? Input • „¢ Ref. Do I need a reservation ? SMT I need a reser"
2007.mtsummit-papers.11,E06-1031,0,0.0159085,"n We have conducted a comprehensive evaluation on two standard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD models independently, as we cannot safely assume that higher WSD evaluation scores necessarily lead to higher translation accuracies. 4.1 Two very different tasks with 4 test sets One set of experiments was conducted using training and evaluation data drawn from the multilingual BTEC corpus, which contains sentences used in conversations in the travel dom"
2007.mtsummit-papers.11,J03-1002,0,0.00340523,"ation lexicons (e.g. “turn” vs. “transfer” in the first example, “get” vs. “buy” in the second example, “open” vs. “have” in the last example). Across all the IWSLT test sets, an average of 19 features per occurrence of a Chinese phrase are observed and used to build the dynamic context-dependent lexicon. This confirms that the rich WSD-style context features are indeed used for SMT translation lexicons, even in the single domain IWSLT corpus where sentences are quite short. 7 The phrase bilexicon was derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall. The language model was trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights were learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline and for the WSD-augmented system. In the remaining sections, we discuss a number of different analyses of the experimental results. 5 Context-dependent modeling consistently improves translation The most obvious observation on the experimental results, as shown in Table 1 for the IWSLT task and Table"
2007.mtsummit-papers.11,P03-1021,0,0.0328371,"ntext-dependent lexicon. This confirms that the rich WSD-style context features are indeed used for SMT translation lexicons, even in the single domain IWSLT corpus where sentences are quite short. 7 The phrase bilexicon was derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall. The language model was trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights were learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline and for the WSD-augmented system. In the remaining sections, we discuss a number of different analyses of the experimental results. 5 Context-dependent modeling consistently improves translation The most obvious observation on the experimental results, as shown in Table 1 for the IWSLT task and Table 2 for the NIST task, is that making the phrasal translation lexicons context-dependent produces higher translation quality on all test sets, as measured by all eight commonly used automated evaluation metrics. Paired bootstrap resampling shows that the improvements on the m"
2007.mtsummit-papers.11,P02-1040,0,0.0846811,"n. Despite these differences, the WSD model supporting our context-dependent phrasal translation lexicon is still a pure WSD model. Just as in any Senseval/SemEval multilingual lexical sample task (e.g., Chklovski et al. (2004)), the task consists of disambiguating between semantic distinctions made by another language. 4 Evaluation on full-scale translation We have conducted a comprehensive evaluation on two standard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD"
2007.mtsummit-papers.11,2006.amta-papers.25,0,0.0279322,"tandard Chinese to English translation tasks, using all eight of the most commonly employed automated evaluation metrics. For every task, we evaluate translation quality with both BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores along with the recently proposed METEOR (Banerjee and Lavie, 2005) with and without WordNet synonyms. In addition, we report for each task four edit-distance style metrics: Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Since our goal is to evaluate actual translation quality, we restrict ourselves to standard MT evaluation methodology. We do not evaluate the disambiguation accuracy of the embedded WSD models independently, as we cannot safely assume that higher WSD evaluation scores necessarily lead to higher translation accuracies. 4.1 Two very different tasks with 4 test sets One set of experiments was conducted using training and evaluation data drawn from the multilingual BTEC corpus, which contains sentences used in conversations in the travel domain, and their translations in several languages. A sub"
2007.mtsummit-papers.11,2005.mtsummit-papers.33,0,0.536754,"ion, which has taken place largely independently of the SMT community, has been directly targeted at the question of how to design context features and combine a wide range of contextual evidence into making a translation or sense prediction. Evaluation of WSD models is typically done on WSD accuracy only—it is implicitly assumed that better WSD models will help higher level applications such as SMT. Recently, several researchers have focused on designing WSD systems that use rich contextual information for the specific purpose of translation, instead of any sense distinctions. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English translation and allows for rich expressive context features, but this system was also only evaluated on WSD accuracy, and not integrated in a full-scale machine translation system. 2.2 Context-dependent SMT To the best of our knowledge, our model represents"
2007.mtsummit-papers.11,P98-2230,1,0.786317,"xt-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Resear"
2007.mtsummit-papers.11,P96-1021,1,0.645726,"dependent probabilities. Since our aim was to study this approach for the broadest possible class of models, we chose one of the most widely used SMT models as the baseline, namely flat phrasebased SMT. In light of the encouraging results, dynamic context-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Gra"
2007.mtsummit-papers.11,J97-3002,1,0.476098,"D scores at all stages of decoding, since the contextdependent WSD scores are defined for every phrase in the bilexicon, just like regular context-independent probabilities. Since our aim was to study this approach for the broadest possible class of models, we chose one of the most widely used SMT models as the baseline, namely flat phrasebased SMT. In light of the encouraging results, dynamic context-dependent phrasal translation lexicons might also be integrated into other current SMT models such as treestructured SMT models employing various kinds of stochastic transduction grammars (e.g., Wu (1997), Wu and Chiang (2007)). For example, the context-dependent predictions might be utilized by a Bracketing ITG based decoder such as that of Wu (1996), Zens et al. (2004), or Cherry and Lin (2007), or alternatively a more grammatically structured statistical MT model that is less reliant on n-gram language modeling, such as the syntactic ITG based “grammatical channel” translation model of (Wu and Wong, 1998). The question remains open as to which type of SMT model could make most effective use of contextdependent phrasal translation lexicons. Acknowledgements This material is based upon work s"
2007.mtsummit-papers.11,C04-1030,0,0.0382302,"Missing"
2007.mtsummit-papers.11,H05-1097,0,\N,Missing
2007.mtsummit-papers.11,C98-2225,1,\N,Missing
2007.tmi-papers.6,P07-1005,0,0.225601,"all improvements were obtained on a slightly weaker SMT baseline, and subsequent evaluations showed that these gains are not consistent across metrics. Gim´enez and M`arquez (2007) also used WSD predictions in Pharaoh for the slightly more general case of very frequent phrases, which in practice essentially limits the set of WSD targets to single words or very short phrases. However, evaluation on the single Europarl Spanish-English task did not yield consistent improvements across metrics: BLEU score did not improve, while there were small improvements in the QUEEN, METEOR and ROUGE metrics. Chan et al. (2007) report an improved BLEU score for a hierarchical phrase-based SMT system on a NIST Chinese-English task, by incorporating WSD predictions only for single words and short phrases of length 1 or 2. However, no results for metrics other than BLEU were reported, and no results on other tasks, so the relia45 bility of this model is not known. What the foregoing attempts at WSD in SMT share is that (1) they focus on single words rather than full phrases, and (2) the evaluations do not show consistent improvement systematically across different tasks and metrics. In contrast, we showed in Carpuat an"
2007.tmi-papers.6,P05-1033,0,0.0629702,"do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improving disambiguation accuracy within SMT architectu"
2007.tmi-papers.6,W04-0802,0,0.0326319,"parallel corpora used to learn the SMT lexicon. Given a Chinese-English sentence pair, a WSD or PSD target in the Chinese sentence is annotated with the English phrase which is consistent with the word alignment. The definition of consistency with the word alignment should be exactly the one used for building the SMT lexicon. Despite the differences introduced by the use of phrasal targets, the disambiguation task remains in the character and spirit of WSD. The translation lexical choice problem is exactly the same task as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. In our SMT-driven approach to PSD rather than WSD, we are only generalizing the definition of the sense disambiguation targets, and automating the sense annotation process. 3.2 Leveraging Senseval classifiers for both WSD and PSD As in Carpuat and Wu (2007), the word sense disambiguation system is modeled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al., 2004). The features employed include position-sensitive, syntactic, and local collocational features,"
2007.tmi-papers.6,P01-1027,0,0.0805489,"to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single words vs. multi-word phrases, and they were not evaluated in terms of translation quality. For instance, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical choice model, but do not report improved translation accuracy. Another problem in the context-sensitive SMT models of Garcia Varea et al. is that they strictly reside within the Bayesian source-channel model, which is word-based. The few recent attempts at integrating single word based WSD models into SMT have failed to obtain clear improvements in terms of translation quality. Carpuat and Wu (2005) show that using word-based Senseval trained models does not help BLEU score"
2007.tmi-papers.6,garcia-varea-etal-2002-efficient,0,0.124056,"s system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single words vs. multi-word phrases, and they were not evaluated in terms of translation quality. For instance, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical choice model, but do not report improved translation accuracy. Another problem in the context-sensitive SMT models of Garcia Varea et al. is that they strictly reside within the Bayesian source-channel model, which is word-based. The few recent attempts at integrating single word based WSD models into SMT have failed to obtain clear improvements in terms of translation quality. Carpuat and Wu (2005) show that using word-based Senseval trained models does not help BLEU score when integrated in a standard"
2007.tmi-papers.6,W07-0719,0,0.0250273,"Missing"
2007.tmi-papers.6,S01-1004,0,0.0142708,"ine translation (SMT) models have met with mixed or disappointing results (e.g., Carpuat and Wu (2005), Cabezas and Resnik (2005)), suggesting that a deeper empirical exploration of the differences and consequences of the assumptions of WSD and SMT is called for. On one hand, word sense disambiguation as a standalone task consists in identifying the correct sense of a given word among a set of predefined sense candidates. In the Senseval series of evaluations, WSD targets are typically single words, both in the lexical sample tasks, where only a predefined set of targets are considered (e.g., Kilgarriff (2001); ), and in the all-words tasks, where all content word in a given corpus must be disambiguated (e.g., Kilgarriff and Rosenzweig (1999)). This focus on single words as WSD targets might be explained by the sense inventory, which is usually derived from a manually constructed dictionary or ontology, where most entries are single words. In addition, historically, as for many other tasks, work on European languages imposed whitespace as an easy way to define convenient the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Gra"
2007.tmi-papers.6,koen-2004-pharaoh,0,0.400558,"he authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improving disambiguation accuracy within"
2007.tmi-papers.6,E06-1031,0,0.0275463,"Missing"
2007.tmi-papers.6,S07-1010,0,0.0116643,"SD model on data extracted from automatically word aligned parallel corpora, and evaluate it on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia et al. (2007) use an inductive logic programming based WSD system to integrate expressive features for Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single words vs. multi-word phrases, and they were not evaluated in terms of translation quality. For instance, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical choice model, but do not report improved translation accuracy. Another problem in the context-sensitive SMT models of Garcia Varea et al. is that they strictly reside within the B"
2007.tmi-papers.6,J04-4002,0,0.0121231,"erial are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improving disambiguation a"
2007.tmi-papers.6,W05-0909,0,0.0425738,"gnificant improvements on the large NIST task, while in contrast, the impact of single-word WSD on translation quality is highly unpredictable. In particular, the single-word WSD results are inconsistent across different test sets, and depend on which evaluation metric is chosen. In order to measure the impact of WSD on translation quality, the translation results were evaluated using all eight of the most commonly used automatic evaluation metrics. In addition to the widely used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with METEOR (Banerjee and Lavie, 2005), Word Error Rate (WER), Position-independent word Error Rate (PER) (Tillmann et al., 1997), CDER (Leusch 48 et al., 2006), and Translation Edit Rate (TER) (Snover et al., 2006). Note that we report METEOR scores computed both with and without using WordNet synonyms to match translation candidates and references, showing that the improvement is not due to context-independent synonym matches at evaluation time. In the sections that follow, we investigate various reasons that PSD outperforms WSD, drawing from data analysis on these comparative experiments. 7 Single-word WSD yields unreliable res"
2007.tmi-papers.6,P02-1040,0,0.104047,"Missing"
2007.tmi-papers.6,P91-1034,0,0.268585,"tegrating a generalized sense disambiguation method into SMT, such that phrasal lexical choice is dynamically influenced by context-dependent probabilities or scores. This Phrase Sense Disambiguation— as opposed to Word Sense Disambiguation— approach appears to be the only model to date that has been shown capable of consistently yielding improvements on translation quality across all 44 different test sets and automatic evaluation metrics. Other related work has all been heavily oriented toward disambiguating single words. In perhaps the earliest study of WSD potential for SMT performance by Brown et al. (1991), the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to single words with exactly two translation candidates, and it is far from clear that the conclusions could generalize to more recent SMT architectures. In contrast with Brown et al.’s work, our approach incorporates the predictions of state-of-the-art WSD models (generalized to PSD models) that use rich contextual features for any phrase i"
2007.tmi-papers.6,P07-1006,0,0.0123474,"T architectures. In contrast with Brown et al.’s work, our approach incorporates the predictions of state-of-the-art WSD models (generalized to PSD models) that use rich contextual features for any phrase in the input vocabulary. More recent work on WSD systems designed for the specific purpose of translation has followed the traditional word-based definition of the WSD task. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, and evaluate it on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia et al. (2007) use an inductive logic programming based WSD system to integrate expressive features for Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via English-Chinese Parallel Text (Ng and Chan, 2007), WSD is still defined as a word-based task. There have been other attempts at using context information for lexical selection in SMT, but the focus was also on single word"
2007.tmi-papers.6,D07-1007,1,0.914733,"the contrastive experiments reported here show that incorporating single-word WSD into phrasal SMT leads to unpredictable and inconsistent effects on translation quality, depending on which evaluation metric one looks at. We then turn to data analysis exploring more closely how and why the multi-word PSD approach outperforms the single-word WSD approach. The analysis shows that dynamic integration of PSD prediction is crucial to this improvement, as it allows all PSD predictions to participate in the segmention of the input sentence that yields the best translation quality. 2 Previous work In Carpuat and Wu (2007), we proposed a novel general framework for integrating a generalized sense disambiguation method into SMT, such that phrasal lexical choice is dynamically influenced by context-dependent probabilities or scores. This Phrase Sense Disambiguation— as opposed to Word Sense Disambiguation— approach appears to be the only model to date that has been shown capable of consistently yielding improvements on translation quality across all 44 different test sets and automatic evaluation metrics. Other related work has all been heavily oriented toward disambiguating single words. In perhaps the earliest"
2007.tmi-papers.6,W04-0822,1,0.843534,"actly the same task as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. In our SMT-driven approach to PSD rather than WSD, we are only generalizing the definition of the sense disambiguation targets, and automating the sense annotation process. 3.2 Leveraging Senseval classifiers for both WSD and PSD As in Carpuat and Wu (2007), the word sense disambiguation system is modeled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al., 2004). The features employed include position-sensitive, syntactic, and local collocational features, and are therefore much richer than those used in most SMT systems. 4 Integrating multi-word PSD vs. single-word WSD into phrasal SMT architectures Unlike single-word WSD, it is non-trivial to incorporate the PSD predictions into an existing phrase-based architecture such as Pharaoh (Koehn, 2004), since the decoder is not set up to easily accept multiple translation probabilities that are dynamically computed in context46 sensitive fashion. While PSD and WSD models differ in principle only by the le"
2007.tmi-papers.6,H05-1097,0,0.0369903,"which is reliably discriminative. However, this was a pilot study, which is limited to single words with exactly two translation candidates, and it is far from clear that the conclusions could generalize to more recent SMT architectures. In contrast with Brown et al.’s work, our approach incorporates the predictions of state-of-the-art WSD models (generalized to PSD models) that use rich contextual features for any phrase in the input vocabulary. More recent work on WSD systems designed for the specific purpose of translation has followed the traditional word-based definition of the WSD task. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, and evaluate it on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia et al. (2007) use an inductive logic programming based WSD system to integrate expressive features for Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a fullscale machine translation system. Even when using automatically-aligned SMT parallel corpora to define WSD tasks, as in the SemEval-2007 English Lexical Sample Task via E"
2007.tmi-papers.6,J97-3002,1,0.162575,"in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 43 word boundaries. Linguistically, however, this oversimplistic modeling approach seems rather questionable, and recalls long-held debates over the issue of what properly constitutes a “word”. In contrast, work in statistical machine translation has for some time recognized the need to segment sentences as required by the task’s evaluation criteria, and today most systems use phrases or segments, and not single words, as the basic unit for lexical choice (e.g., Wu (1997); Och and Ney (2004); Koehn (2004); Chiang (2005)). Note that single-word based SMT architectures already perform a significant amount of sense disambiguation intrinsically, by virtue of combining a priori sense candidate likelihoods (from adequacy criteria as modeled by lexical translation probabilities) with contextual coherence preferences (from fluency criteria as modeled by language model probabilities). Phrasal SMT architectures, furthermore, integrate lexical collocation preferences into the disambiguation choices, raising the bar yet higher. This suggests that to be effective at improv"
2007.tmi-papers.6,2006.iwslt-evaluation.5,1,\N,Missing
2010.jeptalnrecital-long.30,J04-4004,0,0.047035,"Missing"
2010.jeptalnrecital-long.30,P08-1009,0,0.250799,"Missing"
2010.jeptalnrecital-long.30,P05-1066,0,0.234944,"Missing"
2010.jeptalnrecital-long.30,W08-0307,1,0.881445,"Missing"
2010.jeptalnrecital-long.30,2009.mtsummit-caasl.4,0,0.320418,"Missing"
2010.jeptalnrecital-long.30,2007.mtsummit-papers.29,1,0.925361,"Missing"
2010.jeptalnrecital-long.30,P05-1071,1,0.858081,"Missing"
2010.jeptalnrecital-long.30,P09-2056,1,0.892285,"Missing"
2010.jeptalnrecital-long.30,N06-2013,1,0.890456,"Missing"
2010.jeptalnrecital-long.30,D09-1024,0,0.128164,"Missing"
2010.jeptalnrecital-long.30,D07-1103,0,0.0946636,"Missing"
2010.jeptalnrecital-long.30,W04-3250,0,0.0389722,"Missing"
2010.jeptalnrecital-long.30,P07-2045,0,0.0192157,"Missing"
2010.jeptalnrecital-long.30,2006.amta-papers.11,0,0.104292,"Missing"
2010.jeptalnrecital-long.30,W10-1402,1,0.885541,"Missing"
2010.jeptalnrecital-long.30,P08-1114,1,0.90466,"Missing"
2010.jeptalnrecital-long.30,W03-3017,0,0.133964,"Missing"
2010.jeptalnrecital-long.30,nivre-etal-2006-maltparser,0,0.0427623,"Missing"
2010.jeptalnrecital-long.30,J03-1002,0,0.00818449,"Missing"
2010.jeptalnrecital-long.30,P02-1040,0,0.0795527,"Missing"
2010.jeptalnrecital-long.30,2006.amta-papers.25,0,0.0336817,"Missing"
2010.jeptalnrecital-long.30,C04-1073,0,0.282836,"Missing"
2010.jeptalnrecital-long.30,W07-0401,0,0.276404,"Missing"
2010.jeptalnrecital-long.30,P06-1073,0,0.0472667,"Missing"
2012.amta-papers.7,W05-0909,0,0.0140376,"kin et al., 2010). We use the following feature functions The parameters of the log-linear model are tuned by optimizing BLEU on the development set using MIRA (Chiang et al., 2008).4 Phrase extraction is done by aligning the corpus at the word level using both HMM and IBM2 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The translation performance was measured using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). 4 Results We show how SMT output degrades with increasing alignment noise. We see that, surprisingly, even relatively high levels of noise have little impact on translation performance. We then compare the robustness of PBMT systems to that of Translation Memories, a commom computer-aided translation tool. 4.1 Impact on translation performance Figure 3 shows how translation performance, as estimated by BLEU (circles), degrades when the number of misaligned sentence pairs increases. Not surprisingly, increasing the noise level produces a general decrease in performance. Although there are var"
2012.amta-papers.7,D08-1024,0,0.0199038,"Missing"
2012.amta-papers.7,W10-1717,1,0.881245,"Missing"
2012.amta-papers.7,moore-2002-fast,0,0.356444,"Missing"
2012.amta-papers.7,P02-1040,0,0.0867124,"ecent NIST and WMT evaluations (Larkin et al., 2010). We use the following feature functions The parameters of the log-linear model are tuned by optimizing BLEU on the development set using MIRA (Chiang et al., 2008).4 Phrase extraction is done by aligning the corpus at the word level using both HMM and IBM2 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The translation performance was measured using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). 4 Results We show how SMT output degrades with increasing alignment noise. We see that, surprisingly, even relatively high levels of noise have little impact on translation performance. We then compare the robustness of PBMT systems to that of Translation Memories, a commom computer-aided translation tool. 4.1 Impact on translation performance Figure 3 shows how translation performance, as estimated by BLEU (circles), degrades when the number of misaligned sentence pairs increases. Not surprisingly, increasing the noise level produces a general decrease"
2020.emnlp-main.121,D18-2029,0,0.0471134,"Missing"
2020.emnlp-main.121,W19-5435,0,0.0355821,"Missing"
2020.emnlp-main.121,2020.acl-main.747,0,0.120101,"Missing"
2020.emnlp-main.121,D17-1070,0,0.0781977,"Missing"
2020.emnlp-main.121,C04-1151,0,0.0565514,"sing, pages 1563–1580, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 Background Following Vyas et al. (2018), we use the term cross-lingual semantic divergences to refer to differences in meaning between sentences written in two languages. Semantic divergences differ from typological divergences that reflect different ways of encoding the same information across languages (Dorr, 1994). In sentence pairs drawn from comparable documents—written independently in each language but sharing a topic—sentences that contain translated fragments are rarely exactly equivalent (Fung and Cheung, 2004; Munteanu and Marcu, 2005), and sentence alignment errors yield coarse mismatches in meaning (Goutte et al., 2012). In translated sentence pairs, differences in discourse structure across languages (Li et al., 2014) can lead to sentence-level divergences or discrepancies in translation of pronouns (Lapshinovaˇ stari´c et al., Koltunski and Hardmeier, 2017; Soˇ 2018); translation lexical choice requires selecting between near synonyms that introduce languagespecific nuances (Hirst, 1995); typological divergences lead to structural mismatches (Dorr, 1994), and non-literal translation processes"
2020.emnlp-main.121,W19-5407,0,0.0251769,"on task, the final hidden state ht of each yt token is passed through a feedforward layer and a softmax layer to produce the probability Pyt of the yt token belonging to the EQ class. For the sentence task, the model learns to rank x > y, as in Section 3.3. We then minimize the sum of the sentence-level margin-loss and the average token-level cross-entropy loss (LCE ) across all tokens of y, as defined in Equation 2. L= 1  |D0 | X max{0, ξ − F (x) + F (y)} (x,y,z)∈D 0 |y|  1 X LCE (Pyt , zt ) + |y |t=1 (2) Similar multi-task models have been used for Machine Translation Quality Estimation (Kim et al., 2019a,b), albeit with human-annotated training samples and a standard cross-entropy loss for both word-level and sentence-level sub-tasks. 4 Rationalized English-French Semantic Divergences We introduce the Rationalized English-French Semantic Divergences (REFRESD) dataset, which consists of 1,039 English-French sentence-pairs annotated with sentence-level divergence judgments and token-level rationales. Figure 2 shows an example drawn from our corpus. Our annotation protocol is designed to encourage annotators’ sensitivity to semantic divergences other than misalignments, without requiring expert"
2020.emnlp-main.121,W19-5404,0,0.0350372,"Missing"
2020.emnlp-main.121,W17-4810,0,0.0606536,"Missing"
2020.emnlp-main.121,C14-1055,1,0.813147,"g between sentences written in two languages. Semantic divergences differ from typological divergences that reflect different ways of encoding the same information across languages (Dorr, 1994). In sentence pairs drawn from comparable documents—written independently in each language but sharing a topic—sentences that contain translated fragments are rarely exactly equivalent (Fung and Cheung, 2004; Munteanu and Marcu, 2005), and sentence alignment errors yield coarse mismatches in meaning (Goutte et al., 2012). In translated sentence pairs, differences in discourse structure across languages (Li et al., 2014) can lead to sentence-level divergences or discrepancies in translation of pronouns (Lapshinovaˇ stari´c et al., Koltunski and Hardmeier, 2017; Soˇ 2018); translation lexical choice requires selecting between near synonyms that introduce languagespecific nuances (Hirst, 1995); typological divergences lead to structural mismatches (Dorr, 1994), and non-literal translation processes can lead to semantic drifts (Zhai et al., 2018). Despite this broad spectrum of phenomena, recent work has effectively focused on coarse-grained divergences: Vyas et al. (2018) work on subtitles and Common Crawl corp"
2020.emnlp-main.121,P19-1475,0,0.0525599,"Missing"
2020.emnlp-main.121,W18-6482,0,0.0136436,"hanges introduced by human translators during the translation process (Molina and Hurtado Albir, 2002). However, this expensive annotation process does not scale easily. When processing bilingual corpora, any meaning mismatches between the two languages are primarily viewed as noise for the downstream task. In shared tasks for filtering web-crawled parallel corpora (Koehn et al., 2018, 2019), the best performing systems rely on translation models, or cross-lingual sentence embeddings to place bilingual sentences on a clean to noisy scale (JunczysDowmunt, 2018; S´anchez-Cartagena et al., 2018; Lu et al., 2018; Chaudhary et al., 2019). When mining parallel segments in Wikipedia for the WikiMatrix corpus (Schwenk et al., 2019), examples are ranked using the LASER score (Artetxe and Schwenk, 2019), which computes cross-lingual similarity in a language-agnostic sentence embedding space. While this approach yields a very useful corpus of 135M parallel sentences in 1,620 language pairs, we show that LASER fails to detect many semantic divergences in WikiMatrix. 3 Unsupervised Divergence Detection We introduce a model based on multilingual BERT (mBERT) to distinguish divergent from equivalent sentence-pa"
2020.emnlp-main.121,E12-2021,0,0.0721957,"Missing"
2020.emnlp-main.121,N18-1136,1,0.871089,"Missing"
2020.emnlp-main.121,D18-1328,0,0.110606,"epancies in translation of pronouns (Lapshinovaˇ stari´c et al., Koltunski and Hardmeier, 2017; Soˇ 2018); translation lexical choice requires selecting between near synonyms that introduce languagespecific nuances (Hirst, 1995); typological divergences lead to structural mismatches (Dorr, 1994), and non-literal translation processes can lead to semantic drifts (Zhai et al., 2018). Despite this broad spectrum of phenomena, recent work has effectively focused on coarse-grained divergences: Vyas et al. (2018) work on subtitles and Common Crawl corpora where sentence alignment errors abound, and Pham et al. (2018) focus on fixing divergences where content is appended to one side of a translation pair. By contrast, Zhai et al. (2018, 2019) introduce token-level annotations that capture the meaning changes introduced by human translators during the translation process (Molina and Hurtado Albir, 2002). However, this expensive annotation process does not scale easily. When processing bilingual corpora, any meaning mismatches between the two languages are primarily viewed as noise for the downstream task. In shared tasks for filtering web-crawled parallel corpora (Koehn et al., 2018, 2019), the best perform"
2020.emnlp-main.121,D19-1059,0,0.0242018,"Missing"
2020.emnlp-main.121,D19-1410,0,0.0440213,"Missing"
2020.emnlp-main.121,W18-6488,0,0.0467953,"Missing"
2020.emnlp-main.121,N07-1033,0,0.075275,"ataset, which consists of 1,039 English-French sentence-pairs annotated with sentence-level divergence judgments and token-level rationales. Figure 2 shows an example drawn from our corpus. Our annotation protocol is designed to encourage annotators’ sensitivity to semantic divergences other than misalignments, without requiring expert knowledge beyond competence in the languages of interest. We use two strategies for this purpose: (1) we explicitly introduce distinct divergence categories for unrelated sentences and sentences that overlap in meaning; and (2) we ask for annotation rationales (Zaidan et al., 2007) by requiring annotators to highlight tokens indicative of meaning differences in each sentence-pair. Thus, our approach strikes a balance between coarsely annotating sentences with binary distinctions that are fully based on annotators’ intuitions (Vyas et al., 2018), and exhaustively annotating all spans of a sentence-pair with fine-grained labels of translation processes (Zhai et al., 2018). We describe the annotation process and analysis of the collected instances based on data statements protocols described in Bender and Friedman (2018); Gebru et al. (2018). We include more information in"
2020.emnlp-main.121,W18-3814,0,0.0563126,"and sentence alignment errors yield coarse mismatches in meaning (Goutte et al., 2012). In translated sentence pairs, differences in discourse structure across languages (Li et al., 2014) can lead to sentence-level divergences or discrepancies in translation of pronouns (Lapshinovaˇ stari´c et al., Koltunski and Hardmeier, 2017; Soˇ 2018); translation lexical choice requires selecting between near synonyms that introduce languagespecific nuances (Hirst, 1995); typological divergences lead to structural mismatches (Dorr, 1994), and non-literal translation processes can lead to semantic drifts (Zhai et al., 2018). Despite this broad spectrum of phenomena, recent work has effectively focused on coarse-grained divergences: Vyas et al. (2018) work on subtitles and Common Crawl corpora where sentence alignment errors abound, and Pham et al. (2018) focus on fixing divergences where content is appended to one side of a translation pair. By contrast, Zhai et al. (2018, 2019) introduce token-level annotations that capture the meaning changes introduced by human translators during the translation process (Molina and Hurtado Albir, 2002). However, this expensive annotation process does not scale easily. When pr"
2020.emnlp-main.121,P19-1328,0,0.0285462,"entences in the same language. Phrase Replacement Following Pham et al. (2018), we introduce divergences that mimic phrasal edits or mistranslations by substituting random source or target sequences by another sequence of words with matching POS tags (to keep generated sentences as grammatical as possible). Lexical Substitution We mimic particularization and generalization translation operations (Zhai et al., 2019) by substituting English words with hypernyms or hyponyms from WordNet. The replacement word is the highest scoring WordNet candidate in context, according to a BERT language model (Zhou et al., 2019; Qiang et al., 2019). We call all these divergent examples contrastive because each divergent example contrasts with a specific equivalent sample from the seed set. The three sets of transformation rules above create divergences of varying granularity and create an implicit ranking over divergent examples based on the range of edit operations, starting from a single token with lexical substitution, to local short phrases for phrase replacement, and up to half the words in a sentence when deleting subtrees. 3.3 Learning to Rank Contrastive Samples We train the Divergent mBERT model by learning"
2020.figlang-1.26,C18-1214,0,0.0140761,"d no additional processing. Since the TOEFL essays are written by nonnative English speakers, many sentences contain misspellings or grammatical errors, such as “The problems of the pollution is one of the most ones of this century.” We experiment two strategies to address these sources of variability. Spelling Correction We created a cleaned version of the dataset using the Python pyspellchecker library, which finds a given word’s minimum Levenshtein distance neighbor in the OpenSubtitles corpus. In total, we replaced 1536 (train) and 492 (test) misspelled tokens in the data. Error Injection Anastasopoulos et al. (2018) showed that adding synthetic grammatical errors to training data improves neural machine translation of non-native English to Spanish text. To investigate the effect of such methods on metaphor detection, we separately inject the following errors (if applicable) into three copies of each training sentence and append them to the training set: • RT: Missing determiner (includes articles) Evaluation Settings When training the logistic regression and Bi-LSTM classifiers, we ran cross-validation (k = 5) and used early stopping to select a final test model based on validation loss. We then selected"
2020.figlang-1.26,N18-2014,0,0.126506,"a wider range of results for the T OEFL A LL P OS task. Context determines whether a word or phrase is being used in a metaphorical sense. Consider an example from the TOEFL dataset: “The world is a huge stage and nearly everybody is an actor.” The words “stage” and “actor” are used metaphorically to analogize the world to a stage and individuals to actors on that stage. A literal usage of these two words would be “The actor walked across the stage.”, because “actor” and “stage” both occur within the context of a theatrical performance, which also matches the context of the sentence. Beigman Klebanov et al. (2018) establish baselines for metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Ste"
2020.figlang-1.26,D18-1060,0,0.789573,"is an actor.” The words “stage” and “actor” are used metaphorically to analogize the world to a stage and individuals to actors on that stage. A literal usage of these two words would be “The actor walked across the stage.”, because “actor” and “stage” both occur within the context of a theatrical performance, which also matches the context of the sentence. Beigman Klebanov et al. (2018) establish baselines for metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Stemle and Onysko, 2018; Mykowiecka et al., 2018), and showed that contextualized word representations from Bi-LSTM can be more effective. In this work, we investigate whether Gao et al. (2018)’s findings can be replicated when d"
2020.figlang-1.26,W13-0907,0,0.0705994,"n the context of a theatrical performance, which also matches the context of the sentence. Beigman Klebanov et al. (2018) establish baselines for metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Stemle and Onysko, 2018; Mykowiecka et al., 2018), and showed that contextualized word representations from Bi-LSTM can be more effective. In this work, we investigate whether Gao et al. (2018)’s findings can be replicated when detecting metaphors in TOEFL essays rather than the BNC. In addition, we attempt to answer the following question: do contextualized word representations from a Bi-LSTM model detect metaphorical word use more accurately than feature-rich linear models? On the one hand, B"
2020.figlang-1.26,W15-1402,0,0.388041,"ccurately predict whether words are used in a literal or metaphorical sense in a sequence labeling setting. As shown in Table 1, the literal tokens heavily outnumber the metaphorical ones. To account for this imbalance, submissions are evaluated using the F1 score for the positive class (metaphorical). In the table, a “token” refers to a labeled word in the data (not all words are assigned labels/features). We will refer the reader to the shared task description paper for a detailed description of the task. In addition to metaphor annotations, the corpus comes with pre-extracted features from Klebanov et al. (2015), labeled as Provided features in Table 2. These features include unigrams, Stanford POS tags, binned mean concreteness values (Brysbaert et al., 2013), and Topic-Latent Dirichlet Allocation (Blei et al., 2003). Unlabeled tokens are assigned a literal classification and values of zero for all non-word embedding features. 3 Table 2: Features available for use. (LBFGS solver with L2 penalization). We predict a binary classification for each token independently, ignoring other predictions and features in the sequence. System Configurations 3.1 Classifiers We ran our internal experiments using a s"
2020.figlang-1.26,W18-0916,0,0.164513,"or metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Stemle and Onysko, 2018; Mykowiecka et al., 2018), and showed that contextualized word representations from Bi-LSTM can be more effective. In this work, we investigate whether Gao et al. (2018)’s findings can be replicated when detecting metaphors in TOEFL essays rather than the BNC. In addition, we attempt to answer the following question: do contextualized word representations from a Bi-LSTM model detect metaphorical word use more accurately than feature-rich linear models? On the one hand, Bi-LSTM sequence labelers have proven quite successful at learning task-specific representations for many NLP problems. On the other hand, text written"
2020.figlang-1.26,D14-1162,0,0.0894087,"put through a single-layer (2 units) feedforward neural network and apply a softmax function, which outputs a probability distribution for the two output classes. Dropout is applied to the LSTM input (p = 0.5) and from LSTM output to the linear layer (p = 0.1). The models are trained using the Adam algorithm, with learning rates of η = 0.005 and 0.001 for epochs 0 − 10 and 11 − 20, respectively. 3.2 Features We experimented with different input features within each model architecture, which are summarized in Table 2. We obtain word embedding features for each word type by concatenating GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) word embeddings into a 1324-dimensional vector, shown as “GE” in Table 2. All the other features were provided with the 193 Model BL LR LSTM Features Used UL UL, P, WN, T, C, CD GE GE, UL GE, UL, WN, CD UL, P, WN, T, C, CD GE GE, UL GE, UL, WN, CD P 64.3 55.2 58.4 58.7 55.7 61.0 50.6 69.3 73.3 73.8 R 52.6 51.9 54.1 63.0 60.6 61.7 30.5 65.0 61.9 60.4 F1 57.7 53.3 55.7 60.5 57.9 60.7 38.0 66.8 67.1 66.3 Test F1 54.5 52.4 50.5 56.5 58.2 60.9 - Table 3: Summary of results based on 5-fold cross validation on the unmodified training set (P,R,F1) as well as evaluation"
2020.figlang-1.26,N18-1202,0,0.0136818,") feedforward neural network and apply a softmax function, which outputs a probability distribution for the two output classes. Dropout is applied to the LSTM input (p = 0.5) and from LSTM output to the linear layer (p = 0.1). The models are trained using the Adam algorithm, with learning rates of η = 0.005 and 0.001 for epochs 0 − 10 and 11 − 20, respectively. 3.2 Features We experimented with different input features within each model architecture, which are summarized in Table 2. We obtain word embedding features for each word type by concatenating GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) word embeddings into a 1324-dimensional vector, shown as “GE” in Table 2. All the other features were provided with the 193 Model BL LR LSTM Features Used UL UL, P, WN, T, C, CD GE GE, UL GE, UL, WN, CD UL, P, WN, T, C, CD GE GE, UL GE, UL, WN, CD P 64.3 55.2 58.4 58.7 55.7 61.0 50.6 69.3 73.3 73.8 R 52.6 51.9 54.1 63.0 60.6 61.7 30.5 65.0 61.9 60.4 F1 57.7 53.3 55.7 60.5 57.9 60.7 38.0 66.8 67.1 66.3 Test F1 54.5 52.4 50.5 56.5 58.2 60.9 - Table 3: Summary of results based on 5-fold cross validation on the unmodified training set (P,R,F1) as well as evaluation on the blind test set on CodaLa"
2020.figlang-1.26,N16-1020,0,0.15183,"s the context of the sentence. Beigman Klebanov et al. (2018) establish baselines for metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Stemle and Onysko, 2018; Mykowiecka et al., 2018), and showed that contextualized word representations from Bi-LSTM can be more effective. In this work, we investigate whether Gao et al. (2018)’s findings can be replicated when detecting metaphors in TOEFL essays rather than the BNC. In addition, we attempt to answer the following question: do contextualized word representations from a Bi-LSTM model detect metaphorical word use more accurately than feature-rich linear models? On the one hand, Bi-LSTM sequence labelers have proven quite successful at learnin"
2020.figlang-1.26,W18-0918,0,0.136218,"18) establish baselines for metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Stemle and Onysko, 2018; Mykowiecka et al., 2018), and showed that contextualized word representations from Bi-LSTM can be more effective. In this work, we investigate whether Gao et al. (2018)’s findings can be replicated when detecting metaphors in TOEFL essays rather than the BNC. In addition, we attempt to answer the following question: do contextualized word representations from a Bi-LSTM model detect metaphorical word use more accurately than feature-rich linear models? On the one hand, Bi-LSTM sequence labelers have proven quite successful at learning task-specific representations for many NLP problems. On th"
2020.figlang-1.26,P14-1024,0,0.412576,"theatrical performance, which also matches the context of the sentence. Beigman Klebanov et al. (2018) establish baselines for metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Stemle and Onysko, 2018; Mykowiecka et al., 2018), and showed that contextualized word representations from Bi-LSTM can be more effective. In this work, we investigate whether Gao et al. (2018)’s findings can be replicated when detecting metaphors in TOEFL essays rather than the BNC. In addition, we attempt to answer the following question: do contextualized word representations from a Bi-LSTM model detect metaphorical word use more accurately than feature-rich linear models? On the one hand, Bi-LSTM sequence labelers"
2020.figlang-1.26,D11-1063,0,0.171253,"age” both occur within the context of a theatrical performance, which also matches the context of the sentence. Beigman Klebanov et al. (2018) establish baselines for metaphor detection on TOEFL essays using feature-rich logistic regression classifiers, and show that use of metaphors is a strong predictor of the quality of the essay. The same year, Gao et al. (2018) establish a new state-of-the-art with a simple Bi-LSTM model on the VUA dataset drawn from multiple genres in the British National Corpus (BNC). Their approach departed from prior models built on linguistically motivated features (Turney et al., 2011; Hovy et al., 2013; Tsvetkov et al., 2014), visual features (Shutova et al., 2016) or learning custom word embeddings (Stemle and Onysko, 2018; Mykowiecka et al., 2018), and showed that contextualized word representations from Bi-LSTM can be more effective. In this work, we investigate whether Gao et al. (2018)’s findings can be replicated when detecting metaphors in TOEFL essays rather than the BNC. In addition, we attempt to answer the following question: do contextualized word representations from a Bi-LSTM model detect metaphorical word use more accurately than feature-rich linear models?"
2020.findings-emnlp.182,D18-1549,0,0.105079,"corpora which represent source and target language distributions better than the limited parallel corpora. Back-Translation trains the source-totarget translation model pθ (y |x) by maximizing the conditional log-likelihood of target language sentences y given pseudo source sentences x ˜ inferred by a pre-trained target-to-source translation model qφ (x |y) given y. IBT optimizes the dual translation models pθ (y |x) and qφ (x |y) via backtranslation in turn, both for semi-supervised (Zhang et al., 2018; Hoang et al., 2018; Cotterell and Kreutzer, 2018; Niu et al., 2018) and unsupervised MT (Artetxe et al., 2018; Lample et al., 2018a,b). Dual Learning takes the view of cooperative game theory where dual models collaborate with each other to learn to reconstruct the observed source and target monolingual sentences, and is widely used for semi-supervised (He et al., 2016), unsupervised (Wang et al., 2019), and zero-shot multilingual NMT (Sestorain et al., 2018). Concretely, Dual Learning optimizes pθ (y |x) and qφ (x |y) jointly by reconstructing the original target sentence y using pθ (y |x) given the source x ˜ inferred by qφ (x |y), and vice versa. The reconstruction loss is augmented with a languag"
2020.findings-emnlp.182,N19-1071,0,0.0284424,"(ELBO) of log pθ (y): log pθ (y) ≥Ex∼pψ (x |y) [log pθ (y |x)] − DKL [ pψ (x |y) ||q(x)] likelihood objective which is intractable to optimize directly.2 In Section 3.4, we compare and contrast how IBT and Dual Learning approximate Jdual (θ, φ). 3.2 (1) where DKL [ pψ ||q] is the Kullback-Leibler (KL) divergence. However, estimating the prior distribution q(x) by the discrete data distribution qdata (x) makes it difficult to directly compute the KL term. One can estimate q(x) using a language model (LM) trained to maximize the likelihood of the source monolingual data (Miao and Blunsom, 2016; Baziotis et al., 2019), at the cost of introducing additional model bias into the translation model. The non-differentiable KL term requires gradient estimators such as policy gradient (Williams, 1992) or Gumbel-softmax (Jang et al., 2017), which may introduce further training noise (He et al., 2020). To address these issues, we introduce the dual reconstruction objective, which includes two reconstruction terms that resemble the first term in the ELBO objective (Eq. (1)) while excluding the KL term that is challenging to optimize and show that this objective has desirable properties and can be better approximated"
2020.findings-emnlp.182,2004.iwslt-evaluation.1,0,0.0821106,"Missing"
2020.findings-emnlp.182,D18-1045,0,0.0305228,"view of Iterative BackTranslation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative BackTranslation is more effective than Dual Learning despite its relative simplicity. 1 ? ? Taking advantage of monolingual training data via Back-Translation (Sennrich et al., 2016a), Iterative Back-Translation (Zhang et al., 2018; Cotterell and Kreutzer, 2018) or Dual Learning (He et al., 2016) has become a de facto requirement for building high quality Neural Machine Translation (NMT) systems (Edunov et al., 2018; Hassan et al., 2018). However, these methods rely on unrelated heuristic optimization objectives, and it is not clear what their respective strengths and weaknesses are, nor how they relate to the ideal but intractable objective of maximizing the marginalP likelihood of the monolingual data (i.e., pθ (y) = x pθ (y |x)q(x) given target sentences y, an NMT model pθ (y |x), and the prior distribution q(x) on source x). Instead of proposing new methods, this paper sheds new light on how these established techniques work and how to use them. We introduce a dual reconstruction objective to theoret"
2020.findings-emnlp.182,D18-1397,0,0.0181041,"tive cannot be directly optimized since decoding is not differentiable. We compare how it is approximated by IBT vs. Dual Learning. Gradient Approximation To estimate the dual reconstruction objective, one could use sampling or beam search from the model distribution. However, since neither approach is differentiable, the gradients ∇θ J2 and ∇φ J1 cannot be computed directly. IBT blocks the gradients ∇θ J2 and ∇φ J1 assuming that they are negligible, while Dual Learning approximates them by policy gradient (Williams, 1992), which can lead to slow and unstable training (Henderson et al., 2018; Wu et al., 2018). Proposition 1 shows that the objective is maximized when the mutual information is maximized to Imax . Thus, maximizing the mutual information by other means can help side-step this issue. For example, combining the supervised and unsupervised training objectives (Sennrich et al., 2016a; Cotterell and Kreutzer, 2018) to train models jointly on the parallel and monolingual data can help. For unsupervised MT, the denoising auto-encoding objective introduced in Lample et al. (2018a) can be viewed as a way to maximize the mutual information. LM Loss Dual Learning combines the dual reconstruction"
2020.ngt-1.21,D19-1166,1,0.689114,"aining example, (ei , Fi , Wi ) includes a source sentence in English, a reference set Fi = {fi1 , fi2 , ..., fiK } of K translations and corresponding LRF weights Wi = {wi1 , wi2 , ..., wiK }, we create MT training samples by copying the translation pair (ei , fij ), wij × O times.1 Given model parameters θ, this yields a weighted crossentropy loss: Background Unlike in the STAPLE task, recent attempts at generating multiple translations for a single source have targeted output variability along specific stylistic dimensions (Sennrich et al., 2016b; Rabinovich et al., 2016; Niu et al., 2018; Agrawal and Carpuat, 2019) or produce diverse outputs without a specific use case (Kikuchi et al., 2016; Shu et al., 2019). The techniques used can be divided in three categories: (a) constrain the decoding process to generate diverse candidates (Li and Jurafsky, 2016; Li et al., 2015; Cho, 2016); (b) optimize via a diversity promoting loss function (Li et al., 2015); (c) expose the model to different translation candidates with side-constraints (Rabinovich et al., 2016; Sennrich et al., 2016a; Niu et al., 2018; Agrawal and Carpuat, 2019; Shu et al., 2019) or without (Shen et al., 2019). Since it is unclear what dimens"
2020.ngt-1.21,D16-1140,0,0.0243092,"set Fi = {fi1 , fi2 , ..., fiK } of K translations and corresponding LRF weights Wi = {wi1 , wi2 , ..., wiK }, we create MT training samples by copying the translation pair (ei , fij ), wij × O times.1 Given model parameters θ, this yields a weighted crossentropy loss: Background Unlike in the STAPLE task, recent attempts at generating multiple translations for a single source have targeted output variability along specific stylistic dimensions (Sennrich et al., 2016b; Rabinovich et al., 2016; Niu et al., 2018; Agrawal and Carpuat, 2019) or produce diverse outputs without a specific use case (Kikuchi et al., 2016; Shu et al., 2019). The techniques used can be divided in three categories: (a) constrain the decoding process to generate diverse candidates (Li and Jurafsky, 2016; Li et al., 2015; Cho, 2016); (b) optimize via a diversity promoting loss function (Li et al., 2015); (c) expose the model to different translation candidates with side-constraints (Rabinovich et al., 2016; Sennrich et al., 2016a; Niu et al., 2018; Agrawal and Carpuat, 2019; Shu et al., 2019) or without (Shen et al., 2019). Since it is unclear what dimensions of variations are captured in the STAPLE translation, we focus instead o"
2020.ngt-1.21,2020.ngt-1.28,0,0.0332819,"of interest, we use beam search to generate n-best translation candidates. However, since n-best lists are known to lack diversity, we propose to generate hypotheses that better match the requirements of the STAPLE task via: Introduction While machine translation (MT) typically produces a single output for each input, scoring and generation for second language learning applications might benefit from systems whose outputs better capture the diversity of translations produced by language learners. The Duolingo Simultaneous Translation And Paraphrase for Language Education (STAPLE) shared task (Mayhew et al., 2020) provides a framework for developing and testing such systems, grounded in real translations produced by English learners into five native languages (Portuguese, Vietnamese, Hungarian, Japanese, Korean). In this task, given an English sentence prompt, systems are asked to produce a set of translations for that prompt, and are scored based on how well their outputs cover human-curated acceptable translations, weighted by the likelihood that an English learner would respond with each translation (Table 1). 1. Frequency-Aware n-Best Lists: We encourage hypotheses to reflect the diversity and freq"
2020.ngt-1.21,P19-1177,0,0.0167286,"..., fiK } of K translations and corresponding LRF weights Wi = {wi1 , wi2 , ..., wiK }, we create MT training samples by copying the translation pair (ei , fij ), wij × O times.1 Given model parameters θ, this yields a weighted crossentropy loss: Background Unlike in the STAPLE task, recent attempts at generating multiple translations for a single source have targeted output variability along specific stylistic dimensions (Sennrich et al., 2016b; Rabinovich et al., 2016; Niu et al., 2018; Agrawal and Carpuat, 2019) or produce diverse outputs without a specific use case (Kikuchi et al., 2016; Shu et al., 2019). The techniques used can be divided in three categories: (a) constrain the decoding process to generate diverse candidates (Li and Jurafsky, 2016; Li et al., 2015; Cho, 2016); (b) optimize via a diversity promoting loss function (Li et al., 2015); (c) expose the model to different translation candidates with side-constraints (Rabinovich et al., 2016; Sennrich et al., 2016a; Niu et al., 2018; Agrawal and Carpuat, 2019; Shu et al., 2019) or without (Shen et al., 2019). Since it is unclear what dimensions of variations are captured in the STAPLE translation, we focus instead on improving n-best"
2020.ngt-1.21,J05-4003,0,0.15962,"ware Hypotheses Generation While neural MT systems can generate multiple translation candidates per source using beam search, the n-best translations often lack diversity. One issue is that systems are trained on singletranslation training samples. We propose to tailor MT to the STAPLE task by fine-tuning models on LRF-weighted multi-reference samples to obtain fˆ ||e| • Length features |fˆ|, |e|, ||e| , ˆ might indicate |f | mismatches between source and target content. • Word alignment features have proved useful to identify semantic divergences in bitext 1 179 We set O = 1000 in practice. (Munteanu and Marcu, 2005; Vyas et al., 2018). We use the Forward and Reverse Alignment score, the count of unaligned words for source and target, and the top three largest fertilities for source and target. • Scores from various MT models as often done when reranking n-best lists (Cherry and Foster, 2012; Neubig et al., 2015; Hassan et al., 2018) including a left-to-right model, a right-to-left model, and a target-to-source model, which provide different views of the example and might better estimate the adequacy of the translation than the original MT model score. Figure 1: Average of the top-1, top-5, mean and medi"
2020.ngt-1.21,tiedemann-2012-parallel,0,0.035798,"test sets available to participants on codalab. Loss We optimize a Soft Macro-F1 objective (Hsieh et al., 2018) function to approximate the official evaluation metric.2 The true positive (tp), false positive(fp), and true negative (tn) rate for each source prompt ei are estimated as: tpei = fpei = tnei = N X t=1 N X t=1 N X yˆi × yi yˆi × (1 − yi ) (1 − yˆi ) × yi t=1 Then, the precision, recall, F1 for a source ei , and the loss are defined as: tpei Pei = tpei + fpei +  tpei Rei = tpei + fnei +  2 × Pei × Rei F1Macroei = Pei + Rei +  Loss = Other Bitexts We use bitext from OpenSubtitles (Tiedemann, 2012) and Tatoeba (Tiedemann, 2012) as described in Table 3. The Tatoeba corpus provides multiple reference translations for some sources (with 2 translation per source on average), but unlike in the STAPLE data, these translations are not weighted by frequency of usage. M X (1 − F1Macroei ) i=1 4 Experiment Settings 4.1 Data Preprocessing All datasets are pre-processed using Moses tools for normalization, tokenization and lowercasing. We further segment tokens into subwords using a joint source-target Byte Pair Encoding (Sennrich et al., 2016c) model with 32, 000 STAPLE Data The shared task provid"
2020.ngt-1.21,N18-1136,1,0.900193,"Missing"
2020.ngt-1.21,W15-5003,0,0.0218095,"models on LRF-weighted multi-reference samples to obtain fˆ ||e| • Length features |fˆ|, |e|, ||e| , ˆ might indicate |f | mismatches between source and target content. • Word alignment features have proved useful to identify semantic divergences in bitext 1 179 We set O = 1000 in practice. (Munteanu and Marcu, 2005; Vyas et al., 2018). We use the Forward and Reverse Alignment score, the count of unaligned words for source and target, and the top three largest fertilities for source and target. • Scores from various MT models as often done when reranking n-best lists (Cherry and Foster, 2012; Neubig et al., 2015; Hassan et al., 2018) including a left-to-right model, a right-to-left model, and a target-to-source model, which provide different views of the example and might better estimate the adequacy of the translation than the original MT model score. Figure 1: Average of the top-1, top-5, mean and median LRF values across source prompts: the LRF distribution is more uniform for languages with many more references per prompt (e.g. en-ja). • Target 5-gram language model score to estimate the fluency of the hypothesis. of plausible translations in five other languages. These translations are weighted"
2020.ngt-1.21,C18-1086,1,0.901447,"Missing"
2020.ngt-1.21,P02-1040,0,0.116471,"i et al., 2015; Cho, 2016); (b) optimize via a diversity promoting loss function (Li et al., 2015); (c) expose the model to different translation candidates with side-constraints (Rabinovich et al., 2016; Sennrich et al., 2016a; Niu et al., 2018; Agrawal and Carpuat, 2019; Shu et al., 2019) or without (Shen et al., 2019). Since it is unclear what dimensions of variations are captured in the STAPLE translation, we focus instead on improving n-best lists generated by a standard neural MT model. Source texts with multiple references have mostly been used to evaluate rather than train MT systems (Papineni et al., 2002; Banerjee and Lavie, 2005; Qin and Specia, 2015). Evaluation sets with 4 or 5 references have been converted to singlereference training samples (Zheng et al., 2018) to improve MT training, but reference translations vary in arbitrary ways and often exhibit poor diversity, mostly limited to translationese effects. The STAPLE data presents an opportunity to explore multiple translations generated in a more comprehensive fashion. 3 3.1 Llrf (θ) = M X K X (wij × O) log P r(fij |ei ; θ) (1) i=1 j=1 3.2 Hypothesis Filtering as Binary Classification Even when informed by STAPLE data and LRF scores,"
2020.ngt-1.21,D18-1357,0,0.0434192,"Missing"
2020.ngt-1.21,W15-4915,0,0.361207,"versity promoting loss function (Li et al., 2015); (c) expose the model to different translation candidates with side-constraints (Rabinovich et al., 2016; Sennrich et al., 2016a; Niu et al., 2018; Agrawal and Carpuat, 2019; Shu et al., 2019) or without (Shen et al., 2019). Since it is unclear what dimensions of variations are captured in the STAPLE translation, we focus instead on improving n-best lists generated by a standard neural MT model. Source texts with multiple references have mostly been used to evaluate rather than train MT systems (Papineni et al., 2002; Banerjee and Lavie, 2005; Qin and Specia, 2015). Evaluation sets with 4 or 5 references have been converted to singlereference training samples (Zheng et al., 2018) to improve MT training, but reference translations vary in arbitrary ways and often exhibit poor diversity, mostly limited to translationese effects. The STAPLE data presents an opportunity to explore multiple translations generated in a more comprehensive fashion. 3 3.1 Llrf (θ) = M X K X (wij × O) log P r(fij |ei ; θ) (1) i=1 j=1 3.2 Hypothesis Filtering as Binary Classification Even when informed by STAPLE data and LRF scores, n-best lists might include translations that are"
2020.ngt-1.21,N16-1005,0,0.292797,"ences. Given the STAPLE data for a language pair, where the i-th training example, (ei , Fi , Wi ) includes a source sentence in English, a reference set Fi = {fi1 , fi2 , ..., fiK } of K translations and corresponding LRF weights Wi = {wi1 , wi2 , ..., wiK }, we create MT training samples by copying the translation pair (ei , fij ), wij × O times.1 Given model parameters θ, this yields a weighted crossentropy loss: Background Unlike in the STAPLE task, recent attempts at generating multiple translations for a single source have targeted output variability along specific stylistic dimensions (Sennrich et al., 2016b; Rabinovich et al., 2016; Niu et al., 2018; Agrawal and Carpuat, 2019) or produce diverse outputs without a specific use case (Kikuchi et al., 2016; Shu et al., 2019). The techniques used can be divided in three categories: (a) constrain the decoding process to generate diverse candidates (Li and Jurafsky, 2016; Li et al., 2015; Cho, 2016); (b) optimize via a diversity promoting loss function (Li et al., 2015); (c) expose the model to different translation candidates with side-constraints (Rabinovich et al., 2016; Sennrich et al., 2016a; Niu et al., 2018; Agrawal and Carpuat, 2019; Shu et al"
2020.ngt-1.21,P16-1162,0,0.392579,"ences. Given the STAPLE data for a language pair, where the i-th training example, (ei , Fi , Wi ) includes a source sentence in English, a reference set Fi = {fi1 , fi2 , ..., fiK } of K translations and corresponding LRF weights Wi = {wi1 , wi2 , ..., wiK }, we create MT training samples by copying the translation pair (ei , fij ), wij × O times.1 Given model parameters θ, this yields a weighted crossentropy loss: Background Unlike in the STAPLE task, recent attempts at generating multiple translations for a single source have targeted output variability along specific stylistic dimensions (Sennrich et al., 2016b; Rabinovich et al., 2016; Niu et al., 2018; Agrawal and Carpuat, 2019) or produce diverse outputs without a specific use case (Kikuchi et al., 2016; Shu et al., 2019). The techniques used can be divided in three categories: (a) constrain the decoding process to generate diverse candidates (Li and Jurafsky, 2016; Li et al., 2015; Cho, 2016); (b) optimize via a diversity promoting loss function (Li et al., 2015); (c) expose the model to different translation candidates with side-constraints (Rabinovich et al., 2016; Sennrich et al., 2016a; Niu et al., 2018; Agrawal and Carpuat, 2019; Shu et al"
2020.wmt-1.141,P19-1294,0,0.653747,"cases where such a system would be beneficial. For example, content providers meticulously curate lists of terminologies for their domains that indicate preferred translations for technical terms. Lexically constrained APE would also be useful for cross-lingual information retrieval. When displaying snippets from retrieved documents, the query term should appear in the translation output (if it does in the source) as it can make relevance clear to the end user. Here, the query serves as the term. While recent approaches allow inference time adaptation of NMT systems using these terminologies (Dinu et al., 2019; Post and Vilar, 2018), postediting translations with a generic APE system may lead to dropped terms. A constraint-aware APE system would allow to fix systematic translation errors, while keeping the terminologies intact. Inspired by Dinu et al. (2019), we consider a range of representations which augment input sequences with constraint tokens and factors for use in an autoregressive Transformer (AT) APE model. Using this approach, the constraints are explicitly represented in the encoder input sequence, and the model learns to prefer translations that contain the supplied terminologies durin"
2020.wmt-1.141,P17-1141,0,0.0422942,"E pipeline. 3. We analyze the robustness of the constraint translation behavior and suggest a simple data augmentation technique that both improves translation quality and increases the number of correctly translated terms. 2 2.1 Related Work MT with Terminology Constraints Integrating terminology constraints into translation can be divided into two approaches: constrained decoding and input sequence modification. Constrained decoding modifies the decoding process to enforce the generation of the specified terminologies. This includes methods that modify beam search, such as grid beam search (Hokamp and Liu, 2017) and dynamic beam allocations (Post and Vilar, 2018). While these approaches are effective in including terminologies, they come with an increase in inference time due to the added overhead in the search algorithm. The LevT (Gu et al., 2019), which uses a nonautoregressive decoding procedure, can initialize its decoder with a partial or incomplete output sequence. By initializing the decoder output with terminology constraints, Susanto et al. (2020) train a LevT model to perform constrained decoding. Unlike constrained search methods in autoregressive models, this initialization technique does"
2020.wmt-1.141,W16-2378,0,0.0379075,"Missing"
2020.wmt-1.141,W18-6467,0,0.0137064,"roach has the benefit of not adding additional overhead during inference. 2.2 Automatic Post-Editing The APE task has gone through many iterations, since it was originally proposed by Simard et al. (2007). Initially, the task was to improve an unknown phrase-based machine transition (PBMT) system. An additional task to fix errors of an NMT system was introduced at WMT 2018 (Chatterjee et al., 2018). For the APE tasks, the use of the multi-source variant of the neural encoder-decoder model is the most popular approach (Bojar et al., 2017), with the Multi-source Transformer (MST) instantiation (Junczys-Dowmunt and Grundkiewicz, 2018) achieving state-of-the-art results in 2018. Based on the AT model (Vaswani et al., 2017), the MST model consists of two Transformer encoders and a single decoder. The source sentence and the MT system output are fed separately to the two encoders, where the outputs are concatenated and then fed into the decoder to perform post-editing. Recent work has explored alternative architectures for APE. The winner of 2019 APE tasks (Lopes et al., 2019), for example, uses a BERTbased encoder and decoder. Gu et al. (2019) both introduce the LevT model and demonstrate its utility on an APE task. 3 Constr"
2020.wmt-1.141,P07-2045,0,0.0168074,"e relatively small, we augment them with large synthetic datasets for pretraining: artificial (Junczys-Dowmunt and Grundkiewicz, 2016) and eSCAPE (Negri et al., 2018). The artificial dataset is generated using round-trip translation of two PBMT systems. It is already cleaned and tokenized. The eSCAPE dataset, containing 7,258,533 triplets, is created using NMT generated output from various parallel corpora. The data for eSCAPE is noisy, and we follow Lee et al. (2019)’s procedure to filter the dataset, which results in around 5 million triplets. We then tokenize the filtered data using Moses (Koehn et al., 2007).1 For pretraining on the synthetic corpora, we set aside 1,000 randomly sampled triplets as our validation set. Table 1 summarizes the statistics of both the evaluation and pretraining datasets. For both tasks, we use the same preprocessing steps. After tokenization, we truecase the data using Moses. We then use BPE with 32,000 merge operations on the joined vocabulary of source and target language. 5.2 Terminology Dataset We create terminology sets for each APE dataset using Wiktionary.2 We follow the procedure of Dinu et al. (2019), finding term translation pairs (ˇ x, y ˇ) in Wiktionary su"
2020.wmt-1.141,W19-5412,0,0.0224976,"Missing"
2020.wmt-1.141,W19-5413,0,0.0203122,"Missing"
2020.wmt-1.141,L18-1004,0,0.0390869,"Missing"
2020.wmt-1.141,N19-4009,0,0.0335909,"Missing"
2020.wmt-1.141,P02-1040,0,0.105896,"Missing"
2020.wmt-1.141,N18-1119,0,0.413604,"system would be beneficial. For example, content providers meticulously curate lists of terminologies for their domains that indicate preferred translations for technical terms. Lexically constrained APE would also be useful for cross-lingual information retrieval. When displaying snippets from retrieved documents, the query term should appear in the translation output (if it does in the source) as it can make relevance clear to the end user. Here, the query serves as the term. While recent approaches allow inference time adaptation of NMT systems using these terminologies (Dinu et al., 2019; Post and Vilar, 2018), postediting translations with a generic APE system may lead to dropped terms. A constraint-aware APE system would allow to fix systematic translation errors, while keeping the terminologies intact. Inspired by Dinu et al. (2019), we consider a range of representations which augment input sequences with constraint tokens and factors for use in an autoregressive Transformer (AT) APE model. Using this approach, the constraints are explicitly represented in the encoder input sequence, and the model learns to prefer translations that contain the supplied terminologies during decoding. We also exp"
2020.wmt-1.141,P16-1162,0,0.0810005,"uences, the source sentence and the MT output to be postedited. To accommodate these two sequences, we use the MST model of Tebbifakhr et al. (2018), which uses a separate Transformer to encode each sequence. The outputs of each encoder are concatenated and attended to by the decoder. We augment the encoder for the source sentence with the append and replace methods. Figure 1 shows an example of the inputs for the append and replace methods, x+ and x− respectively. To account for the additional input of MT, γ, for the source factors, we use 3 for each token in γ. For Byte-Pair Encoding (BPE) (Sennrich et al., 2016), the corresponding source factor token is applied for all subword units. We train three variants based on MST: an unconstrained version as the baseline (MST), and two constrained versions using the append (MST Append) and replace (MST Replace) methods as described in subsection 4.1. 4.3 Levenshtein Transformer The LevT follows the Transformer encoderdecoder architecture. However, instead of a regular Transformer decoder, the model uses three consecutive layers to simulate the edit operations. The first layer predicts whether each token should be deleted or kept. The second layer predicts how"
2020.wmt-1.141,2006.amta-papers.25,0,0.22635,"Missing"
2020.wmt-1.141,2020.acl-main.325,0,0.279038,"ecoding process to enforce the generation of the specified terminologies. This includes methods that modify beam search, such as grid beam search (Hokamp and Liu, 2017) and dynamic beam allocations (Post and Vilar, 2018). While these approaches are effective in including terminologies, they come with an increase in inference time due to the added overhead in the search algorithm. The LevT (Gu et al., 2019), which uses a nonautoregressive decoding procedure, can initialize its decoder with a partial or incomplete output sequence. By initializing the decoder output with terminology constraints, Susanto et al. (2020) train a LevT model to perform constrained decoding. Unlike constrained search methods in autoregressive models, this initialization technique does not add any significant overhead to the decoding process. When modified to disallow deletion of terms and insertion between consecutive terminology tokens, LevT is able to retain all terminologies without affecting the performance and speed. Alternatively, Dinu et al. (2019) propose modifying the encoder input sequence to represent terminology constraints. During training, the model learns to identify constraints in the input sequence, and translat"
2020.wmt-1.141,W18-6471,0,0.0571416,"cially, when a source side constraint x ˇ(i) matches a sub-sequence in x, it is required that the sub-sequence be translated as y ˇ(i) . See Figure 1 for an example. 4 Models While there are existing models to address the APE task, and the lexical constrained MT task, it is not clear how to represent lexical constraints for APE models which, unlike MT models, take two sequences as input. We propose several techniques to incorporate constraints as additional inputs to the APE encoder by combining the input sequence modification used in constrained MT (Dinu et al., 2019) with the MST method of (Tebbifakhr et al., 2018). For decoding, we experiment with both the AT and the LevT decoders. The LevT decoder can additionally take advantage of different decoder initialization strategies for constrained decoding. We first briefly show how we encode terminology constraints in the input sequence, before deModel Input Init. MST MST Append MST Replace x, γ x+ , γ x− , γ – – – LevT LevT Append LevT Replace MS LevT x x+ x− x, γ γ γ γ y ˇ1 , . . . , y ˇ(t) Figure 2: Setup for the models by the input and initialization at inference. scribing how they are incorporated into the MST and LevT APE models specifically. 4.1 Enco"
2020.wmt-1.56,W18-6311,0,0.0812467,"gory of text, with colloquialisms, errors and minimal revisions. All of these deviations can accumulate error throughout the course of a conversation. Dialogue Translation: Specific interests in translating dialogues can be found as early as Lee and Kim (1997)’s work on Korean-English dialogue translation based on syntactic patterns and n-grams. Though their model parses sentences into speech acts instead of generating full-sentence translations, they have pointed out the importance of context (previous sentence) in interpreting the current sentence properly. The most relevant recent work is (Maruf et al., 2018), in which contexts for both source-side and target-side are utilized as additional generation conditions for the decoder in their NMT model. Several variants of the model architecture and the attention mechanism are explored. However, their experiments are conducted on Europarl and OpenSubtitles. The former is formal language and the latter scripted conversations of movies and TV. Here, in contrast, chat data is informal unscripted real-world language. Chat Context-Aware Machine Translation: translation can be regarded as a special case of context-aware translation. Jean et al. (2017) extends"
2020.wmt-1.56,N18-1118,0,0.256907,"ous consumption of text. The dream of dissolving language barriers, however, will not be fulfilled until MT enables two or more people carry on a synchronous conversation, each speaking their native languages. Building translation systems that enable seamless conversations between an English-speaking customer support agent and a German-speaking customer is the goal of WMT20’s shared task of chat translation (Farajian et al., 2020). In participation of this shared task, we focused on the agent side, translating English utterances into German. Our methods are inspired by Voita et al. (2018) and Bawden et al. (2018), explicitly leveraging broader context to address coreference and cohesion to improve translation quality. ∗ We compare architectures of a standard transformer with a single encoder and a multi-encoder one with an additional transformer encoder to incorporate information from the previous utterance. In the case of blind testing or production use, since customer target utterances (English) will not be given, a separate de→en model was trained and used to back-translate customer utterances. Additionally, given the limited training pairs, we experiment with augmenting our dataset. We selected a"
2020.wmt-1.56,2020.wmt-1.3,0,0.068917,"Missing"
2020.wmt-1.56,P07-2045,0,0.00759038,"018) utilize an off-the-shelf search engine to retrieve training sentence pairs whose source side is similar to a given source sentence and incorporate them as additional input to the decoder. Zhang et al. (2018) use the retrieved examples at prediction time to upweight outputs whose constituents match retrieved n-gram translation candidates. In a similar vein, but at training time rather than prediction time, we use a retrieval system to select similar examples from a larger dataset to augment the smaller in-domain training set. 3 Data Preparation 3.1 Preprocessing We used the Moses toolkit (Koehn et al., 2007) to preprocess our data. The training corpus was tokenized and cleaned. After that, we applied byte pair encoding (BPE) (Sennrich et al., 2016) on the data with the BPE model learned on the data of the pretrained model (Section 4.1.1). Following the pre-trained model, we use its shared vocabulary for both target and source sides. The size of the vocabulary, which is the union of English and German tokens, is 36,628. 3.2 Retrieval-Based Training Data Augmentation There were only 13.85k utterances in the provided parallel WMT20 Chat training data. Given the limited data, we start off with a mode"
2020.wmt-1.56,P18-1117,0,0.438766,"pread tool for asynchronous consumption of text. The dream of dissolving language barriers, however, will not be fulfilled until MT enables two or more people carry on a synchronous conversation, each speaking their native languages. Building translation systems that enable seamless conversations between an English-speaking customer support agent and a German-speaking customer is the goal of WMT20’s shared task of chat translation (Farajian et al., 2020). In participation of this shared task, we focused on the agent side, translating English utterances into German. Our methods are inspired by Voita et al. (2018) and Bawden et al. (2018), explicitly leveraging broader context to address coreference and cohesion to improve translation quality. ∗ We compare architectures of a standard transformer with a single encoder and a multi-encoder one with an additional transformer encoder to incorporate information from the previous utterance. In the case of blind testing or production use, since customer target utterances (English) will not be given, a separate de→en model was trained and used to back-translate customer utterances. Additionally, given the limited training pairs, we experiment with augmenting ou"
2020.wmt-1.56,D17-1301,0,0.0189594,"riants of the model architecture and the attention mechanism are explored. However, their experiments are conducted on Europarl and OpenSubtitles. The former is formal language and the latter scripted conversations of movies and TV. Here, in contrast, chat data is informal unscripted real-world language. Chat Context-Aware Machine Translation: translation can be regarded as a special case of context-aware translation. Jean et al. (2017) extends the vanilla attention-based neural MT model (Bahdanau et al., 2015) by conditioning the decoder on the previous sentence via attention over its words. Wang et al. (2017) propose a crosssentence context-aware model. They integrate the historical representation into NMT with two strategies: a warm-start of encoder and decoder states, and an auxiliary context source for updating decoder. Bawden et al. (2018) use multi-encoder NMT models to exploit context from the previous source and target sentence. Voita et al. (2018) propose a context-aware model based on the Transformer. Their model controls the flow of information from the extended context and improves on pronoun translation. NMT Facilitated with Retrieved Translations: There is a line of NMT research inspi"
2020.wmt-1.56,N18-1120,0,0.0203653,"2018) propose a context-aware model based on the Transformer. Their model controls the flow of information from the extended context and improves on pronoun translation. NMT Facilitated with Retrieved Translations: There is a line of NMT research inspired by example-based translation systems that aims to generate better translations by retrieving and referencing additional translation pairs. Gu et al. (2018) utilize an off-the-shelf search engine to retrieve training sentence pairs whose source side is similar to a given source sentence and incorporate them as additional input to the decoder. Zhang et al. (2018) use the retrieved examples at prediction time to upweight outputs whose constituents match retrieved n-gram translation candidates. In a similar vein, but at training time rather than prediction time, we use a retrieval system to select similar examples from a larger dataset to augment the smaller in-domain training set. 3 Data Preparation 3.1 Preprocessing We used the Moses toolkit (Koehn et al., 2007) to preprocess our data. The training corpus was tokenized and cleaned. After that, we applied byte pair encoding (BPE) (Sennrich et al., 2016) on the data with the BPE model learned on the dat"
2021.acl-long.562,U16-1001,0,0.0622041,"predictions. NMT . 7236 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7236–7249 August 1–6, 2021. ©2021 Association for Computational Linguistics Based on these findings, we introduce a divergent-aware NMT framework that incorporates information about which tokens are indicative of semantic divergences between the source and target side of a training sample. Source-side divergence tags are integrated as feature factors (Haddow and Koehn, 2012; Sennrich and Haddow, 2016; Hoang et al., 2016), while target-side divergence tags form an additional output sequence generated in a multi-task fashion (Garc´ıa-Mart´ınez et al., 2016, 2017). Results on EN↔FR translation show that our approach is a successful mitigation strategy: it helps NMT recover from the negative impact of fine-grained divergences on translation quality, with fewer degenerated hypotheses, and more confident and better calibrated predictions. We make our code publicly available: https://github.com/Elbria/xling-SemDiv-NMT. task dedicated to filtering noisy samples from webcrawled data at WMT, since 2018 (Koehn et al., 2"
2021.acl-long.562,W19-6721,0,0.0332278,"Missing"
2021.acl-long.562,W18-2709,0,0.361518,"remains unclear. This paper aims to understand and mitigate the impact of fine-grained semantic divergences in Figure 1: Equivalent vs. Divergent references on NMT training. Fine-grained divergences (i.e., REF (fineDIV )) provide an imperfect yet potentially useful signal depending on the time step t. We first contribute an analysis of how finegrained divergences in training data affect NMT quality and confidence. Starting from a set of equivalent English-French WikiMatrix sentence pairs, we simulate divergences by gradually “corrupting” them with synthetic fine-grained divergences. Following Khayrallah and Koehn (2018)—who, in contrast, study the impact of noise on MT—we control for different types of fine-grained semantic divergences and different ratios of equivalent vs. divergent data. Our findings indicate that these imperfect training references: hurt translation quality (as measured by BLEU and METEOR) once they overwhelm equivalents; output degenerated text more frequently; and increase the uncertainty of models’ predictions. NMT . 7236 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pa"
2021.acl-long.562,2020.wmt-1.107,0,0.0788879,"Missing"
2021.acl-long.562,C04-1151,0,0.021293,"ion might arise due to the translation process (Zhai et al., 2018) and result in non-literal translations (Zhai et al., 2020a). Divergences might also arise in parallel text extracted from multilingual comparable resources. For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010). Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Espl`a et al., 2019). Noise vs. Semantic Divergences In the context of MT, noise often refers to mismatches in webcrawled parallel corpora that are collected without guarantees about their quality. Khayrallah and Koehn (2018) define five frequent types of noise found in the German-English Paracrawl corpus: misaligned sentences, disfluent text, wrong language, short segments, and untranslated sentences. They examine the impact of noise on translation quality and find that untranslated training in"
2021.acl-long.562,W19-5404,0,0.0434747,"Missing"
2021.acl-long.562,P07-2045,0,0.0208487,"Missing"
2021.acl-long.562,P02-1040,0,0.109363,"ussed in §3.2, except that target embeddings are not tied with output layer weights for factored models. More details are included in B. Other Data & Preprocessing We use the same preprocessing as well as development and test sets as in §3.2, except we learn 5K BPEs as in 3 Schwenk et al. (2019). DIV- FACTORIZED, DIVand DIV- TAGGED models are compared in controlled setups that use the same training data. We also evaluate out-of-domain on the khresmoi-summary test set for the WMT2014 medical translation task (Bojar et al., 2014). AGNOSTIC , Evaluation We evaluate translation quality with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005).4,5 We compute Inference Expected Calibration Error (InfECE) as Wang et al. (2020), which measures the difference in expectation between confidence and accuracy.6 We measure token-level translation accuracy based on Translation Error Rate (TER) alignments between hypotheses and references.7 Unless mentioned otherwise, we decode with a beam size of 5. 4.3 Results We discuss the impact of real divergences along the dimensions surfaced by the synthetic data analysis. Translation Quality Table 3 presents BLEU and METEOR scores across model configurations and"
2021.acl-long.562,N18-2084,0,0.0446201,"Missing"
2021.acl-long.562,2020.eamt-1.31,0,0.0550229,"Missing"
2021.acl-long.562,L16-1443,0,0.021229,"Missing"
2021.acl-long.562,2021.eacl-main.115,0,0.0397661,"Missing"
2021.acl-long.562,W16-2209,0,0.143952,"the uncertainty of models’ predictions. NMT . 7236 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7236–7249 August 1–6, 2021. ©2021 Association for Computational Linguistics Based on these findings, we introduce a divergent-aware NMT framework that incorporates information about which tokens are indicative of semantic divergences between the source and target side of a training sample. Source-side divergence tags are integrated as feature factors (Haddow and Koehn, 2012; Sennrich and Haddow, 2016; Hoang et al., 2016), while target-side divergence tags form an additional output sequence generated in a multi-task fashion (Garc´ıa-Mart´ınez et al., 2016, 2017). Results on EN↔FR translation show that our approach is a successful mitigation strategy: it helps NMT recover from the negative impact of fine-grained divergences on translation quality, with fewer degenerated hypotheses, and more confident and better calibrated predictions. We make our code publicly available: https://github.com/Elbria/xling-SemDiv-NMT. task dedicated to filtering noisy samples from webcrawled data at WMT, since"
2021.acl-long.562,N16-1005,0,0.111719,"iltering strategy recommended by Schwenk et al. (2019). Our prior work shows that thresholding LASER might introduce a number of divergent data in the training pool varying from fine to coarse mismatches (Briakou and Carpuat, 2020). 2. EQUIVALENTS models are trained on WikiMatrix pairs detected as exact translations (§3.2); 3. DIV- AGNOSTIC models are trained on equivalent and fine-grained divergent data without incorporating information that distinguishes between them; 4. DIV- TAGGED models distinguish equivalences from divergences by appending <EQ&gt; vs. <DIV&gt; tags as source-side constraints (Sennrich et al., 2016a). Models’ details Our models are implemented in the Sockeye2 toolkit (Domhan et al., 2020).3 We set the size of factor embeddings to 8, the source token embeddings to 504 and target embeddings to 514, yielding equal model sizes across experiments. All other parameters are kept the same across models, as discussed in §3.2, except that target embeddings are not tied with output layer weights for factored models. More details are included in B. Other Data & Preprocessing We use the same preprocessing as well as development and test sets as in §3.2, except we learn 5K BPEs as in 3 Schwenk et al."
2021.acl-long.562,P16-1009,0,0.125987,"iltering strategy recommended by Schwenk et al. (2019). Our prior work shows that thresholding LASER might introduce a number of divergent data in the training pool varying from fine to coarse mismatches (Briakou and Carpuat, 2020). 2. EQUIVALENTS models are trained on WikiMatrix pairs detected as exact translations (§3.2); 3. DIV- AGNOSTIC models are trained on equivalent and fine-grained divergent data without incorporating information that distinguishes between them; 4. DIV- TAGGED models distinguish equivalences from divergences by appending <EQ&gt; vs. <DIV&gt; tags as source-side constraints (Sennrich et al., 2016a). Models’ details Our models are implemented in the Sockeye2 toolkit (Domhan et al., 2020).3 We set the size of factor embeddings to 8, the source token embeddings to 504 and target embeddings to 514, yielding equal model sizes across experiments. All other parameters are kept the same across models, as discussed in §3.2, except that target embeddings are not tied with output layer weights for factored models. More details are included in B. Other Data & Preprocessing We use the same preprocessing as well as development and test sets as in §3.2, except we learn 5K BPEs as in 3 Schwenk et al."
2021.acl-long.562,P16-1162,0,0.398657,"iltering strategy recommended by Schwenk et al. (2019). Our prior work shows that thresholding LASER might introduce a number of divergent data in the training pool varying from fine to coarse mismatches (Briakou and Carpuat, 2020). 2. EQUIVALENTS models are trained on WikiMatrix pairs detected as exact translations (§3.2); 3. DIV- AGNOSTIC models are trained on equivalent and fine-grained divergent data without incorporating information that distinguishes between them; 4. DIV- TAGGED models distinguish equivalences from divergences by appending <EQ&gt; vs. <DIV&gt; tags as source-side constraints (Sennrich et al., 2016a). Models’ details Our models are implemented in the Sockeye2 toolkit (Domhan et al., 2020).3 We set the size of factor embeddings to 8, the source token embeddings to 504 and target embeddings to 514, yielding equal model sizes across experiments. All other parameters are kept the same across models, as discussed in §3.2, except that target embeddings are not tied with output layer weights for factored models. More details are included in B. Other Data & Preprocessing We use the same preprocessing as well as development and test sets as in §3.2, except we learn 5K BPEs as in 3 Schwenk et al."
2021.acl-long.562,N10-1063,0,0.0198298,"ences We use this term to refer to meaning differences in aligned bilingual text (Vyas et al., 2018; Carpuat et al., 2017). Divergences in manual translation might arise due to the translation process (Zhai et al., 2018) and result in non-literal translations (Zhai et al., 2020a). Divergences might also arise in parallel text extracted from multilingual comparable resources. For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010). Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Espl`a et al., 2019). Noise vs. Semantic Divergences In the context of MT, noise often refers to mismatches in webcrawled parallel corpora that are collected without guarantees about their quality. Khayrallah and Koehn (2018) define five frequent types of noise found in the German-English Paracrawl corpus: misaligned sentences, disfluent text"
2021.acl-long.562,P13-1135,0,0.0115753,"in non-literal translations (Zhai et al., 2020a). Divergences might also arise in parallel text extracted from multilingual comparable resources. For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010). Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Espl`a et al., 2019). Noise vs. Semantic Divergences In the context of MT, noise often refers to mismatches in webcrawled parallel corpora that are collected without guarantees about their quality. Khayrallah and Koehn (2018) define five frequent types of noise found in the German-English Paracrawl corpus: misaligned sentences, disfluent text, wrong language, short segments, and untranslated sentences. They examine the impact of noise on translation quality and find that untranslated training instances cause NMT models to copy the input sentence at inference time. Thei"
2021.acl-long.562,N18-1136,1,0.814919,"Missing"
2021.acl-long.562,2020.acl-main.278,0,0.0899919,"luded in B. Other Data & Preprocessing We use the same preprocessing as well as development and test sets as in §3.2, except we learn 5K BPEs as in 3 Schwenk et al. (2019). DIV- FACTORIZED, DIVand DIV- TAGGED models are compared in controlled setups that use the same training data. We also evaluate out-of-domain on the khresmoi-summary test set for the WMT2014 medical translation task (Bojar et al., 2014). AGNOSTIC , Evaluation We evaluate translation quality with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005).4,5 We compute Inference Expected Calibration Error (InfECE) as Wang et al. (2020), which measures the difference in expectation between confidence and accuracy.6 We measure token-level translation accuracy based on Translation Error Rate (TER) alignments between hypotheses and references.7 Unless mentioned otherwise, we decode with a beam size of 5. 4.3 Results We discuss the impact of real divergences along the dimensions surfaced by the synthetic data analysis. Translation Quality Table 3 presents BLEU and METEOR scores across model configurations and data settings on the TED test sets. First, the model trained on EQUIVALENTS represents a very competitive baseline as it"
2021.acl-long.562,W19-3404,0,0.152277,"tput degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN↔FR tasks. 1 Introduction While parallel texts are essential to Neural Machine Translation (NMT), the degree of parallelism varies widely across samples in practice, for reasons ranging from noise in the extraction process (Roziewski and Stokowiec, 2016) to nonliteral translations (Zhai et al., 2019b, 2020a). For instance (Figure 1), a French SOURCE could be paired with an exact translation into English (EQ), with a mostly equivalent translation where only a few tokens convey divergent meaning (fineDIV ), or with a semantically unrelated, noisy reference (coarse-DIV). Yet, prior work treats parallel samples in a binary fashion: coarse-grained divergences are viewed as noise to be excluded from training (Koehn et al., 2018), whilst others are typically regarded as gold-standard equivalent translations. As a result, the impact of fine-grained divergences on NMT remains unclear. This paper"
2021.acl-long.562,2020.coling-main.522,0,0.250715,"here (n) (n) (x , y ) is the n-th sentence pair consisting of sentences that are assumed to be translations of each other. Under this assumption, model parameters are updated to maximize the token-level crossentropy loss: J (θ) = N X T X (n) log p(yt (n) |y<t , x(n) ; θ) (1) n=1 t=1 2 Background & Motivation Cross-lingual Semantic Divergences We use this term to refer to meaning differences in aligned bilingual text (Vyas et al., 2018; Carpuat et al., 2017). Divergences in manual translation might arise due to the translation process (Zhai et al., 2018) and result in non-literal translations (Zhai et al., 2020a). Divergences might also arise in parallel text extracted from multilingual comparable resources. For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010). Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Esp"
2021.acl-long.562,2020.lrec-1.496,0,0.0305618,"here (n) (n) (x , y ) is the n-th sentence pair consisting of sentences that are assumed to be translations of each other. Under this assumption, model parameters are updated to maximize the token-level crossentropy loss: J (θ) = N X T X (n) log p(yt (n) |y<t , x(n) ; θ) (1) n=1 t=1 2 Background & Motivation Cross-lingual Semantic Divergences We use this term to refer to meaning differences in aligned bilingual text (Vyas et al., 2018; Carpuat et al., 2017). Divergences in manual translation might arise due to the translation process (Zhai et al., 2018) and result in non-literal translations (Zhai et al., 2020a). Divergences might also arise in parallel text extracted from multilingual comparable resources. For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010). Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Esp"
2021.acl-long.562,W18-3814,0,0.0117828,"lihood of the training data, D ≡ {(x(n) , y (n) )}N n=1 , where (n) (n) (x , y ) is the n-th sentence pair consisting of sentences that are assumed to be translations of each other. Under this assumption, model parameters are updated to maximize the token-level crossentropy loss: J (θ) = N X T X (n) log p(yt (n) |y<t , x(n) ; θ) (1) n=1 t=1 2 Background & Motivation Cross-lingual Semantic Divergences We use this term to refer to meaning differences in aligned bilingual text (Vyas et al., 2018; Carpuat et al., 2017). Divergences in manual translation might arise due to the translation process (Zhai et al., 2018) and result in non-literal translations (Zhai et al., 2020a). Divergences might also arise in parallel text extracted from multilingual comparable resources. For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010). Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale par"
2021.emnlp-main.100,S16-1081,0,0.0946627,"this dimension is performed using a wider spectrum of approaches, as presented in the third column of Table 1. The most frequently used metric is referenceBLEU (r- BLEU ), which is based on the n-gram precision of the system output compared to human rewrites of the desired formality. Other approaches include self-BLEU (s-BLEU), where the system output is compared to its input, measuring the semantic similarity between the system input and its output, or regression models (e.g., CNN, BERT ) trained on data annotated for similaritybased tasks, such as the Semantic Textual Similarity task (STS) (Agirre et al., 2016). while similar resources are not available for other languages. Different model architectures have been used by prior work (e.g., CNN, LSTM, GRU, finetuning on pre-trained language models such as RoBERTa and BERT; Table 1). In most papers, Fluency Fluency is typically evaluated with the resulting classifier is evaluated on the test side model-based approaches (see fourth column of Taof the GYAFC corpus, reporting accuracy scores ble 1). Among those, the most frequent method is 1323 that of computing perplexity (PPL) under a language model. The latter is either trained from scratch on the same"
2021.emnlp-main.100,2020.findings-emnlp.263,0,0.0349961,"Missing"
2021.emnlp-main.100,2021.eacl-main.202,0,0.035073,"ee dimensions, or one could develop a singular metric. In line with Briakou et al. (2021a), our study also calls for releasing more human evaluations and more system outputs to enable robust evaluation. Finally, there is still room for improvement in assessing how fluent a rewrite is. Our study provides a framework to address these questions systematically and calls for ST papers to standardize and release data to support larger-scale evaluations. Conclusions Automatic (and human) evaluation processes are well-known problems for the field of Natural Language Generation (Howcroft et al., 2020; Clinciu et al., 2021) and the burgeoning subfield of ST is not immune. ST, in particular, has suffered from a lack of standardization of automatic metrics, a lack of agreement between human judgments and automatics metrics, as well as a blindspot to developing metrics for languages other than English. We address these issues by conducting the first controlled multilingual evaluation for leading ST metrics with a focus on formality, covering metrics for 3 evaluation dimensions and overall ranking for 4 languages. Given our findings, we recommend the formality style transfer community adopt the following best practi"
2021.emnlp-main.100,W11-2123,0,0.0252155,"), METEOR (Banerjee and Lavie, 2005) based on the harmonic mean of unigram precision and recall while accounting for synonym matches, and chrF (Popovi´c, 2015) based on the character n-gram F-score; 3. semantic textual similarity (STS) models constitute supervised methods that we model via fine-tuning multilingual pre-trained language models (i.e., XLM - R, mBERT) to predict a semantic similarity score for a pair of texts on an ordinal scale. Fluency We experiment with perplexity (PPL) and likelihood (LL) scores based on probability scores of language models trained from scratch (e.g., KenLM (Heafield, 2011)), as well as pseudolikelihood scores (PSEUDO - LL) extracted from pre-trained masked language models similarly to Salazar et al. (2020), by masking sentence tokens one by one. 4 Experiment Settings Supervised Metrics For all supervised modelbased approaches, we experiment with fine-tuning two multilingual pre-trained language models: 1. multilingual BERT, dubbed mBERT (Devlin et al., 2019)—a transformer-based model pretrained with a masked language model objective on the concatenation of monolingual Wikipedia corpora from the 104 languages with the largest Wikipedias. 2. XLM - R (Conneau et a"
2021.emnlp-main.100,P14-2029,1,0.912318,"uency Fluency is typically evaluated with the resulting classifier is evaluated on the test side model-based approaches (see fourth column of Taof the GYAFC corpus, reporting accuracy scores ble 1). Among those, the most frequent method is 1323 that of computing perplexity (PPL) under a language model. The latter is either trained from scratch on the same corpus used to train the FoST models (i.e., GYAFC) using different underlying architectures (e.g., KenLM, LSTM), or employ large pre-trained language models (e.g., GPT). A few other works train models on EN data annotated for grammaticality (Heilman et al., 2014) or linguistic acceptability (Warstadt et al., 2019) instead. Overall Systems’ overall quality (see fifth column of Table 2) is mostly evaluated using r-BLEU or by combining independently computed metrics into a single score (e.g., geometric mean - GM(.), harmonic mean - HM(.), F1(.)). Moreover, 6 out of 8 approaches that rely on combined scores do not include fluency scores in their overall evaluation. English Focus Since most of the current work on FoST and ST is in EN, prior work relies heavily on EN resources for designing automatic evaluation methods. For instance, resources for training"
2021.emnlp-main.100,2020.inlg-1.23,0,0.0721364,"Missing"
2021.emnlp-main.100,P19-1607,0,0.0434717,"Missing"
2021.emnlp-main.100,2020.emnlp-main.55,0,0.0747529,"Missing"
2021.emnlp-main.100,D19-1325,0,0.0464824,"Missing"
2021.emnlp-main.100,2021.naacl-main.337,0,0.0625398,"Missing"
2021.emnlp-main.100,2021.naacl-main.171,0,0.0678424,"Missing"
2021.emnlp-main.100,2020.wmt-1.77,0,0.027738,"imension, along and is now confirmed in a multilingual setting. with leading metrics from the other dimensions: This trend might be explained by chrF’s ability to XLM - R formality regression models, chr F and XLM match spelling errors within words via character n- R pseudo-likelihood. r- BLEU gets 30 out of 40 grams. XLM - R trained on STS with zero-shot trans- comparisons correct while the other metrics get 1328 25, 22, and 19 respectively. This indicates that r- BLEU correlates with human judgments better at the corpus-level than at the sentence-level, as in machine translation evaluation (Mathur et al., 2020). We caution that these results are not definitive but rather suggestive of the best performing metric, given the ideal evaluation would be a larger number of systems with which to perform a rank correlation. The complete analysis for each language is in Appendix B. 4. System-level Ranking chrF and XLM - R are the best metrics using a pairwise comparison evaluation. However, an ideal evaluation would be to have a large number of systems with which to draw reliable correlations. 6 We view this work as a strong point of departure for future investigations of ST evaluation. Our work first calls f"
2021.emnlp-main.100,N19-1049,0,0.0345068,"Missing"
2021.emnlp-main.100,D19-5557,0,0.0211767,"treats it as a regression task to better mirror human evaluation. • Our analysis code and meta-evaluation files with system outputs are made public to facilitate further work in developing better automatic metrics for ST: https://github.com/ Elbria/xformal-FoST-meta. 2 2.1 Background Limitations of Automatic Evaluation Recent work highlights the need for research to improve evaluation practices for ST along multiple directions. Not only does ST lack standardized evaluation practices (Yamshchikov et al., 2021), but commonly used methods have major drawbacks which hamper progress in this field. Pang (2019) and Pang and Gimpel (2019) show that the most widely adopted automatic metric, BLEU, can be gamed. They observe that untransferred text achieves the highest BLEU score for the task of sentiment transfer, questioning complex models’ ability to surpass this trivial baseline. Mir et al. (2019) discuss the inherent trade-off between ST evaluation aspects and propose that models are evaluated at specific points of their trade-off plots. Tikhonov et al. (2019) argue that, despite their cost, human-written references are needed for future experiments with style transfer. They also show that comparin"
2021.emnlp-main.100,L16-1147,0,0.0329103,"stics on the training data used for supervised and unsupervised models across the 3 ST evaluation aspects. For datasets that are only available for EN, we use the already available machine translated resources for STS 10 and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service11 (with reported BLEU scores of 37.16 (BR - 33.79 (FR), and 32.67 (IT)).12 The KenLM models for all the languages are trained on 1M randomly sampled sentences from the OpenSubtitles dataset (Lison and Tiedemann, 2016). PT ), 5 Experimental Results We analyze the results of comparing the outputs from the several automatic metrics to their humangenerated counterparts for formality style transfer (§5.1), meaning preservation (§5.2), fluency (§5.3) via conducting segment-level analysis—and then, turn into analyzing system-level rankings to evaluation overall task success (§5.4). 5.1 Formality Transfer Metrics The field is divided on the best way to evaluate the style dimension – formality in our case. Practitioners use either a binary approach (is the new sentence formal or informal?) or a regression approach"
2021.emnlp-main.100,P02-1040,0,0.122269,"FoST evaluation aspects described below, we cover a broad spectrum of approaches that range from dedicated models for the tasks at hand to more lightweight methods relying on unsupervised approaches and automated metrics. Formality We benchmark model-based approaches that fine-tune multilingual pre-trained language models (i.e., XLM - R, mBERT), where the task of formality detection is modeled either as a binary classification task (i.e., formal vs. informal), or as a regression task that predicts different formality levels on an ordinal scale. Meaning Preservation We evaluate the BLEU score (Papineni et al., 2002) of the system output compared to the reference rewrite (r-BLEU) since it is the dominant metric in prior work. Prior reviews of meaning preservation metrics for paraphrase and sentiment ST tasks in EN (Yamshchikov et al., 2021) cover n-gram metrics and embeddingbased approaches. We consider three additional metric classes to compare system outputs with inputs, as human annotators do: 1. n-gram based metrics include: s-BLEU (selfBLEU that compares system outputs with their inputs as opposed to references, i.e., r-BLEU), METEOR (Banerjee and Lavie, 2005) based on the harmonic mean of unigram pr"
2021.emnlp-main.100,Q16-1005,1,0.888573,"Missing"
2021.emnlp-main.100,D14-1162,0,0.0860249,"dam optimizer (Kingma and Ba, 2015), a batch size of 32, and a learning rate of 5e−5 for 3 and 5 epochs for classification and regression tasks, respectively. We perform a grid search on held-out validation sets over learning rate with values: 2e−3, 2e−4, 2e−5, and 5e−5 and over number of epochs with values: 3, 5, and 8. 2. embedding-based methods fall under the category of unsupervised evaluation approaches that rely on either contextual word representations extracted from pre-trained language models or non-contextual pre-trained word embeddings (e.g., word2vec (Mikolov et al., 2013); Glove (Pennington et al., 2014)). For the former, we use BERT-score (Zhang et al., 2020a) which computes the similarity Cross-lingual Transfer For supervised modelbetween each output token and each reference based methods that rely on the availability of token based on BERT contextual embeddings. human-annotated instances to train dedicated modFor the latter, we experiment with two simiels for specific tasks, we experiment with three larity metrics: the first is the cosine distance between the sentence-level feature represen- standard cross-lingual transfer approaches (e.g., Hu et al. (2020)): 1. ZERO - SHOT trains a single"
2021.emnlp-main.100,W15-3049,0,0.0734115,"Missing"
2021.emnlp-main.100,W18-6319,0,0.0164979,"Details on training data used for model-based metrics across the three ST evaluation aspects. on the EN training data and evaluates it on the original test data for each language; 2. TRANSLATE TRAIN uses machine translation ( MT ) to obtain training data in each language through translating the EN training set—and trains independent systems for each language; 3. TRANSLATE - TEST trains a single model on the EN training data and evaluates it on the test data that are translated into EN using MT . Unsupervised Metrics For meaning preservation metrics, we use the open-sourced implementations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR; Popovi´c (2015) for chrF.3,4,5 For BERT-score we use the implementation of Zhang et al. (2020a);6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings.7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudolikelihood.8 PPL and LL scores are extracted from a 5-gram KenLM model (Heafield, 2011).9 Training Data Table 3 presents statistics on the training data used for supervised and unsupervised models across the 3 ST evaluation aspects. For datase"
2021.emnlp-main.100,N18-1012,1,0.912261,"esses are, and how they might generalize to languages other than English. 2.3 Automatic Metrics for FoST Formality Style transfer is often evaluated using model-based approaches. The most frequent method consists of training a binary classifier on human written formal vs. informal pairs. The classifier is later used to predict the percentage of generated outputs that match the desired attribute per evaluated system—the system with the highest percentage is considered the best performing with respect to style. Across methods, the corpus used to train the classifier is the GYAFC parallelcorpus (Rao and Tetreault, 2018) consisting of 1 The complete list is hosted at: https://github. 105K parallel informal-formal human-generated com/fuzhenxin/Style-Transfer-in-Text excerpts. This corpus is curated for FoST in EN, 1322 We systematically review automatic evaluation practices in ST with formality as a case study. We select FoST for this work since it is one of the most frequently studied styles (Jin et al., 2020) and there is human annotated data including human references available for these evaluations (Rao and Tetreault, 2018; Briakou et al., 2021b). Tables 1 and 2 summarize evaluation details for all FoST me"
2021.emnlp-main.100,2020.acl-main.240,0,0.119895,"es, and chrF (Popovi´c, 2015) based on the character n-gram F-score; 3. semantic textual similarity (STS) models constitute supervised methods that we model via fine-tuning multilingual pre-trained language models (i.e., XLM - R, mBERT) to predict a semantic similarity score for a pair of texts on an ordinal scale. Fluency We experiment with perplexity (PPL) and likelihood (LL) scores based on probability scores of language models trained from scratch (e.g., KenLM (Heafield, 2011)), as well as pseudolikelihood scores (PSEUDO - LL) extracted from pre-trained masked language models similarly to Salazar et al. (2020), by masking sentence tokens one by one. 4 Experiment Settings Supervised Metrics For all supervised modelbased approaches, we experiment with fine-tuning two multilingual pre-trained language models: 1. multilingual BERT, dubbed mBERT (Devlin et al., 2019)—a transformer-based model pretrained with a masked language model objective on the concatenation of monolingual Wikipedia corpora from the 104 languages with the largest Wikipedias. 2. XLM - R (Conneau et al., 2020)—a transformer-based masked language model trained on 100 languages using monolingual CommonCrawl data. All models are based on"
2021.emnlp-main.100,D19-1499,0,0.0385034,"Missing"
2021.emnlp-main.100,D19-1365,0,0.034082,"Missing"
2021.emnlp-main.100,2020.coling-main.203,0,0.0808187,"Missing"
2021.emnlp-main.100,Q19-1040,0,0.0143541,"ting classifier is evaluated on the test side model-based approaches (see fourth column of Taof the GYAFC corpus, reporting accuracy scores ble 1). Among those, the most frequent method is 1323 that of computing perplexity (PPL) under a language model. The latter is either trained from scratch on the same corpus used to train the FoST models (i.e., GYAFC) using different underlying architectures (e.g., KenLM, LSTM), or employ large pre-trained language models (e.g., GPT). A few other works train models on EN data annotated for grammaticality (Heilman et al., 2014) or linguistic acceptability (Warstadt et al., 2019) instead. Overall Systems’ overall quality (see fifth column of Table 2) is mostly evaluated using r-BLEU or by combining independently computed metrics into a single score (e.g., geometric mean - GM(.), harmonic mean - HM(.), F1(.)). Moreover, 6 out of 8 approaches that rely on combined scores do not include fluency scores in their overall evaluation. English Focus Since most of the current work on FoST and ST is in EN, prior work relies heavily on EN resources for designing automatic evaluation methods. For instance, resources for training stylistic classifiers or regression models are not a"
2021.emnlp-main.100,2020.emnlp-demos.6,0,0.0279677,"Missing"
2021.emnlp-main.100,2020.acl-main.294,0,0.106522,"learning rate of 5e−5 for 3 and 5 epochs for classification and regression tasks, respectively. We perform a grid search on held-out validation sets over learning rate with values: 2e−3, 2e−4, 2e−5, and 5e−5 and over number of epochs with values: 3, 5, and 8. 2. embedding-based methods fall under the category of unsupervised evaluation approaches that rely on either contextual word representations extracted from pre-trained language models or non-contextual pre-trained word embeddings (e.g., word2vec (Mikolov et al., 2013); Glove (Pennington et al., 2014)). For the former, we use BERT-score (Zhang et al., 2020a) which computes the similarity Cross-lingual Transfer For supervised modelbetween each output token and each reference based methods that rely on the availability of token based on BERT contextual embeddings. human-annotated instances to train dedicated modFor the latter, we experiment with two simiels for specific tasks, we experiment with three larity metrics: the first is the cosine distance between the sentence-level feature represen- standard cross-lingual transfer approaches (e.g., Hu et al. (2020)): 1. ZERO - SHOT trains a single model tations of the compared texts extracted via 2 ave"
2021.emnlp-main.100,2020.acl-main.639,0,0.042812,"Missing"
2021.emnlp-main.477,P18-2049,0,0.0194817,"n- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in parallel (Gu et al., 2018; ity. In NMT, morphologically-aware segmentation van den Oord et al., 2018; Ma et al., 2019) or by iter- is also useful when translating from or into moratively editing an initial sequence (Lee et al., 2018; phologically complex languages (Huck et al., 2017; Ghazvininejad et al., 2019). Architectures differ Ataman and Federico, 2018; Banerjee and Bhatwith the nature of edit operations: the Levenshtein tacharyya, 2018). Tamchyna et al. (2017) propose Transformer (Gu et al., 2019) relies on insertion to overcome data sparsity caused by inflection by and deletion, while EDITOR (Xu and Carpuat, 2021) training NMT models to predict the lemma form and uses insertion and reposition (where each input to- morphological tag of each target word. Different ken can be repositioned or deleted). Edit-based non- from prior work, we incorporate grammatical and autoregressive generation provides a natural way morphological knowledge in an"
2021.emnlp-main.477,W18-1207,0,0.0569005,"Missing"
2021.emnlp-main.477,W19-5301,0,0.0397256,"Missing"
2021.emnlp-main.477,2021.eacl-main.271,0,0.375674,"Autoregressive NMT with Constraints Terminology constraints can be incorporated in autoregressive NMT models via 1) constrained decoding where constraint terms are incorporated in the beam search algorithm (Hokamp and Liu, 2017; Post and Vilar, 2018), or 2) constrained training where NMT models are trained to incorporate constraints using synthetic parallel data augmented with constraint terms on the source side (Song et al., 2019; Dinu et al., 2019). These approaches all assume that the constraints are provided in the correct inflected forms and can be directly copied to the target sentence. Bergmanis and Pinnis (2021) extended the constrained training approach of Dinu et al. (2019) to incorporate lemma-form constraints in an end-to-end way – the inflected form of the lemma constraints are predicted jointly during translation. This approach requires a dedicated NMT model architecture to integrate constraints as additional inputs to the encoder, and learns inflection solely from the parallel data. By contrast, our approach can be applied to multiple NMT architectures and uses linguistically motivated rule that generalize better to rare and unseen terms. can be put into the initial sequence and edited to prod"
2021.emnlp-main.477,E14-1061,0,0.0287022,"rvised data (Cotterell et al., 2018; Kementchedjhieva et al., 2018). The inflection module in our framework differs from those for the context-based inflection task in that it requires cross-lingual context-based inflection – it predicts the inflected form of a target lemma based only on the source language context. Morphologically-Aware Translation In phraseNon-Autoregressive NMT with Constraints In- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in parallel (Gu et al., 2018; ity. In NMT, morphologically-aware segmentation van den Oord et al., 2018; Ma et al., 2019) or by iter- is also useful when translating from or into moratively editing an initial sequence (Lee et al., 2018; phologically complex languages (Huck et al., 2017; Ghazvininejad et al., 2019). Architectures differ Ataman and Federico, 2018; Banerjee and Bhatwith the nature of edit operations: the Levenshtein tacharyya, 2018). Tamchyna et al. (2017) propose Transformer (Gu et al., 2019) relies on insertion to overcome data spa"
2021.emnlp-main.477,K18-3001,0,0.058487,"Missing"
2021.emnlp-main.477,K17-2001,0,0.0619393,"Missing"
2021.emnlp-main.477,N19-1423,0,0.00478504,"objects should be used in accusative case. As shown in Figure (c), “smuikas” is used in instrumental case, since it serves the instrument with which the subject performs the action. Finally, given a lemma and its morphological tag, one can look up its inflected form in a morphological dictionary. We use DEMorphy (Altinok, 2018) for German and Wiktionary5 for Lithuanian. Since most Lithuanian nouns follow a set of declension rules,6 we inflect Lithuanian nouns based on the rules for lemmas unseen in the dictionary. 3.2 Neural Inflection Module As prior work shows that BERT-style architectures (Devlin et al., 2019) can encode morphological information in their hidden representations and disambiguate morphologically ambiguous forms via contextualized encoding (Edmiston, 2020), we build the neural-based inflection module as a substitution model and base it on the encoder-decoder Transformer architecture, which embeds the source sentence through the encoder and the target lemmas through the decoder. Next, the decoder predicts the inflected form of each target word in parallel. The inflection module resembles the architecture of the conditional masked language model (CMLM) (Ghazvininejad et al., 2019) but d"
2021.emnlp-main.477,P19-1294,0,0.0604885,"new translation (MT) has proven useful to adapt terminologies without retraining the base NMT translation lexical choice to new domains (Hokamp model from scratch. This flexibility is enabled by and Liu, 2017) and to improve its consistency in a the cross-lingual nature of the inflection module, document (Ture et al., 2012). In neural MT (NMT), which predicts the inflected form of each target most prior work focuses on incorporating terms lemma based on the source context only. This in the output exactly as given, using soft (Song differs from traditional inflection models that et al., 2019; Dinu et al., 2019; Xu and Carpuat, predict the inflected forms based on pre-specified 2021) or hard constraints (Hokamp and Liu, 2017; morphological tags or monolingual target context. Post and Vilar, 2018). These approaches are Based on this framework, this paper makes the problematic when translating into morphologically following contributions: rich languages where terminology should be • We construct and release test suites to adequately inflected in the output, while it is evaluate models’ ability to inflect termi1 nology constraints for domain adaptation Code and test suites are released at https://githu"
2021.emnlp-main.477,N16-1077,0,0.0248371,"o-syntactic information of the word in a sentence (e.g. tense, case, number). Traditionally, morphological inflection as computational task is framed as predicting the inflected form of a word given its lemma and a set of morphological tags (e.g. N ; ACC ; PL represents a plural noun used in accusative case) (Cotterell et al., 2017). The task was traditionally tackled using hand-engineered finite state transducer that relies on linguistic knowledge (Koskenniemi, 1984; Kaplan and Kay, 1994), while recent work has shown impressive results by modeling it using neural sequence-to-sequence models (Faruqui et al., 2016). More recently, a context-based inflection task has been proposed where the inflected form of a lemma is predicted given the rest of the sentence as context (Cotterell et al., 2018). The stateof-the-art models for the task are neural models trained on supervised data (Cotterell et al., 2018; Kementchedjhieva et al., 2018). The inflection module in our framework differs from those for the context-based inflection task in that it requires cross-lingual context-based inflection – it predicts the inflected form of a target lemma based only on the source language context. Morphologically-Aware Tra"
2021.emnlp-main.477,D19-1633,0,0.0566882,"In phraseNon-Autoregressive NMT with Constraints In- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in parallel (Gu et al., 2018; ity. In NMT, morphologically-aware segmentation van den Oord et al., 2018; Ma et al., 2019) or by iter- is also useful when translating from or into moratively editing an initial sequence (Lee et al., 2018; phologically complex languages (Huck et al., 2017; Ghazvininejad et al., 2019). Architectures differ Ataman and Federico, 2018; Banerjee and Bhatwith the nature of edit operations: the Levenshtein tacharyya, 2018). Tamchyna et al. (2017) propose Transformer (Gu et al., 2019) relies on insertion to overcome data sparsity caused by inflection by and deletion, while EDITOR (Xu and Carpuat, 2021) training NMT models to predict the lemma form and uses insertion and reposition (where each input to- morphological tag of each target word. Different ken can be repositioned or deleted). Edit-based non- from prior work, we incorporate grammatical and autoregressive generation prov"
2021.emnlp-main.477,P17-1141,0,0.0724526,"l choice to new domains (Hokamp model from scratch. This flexibility is enabled by and Liu, 2017) and to improve its consistency in a the cross-lingual nature of the inflection module, document (Ture et al., 2012). In neural MT (NMT), which predicts the inflected form of each target most prior work focuses on incorporating terms lemma based on the source context only. This in the output exactly as given, using soft (Song differs from traditional inflection models that et al., 2019; Dinu et al., 2019; Xu and Carpuat, predict the inflected forms based on pre-specified 2021) or hard constraints (Hokamp and Liu, 2017; morphological tags or monolingual target context. Post and Vilar, 2018). These approaches are Based on this framework, this paper makes the problematic when translating into morphologically following contributions: rich languages where terminology should be • We construct and release test suites to adequately inflected in the output, while it is evaluate models’ ability to inflect termi1 nology constraints for domain adaptation Code and test suites are released at https://github. com/Izecson/terminology-translation (English-German Health) and low-resource 5902 Proceedings of the 2021 Confere"
2021.emnlp-main.477,W17-4706,0,0.0169319,"-Aware Translation In phraseNon-Autoregressive NMT with Constraints In- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in parallel (Gu et al., 2018; ity. In NMT, morphologically-aware segmentation van den Oord et al., 2018; Ma et al., 2019) or by iter- is also useful when translating from or into moratively editing an initial sequence (Lee et al., 2018; phologically complex languages (Huck et al., 2017; Ghazvininejad et al., 2019). Architectures differ Ataman and Federico, 2018; Banerjee and Bhatwith the nature of edit operations: the Levenshtein tacharyya, 2018). Tamchyna et al. (2017) propose Transformer (Gu et al., 2019) relies on insertion to overcome data sparsity caused by inflection by and deletion, while EDITOR (Xu and Carpuat, 2021) training NMT models to predict the lemma form and uses insertion and reposition (where each input to- morphological tag of each target word. Different ken can be repositioned or deleted). Edit-based non- from prior work, we incorporate grammatical and a"
2021.emnlp-main.477,J94-3001,0,0.255176,"phological Inflection Morphological inflection is the process of alternating the morphological form of a lexeme that adds morpho-syntactic information of the word in a sentence (e.g. tense, case, number). Traditionally, morphological inflection as computational task is framed as predicting the inflected form of a word given its lemma and a set of morphological tags (e.g. N ; ACC ; PL represents a plural noun used in accusative case) (Cotterell et al., 2017). The task was traditionally tackled using hand-engineered finite state transducer that relies on linguistic knowledge (Koskenniemi, 1984; Kaplan and Kay, 1994), while recent work has shown impressive results by modeling it using neural sequence-to-sequence models (Faruqui et al., 2016). More recently, a context-based inflection task has been proposed where the inflected form of a lemma is predicted given the rest of the sentence as context (Cotterell et al., 2018). The stateof-the-art models for the task are neural models trained on supervised data (Cotterell et al., 2018; Kementchedjhieva et al., 2018). The inflection module in our framework differs from those for the context-based inflection task in that it requires cross-lingual context-based inf"
2021.emnlp-main.477,K18-3011,0,0.0261302,"erell et al., 2017). The task was traditionally tackled using hand-engineered finite state transducer that relies on linguistic knowledge (Koskenniemi, 1984; Kaplan and Kay, 1994), while recent work has shown impressive results by modeling it using neural sequence-to-sequence models (Faruqui et al., 2016). More recently, a context-based inflection task has been proposed where the inflected form of a lemma is predicted given the rest of the sentence as context (Cotterell et al., 2018). The stateof-the-art models for the task are neural models trained on supervised data (Cotterell et al., 2018; Kementchedjhieva et al., 2018). The inflection module in our framework differs from those for the context-based inflection task in that it requires cross-lingual context-based inflection – it predicts the inflected form of a target lemma based only on the source language context. Morphologically-Aware Translation In phraseNon-Autoregressive NMT with Constraints In- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in"
2021.emnlp-main.477,D16-1139,0,0.0217036,"based on the EDITOR model (Xu and Carpuat, 2021). • NAR with constraints (NAR + C) that integrates constraints as the initial sequence in EDITOR without explicit inflection. MT Models All models are based on the base Transformer (Vaswani et al., 2017).18 All models are trained with the Adam optimizer (Kingma and Ba, 2015) with initial learning rate of 0.0005 and effective batch sizes of 32k tokens for AR models and 64k tokens for NAR models for maximum 300k steps.19 We select the best checkpoint based on validation perplexity. NAR models are trained via sequence-level knowledge distillation (Kim and Rush, 2016). For decoding, we use beam search with a beam size of 4 for AR and AR with TLA, while for AR with CD we use a beam size of 20 as suggested in prior work (Post and Vilar, 2018). To enhance constraint usage in NAR models, we adopt the techniques by Susanto et al. (2020): we prohibit deletions on constraint tokens or insertions within the constraint segments. Neural Inflection Model Its synthetic training data is derived from the MT parallel data. We first lemmatise and part-of-speech tag the target sentences using Stanza. We then randomly select adjectives, verbs, nouns, and proper nouns from e"
2021.emnlp-main.477,E03-1076,0,0.215375,"l et al., 2018). The stateof-the-art models for the task are neural models trained on supervised data (Cotterell et al., 2018; Kementchedjhieva et al., 2018). The inflection module in our framework differs from those for the context-based inflection task in that it requires cross-lingual context-based inflection – it predicts the inflected form of a target lemma based only on the source language context. Morphologically-Aware Translation In phraseNon-Autoregressive NMT with Constraints In- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in parallel (Gu et al., 2018; ity. In NMT, morphologically-aware segmentation van den Oord et al., 2018; Ma et al., 2019) or by iter- is also useful when translating from or into moratively editing an initial sequence (Lee et al., 2018; phologically complex languages (Huck et al., 2017; Ghazvininejad et al., 2019). Architectures differ Ataman and Federico, 2018; Banerjee and Bhatwith the nature of edit operations: the Levenshtein tacharyya, 2018). Tamchyna et al. ("
2021.emnlp-main.477,P84-1038,0,0.178349,"urther editing. Morphological Inflection Morphological inflection is the process of alternating the morphological form of a lexeme that adds morpho-syntactic information of the word in a sentence (e.g. tense, case, number). Traditionally, morphological inflection as computational task is framed as predicting the inflected form of a word given its lemma and a set of morphological tags (e.g. N ; ACC ; PL represents a plural noun used in accusative case) (Cotterell et al., 2017). The task was traditionally tackled using hand-engineered finite state transducer that relies on linguistic knowledge (Koskenniemi, 1984; Kaplan and Kay, 1994), while recent work has shown impressive results by modeling it using neural sequence-to-sequence models (Faruqui et al., 2016). More recently, a context-based inflection task has been proposed where the inflected form of a lemma is predicted given the rest of the sentence as context (Cotterell et al., 2018). The stateof-the-art models for the task are neural models trained on supervised data (Cotterell et al., 2018; Kementchedjhieva et al., 2018). The inflection module in our framework differs from those for the context-based inflection task in that it requires cross-li"
2021.emnlp-main.477,D18-1149,0,0.0218132,"ly on the source language context. Morphologically-Aware Translation In phraseNon-Autoregressive NMT with Constraints In- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in parallel (Gu et al., 2018; ity. In NMT, morphologically-aware segmentation van den Oord et al., 2018; Ma et al., 2019) or by iter- is also useful when translating from or into moratively editing an initial sequence (Lee et al., 2018; phologically complex languages (Huck et al., 2017; Ghazvininejad et al., 2019). Architectures differ Ataman and Federico, 2018; Banerjee and Bhatwith the nature of edit operations: the Levenshtein tacharyya, 2018). Tamchyna et al. (2017) propose Transformer (Gu et al., 2019) relies on insertion to overcome data sparsity caused by inflection by and deletion, while EDITOR (Xu and Carpuat, 2021) training NMT models to predict the lemma form and uses insertion and reposition (where each input to- morphological tag of each target word. Different ken can be repositioned or deleted). Edit-based non"
2021.emnlp-main.477,D19-1437,0,0.015377,"hat it requires cross-lingual context-based inflection – it predicts the inflected form of a target lemma based only on the source language context. Morphologically-Aware Translation In phraseNon-Autoregressive NMT with Constraints In- based MT, modeling morphological compounds on stead of generating the output sequence incre- the source (Koehn and Knight, 2003) and target mentally from left to right, non-autoregressive sides (Cap et al., 2014) improves translation qualNMT generates tokens in parallel (Gu et al., 2018; ity. In NMT, morphologically-aware segmentation van den Oord et al., 2018; Ma et al., 2019) or by iter- is also useful when translating from or into moratively editing an initial sequence (Lee et al., 2018; phologically complex languages (Huck et al., 2017; Ghazvininejad et al., 2019). Architectures differ Ataman and Federico, 2018; Banerjee and Bhatwith the nature of edit operations: the Levenshtein tacharyya, 2018). Tamchyna et al. (2017) propose Transformer (Gu et al., 2019) relies on insertion to overcome data sparsity caused by inflection by and deletion, while EDITOR (Xu and Carpuat, 2021) training NMT models to predict the lemma form and uses insertion and reposition (where e"
2021.emnlp-main.477,N18-1031,0,0.0327342,"Missing"
2021.emnlp-main.477,W18-6319,0,0.0416337,"uite. 5 Experimental Settings Training Data For English→German (En-De), we use the training corpora from WMT14 (Bojar et al., 2014) and newstest2013 for validation. For English→Lithuanian, we use the training data from WMT19 (Barrault et al., 2019) and newsdev2019 as the validation set. For preprocessing, we apply normalization, tokenization, true-casing, and BPE (Sennrich et al., 2016).17 Baselines We compare our model with the following baselines: • Auto-Regressive (AR) baseline without integrating terminology constraints. • AR with Constrained Decoding (CD) to incorporate hard constraints (Post, 2018). • AR with Target Lemma Annotation (TLA) that integrates lemma constraints as an additional input stream on the source side (Bergmanis and Pinnis, 2021). • Non-AutoRegressive (NAR) baseline based on the EDITOR model (Xu and Carpuat, 2021). • NAR with constraints (NAR + C) that integrates constraints as the initial sequence in EDITOR without explicit inflection. MT Models All models are based on the base Transformer (Vaswani et al., 2017).18 All models are trained with the Adam optimizer (Kingma and Ba, 2015) with initial learning rate of 0.0005 and effective batch sizes of 32k tokens for AR m"
2021.emnlp-main.477,N18-1119,0,0.305236,"enabled by and Liu, 2017) and to improve its consistency in a the cross-lingual nature of the inflection module, document (Ture et al., 2012). In neural MT (NMT), which predicts the inflected form of each target most prior work focuses on incorporating terms lemma based on the source context only. This in the output exactly as given, using soft (Song differs from traditional inflection models that et al., 2019; Dinu et al., 2019; Xu and Carpuat, predict the inflected forms based on pre-specified 2021) or hard constraints (Hokamp and Liu, 2017; morphological tags or monolingual target context. Post and Vilar, 2018). These approaches are Based on this framework, this paper makes the problematic when translating into morphologically following contributions: rich languages where terminology should be • We construct and release test suites to adequately inflected in the output, while it is evaluate models’ ability to inflect termi1 nology constraints for domain adaptation Code and test suites are released at https://github. com/Izecson/terminology-translation (English-German Health) and low-resource 5902 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5902–5914"
2021.emnlp-main.477,E17-2025,0,0.0671461,"Missing"
2021.emnlp-main.477,P16-1162,0,0.0602565,"cc/ 15 https://lithuanian.english-dictionary. help 5906 annotated with its target translation to encourage consistent translation of keyphrases within a document.16 Table 2 shows the number of sentences and constraints in each test suite. 5 Experimental Settings Training Data For English→German (En-De), we use the training corpora from WMT14 (Bojar et al., 2014) and newstest2013 for validation. For English→Lithuanian, we use the training data from WMT19 (Barrault et al., 2019) and newsdev2019 as the validation set. For preprocessing, we apply normalization, tokenization, true-casing, and BPE (Sennrich et al., 2016).17 Baselines We compare our model with the following baselines: • Auto-Regressive (AR) baseline without integrating terminology constraints. • AR with Constrained Decoding (CD) to incorporate hard constraints (Post, 2018). • AR with Target Lemma Annotation (TLA) that integrates lemma constraints as an additional input stream on the source side (Bergmanis and Pinnis, 2021). • Non-AutoRegressive (NAR) baseline based on the EDITOR model (Xu and Carpuat, 2021). • NAR with constraints (NAR + C) that integrates constraints as the initial sequence in EDITOR without explicit inflection. MT Models All"
2021.emnlp-main.477,N19-1044,0,0.0180296,"ssive translation, and outperforms the existing TLA approach for inflecting terminology translation. We open-source the code to facilitate replication and extensions. MT 2 Background Autoregressive NMT with Constraints Terminology constraints can be incorporated in autoregressive NMT models via 1) constrained decoding where constraint terms are incorporated in the beam search algorithm (Hokamp and Liu, 2017; Post and Vilar, 2018), or 2) constrained training where NMT models are trained to incorporate constraints using synthetic parallel data augmented with constraint terms on the source side (Song et al., 2019; Dinu et al., 2019). These approaches all assume that the constraints are provided in the correct inflected forms and can be directly copied to the target sentence. Bergmanis and Pinnis (2021) extended the constrained training approach of Dinu et al. (2019) to incorporate lemma-form constraints in an end-to-end way – the inflected form of the lemma constraints are predicted jointly during translation. This approach requires a dedicated NMT model architecture to integrate constraints as additional inputs to the encoder, and learns inflection solely from the parallel data. By contrast, our appr"
2021.emnlp-main.477,2020.wmt-1.141,1,0.792673,"corporate lemma-form constraints in an end-to-end way – the inflected form of the lemma constraints are predicted jointly during translation. This approach requires a dedicated NMT model architecture to integrate constraints as additional inputs to the encoder, and learns inflection solely from the parallel data. By contrast, our approach can be applied to multiple NMT architectures and uses linguistically motivated rule that generalize better to rare and unseen terms. can be put into the initial sequence and edited to produce the final translation (Susanto et al., 2020; Xu and Carpuat, 2021; Wan et al., 2020). Our approach can augment this family of techniques by inflecting constraints before they are used for further editing. Morphological Inflection Morphological inflection is the process of alternating the morphological form of a lexeme that adds morpho-syntactic information of the word in a sentence (e.g. tense, case, number). Traditionally, morphological inflection as computational task is framed as predicting the inflected form of a word given its lemma and a set of morphological tags (e.g. N ; ACC ; PL represents a plural noun used in accusative case) (Cotterell et al., 2017). The task was"
2021.emnlp-main.477,2020.acl-main.325,0,0.0715152,"raining approach of Dinu et al. (2019) to incorporate lemma-form constraints in an end-to-end way – the inflected form of the lemma constraints are predicted jointly during translation. This approach requires a dedicated NMT model architecture to integrate constraints as additional inputs to the encoder, and learns inflection solely from the parallel data. By contrast, our approach can be applied to multiple NMT architectures and uses linguistically motivated rule that generalize better to rare and unseen terms. can be put into the initial sequence and edited to produce the final translation (Susanto et al., 2020; Xu and Carpuat, 2021; Wan et al., 2020). Our approach can augment this family of techniques by inflecting constraints before they are used for further editing. Morphological Inflection Morphological inflection is the process of alternating the morphological form of a lexeme that adds morpho-syntactic information of the word in a sentence (e.g. tense, case, number). Traditionally, morphological inflection as computational task is framed as predicting the inflected form of a word given its lemma and a set of morphological tags (e.g. N ; ACC ; PL represents a plural noun used in accusative case"
2021.emnlp-main.477,W17-4704,0,0.0270947,"Missing"
2021.emnlp-main.477,N12-1046,0,0.0352574,"strained proach with lower training costs. NMT techniques. Compared with TLA, this framework is more flexible, as it can be applied to 1 Introduction diverse types of NMT architectures and inflection Incorporating terminology constraints in machine modules, and facilitates fast adaptation to new translation (MT) has proven useful to adapt terminologies without retraining the base NMT translation lexical choice to new domains (Hokamp model from scratch. This flexibility is enabled by and Liu, 2017) and to improve its consistency in a the cross-lingual nature of the inflection module, document (Ture et al., 2012). In neural MT (NMT), which predicts the inflected form of each target most prior work focuses on incorporating terms lemma based on the source context only. This in the output exactly as given, using soft (Song differs from traditional inflection models that et al., 2019; Dinu et al., 2019; Xu and Carpuat, predict the inflected forms based on pre-specified 2021) or hard constraints (Hokamp and Liu, 2017; morphological tags or monolingual target context. Post and Vilar, 2018). These approaches are Based on this framework, this paper makes the problematic when translating into morphologically f"
2021.eval4nlp-1.22,2020.acl-main.747,0,0.0248718,"roduces the probability that the token yt belongs to the equivalent class. For sentencelevel prediction, the model uses margin-loss and for token-level prediction, it uses cross-entropy loss of all tokens. The word-level evaluation on this model found that it outperforms Random Baseline across all metrics. Therefore, this model proves that we can benefit from training even with noisy word-level labels. We can map this task to identifying the error in the word-level QE by marking all divergences as errors. We made a small change to the original Divergent mBERT model by fine-tuning XLM-Roberta (Conneau et al., 2020) rather than mBERT, and keeping the rest of the model architecture, loss definition, and training data unchanged. As a result, this model is trained on French-English sentence pairs, where positive examples of equivalence are 2.2 Divergent mBERT drawn from bitext with a filtering step to ensure Briakou and Carpuat (2020) introduced the Dithat they are not noisy, and negative samples are vergent mBERT model which is a BERT-based automatically generated by corrupting the positive 1 https://github.com/marcotcr/lime samples to introduce meaning mismatches (e.g., 231 by deleting dependency subtrees"
2021.eval4nlp-1.22,2021.eval4nlp-1.17,0,0.434964,"s an average gain of 0.18 for AUC, 0.071 for AP and 0.12 for Recall at Top-K score over the average of all languages’ baseline scores. Finally, for sentence-level scores, it has achieved an improvement of 0.36 for Et-En, 0.359 for Ro-En, 0.271 for De-Zh, and 0.06 for Ru-De compared to the average of all baseline models for Pearson’s correlation. Quality estimation (QE) is the task of predicting the quality of the machine translation (MT) output without reference translation. Predictions can be done at different levels of granularity, such as sentences or words. The explainable QE shared task (Fomicheva et al., 2021a) proposes to frame the identification of translation errors as an explainable QE task, where sentence-level quality judgments are explained by highlighting the words responsible for errors in the MT hypothesis. Given a source sentence and an MT hypothesis, systems are thus asked to provide word-level judgments of translation quality in addition to sentence-level judgments. Our submission builds on state-of-theart sentence-level QE models, MonoTransQuest (Ranasinghe et al., 2020a,b). As suggested 2 Approach by the organizers, we rely on the LIME explanation model (Ribeiro et al., 2016) to obt"
2021.eval4nlp-1.22,W18-6451,0,0.035451,"Missing"
2021.eval4nlp-1.22,N18-1136,1,0.891086,"Missing"
2021.eval4nlp-1.22,W19-5406,0,0.0272127,"picked, and then the synthetic divergent examples were generated. The synthetic data was generated similar to the original model’s synthetic data generation process which is: subtree deletion by deleting a randomly selected subtree in the dependency parse of the English sentence, or French words aligned to English words in that subtree, Phrase Replacement by substituting random source or target sequences by another sequence of words with matching POS tags and lexical substitution by substituting English words with hypernyms or hyponyms from WordNet. Ensembling Method We adopt the approach of Kepler et al. (2019) for building ensemble models for word-level quality estimation, which simply averages the predictions of the ensemble components. While their ensemble had five models, we average the predictions of the two models above, either at the sentence or word level. Given a source sentence (src) and the machine translation hypothesis (mt), Divergent mBERT and MonoTransQuest-LIME produce word-level scores for each word in the MT hypothesis. These are averaged to produce the final wordlevel score. The same process is used to combine sentence-level predictions. The overall system architecture is shown in"
2021.eval4nlp-1.22,2020.wmt-1.122,0,0.691865,"dictions can be done at different levels of granularity, such as sentences or words. The explainable QE shared task (Fomicheva et al., 2021a) proposes to frame the identification of translation errors as an explainable QE task, where sentence-level quality judgments are explained by highlighting the words responsible for errors in the MT hypothesis. Given a source sentence and an MT hypothesis, systems are thus asked to provide word-level judgments of translation quality in addition to sentence-level judgments. Our submission builds on state-of-theart sentence-level QE models, MonoTransQuest (Ranasinghe et al., 2020a,b). As suggested 2 Approach by the organizers, we rely on the LIME explanation model (Ribeiro et al., 2016) to obtain word-level We first describe the two components of our ensemprediction from the MonoTransQuest model’s ble and then explain how they are combined. 230 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 230–237 c November 10, 2021. 2021 Association for Computational Linguistics 2.1 MonoTransQuest-LIME Model The first ensemble component is based on one of the baselines provided by the organizers. It uses MonoTransQuest, the state-"
2021.eval4nlp-1.22,2020.coling-main.445,0,0.150701,"dictions can be done at different levels of granularity, such as sentences or words. The explainable QE shared task (Fomicheva et al., 2021a) proposes to frame the identification of translation errors as an explainable QE task, where sentence-level quality judgments are explained by highlighting the words responsible for errors in the MT hypothesis. Given a source sentence and an MT hypothesis, systems are thus asked to provide word-level judgments of translation quality in addition to sentence-level judgments. Our submission builds on state-of-theart sentence-level QE models, MonoTransQuest (Ranasinghe et al., 2020a,b). As suggested 2 Approach by the organizers, we rely on the LIME explanation model (Ribeiro et al., 2016) to obtain word-level We first describe the two components of our ensemprediction from the MonoTransQuest model’s ble and then explain how they are combined. 230 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 230–237 c November 10, 2021. 2021 Association for Computational Linguistics 2.1 MonoTransQuest-LIME Model The first ensemble component is based on one of the baselines provided by the organizers. It uses MonoTransQuest, the state-"
2021.findings-acl.330,S16-1081,0,0.056727,"Missing"
2021.findings-acl.330,P14-2029,0,0.0613734,"Missing"
2021.findings-acl.330,W08-0909,0,0.116571,"Missing"
2021.findings-acl.330,2020.acl-main.709,0,0.223435,"e the desired reading grade level is given as input, while providing fine-grained control on simplification by incorporating lexical complexity signals into our model. We adopt a nonautoregressive sequence-to-sequence model (Xu and Carpuat, 2020) that iteratively refines an input sequence to reach the desired degree of simplification and seamlessly integrate lexical complexity. Unlike commonly used autoregressive (AR) models for simplification (Specia, 2010; Nisioi et al., 2017; Zhang and Lapata, 2017; Wubben et al., 2012; Scarton and Specia, 2018; Nishihara et al., 2019; Martin et al., 2020; Jiang et al., 2020, among others), our model relies on explicit edit 3757 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3757–3769 August 1–6, 2021. ©2021 Association for Computational Linguistics operations. It therefore has the potential of modeling the simplification process more directly than AR models which need to learn to copy operations implicitly. Unlike existing edit-based models for simplification which rely on pipelines of independently trained components (Alva-Manchego et al., 2017; Malmi et al., 2019; Mallinson et al., 2020), our model is trained end-to-end via i"
2021.findings-acl.330,2020.findings-emnlp.111,0,0.202424,"ishihara et al., 2019; Martin et al., 2020; Jiang et al., 2020, among others), our model relies on explicit edit 3757 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3757–3769 August 1–6, 2021. ©2021 Association for Computational Linguistics operations. It therefore has the potential of modeling the simplification process more directly than AR models which need to learn to copy operations implicitly. Unlike existing edit-based models for simplification which rely on pipelines of independently trained components (Alva-Manchego et al., 2017; Malmi et al., 2019; Mallinson et al., 2020), our model is trained end-to-end via imitation learning and thus learns to apply sequences of edits to transform the original source into the final simplified text. Furthermore, our approach does not require a custom architecture for simplification: it repurposes a non-autoregressive (NAR) model introduced for Machine Translation (MT) and can seamlessly incorporate lexical complexity information derived from data statistics in the initial sequence to be refined. Based on extensive experiments on the Newsela English corpus, we show that our approach generates simplified outputs that match the"
2021.findings-acl.330,D19-1510,0,0.0980467,"and Specia, 2018; Nishihara et al., 2019; Martin et al., 2020; Jiang et al., 2020, among others), our model relies on explicit edit 3757 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3757–3769 August 1–6, 2021. ©2021 Association for Computational Linguistics operations. It therefore has the potential of modeling the simplification process more directly than AR models which need to learn to copy operations implicitly. Unlike existing edit-based models for simplification which rely on pipelines of independently trained components (Alva-Manchego et al., 2017; Malmi et al., 2019; Mallinson et al., 2020), our model is trained end-to-end via imitation learning and thus learns to apply sequences of edits to transform the original source into the final simplified text. Furthermore, our approach does not require a custom architecture for simplification: it repurposes a non-autoregressive (NAR) model introduced for Machine Translation (MT) and can seamlessly incorporate lexical complexity information derived from data statistics in the initial sequence to be refined. Based on extensive experiments on the Newsela English corpus, we show that our approach generates simplifie"
2021.findings-acl.330,2020.lrec-1.577,0,0.0619681,"Missing"
2021.findings-acl.330,P19-2036,0,0.108092,"implification tools for deaf or hard-ofhearing users also show that they prefer lexical simplification to be applied on-demand (Alonzo et al., 2020). Yet, research in TS has mostly focused on developing models that generate a generic simplified output for a given source text (Xu et al., 2015; Zhang and Lapata, 2017; Alva-Manchego et al., 2020). We contrast this Generic TS with Controllable TS which specifies desired output properties. Prior work has addressed Controllable TS for either high-level properties, such as the target reading grade level for the entire text (Scarton and Specia, 2018; Nishihara et al., 2019), or low-level properties, such as the compression ratio or the nature of the simplification operation to use (Mallinson and Lapata, 2019; Martin et al., 2020; Maddela et al., 2020). Specifying the desired reading grade level might be more intuitive for lay users. However, it provides only weak control over the nature of simplification. As illustrated in Table 1, simplifying text to different grade levels results in diverse edits. To rewrite the grade 10 original for grade 5, the complex text is split into two sentences and paraphrased. When simplifying for grade 3, phrases are further simplif"
2021.findings-acl.330,P19-1607,0,0.167276,"urce-target attention mechanism. Lexical complexity signals We automatically identify the source words that are too complex for the target grade and delete them from the initial sequence to be refined by EDITOR. This simple strategy provides finer-grained guidance to the simplification process than the sequence-level sideconstraint, while leaving the EDITOR model the flexibility to rewrite the output without constraints. We quantify the relatedness between each vocabulary word (w) and grade-level (g) using their Pointwise Mutual Information (PMI) in the newsela corpus (Nishihara et al., 2019; Kajiwara, 2019): P M I(w, g) = log p(w|g) p(w) (1) Here, p(w|g) is the probability that word w appears in sentences of grade level g and p(w) is the probability of word w in the entire training corpus. While the desired grade level gt is known in the task, we automatically predict the complexity gs of each source sentence si using the Automatic Readability Index (ARI; Senter and Smith (1967)). The initial decoding sequence sˆi takes the source sequence and deletes all words that are strongly related to the source grade level and unlikely to be found in text of the target grade level, with the 3758 exception"
2021.findings-acl.330,P17-2014,0,0.017031,"3, phrases are further simplified, and content is entirely deleted. In this work, we adopt the intuitive framing for Controllable TS where the desired reading grade level is given as input, while providing fine-grained control on simplification by incorporating lexical complexity signals into our model. We adopt a nonautoregressive sequence-to-sequence model (Xu and Carpuat, 2020) that iteratively refines an input sequence to reach the desired degree of simplification and seamlessly integrate lexical complexity. Unlike commonly used autoregressive (AR) models for simplification (Specia, 2010; Nisioi et al., 2017; Zhang and Lapata, 2017; Wubben et al., 2012; Scarton and Specia, 2018; Nishihara et al., 2019; Martin et al., 2020; Jiang et al., 2020, among others), our model relies on explicit edit 3757 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3757–3769 August 1–6, 2021. ©2021 Association for Computational Linguistics operations. It therefore has the potential of modeling the simplification process more directly than AR models which need to learn to copy operations implicitly. Unlike existing edit-based models for simplification which rely on pipelines of independ"
2021.findings-acl.330,2020.acl-main.707,0,0.0379332,"d need to generate the entire output sequence from scratch thus potentially wasting capacity in learning copying operations. Edit-based Generic TS Recent work incorporates edit operations into neural text simplifications more directly. These approaches rely on custom multi-step architectures. They first learn to tag the source token representing the type of edit operations to be performed, and then use a secondary model for in-filling new tokens or executing the edit operation. The tagging and editing model are either 3764 trained independently (Alva-Manchego et al., 2017; Malmi et al., 2019; Kumar et al., 2020; Mallinson et al., 2020) or jointly (Dong et al., 2019). By contrast, we use a single model trained end-to-end to generate sequences of edit operations to transform the entire source sequence. Lexical Complexity for TS Nishihara et al. (2019) introduced a training loss for Controllable TS that weights words that frequently appear in the sentences of a specific grade-level. By contrast, we use lexical complexity information to define the initial sequence for refinement, which does not require any change to the model architecture nor to the training process. For Generic TS, Kajiwara (2019) used"
2021.findings-acl.330,N18-1119,0,0.0137192,"valuation on Newsela-Auto: our Transformer baseline is comparable to SOTA models. level tokens as side constraints (Scarton and Specia, 2018). Mean Squared Error (MSE) between the predicted ARI grade level of the system output and the desired target grade level (Scarton and Specia, 2018; Nishihara et al., 2019). 4 Evaluation of Controllable TS 4.1 2. AR + PMI-based constraints is an AR Transformer model which incorporates lexical complexity information as hard constraints during decoding (Kajiwara, 2019): complex words are excluded from beam search using the dynamic beam allocation algorithm (Post and Vilar, 2018). While this approach was introduced for Generic TS, we adapt it to Controllable TS by defining hard constraints using the same criteria as for deleting words in initial sequences for EDITOR (Section 2). 3. AR + PMI weighted loss (Nishihara et al., 2019) is an AR Transformer model trained with a loss that weights words based on their PMI values with the desired target grade level. 3.3 Automatic Evaluation Metrics We evaluate the output of the models using the following text simplification evaluation metrics: SARI (Xu et al., 2016) measures lexical simplification based on the words that are add"
2021.findings-acl.330,P18-2113,0,0.267033,"t al., 2009). Studies of simplification tools for deaf or hard-ofhearing users also show that they prefer lexical simplification to be applied on-demand (Alonzo et al., 2020). Yet, research in TS has mostly focused on developing models that generate a generic simplified output for a given source text (Xu et al., 2015; Zhang and Lapata, 2017; Alva-Manchego et al., 2020). We contrast this Generic TS with Controllable TS which specifies desired output properties. Prior work has addressed Controllable TS for either high-level properties, such as the target reading grade level for the entire text (Scarton and Specia, 2018; Nishihara et al., 2019), or low-level properties, such as the compression ratio or the nature of the simplification operation to use (Mallinson and Lapata, 2019; Martin et al., 2020; Maddela et al., 2020). Specifying the desired reading grade level might be more intuitive for lay users. However, it provides only weak control over the nature of simplification. As illustrated in Table 1, simplifying text to different grade levels results in diverse edits. To rewrite the grade 10 original for grade 5, the complex text is split into two sentences and paraphrased. When simplifying for grade 3, ph"
2021.findings-acl.330,Q15-1021,0,0.163404,"ell as sentence splitting. 1 10 5 3 Text Tesla is a maker of electric cars, which do not need gas and can be charged by being plugged into a wall socket. Tesla cars can be charged by being plugged in, like a phone. They do not need any gas. Tesla builds cars that do not need gas. Table 1: Simplified text changes depending on the reading grade level of the target audience. The bold font highlights changes compared to the grade 10 version. Introduction Text simplification (TS) aims to automatically rewrite text so that it is easier to read. What makes text simple depends on its target audience (Xu et al., 2015): replacing complex or specialized terms with simpler synonyms might be helpful for non-native speakers (Petersen and Ostendorf, 2007; Allen, 2009) whereas restructuring text into short sentences with simple words might better match the literacy skills of children (Watanabe et al., 2009). Studies of simplification tools for deaf or hard-ofhearing users also show that they prefer lexical simplification to be applied on-demand (Alonzo et al., 2020). Yet, research in TS has mostly focused on developing models that generate a generic simplified output for a given source text (Xu et al., 2015; Zhan"
2021.findings-acl.330,Q16-1029,0,0.051172,"Missing"
2021.findings-acl.330,D17-1062,0,0.0581012,"015): replacing complex or specialized terms with simpler synonyms might be helpful for non-native speakers (Petersen and Ostendorf, 2007; Allen, 2009) whereas restructuring text into short sentences with simple words might better match the literacy skills of children (Watanabe et al., 2009). Studies of simplification tools for deaf or hard-ofhearing users also show that they prefer lexical simplification to be applied on-demand (Alonzo et al., 2020). Yet, research in TS has mostly focused on developing models that generate a generic simplified output for a given source text (Xu et al., 2015; Zhang and Lapata, 2017; Alva-Manchego et al., 2020). We contrast this Generic TS with Controllable TS which specifies desired output properties. Prior work has addressed Controllable TS for either high-level properties, such as the target reading grade level for the entire text (Scarton and Specia, 2018; Nishihara et al., 2019), or low-level properties, such as the compression ratio or the nature of the simplification operation to use (Mallinson and Lapata, 2019; Martin et al., 2020; Maddela et al., 2020). Specifying the desired reading grade level might be more intuitive for lay users. However, it provides only we"
2021.findings-acl.330,2020.acl-main.325,0,0.0194939,"19) used complex words as negative constrained for decoding with an autoregressive model. By contrast our approach provides more flexibility to the model which results in better outputs in practice. Non-autoregressive Seq2Seq Models They have primarily been used to speed up Machine Translation by allowing parallel edit operations on the output sequence (Lee et al., 2018; Gu et al., 2018; Ghazvininejad et al., 2019; Stern et al., 2019; Chan et al., 2020; Xu and Carpuat, 2020). Refinement approaches have been used to incorporate terminology constraints in machine translation, including as hard (Susanto et al., 2020) and soft constraints (Xu and Carpuat, 2020). They have also shown promise for Automatic Post Editing (APE) (Gu et al., 2019; Wan et al., 2020) , and grammatical error correction (Awasthi et al., 2019). In this work, we show that they are a good fit to incorporate lexical complexity information for Controllable TS. 7 Conclusion We introduced an approach that repurposes a nonautoregressive sequence-to-sequence model to incorporate lexical complexity signals in Controllable TS. An extensive empirical study showed that our approach generates simplified outputs that better match the desired target"
2021.findings-acl.330,2020.wmt-1.141,1,0.751922,"model which results in better outputs in practice. Non-autoregressive Seq2Seq Models They have primarily been used to speed up Machine Translation by allowing parallel edit operations on the output sequence (Lee et al., 2018; Gu et al., 2018; Ghazvininejad et al., 2019; Stern et al., 2019; Chan et al., 2020; Xu and Carpuat, 2020). Refinement approaches have been used to incorporate terminology constraints in machine translation, including as hard (Susanto et al., 2020) and soft constraints (Xu and Carpuat, 2020). They have also shown promise for Automatic Post Editing (APE) (Gu et al., 2019; Wan et al., 2020) , and grammatical error correction (Awasthi et al., 2019). In this work, we show that they are a good fit to incorporate lexical complexity information for Controllable TS. 7 Conclusion We introduced an approach that repurposes a nonautoregressive sequence-to-sequence model to incorporate lexical complexity signals in Controllable TS. An extensive empirical study showed that our approach generates simplified outputs that better match the desired target-grade complexity than AR models. Analysis revealed promising directions for future work, such as improving grammaticality while encouraging ti"
2021.findings-acl.330,P12-1107,0,0.0624878,"Missing"
2021.findings-acl.385,W14-3346,0,0.0205456,"eps and select the best checkpoint based on validation perplexity (see Appendix for details). During inference, we set the maximum number of iterations to 10. All word alignments in this paper are generated automatically using fastalign (Dyer et al., 2013).5 score(ˆ y |x, y) = λ sim(ˆ y , y) + (1 − λ) cxty(ˆ y , x) 4 where the similarity sim(ˆ y , y) measures how faithful the hypothesis y ˆ is to the original reference y and the complexity cxty(ˆ y , x) captures the relationship between the target sequence y ˆ and source sequence x. The similarity function is the smoothed sentence-level BLEU (Chen and Cherry, 2014) w.r.t the original reference. We use three different complexity functions: 1) FRS, 2) wordalignment score2 that measures complexity on a 1 This is inspired by sequence-level interpolation (Kim and Rush, 2016), but they select hypothesis using BLEU while we use more diverse criteria. We use beam search with k = 32. 2 Sum of the log probabilities of each target word conditioned on its aligned source words given by fast-align. Preliminary: SLKD Helps NAR Learn Word Alignment Our work is motivated by the hypothesis that SLKD helps NAR models learn (implicit) alignment between source and target wo"
2021.findings-acl.385,P11-2031,0,0.0603864,"w/ SLKD LevT w/o SLKD LevT w/ SLKD Conf ECE 63.9 63.7 65.1 66.8 65.9 72.3 74.2 86.5 53.3 71.3 10.34 10.49 21.41 20.26 15.17 Table 3: Average token-level accuracy (Acc), confidence (Conf ), and inference ECE (ECE) of AR and the two NAR models trained with and without SLKD. Table 2: Translation quality on WMT14 De-En. In the bottom two groups, models are trained on distilled data with similar faithfulness (Faith) but varying degree of reordering (FRS) and lexical diversity (LexDiv). ↓ marks significant drops compared to the first row in each group based on the paired bootstrap test at p &lt; 0.05 (Clark et al., 2011). this hypothesis by evaluating the effect of SLKD on two datasets: a) En-De train/dev/test sets from WMT14, and b) a synthetic version of the same task, where word alignment information is embedded by pre-reordering the source words so that they are monotonically aligned with target words (in train/dev/test sets). While SLKD improves BLEU by +2.4 on the original En-De task, it has no benefit on the synthetic task (Table 1). This supports our hypothesis and is consistent with other findings on real data: Ghazvininejad et al. (2019) and Gu et al. (2019) showed that SLKD improves the quality of"
2021.findings-acl.385,N19-1423,0,0.00578692,"EU scores on the original WMT14 En-De and the synthetic reordered version. For each task, we compare LevT models trained on real vs. distilled data. word level, and 3) NMT score3 that measures complexity on a sentence level. 3 Experimental Settings Set-Up We use En-De and De-En datasets from WMT14 (Bojar et al., 2014) with the same preprocessing steps as Gu et al. (2019). We evaluate translation quality with case-sensitive tokenized BLEU,4 using the Moses tokenizer. Models We use two state-of-the-art NAR models: • Mask-Predict (MaskT) (Ghazvininejad et al., 2019) uses a masked language model (Devlin et al., 2019) to generate the target sequence by iteratively masking out and regenerating the subset of tokens that the model is least confident about. • Levenshtein Transformer (LevT) (Gu et al., 2019) generates the target sequence through iterative insertion and deletion steps. All AR and NAR models adopt the base Transformer architecture (Vaswani et al., 2017). We train all models using a batch size of 64, 800 tokens for maximum 300, 000 steps and select the best checkpoint based on validation perplexity (see Appendix for details). During inference, we set the maximum number of iterations to 10. All wor"
2021.findings-acl.385,N13-1073,0,0.0418738,"ting the subset of tokens that the model is least confident about. • Levenshtein Transformer (LevT) (Gu et al., 2019) generates the target sequence through iterative insertion and deletion steps. All AR and NAR models adopt the base Transformer architecture (Vaswani et al., 2017). We train all models using a batch size of 64, 800 tokens for maximum 300, 000 steps and select the best checkpoint based on validation perplexity (see Appendix for details). During inference, we set the maximum number of iterations to 10. All word alignments in this paper are generated automatically using fastalign (Dyer et al., 2013).5 score(ˆ y |x, y) = λ sim(ˆ y , y) + (1 − λ) cxty(ˆ y , x) 4 where the similarity sim(ˆ y , y) measures how faithful the hypothesis y ˆ is to the original reference y and the complexity cxty(ˆ y , x) captures the relationship between the target sequence y ˆ and source sequence x. The similarity function is the smoothed sentence-level BLEU (Chen and Cherry, 2014) w.r.t the original reference. We use three different complexity functions: 1) FRS, 2) wordalignment score2 that measures complexity on a 1 This is inspired by sequence-level interpolation (Kim and Rush, 2016), but they select hypothe"
2021.findings-acl.385,D19-1633,0,0.124244,"e different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently. 1 Introduction and Background When training NAR models for neural machine translation (NMT), sequence-level knowledge distillation (Kim and Rush, 2016) is key to match the translation quality of autoregressive (AR) models (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019). Knowledge distillation was first proposed to obtain small student models that match the quality of a higher-capacity teacher models (Liang et al., 2008; Hinton et al., 2015). Sequence-level knowledge distillation (SLKD) trains the student model p(y |x) to approximate the teacher distribution q(y |x) by maximizing the following objecP tive: P LSEQ-KD = − y∈Y q(y |x) log p(y |x) ≈ ˆ ] log p(y |x), where Y repre− y∈Y 1 [y = y sents the space of all possible target sequences, ˆ is the output from running beam search with and y the teacher model q. ∗ Work done during internship"
2021.findings-acl.385,D16-1139,0,0.127027,"rmer and the Mask-Predict NAR models on the WMT14 German-English task, this paper shows that different types of complexity have different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently. 1 Introduction and Background When training NAR models for neural machine translation (NMT), sequence-level knowledge distillation (Kim and Rush, 2016) is key to match the translation quality of autoregressive (AR) models (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019). Knowledge distillation was first proposed to obtain small student models that match the quality of a higher-capacity teacher models (Liang et al., 2008; Hinton et al., 2015). Sequence-level knowledge distillation (SLKD) trains the student model p(y |x) to approximate the teacher distribution q(y |x) by maximizing the following objecP tive: P LSEQ-KD = − y∈Y q(y |x) log p(y |x) ≈ ˆ ] log p(y |x), where Y repre− y∈Y 1 [y = y sents the space of a"
2021.findings-acl.385,D18-1149,0,0.0128127,"of complexity have different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently. 1 Introduction and Background When training NAR models for neural machine translation (NMT), sequence-level knowledge distillation (Kim and Rush, 2016) is key to match the translation quality of autoregressive (AR) models (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019). Knowledge distillation was first proposed to obtain small student models that match the quality of a higher-capacity teacher models (Liang et al., 2008; Hinton et al., 2015). Sequence-level knowledge distillation (SLKD) trains the student model p(y |x) to approximate the teacher distribution q(y |x) by maximizing the following objecP tive: P LSEQ-KD = − y∈Y q(y |x) log p(y |x) ≈ ˆ ] log p(y |x), where Y repre− y∈Y 1 [y = y sents the space of all possible target sequences, ˆ is the output from running beam search with and y the teacher model q. ∗"
2021.findings-acl.385,2020.acl-main.15,0,0.0237838,"Missing"
2021.findings-acl.385,P16-1162,0,0.0344491,"Missing"
2021.findings-acl.385,W11-2102,0,0.0312409,"en source and target and thus improve translation quality. Further analysis shows that knowledge distillation lowers model uncertainty by reducing lexical diversity, which affects the calibration of Mask-Predict and Levenshtein Transformer models in opposite directions. 4392 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4392–4400 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Generating Diverse Distilled References We measure distilled corpus complexity with: • Word Reordering Degree computed by the average fuzzy reordering score (FRS) (Talbot et al., 2011) over all sentence pairs. FRS is an MT evaluation metric introduced to distinguish significant changes in reordering rules of MT systems on syntactically distant language pairs. A higher FRS indicates that the hypothesis is more monotonically aligned to the source. Zhou et al. (2019) show that distilled data has a higher FRS than the real data which may benefit NAR models. • Lexical Diversity which captures the diversity of target word choices given a source word. We compute the lexical diversity LD(d) of the distilled corpus d by averaging the entropy of target words y conditioned on a source"
2021.findings-acl.385,P19-1580,0,0.0166303,"ity while controlling for faithfulness (2nd and 3rd group of rows in Table 2). While the absolute BLEU deltas are small, BLEU decreases significantly as the lexical diversity increases despite reduced degree of reordering. This indicates that increased lexical diversity prevails over the effect of lower degree of reordering in decreasing BLEU scores. 6 Increases Confidence of Source-Target Attention SLKD To better understand how SLKD helps NAR learn the alignment between source and target, we measure how the confidence of the source-target attention changes over decoding iterations. Following Voita et al. (2019), we define the confidence of attention heads as the average of the maximum attention weights over source tokens, where the average is taken over target tokens. Higher confidence scores indicate that the model is more certain about which parts of the source sequence to attend to when predicting the target tokens. As seen in Figure 2, SLKD increases the confidence of source-target attention on both MaskT and LevT. The increase is larger for MaskT than for LevT. For LevT, SLKD increases the attention confidence the most at early decoding iterations. At later iterations, as the model becomes more"
2021.findings-acl.385,2020.acl-main.278,0,0.0156818,"that higher-capacity NAR models require more complex distilled data to achieve better translation quality. They further show that generating distilled references with mixture of experts (Shen et al., 2019) improves NAR translation quality. However, training samples can be complex in different ways, and it remains unclear how different types of data complexity alter the internal working of NAR models and their translation quality. We also anticipate that data complexity may impact the uncertainty and calibration of NAR models – an understudied question, unlike for AR models (Ott et al., 2018; Wang et al., 2020). This paper focuses on two types of data complexity – lexical diversity and degree of word reordering. We expose two state-of-the-art NAR models (Mask-Predict (Ghazvininejad et al., 2019) and Levenshtein Transformer (Gu et al., 2019)) to distilled references of varying complexity on the WMT14 German-English task. Experiments show that decreasing reordering complexity and reducing lexical diversity via distillation both help NAR models learn better alignment between source and target and thus improve translation quality. Further analysis shows that knowledge distillation lowers model uncertain"
2021.gem-1.6,S16-1081,0,0.0479348,"Missing"
2021.gem-1.6,2020.cl-1.4,0,0.0238665,"ems as their evaluation is primarily based on human judgments. Concretely, out of the 97 papers we reviewed, 69 of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and therefore should be included in our structured review. We did not review papers for text simplification, as it has been studied separately (Alva-Manchego et al., 2020; Sikka et al., 2020) and metrics for automatic evaluation have been widely adopted (Xu et al., 2016). Our final list consists of 97 papers: 86 of them are from top-tier NLP and AI venues: ACL , EACL , EMNLP , NAACL , TACL , IEEE , AAAI , Neur IPS, ICML, and ICLR, and the remaining 11 are pre-prints which have not been peer-reviewed. Review Structure We review each paper based on a predefined set of criteria (Table 2). The rationale behind their choice is to collect information on the evaluation aspects that are underspecified in NLP in general as well as those specific to the ST task. For thi"
2021.gem-1.6,P09-1032,0,0.0315401,"Missing"
2021.gem-1.6,2021.eacl-main.29,0,0.0808413,"Missing"
2021.gem-1.6,2020.inlg-1.24,0,0.0421945,"utes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Language Generation (NLG) systems (Howcroft et al., 2020; Lee, 2020; Belz et al., 2020, 2021; Shimorina and Belz, 2021), we conduct a structured review of human evaluation for neural style transfer systems as their evaluation is primarily based on human judgments. Concretely, out of the 97 papers we reviewed, 69 of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and therefore should be incl"
2021.gem-1.6,W07-0718,0,0.120501,"Missing"
2021.gem-1.6,2020.isa-1.5,0,0.0110905,"stic attributes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Language Generation (NLG) systems (Howcroft et al., 2020; Lee, 2020; Belz et al., 2020, 2021; Shimorina and Belz, 2021), we conduct a structured review of human evaluation for neural style transfer systems as their evaluation is primarily based on human judgments. Concretely, out of the 97 papers we reviewed, 69 of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and there"
2021.gem-1.6,W13-2305,0,0.0750907,"Missing"
2021.gem-1.6,D19-1325,0,0.0440185,"Missing"
2021.gem-1.6,2021.humeval-1.8,0,0.0802246,"Missing"
2021.gem-1.6,P16-1094,0,0.0179725,"Missing"
2021.gem-1.6,P14-2029,1,0.864675,"Missing"
2021.gem-1.6,N18-1169,0,0.0658276,"Missing"
2021.gem-1.6,2020.inlg-1.23,0,0.0746458,"Missing"
2021.gem-1.6,2020.emnlp-main.602,0,0.0250282,"Missing"
2021.gem-1.6,N19-1049,0,0.0225152,"across three dimensions: style transfer (has the desired attributed been changed FORMALITY Gotta see both sides of the story. (informal) You have to consider both sides of the story. (formal) SENTIMENT The screen is just the right size. (positive) The screen is too small. (negative) AUTHOR IMITATION Bring her out to me. (modern) Call her forth to me. (shakespearean) Table 1: Examples of three ST attributes: formality, sentiment and Shakespearean transfer. as intended?), meaning preservation (are the other attributes preserved?), and fluency (is the output well-formed?) (Pang and Gimpel, 2019; Mir et al., 2019). Given the large spectrum of stylistic attributes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Language Generation (NL"
2021.gem-1.6,D19-1306,0,0.046338,"Missing"
2021.gem-1.6,2021.humeval-1.15,0,0.0677632,"Missing"
2021.gem-1.6,2020.emnlp-main.55,0,0.0597224,"Missing"
2021.gem-1.6,D19-5614,0,0.0189124,"y is usually evaluated across three dimensions: style transfer (has the desired attributed been changed FORMALITY Gotta see both sides of the story. (informal) You have to consider both sides of the story. (formal) SENTIMENT The screen is just the right size. (positive) The screen is too small. (negative) AUTHOR IMITATION Bring her out to me. (modern) Call her forth to me. (shakespearean) Table 1: Examples of three ST attributes: formality, sentiment and Shakespearean transfer. as intended?), meaning preservation (are the other attributes preserved?), and fluency (is the output well-formed?) (Pang and Gimpel, 2019; Mir et al., 2019). Given the large spectrum of stylistic attributes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Lang"
2021.gem-1.6,N18-1012,1,0.902839,"Missing"
2021.gem-1.6,2020.evalnlgeval-1.2,0,0.0384816,"nd of 64 4.2 Releasing Annotations of the style dimension targeted. Furthermore, since ST includes rewriting text according to pragmatic aspects of language use, who the human judgments are matters since differences in communication norms and expectations might result in different judgments for the same text. Standardizing and describing protocols is also key to assessing the alignment of the evaluation with the models and task proposed (H¨am¨al¨ainen and Alnajjar, 2021), and to understand potential biases and ethical issues that might arise from, e.g., compensation mechanisms (Vaughan, 2018; Schoch et al., 2020; Shmueli et al., 2021). Making human-annotated judgments available would enable the development of better automatic metrics for ST. If all annotations had been released with the papers reviewed, we estimate that more than 10K human judgments per evaluation aspect would be available. Today this would suffice to train and evaluate dedicated evaluation models. In addition, raw annotations can shed light on the difficulty of the task and nature of the data: they can be aggregated in multiple ways (Oortwijn et al., 2021), or used to account for annotator bias in model training (Beigman and Beigman"
2021.gem-1.6,2021.naacl-main.295,0,0.027822,"g Annotations of the style dimension targeted. Furthermore, since ST includes rewriting text according to pragmatic aspects of language use, who the human judgments are matters since differences in communication norms and expectations might result in different judgments for the same text. Standardizing and describing protocols is also key to assessing the alignment of the evaluation with the models and task proposed (H¨am¨al¨ainen and Alnajjar, 2021), and to understand potential biases and ethical issues that might arise from, e.g., compensation mechanisms (Vaughan, 2018; Schoch et al., 2020; Shmueli et al., 2021). Making human-annotated judgments available would enable the development of better automatic metrics for ST. If all annotations had been released with the papers reviewed, we estimate that more than 10K human judgments per evaluation aspect would be available. Today this would suffice to train and evaluate dedicated evaluation models. In addition, raw annotations can shed light on the difficulty of the task and nature of the data: they can be aggregated in multiple ways (Oortwijn et al., 2021), or used to account for annotator bias in model training (Beigman and Beigman Klebanov, 2009). Final"
2021.gem-1.6,Q16-1029,0,0.0303553,"of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and therefore should be included in our structured review. We did not review papers for text simplification, as it has been studied separately (Alva-Manchego et al., 2020; Sikka et al., 2020) and metrics for automatic evaluation have been widely adopted (Xu et al., 2016). Our final list consists of 97 papers: 86 of them are from top-tier NLP and AI venues: ACL , EACL , EMNLP , NAACL , TACL , IEEE , AAAI , Neur IPS, ICML, and ICLR, and the remaining 11 are pre-prints which have not been peer-reviewed. Review Structure We review each paper based on a predefined set of criteria (Table 2). The rationale behind their choice is to collect information on the evaluation aspects that are underspecified in NLP in general as well as those specific to the ST task. For this work, we call the former global criteria. The latter is called dimension-specific criteria and is m"
2021.hcinlp-1.14,W18-4104,0,0.0308742,"ircumstances) the sentence could be reasonably uttered” (Kruszewski et al., 2016). If the source text is expected to reflect “ordinary real-life,&quot; the output should be plausible to be believable. MT output may also be unbelievable if it violates commonsense reasoning, a challenging element of Natural Language Understanding (Mostafazadeh et al., 2016). Lack of discourse coherence might likewise signal unbelievable translations. For document generation, improving the consistency of generated documents makes it harder for human subjects to distinguish automatically generated text from real text (Karuna et al., 2018). Grammatical errors are related to fluency, a traditional MT quality evaluation feature. Fluency has been defined as a judgment of “whether the translation reads like good English...without knowing the accuracy of the content,” and is typically combined with adequacy, an assessment of “the degree to which the information in a professional translation can be found in an MT (or control) output of the same text” (White et al., 1994). A user who cannot understand the source cannot judge adequacy, but may use expectations based on features like fluency and reasonableness to guess. Believability co"
2021.hcinlp-1.14,J16-4003,0,0.0138203,". The bottom-left translation is not believable so a monolingual user would not be misled by it. However, the more believable top-left translation might mislead a monolingual user. Because these judgments are based on perception, they may be more subjective than traditional MT DA features. We control for some factors that may affect believability (Section 3), resulting in annotations that are similarly reliable to the DA features (Section 4). 2 sibility can be thought of as, “whether in an ordinary real-life situation (not “fairy-tale” circumstances) the sentence could be reasonably uttered” (Kruszewski et al., 2016). If the source text is expected to reflect “ordinary real-life,&quot; the output should be plausible to be believable. MT output may also be unbelievable if it violates commonsense reasoning, a challenging element of Natural Language Understanding (Mostafazadeh et al., 2016). Lack of discourse coherence might likewise signal unbelievable translations. For document generation, improving the consistency of generated documents makes it harder for human subjects to distinguish automatically generated text from real text (Karuna et al., 2018). Grammatical errors are related to fluency, a traditional MT"
2021.hcinlp-1.14,W19-6623,1,0.846607,"between annotators (see Section 4). We note that factors such as foreign language proficiency may affect believability judgments. Further work with a wider variety of annotators is needed to identify and quantify those effects. MT Systems Because our goal is to examine segments annotated for believability, fluency, and adequacy judgments rather than to compare systems, we need MT that will produce outputs across a range of quality. Output that is inadequate but believable is of particular interest, so we rely on estimates of the distribution of “fluently inadequate” translations on MTTT from Martindale et al. (2019) to inform our choice of models. They estimated that fluently inadequate translations were most frequent in the “general” NMT models, trained on out-of-domain data. We use their Arabic, Farsi, and Korean “general” models to capture the range of training data sizes and output quality we believe will provide interesting examples for our analysis. The training data is 49M, 6.2M, and 1.4M segments in Arabic, Farsi, and Korean, respectively. The systems are built in Sockeye (Hieber et al., 2017) using the ‘SockeyeNMT rm1’ settings from the MTTT leaderboard2 . The resulting systems achieved BLEU (Pa"
2021.hcinlp-1.14,N16-1098,0,0.0241076,"ional MT DA features. We control for some factors that may affect believability (Section 3), resulting in annotations that are similarly reliable to the DA features (Section 4). 2 sibility can be thought of as, “whether in an ordinary real-life situation (not “fairy-tale” circumstances) the sentence could be reasonably uttered” (Kruszewski et al., 2016). If the source text is expected to reflect “ordinary real-life,&quot; the output should be plausible to be believable. MT output may also be unbelievable if it violates commonsense reasoning, a challenging element of Natural Language Understanding (Mostafazadeh et al., 2016). Lack of discourse coherence might likewise signal unbelievable translations. For document generation, improving the consistency of generated documents makes it harder for human subjects to distinguish automatically generated text from real text (Karuna et al., 2018). Grammatical errors are related to fluency, a traditional MT quality evaluation feature. Fluency has been defined as a judgment of “whether the translation reads like good English...without knowing the accuracy of the content,” and is typically combined with adequacy, an assessment of “the degree to which the information in a pro"
2021.hcinlp-1.14,W13-2305,0,0.0346815,"label for each feature. We calculate scores following Bojar et al. (2018). Each annotator’s raw scores are converted to z-scores based on their own mean and standard deviation, and the z-scores for each segment are averaged across annotators. Segments with positive z-scores are labeled TRUE and negative z-scores are labeled FALSE. Annotating Believability To understand the relationships between believability and traditional MT quality criteria (fluency and adequacy), we hired professionals to annotate MT output for these characteristics in tasks based on the fluency and adequacy DA methods of Graham et al. (2013) and Bojar et al. (2016). The annotated data sets are available at: https://github.com/ mjmartindale/mt_believability . Test Data We chose a test set that is comparable across three typologically different languages with different amounts of training data. Our test data comes from The Multi-Target TED Talks Task (MTTT)–a collection of bitexts across 20 languages (Duh, 2018). The test set is fully sentence parallel with original talk transcripts as the English and human translations for the other languages. We use the non-English translations in MTTT as “source” and machine translate into Engli"
2021.hcinlp-1.14,E17-3017,0,0.0276681,"rest, so we rely on estimates of the distribution of “fluently inadequate” translations on MTTT from Martindale et al. (2019) to inform our choice of models. They estimated that fluently inadequate translations were most frequent in the “general” NMT models, trained on out-of-domain data. We use their Arabic, Farsi, and Korean “general” models to capture the range of training data sizes and output quality we believe will provide interesting examples for our analysis. The training data is 49M, 6.2M, and 1.4M segments in Arabic, Farsi, and Korean, respectively. The systems are built in Sockeye (Hieber et al., 2017) using the ‘SockeyeNMT rm1’ settings from the MTTT leaderboard2 . The resulting systems achieved BLEU (Papineni et al., 2002) scores of 26.6 for Arabic, 22.2 for Farsi and 11.6 for Korean. Tasks We followed segment-level DA scoring best practices established by WMT (Barrault et al., 2019). The fluency and adequacy questions were taken directly from WMT16 (Bojar et al., 2016). The believability question uses our definition of believability with an introductory phrase to assure the annotator that we understand that it is not possible to truly evaluate the meaning without the source: “Even withou"
2021.hcinlp-1.14,P02-1040,0,0.109997,"9) to inform our choice of models. They estimated that fluently inadequate translations were most frequent in the “general” NMT models, trained on out-of-domain data. We use their Arabic, Farsi, and Korean “general” models to capture the range of training data sizes and output quality we believe will provide interesting examples for our analysis. The training data is 49M, 6.2M, and 1.4M segments in Arabic, Farsi, and Korean, respectively. The systems are built in Sockeye (Hieber et al., 2017) using the ‘SockeyeNMT rm1’ settings from the MTTT leaderboard2 . The resulting systems achieved BLEU (Papineni et al., 2002) scores of 26.6 for Arabic, 22.2 for Farsi and 11.6 for Korean. Tasks We followed segment-level DA scoring best practices established by WMT (Barrault et al., 2019). The fluency and adequacy questions were taken directly from WMT16 (Bojar et al., 2016). The believability question uses our definition of believability with an introductory phrase to assure the annotator that we understand that it is not possible to truly evaluate the meaning without the source: “Even without having seen the source text, I believe the meaning of this translation is likely to match the meaning of the original.” Ann"
2021.hcinlp-1.14,2009.eamt-1.5,0,0.0369201,"ualitative analysis of examples where believability and fluency judgments disagreed suggests that semantic features can overwhelm grammatical features in believability judgments. A full qualitative analysis of the believabilityannotated examples would suggest features that may have influenced annotator’s judgments and could indicate approaches that may be effective in automatically predicting believability. Believability used alone could enable an adversarial MT system to deliberately mask errors and produce misleading output, but believability predictions combined with MT quality estimation (Specia et al., 2009) could be used to flag potentially misleading output. Because believability is a user-centric metric, gaining a complete understanding would require more user-centric methods. The annotator agreement in our results may indicate that believability is less subjective than one might expect, or it may simply indicate that our annotators were fairly homogeneous. A user study could not only tell us exactly what features were most salient in which contexts, but could indicate whether demographic features such as age or education affect perceptions of believability. Qualitative Analysis Based on infor"
2021.hcinlp-1.14,1994.amta-1.25,0,0.874424,"Missing"
C02-1162,J96-1001,0,\N,Missing
C02-1162,P98-1069,1,\N,Missing
C02-1162,C98-1066,1,\N,Missing
C02-1162,N01-1006,1,\N,Missing
C02-1162,J90-2002,0,\N,Missing
C02-1162,P93-1001,0,\N,Missing
C02-1162,J95-4004,0,\N,Missing
C02-1162,P98-2127,0,\N,Missing
C02-1162,C98-2122,0,\N,Missing
C02-1162,P99-1004,0,\N,Missing
C04-1058,J95-4004,0,0.0775075,"typeGaz 0=not-inGaz wcaptype 0=allupper =&gt; ne=I-MISC ne 0=I-LOC word:[-3,-1]=universidad =&gt; ne=I-ORG ne 1=O ne 2=O word 0=de captypeLex 0=not-inLex captypeGaz 0=inGaz wcaptype 0=alllower =&gt; ne=O The AdaBoost.MH base model’s high accuracy sets a high bar for error correction. Aside from brute-force en masse voting of the sort at CoNLL-2002 described above, we do not know of any existing post-boosting models that improve rather than degrade accuracy. We aim to further improve performance, and propose using a piped error corrector. 4.2 Transformation-based Learning Transformation-based learning (Brill, 1995), or TBL, is one of the most successful rule-based machine learning algorithms. The central idea of TBL is to learn an ordered list of rules, each of which evaluates on the results of those preceding it. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Transformation-based learning has been used to tackle a wide range of NLP problems, ranging from part-ofspeech tagging (Brill, 1995) to parsing (Brill, 1996) to segmentation and message understanding (Day et al., 1997). In general, it achi"
C04-1058,W02-2004,0,0.278193,"Missing"
C04-1058,A97-1051,0,0.0818433,"formation-based learning (Brill, 1995), or TBL, is one of the most successful rule-based machine learning algorithms. The central idea of TBL is to learn an ordered list of rules, each of which evaluates on the results of those preceding it. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. Transformation-based learning has been used to tackle a wide range of NLP problems, ranging from part-ofspeech tagging (Brill, 1995) to parsing (Brill, 1996) to segmentation and message understanding (Day et al., 1997). In general, it achieves state-of-the-art performances and is fairly resistant to overtraining. 5 Conclusion We have investigated frequently raised questions about N-fold Templated Piped Correction (NTPC), a generalpurpose, conservative error correcting model, which has been shown to reliably deliver small but consistent gains on the accuracy of even high-performing base models on high-dimensional NLP tasks, with little risk of accidental degradation. Experimental evidence shows that when error-correcting high-accuracy base models, simple models and hypotheses are more beneficial than complex"
C04-1058,W03-0419,0,0.0665735,"Missing"
C04-1058,W02-2024,0,0.016439,"forming three teams in the CoNLL-2002 Named Entity Recognition shared task evaluation used boosting as their base system (Carreras et al., 2002)(Wu et al., 2002). However, precedents for improving performance after boosting are few. At the CoNLL-2002 shared task session, Tjong Kim Sang (unpublished) described an experiment using voting to combine the NER outputs from the shared task participants which, predictably, produced better results than the individual systems. A couple of the individual systems were boosting models, so in some sense this could be regarded as an example. Tsukamoto et al.(2002) used piped AdaBoost.MH models for NER. Their experimental results were somewhat disappointing, but this could perhaps be attributable to various reasons including the feature engineering or not using cross-validation sampling in the stacking. Appendix The following examples show the top 10 rules learned for English and Spanish on the bracketing + classification task. (Models M6 and M8) English ne -2=ZZZ ne -1=ZZZ word:[1,3]=21 nonnevocab 0=inNonNeVocab nevocab 0=inNeVocab captype 0=firstword-firstupper =&gt; ne=I-ORG ne 1=O ne 2=O word -1=ZZZ nonnevocab 0=inNonNeVocab nevocab 0=not-inNeVocab cap"
C04-1058,W02-2031,0,0.0638367,"Missing"
C04-1058,W02-2035,1,0.564651,"Missing"
C04-1058,wu-etal-2004-raising,1,0.898058,"imization techniques. Thus the results were surprising: the simplest models kept outperforming the “sophisticated” models. This paper attempts to investigate some of the key reasons why. To avoid reinventing the wheel, we originally considered adapting an existing error-driven method, transformation-based learning (TBL) for this purpose. TBL seems well suited to the problem as it is inherently an error corrector and, on its own, has been shown to achieve high accuracies on a variety of problems (see Section 4). Our original goal was to adapt TBL for error correction of high-performing models (Wu et al., 2004a), with two main principles: (1) since it is not clear that the usual assumptions made about the distribution of the training/test data are valid in such extreme operating ranges, empirical observations would take precedence over theoretical models, which implies that (2) any model would have to be empirically justified by testing on a diverse range of data. Experimental observations, however, increasingly drove us toward different goals. Our resulting error corrector, NTPC, was instead constructed on the principle of making as few assumptions as possible in order to robustly generalize over"
C04-1190,S01-1004,0,0.0654158,"tically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv = Cv for eigenvalues λ ≥ 0 and eigenvectors v ∈ F . Because Supervised KPCA baseline model Our baseline WSD model is a supervised learning model that also makes use of Kernel Principal Component Analysis (KPCA), proposed by (Sch¨olkopf et al., 1998) as a generalization of PCA. KPCA has been successfully applied in many areas such as de-noising of images of hand-written digits (Mika et al., 1999) and modeling the distribution of non-linear data sets in the context of shape modelling for rea"
C04-1190,W02-1002,0,0.124954,"affects the model, and propose a new semi-supervised model that takes advantage of unlabeled data, along with a composite model that combines both the supervised and semi-supervised models. Finally, details of the experimental setup and comparative results are given. 2 Related work The long history of WSD research includes numerous statistically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv = Cv for eigenvalues λ ≥ 0 and eigenvectors v ∈ F . Because Supervised KPCA baseline model Our baseline WSD model is a supervised learning model that also makes"
C04-1190,W96-0208,0,0.0268833,"s. After a brief look at related work, we review the baseline supervised WSD model, which is based on Kernel PCA. We then discuss how data sparseness affects the model, and propose a new semi-supervised model that takes advantage of unlabeled data, along with a composite model that combines both the supervised and semi-supervised models. Finally, details of the experimental setup and comparative results are given. 2 Related work The long history of WSD research includes numerous statistically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv"
C04-1190,S01-1005,0,0.0396358,"Missing"
C04-1190,S01-1034,0,0.0305888,"we review the baseline supervised WSD model, which is based on Kernel PCA. We then discuss how data sparseness affects the model, and propose a new semi-supervised model that takes advantage of unlabeled data, along with a composite model that combines both the supervised and semi-supervised models. Finally, details of the experimental setup and comparative results are given. 2 Related work The long history of WSD research includes numerous statistically trained methods; space only permits us to summarize a few key points here. Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. The Senseval series of evaluations facilitates comparing the strengths and weaknesses of various WSD models on common data sets, with Senseval-1 (Kilgarriff and Rosenzweig, 1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held in 1998, 2001, and 2004 respectively. 3 To do this requires solving the equation λv = Cv for eigenvalues λ ≥ 0 and eigenvecto"
C04-1190,P04-1081,1,0.689727,"d individual models. Although empirical results with supervised KPCA models demonstrate significantly better accuracy compared to the state-of-the-art achieved by either na¨ıve Bayes or maximum entropy models on Senseval-2 data, we identify specific sparse data conditions under which supervised KPCA models deteriorate to essentially a most-frequent-sense predictor. We discuss the potential of KPCA for leveraging unannotated data for partially-unsupervised training to address these issues, leading to a composite model that combines both the supervised and semi-supervised models. 1 Introduction Wu et al. (2004) propose an efficient and accurate new supervised learning model for word sense disambiguation (WSD), that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique to make predictions implicitly based on generalizations over feature combinations. Experiments performed on the Senseval2 English lexical sample data show that KPCA-based word sense disambiguation method is capable of outperforming other widely used WSD models including na¨ıve Bayes, maximum entropy, and SVM models. Despite the excellent performance of the supervised KPCA-based WSD model on average, though, our furt"
C04-1190,W02-0813,0,\N,Missing
C04-1190,P03-1004,0,\N,Missing
C14-1055,al-saif-markert-2010-leeds,0,0.0200724,"lied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugu"
C14-1055,P11-1061,0,0.0245884,"otated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and pr"
C14-1055,P97-1011,0,0.238511,"Missing"
C14-1055,D11-1027,0,0.0282846,"dies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we ca"
C14-1055,P09-1042,0,0.0306296,"i (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License detail"
C14-1055,W04-1101,0,0.0901396,"Missing"
C14-1055,P06-2057,0,0.0221681,"and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577 Proceedings of COLING 2014,"
C14-1055,P03-1056,0,0.0521517,"n; multiway classification labels one of the five possible classes: non-discourse use, TEMPORAL, COMPARISON, CONTINGENCY and EXPANSION. This series of classifiers results in a system that can assign the same labels as the classifiers trained for English. To complete our presentation of the approach, we now turn to describe the features used to represent instances of potential discourse connectives. 5.2 Features The following set of features for each expression we need to classify are extracted solely from the Chinese part of the corpus4 . The syntactic parse trees were obtained automatically (Levy and Manning, 2003). Connective The connective expressions themselves. The vast majority of connectives (at least in English) are unambiguous, so using the identity of the connective is a hard-to-beat baseline for sense prediction (Pitler et al., 2008). Categories The syntactic category of the expression itself, as well as that of its parents, and its left and right siblings (if any). These features are adapted from Pitler and Nenkova (2009). Depth Depth of the expressions’s syntactic category in the parse tree for the sentence. POS bigram Bigram of part-of-speech tags of the entire sentence. Production pairs Pa"
C14-1055,li-etal-2010-enriching,0,0.0205623,"Missing"
C14-1055,D09-1036,0,0.435052,"labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been devel"
C14-1055,P11-1100,0,0.0134467,"urse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource"
C14-1055,P97-1013,0,0.177418,"divergence, and discourse connective ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations, using the parallel corpus instead of discourse annotation in the language of interest. Our resulting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification. 1 Introduction The analysis of the way spans of text semantically connect with each other to create a coherent text has a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al., 1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relation"
C14-1055,W12-0117,0,0.146065,"he use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another"
C14-1055,W11-2022,0,0.188077,"t available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 577–587, Dublin, Ireland, August 23-29 2014. The goal of our work is not only to measure the accura"
C14-1055,W09-3029,0,0.0231369,"2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009"
C14-1055,H05-1108,0,0.0116767,"2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577"
C14-1055,W12-1614,0,0.256277,"small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish ("
C14-1055,D08-1020,1,0.814766,"esource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-"
C14-1055,P09-2004,1,0.953807,"ause of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated w"
C14-1055,C08-2022,1,0.929939,"Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, s"
C14-1055,P09-1077,1,0.828179,"annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relatio"
C14-1055,prasad-etal-2008-penn,0,0.146439,"lting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification. 1 Introduction The analysis of the way spans of text semantically connect with each other to create a coherent text has a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al., 1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and c"
C14-1055,C10-2118,0,0.0144162,"were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza e"
C14-1055,P11-2052,0,0.0372337,"Missing"
C14-1055,D07-1010,0,0.0748826,"97; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the r"
C14-1055,P11-2111,0,0.0598275,"Missing"
C14-1055,H01-1035,0,0.0189898,"TB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International Licen"
C14-1055,I08-7009,0,0.0315716,"; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado"
C14-1055,P12-1008,0,0.28866,"sing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations,"
C14-1055,L08-1000,0,\N,Missing
C18-1086,P17-2061,0,0.0172691,"essfully implemented side constraints as a special token added to each source sentence. These tokens are embedded into the source sentence representation and control target sequence generation via the attention mechanism. Sennrich et al. (2016a) append <T&gt; or <V&gt; (i.e. T-V pronoun distinction) to the source text to indicate which pronoun is preferred in the German output. Johnson et al. (2017) and Niu et al. (2018) concatenate parallel data of various language directions and mark the source with the desired output language to perform multilingual or bi-directional NMT. Kobus et al. (2017) and Chu et al. (2017) add domain tags for domain adaptation in NMT. 3 Approach We describe our unified model for performing FT in both directions (Section 3.1), our FSMT model with side constraints (Section 3.2) and finally our multi-task learning model that jointly learns to perform FT and FSMT (Section 3.3). All models rely on the same NMT architecture: attentional recurrent sequenceto-sequence models. 3.1 Bi-Directional Formality Transfer Rao and Tetreault (2018) used independent neural machine translation models for each formality transfer direction (informal→formal and formal→informal). Inspired by the bi-dir"
C18-1086,W11-1601,0,0.0278197,"nstance, Xu et al. (2012) scrape modern translations of Shakespeare’s plays and use a phrase-based MT (PBMT) system to paraphrase Shakespearean English into/from modern English. Jhamtani et al. (2017) improve performance on this dataset using neural translation model with pointers to enable copy actions. The availability of parallel standard and simple Wikipedia (and sometimes additional human rewrites) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax-based MT (Zhu et al., 2010; Xu et al., 2016), phrase-based MT (Coster and Kauchak, 2011; Wubben et al., 2012) to neural MT (Wang et al., 2016) trained via reinforcement learning (Zhang and Lapata, 2017). Naturally occurring examples of parallel formal-informal sentences are harder to find. Prior work relied on synthetic examples generated based on lists of words of known formality (Sheikha and Inkpen, 2011). This state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, GYAFC (Grammarly’s Yahoo Answers Formality Corpus). 110K informal sentences were collected from Yahoo Answers and they were rewritten in a formal st"
C18-1086,E17-3017,0,0.0368815,"so we use a bilingual semantic similarity detector to select 20,005,000 least divergent examples from ∼27.5M deduplicated sentence pairs in the original set (Vyas et al., 2018). Selected examples are then randomly split into a 20M training pool, a 2.5K dev set and a 2.5K test set. Preprocessing: We apply four pre-processing steps to both FT and MT data: normalization, tokenization, true-casing, and joint source-target BPE with 32,000 operations for NMT (Sennrich et al., 2016b). NMT Configuration: We use the standard attentional encoder-decoder architecture implemented in the Sockeye toolkit (Hieber et al., 2017). Our translation model uses a bi-directional encoder with a single 1011 LSTM layer (Bahdanau et al., 2015) of size 512, multilayer perceptron attention with a layer size of 512, and word representations of size 512. We apply layer normalization and tie the source and target embeddings as well as the output layer’s weight matrix. We add dropout to embeddings and RNNs of the encoder and decoder with probability 0.2. We train using the Adam optimizer with a batch size of 64 sentences and checkpoint the model every 1000 updates (Kingma and Ba, 2015). Training stops after 8 checkpoints without imp"
C18-1086,W17-4902,0,0.105263,"le NMT model that can transfer between styles in both directions. 2 Background Style Transfer can naturally be framed as a sequence to sequence translation problem given sentence pairs that are paraphrases in two distinct styles. These parallel style corpora are constructed by creatively collecting existing texts of varying styles, and are therefore rare and much smaller than machine translation parallel corpora. For instance, Xu et al. (2012) scrape modern translations of Shakespeare’s plays and use a phrase-based MT (PBMT) system to paraphrase Shakespearean English into/from modern English. Jhamtani et al. (2017) improve performance on this dataset using neural translation model with pointers to enable copy actions. The availability of parallel standard and simple Wikipedia (and sometimes additional human rewrites) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax-based MT (Zhu et al., 2010; Xu et al., 2016), phrase-based MT (Coster and Kauchak, 2011; Wubben et al., 2012) to neural MT (Wang et al., 2016) trained via reinforcement learning (Zhang and Lapata, 2017). Naturally occurring examples of parallel formal-informal se"
C18-1086,Q17-1024,0,0.0479336,"Missing"
C18-1086,P17-4012,0,0.0361441,"8.34 38.04 39.09 Table 1: Automatic evaluation of Formality Transfer with BLEU scores. The bi-directional model with three stacked improvements achieves the best overall performance. The improvement over the second best system is statistically significant at p < 0.05 using bootstrap resampling (Koehn, 2004). 6 6.1 Formality Transfer Experiments Baseline Models from Rao and Tetreault (2018) PBMT is a phrase-based machine translation model trained on the GYAFC corpus using a training regime consisting of self-training, data sub-selection and a large language model. NMT Baseline uses OpenNMT-py (Klein et al., 2017). Rao and Tetreault (2018) use a pre-processing step to make source informal sentences more formal and source formal sentences more informal by rules such as re-casing. Word embeddings pre-trained on Yahoo Answers are also used. NMT Combined is Rao and Tetreault’s best performing NMT model trained on the rule-processed GYAFC corpus, with additional forward and backward translations produced by the PBMT model. 6.2 Our Models NMT Baseline: Our NMT baseline uses Sockeye instead of OpenNMT-py and is trained on raw datasets of two domains and two transfer directions. Bi-directional FT: Our initial"
C18-1086,kobus-etal-2017-domain,0,0.0231478,"1). Prior work has successfully implemented side constraints as a special token added to each source sentence. These tokens are embedded into the source sentence representation and control target sequence generation via the attention mechanism. Sennrich et al. (2016a) append <T&gt; or <V&gt; (i.e. T-V pronoun distinction) to the source text to indicate which pronoun is preferred in the German output. Johnson et al. (2017) and Niu et al. (2018) concatenate parallel data of various language directions and mark the source with the desired output language to perform multilingual or bi-directional NMT. Kobus et al. (2017) and Chu et al. (2017) add domain tags for domain adaptation in NMT. 3 Approach We describe our unified model for performing FT in both directions (Section 3.1), our FSMT model with side constraints (Section 3.2) and finally our multi-task learning model that jointly learns to perform FT and FSMT (Section 3.3). All models rely on the same NMT architecture: attentional recurrent sequenceto-sequence models. 3.1 Bi-Directional Formality Transfer Rao and Tetreault (2018) used independent neural machine translation models for each formality transfer direction (informal→formal and formal→informal)."
C18-1086,W04-3250,0,0.0380691,"n E&M + F&R + ensemble decoding (×4) + multi-task learning (MultiTask-tag-style) Informal→Formal E&M F&R 68.22 72.94 58.80 68.28 68.41 74.22 65.34 71.28 66.30 71.97 69.20 73.52 71.36 74.49 72.13 75.37 Formal→Informal E&M F&R 33.54 32.64 30.57 36.71 33.56 35.03 32.36 36.23 34.00 36.33 35.44 37.72 36.18 38.34 38.04 39.09 Table 1: Automatic evaluation of Formality Transfer with BLEU scores. The bi-directional model with three stacked improvements achieves the best overall performance. The improvement over the second best system is statistically significant at p < 0.05 using bootstrap resampling (Koehn, 2004). 6 6.1 Formality Transfer Experiments Baseline Models from Rao and Tetreault (2018) PBMT is a phrase-based machine translation model trained on the GYAFC corpus using a training regime consisting of self-training, data sub-selection and a large language model. NMT Baseline uses OpenNMT-py (Klein et al., 2017). Rao and Tetreault (2018) use a pre-processing step to make source informal sentences more formal and source formal sentences more informal by rules such as re-casing. Word embeddings pre-trained on Yahoo Answers are also used. NMT Combined is Rao and Tetreault’s best performing NMT mode"
C18-1086,2015.iwslt-papers.2,0,0.02782,"of work considers transformations that alter the original meaning (e.g., changes in sentiment or topic), while we view style transfer as meaning-preserving. An auto-encoder is used to encode a sequence to a latent representation which is then decoded to get the style transferred output sequence (Mueller et al., 2017; Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Prabhumoye et al., 2018). Style in Machine Translation has received little attention in recent MT architectures. Mima et al. (1997) improve rule-based MT by using extra-linguistic information such as speaker’s role and gender. Lewis et al. (2015) and Niu and Carpuat (2016) equate style with domain, and train conversational MT systems by selecting in-domain (i.e. conversation-like) training data. Similarly, Wintner et al. (2017) and Michel and Neubig (2018) take an adaptation approach to personalize MT with gender-specific or speaker-specific data. Other work has focused on specific realizations of stylistic variations, such as T-V pronoun selection for translation into German (Sennrich et al., 2016a) or controlling voice (Yamagishi et al., 2016). In contrast, we adopt the broader range of style variations considered in our prior work,"
C18-1086,L16-1147,0,0.0207216,"y Rao and Tetreault (2018) as our FT data. This corpus consists of 110K informal sentences from two domains of Yahoo Answers (Entertainment and Music (E&M) and Family and Relationships (F&R)) paired with their formal rewrites by humans. The train split consists of 100K informal-formal sentence pairs whereas the dev/test sets consist of roughly 5K source-style sentences paired with four reference target-style human rewrites for both transfer directions. FSMT data: We evaluate the FSMT models on a large-scale French to English (FR-EN) translation task. Examples are drawn from OpenSubtitles2016 (Lison and Tiedemann, 2016) which consists of movie and television subtitles and is thus more similar to the GYAFC corpus compared to news or parliament proceedings. This is a noisy dataset where aligned French and English sentences often do not have the same meaning, so we use a bilingual semantic similarity detector to select 20,005,000 least divergent examples from ∼27.5M deduplicated sentence pairs in the original set (Vyas et al., 2018). Selected examples are then randomly split into a 20M training pool, a 2.5K dev set and a 2.5K test set. Preprocessing: We apply four pre-processing steps to both FT and MT data: no"
C18-1086,N15-1092,0,0.0482052,"diverse styles, but would ideally require not only sentence pairs, but e.g., sentence triplets that contain a French input, its formal English translation, and its informal English translation. We hypothesize that FT and FSMT can benefit from being addressed jointly, by sharing information from two distinct types of supervision: sentence pairs in the same language that capture style difference, and translation pairs drawn from corpora of various styles. Inspired by the benefits of multi-task learning (Caruana, 1997) for natural language processing tasks in general (Collobert and Weston, 2008; Liu et al., 2015; Luong et al., 2016), and for multilingual MT in particular (Johnson et al., 2017), we introduce a model based on Neural Machine Translation (NMT) that jointly learns to perform both monolingual FT and bilingual FSMT. As can be seen in Figure 1, given an English sentence and a tag (formal or informal), our model paraphrases the input sentence into the desired formality. The same model can also take in a French sentence, and produce a formal or an informal English translation as desired. Designing this model requires addressing several questions: Can we build a single model that performs forma"
C18-1086,P18-2050,0,0.0144182,"atent representation which is then decoded to get the style transferred output sequence (Mueller et al., 2017; Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Prabhumoye et al., 2018). Style in Machine Translation has received little attention in recent MT architectures. Mima et al. (1997) improve rule-based MT by using extra-linguistic information such as speaker’s role and gender. Lewis et al. (2015) and Niu and Carpuat (2016) equate style with domain, and train conversational MT systems by selecting in-domain (i.e. conversation-like) training data. Similarly, Wintner et al. (2017) and Michel and Neubig (2018) take an adaptation approach to personalize MT with gender-specific or speaker-specific data. Other work has focused on specific realizations of stylistic variations, such as T-V pronoun selection for translation into German (Sennrich et al., 2016a) or controlling voice (Yamagishi et al., 2016). In contrast, we adopt the broader range of style variations considered in our prior work, which introduced the FSMT task (Niu et al., 2017): in FSMT, the MT system takes a desired formality level as 1009 an additional input, to represent the target audience of a translation, which human translators imp"
C18-1086,P10-2041,0,0.0481148,"al. (2016a), we use side constraints on parallel translation examples to control output formality. At training time, this requires a tag that captures the formality of the target sentence for every sentence pair. Given the vast range of text variations that influence style, we cannot obtain tags using rules as for T-V pronoun distinctions (Sennrich et al., 2016a). Instead, we categorize FrenchEnglish parallel data into formal vs. informal categories by comparing them to the informal and formal English from the GYAFC corpus. We adopt a data selection technique, Cross-Entropy Difference (CED) (Moore and Lewis, 2010), to rank English sentences in the bilingual corpus by their relative distance to each style. First, we consider formal English as the target style and define CED(s) = Hf ormal (s) − Hinf ormal (s), where Hf ormal (s) is the cross-entropy between a sentence s and the formal language model. Smaller CED indicates an English sentence that is more similar to the formal English corpus and less similar to the informal English corpus. We rank English sentences by their CED scores and select the top N sentences (choice of N discussed in Section 6). Pairing these N English sentences with their parallel"
C18-1086,W17-4903,1,0.771884,"le NMT MultiTask-random PBMT-random (Niu et al.) +Tag? X X Random? X X FR→Formal-EN 27.15 25.02 23.25 25.24 29.12 FR→Informal-EN 26.70 25.20 23.41 25.14 29.02 Table 4: BLEU scores of various FSMT models. “+Tag” indicates using formality tags for bilingual data while “Random” indicates using randomly selected bilingual data. translation hypotheses by their closeness to the desired formality level. We adapt this system to our evaluation scenario — we calculate median scores for informal and formal data (i.e. −0.41 and −0.27 respectively) in GYAFC respectively by a PCA-LSA-based formality model (Niu and Carpuat, 2017; Niu et al., 2017) and use them as desired formality levels.6 The bilingual training data is randomly selected. 7.2 Results Automatic Evaluation. We compute BLEU scores on the held out test set for all models as a sanity check on translation quality. Because there is only one reference translation of unknown style for each input sentence, these BLEU scores conflate translation errors and stylistic mismatch, and are therefore not sufficient to evaluate FSMT performance. We include them for completeness here, as indicators of general translation quality, and will rely on human evaluation as pri"
C18-1086,D17-1299,1,0.869974,"tions that generate natural language, as the style of a text conveys important information beyond its literal meaning (Hovy, 1987). Heylighen and Dewaele (1999) and Biber (2014) have argued that the formal-informal dimension is a core dimension of stylistic variation. In this work, we focus on the problem of generating text for a desired formality level. It has been recently studied in two distinct settings: (1) Rao and Tetreault (2018) addressed the task of Formality Transfer (FT) where given an informal sentence in English, systems are asked to output a formal equivalent, or vice-versa; (2) Niu et al. (2017) introduced the task of FormalitySensitive Machine Translation (FSMT), where given a sentence in French and a desired formality level (approximating the intended audience of the translation), systems are asked to produce an English translation of the desired formality level. While FT and FSMT can both be framed as Machine Translation (MT), appropriate training examples are much harder to obtain than for traditional machine translation tasks. FT requires sentence pairs that express the same meaning in two different styles, which rarely occur naturally and are therefore only available in small q"
C18-1086,W18-2710,1,0.918928,"s as the context vector for each decoding step. In the joint model, we employ Side Constraints as the formality input to restrict the generation of the output sentence (Figure 1). Prior work has successfully implemented side constraints as a special token added to each source sentence. These tokens are embedded into the source sentence representation and control target sequence generation via the attention mechanism. Sennrich et al. (2016a) append <T&gt; or <V&gt; (i.e. T-V pronoun distinction) to the source text to indicate which pronoun is preferred in the German output. Johnson et al. (2017) and Niu et al. (2018) concatenate parallel data of various language directions and mark the source with the desired output language to perform multilingual or bi-directional NMT. Kobus et al. (2017) and Chu et al. (2017) add domain tags for domain adaptation in NMT. 3 Approach We describe our unified model for performing FT in both directions (Section 3.1), our FSMT model with side constraints (Section 3.2) and finally our multi-task learning model that jointly learns to perform FT and FSMT (Section 3.3). All models rely on the same NMT architecture: attentional recurrent sequenceto-sequence models. 3.1 Bi-Directi"
C18-1086,P02-1040,0,0.101722,"get embeddings as well as the output layer’s weight matrix. We add dropout to embeddings and RNNs of the encoder and decoder with probability 0.2. We train using the Adam optimizer with a batch size of 64 sentences and checkpoint the model every 1000 updates (Kingma and Ba, 2015). Training stops after 8 checkpoints without improvement of validation perplexity. We decode with a beam size of 5. We train four randomly seeded models for each experiment and combine them in a linear ensemble for decoding.1 5 Evaluation Protocol 5.1 Automatic Evaluation We evaluate both FT and FSMT tasks using BLEU (Papineni et al., 2002), which compares the model output with four reference target-style rewrites for FT and a single reference translation for FSMT. We report case-sensitive BLEU with standard WMT tokenization.2 For FT, Rao and Tetreault (2018) show that BLEU correlates well with the overall system ranking assigned by humans. For FSMT, BLEU is an imperfect metric as it conflates mismatches due to translation errors and due to correct style variations. We therefore turn to human evaluation to isolate formality differences from translation quality. 5.2 Human Evaluation Following Rao and Tetreault (2018), we assess m"
C18-1086,P18-1080,0,0.029313,"ed on both PBMT and NMT models (Rao and Tetreault, 2018). In this work, we leverage this corpus to enable multi-task FT and FSMT. Recent work also explores how to perform style transfer without parallel data. However, this line of work considers transformations that alter the original meaning (e.g., changes in sentiment or topic), while we view style transfer as meaning-preserving. An auto-encoder is used to encode a sequence to a latent representation which is then decoded to get the style transferred output sequence (Mueller et al., 2017; Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Prabhumoye et al., 2018). Style in Machine Translation has received little attention in recent MT architectures. Mima et al. (1997) improve rule-based MT by using extra-linguistic information such as speaker’s role and gender. Lewis et al. (2015) and Niu and Carpuat (2016) equate style with domain, and train conversational MT systems by selecting in-domain (i.e. conversation-like) training data. Similarly, Wintner et al. (2017) and Michel and Neubig (2018) take an adaptation approach to personalize MT with gender-specific or speaker-specific data. Other work has focused on specific realizations of stylistic variation"
C18-1086,E17-2025,0,0.0293511,"tence denoting the desired target formality level i.e. <F&gt; for transferring to formal and <I&gt; for transferring to informal. This enables our FT model to learn to transfer to the correct style via attending to the tag in the source embedding. We train an NMT model on this combined dataset. Since both the source and target sentences come from the same language, we encourage their representations to lie in the same distributional vector space by (1) building a shared Byte-Pair Encoding (BPE) model on source and target data (Sennrich et al., 2016b) and (2) tying source and target word embeddings (Press and Wolf, 2017). 3.2 Formality-Sensitive Machine Translation with Side Constraints Inspired by Sennrich et al. (2016a), we use side constraints on parallel translation examples to control output formality. At training time, this requires a tag that captures the formality of the target sentence for every sentence pair. Given the vast range of text variations that influence style, we cannot obtain tags using rules as for T-V pronoun distinctions (Sennrich et al., 2016a). Instead, we categorize FrenchEnglish parallel data into formal vs. informal categories by comparing them to the informal and formal English f"
C18-1086,N18-1012,1,0.0793388,"-sensitive translation without being explicitly trained on styleannotated translation examples. 1 Introduction Generating language in the appropriate style is a requirement for applications that generate natural language, as the style of a text conveys important information beyond its literal meaning (Hovy, 1987). Heylighen and Dewaele (1999) and Biber (2014) have argued that the formal-informal dimension is a core dimension of stylistic variation. In this work, we focus on the problem of generating text for a desired formality level. It has been recently studied in two distinct settings: (1) Rao and Tetreault (2018) addressed the task of Formality Transfer (FT) where given an informal sentence in English, systems are asked to output a formal equivalent, or vice-versa; (2) Niu et al. (2017) introduced the task of FormalitySensitive Machine Translation (FSMT), where given a sentence in French and a desired formality level (approximating the intended audience of the translation), systems are asked to produce an English translation of the desired formality level. While FT and FSMT can both be framed as Machine Translation (MT), appropriate training examples are much harder to obtain than for traditional mach"
C18-1086,N16-1005,0,0.478047,"n recent MT architectures. Mima et al. (1997) improve rule-based MT by using extra-linguistic information such as speaker’s role and gender. Lewis et al. (2015) and Niu and Carpuat (2016) equate style with domain, and train conversational MT systems by selecting in-domain (i.e. conversation-like) training data. Similarly, Wintner et al. (2017) and Michel and Neubig (2018) take an adaptation approach to personalize MT with gender-specific or speaker-specific data. Other work has focused on specific realizations of stylistic variations, such as T-V pronoun selection for translation into German (Sennrich et al., 2016a) or controlling voice (Yamagishi et al., 2016). In contrast, we adopt the broader range of style variations considered in our prior work, which introduced the FSMT task (Niu et al., 2017): in FSMT, the MT system takes a desired formality level as 1009 an additional input, to represent the target audience of a translation, which human translators implicitly take into account. This task was addressed via n-best re-ranking in phrase-based MT — translation hypotheses whose formality are closer to desired formality are promoted. By contrast, in this work we use neural MT which is based on the Att"
C18-1086,P16-1162,0,0.813068,"n recent MT architectures. Mima et al. (1997) improve rule-based MT by using extra-linguistic information such as speaker’s role and gender. Lewis et al. (2015) and Niu and Carpuat (2016) equate style with domain, and train conversational MT systems by selecting in-domain (i.e. conversation-like) training data. Similarly, Wintner et al. (2017) and Michel and Neubig (2018) take an adaptation approach to personalize MT with gender-specific or speaker-specific data. Other work has focused on specific realizations of stylistic variations, such as T-V pronoun selection for translation into German (Sennrich et al., 2016a) or controlling voice (Yamagishi et al., 2016). In contrast, we adopt the broader range of style variations considered in our prior work, which introduced the FSMT task (Niu et al., 2017): in FSMT, the MT system takes a desired formality level as 1009 an additional input, to represent the target audience of a translation, which human translators implicitly take into account. This task was addressed via n-best re-ranking in phrase-based MT — translation hypotheses whose formality are closer to desired formality are promoted. By contrast, in this work we use neural MT which is based on the Att"
C18-1086,W11-2826,0,0.232115,"of parallel standard and simple Wikipedia (and sometimes additional human rewrites) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax-based MT (Zhu et al., 2010; Xu et al., 2016), phrase-based MT (Coster and Kauchak, 2011; Wubben et al., 2012) to neural MT (Wang et al., 2016) trained via reinforcement learning (Zhang and Lapata, 2017). Naturally occurring examples of parallel formal-informal sentences are harder to find. Prior work relied on synthetic examples generated based on lists of words of known formality (Sheikha and Inkpen, 2011). This state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, GYAFC (Grammarly’s Yahoo Answers Formality Corpus). 110K informal sentences were collected from Yahoo Answers and they were rewritten in a formal style via crowd-sourcing, which made it possible to benchmark style transfer systems based on both PBMT and NMT models (Rao and Tetreault, 2018). In this work, we leverage this corpus to enable multi-task FT and FSMT. Recent work also explores how to perform style transfer without parallel data. However, this line of work c"
C18-1086,N18-1136,1,0.841143,"for both transfer directions. FSMT data: We evaluate the FSMT models on a large-scale French to English (FR-EN) translation task. Examples are drawn from OpenSubtitles2016 (Lison and Tiedemann, 2016) which consists of movie and television subtitles and is thus more similar to the GYAFC corpus compared to news or parliament proceedings. This is a noisy dataset where aligned French and English sentences often do not have the same meaning, so we use a bilingual semantic similarity detector to select 20,005,000 least divergent examples from ∼27.5M deduplicated sentence pairs in the original set (Vyas et al., 2018). Selected examples are then randomly split into a 20M training pool, a 2.5K dev set and a 2.5K test set. Preprocessing: We apply four pre-processing steps to both FT and MT data: normalization, tokenization, true-casing, and joint source-target BPE with 32,000 operations for NMT (Sennrich et al., 2016b). NMT Configuration: We use the standard attentional encoder-decoder architecture implemented in the Sockeye toolkit (Hieber et al., 2017). Our translation model uses a bi-directional encoder with a single 1011 LSTM layer (Bahdanau et al., 2015) of size 512, multilayer perceptron attention with"
C18-1086,E17-1101,0,0.0237388,"o encode a sequence to a latent representation which is then decoded to get the style transferred output sequence (Mueller et al., 2017; Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Prabhumoye et al., 2018). Style in Machine Translation has received little attention in recent MT architectures. Mima et al. (1997) improve rule-based MT by using extra-linguistic information such as speaker’s role and gender. Lewis et al. (2015) and Niu and Carpuat (2016) equate style with domain, and train conversational MT systems by selecting in-domain (i.e. conversation-like) training data. Similarly, Wintner et al. (2017) and Michel and Neubig (2018) take an adaptation approach to personalize MT with gender-specific or speaker-specific data. Other work has focused on specific realizations of stylistic variations, such as T-V pronoun selection for translation into German (Sennrich et al., 2016a) or controlling voice (Yamagishi et al., 2016). In contrast, we adopt the broader range of style variations considered in our prior work, which introduced the FSMT task (Niu et al., 2017): in FSMT, the MT system takes a desired formality level as 1009 an additional input, to represent the target audience of a translation"
C18-1086,P12-1107,0,0.148234,"Missing"
C18-1086,C12-1177,0,0.161683,"FT task, and yielding competitive performance on FSMT without style-annotated translation examples. Along the way, we also improve over prior results on FT using a single NMT model that can transfer between styles in both directions. 2 Background Style Transfer can naturally be framed as a sequence to sequence translation problem given sentence pairs that are paraphrases in two distinct styles. These parallel style corpora are constructed by creatively collecting existing texts of varying styles, and are therefore rare and much smaller than machine translation parallel corpora. For instance, Xu et al. (2012) scrape modern translations of Shakespeare’s plays and use a phrase-based MT (PBMT) system to paraphrase Shakespearean English into/from modern English. Jhamtani et al. (2017) improve performance on this dataset using neural translation model with pointers to enable copy actions. The availability of parallel standard and simple Wikipedia (and sometimes additional human rewrites) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax-based MT (Zhu et al., 2010; Xu et al., 2016), phrase-based MT (Coster and Kauchak, 2011;"
C18-1086,Q16-1029,0,0.026278,"translation parallel corpora. For instance, Xu et al. (2012) scrape modern translations of Shakespeare’s plays and use a phrase-based MT (PBMT) system to paraphrase Shakespearean English into/from modern English. Jhamtani et al. (2017) improve performance on this dataset using neural translation model with pointers to enable copy actions. The availability of parallel standard and simple Wikipedia (and sometimes additional human rewrites) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax-based MT (Zhu et al., 2010; Xu et al., 2016), phrase-based MT (Coster and Kauchak, 2011; Wubben et al., 2012) to neural MT (Wang et al., 2016) trained via reinforcement learning (Zhang and Lapata, 2017). Naturally occurring examples of parallel formal-informal sentences are harder to find. Prior work relied on synthetic examples generated based on lists of words of known formality (Sheikha and Inkpen, 2011). This state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, GYAFC (Grammarly’s Yahoo Answers Formality Corpus). 110K informal sentences were collected from Yahoo Ans"
C18-1086,W16-4620,0,0.0379762,"improve rule-based MT by using extra-linguistic information such as speaker’s role and gender. Lewis et al. (2015) and Niu and Carpuat (2016) equate style with domain, and train conversational MT systems by selecting in-domain (i.e. conversation-like) training data. Similarly, Wintner et al. (2017) and Michel and Neubig (2018) take an adaptation approach to personalize MT with gender-specific or speaker-specific data. Other work has focused on specific realizations of stylistic variations, such as T-V pronoun selection for translation into German (Sennrich et al., 2016a) or controlling voice (Yamagishi et al., 2016). In contrast, we adopt the broader range of style variations considered in our prior work, which introduced the FSMT task (Niu et al., 2017): in FSMT, the MT system takes a desired formality level as 1009 an additional input, to represent the target audience of a translation, which human translators implicitly take into account. This task was addressed via n-best re-ranking in phrase-based MT — translation hypotheses whose formality are closer to desired formality are promoted. By contrast, in this work we use neural MT which is based on the Attentional Recurrent EncoderDecoder model (Bahdana"
C18-1086,D17-1062,0,0.0407789,"to paraphrase Shakespearean English into/from modern English. Jhamtani et al. (2017) improve performance on this dataset using neural translation model with pointers to enable copy actions. The availability of parallel standard and simple Wikipedia (and sometimes additional human rewrites) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax-based MT (Zhu et al., 2010; Xu et al., 2016), phrase-based MT (Coster and Kauchak, 2011; Wubben et al., 2012) to neural MT (Wang et al., 2016) trained via reinforcement learning (Zhang and Lapata, 2017). Naturally occurring examples of parallel formal-informal sentences are harder to find. Prior work relied on synthetic examples generated based on lists of words of known formality (Sheikha and Inkpen, 2011). This state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, GYAFC (Grammarly’s Yahoo Answers Formality Corpus). 110K informal sentences were collected from Yahoo Answers and they were rewritten in a formal style via crowd-sourcing, which made it possible to benchmark style transfer systems based on both PBMT and NMT model"
C18-1086,C10-1152,0,0.0768786,"ller than machine translation parallel corpora. For instance, Xu et al. (2012) scrape modern translations of Shakespeare’s plays and use a phrase-based MT (PBMT) system to paraphrase Shakespearean English into/from modern English. Jhamtani et al. (2017) improve performance on this dataset using neural translation model with pointers to enable copy actions. The availability of parallel standard and simple Wikipedia (and sometimes additional human rewrites) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax-based MT (Zhu et al., 2010; Xu et al., 2016), phrase-based MT (Coster and Kauchak, 2011; Wubben et al., 2012) to neural MT (Wang et al., 2016) trained via reinforcement learning (Zhang and Lapata, 2017). Naturally occurring examples of parallel formal-informal sentences are harder to find. Prior work relied on synthetic examples generated based on lists of words of known formality (Sheikha and Inkpen, 2011). This state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, GYAFC (Grammarly’s Yahoo Answers Formality Corpus). 110K informal sentences were collec"
carpuat-wu-2008-evaluation,koen-2004-pharaoh,0,\N,Missing
carpuat-wu-2008-evaluation,2007.tmi-papers.28,0,\N,Missing
carpuat-wu-2008-evaluation,A94-1030,1,\N,Missing
carpuat-wu-2008-evaluation,J96-1002,0,\N,Missing
carpuat-wu-2008-evaluation,P02-1040,0,\N,Missing
carpuat-wu-2008-evaluation,P01-1027,0,\N,Missing
carpuat-wu-2008-evaluation,W05-0909,0,\N,Missing
carpuat-wu-2008-evaluation,P05-1048,1,\N,Missing
carpuat-wu-2008-evaluation,D07-1007,1,\N,Missing
carpuat-wu-2008-evaluation,P07-1005,0,\N,Missing
carpuat-wu-2008-evaluation,J03-1002,0,\N,Missing
carpuat-wu-2008-evaluation,J97-3002,1,\N,Missing
carpuat-wu-2008-evaluation,2007.tmi-papers.6,1,\N,Missing
carpuat-wu-2008-evaluation,garcia-varea-etal-2002-efficient,0,\N,Missing
carpuat-wu-2008-evaluation,W04-0822,1,\N,Missing
D07-1007,W05-0909,0,0.038279,"Missing"
D07-1007,P91-1034,0,0.141512,"ation direction, and take full advantage of not residing strictly within the Bayesian sourcechannel model in order to benefit from the much richer Senseval-style feature set this facilitates. Garcia Varea et al. found that the best results are obtained when the training of the context-dependent translation model is fully incorporated with the EM training of the SMT system. As described below, the training of our new WSD model, though not incorporated within the EM training, is also far more closely tied to the SMT model than is the case with traditional standalone WSD models. In contrast with Brown et al. (1991), our approach incorporates the predictions of state-of-theart WSD models that use rich contextual features for any phrase in the input vocabulary. In Brown et al.’s early study of WSD impact on SMT performance, the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT archit"
D07-1007,I05-2021,1,0.881308,"em, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. Common assumptions about the role and usefulness of word sense disambiguation (WSD) models in full-scale statistical machine translation (SMT) systems have recently been challenged. On the one hand, in previous work (Carpuat and Wu, 2005b) we obtained disappointing results when using the predictions of a Senseval WSD system in conjunction with a standard word-based SMT system: we reported slightly lower BLEU scores despite trying to incorporate WSD using a number of apparently sensible methods. These results cast doubt on the assumption that sophisticated dedicated WSD systems that were developed independently from any particular NLP application can easily be integrated into a SMT system so as to improve translation quality through stronger models of context and rich linguistic information. Rather, it has been argued, SMT sys"
D07-1007,P05-1048,1,0.924497,"em, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. Common assumptions about the role and usefulness of word sense disambiguation (WSD) models in full-scale statistical machine translation (SMT) systems have recently been challenged. On the one hand, in previous work (Carpuat and Wu, 2005b) we obtained disappointing results when using the predictions of a Senseval WSD system in conjunction with a standard word-based SMT system: we reported slightly lower BLEU scores despite trying to incorporate WSD using a number of apparently sensible methods. These results cast doubt on the assumption that sophisticated dedicated WSD systems that were developed independently from any particular NLP application can easily be integrated into a SMT system so as to improve translation quality through stronger models of context and rich linguistic information. Rather, it has been argued, SMT sys"
D07-1007,W04-0822,1,0.64632,"o extract the WSD training data therefore depends on the one used by the SMT system. This presents the advantage of training WSD and SMT models on exactly the same data, thus eliminating domain mismatches between Senseval data and parallel corpora. But most importantly, this allows WSD training data to be generated entirely automatically, since the parallel corpus is automatically phrase-aligned in order to learn the SMT phrase bilexicon. 4.4 The WSD system The word sense disambiguation subsystem is modeled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al., 2004). The features employed are typical of WSD and are therefore far richer than those used in most SMT systems. The feature set consists of positionsensitive, syntactic, and local collocational features, since these features yielded the best results when combined in a na¨ıve Bayes model on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002). These features scale easily to the bigger vocabulary and sense candidates to be considered in a SMT task. The Senseval system consists of an ensemble of four combined WSD models: The first model is a na¨ıve Bayes model, since Yarowsky and Flo"
D07-1007,2006.iwslt-evaluation.5,1,0.848993,"In this paper, we present first results with a new architecture that integrates a state-of-the-art WSD model into phrase-based SMT so as to perform multi-word phrasal lexical disambiguation, and show that this new WSD approach not only produces gains across all available Chinese-English IWSLT06 test sets for all eight commonly used automated MT evaluation metrics, but also produces statistically significant gains on the much larger NIST Chinese-English task. The main difference between this approach and several of our earlier approaches as described in Carpuat and Wu (2005b) and subsequently Carpuat et al. (2006) lies in the fact that we focus on repurposing the WSD system for multi-word phrase-based SMT. Rather than using a generic Senseval WSD model as we did in Carpuat and Wu (2005b), here both the WSD training and the WSD predictions are integrated into the phrase-based SMT framework. Furthermore, rather than using a single word based WSD approach to augment a phrase-based SMT model as we did in Carpuat et al. (2006) to improve BLEU and NIST scores, here the WSD training and predictions operate on full multi-word phrasal units, resulting in significantly more reliable and consistent gains as evalu"
D07-1007,W07-0403,0,0.0489086,"Missing"
D07-1007,W04-0802,0,0.151868,"ion candidates seen during SMT training. • Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system. However, despite these adaptations to the SMT task, the core sense disambiguation task remains pure WSD: • The rich context features are typical of WSD and almost never used in SMT. • The dynamic integration of context-sensitive translation probabilities is not typical of SMT. • Although it is embedded in a real SMT system, the WSD task is exactly the same as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. We begin by presenting the WSD module and the SMT integration technique. We then show that incorporating it into a standard phrase-based SMT baseline system consistently improves translation quality across all three different test sets from the Chinese-English IWSLT text translation evaluation, as well as on the larger NIST Chinese-English translation task. Depending on the metric, the individual gains are sometimes modest, but remarkably, incorporating WSD never hurts, and helps enough to always make it a"
D07-1007,P04-1039,0,0.0176286,"based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures. 3 Problems in translation-oriented WSD The close relationship between WSD and SMT has been emphasized since the emergence of WSD as an independent task. However, most of previous research has focused on using multilingual resources typically used in SMT systems to improve WSD accuracy, e.g., Dagan and Itai (1994), Li and Li (2002), Diab (2004). In contrast, this paper focuses on the converse goal of using WSD models to improve actual translation quality. Recently, several researchers have focused on designing WSD systems for the specific purpose of translation. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English translation,"
D07-1007,P01-1027,0,0.207028,"babilities versus the WSD model predictions for single words. In addition, the single-word model does not generalize to WSD for phrasal lexical choice, as overlapping spans cannot be specified with the XML markup scheme. Providing WSD predictions for phrases would require committing to a phrase segmentation of the input sentence before decoding, which is likely to hurt translation quality. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical 63 choice model, but not improved translation accuracy. In contrast, our evaluation in this paper is conducted on the actual decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used, rather than only BLEU score, so as to maintain a more balanced perspective. Another problem in the context-sensitive lexical choice in SMT models of Garcia Varea et al. is that their feat"
D07-1007,garcia-varea-etal-2002-efficient,0,0.0665362,"predictions for single words. In addition, the single-word model does not generalize to WSD for phrasal lexical choice, as overlapping spans cannot be specified with the XML markup scheme. Providing WSD predictions for phrases would require committing to a phrase segmentation of the input sentence before decoding, which is likely to hurt translation quality. It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al. (2001) and Garcia-Varea et al. (2002) show improved alignment error rate with a maximum entropy based context-dependent lexical 63 choice model, but not improved translation accuracy. In contrast, our evaluation in this paper is conducted on the actual decoding task, rather than intermediate tasks such as word alignment. Moreover, in the present work, all commonly available automated MT evaluation metrics are used, rather than only BLEU score, so as to maintain a more balanced perspective. Another problem in the context-sensitive lexical choice in SMT models of Garcia Varea et al. is that their feature set is insufficiently rich"
D07-1007,S01-1004,0,0.00733519,"incorporating WSD never hurts, and helps enough to always make it a worthwile additional component in an SMT system. Finally, we analyze the reasons for the improvement. 2 Problems in context-sensitive lexical choice for SMT To the best of our knowledge, there has been no previous attempt at integrating a state-of-the-art WSD system for fully phrasal multi-word lexical choice into phrase-based SMT, with evaluation of the resulting system on a translation task. While there are many evaluations of WSD quality, in particular the Senseval series of shared tasks (Kilgarriff and Rosenzweig (1999), Kilgarriff (2001), Mihalcea et al. (2004)), very little work has been done to address the actual integration of WSD in realistic SMT applications. To fully integrate WSD into phrase-based SMT, it is necessary to perform lexical disambiguation on multi-word phrasal lexical units; in contrast, the model reported in Cabezas and Resnik (2005) can only perform lexical disambiguation on single words. Like the model proposed in this paper, Cabezas and Resnik attempted to integrate phrasebased WSD models into decoding. However, although they reported that incorporating these predictions via the Pharaoh XML markup sche"
D07-1007,W02-1002,0,0.00644799,"ed the best results when combined in a na¨ıve Bayes model on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002). These features scale easily to the bigger vocabulary and sense candidates to be considered in a SMT task. The Senseval system consists of an ensemble of four combined WSD models: The first model is a na¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (Klein and Manning, 2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third model is a boosting model (Freund and Schapire, 1997), since boosting has consistently turned in very competitive scores on related tasks such as named entity classification. We also use the Adaboost.MH algorithm. The fourth model is a Kernel PCA-based model (Wu et al., 2004). Kernel Principal Component Analysis or KPCA is a nonlinear kernel method for 65 extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonl"
D07-1007,N03-1017,0,0.0269456,"(Carpuat and Wu, 2007). 4.2 WSD uses the same sense definitions as the SMT system Instead of using pre-defined sense inventories, the WSD models disambiguate between the SMT translation candidates. In order to closely integrate WSD predictions into the SMT system, we need to formulate WSD models so that they produce features that can directly be used in translation decisions taken by the SMT system. It is therefore necessary for the WSD and SMT systems to consider exactly the same translation candidates for a given word in the input language. Assuming a standard phrase-based SMT system (e.g., Koehn et al. (2003)), WSD senses are thus either words or phrases, as learned in the SMT phrasal translation lexicon. Those “sense” candidates are very different from those typically used even in dedicated WSD tasks, even in the multilingual Senseval tasks. Each candidate is a phrase that is not necessarily a syntactic noun or verb phrase as in manually compiled dictionaries. It is quite possible that distinct “senses” in our WSD for SMT system could be considered synonyms in a traditional WSD framework, especially in monolingual WSD. In addition to the consistency requirements for integration, this requirement"
D07-1007,koen-2004-pharaoh,0,0.0645239,"input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 4.5 Integrating WSD predictions in phrase-based SMT architectures It is non-trivial to incorporate WSD into an existing phrase-based architecture such as Pharaoh (Koehn, 2004), since the decoder is not set up to easily accept multiple translation probabilities that are dynamically computed in context-sensitive fashion. For every phrase in a given SMT input sentence, the WSD probabilities can be used as additional feature in a loglinear translation model, in combination with typical context-independent SMT bilexicon probabilities. We overcome this obstacle by devising a calling architecture that reinitializes the decoder with dynamically generated lexicons on a per-sentence basis. Unlike a n-best reranking approach, which is limited by the lexical choices made by th"
D07-1007,P02-1044,0,0.0298416,"for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures. 3 Problems in translation-oriented WSD The close relationship between WSD and SMT has been emphasized since the emergence of WSD as an independent task. However, most of previous research has focused on using multilingual resources typically used in SMT systems to improve WSD accuracy, e.g., Dagan and Itai (1994), Li and Li (2002), Diab (2004). In contrast, this paper focuses on the converse goal of using WSD models to improve actual translation quality. Recently, several researchers have focused on designing WSD systems for the specific purpose of translation. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English"
D07-1007,W04-0807,0,0.0596988,"never hurts, and helps enough to always make it a worthwile additional component in an SMT system. Finally, we analyze the reasons for the improvement. 2 Problems in context-sensitive lexical choice for SMT To the best of our knowledge, there has been no previous attempt at integrating a state-of-the-art WSD system for fully phrasal multi-word lexical choice into phrase-based SMT, with evaluation of the resulting system on a translation task. While there are many evaluations of WSD quality, in particular the Senseval series of shared tasks (Kilgarriff and Rosenzweig (1999), Kilgarriff (2001), Mihalcea et al. (2004)), very little work has been done to address the actual integration of WSD in realistic SMT applications. To fully integrate WSD into phrase-based SMT, it is necessary to perform lexical disambiguation on multi-word phrasal lexical units; in contrast, the model reported in Cabezas and Resnik (2005) can only perform lexical disambiguation on single words. Like the model proposed in this paper, Cabezas and Resnik attempted to integrate phrasebased WSD models into decoding. However, although they reported that incorporating these predictions via the Pharaoh XML markup scheme yielded a small impro"
D07-1007,P03-1058,0,0.241453,"ly, several researchers have focused on designing WSD systems for the specific purpose of translation. Vickrey et al. (2005) train a logistic regression WSD model on data extracted from automatically word aligned parallel corpora, but evaluate on a blank filling task, which is essentially an evaluation of WSD accuracy. Specia (2006) describes an inductive logic programming-based WSD system, which was specifically designed for the purpose of Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a full-scale machine translation system. Ng et al. (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. The purpose of the study was to lower the annotation cost for supervised WSD, as suggested earlier by Resnik and Yarowsky (1999). However this result is also encouraging for the integration of WSD in SMT, since it suggests that accurate WSD can be achieved using training data of the kind needed for SMT. 4 4.1 Building WSD models for phrase-based SMT WSD models for every phrase in the input vocabulary Just like for the baseline phrase translation model, WSD models are defined fo"
D07-1007,J03-1002,0,0.00760388,"ized. The Chinese side was word segmented using the LDC segmenter. 5.2 Baseline SMT system Since our focus is not on a specific SMT architecture, we use the off-the-shelf phrase-based decoder 66 translation predictions improves WER PER CDER 88.26 83.87 61.71 57.29 70.32 67.38 Pharaoh (Koehn, 2004) trained on the IWSLT training set. Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used. The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall using the grow-diag-final heuristic. The language model is trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights are learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline, and the WSD-augmented system. Due to time constraints, this optimization was only conducted on the IWSLT task. The weights used in the WSD-augmented NIST model are based on the best IWSLT model. Given that the two tasks are quite different, we expect further improvem"
D07-1007,P03-1021,0,0.0252698,"SLT training set. Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used. The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall using the grow-diag-final heuristic. The language model is trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002). The loglinear model weights are learned using Chiang’s implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline, and the WSD-augmented system. Due to time constraints, this optimization was only conducted on the IWSLT task. The weights used in the WSD-augmented NIST model are based on the best IWSLT model. Given that the two tasks are quite different, we expect further improvements on the WSD-augmented system after running maximum BLEU optimization for the NIST task. 6 Results and discussion Using WSD predictions in SMT yields better translation quality on all test sets, as measured by all eight commonly used automatic evaluation metrics. Table 3: Translation examples with and wi"
D07-1007,P02-1040,0,0.107212,"Missing"
D07-1007,2006.iwslt-evaluation.1,0,0.00627121,"WSD 51.05 9.142 74.13 71.44 34.68 39.75 31.71 34.58 Table 2: Evaluation results on the NIST test set: integrating the WSD BLEU, NIST, METEOR, WER, PER, CDER and TER Exper. BLEU NIST METEOR METEOR TER (no syn) SMT 20.41 7.155 60.21 56.15 76.76 SMT+WSD 20.92 7.468 60.30 56.79 71.34 5.1 Data set Preliminary experiments are conducted using training and evaluation data drawn from the multilingual BTEC corpus, which contains sentences used in conversations in the travel domain, and their translations in several languages. A subset of this data was made available for the IWSLT06 evaluation campaign (Paul, 2006); the training set consists of 40000 sentence pairs, and each test set contains around 500 sentences. We used only the pure text data, and not the speech transcriptions, so that speech-specific issues would not interfere with our primary goal of understanding the effect of integrating WSD in a fullscale phrase-based model. A larger scale evaluation is conducted on the standard NIST Chinese-English test set (MT-04), which contains 1788 sentences drawn from newswire corpora, and therefore of a much wider domain than the IWSLT data set. The training set consists of about 1 million sentence pairs"
D07-1007,2006.amta-papers.25,0,0.0330913,"Missing"
D07-1007,W06-2505,0,0.337401,"int Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 61–72, Prague, June 2007. 2007 Association for Computational Linguistics and unsupervised, on a Senseval WSD task (Carpuat and Wu, 2005a), and therefore suggest that WSD should have a role to play in state-of-the-art SMT systems. In addition to the Senseval shared tasks, which have provided standard sense inventories and data sets, WSD research has also turned increasingly to designing specific models for a particular application. For instance, Vickrey et al. (2005) and Specia (2006) proposed WSD systems designed for French to English, and Portuguese to English translation respectively, and present a more optimistic outlook for the use of WSD in MT, although these WSD systems have not yet been integrated nor evaluated in full-scale machine translation systems. Taken together, these seemingly contradictory results suggest that improving SMT lexical choice accuracy remains a key challenge to improve current SMT quality, and that it is still unclear what is the most appropriate integration framework for the WSD models in SMT. In this paper, we present first results with a ne"
D07-1007,H05-1097,0,0.67277,". ∗ 61 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 61–72, Prague, June 2007. 2007 Association for Computational Linguistics and unsupervised, on a Senseval WSD task (Carpuat and Wu, 2005a), and therefore suggest that WSD should have a role to play in state-of-the-art SMT systems. In addition to the Senseval shared tasks, which have provided standard sense inventories and data sets, WSD research has also turned increasingly to designing specific models for a particular application. For instance, Vickrey et al. (2005) and Specia (2006) proposed WSD systems designed for French to English, and Portuguese to English translation respectively, and present a more optimistic outlook for the use of WSD in MT, although these WSD systems have not yet been integrated nor evaluated in full-scale machine translation systems. Taken together, these seemingly contradictory results suggest that improving SMT lexical choice accuracy remains a key challenge to improve current SMT quality, and that it is still unclear what is the most appropriate integration framework for the WSD models in SMT. In this paper, we present first"
D07-1007,P98-2230,1,0.61049,"Missing"
D07-1007,P04-1081,1,0.558695,"e the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (Klein and Manning, 2002) found that this model yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. The third model is a boosting model (Freund and Schapire, 1997), since boosting has consistently turned in very competitive scores on related tasks such as named entity classification. We also use the Adaboost.MH algorithm. The fourth model is a Kernel PCA-based model (Wu et al., 2004). Kernel Principal Component Analysis or KPCA is a nonlinear kernel method for 65 extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. All these classifiers have the ability to handle large numbers"
D07-1007,P96-1021,1,0.501543,"Missing"
D07-1007,C04-1030,0,0.59701,"en during SMT training. • Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system. However, despite these adaptations to the SMT task, the core sense disambiguation task remains pure WSD: • The rich context features are typical of WSD and almost never used in SMT. • The dynamic integration of context-sensitive translation probabilities is not typical of SMT. • Although it is embedded in a real SMT system, the WSD task is exactly the same as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g., Chklovski et al. (2004)), where sense inventories represent the semantic distinctions made by another language. We begin by presenting the WSD module and the SMT integration technique. We then show that incorporating it into a standard phrase-based SMT baseline system consistently improves translation quality across all three different test sets from the Chinese-English IWSLT text translation evaluation, as well as on the larger NIST Chinese-English translation task. Depending on the metric, the individual gains are sometimes modest, but remarkably, incorporating WSD never hurts, and helps enough to always make it a"
D07-1007,E06-1031,0,\N,Missing
D07-1007,J94-4003,0,\N,Missing
D07-1007,J04-1001,0,\N,Missing
D07-1007,P06-3010,0,\N,Missing
D07-1007,J97-3002,1,\N,Missing
D07-1007,C98-2225,1,\N,Missing
D07-1007,2007.tmi-papers.6,1,\N,Missing
D17-1299,I13-1010,0,0.0329734,"utomatically analyzing and generating natural language requires capturing not only what is said, but also how to say it. Consider the sentences “anybody hurt?” and “is someone wounded?”. The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used. Such differences in formality have been identified as an important dimension of style (Trudgill, 1992) or tone (Halliday, 1978) variation. In this paper, we build on prior computational work that has focused on analyzing formality of texts (Lahiri and Lu, 2011; Brooke and Hirst, 2013; Pavlick and Nenkova, 2015; Pavlick and Tetreault, 2016) with a different aim: modeling formality for the purpose of controlling style in applications that generate language, with a focus on machine translation. Human translators translate a document for a specific audience (Nida and Taber Charles, 1969), and often ask what is the expected tone of the content when taking a new translation job. We design a machine translation system that operates under similar conditions and explicitly takes an expected level of formality as input. While ultimately we would like systems to preserve the formali"
D17-1299,C14-1205,0,0.417506,"ring weights help promote candidate sentences whose formality scores approach the expected level. 3 Formality Modeling The FSMT system requires quantifying the formality level of a sentence. Following prior work, we define sentence-level formality based on lexical formality scores (Brooke et al., 2010; Pavlick and Nenkova, 2015). We conduct an empirical comparison of existing techniques that can be adapted as lexical formality models, and introduce a sentence-level formality scheme based on weighted average. 3.1 Lexical Formality State-of-the-art lexical formality models (Brooke et al., 2010; Brooke and Hirst, 2014) are based on vector space models of word meaning, and a set of pre-selected seed words that are representative of formal and informal language. SimDiff Brooke et al. (2010) proposed to score the formality of a word w by comparing its meaning to that of seed words of known formality using cosine similarity. Intuitively, w is more likely formal if it is semantically closer to formal seed words than to informal seed words. Formally, given a formal word set Sf and an informal word set Si , SimDiff scores a word w by score(w) = The neutral word is typically selected from function words. We select"
D17-1299,C10-2011,0,0.137074,"the sentence-level formality score for h. f (h; `), along with standard model features, is fed into a standard re-ranking model. When training the re-ranking model, the parameter ` is set to the actual formality score of the reference translation for each instance. At test time, ` is provided by the user. The re-scoring weights help promote candidate sentences whose formality scores approach the expected level. 3 Formality Modeling The FSMT system requires quantifying the formality level of a sentence. Following prior work, we define sentence-level formality based on lexical formality scores (Brooke et al., 2010; Pavlick and Nenkova, 2015). We conduct an empirical comparison of existing techniques that can be adapted as lexical formality models, and introduce a sentence-level formality scheme based on weighted average. 3.1 Lexical Formality State-of-the-art lexical formality models (Brooke et al., 2010; Brooke and Hirst, 2014) are based on vector space models of word meaning, and a set of pre-selected seed words that are representative of formal and informal language. SimDiff Brooke et al. (2010) proposed to score the formality of a word w by comparing its meaning to that of seed words of known forma"
D17-1299,N12-1047,0,0.0196188,"plit into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments). Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles. Moses (Koehn et al., 2007) is used to build our phrase-based MT system. We followed the standard training pipeline with default parameters.3 Word alignments were generated using fast align (Dyer et al., 2013), and symmetrized using the grow-diag-final-and heuristic. We used 4-gram language models, trained using KenLM (Heafield, 2011). Model weights were tuned using batch MIRA (Cherry and Foster, 2012). We used constant size n=1000 for n-best lists in all experiments. The re-ranking is a log-linear model trained using batch MIRA. 4 We report results averaged over 5 random tuning re-starts to compensate for tuning noise (Clark et al., 2011). FSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human reference translations with different 3 1 https://catalog.ldc.upenn.edu/ LDC97S42 2 https://talkbank.org/access/CABank/ CallFriend/ LSA W2V 0.660 0.654 0.657 0.585 0.656 0.663 0.664 0.644 0.540 http:"
D17-1299,cieri-etal-2004-fisher,0,0.0203611,"we normalized the LSA word vectors to make them have unit length for SVM and PCA, but did not applied it to word2vec. This suggests that the magnitude of LSA word vectors is harmful for formality modeling. We also compared formality models based on word representations to a baseline that relies on unigram models to compare word statistics in corpora representative of formal vs. informal language (Pavlick and Nenkova, 2015). This method requires language examples of diverse formality. Conversational transcripts are generally considered as casual text, so we concatenated corpora such as Fisher (Cieri et al., 2004), Switchboard (Godfrey et al., 1992), SBCSAE (Bois et al., 20002005), CallHome1 , CallFriend2 , BOLT SMS/Chat (Song et al., 2014) and NPS Chatroom (Forsythand and Martell, 2007). As the formal counterpart, we extracted comparable size of text from Europarl (Koehn, 2005). This results in 30 Million tokens of formal corpora (1.1M segments) and 29 Million tokens of informal corpora (2.7M segments). Table 1 shows that all models based on the vector space achieve similar performance in terms of Spearman’s ρ (except SVM-W2V which yields lower performance). The baseline method based on unigram models"
D17-1299,P11-2031,0,0.0121614,"is used to build our phrase-based MT system. We followed the standard training pipeline with default parameters.3 Word alignments were generated using fast align (Dyer et al., 2013), and symmetrized using the grow-diag-final-and heuristic. We used 4-gram language models, trained using KenLM (Heafield, 2011). Model weights were tuned using batch MIRA (Cherry and Foster, 2012). We used constant size n=1000 for n-best lists in all experiments. The re-ranking is a log-linear model trained using batch MIRA. 4 We report results averaged over 5 random tuning re-starts to compensate for tuning noise (Clark et al., 2011). FSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human reference translations with different 3 1 https://catalog.ldc.upenn.edu/ LDC97S42 2 https://talkbank.org/access/CABank/ CallFriend/ LSA W2V 0.660 0.654 0.657 0.585 0.656 0.663 0.664 0.644 0.540 http://www.statmt.org/moses/?n=Moses. Baseline 4 https://github.com/moses-smt/ mosesdecoder/tree/master/scripts/ nbest-rescore 2816 Desired formality None (baseline) low neutral high Informal test set 39.74 40.27 38.70 37.58 Neutral test set 40.17"
D17-1299,N13-1073,0,0.0185992,"16), which is extracted from movie and TV subtitles, covers a wider spectrum of styles, but overall tends to be informal since it primarily contains conversations. Each parallel corpus was split into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments). Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles. Moses (Koehn et al., 2007) is used to build our phrase-based MT system. We followed the standard training pipeline with default parameters.3 Word alignments were generated using fast align (Dyer et al., 2013), and symmetrized using the grow-diag-final-and heuristic. We used 4-gram language models, trained using KenLM (Heafield, 2011). Model weights were tuned using batch MIRA (Cherry and Foster, 2012). We used constant size n=1000 for n-best lists in all experiments. The re-ranking is a log-linear model trained using batch MIRA. 4 We report results averaged over 5 random tuning re-starts to compensate for tuning noise (Clark et al., 2011). FSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human ref"
D17-1299,eisele-chen-2010-multiun,0,0.0189662,"s based on the vector space achieve similar performance in terms of Spearman’s ρ (except SVM-W2V which yields lower performance). The baseline method based on unigram models was outperformed by 0.1+ point. So we select D ENSIFIER-LSA as a representative for our FSMT system. SimDiff SVM PCA D ENSIFIER baseline Table 1: Sentence-level formality quantifying evaluation (Spearman’s ρ) among different models with different vector spaces. 4 Evaluation of the FSMT System Set-up We evaluate this approach on a French to English translation task. Two parallel FrenchEnglish corpora are used: (1) MultiUN (Eisele and Chen, 2010), which is extracted from the United Nations website, and can be considered to be formal text; (2) OpenSubtitles2016 (Lison and Tiedemann, 2016), which is extracted from movie and TV subtitles, covers a wider spectrum of styles, but overall tends to be informal since it primarily contains conversations. Each parallel corpus was split into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments). Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles. Moses (Koehn et al., 2007) is used to build our p"
D17-1299,W13-2305,0,0.0113999,"ions were restricted to the nbest list, not all sentences could be translated into stylistically different language. Of the remaining 21 pairs where annotators judged one output more formal than the other, in all but one case the translation produced by our FSMT system with high formality level parameter was judged to be more formal. Overall this indicates that our formality scoring and ranking procedure are effective. To determine whether re-ranking based on formality might have a detrimental effect on quality, we also had annotators rate the fluency and adequacy of the segments. Inspired by Graham et al. (2013), annotators were first asked to assess fluency without a reference and separately adequacy with a reference. Both assessments used a sliding scale. Each segment was evaluated by an average of 7 annotators. After rescaling the ratings into the [0, 1] range, we observed a 0.75 level of fluency for informal translations and 0.70 for formal ones. This slight difference fits our expectation that more casual language may feel more fluent while more formal language may feel more stilted. The adequacy ratings were 0.65 and 0.64 for informal and translations respectively, indicating that adjusting the"
D17-1299,N15-1023,0,0.476777,"nd generating natural language requires capturing not only what is said, but also how to say it. Consider the sentences “anybody hurt?” and “is someone wounded?”. The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used. Such differences in formality have been identified as an important dimension of style (Trudgill, 1992) or tone (Halliday, 1978) variation. In this paper, we build on prior computational work that has focused on analyzing formality of texts (Lahiri and Lu, 2011; Brooke and Hirst, 2013; Pavlick and Nenkova, 2015; Pavlick and Tetreault, 2016) with a different aim: modeling formality for the purpose of controlling style in applications that generate language, with a focus on machine translation. Human translators translate a document for a specific audience (Nida and Taber Charles, 1969), and often ask what is the expected tone of the content when taking a new translation job. We design a machine translation system that operates under similar conditions and explicitly takes an expected level of formality as input. While ultimately we would like systems to preserve the formality of the source, this is a"
D17-1299,Q16-1005,0,0.104745,"age requires capturing not only what is said, but also how to say it. Consider the sentences “anybody hurt?” and “is someone wounded?”. The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used. Such differences in formality have been identified as an important dimension of style (Trudgill, 1992) or tone (Halliday, 1978) variation. In this paper, we build on prior computational work that has focused on analyzing formality of texts (Lahiri and Lu, 2011; Brooke and Hirst, 2013; Pavlick and Nenkova, 2015; Pavlick and Tetreault, 2016) with a different aim: modeling formality for the purpose of controlling style in applications that generate language, with a focus on machine translation. Human translators translate a document for a specific audience (Nida and Taber Charles, 1969), and often ask what is the expected tone of the content when taking a new translation job. We design a machine translation system that operates under similar conditions and explicitly takes an expected level of formality as input. While ultimately we would like systems to preserve the formality of the source, this is a Marine Carpuat Dept. of Compu"
D17-1299,N16-1091,0,0.00943653,"Missing"
D17-1299,W11-2123,0,0.0190656,"primarily contains conversations. Each parallel corpus was split into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments). Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles. Moses (Koehn et al., 2007) is used to build our phrase-based MT system. We followed the standard training pipeline with default parameters.3 Word alignments were generated using fast align (Dyer et al., 2013), and symmetrized using the grow-diag-final-and heuristic. We used 4-gram language models, trained using KenLM (Heafield, 2011). Model weights were tuned using batch MIRA (Cherry and Foster, 2012). We used constant size n=1000 for n-best lists in all experiments. The re-ranking is a log-linear model trained using batch MIRA. 4 We report results averaged over 5 random tuning re-starts to compensate for tuning noise (Clark et al., 2011). FSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human reference translations with different 3 1 https://catalog.ldc.upenn.edu/ LDC97S42 2 https://talkbank.org/access/CABank/ CallFriend"
D17-1299,2005.mtsummit-papers.11,0,0.00844188,"seline that relies on unigram models to compare word statistics in corpora representative of formal vs. informal language (Pavlick and Nenkova, 2015). This method requires language examples of diverse formality. Conversational transcripts are generally considered as casual text, so we concatenated corpora such as Fisher (Cieri et al., 2004), Switchboard (Godfrey et al., 1992), SBCSAE (Bois et al., 20002005), CallHome1 , CallFriend2 , BOLT SMS/Chat (Song et al., 2014) and NPS Chatroom (Forsythand and Martell, 2007). As the formal counterpart, we extracted comparable size of text from Europarl (Koehn, 2005). This results in 30 Million tokens of formal corpora (1.1M segments) and 29 Million tokens of informal corpora (2.7M segments). Table 1 shows that all models based on the vector space achieve similar performance in terms of Spearman’s ρ (except SVM-W2V which yields lower performance). The baseline method based on unigram models was outperformed by 0.1+ point. So we select D ENSIFIER-LSA as a representative for our FSMT system. SimDiff SVM PCA D ENSIFIER baseline Table 1: Sentence-level formality quantifying evaluation (Spearman’s ρ) among different models with different vector spaces. 4 Evalu"
D17-1299,L16-1147,0,0.0561316,"Missing"
D17-1299,P07-2045,0,\N,Missing
D17-1299,song-etal-2014-collecting,0,\N,Missing
D19-1166,D16-1034,0,0.0533334,"Missing"
D19-1166,gerber-hovy-1998-improving,0,0.385229,"ria, 2007; Siddharthan and Katsos, 2010). Such readers would also benefit from MT output that is better targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification has been used to improve MT by restructuring complex sentences into shorter and simpler segments that are easier to translate (Gerber and Hovy, 1998; Štajner and Popovic, 2016; Hasler et al., 2017). Contemporaneously to our work, Marchisio et al. (2019) show that tagging the English side of parallel corpora with automatic readability scores can help translate the same input in a simpler or more complex form. Our work shares the goal of controlling translation complexity, but considers a broader range of reading grade levels and simplification operations grounded in professionally edited text simplification corpora. Building a model for this task ideally requires rich annotation for evaluation and supervised training that is not available"
D19-1166,W08-0909,0,0.253351,"ion and SARI shows high correlation with human scores for simplicity. In the cross-lingual setting, we cannot directly compare the Spanish input with English hypotheses and references, therefore we use the baseline machine translation of Spanish into English as a pseudo-source text. The resulting SARI score directly measures the improvement over baseline machine translation. In addition to BLEU and SARI, we report Pearson’s correlation coefficient (PCC) to measure the strength of the linear relationship between the complexity of our system outputs and the complexity of reference translations. Heilman et al. (2008) use it to evaluate the performance of reading difficulty prediction. Here we estimate the reading grade level complexity of MT outputs and reference translations using the Automatic Readability Index (ARI)4 score, which combines evidence from the number of characters per word and number of words per sentence using hand-tuned 3 https://github.com/cocoxu/ simplification 4 https://github.com/mmautner/ readability 1553 Source (Spanish) Target (English) Grade word types tokens/segment sents/segment word types tokens/segment sents/segment 2 3 4 5 6 7 8 9 10 12 2,628 8,431 17,082 16,945 22,352 19,31"
D19-1166,E17-3017,0,0.0290923,"Newsela based on aligned segments between Spanish and English articles that have the same reading grade level. We use this dataset to provide in-domain MT training examples which includes roughly 70k instances. All datasets are pre-processed using Moses tools for normalization, tokenization and truecasing (Koehn et al., 2007). We further segment tokens into subwords using a joint source-target byte pair encoding model with 32,000 operations (Sennrich et al., 2015). 5.3 Sequence-to-Sequence Model Configuration We use the standard encoder-decoder architecture implemented in the Sockeye toolkit (Hieber et al., 2017). Both encoder and decoder have two Long Short Term Memory (LSTM) layers (Bahdanau et al., 2015), hidden states of size 500 and dropout of 0.3 applied to the RNNs of the encoder and decoder which is same as what was used by Scarton and Specia (2018). We observe that dot product based attention works best in our scenario, perhaps indicating that the task of complexity controlled translation requires mostly local changes that do not lead to long distance reorderings across sentences. We train using the Adam (Kingma and Ba, 2014) optimizer with a batch size of 256 segments and checkpoint the mode"
D19-1166,W03-1602,0,0.124876,"show that these multitask models outperform pipeline approaches that translate and simplify text independently. 1 Introduction Generating text at the right level of complexity can make machine translation (MT) more useful for a wide range of users. As Xu et al. (2015) note, simplifying text makes it possible to develop reading aids for people with low-literacy (Watanabe et al., 2009; De Belder and Moens, 2010), for non-native speakers and language learners (Petersen and Ostendorf, 2007; Allen, 2009), for people who suffer from language impairments (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), and for readers lacking expert knowledge of the topic discussed (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Such readers would also benefit from MT output that is better targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification has been used to improve MT by restructuring"
D19-1166,E99-1042,0,0.357382,"ng grade level than the original Spanish. We show that these multitask models outperform pipeline approaches that translate and simplify text independently. 1 Introduction Generating text at the right level of complexity can make machine translation (MT) more useful for a wide range of users. As Xu et al. (2015) note, simplifying text makes it possible to develop reading aids for people with low-literacy (Watanabe et al., 2009; De Belder and Moens, 2010), for non-native speakers and language learners (Petersen and Ostendorf, 2007; Allen, 2009), for people who suffer from language impairments (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), and for readers lacking expert knowledge of the topic discussed (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Such readers would also benefit from MT output that is better targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification ha"
D19-1166,C96-2183,0,0.101514,"al., 2009; De Belder and Moens, 2010), for non-native speakers and language learners (Petersen and Ostendorf, 2007; Allen, 2009), for people who suffer from language impairments (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), and for readers lacking expert knowledge of the topic discussed (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Such readers would also benefit from MT output that is better targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification has been used to improve MT by restructuring complex sentences into shorter and simpler segments that are easier to translate (Gerber and Hovy, 1998; Štajner and Popovic, 2016; Hasler et al., 2017). Contemporaneously to our work, Marchisio et al. (2019) show that tagging the English side of parallel corpora with automatic readability scores can help translate the same input in a simpler or more complex form. Our work shares"
D19-1166,P11-2117,0,0.0284812,"oens, 2010), for non-native speakers and language learners (Petersen and Ostendorf, 2007; Allen, 2009), for people who suffer from language impairments (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), and for readers lacking expert knowledge of the topic discussed (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Such readers would also benefit from MT output that is better targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification has been used to improve MT by restructuring complex sentences into shorter and simpler segments that are easier to translate (Gerber and Hovy, 1998; Štajner and Popovic, 2016; Hasler et al., 2017). Contemporaneously to our work, Marchisio et al. (2019) show that tagging the English side of parallel corpora with automatic readability scores can help translate the same input in a simpler or more complex form. Our work shares the goal of controlling t"
D19-1166,W07-1007,0,0.0379688,"implify text independently. 1 Introduction Generating text at the right level of complexity can make machine translation (MT) more useful for a wide range of users. As Xu et al. (2015) note, simplifying text makes it possible to develop reading aids for people with low-literacy (Watanabe et al., 2009; De Belder and Moens, 2010), for non-native speakers and language learners (Petersen and Ostendorf, 2007; Allen, 2009), for people who suffer from language impairments (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), and for readers lacking expert knowledge of the topic discussed (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Such readers would also benefit from MT output that is better targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification has been used to improve MT by restructuring complex sentences into shorter and simpler segments that are easier to translate (Gerber an"
D19-1166,J93-1004,0,0.419938,"is challenging because text is neither simplified nor translated sentence by sentence, and as a result, equivalent content might move from one sentence to the next. Past work has introduced techniques to align segments of different complexity within documents of the same language (Xu et al., 2015; Paetzold et al., 2017; Štajner et al., 2018). Complexity controlled MT requires aligning segments of different complexity in English and Spanish. Existing methods for aligning sentences in English and Spanish parallel corpora are not well suited to this task. For instance, the GaleChurch algorithm (Gale and Church, 1993) assumes that aligned sentences should have similar length. This assumption does not hold if the English article is a simplification of the Spanish article. Consider the following Spanish text and its English translation in Newsela: Spanish: LA HAYA, Holanda - Te has tomado alguna vez una selfie?, Hoy en día es muy fácil. Solo necesitas un teléfono inteligente. Google Translated English: THE HAGUE, Netherlands - Have you ever taken a selfie? Today is very easy. You only need a smart phone. Original English Version: THE HAGUE, Netherlands - All you need is a smartphone to take a selfie. It is t"
D19-1166,W13-2902,0,0.0234868,"from complex to simple English using reinforcement learning to directly optimize the metrics that evaluate complexity (SARI) and fluency and adequacy (BLEU). Scarton and Specia (2018) address the task of producing text of varying levels of complexity for different target audiences. They show that neural sequence-tosequence models informed by target-complexity tokens inserted in the input sequence perform well on this task. While the vast majority of text simplification work has focused on English, Spanish (Štajner et al., 2015), Italian (Brunato et al., 2016; Aprosio et al., 2019) and German (Klaper et al., 2013) have also received some attention. While most MT approaches only indirectly capture style properties (e.g., via domain adaptation), a growing number of studies share the goal of considering source texts and their translation in their pragmatic context. Mirkin and Meunier (2015) introduce personalized MT. Rabinovich et al. (2016) and Vanmassenhove et al. (2018) suggest that the gender of the author is implicitly marked in the source text and that dedicated statistical and neural systems better preserve gender traits in MT output. Neural MT has enabled more flexible ways to control stylistic pr"
D19-1166,P07-2045,0,0.00638426,"roughly 20K instances, drawn from the same articles as the MT+preserve and MT+simplify test set. For Spanish, we have 110k segment pairs, which will be used to train the Spanish simplification baseline. Bilingual Parallel Data (Newsela) We also extract parallel Spanish-English segments from Newsela based on aligned segments between Spanish and English articles that have the same reading grade level. We use this dataset to provide in-domain MT training examples which includes roughly 70k instances. All datasets are pre-processed using Moses tools for normalization, tokenization and truecasing (Koehn et al., 2007). We further segment tokens into subwords using a joint source-target byte pair encoding model with 32,000 operations (Sennrich et al., 2015). 5.3 Sequence-to-Sequence Model Configuration We use the standard encoder-decoder architecture implemented in the Sockeye toolkit (Hieber et al., 2017). Both encoder and decoder have two Long Short Term Memory (LSTM) layers (Bahdanau et al., 2015), hidden states of size 500 and dropout of 0.3 applied to the RNNs of the encoder and decoder which is same as what was used by Scarton and Specia (2018). We observe that dot product based attention works best i"
D19-1166,W19-6619,0,0.264522,"targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification has been used to improve MT by restructuring complex sentences into shorter and simpler segments that are easier to translate (Gerber and Hovy, 1998; Štajner and Popovic, 2016; Hasler et al., 2017). Contemporaneously to our work, Marchisio et al. (2019) show that tagging the English side of parallel corpora with automatic readability scores can help translate the same input in a simpler or more complex form. Our work shares the goal of controlling translation complexity, but considers a broader range of reading grade levels and simplification operations grounded in professionally edited text simplification corpora. Building a model for this task ideally requires rich annotation for evaluation and supervised training that is not available in bilingual parallel corpora typically used in MT. Controlling the complexity of Spanish-English transla"
D19-1166,N16-1005,0,0.2067,"e attention. While most MT approaches only indirectly capture style properties (e.g., via domain adaptation), a growing number of studies share the goal of considering source texts and their translation in their pragmatic context. Mirkin and Meunier (2015) introduce personalized MT. Rabinovich et al. (2016) and Vanmassenhove et al. (2018) suggest that the gender of the author is implicitly marked in the source text and that dedicated statistical and neural systems better preserve gender traits in MT output. Neural MT has enabled more flexible ways to control stylistic properties of MT output. Sennrich et al. (2016) first propose to append a special token to the source that neural MT models can attend to and to select the formal (Sie) or informal (du) version of second person pronouns when translating into German. Niu et al. (2018) show that multi-task models can jointly translate between languages and styles, producing formal and informal translations with broader lexical and phrasal at https://Newsela.com/data/. Scripts to replicate our model configurations and our cross-lingual segment aligner are available at https://github.com/ sweta20/ComplexityControlledMT. changes than the local pronoun changes i"
D19-1166,D15-1238,0,0.0857033,"diences. They show that neural sequence-tosequence models informed by target-complexity tokens inserted in the input sequence perform well on this task. While the vast majority of text simplification work has focused on English, Spanish (Štajner et al., 2015), Italian (Brunato et al., 2016; Aprosio et al., 2019) and German (Klaper et al., 2013) have also received some attention. While most MT approaches only indirectly capture style properties (e.g., via domain adaptation), a growing number of studies share the goal of considering source texts and their translation in their pragmatic context. Mirkin and Meunier (2015) introduce personalized MT. Rabinovich et al. (2016) and Vanmassenhove et al. (2018) suggest that the gender of the author is implicitly marked in the source text and that dedicated statistical and neural systems better preserve gender traits in MT output. Neural MT has enabled more flexible ways to control stylistic properties of MT output. Sennrich et al. (2016) first propose to append a special token to the source that neural MT models can attend to and to select the formal (Sie) or informal (du) version of second person pronouns when translating into German. Niu et al. (2018) show that mul"
D19-1166,P19-2036,0,0.0697526,"language impairments (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), and for readers lacking expert knowledge of the topic discussed (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Such readers would also benefit from MT output that is better targeted to their needs by being easier to read than the original. Prior work on text complexity has focused on simplifying input text in one language, primarily English (Chandrasekar et al., 1996; Coster and Kauchak, 2011; Siddharthan, 2014; Xu et al., 2015; Zhang and Lapata, 2017; Scarton and Specia, 2018; Kriz et al., 2019; Nishihara et al., 2019). Simplification has been used to improve MT by restructuring complex sentences into shorter and simpler segments that are easier to translate (Gerber and Hovy, 1998; Štajner and Popovic, 2016; Hasler et al., 2017). Contemporaneously to our work, Marchisio et al. (2019) show that tagging the English side of parallel corpora with automatic readability scores can help translate the same input in a simpler or more complex form. Our work shares the goal of controlling translation complexity, but considers a broader range of reading grade levels and simplification operations grounded in professiona"
D19-1166,P17-2014,0,0.0169791,": an input language segment si and a target complexity c. The goal is to produce a translation so in the output language that has complexity c. For instance, given input Spanish sentences in Table 1, complexity controlled MT aims to produce English translations at a specific level of complexity, which might differ from the complexity of the original Spanish. Model We model P (so |si , c; θ) as a neural encoder-decoder with attention (Bahdanau et al., 2015). This architecture has been used successfully for the two related tasks of text simplification (Wang et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017; Scarton and Specia, 2018) and machine translation (Bahdanau et al., 2015). The encoder constructs hidden representation for each word in the input sequence, while the decoder generates the target sequence, conditioned on hidden source representations. We hypothesize that training a single encoder-decoder model to perform the two distinct tasks of machine translation and text simplification will yield a model that can perform complexity controlled MT. We adopt the multitask framework proposed by Johnson et al. (2016) to train multilingual neural MT systems. Representing target complexity Targ"
D19-1532,P18-1073,0,0.15709,"gual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also been used to evaluate multilingual word embeddings (Lample et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018), leading to significant advances on the state-of-the-art. However, Peirsman and Pad´o (2011) show that automatically induced bilingual lexicons exhibit multiple semantic relations including not only synonymy, but also hypernymy and co-hyponymy. This has prompted work on identifying hypernymy in cross-lingual settings (Vyas and Carpuat, 2016; Upadhyay et al., 2018). This paper broadens the scope of relations studied in cross-lingual settings, and addresses for the first time the task of distinguishing between multiple semantic relations for words in different languages."
D19-1532,Q17-1010,0,0.0472108,"ingual examples, we need to define a validation set using the resources available to us. We construct a validation set by randomly removing 1000 pairs from the training data, and automatically translating the right side of each example with the bilingual dictionary used for training. This process yields a noisy validation set, which is solely used for tuning hyper-parameters. Unlabeled Resources The bilingual dictionary for knowledge distillation is obtained from the MUSE project (Lample et al., 2018) for EnHi, while the MDBG dictionary is used for EnZh.3 We use FastText bilingual embeddings (Bojanowski et al., 2017).4 We extract English paths for the monolingual model from a dump of the English Wikipedia.5 Cross-lingual paths are extracted from a random sample of the WMT18 parallel corpora6 for En-Zh (∼5M sentences) and the IIT Bombay English-Hindi corpus (Kunchukuttan et al., 2018) for En-Hi (∼1M sentences). All corpora are parsed using YaraParser (Rasooli and 3 https://www.mdbg.net/chinese/ dictionary?page=cc-cedict 4 https://fasttext.cc/docs/en/aligned-vectors.html 5 https://dumps.wikimedia.org/enwiki/ 6 http://statmt.org/wmt18/ translation-task.html Tetreault, 2015) trained on the treebank of the cor"
D19-1532,P13-1133,0,0.18224,"ths. All of the 2m paths paths are encoded using a single LSTM, and averaged to form vpaths(xe ,yf ) . Two special cases arise from this definition. First, a path can be a single alignment link if xe and yf are aligned to each other i.e. xf = yf and ye = xe . Second if no path is found in the corpus, vpaths(xe ,yf ) is set to the zero vector. 4 Weakly Supervised Training via Knowledge Distillation Cross-lingual examples that would enable fully supervised training of B I L EX N ET are hard to obtain: examples of relations such as synonymy or hypernymy can be derived from multilingual WordNets (Bond and Foster, 2013), but such resources are not available for many languages, and only cover a subset of semantic relations. Instead, we introduce a dictionary-guided variant of knowledge distillation to train B I L EX N ET. This procedure only relies on a set of monolingual labeled examples that are readily available for various lexical relations in English, and a translation dictionary that maps words in the source language to the target language. Our approach transfers knowledge from a monolingual teacher model to a cross-lingual student model. The teacher model is a monolingual L EX NET model (say Me ) train"
D19-1532,D18-1038,0,0.0227277,"Missing"
D19-1532,N18-2029,0,0.0251118,"Missing"
D19-1532,P15-1119,0,0.0207252,"290 translations, we pair xe with each of these translations, and use E N L EX N ET to predict the relation for each of these pairs. The relation for (xe , yf ) is then chosen as the most general relation among those predicted for the translated pairs according to the order in which they appear in Table 2.10 B I L EX N ET (N O D ISTILLATION ) A simple strategy for cross-lingual transfer consists of seeding a vanilla L EX NET model with bilingual embeddings in the source and target languages before training. This strategy has been successfully used for other NLP tasks (Klementiev et al., 2012; Guo et al., 2015, inter alia). By keeping the embeddings fixed, we can use source language data to train the monolingual L EX NET model using features based on source embeddings and source language paths as usual. At inference time, the model uses both the source and target embeddings as input, and the cross-lingual paths defined above. S PECIALIZED T ENSOR M ODEL (STM) How does a model that has primarily been used for comparing words in the same language perform on cross-lingual comparisons? Our final baseline aims to answer this question. Proposed by Glavaˇs and Vuli´c (2018), STM is a neural architecture f"
D19-1532,C92-2082,0,0.31887,"he classification labels are translation invariant. This work adapts distillation to a setting where labels might change when samples are translated. 3 B I L EX N ET: a Classifier for Cross-Lingual Semantic Relations nmod root nmod .. सुअर सिहत िविभनन परजाित के जानवरों .. .. different species of animals including pigs .. root nmod nmod Figure 2: The English path between animals and pigs has three edges: [X/NOUN/nmod/>, species/NOUN/root/∧, and Y/NOUN/nmod/>]. The path between animals and sar is defined as a combination of the English path and the Hindi path between яAnvro and sar. context (Hearst, 1992; Snow et al., 2004; Shwartz et al., 2016). For classification, vxy is input to a multi-class classifier, parameterized as a feedforward neural network with a single hidden layer. lout =W2 ∗ ReLU(W1 ∗ vxy ) ˆli = P exp(lout,i ) k j=1 exp(lout,j ) lpred = arg max ˆli (1) i The task of classifying semantic relations is a multi-class classification problem, where the classes are the set of five semantic relations from Pavlick et al. (2015): Equivalence (X is the same as Y), Forward Entail (X is more specific than/ is a type of Y), Backward Entail (X is more general than/encompasses Y), Exclusion"
D19-1532,N13-1132,0,0.020663,"Missing"
D19-1532,J94-4004,0,0.464509,"s(xe ,yf ) from a word-aligned parallel corpus (Figure 2). We first extract all parallel sentences which contain xe on the source side and yf on the target side. For each sentence, using word alignments, we can extract xf , the target word aligned to xe , and ye the source 5287 word aligned to yf . We then extract a path connecting the two word in the source sentence i.e. xe and ye . Similarly, we also extract a corresponding path connecting the two word in the target sentence i.e. xf and yf , since different languages can encode the same information differently due to structural divergences (Dorr, 1994). Thus, if the parallel corpus contains m sentence pairs where xe occurs on the source side and yf on the target side, we extract a total of 2m paths. All of the 2m paths paths are encoded using a single LSTM, and averaged to form vpaths(xe ,yf ) . Two special cases arise from this definition. First, a path can be a single alignment link if xe and yf are aligned to each other i.e. xf = yf and ye = xe . Second if no path is found in the corpus, vpaths(xe ,yf ) is set to the zero vector. 4 Weakly Supervised Training via Knowledge Distillation Cross-lingual examples that would enable fully superv"
D19-1532,J17-2001,0,0.0151096,"e (mostly English) (Nastase et al., 2013; Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Ponzetto, 2017; Ono et al., 2015). Methods that deal with multiple semantic relations are fewer (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2006; Turney, 2008), and recent shared tasks have shown that this In cross-lingual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also been used to evaluate multilingual word embeddings (Lample et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018), leading to significant advances on the state-of-the-art. However, Peirsman and Pad´o (2011) show that automatically induced bilingual lexicons exhibit multiple semantic relations including not only synonymy, but also hypernymy and co-hyponymy. This has prompted work on ide"
D19-1532,P98-1069,0,0.280635,"in the same language (mostly English) (Nastase et al., 2013; Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Ponzetto, 2017; Ono et al., 2015). Methods that deal with multiple semantic relations are fewer (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2006; Turney, 2008), and recent shared tasks have shown that this In cross-lingual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also been used to evaluate multilingual word embeddings (Lample et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018), leading to significant advances on the state-of-the-art. However, Peirsman and Pad´o (2011) show that automatically induced bilingual lexicons exhibit multiple semantic relations including not only synonymy, but also hypernymy and co-hypon"
D19-1532,N13-1092,0,0.0721323,"Missing"
D19-1532,D17-1185,0,0.0307659,"Missing"
D19-1532,C12-1089,0,0.0327842,"on the CogALex dataset. 5290 translations, we pair xe with each of these translations, and use E N L EX N ET to predict the relation for each of these pairs. The relation for (xe , yf ) is then chosen as the most general relation among those predicted for the translated pairs according to the order in which they appear in Table 2.10 B I L EX N ET (N O D ISTILLATION ) A simple strategy for cross-lingual transfer consists of seeding a vanilla L EX NET model with bilingual embeddings in the source and target languages before training. This strategy has been successfully used for other NLP tasks (Klementiev et al., 2012; Guo et al., 2015, inter alia). By keeping the embeddings fixed, we can use source language data to train the monolingual L EX NET model using features based on source embeddings and source language paths as usual. At inference time, the model uses both the source and target embeddings as input, and the cross-lingual paths defined above. S PECIALIZED T ENSOR M ODEL (STM) How does a model that has primarily been used for comparing words in the same language perform on cross-lingual comparisons? Our final baseline aims to answer this question. Proposed by Glavaˇs and Vuli´c (2018), STM is a neu"
D19-1532,P07-2045,0,0.00922945,"Missing"
D19-1532,L18-1548,0,0.0609027,"Missing"
D19-1532,N15-1098,0,0.0308902,"s M ULTI L EX R EL is used as a test set to evaluate our models. Training only requires English labeled examples, and other resources derived from raw monolingual and parallel corpora. 6.1 Data English Supervision The English training samples are derived from the English Lexical-XXXL PPDB. After filtering away pairs containing nonalphabetic characters, we choose a random sample as training pairs. The number of samples for all classes is balanced, except Exclusion (since there are fewer examples of this class in PPDB). All in all, the size of the training set is ∼20K pairs. Like previous work (Levy et al., 2015), we ensure a lexical split where the English words in the test data are not seen in the training data. This makes the task challenging as it prevents the model from memorizing patterns of words such as their “prototypicality” for certain relations i.e. whether certain words are likely to appear in specific relations. Validation data Since we assume no access to labeled cross-lingual examples, we need to define a validation set using the resources available to us. We construct a validation set by randomly removing 1000 pairs from the training data, and automatically translating the right side"
D19-1532,W07-1431,0,0.139828,"emantic relations is a multi-class classification problem, where the classes are the set of five semantic relations from Pavlick et al. (2015): Equivalence (X is the same as Y), Forward Entail (X is more specific than/ is a type of Y), Backward Entail (X is more general than/encompasses Y), Exclusion (X is mutually exclusive with/is opposite to Y), and Other (X is not related or related in other ways to Y). We choose these relations as they have been useful in describing lexical relations between English paraphrases (Pavlick et al., 2015), and in downstream natural language inference systems (MacCartney and Manning, 2007, 2009). Our classifier, B I L EX N ET, adapts the L EX NET English classifier (Shwartz and Dagan, 2016a,b) to cross-lingual settings. B I L EX N ET represents the input word pair (x, y) by a feature vector vxy , consisting of complementary distributional and pathbased features i.e. vxy = [vx ;vy ;vpaths(x,y) ]. The distributional semantic properties of x and y are captured by bilingual word embeddings vx and vy . vpaths(x,y) encodes lexico-syntactic paths that represent the relation between words x and y in W1 and W2 are the weights of the network, and the biases have been omitted for simplic"
D19-1532,W09-3714,0,0.108985,"emantic relations other than synonymy in practice, as can be seen (Table 1) in examples drawn from the MUSE dictionary (Lample et al., 2018). Peirsman and Pad´o (2011) show that distributional translation lexicons contain hyponyms and co-hyponyms, and that treating all translations as synonyms hurts cross-lingual projection performance. In the Paraphrase Database (Ganitkevitch et al., 2013), Pavlick et al. (2015) find that the diversity of semantic relations discovered in word-aligned parallel corpora yields paraphrases that span the lexical relations defined in the natural logic framework of MacCartney and Manning (2009). These non-synonymous translations are not just noise. They can be found even in high-quality parallel corpora, since the strategies used by professional translators to deal with words that do not have a direct equivalent in the target language include replacement by near-synonyms, hypernyms or negated antonyms (Baker, 2011; Venuti, 2012; Chesterman, 2016). In this work, we classify semantic relations between words in different languages. Given a word pair (water, py), the classification task is to select one of the five entailment classes (Figure 1) defined under the natural logic framework"
D19-1532,D17-1269,0,0.027061,"Missing"
D19-1532,P13-2017,0,0.0208178,"Missing"
D19-1532,Q17-1022,0,0.0523773,"Missing"
D19-1532,D17-1264,0,0.0404453,"Missing"
D19-1532,N15-1100,0,0.0279827,"ility: dog-pet), disjunction (incompatibility: cat-dog), and antonyms (open-shut), and how these relations can be used to characterize differences in meaning. Variations on these fundamental relations have been used within semantic networks such as WordNet (Fellbaum, 1998), or as the basis of a framework for inference without formal logic representations (MacCartney and Manning, 2009). Recent work on semantic relation prediction largely focuses on a single relation between words in the same language (mostly English) (Nastase et al., 2013; Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Ponzetto, 2017; Ono et al., 2015). Methods that deal with multiple semantic relations are fewer (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2006; Turney, 2008), and recent shared tasks have shown that this In cross-lingual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distri"
D19-1532,P06-1015,0,0.0410252,", and how these relations can be used to characterize differences in meaning. Variations on these fundamental relations have been used within semantic networks such as WordNet (Fellbaum, 1998), or as the basis of a framework for inference without formal logic representations (MacCartney and Manning, 2009). Recent work on semantic relation prediction largely focuses on a single relation between words in the same language (mostly English) (Nastase et al., 2013; Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Ponzetto, 2017; Ono et al., 2015). Methods that deal with multiple semantic relations are fewer (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2006; Turney, 2008), and recent shared tasks have shown that this In cross-lingual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recentl"
D19-1532,P15-1146,0,0.0607411,"Missing"
D19-1532,W06-3909,0,0.121425,"used to characterize differences in meaning. Variations on these fundamental relations have been used within semantic networks such as WordNet (Fellbaum, 1998), or as the basis of a framework for inference without formal logic representations (MacCartney and Manning, 2009). Recent work on semantic relation prediction largely focuses on a single relation between words in the same language (mostly English) (Nastase et al., 2013; Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Ponzetto, 2017; Ono et al., 2015). Methods that deal with multiple semantic relations are fewer (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2006; Turney, 2008), and recent shared tasks have shown that this In cross-lingual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also"
D19-1532,P95-1050,0,0.269911,"etween words in the same language (mostly English) (Nastase et al., 2013; Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Ponzetto, 2017; Ono et al., 2015). Methods that deal with multiple semantic relations are fewer (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2006; Turney, 2008), and recent shared tasks have shown that this In cross-lingual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also been used to evaluate multilingual word embeddings (Lample et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018), leading to significant advances on the state-of-the-art. However, Peirsman and Pad´o (2011) show that automatically induced bilingual lexicons exhibit multiple semantic relations including not only synonymy, but also hy"
D19-1532,N19-1390,0,0.0178805,"rds in different languages. Previous work studying semantic relations in multiple languages has focused on the different task of cross-lingual transfer. In such settings, the focus is on identifying semantic relations between two words in the same language, without training data in that language. There are broadly two strategies to solve this problem. One line of work (Glavaˇs and Vuli´c, 2018; Mrkˇsi´c et al., 2017) uses model transfer, where a single model is trained on data from a high-resource language, and is then ported to the target language using cross-lingual embeddings. In contrast, Roth and Upadhyay (2019) translate training data from English into a target language using a combination of unsupervised cross-lingual embeddings (Artetxe et al., 2018) and monolingual information from the target language. Our approach for cross-lingual semantic relation classification builds on the monolingual classifier L EX NET (Shwartz and Dagan, 2016a,b), which achieved the highest performance (45 F1) 5286 among participating teams on the CogALex-V shared task on identification of semantic relations (Santus et al., 2016) without ontologies or structured information. We adapt L EX NET to make cross-lingual predic"
D19-1532,W16-5309,0,0.138726,"ocedure for B I L EX N ET that leverages weak supervision in the form of examples translated from English via a knowledge distillation technique guided by translation dic5285 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5285–5296, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Translation is a challenging problem, especially when ontologies and other structured resources are not available, and models are trained only on raw corpora (Santus et al., 2016). Equivalence Reverse Entailment water water Forward Entailment Exclusion Equivalence water पय Other Related Forward Entailment Reverse Entailment Exclusion Equivalence Other Reverse Entailment water liquid Forward Entailment Exclusion Translation Other Related Figure 1: On the left, we illustrate cross-lingual semantic relation classification: given the pair (water, py) as input, the task is to select the Equivalence class (in bold/green) from the five possible relations. On the right, we show that semantic relations change by translation. py translates to liquid and water, and their respecti"
D19-1532,W16-5310,0,0.313015,"tion task is to select one of the five entailment classes (Figure 1) defined under the natural logic framework of MacCartney and Manning (2009). This cross-lingual task cannot be solved by translation, as translation does not preserve semantic relations. We also cannot assume that labeled examples exist for all language pairs and learning from English labeled examples is complicated by translation ambiguity (Figure 1). We introduce B I L EX N ET, a neural classifier for semantic relations based on cross-lingual distributional and path-based features inspired by the monolingual L EX NET model (Shwartz and Dagan, 2016a,b) (Section 3). We then design a novel training procedure for B I L EX N ET that leverages weak supervision in the form of examples translated from English via a knowledge distillation technique guided by translation dic5285 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5285–5296, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Translation is a challenging problem, especially when ontologies and other structured resources are not ava"
D19-1532,W16-5304,0,0.0298614,"Missing"
D19-1532,P16-1226,0,0.0286816,"Missing"
D19-1532,P18-1072,0,0.0549206,"Missing"
D19-1532,N18-1103,0,0.0398343,"Missing"
D19-1532,N16-1142,1,0.862869,"learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also been used to evaluate multilingual word embeddings (Lample et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018), leading to significant advances on the state-of-the-art. However, Peirsman and Pad´o (2011) show that automatically induced bilingual lexicons exhibit multiple semantic relations including not only synonymy, but also hypernymy and co-hyponymy. This has prompted work on identifying hypernymy in cross-lingual settings (Vyas and Carpuat, 2016; Upadhyay et al., 2018). This paper broadens the scope of relations studied in cross-lingual settings, and addresses for the first time the task of distinguishing between multiple semantic relations for words in different languages. Previous work studying semantic relations in multiple languages has focused on the different task of cross-lingual transfer. In such settings, the focus is on identifying semantic relations between two words in the same language, without training data in that language. There are broadly two strategies to solve this problem. One line of work (Glavaˇs and Vuli´c, 20"
D19-1532,P17-1130,0,0.0309876,"osing to model cross-lingual relations using lexicosyntactic paths from both languages. Finally, our training procedure uses knowledge distillation (Hinton et al., 2014) to alleviate the lack of annotated cross-lingual pairs. Knowledge distillation has been proposed to compress a model with many parameters (the teacher model) to a model with fewer parameters (the student model). It has also been used successfully to learn mappings between languages (Nakashole and Flauger, 2017) or to transfer knowledge from models trained on one language to a different target language for text classification (Xu and Yang, 2017) and belief tracking (Chen et al., 2018a), in settings where the classification labels are translation invariant. This work adapts distillation to a setting where labels might change when samples are translated. 3 B I L EX N ET: a Classifier for Cross-Lingual Semantic Relations nmod root nmod .. सुअर सिहत िविभनन परजाित के जानवरों .. .. different species of animals including pigs .. root nmod nmod Figure 2: The English path between animals and pigs has three edges: [X/NOUN/nmod/>, species/NOUN/root/∧, and Y/NOUN/nmod/>]. The path between animals and sar is defined as a combination of the Engl"
D19-1532,D12-1111,0,0.0741809,"Missing"
D19-1532,P14-1024,0,0.0331218,"ish tasks. 1 Lexicon Entry Semantic Relation writer, rcnAkAr writer is more specific than rcnAkAr (creator) council, mE/pErqX council is more general than mE/pErqX (council of ministers) father, ccA father is mutually exclusive to ccA (father’s brother) Table 1: Semantic relations between word pairs in an English-Hindi lexicon (Lample et al., 2018) Introduction Natural Language Processing (NLP) often uses translation lexicons for projecting models and data from one language to another under the assumption that words and their translations in these lexicons are synonyms (Mayhew et al., 2017; Tsvetkov et al., 2014). However, translation lexicons include semantic relations other than synonymy in practice, as can be seen (Table 1) in examples drawn from the MUSE dictionary (Lample et al., 2018). Peirsman and Pad´o (2011) show that distributional translation lexicons contain hyponyms and co-hyponyms, and that treating all translations as synonyms hurts cross-lingual projection performance. In the Paraphrase Database (Ganitkevitch et al., 2013), Pavlick et al. (2015) find that the diversity of semantic relations discovered in word-aligned parallel corpora yields paraphrases that span the lexical relations d"
D19-1532,C08-1114,0,0.0513105,"s in meaning. Variations on these fundamental relations have been used within semantic networks such as WordNet (Fellbaum, 1998), or as the basis of a framework for inference without formal logic representations (MacCartney and Manning, 2009). Recent work on semantic relation prediction largely focuses on a single relation between words in the same language (mostly English) (Nastase et al., 2013; Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Ponzetto, 2017; Ono et al., 2015). Methods that deal with multiple semantic relations are fewer (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2006; Turney, 2008), and recent shared tasks have shown that this In cross-lingual settings, studies of semantic relations between words are mostly limited to the translation equivalence relation. Dictionary induction aims to automatically discover words that are translations of each other using monolingual or comparable corpora (Rapp, 1995; Fung and Yee, 1998; Irvine and Callison-Burch, 2017). The task is typically framed as unsupervised learning, and models rely on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also been used to e"
D19-1532,N18-1056,1,0.930771,"ly on distributional properties to discover words that have the same meaning in the two languages. Recently, dictionary induction has also been used to evaluate multilingual word embeddings (Lample et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018), leading to significant advances on the state-of-the-art. However, Peirsman and Pad´o (2011) show that automatically induced bilingual lexicons exhibit multiple semantic relations including not only synonymy, but also hypernymy and co-hyponymy. This has prompted work on identifying hypernymy in cross-lingual settings (Vyas and Carpuat, 2016; Upadhyay et al., 2018). This paper broadens the scope of relations studied in cross-lingual settings, and addresses for the first time the task of distinguishing between multiple semantic relations for words in different languages. Previous work studying semantic relations in multiple languages has focused on the different task of cross-lingual transfer. In such settings, the focus is on identifying semantic relations between two words in the same language, without training data in that language. There are broadly two strategies to solve this problem. One line of work (Glavaˇs and Vuli´c, 2018; Mrkˇsi´c et al., 201"
I05-2021,W04-0821,0,0.0123974,"MT models. WER is defined as the percentage of words to be inserted, deleted or replaced in the translation in order to obtain the sentence of reference. However, WER does not isolate WSD performance since it also encompasses many other types of errors. Also, since the choice of a translation for a particular word affects the translation of other words in the sentence, the effect of WSD performance on WER is unclear. In contrast, the Senseval accuracy metric counts each incorrect translation choice only once. Apart from the voted WSD system described in section 4, and the unsupervised system (Cabezas et al., 2004) mentioned in section 6.4, systems built and optimized for the Senseval-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or co"
I05-2021,P05-1048,1,0.736175,"t explicitly addressed. However, recent progress in machine translation and the continuous improvement on evaluation metrics such as BLEU (Papineni et al., 2002) suggest that SMT systems are already very good at choosing correct word translations. BLEU score with low order n-grams can be seen as an evaluation of the translation adequacy, which suggests that as SMT systems achieve higher BLEU score, their ability to disambiguate word translations improves. In other work, we have been conducting comparative studies testing whether state-of-the-art WSD models can improve SMT translation quality (Carpuat and Wu, 2005). Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system, we found that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone. The surprising difficulty of this challenge might suggest that SMT models are sufficiently strong at word level disambiguation on their own, and has recently encouraged speculation that SMT performs WSD as well as the dedicated WSD models. The studies described in this paper are aimed at directly testing th"
I05-2021,W04-0822,1,0.61499,"which implements an efficient greedy decoding algorithm, is used to translate the Chinese sentences, using the alignment model and language model previously described. Notice that very little contextual information is available to the IBM SMT models. Lexical choice during decoding essentially depends on the translation probabilities learned for the target word, and on the English language model scores. 4 The WSD system The WSD system used here is based on the model that achieved the best performance on the Senseval-3 Chinese lexical sample task, outperforming other systems by a large margin (Carpuat et al., 2004). The model consists of an ensemble of four highly accurate classifiers combined by majority vote: a naive Bayes classifier, a maximum entropy model (Jaynes, 1978), a boosting model (Freund and Schapire, 1997), and a Kernel PCA-based model (Wu et al., 2004), which has the advantage of having a signficantly different bias. All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. The feature set used consists"
I05-2021,W04-0802,0,0.200134,"et senses for a set of new occurrences of the target word in context. We use the SMT sytem described in Section 3 to translate the Chinese sentences of the Senseval evaluation test set, and extract the translation chosen for each 122 of the target word occurrences. In order to evaluate the predictions of the SMT model just like any WSD model, we need to map the English translations to HowNet senses. This mapping is done using HowNet, which provides English glosses for each of the senses of every Chinese word. Note that Senseval-3 also defined a translation or multilingual lexical sample task (Chklovski et al., 2004), which is just like the English lexical sample task, except that the WSD systems are expected to predict Hindi translations instead of WordNet senses. This translation task might seem to be a more natural evaluation framework for SMT than the monolingual Chinese lexical sample task. However, in practice, there is very little data available to train an Englishto-Hindi SMT model, which would significantly hinder its performance and bias the study in favor of the dedicated WSD models. 5.2 Allowing the SMT model to exploit the Senseval data Comparing the Senseval WSD models with a regular SMT mod"
I05-2021,P04-1039,0,0.0448296,"ovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. In all this work, the goal is to achieve accurate WSD with minimum amounts of annotated data. Again, this differs from our objective which is to directly evaluate an SMT model as a WSD model. 8 Conclusion We presented empirical results casting doubt on the increasingly common assumption that SMT models are very good at WSD, even though they do not explicitly address WSD as an independent task. Using the Senseval-3 Chinese lexical sample task as a testbed, we"
I05-2021,N03-1010,0,0.0172677,"nsists of about 1 million sentences from the United Nations Chinese-English parallel corpus from LDC. This corpus was automatically sentence-aligned, so the training data does not require as much manual annotation as for the WSD model. 3.2 Language model The English language model is a trigram model trained on the Gigaword newswire data and on the English side of the UN and Xinhua parallel corpora. The language model is also trained using a publicly available software, the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). 3.3 Decoding The ISI ReWrite decoder (Germann, 2003), which implements an efficient greedy decoding algorithm, is used to translate the Chinese sentences, using the alignment model and language model previously described. Notice that very little contextual information is available to the IBM SMT models. Lexical choice during decoding essentially depends on the translation probabilities learned for the target word, and on the English language model scores. 4 The WSD system The WSD system used here is based on the model that achieved the best performance on the Senseval-3 Chinese lexical sample task, outperforming other systems by a large margin"
I05-2021,P02-1044,0,0.046374,"n of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. In all this work, the goal is to achieve accurate WSD with minimum amounts of annotated data. Again, this differs from our objective which is to directly evaluate an SMT model as a WSD model. 8 Conclusion We presented empirical results casting doubt on the increasingly common assumption that SMT models are very good at WSD, even though they do not exp"
I05-2021,W04-0807,0,0.0478446,"slation for a particular word affects the translation of other words in the sentence, the effect of WSD performance on WER is unclear. In contrast, the Senseval accuracy metric counts each incorrect translation choice only once. Apart from the voted WSD system described in section 4, and the unsupervised system (Cabezas et al., 2004) mentioned in section 6.4, systems built and optimized for the Senseval-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a tra"
I05-2021,P03-1058,0,0.0555526,"l-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. In all this work, the goal is to achieve accurate WSD with minimum amounts of annotated data. Again, this differs from our objective which is to directly evaluate an SMT model as a WSD model. 8 Conclusion We pre"
I05-2021,W04-0847,0,0.022527,"entence of reference. However, WER does not isolate WSD performance since it also encompasses many other types of errors. Also, since the choice of a translation for a particular word affects the translation of other words in the sentence, the effect of WSD performance on WER is unclear. In contrast, the Senseval accuracy metric counts each incorrect translation choice only once. Apart from the voted WSD system described in section 4, and the unsupervised system (Cabezas et al., 2004) mentioned in section 6.4, systems built and optimized for the Senseval-3 Chinese lexical sample task, include Niu et al. (2004). Many of the Senseval-type WSD system are not language specific and the presentation of the results in the English lexical sample task (Midhalcea et al., 2004), English-Hindi multilingual task (Chklovski et al., 2004), or any of the lexical sample tasks defined in other languages, give a good overview of the variety of approaches to WSD. Most previous work on multilingual WSD has focused on the different problem of exploiting bilingual resources (e.g., parallel or comparable corpora, or even full MT systems) to help WSD. For instance, Ng et al. (2003) showed that it is possible to use word al"
I05-2021,J03-1002,0,0.00440369,"y the language model. That is, the predictions benefit from the sentential context of the target language. This has the general effect of improving translation fluency. Another major difference with most lexical sample WSD models is that SMT models are always unsupervised. SMT models learn from large sets of bisentences but the correct word alignment between the two sentences is unknown. SMT models cannot therefore 121 To build a representative baseline SMT system, we restricted ourselves to making use of freely available tools. 3.1 Alignment model The alignment model was trained with GIZA++ (Och and Ney, 2003), which implements the most typical IBM and HMM alignment models. Translation quality could be improved using more advanced hybrid phrasal or tree models, but this would interfere with the questions being investigated here. The alignment model used is IBM-4, as required by our decoder. The training scheme is IBM-1, HMM, IBM-3 and IBM-4, as specified in (Och and Ney, 2003). The training corpus consists of about 1 million sentences from the United Nations Chinese-English parallel corpus from LDC. This corpus was automatically sentence-aligned, so the training data does not require as much manual"
I05-2021,P02-1040,0,0.0780761,"WSD either explicitly or implicitly. Since the Senseval models have been built and optimized specifically to address the WSD problems, they typically use richer disambiguating information than SMT systems. This, however, raises the question of whether the sophisticated WSD models are in fact needed in practice. In many machine translation architectures, in particular most current statistical machine translation (SMT) models, the WSD problem is typically not explicitly addressed. However, recent progress in machine translation and the continuous improvement on evaluation metrics such as BLEU (Papineni et al., 2002) suggest that SMT systems are already very good at choosing correct word translations. BLEU score with low order n-grams can be seen as an evaluation of the translation adequacy, which suggests that as SMT systems achieve higher BLEU score, their ability to disambiguate word translations improves. In other work, we have been conducting comparative studies testing whether state-of-the-art WSD models can improve SMT translation quality (Carpuat and Wu, 2005). Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system,"
I05-2021,P98-2230,1,0.914616,"Missing"
I05-2021,P04-1081,1,0.746599,"oice during decoding essentially depends on the translation probabilities learned for the target word, and on the English language model scores. 4 The WSD system The WSD system used here is based on the model that achieved the best performance on the Senseval-3 Chinese lexical sample task, outperforming other systems by a large margin (Carpuat et al., 2004). The model consists of an ensemble of four highly accurate classifiers combined by majority vote: a naive Bayes classifier, a maximum entropy model (Jaynes, 1978), a boosting model (Freund and Schapire, 1997), and a Kernel PCA-based model (Wu et al., 2004), which has the advantage of having a signficantly different bias. All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. The feature set used consists of position-sensitive, syntactic, and local collocational features, as described by Yarowsky and Florian (2002). 5 Experimental method 5.1 Senseval-3 Chinese lexical sample task The Senseval-3 Chinese lexical sample task includes 20 target word types. For"
I05-2021,C98-2225,1,\N,Missing
N04-4010,W99-0612,0,0.0357195,"Missing"
N04-4010,W03-1026,0,0.0267296,"Missing"
N04-4010,O03-5001,0,0.0577236,"Missing"
N04-4010,W02-2024,0,0.0232548,"Missing"
N04-4010,W03-0419,0,0.0212412,"Missing"
N04-4010,W03-0433,1,0.873511,"Missing"
N04-4010,C02-1012,0,\N,Missing
N10-1029,D09-1050,0,0.0421377,"meaning and produce appropriate translations and avoid the generation of unnatural or nonsensical sentences in the target language.” However, statistical machine translation (SMT) typically does not model MWEs explicitly. SMT 1 The research was partially funded by IBM under the DARPA GALE project. As a result, the usefulness of explicitly modeling MWEs in the SMT framework has not yet been studied systematically. Previous work has focused on automatically learning and integrating translations of very specific MWE categories, such as, for instance, idiomatic Chinese four character expressions (Bai et al., 2009) or domain specific MWEs (Ren et al., 2009). MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. In this paper, we consider a more general problem: we view SMT as an extrinsic evaluation of the usefulness of monolingual MWEs as used pervasively in natural language"
N10-1029,2005.mtsummit-posters.11,0,0.483322,"e research was partially funded by IBM under the DARPA GALE project. As a result, the usefulness of explicitly modeling MWEs in the SMT framework has not yet been studied systematically. Previous work has focused on automatically learning and integrating translations of very specific MWE categories, such as, for instance, idiomatic Chinese four character expressions (Bai et al., 2009) or domain specific MWEs (Ren et al., 2009). MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. In this paper, we consider a more general problem: we view SMT as an extrinsic evaluation of the usefulness of monolingual MWEs as used pervasively in natural language regardless of domain, idiomaticity and compositionality. A MWE is compositional if its meaning as a unit can be predicted from the meaning of its component words such as in make a decision meaning to decide. Some MWEs are more predictable"
N10-1029,P02-1040,0,0.110059,"rge amounts of data available. However, we tackle the less common English to Arabic direction in order to take advantage of the rich lexical resources available for English on the input side. Our test set consists of the 813 newswire sentences of the 2008 NIST Open Machine Translation Evaluation, which is standard evaluation data for Arabic-English translation. The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference2 . Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (Papineni et al., 2002), which is based on n-gram precisions for n = 1..4, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 3 4.1 2 Static integration of MWE in SMT Dynamic integration of MWE in SMT The second strategy attempts to encourage cohesive translations of MWEs without ignoring their components. Word alignment and phrasal translation extraction are conducted without any MWE knowledge, so that the SMT system can learn word-forword translations from consistently translated compositional MWEs. MWE knowledge is integrated as a feature in the tr"
N10-1029,W09-2907,0,0.594536,"Missing"
N10-1029,J93-1007,0,0.481454,"Missing"
N10-1029,2006.amta-papers.25,0,0.0445852,"advantage of the rich lexical resources available for English on the input side. Our test set consists of the 813 newswire sentences of the 2008 NIST Open Machine Translation Evaluation, which is standard evaluation data for Arabic-English translation. The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference2 . Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (Papineni et al., 2002), which is based on n-gram precisions for n = 1..4, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 3 4.1 2 Static integration of MWE in SMT Dynamic integration of MWE in SMT The second strategy attempts to encourage cohesive translations of MWEs without ignoring their components. Word alignment and phrasal translation extraction are conducted without any MWE knowledge, so that the SMT system can learn word-forword translations from consistently translated compositional MWEs. MWE knowledge is integrated as a feature in the translation lexicon. For each entry, in addition to the standard phrasal translation probabilities, we define a"
N10-1029,P07-2045,0,\N,Missing
N10-1029,W09-2903,1,\N,Missing
N16-1142,W11-2501,0,0.0415599,"synonymy, hypernymy, some meronymy relations, but also cause-effect relations (murder entails death), and other associations (ocean entails water) (Kotlerman et al., 2010). We extend this definition to the cross-lingual case by modifying the second condition. Given a word 1188 English-French affection → sentiment aspirin 6→ drogue water → humide feeling 6→ nostalgie Table 2: Examples of monolingual and cross-lingual lexical entailment: → can be read as “entails”, 6→ as “does not entail”. When evaluating lexical entailment, we use the same approach as in monolingual tasks (Baroni et al., 2012; Baroni and Lenci, 2011; Kotlerman et al., 2010; Turney and Mohammad, 2015): given a bilingual word pair, systems are asked to make a binary true/false decision on whether the first word entails the second. We describe the collection of gold standard annotations in Section 5.2. 3 Unsupervised Detection of Lexical Entailment We choose to detect lexical entailment without supervision. As in the monolingual case, detection can be done using a scoring function which quantifies the directional semantic similarity of an input word pair. On monolingual tasks, despite reaching better performance, supervised systems do not r"
N16-1142,E12-1004,0,0.0521876,"c relations, such as synonymy, hypernymy, some meronymy relations, but also cause-effect relations (murder entails death), and other associations (ocean entails water) (Kotlerman et al., 2010). We extend this definition to the cross-lingual case by modifying the second condition. Given a word 1188 English-French affection → sentiment aspirin 6→ drogue water → humide feeling 6→ nostalgie Table 2: Examples of monolingual and cross-lingual lexical entailment: → can be read as “entails”, 6→ as “does not entail”. When evaluating lexical entailment, we use the same approach as in monolingual tasks (Baroni et al., 2012; Baroni and Lenci, 2011; Kotlerman et al., 2010; Turney and Mohammad, 2015): given a bilingual word pair, systems are asked to make a binary true/false decision on whether the first word entails the second. We describe the collection of gold standard annotations in Section 5.2. 3 Unsupervised Detection of Lexical Entailment We choose to detect lexical entailment without supervision. As in the monolingual case, detection can be done using a scoring function which quantifies the directional semantic similarity of an input word pair. On monolingual tasks, despite reaching better performance, sup"
N16-1142,D07-1007,1,0.802329,"Missing"
N16-1142,P02-1033,0,0.10451,"g word representations. 1 house house house home condo apartment Introduction foyer (foyer) maison (house) chambre (chamber) appartement appartement appartement Table 1: Examples of translations drawn from an EnglishMultilingual Natural Language Processing lacks techniques to automatically compare and contrast the meaning of words across languages. Machine translation (Koehn, 2010) lets us discover translation correspondences in bilingual texts, but a word and its translation often do not cover the exact same semantic space: distinct word senses might translate differently (Gale et al., 1992; Diab and Resnik, 2002, among others); semantic relations and associations do not always translate, an important issue when constructing multilingual ontologies (Fellbaum and Vossen, 2012); and words in parallel text might be translated non-literally due to lexical gaps French bilexicon automatically learned on parallel text. We aim to design models that capture these differences and similarities in word meaning across languages, beyond translation correspondences. As a first step, we introduce cross-lingual lexical entailment, the task of detecting whether the meaning of a word in one language can be inferred from"
N16-1142,N16-1163,1,0.882337,"Missing"
N16-1142,P15-1144,0,0.0291828,"sparse linear combinations of basis elements. In contrast with dimensionality reduction techniques such as PCA, the learned basis vectors need not be orthogonal, which gives more flexibility to represent the data (Mairal et al., 2009). These models have been introduced as word representations in monolingual settings (Murphy et al., 2012) with the goal of obtaining interpretable, cognitively-plausible representations. We review the monolingual models, before introducing our novel bilingual formulation. 4.1 Review: Learning Monolingual Sparse Representations Previous work (Murphy et al., 2012; Faruqui et al., 2015) on obtaining sparse monolingual representations is based on a variant of the Nonnegative Matrix Factorization problem. Given a matrix X containing v dense word representations arranged row-wise, sparse representations for the v words can be oba many-to-one mapping from e to f , which captures translation ambiguity by allowing multiple words in e to be aligned to the same word in f . tained by solving the following optimization problem argmin A,D v X ||Ai DT − Xi ||22 + λ||Ai ||1 i=1 subject to Aij &gt;= 0, ∀i, j DTi Di (3) &lt;= 1, ∀i ve X 1 ||Aei De T − Xei ||22 + λe ||Aei ||1 2 Ae ,De ,Af ,Df arg"
N16-1142,N15-1004,0,0.0503191,"Missing"
N16-1142,P04-1067,0,0.143733,"Missing"
N16-1142,P05-1014,0,0.735976,"unction which quantifies the directional semantic similarity of an input word pair. On monolingual tasks, despite reaching better performance, supervised systems do not really learn entailment relations for word pairs, but instead learn when a particular word in the pair is a “prototypical hypernym” (Levy et al., 2015). 1 Thus, we limit our investigation to unsupervised models. As a result, our approach only requires a small number of annotated examples to tune the scoring threshold. We use the balAPinc score (Kotlerman et al., 2009), which is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005b): given feature representations of the contexts of two words u and v, u is assumed to entail v if all features of u tend to appear within the features of v. Formally, balAPinc is the geometric mean of a symmetric similarity score, LIN (Lin, 1998), and an asymmetric score, APinc. Given a directional entailment pair (u → v), p balAPinc(u → v) = LIN(u, v) · APinc(u → v) Assume we are given ranked feature lists F Vu and F Vv for words u and v respectively. Let wu (f ) denote the weight of a particular feature f in F Vu . LIN is defined by P [wu (f ) + wv (f )] f ∈F Vu ∩F Vv P (1) LIN(u, v) = P w"
N16-1142,C14-1048,0,0.0450735,"Missing"
N16-1142,P14-1006,0,0.0489555,"Missing"
N16-1142,P12-1092,0,0.130172,"Missing"
N16-1142,D12-1002,0,0.0593201,"Missing"
N16-1142,N15-1070,0,0.0382976,"Missing"
N16-1142,D07-1103,0,0.0669007,"Missing"
N16-1142,C12-1089,0,0.116179,"Missing"
N16-1142,2005.mtsummit-papers.11,0,0.0968258,"Missing"
N16-1142,J10-4005,0,0.0604894,"Missing"
N16-1142,P09-2018,0,0.0784857,"out supervision. As in the monolingual case, detection can be done using a scoring function which quantifies the directional semantic similarity of an input word pair. On monolingual tasks, despite reaching better performance, supervised systems do not really learn entailment relations for word pairs, but instead learn when a particular word in the pair is a “prototypical hypernym” (Levy et al., 2015). 1 Thus, we limit our investigation to unsupervised models. As a result, our approach only requires a small number of annotated examples to tune the scoring threshold. We use the balAPinc score (Kotlerman et al., 2009), which is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005b): given feature representations of the contexts of two words u and v, u is assumed to entail v if all features of u tend to appear within the features of v. Formally, balAPinc is the geometric mean of a symmetric similarity score, LIN (Lin, 1998), and an asymmetric score, APinc. Given a directional entailment pair (u → v), p balAPinc(u → v) = LIN(u, v) · APinc(u → v) Assume we are given ranked feature lists F Vu and F Vv for words u and v respectively. Let wu (f ) denote the weight of a particular feature f in"
N16-1142,Q15-1027,0,0.0519953,"Missing"
N16-1142,S10-1003,0,0.119634,"Missing"
N16-1142,N15-1098,0,0.0337749,"ision on whether the first word entails the second. We describe the collection of gold standard annotations in Section 5.2. 3 Unsupervised Detection of Lexical Entailment We choose to detect lexical entailment without supervision. As in the monolingual case, detection can be done using a scoring function which quantifies the directional semantic similarity of an input word pair. On monolingual tasks, despite reaching better performance, supervised systems do not really learn entailment relations for word pairs, but instead learn when a particular word in the pair is a “prototypical hypernym” (Levy et al., 2015). 1 Thus, we limit our investigation to unsupervised models. As a result, our approach only requires a small number of annotated examples to tune the scoring threshold. We use the balAPinc score (Kotlerman et al., 2009), which is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005b): given feature representations of the contexts of two words u and v, u is assumed to entail v if all features of u tend to appear within the features of v. Formally, balAPinc is the geometric mean of a symmetric similarity score, LIN (Lin, 1998), and an asymmetric score, APinc. Given a directio"
N16-1142,P98-2127,0,0.110809,"in the pair is a “prototypical hypernym” (Levy et al., 2015). 1 Thus, we limit our investigation to unsupervised models. As a result, our approach only requires a small number of annotated examples to tune the scoring threshold. We use the balAPinc score (Kotlerman et al., 2009), which is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005b): given feature representations of the contexts of two words u and v, u is assumed to entail v if all features of u tend to appear within the features of v. Formally, balAPinc is the geometric mean of a symmetric similarity score, LIN (Lin, 1998), and an asymmetric score, APinc. Given a directional entailment pair (u → v), p balAPinc(u → v) = LIN(u, v) · APinc(u → v) Assume we are given ranked feature lists F Vu and F Vv for words u and v respectively. Let wu (f ) denote the weight of a particular feature f in F Vu . LIN is defined by P [wu (f ) + wv (f )] f ∈F Vu ∩F Vv P (1) LIN(u, v) = P wv (f ) wu (f ) + f ∈F Vu f ∈F Vv APinc is a modified asymmetric version of the Average Precision metric used in Information Retrieval: |FP Vu | APinc(u → v) = r=1 [P (r, F Vu , F Vv ) · rel0 (fr )] |F Vu | (2) where, P (r, F Vu , F Vv ) |# features"
N16-1142,N15-1028,0,0.0570692,"Missing"
N16-1142,N10-1045,0,0.0620743,"Missing"
N16-1142,D09-1092,0,0.0920876,"Missing"
N16-1142,C12-1118,0,0.095085,"Missing"
N16-1142,D14-1113,0,0.0668427,"Missing"
N16-1142,P15-1146,0,0.0674866,"Missing"
N16-1142,D14-1162,0,0.0839202,"Missing"
N16-1142,N10-1013,0,0.100786,"Missing"
N16-1142,C90-2057,0,0.288419,"Missing"
N16-1142,J09-3004,0,0.0172292,"y due to lexical gaps French bilexicon automatically learned on parallel text. We aim to design models that capture these differences and similarities in word meaning across languages, beyond translation correspondences. As a first step, we introduce cross-lingual lexical entailment, the task of detecting whether the meaning of a word in one language can be inferred from the meaning of a word in another language. In monolingual settings, lexical entailment has received significant attention as a representation-agnostic way of modeling lexical semantics, and as a step toward textual inference (Zhitomirsky-Geffet and Dagan, 2009; Turney and Mohammad, 2015; Levy et 1187 Proceedings of NAACL-HLT 2016, pages 1187–1197, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics al., 2015; Pavlick et al., 2015). We hypothesize that the cross-lingual task can help do the same with multilingual texts. Building on prior work on the monolingual task, we take an unsupervised approach, and use a directional semantic similarity metric motivated by the distributional inclusion hypothesis (Geffet and Dagan, 2005a; Kotlerman et al., 2010): we assume a word e entails a word f if the prominent context f"
N16-1142,W09-2413,0,\N,Missing
N16-1142,P07-2045,0,\N,Missing
N16-1142,C98-2122,0,\N,Missing
N16-1142,D15-1127,0,\N,Missing
N16-1142,J13-3001,0,\N,Missing
N16-1142,N13-1073,0,\N,Missing
N16-1163,P14-1023,0,0.0576199,"them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance. 1 Introduction Vector space models (VSMs) provide a powerful tool for representing word meanings and modeling the relations between them. While these models have demonstrated impressive success in capturing some aspects of word meaning (Landauer and Dumais, 1997; Turney et al., 2010; Mikolov et al., 2013; Baroni et al., 2014; Levy et al., 2014), they generally fail to capture the fact that single word forms often have multiple meanings. This can lead to counterintuitive results—for example, it should be possible for the nearest word to rock to be stone in everyday usage, punk in discussions of music, and crack (cocaine) in discussions about drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in"
N16-1163,D07-1007,1,0.78531,"ecent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–1383, c San Diego, California, June 12-"
N16-1163,P02-1033,1,0.7973,"aine) in discussions about drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–"
N16-1163,J93-1003,0,0.134511,"s 48 items in ESL, 87 items in RD, and 77 items in TOEFL. 2 The designated development set of MEN-3k (2000 items) was used for tuning. 3 To alleviate sparsity we lemmatized the ukWaC corpus. Runs without lemmatization produced weaker results. 1380 ∼5.8M lines of segmented Chinese-English parallel text from the DARPA BOLT project and the Broadcast Conversation subset of the segmented Chinese-English parallel data in the OntoNotes corpus (Weischedel et al., 2013).4 We perform word alignment with the Berkeley aligner (Liang et al., 2006). We filter out noisy alignments using the Gtest statistic (Dunning, 1993), with a threshold selected during tuning on a development set. We set α (see Equation 1) to 1.0. Each sensesense edge hei (cj ), ei0 (cj )i has individual weight 0 < βr ≤ 1, computed by obtaining the G-test statistic for the alignment of ei with cj and for the alignment of ei0 with cj , running these values through a logistic function, and averaging. Parameters for these computations, as well as the G-test statistic threshold below which we filtered out noisy alignments, were selected during tuning on the development set. Note that we have not currently incorporated special treatment for alig"
N16-1163,C14-1048,0,0.102654,"its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves perform"
N16-1163,P12-1092,0,0.335954,"arine@cs.umd.edu Abstract create a graph structure comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using paralle"
N16-1163,N15-1070,0,0.255088,"es, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance. 1 Introduction V"
N16-1163,S10-1003,0,0.101687,"al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–1383, c San Diego, California, June 12-17, 2016. 2016 Associatio"
N16-1163,W14-1618,0,0.0324777,"a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance. 1 Introduction Vector space models (VSMs) provide a powerful tool for representing word meanings and modeling the relations between them. While these models have demonstrated impressive success in capturing some aspects of word meaning (Landauer and Dumais, 1997; Turney et al., 2010; Mikolov et al., 2013; Baroni et al., 2014; Levy et al., 2014), they generally fail to capture the fact that single word forms often have multiple meanings. This can lead to counterintuitive results—for example, it should be possible for the nearest word to rock to be stone in everyday usage, punk in discussions of music, and crack (cocaine) in discussions about drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. re"
N16-1163,Q15-1016,0,0.0450174,"in and Goodenough, 1965), MC-30 (Miller and Charles, 1991), and the designated test subset (1000 items) of MEN3k (Bruni et al., 2014), using avgSim (Jauhar et al., 2015, eq. 8) as the similarity rating, and evaluating model ratings against human similarity ratings via Spearman’s rank correlation coefficient (ρ).2 Initial word representations. We use the word2vec (Mikolov et al., 2013) skip-gram architecture to train 80-dimensional word vectors (in keeping with Jauhar et al.), based on evidence that this model shows consistently strong performance on a wide array of tasks (Baroni et al., 2014; Levy et al., 2015). Training is on ukWaC (Ferraresi et al., 2008), a diverse 2B-word web corpus.3 Sense-graph construction from parallel text. To construct the sense graph per Section 2, we use 1 Because it is not clear how multi-word phrases should best be treated (and this is not a question being investigated here), we filter out any questions containing multi-word phrases for any of the relevant items (probe or possible response), and any questions for which any of the relevant items is completely out of vocabulary (no vectors available) for any of the evaluated models. This leaves 48 items in ESL, 87 items"
N16-1163,N06-1014,0,0.014505,"f vocabulary (no vectors available) for any of the evaluated models. This leaves 48 items in ESL, 87 items in RD, and 77 items in TOEFL. 2 The designated development set of MEN-3k (2000 items) was used for tuning. 3 To alleviate sparsity we lemmatized the ukWaC corpus. Runs without lemmatization produced weaker results. 1380 ∼5.8M lines of segmented Chinese-English parallel text from the DARPA BOLT project and the Broadcast Conversation subset of the segmented Chinese-English parallel data in the OntoNotes corpus (Weischedel et al., 2013).4 We perform word alignment with the Berkeley aligner (Liang et al., 2006). We filter out noisy alignments using the Gtest statistic (Dunning, 1993), with a threshold selected during tuning on a development set. We set α (see Equation 1) to 1.0. Each sensesense edge hei (cj ), ei0 (cj )i has individual weight 0 < βr ≤ 1, computed by obtaining the G-test statistic for the alignment of ei with cj and for the alignment of ei0 with cj , running these values through a logistic function, and averaging. Parameters for these computations, as well as the G-test statistic threshold below which we filtered out noisy alignments, were selected during tuning on the development se"
N16-1163,D14-1113,0,0.289377,"Missing"
N16-1163,P03-1058,0,0.230636,"out drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–1383, c San Diego"
N16-1163,N10-1013,0,0.270359,"{aetting, resnik}@umd.edu, marine@cs.umd.edu Abstract create a graph structure comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations l"
N16-1163,C14-1016,0,0.0959426,"ture comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derive"
N16-1163,W09-2413,0,\N,Missing
N18-1056,Q16-1031,0,0.0334641,"X1 + λx Sij ||Aei − Af j ||22 (1) 2 Dependency Contexts without a Treebank Using dependency contexts in multilingual settings may not always be possible, as dependency treebanks are not available for many languages. To circumvent this issue, we use related languages to train a weak dependency parser. We train a delexicalized parser using treebanks of related languages, where the word form based Bilingual Sparse Coding i,j s.t. Ak > 0 kDki k22 ≤ 1 k ∈ {e, f } where S is a translation matrix, and Ae and Af 3 More sophisticated techniques for transferring syntactic knowledge have been proposed (Ammar et al., 2016; Rasooli and Collins, 2017), but we prioritize simplicity and show that a simple delexicalized parser is effective. 609 are sparse matrices which are bilingual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment"
N18-1056,C16-1012,0,0.0216483,"potential to complement existing work on creating cross-lingual ontologies such as BabelNet and the Open Multilingual Wordnet, which are noisy because they are compiled semi-automatically, and have limited language coverage. In general, distributional approaches can help refine ontology construction for any language where sufficient resources are available. It remains to be seen how our approach performs for other language pairs beyond simluated low-resource settings. We anticipate that replacing our delexicalized parser with more sophisticated transfer strategies (Rasooli and Collins, 2017; Aufrant et al., 2016) might be beneficial in such settings.While our delexicalized parsing based approach exhibits robustness, it can benefit from more sophisticated approaches for transfer parsing (Rasooli and Collins, 2017; Aufrant et al., 2016) to improve parser performance. We aim to explore these and other directions in the future. Acknowledgments The authors would like to thank the members of the CLIP lab at the University of Maryland, members of the Cognitive Computation Group at the University of Pennsylvania, and the anonymous reviewers from EMNLP/CoNLL 2017 and NAACL 2018 for their constructive feedback."
N18-1056,P14-2131,0,0.0341054,"ll-defined relation than entailment. Also, we improve upon our previous approach by using dependency based embeddings (§6.1), and show that the improvements hold even when exposed to data scarce settings (§6.3). det nsubj dobj det dobj The tired traveler roamed the sandy desert, seeking food amod amod advcl Figure 2: Example Dependency Tree. We also do a more comprehensive evaluation on four languages paired with English, instead of just French. Dependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016). Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy. Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English. Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction. A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts. This"
N18-1056,E12-1004,0,0.0621056,"and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4 . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English. To begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011; Baroni et al., 2012; Kotlerman et al., 2010). 4.1 Annotation Setup The annotation task requires annotators to be fluent in both English and the non-English language. To ensure only fluent speakers perform the task, for each language, we provide task instructions in the non-English language itself. Also, we restrict the task to annotators verified by CrowdFlower to have those language skills. Finally, annotators also 4 http://crowdflower.com pair French-English Russian-English Arabic-English Chinese-English #crowdsourced #pos (= #neg) 2115 2264 2144 2165 763 706 691 806 Table 1: Crowd-sourced dataset statistics."
N18-1056,J10-4006,0,0.0368243,"Af SVD fruit Parsed Corpus 0.8 Ae Xe Figure 1: The B I S PARSE -D EP approach, which learns sparse bilingual embeddings using dependency based contexts. The resulting sparse embeddings, together with an unsupervised entailment scorer, can detect hypernyms across languages (e.g., pomme is a fruit). 3.1 Dependency Based Context Extraction The context of a word can be described in multiple ways using its syntactic neighborhood in a dependency graph. For instance, in Figure 2, we describe the context for a target word (traveler) in the following two ways: • F ULL context (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014): Children and parent words, concatenated with the label and direction of the relation (eg. roamed#nsubj−1 and tired#amod are contexts for traveler). • J OINT context (Chersoni et al., 2016): Parent concatenated with each of its siblings (eg. roamed#desert and roamed#seeking are contexts for traveler). These two contexts exploit different amounts of syntactic information – J OINT does not require labeled parses, unlike F ULL. The J OINT context combines parent and sibling information, while F ULL keeps them as distinct contexts. Both encode directionality into the con"
N18-1056,W11-2501,0,0.357939,"(Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4 . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English. To begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011; Baroni et al., 2012; Kotlerman et al., 2010). 4.1 Annotation Setup The annotation task requires annotators to be fluent in both English and the non-English language. To ensure only fluent speakers perform the task, for each language, we provide task instructions in the non-English language itself. Also, we restrict the task to annotators verified by CrowdFlower to have those language skills. Finally, annotators also 4 http://crowdflower.com pair French-English Russian-English Arabic-English Chinese-English #crowdsourced #pos (= #neg) 2115 2264 2144 2165 763 706 691 806 Table 1: Crowd-sourced"
N18-1056,P13-1133,0,0.0696763,"03; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4 . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English. To begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011;"
N18-1056,C92-2082,0,0.315395,"ld even when exposed to data scarce settings (§6.3). det nsubj dobj det dobj The tired traveler roamed the sandy desert, seeking food amod amod advcl Figure 2: Example Dependency Tree. We also do a more comprehensive evaluation on four languages paired with English, instead of just French. Dependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016). Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy. Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English. Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction. A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts. This scenario is more relevant in a cross-lingual setting, where treebanks might not be available for many languages. 3 Our Approach – B I S PARSE -D EP We p"
N18-1056,J15-4004,0,0.095916,"Missing"
N18-1056,E17-2064,0,0.0375479,"Missing"
N18-1056,2005.mtsummit-papers.11,0,0.0501587,"Missing"
N18-1056,D16-1205,0,0.0465809,"Missing"
N18-1056,P15-1038,0,0.0131658,"ly aligning the monolingual vectors. We compute the translation matrix using word alignments derived from parallel corpora (see corpus statistics in Table ??). While we use parallel corpora to generate the translation matrix to be comparable to baselines (§5.2), we can obtain the matrix from any bilingual dictionary. The monolingual corpora are parsed using Yara Parser (Rasooli and Tetreault, 2015), trained on the corresponding treebank from the Universal Dependency Treebank (McDonald et al., 2013) (UDT-v1.4). Yara Parser was chosen as it is fast, and competitive with stateof-the-art parsers (Choi et al., 2015). The monolingual corpora was POS-tagged using TurboTagger (Martins et al., 2013). We induce dependency contexts for words by first thresholding the language vocabulary to the top 50,000 nouns, verbs and adjectives. A co-occurrence matrix is computed over this vocabulary using the context types in §3.1. Inducing Dependency Contexts The entries of the word-context co-occurrence matrix are reweighted using Positive Pointwise Mutual Information (Bullinaria and Levy, 2007). The resulting matrix is reduced to 1000 dimensions using SVD (Golub and Kahan, 1965).6 These vectors are used as Xe , Xf in t"
N18-1056,E14-1049,0,0.0548402,"rades only marginally. In all these cases, it compares favorably with models that have been supplied with all necessary resources, showing promise for low-resource settings. We extensively evaluate B I S PARSE -D EP on a new crowd-sourced cross-lingual dataset, with over 2900 hypernym pairs, spanning four languages from distinct families – French, Russian, Arabic and Chinese – and release the datasets for future evaluations. 2 Related Work Cross-lingual Distributional Semantics Cross-lingual word embeddings have been shown to encode semantics across languages in tasks such as word similarity (Faruqui and Dyer, 2014) and lexicon induction (Vuli´c and Moens, 2015). Our works stands apart in two aspects (1) In contrast to tasks involving similarity and synonymy (symmetric relations), the focus of our work is on detecting asymmetric relations across languages, using cross-lingual embeddings. (2) Unlike most previous work, we use dependency context instead of lexical context to induce crosslingual embeddings, which allows us to abstract away from language specific word order, and (as we show) improves hypernymy detection. More closely related is our prior work (Vyas and Carpuat, 2016) where we used lexical co"
N18-1056,P14-1113,0,0.0910424,"other asymmetric semantic relationships can improve language understanding when translations are not exactly equivalent. One such relationship is cross-lingual hypernymy – identifying that e´ cureuil (“squirrel” in French) is a kind of rodent, or ворона (“crow” in Russian) is a kind of bird. The ability to detect hypernyms across languages serves as a building block in a range of cross-lingual tasks, including Recognizing Textual Entailment (RTE) (Negri et al., 2012, ∗ These authors contributed equally. https://github.com/yogarshi/ bisparse-dep/ 1 2013), constructing multilingual taxonomies (Fu et al., 2014), event coreference across multilingual news sources (Vossen et al., 2015), and evaluating Machine Translation output (Pad´o et al., 2009). Building models that can robustly identify hypernymy across the spectrum of human languages is a challenging problem, that is further compounded in low resource settings. At first glance, translating words to English and then identifying hypernyms in a monolingual setting may appear to be a sufficient solution. However, this approach cannot capture many phenomena. For instance, the English words cook, leader and supervisor can all be hypernyms of the Frenc"
N18-1056,P05-1014,0,0.638449,"ntifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation d"
N18-1056,P09-2018,0,0.0192746,"ween the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomat"
N18-1056,S12-1012,0,0.0385643,"a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and B"
N18-1056,P14-2050,0,0.0461982,"pus 0.8 Ae Xe Figure 1: The B I S PARSE -D EP approach, which learns sparse bilingual embeddings using dependency based contexts. The resulting sparse embeddings, together with an unsupervised entailment scorer, can detect hypernyms across languages (e.g., pomme is a fruit). 3.1 Dependency Based Context Extraction The context of a word can be described in multiple ways using its syntactic neighborhood in a dependency graph. For instance, in Figure 2, we describe the context for a target word (traveler) in the following two ways: • F ULL context (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014): Children and parent words, concatenated with the label and direction of the relation (eg. roamed#nsubj−1 and tired#amod are contexts for traveler). • J OINT context (Chersoni et al., 2016): Parent concatenated with each of its siblings (eg. roamed#desert and roamed#seeking are contexts for traveler). These two contexts exploit different amounts of syntactic information – J OINT does not require labeled parses, unlike F ULL. The J OINT context combines parent and sibling information, while F ULL keeps them as distinct contexts. Both encode directionality into the context, either through label"
N18-1056,P98-2127,0,0.0960675,"are bilingual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilin"
N18-1056,W15-1521,0,0.0861335,"Missing"
N18-1056,P13-2109,0,0.0303818,"ord alignments derived from parallel corpora (see corpus statistics in Table ??). While we use parallel corpora to generate the translation matrix to be comparable to baselines (§5.2), we can obtain the matrix from any bilingual dictionary. The monolingual corpora are parsed using Yara Parser (Rasooli and Tetreault, 2015), trained on the corresponding treebank from the Universal Dependency Treebank (McDonald et al., 2013) (UDT-v1.4). Yara Parser was chosen as it is fast, and competitive with stateof-the-art parsers (Choi et al., 2015). The monolingual corpora was POS-tagged using TurboTagger (Martins et al., 2013). We induce dependency contexts for words by first thresholding the language vocabulary to the top 50,000 nouns, verbs and adjectives. A co-occurrence matrix is computed over this vocabulary using the context types in §3.1. Inducing Dependency Contexts The entries of the word-context co-occurrence matrix are reweighted using Positive Pointwise Mutual Information (Bullinaria and Levy, 2007). The resulting matrix is reduced to 1000 dimensions using SVD (Golub and Kahan, 1965).6 These vectors are used as Xe , Xf in the setup from §3.3 to generate 100 dimensional sparse bilingual vectors. Evaluati"
N18-1056,D11-1006,0,0.0481171,"ct contexts. Both encode directionality into the context, either through label direction or through sibling-parent relations. We use word-context co-occurrences generated using these contexts in a distributional semantic model (DSM) in lieu of window based contexts to generate dependency based embeddings. 3.2 features are turned off, so that the parser is trained on purely non-lexical features (e.g. POS tags). The rationale behind this is that related languages show common syntactic structure that can be transferred to the original language, with delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011, inter alia) being one popular approach.3 3.3 Given a dependency based co-occurrence matrix described in the previous section(s), we generate B I S PARSE -D EP embeddings using the framework from our prior work (Vyas and Carpuat, 2016), which we henceforth call B I S PARSE. B I S PARSE generates sparse, bilingual word embeddings using a dictionary learning objective with a sparsity inducing l1 penalty. We give a brief overview of this approach, the full details of which can be found in our prior work. For two languages with vocabularies ve and vf , and monolingual dependency embeddings Xe and"
N18-1056,N16-1118,0,0.0438936,"o, we improve upon our previous approach by using dependency based embeddings (§6.1), and show that the improvements hold even when exposed to data scarce settings (§6.3). det nsubj dobj det dobj The tired traveler roamed the sandy desert, seeking food amod amod advcl Figure 2: Example Dependency Tree. We also do a more comprehensive evaluation on four languages paired with English, instead of just French. Dependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016). Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy. Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English. Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction. A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts. This scenario is more relevant in a cross-ling"
N18-1056,P09-1034,0,0.214013,"Missing"
N18-1056,J07-2002,0,0.182639,"Missing"
N18-1056,Q17-1020,0,0.106759,"Af j ||22 (1) 2 Dependency Contexts without a Treebank Using dependency contexts in multilingual settings may not always be possible, as dependency treebanks are not available for many languages. To circumvent this issue, we use related languages to train a weak dependency parser. We train a delexicalized parser using treebanks of related languages, where the word form based Bilingual Sparse Coding i,j s.t. Ak > 0 kDki k22 ≤ 1 k ∈ {e, f } where S is a translation matrix, and Ae and Af 3 More sophisticated techniques for transferring syntactic knowledge have been proposed (Ammar et al., 2016; Rasooli and Collins, 2017), but we prioritize simplicity and show that a simple delexicalized parser is effective. 609 are sparse matrices which are bilingual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers"
N18-1056,D16-1234,0,0.11608,"ypernymy monolingually precludes identifying leader or supervisor as a hypernyms of chef. Similarly, language-specific usage patterns can also influence hypernymy decisions. For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2 This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016). State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages. Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk, 2 All examples are from our dataset. 607 Proceedings of NAACL-HLT 2018, pages 607–618 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016). Furthermore, monolingual distributional approaches cannot be applied directly to the crosslingual task, because t"
N18-1056,E14-4008,0,0.34329,"Missing"
N18-1056,W15-4208,0,0.112978,"Missing"
N18-1056,E17-1007,0,0.648684,"precludes identifying leader or supervisor as a hypernyms of chef. Similarly, language-specific usage patterns can also influence hypernymy decisions. For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2 This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016). State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages. Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk, 2 All examples are from our dataset. 607 Proceedings of NAACL-HLT 2018, pages 607–618 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016). Furthermore, monolingual distributional approaches cannot be applied directly to the crosslingual task, because the vector spaces of two"
N18-1056,tiedemann-2012-parallel,0,0.0166915,"Missing"
N18-1056,W15-0814,0,0.0178851,"nding when translations are not exactly equivalent. One such relationship is cross-lingual hypernymy – identifying that e´ cureuil (“squirrel” in French) is a kind of rodent, or ворона (“crow” in Russian) is a kind of bird. The ability to detect hypernyms across languages serves as a building block in a range of cross-lingual tasks, including Recognizing Textual Entailment (RTE) (Negri et al., 2012, ∗ These authors contributed equally. https://github.com/yogarshi/ bisparse-dep/ 1 2013), constructing multilingual taxonomies (Fu et al., 2014), event coreference across multilingual news sources (Vossen et al., 2015), and evaluating Machine Translation output (Pad´o et al., 2009). Building models that can robustly identify hypernymy across the spectrum of human languages is a challenging problem, that is further compounded in low resource settings. At first glance, translating words to English and then identifying hypernyms in a monolingual setting may appear to be a sufficient solution. However, this approach cannot capture many phenomena. For instance, the English words cook, leader and supervisor can all be hypernyms of the French word chef, as the French word does not have a exact translation in Engli"
N18-1056,E17-2065,0,0.193563,"Missing"
N18-1056,P15-2070,0,0.0871147,"Missing"
N18-1056,P15-2118,0,0.0712993,"Missing"
N18-1056,N16-1142,1,0.700744,"ible usages. However, translating chef to cook and then determining hypernymy monolingually precludes identifying leader or supervisor as a hypernyms of chef. Similarly, language-specific usage patterns can also influence hypernymy decisions. For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2 This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016). State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages. Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk, 2 All examples are from our dataset. 607 Proceedings of NAACL-HLT 2018, pages 607–618 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2016). Furthermore, monolingual distributional approac"
N18-1056,W03-1011,0,0.037717,"ual representations in a shared semantic space. The translation matrix S (of size ve × vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages. 3.4 Unsupervised Entailment Scorer A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the B I S PARSE -D EP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision. 4 Crowd-Sourcing Annotations There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bo"
N18-1056,C98-2122,0,\N,Missing
N18-1056,S13-2005,0,\N,Missing
N18-1056,S12-1053,0,\N,Missing
N18-1136,E09-1003,0,0.0848635,"Missing"
N18-1136,S14-2010,0,0.0155843,"etecting divergence in parallel sentences, with the goal of (1) detecting differences ranging from large mismatches to subtle nuances, (2) without manual annotation. Cross-Lingual Semantic Similarity Model We address the first requirement using a neural model that compares the meaning of sentences using a range of granularities. We repurpose the Very Deep Pairwise Interaction (VDPWI) model, which has been previously been used to detect semantic textual similarity (STS) between English sentence pairs (He and Lin, 2016). It achieved competitive performance on data from the STS 2014 shared task (Agirre et al., 2014), and outperformed previous approaches on sentence classification tasks (He et al., 2015; Tai et al., 2015), with fewer parameters, faster training, and without requiring expensive external resources such as WordNet. The VDPWI model was designed for comparing the meaning of sentences in the same language, based not only on word-to-word similarity comparisons, but also on comparisons between overlapping spans of the two sentences, as learned by a deep convolutional neural network. We adapt the model to our cross-lingual task by initializing it with bilingual embeddings. To the best of our knowl"
N18-1136,W17-3209,1,0.863716,"an errors. Concurrently with our work, Hassan et al. (2018) claim that even small amounts of noise can have adverse effects on neural MT models, as they tend to assign high probabilities to rare events. They filter out noise and select relevant in-domain examples jointly, using similarities between sentence embeddings obtained from the encoder of a bidirectional neural MT system trained on clean indomain data. In contrast, we detect semantic divergence with dedicated models that require only 5000 parallel examples (see Section 5). This work builds on our initial study of semantic divergences (Carpuat et al., 2017), where we provide a framework for evaluating the impact of meaning mismatches in parallel segments on MT via data selection: we show that filtering out the most divergent segments in a training corpus improves translation quality. However, we previously detect mismatches using a cross-lingual entailment classifier, which is based on surface features only, and requires manually annotated training examples (Negri et al., 2012, 2013). In this paper, we detect divergences using a semanticallymotivated model that can be trained given any existing parallel corpus without manual intervention. 1504 3"
N18-1136,2012.eamt-1.60,0,0.0313791,"Missing"
N18-1136,2016.amta-researchers.8,0,0.149727,"Missing"
N18-1136,W17-3203,0,0.0172985,". At decoding time, we construct a new model by averaging the parameters for the 8 checkpoints with best validation perplexity, and decode with a beam of 5. All experiments are run thrice with distinct random seeds. Note that the baseline in this work is much stronger than in our prior work ( &gt;5 BLEU). This is due to multiple factors that have been recommended as best practices for neural MT and have been incorporated in the present baseline – deduplication of the training data, ensemble decoding using multiple random runs, use of Adam as the optimizer instead of AdaDelta (Bahar et al., 2017; Denkowski and Neubig, 2017), and checkpoint averaging (Bahar et al., 2017) – as well as a more recent neural modeling toolkit. 6.3 We train English-French neural MT systems by selecting the least divergent half of the training corpus with the following criteria: • S EMANTIC S IMILARITY (Section 3) • PARALLEL: the non-parallel sentence detector (Section 5.2) • E NTAILMENT: the entailment classifier (Section 5.5), as in Carpuat et al. (2017) • R ANDOM: Randomly downsampling the training corpus Neural MT System We use the attentional encoder-decoder model (Bahdanau et al., 2015) implemented in the SockEye toolkit (Hieber e"
N18-1136,J94-4004,0,0.890573,"drawn from translated texts (Tiedemann, 2011; Xu and Yvon, 2016), or the noisy process of extracting parallel segments from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005). In contrast, we view translation as a process that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent. This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst, 1995), typological divergences lead to structural mismatches (Dorr, 1994), differences in discourse organization can make it impossible to find oneto-one sentence alignments (Li et al., 2014). Cross-linguistic variations in other discourse phenomena such as coreference, discourse relation and modality (Lapshinova-Koltunski, 2015) compounded with translation effects that distinguish In this paper, we aim to provide empirical evidence that semantic divergences exist in parallel corpora and matter for downstream applications. This requires an automatic method to distinguish semantically equivalent sentence pairs from semantically divergent pairs, so that parallel corp"
N18-1136,C04-1151,0,0.417267,"ved from word alignments, and that these divergences matter for neural machine translation. 1 Introduction Parallel sentence pairs are sentences that are translations of each other, and are therefore often assumed to convey the same meaning in the source and target language. Occasional mismatches between source and target have been primarily viewed as alignment noise (Goutte et al., 2012) due to imperfect sentence alignment tools in parallel corpora drawn from translated texts (Tiedemann, 2011; Xu and Yvon, 2016), or the noisy process of extracting parallel segments from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005). In contrast, we view translation as a process that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent. This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst, 1995), typological divergences lead to structural mismatches (Dorr, 1994), differences in discourse organization can make it impossible to find oneto-one sentence alignments (Li et al., 2014). Cross-linguistic variations in other"
N18-1136,2012.amta-papers.7,1,0.800115,"ral model of bilingual semantic similarity which can be trained for any parallel corpus without any manual annotation. We show that our semantic model detects divergences more accurately than models based on surface features derived from word alignments, and that these divergences matter for neural machine translation. 1 Introduction Parallel sentence pairs are sentences that are translations of each other, and are therefore often assumed to convey the same meaning in the source and target language. Occasional mismatches between source and target have been primarily viewed as alignment noise (Goutte et al., 2012) due to imperfect sentence alignment tools in parallel corpora drawn from translated texts (Tiedemann, 2011; Xu and Yvon, 2016), or the noisy process of extracting parallel segments from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005). In contrast, we view translation as a process that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent. This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst,"
N18-1136,habash-dorr-2002-handling,0,0.127766,"ent does not necessarily imply semantic equivalence, and that this distinction matters in practice for a downstream NLP application. 1503 Proceedings of NAACL-HLT 2018, pages 1503–1515 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Background Translation Divergences We use the term semantic divergences to refer to bilingual sentence pairs, including translations, that do not have the same meaning. These differ from typological divergences, which have been defined as structural differences between sentences that convey the same meaning (Dorr, 1994; Habash and Dorr, 2002), and reflect the fact that languages do not encode the same information in the same way. Non-Parallel Corpora Mismatches in bilingual sentence pairs have previously been studied to extract parallel segments from non-parallel corpora, and augment MT training data (Fung and Cheung, 2004; Munteanu and Marcu, 2005, 2006; AbduIRauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012, inter alia). Methods for parallel sentence extraction rely primarily on surface features based on translation lexicons and word alignment patterns (Munteanu and Marcu, 2005, 2006). Similar features have prove"
N18-1136,D15-1181,0,0.0339054,"from large mismatches to subtle nuances, (2) without manual annotation. Cross-Lingual Semantic Similarity Model We address the first requirement using a neural model that compares the meaning of sentences using a range of granularities. We repurpose the Very Deep Pairwise Interaction (VDPWI) model, which has been previously been used to detect semantic textual similarity (STS) between English sentence pairs (He and Lin, 2016). It achieved competitive performance on data from the STS 2014 shared task (Agirre et al., 2014), and outperformed previous approaches on sentence classification tasks (He et al., 2015; Tai et al., 2015), with fewer parameters, faster training, and without requiring expensive external resources such as WordNet. The VDPWI model was designed for comparing the meaning of sentences in the same language, based not only on word-to-word similarity comparisons, but also on comparisons between overlapping spans of the two sentences, as learned by a deep convolutional neural network. We adapt the model to our cross-lingual task by initializing it with bilingual embeddings. To the best of our knowledge, this is the first time this model has been used for cross-lingual tasks in such a"
N18-1136,N16-1108,0,0.151909,"stream applications. This requires an automatic method to distinguish semantically equivalent sentence pairs from semantically divergent pairs, so that parallel corpora can be used more judiciously in downstream cross-lingual NLP applications. We propose a semantic model to automatically detect whether a sentence pair is semantically divergent (Section 3). While prior work relied on surface cues to detect mis-aligments, our approach focuses on comparing the meaning of words and overlapping text spans using bilingual word embeddings (Luong et al., 2015) and a deep convolutional neural network (He and Lin, 2016). Crucially, training this model requires no manual annotation. Noisy supervision is obtained automatically borrowing techniques developed for parallel sentence extraction (Munteanu and Marcu, 2005). Our model can thus easily be trained to detect semantic divergences in any parallel corpus. We extensively evaluate our semanticallymotivated models on intrinsic and extrinsic tasks: detection of divergent examples in two parallel English-French data sets (Section 5), and data selection for English-French and VietnameseEnglish machine translation (MT) (Section 6).The semantic models significantly"
N18-1136,E17-3017,0,0.0834797,"Missing"
N18-1136,S12-1102,0,0.0283599,"Missing"
N18-1136,W04-3250,0,0.51446,"Missing"
N18-1136,2005.mtsummit-papers.11,0,0.127475,"g equivalent from divergent examples. The break down per class shows that both equivalent and divergent examples are better detected. The improvement is larger for divergent examples with gains of about 10 points for F-score for the divergent class, when compared to the next-best scores. Among the baseline methods, the nonentailment model is the weakest. While it uses manually constructed training examples, these examples are drawn from distant domains, and the categories annotated do not exactly match the task 6 As in the prior work, alignments are trained on 2M sentence pairs from Europarl (Koehn, 2005) using the Berkeley aligner (Liang et al., 2006). The classifier is the linear SVM from Scikit-Learn. at hand. In contrast, the other models benefit from training on examples drawn from the same corpus as each test set. Next, the machine translation based model and the sentence embedding model, both of which are unsupervised, are significantly weaker than the two supervised models trained on synthetic data, highlighting the benefits of the automatically constructed divergence examples. The strength of the semantic similarity model compared to the sentence embeddings model highlights the benefi"
N18-1136,P11-1132,0,0.0877161,"Missing"
N18-1136,W15-2521,0,0.0211313,"that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent. This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst, 1995), typological divergences lead to structural mismatches (Dorr, 1994), differences in discourse organization can make it impossible to find oneto-one sentence alignments (Li et al., 2014). Cross-linguistic variations in other discourse phenomena such as coreference, discourse relation and modality (Lapshinova-Koltunski, 2015) compounded with translation effects that distinguish In this paper, we aim to provide empirical evidence that semantic divergences exist in parallel corpora and matter for downstream applications. This requires an automatic method to distinguish semantically equivalent sentence pairs from semantically divergent pairs, so that parallel corpora can be used more judiciously in downstream cross-lingual NLP applications. We propose a semantic model to automatically detect whether a sentence pair is semantically divergent (Section 3). While prior work relied on surface cues to detect mis-aligments,"
N18-1136,P14-2047,1,0.845089,"gments from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005). In contrast, we view translation as a process that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent. This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst, 1995), typological divergences lead to structural mismatches (Dorr, 1994), differences in discourse organization can make it impossible to find oneto-one sentence alignments (Li et al., 2014). Cross-linguistic variations in other discourse phenomena such as coreference, discourse relation and modality (Lapshinova-Koltunski, 2015) compounded with translation effects that distinguish In this paper, we aim to provide empirical evidence that semantic divergences exist in parallel corpora and matter for downstream applications. This requires an automatic method to distinguish semantically equivalent sentence pairs from semantically divergent pairs, so that parallel corpora can be used more judiciously in downstream cross-lingual NLP applications. We propose a semantic model to automati"
N18-1136,N06-1014,0,0.110688,"break down per class shows that both equivalent and divergent examples are better detected. The improvement is larger for divergent examples with gains of about 10 points for F-score for the divergent class, when compared to the next-best scores. Among the baseline methods, the nonentailment model is the weakest. While it uses manually constructed training examples, these examples are drawn from distant domains, and the categories annotated do not exactly match the task 6 As in the prior work, alignments are trained on 2M sentence pairs from Europarl (Koehn, 2005) using the Berkeley aligner (Liang et al., 2006). The classifier is the linear SVM from Scikit-Learn. at hand. In contrast, the other models benefit from training on examples drawn from the same corpus as each test set. Next, the machine translation based model and the sentence embedding model, both of which are unsupervised, are significantly weaker than the two supervised models trained on synthetic data, highlighting the benefits of the automatically constructed divergence examples. The strength of the semantic similarity model compared to the sentence embeddings model highlights the benefits of the fine-grained representation of bilingu"
N18-1136,L16-1147,0,0.0615432,"Missing"
N18-1136,S16-1102,0,0.0609069,"Missing"
N18-1136,W15-1521,0,0.429021,"tic divergences exist in parallel corpora and matter for downstream applications. This requires an automatic method to distinguish semantically equivalent sentence pairs from semantically divergent pairs, so that parallel corpora can be used more judiciously in downstream cross-lingual NLP applications. We propose a semantic model to automatically detect whether a sentence pair is semantically divergent (Section 3). While prior work relied on surface cues to detect mis-aligments, our approach focuses on comparing the meaning of words and overlapping text spans using bilingual word embeddings (Luong et al., 2015) and a deep convolutional neural network (He and Lin, 2016). Crucially, training this model requires no manual annotation. Noisy supervision is obtained automatically borrowing techniques developed for parallel sentence extraction (Munteanu and Marcu, 2005). Our model can thus easily be trained to detect semantic divergences in any parallel corpus. We extensively evaluate our semanticallymotivated models on intrinsic and extrinsic tasks: detection of divergent examples in two parallel English-French data sets (Section 5), and data selection for English-French and VietnameseEnglish machine tran"
N18-1136,J05-4003,0,0.370528,"s, and that these divergences matter for neural machine translation. 1 Introduction Parallel sentence pairs are sentences that are translations of each other, and are therefore often assumed to convey the same meaning in the source and target language. Occasional mismatches between source and target have been primarily viewed as alignment noise (Goutte et al., 2012) due to imperfect sentence alignment tools in parallel corpora drawn from translated texts (Tiedemann, 2011; Xu and Yvon, 2016), or the noisy process of extracting parallel segments from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005). In contrast, we view translation as a process that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent. This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst, 1995), typological divergences lead to structural mismatches (Dorr, 1994), differences in discourse organization can make it impossible to find oneto-one sentence alignments (Li et al., 2014). Cross-linguistic variations in other discourse phenomena such as"
N18-1136,P06-1011,0,0.0969935,"Missing"
N18-1136,S12-1053,0,0.0495254,"Missing"
N18-1136,S13-2005,0,0.0412842,"Missing"
N18-1136,W10-0734,0,0.0748432,"Missing"
N18-1136,N12-1061,0,0.0273643,"luding translations, that do not have the same meaning. These differ from typological divergences, which have been defined as structural differences between sentences that convey the same meaning (Dorr, 1994; Habash and Dorr, 2002), and reflect the fact that languages do not encode the same information in the same way. Non-Parallel Corpora Mismatches in bilingual sentence pairs have previously been studied to extract parallel segments from non-parallel corpora, and augment MT training data (Fung and Cheung, 2004; Munteanu and Marcu, 2005, 2006; AbduIRauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012, inter alia). Methods for parallel sentence extraction rely primarily on surface features based on translation lexicons and word alignment patterns (Munteanu and Marcu, 2005, 2006). Similar features have proved to be useful for the related task of translation quality estimation (Specia et al., 2010, 2016), which aims to detect divergences introduced by MT errors, rather than human translation. Recently, sentence embeddings have also been used to detect parallelism (Espa˜naBonet et al., 2017; Schwenk and Douze, 2017). Although embeddings capture semantic generalizations, these models are train"
N18-1136,W17-2619,0,0.0226528,"Munteanu and Marcu, 2005, 2006; AbduIRauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012, inter alia). Methods for parallel sentence extraction rely primarily on surface features based on translation lexicons and word alignment patterns (Munteanu and Marcu, 2005, 2006). Similar features have proved to be useful for the related task of translation quality estimation (Specia et al., 2010, 2016), which aims to detect divergences introduced by MT errors, rather than human translation. Recently, sentence embeddings have also been used to detect parallelism (Espa˜naBonet et al., 2017; Schwenk and Douze, 2017). Although embeddings capture semantic generalizations, these models are trained with neural MT objectives, which do not distinguish semantically equivalent segments from divergent parallel segments. Cross-Lingual Sentence Semantics Crosslingual semantic textual similarity (Agirre et al., 2016) and cross-lingual textual entailment (Negri and Mehdad, 2010; Negri et al., 2012, 2013) seek to characterize semantic relations between sentences in two different languages beyond translation equivalence, and are therefore directly relevant to our goal. While the human judgments obtained for each task d"
N18-1136,P16-1162,0,0.220966,"Missing"
N18-1136,N10-1063,0,0.04355,"sentence pairs, including translations, that do not have the same meaning. These differ from typological divergences, which have been defined as structural differences between sentences that convey the same meaning (Dorr, 1994; Habash and Dorr, 2002), and reflect the fact that languages do not encode the same information in the same way. Non-Parallel Corpora Mismatches in bilingual sentence pairs have previously been studied to extract parallel segments from non-parallel corpora, and augment MT training data (Fung and Cheung, 2004; Munteanu and Marcu, 2005, 2006; AbduIRauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012, inter alia). Methods for parallel sentence extraction rely primarily on surface features based on translation lexicons and word alignment patterns (Munteanu and Marcu, 2005, 2006). Similar features have proved to be useful for the related task of translation quality estimation (Specia et al., 2010, 2016), which aims to detect divergences introduced by MT errors, rather than human translation. Recently, sentence embeddings have also been used to detect parallelism (Espa˜naBonet et al., 2017; Schwenk and Douze, 2017). Although embeddings capture semantic generalizations,"
N18-1136,P13-1135,0,0.0456271,"Missing"
N18-1136,P15-1150,0,0.0604188,"Missing"
N18-1136,S13-2023,0,0.0317005,"Missing"
N18-1136,L16-1099,0,0.0458688,"at our semantic model detects divergences more accurately than models based on surface features derived from word alignments, and that these divergences matter for neural machine translation. 1 Introduction Parallel sentence pairs are sentences that are translations of each other, and are therefore often assumed to convey the same meaning in the source and target language. Occasional mismatches between source and target have been primarily viewed as alignment noise (Goutte et al., 2012) due to imperfect sentence alignment tools in parallel corpora drawn from translated texts (Tiedemann, 2011; Xu and Yvon, 2016), or the noisy process of extracting parallel segments from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005). In contrast, we view translation as a process that inherently introduces meaning mismatches, so that even correctly aligned sentence pairs are not necessarily semantically equivalent. This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst, 1995), typological divergences lead to structural mismatches (Dorr, 1994), differences in discourse organization can make it im"
N18-1136,S13-2021,0,0.0468817,"Missing"
N18-1136,W14-3315,0,\N,Missing
N19-1043,D18-1549,0,0.0212929,"corpus is then built by swapping the source and target sentences of a parallel corpus and appending the swapped version to the original. Prior work has sought to simplify the optimization of reconstruction losses by side-stepping beam search. Tu et al. (2017) first proposed to reconstruct NMT input from the decoder’s hidden states while Wang et al. (2018a,b) suggested to use both encoder and decoder hidden states to improve translation of dropped pronouns. However, these models might achieve low reconstruction errors by learning to copy the input to hidden states. To avoid copying the input, Artetxe et al. (2018) and Lample et al. (2018) used denoising autoencoders (Vincent et al., 2008) in unsupervised NMT. 3.1 Bi-Directional Reconstruction Our bi-directional model performs both forward translation and backward reconstruction. By contrast, uni-directional models require an auxiliary reconstruction module, which introduces additional parameters. This module can be either a decoder-based reconstructor (Tu et al., 2017; Wang et al., 2018a,b) or a reversed dual NMT model (Cheng et al., 2016; He et al., 2016; Wang et al., 2018c; Zhang et al., 2018). Here the reconstructor, which shares the same parameter"
N19-1043,P16-1185,0,0.140656,"where the probability of predicting target token et at step t is conditioned on the previously generated sequence of tokens e&lt;t and the source sequence f given the model parameter θ. Suppose each token is indexed and represented as a one-hot vector, its probability is realized as a softmax function over a linear transformation a(ht ) where ht is the decoder’s hidden state at step t: Using round-trip translations (f → e → fˆ) as a training signal for NMT usually requires auxiliary models to perform back-translation and cannot be trained end-to-end without reinforcement learning. For instance, Cheng et al. (2016) added a reconstruction loss for monolingual examples to the training objective. He et al. (2016) evaluated the quality of e by a language model and fˆ by a reconstruction likelihood. Both approaches have symmetric forward and backward translation models which are updated alternatively. This require policy gradient algorithms for training, which are not always stable. P (et |e&lt;t , f ; θ) = softmax(a(ht ))&gt; et . (1) The hidden state is calculated by a neural network g given the embeddings of the previous target tokens e&lt;t in the embedding matrix E(e&lt;t ) and the context ct coming from the source"
N19-1043,Q17-1024,0,0.098081,"Missing"
N19-1043,D18-1045,0,0.0468405,"Missing"
N19-1043,W17-3204,0,0.02895,"d inputs, obtained by backtranslating translation hypotheses into the input language. We leverage differentiable sampling and bi-directional NMT to train models end-to-end, without introducing additional parameters. This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (Koehn and Knowles, 2017). Past work has addressed this problem by leveraging monolingual data (Sennrich et al., 2016a; Ramachandran et al., 2017) or multilingual parallel data (Zoph et al., 2016; Johnson et al., 2017; Gu et al., 2018a). We hypothesize that the traditional training can be complemented by better leveraging limited training data. To this end, we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples. Input reconstruction is motivated by the idea of rou"
N19-1043,N18-1032,0,0.287874,"is approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (Koehn and Knowles, 2017). Past work has addressed this problem by leveraging monolingual data (Sennrich et al., 2016a; Ramachandran et al., 2017) or multilingual parallel data (Zoph et al., 2016; Johnson et al., 2017; Gu et al., 2018a). We hypothesize that the traditional training can be complemented by better leveraging limited training data. To this end, we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples. Input reconstruction is motivated by the idea of round-trip translation. Suppose sentence f is translated forward to e using model θf e and then translated back to fˆ using model θef , then e is more likely to be a good translation if the distance between fˆ an"
N19-1043,N18-2081,0,0.254908,"is approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (Koehn and Knowles, 2017). Past work has addressed this problem by leveraging monolingual data (Sennrich et al., 2016a; Ramachandran et al., 2017) or multilingual parallel data (Zoph et al., 2016; Johnson et al., 2017; Gu et al., 2018a). We hypothesize that the traditional training can be complemented by better leveraging limited training data. To this end, we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples. Input reconstruction is motivated by the idea of round-trip translation. Suppose sentence f is translated forward to e using model θf e and then translated back to fˆ using model θef , then e is more likely to be a good translation if the distance between fˆ an"
N19-1043,N18-1031,0,0.0527838,"Missing"
N19-1043,W18-2710,1,0.892282,"Suppose sentence f is translated forward to e using model θf e and then translated back to fˆ using model θef , then e is more likely to be a good translation if the distance between fˆ and f is small (Brislin, 1970). Prior work applied round-trip translation to monolingual examples and sampled the intermediate translation e from a K-best list generated by model • Translations are sampled using the StraightThrough Gumbel Softmax (STGS) estimator (Jang et al., 2017; Bengio et al., 2013), which allows back-propagating reconstruction errors. • Our approach builds on the bi-directional NMT model (Niu et al., 2018; Johnson et al., 2017), which improves low-resource translation by jointly modeling translation in both directions (e.g., Swahili ↔ English). A single bi-directional model is used as a translator and a reconstructor (i.e. θef = θf e ) without introducing more parameters. Experiments show that our approach outperforms reconstruction from hidden states. It achieves consistent improvements across various low-resource language pairs and directions, showing its effectiveness in making better use of limited parallel data. 442 Proceedings of NAACL-HLT 2019, pages 442–448 c Minneapolis, Minnesota, Ju"
N19-1043,E17-3017,0,0.0308184,"er used for sampling only consumes its previously predicted eˆ&lt;t . This contrasts with the usual teacher forcing strategy (Williams and Zipser, 1989), which always feeds in the groundtruth previous tokens e&lt;t when predicting the current token eˆt . With teacher forcing, the sequence concatenation [e&lt;t ; eˆt ] is probably coherent at each time step, but the actual predicted sequence [ˆ e&lt;t ; eˆt ] would break the continuity.2 4.2 Model Configuration and Baseline We build NMT models upon the attentional RNN encoder-decoder architecture (Bahdanau et al., 2015) implemented in the Sockeye toolkit (Hieber et al., 2017). Our translation model uses a bidirectional encoder with a single LSTM layer of size 512, multilayer perceptron attention with a layer size of 512, and word representations of size 512. We apply layer normalization (Ba et al., 3 https://www.iarpa.gov/index.php/ research-programs/material 4 http://casmacat.eu/corpus/ global-voices.html 5 http://data.statmt.org/wmt18/ translation-task/preprocessed/ 1 i.e. Gk = − log(− log(uk )) and uk ∼ Uniform(0, 1). Sampling with teacher forcing yielded consistently worse BLEU than baselines in preliminary experiments. 2 444 Model Baseline H IDDEN ∆ β=0 ∆ β ="
N19-1043,W18-6319,0,0.0218067,"tracted from the held-out ANALYSIS set of MATERIAL. Parallel Turkish↔English (TR↔EN) data is provided by the WMT news translation task (Bojar et al., 2018). We use pre-processed “corpus”, “newsdev2016”, “newstest2017” as training, development and test sets.5 We apply normalization, tokenization, truecasing, joint source-target BPE with 32,000 operations (Sennrich et al., 2016b) and sentencefiltering (length 80 cutoff) to parallel data. Itemized data statistics after preprocessing can be found in Table 1. We report case-insensitive BLEU with the WMT standard ‘13a’ tokenization using SacreBLEU (Post, 2018). k where Gk is i.i.d. and drawn from Gumbel(0, 1)1 . We use scaled Gumbel with parameter β, i.e. Gumbel(0, β), to control the randomness. The sampling becomes deterministic (which is equivalent to greedy search) as β approaches 0. Since arg max is not a differentiable operation, we approximate its gradient with the StraightThrough Gumbel Softmax (STGS) (Jang et al., 2017; Bengio et al., 2013): ∇θ et ≈ ∇θ e˜t , where  e˜t = softmax (a(ht ) + G)/τ (6) As τ approaches 0, softmax is closer to arg max but training might be more unstable. While the STGS estimator is biased when τ is large, it perf"
N19-1043,D18-1333,0,0.0307421,"vely translated to e or f . The language is marked by a tag (e.g., &lt;en&gt;) at the beginning of each source sentence (Johnson et al., 2017; Niu et al., 2018). To facilitate symmetric reconstruction, we also add language tags to target sentences. The training data corpus is then built by swapping the source and target sentences of a parallel corpus and appending the swapped version to the original. Prior work has sought to simplify the optimization of reconstruction losses by side-stepping beam search. Tu et al. (2017) first proposed to reconstruct NMT input from the decoder’s hidden states while Wang et al. (2018a,b) suggested to use both encoder and decoder hidden states to improve translation of dropped pronouns. However, these models might achieve low reconstruction errors by learning to copy the input to hidden states. To avoid copying the input, Artetxe et al. (2018) and Lample et al. (2018) used denoising autoencoders (Vincent et al., 2008) in unsupervised NMT. 3.1 Bi-Directional Reconstruction Our bi-directional model performs both forward translation and backward reconstruction. By contrast, uni-directional models require an auxiliary reconstruction module, which introduces additional paramete"
N19-1043,E17-2025,0,0.0631257,"Missing"
N19-1043,D17-1039,0,0.0196144,"ng and bi-directional NMT to train models end-to-end, without introducing additional parameters. This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (Koehn and Knowles, 2017). Past work has addressed this problem by leveraging monolingual data (Sennrich et al., 2016a; Ramachandran et al., 2017) or multilingual parallel data (Zoph et al., 2016; Johnson et al., 2017; Gu et al., 2018a). We hypothesize that the traditional training can be complemented by better leveraging limited training data. To this end, we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples. Input reconstruction is motivated by the idea of round-trip translation. Suppose sentence f is translated forward to e using model θf e and then translated back to fˆ using"
N19-1043,P16-1009,0,0.455539,"ge differentiable sampling and bi-directional NMT to train models end-to-end, without introducing additional parameters. This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (Koehn and Knowles, 2017). Past work has addressed this problem by leveraging monolingual data (Sennrich et al., 2016a; Ramachandran et al., 2017) or multilingual parallel data (Zoph et al., 2016; Johnson et al., 2017; Gu et al., 2018a). We hypothesize that the traditional training can be complemented by better leveraging limited training data. To this end, we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples. Input reconstruction is motivated by the idea of round-trip translation. Suppose sentence f is translated forward to e using model θf e and then"
N19-1043,P16-1162,0,0.694193,"ge differentiable sampling and bi-directional NMT to train models end-to-end, without introducing additional parameters. This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (Koehn and Knowles, 2017). Past work has addressed this problem by leveraging monolingual data (Sennrich et al., 2016a; Ramachandran et al., 2017) or multilingual parallel data (Zoph et al., 2016; Johnson et al., 2017; Gu et al., 2018a). We hypothesize that the traditional training can be complemented by better leveraging limited training data. To this end, we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples. Input reconstruction is motivated by the idea of round-trip translation. Suppose sentence f is translated forward to e using model θf e and then"
N19-1043,D16-1163,0,0.014811,"out introducing additional parameters. This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) performance degrades sharply when parallel training data is limited (Koehn and Knowles, 2017). Past work has addressed this problem by leveraging monolingual data (Sennrich et al., 2016a; Ramachandran et al., 2017) or multilingual parallel data (Zoph et al., 2016; Johnson et al., 2017; Gu et al., 2018a). We hypothesize that the traditional training can be complemented by better leveraging limited training data. To this end, we propose a new training objective for this model by augmenting the standard translation cross-entropy loss with a differentiable input reconstruction loss to further exploit the source side of parallel samples. Input reconstruction is motivated by the idea of round-trip translation. Suppose sentence f is translated forward to e using model θf e and then translated back to fˆ using model θef , then e is more likely to be a good tr"
N19-1043,P13-1135,0,0.0285781,"p beam search and back-propagate error signals. We use the Gumbel-Max reparameterization trick (Maddison et al., 2014) to sample a translation token at each time step from the softmax distribution in Equation 1:   et = one-hot arg max a(ht )k + Gk (5) Experiments 4.1 Tasks and Data We evaluate our approach on four low-resource language pairs. Parallel data for Swahili↔English (SW↔EN), Tagalog↔English (TL↔EN) and Somali↔English (SO↔EN) contains a mixture of domains such as news and weblogs and is collected from the IARPA MATERIAL program3 , the Global Voices parallel corpus4 , Common Crawl (Smith et al., 2013), and the LORELEI Somali representative language pack (LDC2018T11). The test samples are extracted from the held-out ANALYSIS set of MATERIAL. Parallel Turkish↔English (TR↔EN) data is provided by the WMT news translation task (Bojar et al., 2018). We use pre-processed “corpus”, “newsdev2016”, “newstest2017” as training, development and test sets.5 We apply normalization, tokenization, truecasing, joint source-target BPE with 32,000 operations (Sennrich et al., 2016b) and sentencefiltering (length 80 cutoff) to parallel data. Itemized data statistics after preprocessing can be found in Table 1."
N19-1189,P07-2045,0,0.0156253,"for patent. Unlabeled-domain Data For additional unlabeled-domain data, we use web-crawled bitext from the Paracrawl project.4 We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively. Data Preprocessing All datasets are tokenized using the Moses (Koehn et al., 2007) tokenizer. We learn byte pair encoding (BPE) segmentation 2 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 3 Appendix A explains our choice of 15k in detail. 4 https://www.paracrawl.eu/ models (Sennrich et al., 2016) from general domain data. The BPE models are trained separately for each language, and the number of BPE symbols is set to 30k. We then apply the BPE models to in-domain and Paracrawl data, so that the parameters of the generic model can be applied as an initialization for continued training. Once we have a converged generic NMT model, which is very expe"
N19-1189,W17-3204,0,0.029107,"o the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs. 1 Domain Data Generic Model In-Domain Data Continued Training Initialization Domain Specific Model Unlabeled-Domain Data Figure 1: Workflow of our domain adaptation system. Introduction Neural machine translation (NMT) performance often drops when training and test domains do not match and when in-domain training data is scarce (Koehn and Knowles, 2017). Tailoring the NMT system to each domain could improve performance, but unfortunately high-quality parallel data does not exist for all domains. Domain adaptation techniques address this problem by exploiting diverse data sources to improve indomain translation, including general domain data that does not match the domain of interest, and unlabeled domain data whose domain is unknown (e.g. webcrawl like Paracrawl). One approach to exploit unlabeled-domain bitext is to apply data selection techniques (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) to find bitext that are simila"
N19-1189,N19-1208,1,0.732218,"Missing"
N19-1189,L16-1147,0,0.0208309,"ange from adding it to the already selected subset of N . The selected sentence is the one which most decreases Hn , the cross-entropy between previously selected n-sentence corpus and I. Curriculum Learning Training Strategy 1 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 1904 3 Experiments and Results We evaluate on four domain adaptation tasks. The code base is provided to ensure reproducibility.2 3.1 Data and Setup General Domain Data We have two general domain datasets, Russian-English (ru) and German-English (de). Both are a concatenation of OpenSubtitles2018 (Lison and Tiedemann, 2016) and WMT 2017 (Bojar et al., 2017), which contains data from several domains, e.g. parliamentary proceedings (Europarl, UN Parallel Corpus), political/economic news (news commentary, Rapid corpus), and web-crawled parallel corpus (Common Crawl, Yandex, Wikipedia titles). We performed sentence length filtering (up to 80 words) after tokenization, ending up with 28 million sentence pairs for German and 51 million sentence pairs for Russian. In-domain Data We evaluate our proposed methods on two distinct domains per language pair: • TED talks: data-split from Duh (2018). • Patents: from the World"
N19-1189,D15-1166,0,0.0533833,"by exploiting diverse data sources to improve indomain translation, including general domain data that does not match the domain of interest, and unlabeled domain data whose domain is unknown (e.g. webcrawl like Paracrawl). One approach to exploit unlabeled-domain bitext is to apply data selection techniques (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) to find bitext that are similar to in-domain data. This selected data can additionally be combined with in-domain bitext and trained in a continued training framework, as shown in Figure 1. Continued training or fine-tuning (Luong et al., 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017) is an adaptation technique where a model is first trained on the large general domain data, then used as initialization of a new model which is further trained on in-domain bitext. In our framework, the selected samples are concatenated with in-domain data, then used for continued training. This effectively increases the in-domain training size with “pseudo” in-domain samples, and is helpful in continued training (Koehn et al., 2018). A challenge with employing data selection in continued training is that there exists no clear-cut way to define"
N19-1189,2012.iwslt-papers.7,0,0.0213726,"les and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They adopt gradual fine-tuning, which does the opposite of our method: training starts from the whole dataset, and the training set gradually decreases by removing less similar sentences. Wang et al. (2018) use a similar approach, where the NMT model is trained on progressively noisereduced data batches. However, such schedules have the risk of wasting computation on non-re"
N19-1189,D09-1074,0,0.0379615,"c domain adaptation methods, our curriculum learning approach has connections to instance weighting. In our work, the presentation of certain examples at specific training phases is equivalent to up-weighting those examples and down-weight the others at that time. Weights of similar samples and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They adopt gradual fine-tuning, which does the opposite of our method: training starts fr"
N19-1189,P10-2041,0,0.891363,"est domains do not match and when in-domain training data is scarce (Koehn and Knowles, 2017). Tailoring the NMT system to each domain could improve performance, but unfortunately high-quality parallel data does not exist for all domains. Domain adaptation techniques address this problem by exploiting diverse data sources to improve indomain translation, including general domain data that does not match the domain of interest, and unlabeled domain data whose domain is unknown (e.g. webcrawl like Paracrawl). One approach to exploit unlabeled-domain bitext is to apply data selection techniques (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) to find bitext that are similar to in-domain data. This selected data can additionally be combined with in-domain bitext and trained in a continued training framework, as shown in Figure 1. Continued training or fine-tuning (Luong et al., 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017) is an adaptation technique where a model is first trained on the large general domain data, then used as initialization of a new model which is further trained on in-domain bitext. In our framework, the selected samples are concatenated with in-domain data, then us"
N19-1189,N19-1119,0,0.0486853,"ests that CDS dominates ML for distant domains such as Patent, while ML can do slightly better than CDS for domains that are not that distant such as TED. 5 Related Work Curriculum learning has shown its potential to improve sample efficiency for neural models (Graves et al., 2017; Weinshall and Cohen, 2018) by guiding the order of presented samples, usually from easier-to-learn samples to difficult samples. Although there is no single criterion to measure difficulty for general neural machine translation tasks (Kocmi and Bojar, 2017; Wang et al., 2018; Zhang et al., 2018; Kumar et al., 2019; Platanios et al., 2019), for the domain adaptation scenario, we measure difficulty based on the distance from indomain data. Compared to previous work, our application of curriculum learning mainly focuses on improvements on translation quality without consideration of convergence speed. 12 In Figure 8, for the purpose of fair comparison, each distribution is defined on the same vocabulary, consisting of the source side vocabulary of TED, patent and Paracrawl data. Chu and Wang (2018) surveyed recent domain adaptation methods for NMT. In their taxonomy, our workflow in Figure 1 can be considered a hybrid that uses b"
N19-1189,P16-1162,0,0.142635,"ering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively. Data Preprocessing All datasets are tokenized using the Moses (Koehn et al., 2007) tokenizer. We learn byte pair encoding (BPE) segmentation 2 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 3 Appendix A explains our choice of 15k in detail. 4 https://www.paracrawl.eu/ models (Sennrich et al., 2016) from general domain data. The BPE models are trained separately for each language, and the number of BPE symbols is set to 30k. We then apply the BPE models to in-domain and Paracrawl data, so that the parameters of the generic model can be applied as an initialization for continued training. Once we have a converged generic NMT model, which is very expensive to train, we can adapt it to different domains, without building up a new vocabulary and retraining the model. NMT Setup Our NMT models are developed in Sockeye5 (Hieber et al., 2017). The generic model and continued training model are t"
N19-1189,W10-1759,0,0.0296606,"les and down-weight the others at that time. Weights of similar samples and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They adopt gradual fine-tuning, which does the opposite of our method: training starts from the whole dataset, and the training set gradually decreases by removing less similar sentences. Wang et al. (2018) use a similar approach, where the NMT model is trained on progressively noisereduced data batches"
N19-1189,N19-1209,1,0.789476,"till used, so the model will not forget what it just learned. (3) It builds on a strong continued training baseline, which continues on in-domain data. (4) The method implements best practices that have shown to be helpful in NMT, e.g. bucketing, mini-batching, and data shuffling. For future work, it would be interesting to measure how curriculum learning models perform on the general domain test set (rather than the indomain test set we focus on in this work); do they suffer more or less from catastrophic forgetting (Goodfellow et al., 2014; Kirkpatrick et al., 2017; Khayrallah et al., 2018; Thompson et al., 2019)? Acknowledgments This work is supported in part by a AWS Machine Learning Research Award and a grant from the Office of the Director of National Intelligence, Intelligence Advanced Research Projects Activity (IARPA), via contract FA8650-17-C-9115. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors. We thank the organizers and participants of the 2018 Machine Translation Marathon for providing a productive environment to start this project. We also than"
N19-1189,D17-1155,0,0.0543658,"a modified training procedure based for continued training. For data-centric domain adaptation methods, our curriculum learning approach has connections to instance weighting. In our work, the presentation of certain examples at specific training phases is equivalent to up-weighting those examples and down-weight the others at that time. Weights of similar samples and less similar ones are adjusted dynamically during the training of NMT models based on the curriculum training strategy. In NMT, instance weighting is usually implemented by modifying the objective function (Chen and Huang, 2016; Wang et al., 2017; Chen et al., 2017). In statistical machine translation, Matsoukas et al. (2009) extract features from sentences to capture their domains and then use a classifier to map features to sentence weights. Foster et al. extend this method by weighting at the level of phrase pairs. Shah et al. (2010) use resampling to weight corpora and alignments. Mansour and Ney (2012) focus on sentence-level weighting for phrase extraction. Zhou et al. (2015) weight examples based on their word distributions. For model-centric domain adaptation methods, our work is related to van der Wees et al. (2017). They ado"
N19-1189,W18-6314,0,0.289006,"distant from the Paracrawl data than TED is. Figure 2 suggests that CDS dominates ML for distant domains such as Patent, while ML can do slightly better than CDS for domains that are not that distant such as TED. 5 Related Work Curriculum learning has shown its potential to improve sample efficiency for neural models (Graves et al., 2017; Weinshall and Cohen, 2018) by guiding the order of presented samples, usually from easier-to-learn samples to difficult samples. Although there is no single criterion to measure difficulty for general neural machine translation tasks (Kocmi and Bojar, 2017; Wang et al., 2018; Zhang et al., 2018; Kumar et al., 2019; Platanios et al., 2019), for the domain adaptation scenario, we measure difficulty based on the distance from indomain data. Compared to previous work, our application of curriculum learning mainly focuses on improvements on translation quality without consideration of convergence speed. 12 In Figure 8, for the purpose of fair comparison, each distribution is defined on the same vocabulary, consisting of the source side vocabulary of TED, patent and Paracrawl data. Chu and Wang (2018) surveyed recent domain adaptation methods for NMT. In their taxonomy"
N19-1189,D17-1147,0,0.148955,"Missing"
N19-1189,D17-1319,0,0.0153486,"sian. In-domain Data We evaluate our proposed methods on two distinct domains per language pair: • TED talks: data-split from Duh (2018). • Patents: from the World International Property Organization COPPA-V2 dataset (Junczys-Dowmunt et al., 2016). We randomly sample 15k parallel sentences from the original corpora as our in-domain bitext.3 We also have around 2k sentences of development and test data for TED and 3k for patent. Unlabeled-domain Data For additional unlabeled-domain data, we use web-crawled bitext from the Paracrawl project.4 We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively. Data Preprocessing All datasets are tokenized using the Moses (Koehn et al., 2007) tokenizer. We learn byte pair encoding (BPE) segmentation 2 https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 3 Appendix A explains our choice of 15k in det"
N19-1207,P17-2058,0,0.316307,"in ML training, prefixes y <t are subsequences of reference translations. As a result, the model is never exposed to its own errors during training and errors accumulate at test time. This mismatch is known as the exposure bias problem (Ranzato et al., 2015). 2.1 Despite the empirical success of scheduled sampling, one limitation is that the discontinuity of the argmax operation makes it impossible to penalize errors made in previous steps, which can lead to slow and unstable training. We address this issue using a continuous relaxation to the greedy search and sampling process, similarly to Goyal et al. (2017), which we describe in Section 2.2. Another limitation of scheduled sampling is that it incorrectly assumes that the reference and predicted sequence are aligned by time indices which introduces additional noise to the training signal.2 We address this problem with a novel differentiable sampling algorithm with an alignment based objective called soft aligned maximum likelihood (SAML). It is used in combination with maximum likelihood to define our training objective J = JM L + JSAM L , where JM L is computed based on reference translations, and JSAM L is computed based on sampled translations"
N19-1207,2015.iwslt-evaluation.11,0,0.122095,"Missing"
N19-1207,P18-2053,0,0.0240707,"iques such as policy gradient (Williams, 1992) and actor-critic (Sutton and Barto, 2018; Degris et al., 2012) are thus required to find an unbiased estimation of the gradient to optimize the model. Due to the high variance of the gradient estimation, training with RL can be slow and unstable (Henderson et al., 2018; Wu et al., 2018). Recent alternatives use data augmentation to incorporate the sentence-level reward into the training objective more efficiently (Norouzi et al., 2016). Finally, our SAML loss shares the idea of flexible reference word order with the bag-of-word loss introduced by Ma et al. (2018) to improve source coverage. However, their loss is computed with teacher forcing and therefore does not address exposure bias. 5 Conclusion We introduced a differentiable sampling algorithm which exposes a sequence-to-sequence model to its own predictions during training and compares them to reference sequences flexibly to backpropagate reliable error signals. By soft aligning reference and sampled sequences, our approach consistently improves BLEU over maximum likelihood and scheduled sampling baselines on three IWSLT tasks, with larger improvements for greedy search and smaller beam sizes."
N19-1207,N19-1043,1,0.741282,"pled in our approach. where τ is the temperature parameter. As τ diminishes to zero, z˜ becomes the same as one-hot sample z. The Straight-Through Gumbel-Softmax maintains the differentiability of the Gumbel-Softmax estimator while allowing for discrete sampling by taking different paths in the forward and backward pass. It uses argmax to get the one-hot sample z in the forward pass, but uses its continuous approximation z˜ in the backward pass. While ST estimators are biased, they have been shown to work well in latent tree learning (Choi et al., 2018) and semisupervised machine translation (Niu et al., 2019). 2.3 Soft Aligned Maximum Likelihood The soft aligned maximum likelihood (SAML) is defined as the probability that the reference can be aligned with the sampled output using a soft alignment predicted by the model: 0 PSAM L (y |x) = T X T Y atj · p(yt |y ˜<j , x; θ) t=1 j=1 (6) where T is the length of the reference sequence, T 0 is the length of the sampled sequence, atj is the predicted soft alignment between the reference word yt and sampled prefix y ˜<j . Training with the SAML objective consists in maximizing: JSAM L (θ) = N X log PSAM L (y (n) |x(n) ) (7) n=1 The conditional probability"
N19-1207,E17-3017,0,0.0533699,"Missing"
N19-1207,D18-1397,0,0.0317213,"nau et al., 2016; Sutton and Barto, 2018; Van Hasselt et al., 2016) address exposure bias by directly optimizing a sentence-level reward for the model generated sequences. Evaluation metrics such as BLEU can be used as rewards, but they are discontinuous and hard to optimize. Techniques such as policy gradient (Williams, 1992) and actor-critic (Sutton and Barto, 2018; Degris et al., 2012) are thus required to find an unbiased estimation of the gradient to optimize the model. Due to the high variance of the gradient estimation, training with RL can be slow and unstable (Henderson et al., 2018; Wu et al., 2018). Recent alternatives use data augmentation to incorporate the sentence-level reward into the training objective more efficiently (Norouzi et al., 2016). Finally, our SAML loss shares the idea of flexible reference word order with the bag-of-word loss introduced by Ma et al. (2018) to improve source coverage. However, their loss is computed with teacher forcing and therefore does not address exposure bias. 5 Conclusion We introduced a differentiable sampling algorithm which exposes a sequence-to-sequence model to its own predictions during training and compares them to reference sequences flex"
P04-1081,S01-1014,0,0.0478551,"Missing"
P04-1081,W02-0813,0,0.0514893,"based model outperforms the SVM-based model. It is hoped that these highly encouraging first results on KPCA for natural language processing tasks will inspire further development of these directions. 1 Introduction Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly based on generalizations over feature combinations. The 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for su"
P04-1081,1993.eamt-1.1,0,0.0871062,"Missing"
P04-1081,C02-1054,0,0.0758152,"Missing"
P04-1081,S01-1004,0,0.356016,"uation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly based on generalizations over feature combinations. The 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. technique is applicable whenever vector representations of a disambiguation task can be generated; thus many properties of our technique can be expected to be highly attractive fro"
P04-1081,W02-1002,0,0.238923,"the SVM-based model. It is hoped that these highly encouraging first results on KPCA for natural language processing tasks will inspire further development of these directions. 1 Introduction Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly based on generalizations over feature combinations. The 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in"
P04-1081,P03-1004,0,0.0503102,"to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). A major advantage of KPCA is that, unlike other common analysis techniques, as with other kernel methods it inherently takes combinations of predictive features into account when optimizing dimensionality reduction. For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). Another advantage of KPCA for the WSD task is that the dimensionality of the input data is generally very Table 1: Two of the Senseval-2 sense classes for the target word “art”, from WordNet 1.7 (Fellbaum 1998). Class 1 2 Sense the creation of beautiful or significant things a superior skill large, a condition where kernel methods excel. Nonlinear principal components (Diamantaras and Kung, 1996) may be defined as follows. Suppose we are given a training set of M pairs (x t , ct ) where the observed vectors xt ∈ Rn in an ndimensional input space X represent the context of the target word be"
P04-1081,W03-0429,0,0.0669449,"Missing"
P04-1081,W96-0208,0,0.0849957,"data. We also contrast against another type of kernel method, the support vector machine (SVM) model, and show that our KPCA-based model outperforms the SVM-based model. It is hoped that these highly encouraging first results on KPCA for natural language processing tasks will inspire further development of these directions. 1 Introduction Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al. (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)). A good foundation for comparative studies has been established by the Senseval data and evaluations; of particular relevance here are the lexical sample tasks from Senseval-1 (Kilgarriff and Rosenzweig, 1999) and Senseval-2 (Kilgarriff, 2001). We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly base"
P04-1081,W01-0507,0,0.0663878,"Missing"
P04-1081,S01-1040,0,\N,Missing
P04-1081,S01-1034,0,\N,Missing
P05-1048,P91-1034,0,0.15708,"atically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. This differs from our approach which consists of producing the translation of complete sentences, and not only of a predefined set of target words. Brown et al. (1991) proposed a WSD algorithm to disambiguate English translations of French target words based on the single most informative context feature. In a pilot study, they found that using this WSD method in their French-English SMT system helped translation quality, manually evaluated using the number of acceptable translations. However, this study is limited to the unrealistic case of words that have exactly two senses in the other language. Most previous work has focused on the distinct problem of exploiting various bilingual resources (e.g., parallel or comparable corpora, or even MT systems) to he"
P05-1048,W04-0822,1,0.870275,"n methodology and datasets from the Senseval-3 Chinese lexical sample task. We showed the accuracy of the SMT model to be significantly lower than that of all the dedicated WSD models considered, even after adding the lexical sample data to the training set for SMT to allow for a fair comparison. These results highlight the relative strength, and the potential hoped-for advantage of dedicated supervised WSD models. 3 The WSD system The WSD system used for the experiments is based on the model that achieved the best performance, by a large margin, on the Senseval-3 Chinese lexical sample task (Carpuat et al., 2004). 3.1 Classification model The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a naive Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (2002) found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance. (Note, however, that a different subset of either Senseval-1 or Senseva"
P05-1048,W02-1002,0,0.00768203,"ls. 3 The WSD system The WSD system used for the experiments is based on the model that achieved the best performance, by a large margin, on the Senseval-3 Chinese lexical sample task (Carpuat et al., 2004). 3.1 Classification model The model consists of an ensemble of four voting models combined by majority vote. The first voting model is a naive Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (2002) found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance. (Note, however, that a different subset of either Senseval-1 or Senseval-2 English lexical sample data was used for their comparison.) The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification (Carreras et al., 2002) . Specifically, an AdaBoost.MH model was used (Schapire and Singer, 2000), which is a multiclass generalization of the original boosting algorithm,"
P05-1048,P02-1044,0,0.0471594,"ace problems still to be outdated”). 0  6.6 BLEU score bias The “language model effect” highlights one of the potential weaknesses of the BLEU score. BLEU penalizes for phrasal incoherence, which in the present study means that it can sometimes sacrifice adequacy for fluency. However, the characteristics of BLEU are by no means solely responsible for the problems with WSD that we observed. To doublecheck that n-gram effects were not unduly impacting our study, we also evaluated using BLEU-1, which gave largely similar results as the standard BLEU-4 scores reported above. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. T"
P05-1048,P02-1038,0,0.0742986,"r all, WSD is a method of compensating for sparse data. Thus it may be that the present inability of WSD models to help improve accuracy of SMT systems stems not from an inherent weakness of dedicated WSD models, but rather from limitations of present-day SMT architectures. To further test this, our experiments could be tried on other statistical MT models. For example, the WSD model’s predictions could be employed in a Bracketing ITG translation model such as Wu (1996) or Zens et al. (2004), or alternatively they could be incorporated as features for reranking in a maximum-entropy SMT model (Och and Ney, 2002), instead of using them to constrain the sentence translation hypotheses as done here. However, the preceding discussion argues that it is doubtful that this would produce significantly different results, since the inherent problem from the “language model effect” would largely remain, causing sentence translations that include the WSD’s preferred lexical choices to be discounted. For similar reasons, we suspect our findings may also hold even for more sophisticated statistical MT models that rely heavily on n-gram language models. A more grammatically structured statistical MT model that less"
P05-1048,J03-1002,0,0.00663601,"e potential contribution of WSD should be easier to see against this baseline. Note that our focus here is not on the SMT model itself; our aim is to evaluate the impact of WSD on a real Chinese to English statistical machine translation task. 4 Table 1: Example of the translation candidates before and after mapping for the target word “ ” (lu) HowNet Sense ID HowNet glosses 56520 56521 56524 56525, 56526, 56527, 56528 56530, 56531, 56532 56533, 56534 distance sort Lu path, road, route, way line, means, sequence district, region 4.1 Alignment model The alignment model was trained with GIZA++ (Och and Ney, 2003), which implements the most typical IBM and HMM alignment models. Translation quality could be improved using more advanced hybrid phrasal or tree models, but this would interfere with the questions being investigated here. The alignment model used is IBM-4, as required by our decoder. The training scheme consists of IBM-1, HMM, IBM-3 and IBM-4, following (Och and Ney, 2003). The training corpus consists of about 1 million sentences from the United Nations Chinese-English parallel corpus from LDC. This corpus was automatically sentence-aligned, so the training data does not require as much man"
P05-1048,P02-1040,0,0.108677,"of WSD errors. However, we do not have a corpus which contains both sense annotation and multiple reference translations: the MT evaluation corpus is not annotated with the correct senses of Senseval target words, and the Senseval corpus does not include English translations of the sentences. 6 6.1 Results Even state-of-the-art WSD does not help BLEU score Table 2 summarizes the translation quality scores obtained with and without the WSD model. Using our WSD model to constrain the translation candidates given to the decoder hurts translation quality, as measured by the automated BLEU metric (Papineni et al., 2002). Note that we are evaluating on only difficult sentences containing the problematic target words from the lexical sample task, so BLEU scores can be expected to be on the low side. 391 BLEU score 0.1310 0.1253 0.1239 0.1232 WSD still does not help BLEU score with improved translation candidates One could argue that the translation candidates chosen by the WSD models do not help because they are only glosses obtained from the HowNet dictionary. They consist of the root form of words only, while the SMT model can learn many more translations for each target word, including inflected forms and s"
P05-1048,P98-2230,1,0.781111,"Missing"
P05-1048,P04-1081,1,0.714551,"nt subset of either Senseval-1 or Senseval-2 English lexical sample data was used for their comparison.) The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification (Carreras et al., 2002) . Specifically, an AdaBoost.MH model was used (Schapire and Singer, 2000), which is a multiclass generalization of the original boosting algorithm, with boosting on top of decision stump classifiers (i.e., decision trees of depth one). The fourth voting model is a Kernel PCA-based model (Wu et al., 2004). Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space. (Carpuat et al., 2004) showed that KPCAbased WSD models achieve cl"
P05-1048,W02-2004,0,0.0369807,"Missing"
P05-1048,P96-1021,1,0.666101,"xical choices, then perhaps some alternative model striking a different balance of adequacy and fluency is called for. Ultimately, after all, WSD is a method of compensating for sparse data. Thus it may be that the present inability of WSD models to help improve accuracy of SMT systems stems not from an inherent weakness of dedicated WSD models, but rather from limitations of present-day SMT architectures. To further test this, our experiments could be tried on other statistical MT models. For example, the WSD model’s predictions could be employed in a Bracketing ITG translation model such as Wu (1996) or Zens et al. (2004), or alternatively they could be incorporated as features for reranking in a maximum-entropy SMT model (Och and Ney, 2002), instead of using them to constrain the sentence translation hypotheses as done here. However, the preceding discussion argues that it is doubtful that this would produce significantly different results, since the inherent problem from the “language model effect” would largely remain, causing sentence translations that include the WSD’s preferred lexical choices to be discounted. For similar reasons, we suspect our findings may also hold even for more"
P05-1048,P04-1039,0,0.0269856,"ential weaknesses of the BLEU score. BLEU penalizes for phrasal incoherence, which in the present study means that it can sometimes sacrifice adequacy for fluency. However, the characteristics of BLEU are by no means solely responsible for the problems with WSD that we observed. To doublecheck that n-gram effects were not unduly impacting our study, we also evaluated using BLEU-1, which gave largely similar results as the standard BLEU-4 scores reported above. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. This differs from our approach which consists of producing the translation of complete sentences, and not only"
P05-1048,C04-1030,0,0.0525017,"l weaknesses of the BLEU score. BLEU penalizes for phrasal incoherence, which in the present study means that it can sometimes sacrifice adequacy for fluency. However, the characteristics of BLEU are by no means solely responsible for the problems with WSD that we observed. To doublecheck that n-gram effects were not unduly impacting our study, we also evaluated using BLEU-1, which gave largely similar results as the standard BLEU-4 scores reported above. Other work includes Li and Li (2002) who propose a bilingual bootstrapping method to learn a translation disambiguation WSD model, and Diab (2004) who exploited large amounts of automatically generated noisy parallel data to learn WSD models in an unsupervised bootstrapping scheme. 7 8 Related work Most translation disambiguation tasks are defined similarly to the Senseval Multilingual lexical sample tasks. In Senseval-3, the English to Hindi translation disambigation task was defined identically to the English lexical sample task, except that the WSD models are expected to predict Hindi translations instead of WordNet senses. This differs from our approach which consists of producing the translation of complete sentences, and not only"
P05-1048,J04-1001,0,\N,Missing
P05-1048,P03-1058,0,\N,Missing
P05-1048,C98-2225,1,\N,Missing
P05-1048,N03-1010,0,\N,Missing
P10-2033,2010.jeptalnrecital-long.30,1,0.424339,"Missing"
P10-2033,2009.mtsummit-papers.2,0,0.0112858,"le. We apply statistical significance tests to prune unreliable phrase-pairs Based on these analyses, we propose a new method to help phrase-based SMT systems deal with Arabic-English word order differences due to VS constructions. As in related work on syntactic reordering by preprocessing, our method attempts to make Arabic and English word order closer to each other by reordering Arabic VS constructions into SV. However, unlike in previous work, the reordered Arabic sentences are used only for word alignment. Phrase translation extraction and de180 and score remaining phrase-table entries (Chen et al., 2009). We use a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on the NIST MT06 test set. For all systems, the English data is tokenized using simple punctuation-based rules. The Arabic side is segmented according to the Arabic Treebank (PATB3) tokenization scheme (Maamouri et al., 2009) using the MADA+TOKAN morphological analyzer and tokenizer (Habash and Rambow, 2005). MADA-produced Arabic lemmas are used for word alignment. 6 Table 3: Evaluation on all test sets: on the total of 4432 test sentences, improvements are statistically significant"
P10-2033,P08-1009,0,0.0320597,"has been more successful on language pairs other than Arabic-English, perhaps due to more accurate parsers and less ambiguous reordering patterns than for Arabic VS. For instance, Collins et al. (2005) apply six manually defined transformations to German parse trees which improve German-English translation by 0.4 BLEU on the Europarl task. Xia and McCord (2004) learn reordering rules for French to English translations, which arguably presents less syntactic distortion than Arabic-English. Zhang et al. (2007) limit reordering to decoding for Chinese-English SMT using a lattice representation. Cherry (2008) uses dependency parses as cohesion constraints in decoding for French-English SMT. For Arabic-English phrase-based SMT, the impact of syntactic reordering as preprocessing is less clear. Habash (2007) proposes to learn syntactic reordering rules targeting Arabic-English word order differences and integrates them as deterministic preprocessing. He reports improvements in BLEU compared to phrase-based SMT limited to monotonic decoding, but these improvements do not hold with distortion. Instead of applying reordering rules deterministically, Crego and Habash (2008) use a lattice input to repres"
P10-2033,P05-1066,0,0.241763,"parallel treebank. Our analysis shows that VS reordering rules are not straightforward and that SMT should therefore benefit from direct modeling of Arabic verb subject translation. In order to detect VS constructions, we use our state-of-the-art Arabic dependency parser, which is essentially the CATIB E X baseline in our subsequent parsing work in Marton et al. (2010), and is further described there. We show that VS subjects and their exact boundaries are hard to identify accurately. Given the noise in VS detection, existing strategies for source-side reordering (e.g., Xia and McCord (2004), Collins et al. (2005), Wang et al. (2007)) or using deWe study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy"
P10-2033,W08-0307,1,0.945077,"es, even on a strong large-scale baseline and despite noisy parses. 1 Introduction Modern Standard Arabic (MSA) is a morphosyntactically complex language, with different phenomena from English, a fact that raises many interesting issues for natural language processing and Arabic-to-English statistical machine translation (SMT). While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sadat and Habash (2006), Zollmann et al. (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al. (2009), Crego and Habash (2008), Habash (2007)). Arabic verbal constructions are particularly challenging since subjects can occur in pre-verbal (SV), post-verbal (VS) or pro-dropped (“null subject”) constructions. As a result, training data for learning verbal construction translations is split between the different constructions and their patterns; and complex reordering schemas are needed in order to translate them into primarily 1 http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ResultsRelease/ currentArabic.html 178 Proceedings of the ACL 2010 Conference Short Papers, pages 178–183, c Uppsala, Sweden, 11-16 July 2010. 20"
P10-2033,W08-0306,0,0.0382337,"Missing"
P10-2033,W03-3017,0,0.0495428,"Missing"
P10-2033,P05-1071,1,0.216532,"Missing"
P10-2033,J08-4003,0,0.0138759,"Missing"
P10-2033,J03-1002,0,0.00389424,"5 65.75 74.23 83.31 65.50 • medium-scale the bitext consists of 12M words on the Arabic side (LDC2007E103). The language model is trained on the English side of the large bitext. • large-scale the bitext consists of several newswire LDC corpora, and has 64M words on the Arabic side. The language model is trained on the English side of the bitext augmented with Gigaword data. Reordering Arabic VS for SMT word alignment Except from this difference in training data, the two systems are identical. They use a standard phrase-based architecture. The parallel corpus is word-aligned using the GIZA++ (Och and Ney, 2003), which sequentially learns word alignments for the IBM1, HMM, IBM3 and IBM4 models. The resulting alignments in both translation directions are intersected and augmented using the grow-diag-final-and heuristic (Koehn et al., 2007). Phrase translations of up to 10 words are extracted in the Moses phrase-table. We apply statistical significance tests to prune unreliable phrase-pairs Based on these analyses, we propose a new method to help phrase-based SMT systems deal with Arabic-English word order differences due to VS constructions. As in related work on syntactic reordering by preprocessing,"
P10-2033,P09-2056,1,0.562753,"Missing"
P10-2033,P02-1040,0,0.108224,"Missing"
P10-2033,2007.mtsummit-papers.29,1,0.951896,"e-scale baseline and despite noisy parses. 1 Introduction Modern Standard Arabic (MSA) is a morphosyntactically complex language, with different phenomena from English, a fact that raises many interesting issues for natural language processing and Arabic-to-English statistical machine translation (SMT). While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sadat and Habash (2006), Zollmann et al. (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al. (2009), Crego and Habash (2008), Habash (2007)). Arabic verbal constructions are particularly challenging since subjects can occur in pre-verbal (SV), post-verbal (VS) or pro-dropped (“null subject”) constructions. As a result, training data for learning verbal construction translations is split between the different constructions and their patterns; and complex reordering schemas are needed in order to translate them into primarily 1 http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ResultsRelease/ currentArabic.html 178 Proceedings of the ACL 2010 Conference Short Papers, pages 178–183, c Uppsala, Sweden, 11-16 July 2010. 2010 Association"
P10-2033,P06-1001,1,0.829926,"ser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses. 1 Introduction Modern Standard Arabic (MSA) is a morphosyntactically complex language, with different phenomena from English, a fact that raises many interesting issues for natural language processing and Arabic-to-English statistical machine translation (SMT). While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sadat and Habash (2006), Zollmann et al. (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al. (2009), Crego and Habash (2008), Habash (2007)). Arabic verbal constructions are particularly challenging since subjects can occur in pre-verbal (SV), post-verbal (VS) or pro-dropped (“null subject”) constructions. As a result, training data for learning verbal construction translations is split between the different constructions and their patterns; and complex reordering schemas are needed in order to translate them into primarily 1 http://www.itl.nist.gov/iad/ mig/tests/mt"
P10-2033,D09-1024,0,0.023611,"Missing"
P10-2033,N03-1017,0,0.005535,"Missing"
P10-2033,2006.amta-papers.25,0,0.0496745,"Missing"
P10-2033,D07-1077,0,0.028413,"analysis shows that VS reordering rules are not straightforward and that SMT should therefore benefit from direct modeling of Arabic verb subject translation. In order to detect VS constructions, we use our state-of-the-art Arabic dependency parser, which is essentially the CATIB E X baseline in our subsequent parsing work in Marton et al. (2010), and is further described there. We show that VS subjects and their exact boundaries are hard to identify accurately. Given the noise in VS detection, existing strategies for source-side reordering (e.g., Xia and McCord (2004), Collins et al. (2005), Wang et al. (2007)) or using deWe study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improv"
P10-2033,C04-1073,0,0.253143,"aligned Arabic-English parallel treebank. Our analysis shows that VS reordering rules are not straightforward and that SMT should therefore benefit from direct modeling of Arabic verb subject translation. In order to detect VS constructions, we use our state-of-the-art Arabic dependency parser, which is essentially the CATIB E X baseline in our subsequent parsing work in Marton et al. (2010), and is further described there. We show that VS subjects and their exact boundaries are hard to identify accurately. Given the noise in VS detection, existing strategies for source-side reordering (e.g., Xia and McCord (2004), Collins et al. (2005), Wang et al. (2007)) or using deWe study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignme"
P10-2033,W04-3250,0,0.0118151,"Missing"
P10-2033,W07-0401,0,0.048109,"translations that do not break subject boundaries. Syntactically motivated reordering for phrasebased SMT has been more successful on language pairs other than Arabic-English, perhaps due to more accurate parsers and less ambiguous reordering patterns than for Arabic VS. For instance, Collins et al. (2005) apply six manually defined transformations to German parse trees which improve German-English translation by 0.4 BLEU on the Europarl task. Xia and McCord (2004) learn reordering rules for French to English translations, which arguably presents less syntactic distortion than Arabic-English. Zhang et al. (2007) limit reordering to decoding for Chinese-English SMT using a lattice representation. Cherry (2008) uses dependency parses as cohesion constraints in decoding for French-English SMT. For Arabic-English phrase-based SMT, the impact of syntactic reordering as preprocessing is less clear. Habash (2007) proposes to learn syntactic reordering rules targeting Arabic-English word order differences and integrates them as deterministic preprocessing. He reports improvements in BLEU compared to phrase-based SMT limited to monotonic decoding, but these improvements do not hold with distortion. Instead of"
P10-2033,N04-4015,0,0.0894678,"ions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses. 1 Introduction Modern Standard Arabic (MSA) is a morphosyntactically complex language, with different phenomena from English, a fact that raises many interesting issues for natural language processing and Arabic-to-English statistical machine translation (SMT). While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sadat and Habash (2006), Zollmann et al. (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al. (2009), Crego and Habash (2008), Habash (2007)). Arabic verbal constructions are particularly challenging since subjects can occur in pre-verbal (SV), post-verbal (VS) or pro-dropped (“null subject”) constructions. As a result, training data for learning verbal construction translations is split between the different constructions and their patterns; and complex reordering schemas are needed in order to translate them into primarily 1 http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ResultsRelease/ currentArabic."
P10-2033,maamouri-etal-2008-enhancing,0,0.0848332,"Missing"
P10-2033,P06-1073,0,0.0210988,"Missing"
P10-2033,N06-2051,0,0.0191457,"to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses. 1 Introduction Modern Standard Arabic (MSA) is a morphosyntactically complex language, with different phenomena from English, a fact that raises many interesting issues for natural language processing and Arabic-to-English statistical machine translation (SMT). While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sadat and Habash (2006), Zollmann et al. (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al. (2009), Crego and Habash (2008), Habash (2007)). Arabic verbal constructions are particularly challenging since subjects can occur in pre-verbal (SV), post-verbal (VS) or pro-dropped (“null subject”) constructions. As a result, training data for learning verbal construction translations is split between the different constructions and their patterns; and complex reordering schemas are needed in order to translate them into primarily 1 http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ResultsRelease/ cu"
P10-2033,W10-1402,1,0.41613,"modeling should further improve SMT. We attempt to get a better understanding of translation patterns for Arabic verb constructions, particularly VS constructions, by studying their occurrence and reordering patterns in a handaligned Arabic-English parallel treebank. Our analysis shows that VS reordering rules are not straightforward and that SMT should therefore benefit from direct modeling of Arabic verb subject translation. In order to detect VS constructions, we use our state-of-the-art Arabic dependency parser, which is essentially the CATIB E X baseline in our subsequent parsing work in Marton et al. (2010), and is further described there. We show that VS subjects and their exact boundaries are hard to identify accurately. Given the noise in VS detection, existing strategies for source-side reordering (e.g., Xia and McCord (2004), Collins et al. (2005), Wang et al. (2007)) or using deWe study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reorderin"
P10-2033,nivre-etal-2006-maltparser,0,0.0108731,"Missing"
P10-2033,D07-1103,0,\N,Missing
P10-2033,W11-2127,1,\N,Missing
P10-2033,J04-4004,0,\N,Missing
P10-2033,J93-2003,0,\N,Missing
P10-2033,C96-2141,0,\N,Missing
P10-2033,N09-2001,0,\N,Missing
P10-2033,W10-1735,0,\N,Missing
P10-2033,P07-2045,0,\N,Missing
P10-2033,P08-2030,1,\N,Missing
P10-2033,N06-2013,1,\N,Missing
P10-2033,P11-2067,0,\N,Missing
P10-2033,C10-1045,0,\N,Missing
P10-2033,P08-1114,1,\N,Missing
P10-2033,2006.amta-papers.11,0,\N,Missing
P13-1141,P10-1088,0,0.0107299,"esults are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used in a new sense?” Note that for both the dictionary mining setting and the active learning setting, it is important to consider words in"
P13-1141,D07-1007,1,0.182543,"ntific domain, the word gains a new sense: “ratio”, which simply does not exist in the parliament domain. In a science domain, the “report” sense exists, but it is dominated about 12:1 by “ratio.” In a medical domain, the “report” sense remains dominant (about 2:1), but the new “ratio” sense appears frequently. In this paper we define a new task that we call S ENSE S POTTING. The goal of this task is to identify words in a new domain monolingual text that appeared in old domain text but which have a new, previously unseen sense1 . We operate under the framework of phrase sense disambiguation (Carpuat and Wu, 2007), in which we take automatically align parallel data in an old domain to generate an initial old-domain sense inventory. This sense inventory provides the set of “known” word senses in the form of phrasal translations. Concrete examples are shown in Table 1. One of our key contributions is the development of a rich set of features based on monolingual text that are indicative of new word senses. This work is driven by an application need. When machine translation (MT) systems are applied in a new domain, many errors are a result of: (1) previously unseen (OOV) source language words, or (2) sou"
P13-1141,P07-1007,0,0.0197521,"ethods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine translation has focused o"
P13-1141,cook-stevenson-2010-automatically,0,0.0531426,"context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct S ENSE S POTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in S ENSE S POTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent langua"
P13-1141,P11-2071,1,0.451453,"Missing"
P13-1141,N06-1017,0,0.0974335,"induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine trans"
P13-1141,P98-1069,0,0.225714,"ly unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (toke"
P13-1141,W11-2508,0,0.0205318,"sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct S ENSE S POTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in S ENSE S POTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) u"
P13-1141,P08-1088,0,0.00478932,"ords that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used in a new sense?” Note"
P13-1141,P07-2045,0,0.0035526,"stributions. SpreadNorm is the ratio of spread of the prior and posterior distributions, where spared is the difference between maximum and minimum probabilities of the distribution as defined earlier. ConfusionNorm is the ratio of confusion of the prior and posterior distributions, where confusion is defined as earlier. 5 Data and Gold Standard The first component of our task is a parallel corpus of old domain data, for which we use the French-English Hansard parliamentary proceedings (http://www.parl.gc.ca). From this, we extract an old domain sense dictionary, using the Moses MT framework (Koehn et al., 2007). This defines our old domain sense dictionary. For new domains, we use three sources: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts, and (3) a corpus of translated movie subtitles (Tiedemann, 2009). Basic statistics are shown in Table 2. In all parallel corpora, we normalize the English for American spelling. To create the gold standard truth, we followed a lexical sample apparoach and collected a set of 300 “representative types” that are interesting to evaluate on, because they have multiple senses within a single domain or whose senses are likely to ch"
P13-1141,E12-1060,0,0.012705,"ot received as much attention, and is typically addressed within word sense induction, rather than as a distinct S ENSE S POTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in S ENSE S POTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) use parallel Latin-English data to learn to disambiguate Latin words into English senses. New English translations are used as evidence that Latin words have shifted sense. In contrast, the S"
P13-1141,P04-1036,0,0.0855446,"re unknown in parallel data. Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on"
P13-1141,J07-4005,0,0.1011,"data. Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for ma"
P13-1141,W07-0737,0,0.0115962,"Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine translation has focused on collecting translations for longer unknown segments (e.g., Bloodgood and CallisonBurch (2010)). There has been some interest in detecting which phrases that are hard to translate for a given system (Mohit and Hwa, 2007), but difficulties can arise for many reasons: S ENSE S POTTING focuses on a single problem. probabilities as well as their difference. These are our Type:RelFreq features. 4 Topic Model Feature The intuition behind the topic model feature is that if a word’s distribution over topics changes when moving into a new domain, it is likely to also gain a new sense. For example, suppose that in our old domain, the French word enceinte is only used with the sense “wall,” but in our new domain, enceinte may have senses corresponding to either “wall” or to “pregnant.” We would expect to see this reflec"
P13-1141,P95-1050,0,0.25233,"(1) previously unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is t"
P13-1141,W09-0214,0,0.0937541,"Missing"
P13-1141,W02-2026,0,0.0401843,"ce language words, or (2) source language words that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known"
P13-1141,S07-1002,0,\N,Missing
P13-1141,C98-1066,0,\N,Missing
P14-2047,W13-3306,0,0.135469,"Missing"
P14-2047,D07-1007,1,0.794195,"ces, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation qu"
P14-2047,W13-3303,0,0.10424,"very long sentences which at times require the use of multiple English sentences to express the same content and preserve grammaticality. Similarly discourse connectives like because, but, since and while often relate information expressed in simple sentential clauses. There are a number of possible complications in translating these connectives: they may be ambiguous between possible senses, e.g., English while is ambiguous between COMPARISON and TEMPORAL; explicit discourse connectives may be translated into implicit discourse relations or translated in morphology rather than lexical items (Meyer and Webber, 2013; Meyer and Pol´akov´a, 2013). In our work, we quantify the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004;"
P14-2047,P07-1005,0,0.0458452,"uality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation quality than using tra"
P14-2047,P07-1017,0,0.0288128,"y the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited re"
P14-2047,C96-2183,0,0.0371132,"Missing"
P14-2047,P09-2004,1,0.922921,"Missing"
P14-2047,P05-1066,0,0.0606216,"Missing"
P14-2047,prasad-etal-2008-penn,0,0.490232,"Missing"
P14-2047,N04-1035,0,0.0469702,"eyer and Webber, 2013; Meyer and Pol´akov´a, 2013). In our work, we quantify the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English tra"
P14-2047,2006.amta-papers.25,0,0.0432856,"tems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation quality than using translated references, especially at the segment level. The data for the analysis is drawn from an extended set of newswire reports in the 2008/2010 NIST Metrics for Machine Translation 283 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 283–288, c Baltimor"
P14-2047,N06-2013,0,0.0340844,"tween information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides m"
P14-2047,P11-2111,0,0.188979,"Missing"
P14-2047,W04-1101,0,0.249046,"Missing"
P14-2047,J93-2004,0,\N,Missing
P16-2059,S12-1051,0,0.0219808,"pirical comparisons with the Wieting et al. (2016) embeddings, and also define a simplified version of that objective, Jpa , to allow for controlled comparisons with Jbi . Jpa uses random initialization and penalizes large values in W with a ||W ||2F regularization term1 . The choice of distance function (Euclidean distance or cosine similarity) and of the negative sampling strategy2 are viewed as tunable hyperparameters. 3 3.1 Experiments Evaluating Sentence Representations Following Wieting et al. (2016), the models above are evaluated on the four Semantic Textual Similarity (STS) datasets (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which provide pairs of English sentences from different domains (e.g., Tweets, news, webforums, image captions), annotated with human judgments of similarity on a 1 to 5 scale. Systems have to output a similarity score for each pair. Systems are evaluated using the Pearson correlation between gold and predicted rankings. The Sentences Involving Compositional Knowledge (SICK) test set (Marelli et al., 2014) provides a complementary evaluation. It consists of sentence pairs annotated with semantic relatedness scores. While STS exa"
P16-2059,S13-1004,0,0.0162964,"ith the Wieting et al. (2016) embeddings, and also define a simplified version of that objective, Jpa , to allow for controlled comparisons with Jbi . Jpa uses random initialization and penalizes large values in W with a ||W ||2F regularization term1 . The choice of distance function (Euclidean distance or cosine similarity) and of the negative sampling strategy2 are viewed as tunable hyperparameters. 3 3.1 Experiments Evaluating Sentence Representations Following Wieting et al. (2016), the models above are evaluated on the four Semantic Textual Similarity (STS) datasets (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which provide pairs of English sentences from different domains (e.g., Tweets, news, webforums, image captions), annotated with human judgments of similarity on a 1 to 5 scale. Systems have to output a similarity score for each pair. Systems are evaluated using the Pearson correlation between gold and predicted rankings. The Sentences Involving Compositional Knowledge (SICK) test set (Marelli et al., 2014) provides a complementary evaluation. It consists of sentence pairs annotated with semantic relatedness scores. While STS examples were simply dra"
P16-2059,S14-2010,0,0.0177493,". (2016) embeddings, and also define a simplified version of that objective, Jpa , to allow for controlled comparisons with Jbi . Jpa uses random initialization and penalizes large values in W with a ||W ||2F regularization term1 . The choice of distance function (Euclidean distance or cosine similarity) and of the negative sampling strategy2 are viewed as tunable hyperparameters. 3 3.1 Experiments Evaluating Sentence Representations Following Wieting et al. (2016), the models above are evaluated on the four Semantic Textual Similarity (STS) datasets (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which provide pairs of English sentences from different domains (e.g., Tweets, news, webforums, image captions), annotated with human judgments of similarity on a 1 to 5 scale. Systems have to output a similarity score for each pair. Systems are evaluated using the Pearson correlation between gold and predicted rankings. The Sentences Involving Compositional Knowledge (SICK) test set (Marelli et al., 2014) provides a complementary evaluation. It consists of sentence pairs annotated with semantic relatedness scores. While STS examples were simply drawn from existing NLP"
P16-2059,S15-2045,0,0.0121994,"and also define a simplified version of that objective, Jpa , to allow for controlled comparisons with Jbi . Jpa uses random initialization and penalizes large values in W with a ||W ||2F regularization term1 . The choice of distance function (Euclidean distance or cosine similarity) and of the negative sampling strategy2 are viewed as tunable hyperparameters. 3 3.1 Experiments Evaluating Sentence Representations Following Wieting et al. (2016), the models above are evaluated on the four Semantic Textual Similarity (STS) datasets (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which provide pairs of English sentences from different domains (e.g., Tweets, news, webforums, image captions), annotated with human judgments of similarity on a 1 to 5 scale. Systems have to output a similarity score for each pair. Systems are evaluated using the Pearson correlation between gold and predicted rankings. The Sentences Involving Compositional Knowledge (SICK) test set (Marelli et al., 2014) provides a complementary evaluation. It consists of sentence pairs annotated with semantic relatedness scores. While STS examples were simply drawn from existing NLP datasets, SICK example"
P16-2059,D10-1115,0,0.0439569,"methods for distributional word representations (Baroni et al., 2014) has brought renewed interest to the question of how to compose semantic representations of words to capture the semantics of phrases and sentences. These representations offer the promise of capturing phrasal or sentential semantics in a general fashion, and could in principle benefit any NLP applications that analyze text beyond the word level, and improve their ability to generalize beyond contexts seen in training. While most prior work has focused either on composing words into short phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Hermann et al., 2012; Fyshe et al., 2015), or on supervised task-specific composition functions (Socher et al., 2013; Iyyer et al., 2015; Rockt¨aschel et al., 2015; Iyyer et al., 2014; Tai et al., 2015, inter alia), Wieting et al. (2016) recently showed that a simple composition architecture (vector averaging) can yield sentence models that consistently perform well in semantic textual similarity tasks in a wide range of domains, and outperform more complex sequence models (Tai et al., 2015). Interestingly, these models are trained using PPDB, the paraphrase database (Ganitkevitch et al., 20"
P16-2059,P14-1023,0,0.0208226,"al document categorization or machine translation. In this work, we evaluate the quality of the monolingual representations learned with a variant of the bilingual compositional model of Hermann and Blunsom (2014), when viewing translations in a second language as a semantic annotation as the original language text. We show that compositional objectives based on phrase translation pairs outperform compositional objectives based on bilingual sentences and on monolingual paraphrases. 1 Introduction The effectiveness of new representation learning methods for distributional word representations (Baroni et al., 2014) has brought renewed interest to the question of how to compose semantic representations of words to capture the semantics of phrases and sentences. These representations offer the promise of capturing phrasal or sentential semantics in a general fashion, and could in principle benefit any NLP applications that analyze text beyond the word level, and improve their ability to generalize beyond contexts seen in training. While most prior work has focused either on composing words into short phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Hermann et al., 2012; Fyshe et al., 2015)"
P16-2059,D07-1007,1,0.786708,"Missing"
P16-2059,P04-1039,0,0.0834091,"Missing"
P16-2059,N15-1004,0,0.023076,"aroni et al., 2014) has brought renewed interest to the question of how to compose semantic representations of words to capture the semantics of phrases and sentences. These representations offer the promise of capturing phrasal or sentential semantics in a general fashion, and could in principle benefit any NLP applications that analyze text beyond the word level, and improve their ability to generalize beyond contexts seen in training. While most prior work has focused either on composing words into short phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Hermann et al., 2012; Fyshe et al., 2015), or on supervised task-specific composition functions (Socher et al., 2013; Iyyer et al., 2015; Rockt¨aschel et al., 2015; Iyyer et al., 2014; Tai et al., 2015, inter alia), Wieting et al. (2016) recently showed that a simple composition architecture (vector averaging) can yield sentence models that consistently perform well in semantic textual similarity tasks in a wide range of domains, and outperform more complex sequence models (Tai et al., 2015). Interestingly, these models are trained using PPDB, the paraphrase database (Ganitkevitch et al., 2013), which was learned from bilingual paral"
P16-2059,N13-1092,0,0.0943482,"Missing"
P16-2059,P14-1006,0,0.374192,"l., 2015; Iyyer et al., 2014; Tai et al., 2015, inter alia), Wieting et al. (2016) recently showed that a simple composition architecture (vector averaging) can yield sentence models that consistently perform well in semantic textual similarity tasks in a wide range of domains, and outperform more complex sequence models (Tai et al., 2015). Interestingly, these models are trained using PPDB, the paraphrase database (Ganitkevitch et al., 2013), which was learned from bilingual parallel corpora. In bilingual settings, there are also a few examples of bilingual sentence models (Zou et al., 2013; Hermann and Blunsom, 2014; Lauly et al., 2014; Gouws et al., 2014). However, they have only been evaluated in cross-lingual transfer settings (e.g., cross-lingual document classification, or machine translation), which do not directly evaluate the quality of the sentence-level semantic representations learned. In this work, we directly evaluate the usefulness of modeling semantic equivalence using compositional models of translated texts for detecting semantic textual similarity in a single language. For instance, in addition to using translated texts to model cross-lingual transfer from English to a foreign language,"
P16-2059,S12-1021,0,0.0250932,"ord representations (Baroni et al., 2014) has brought renewed interest to the question of how to compose semantic representations of words to capture the semantics of phrases and sentences. These representations offer the promise of capturing phrasal or sentential semantics in a general fashion, and could in principle benefit any NLP applications that analyze text beyond the word level, and improve their ability to generalize beyond contexts seen in training. While most prior work has focused either on composing words into short phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Hermann et al., 2012; Fyshe et al., 2015), or on supervised task-specific composition functions (Socher et al., 2013; Iyyer et al., 2015; Rockt¨aschel et al., 2015; Iyyer et al., 2014; Tai et al., 2015, inter alia), Wieting et al. (2016) recently showed that a simple composition architecture (vector averaging) can yield sentence models that consistently perform well in semantic textual similarity tasks in a wide range of domains, and outperform more complex sequence models (Tai et al., 2015). Interestingly, these models are trained using PPDB, the paraphrase database (Ganitkevitch et al., 2013), which was learned"
P16-2059,D14-1070,0,0.0676059,"Missing"
P16-2059,P15-1162,0,0.102347,"Missing"
P16-2059,P07-2045,0,0.00440849,"Missing"
P16-2059,2005.mtsummit-papers.11,0,0.139653,"Missing"
P16-2059,S14-2001,0,0.0278645,"ations Following Wieting et al. (2016), the models above are evaluated on the four Semantic Textual Similarity (STS) datasets (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which provide pairs of English sentences from different domains (e.g., Tweets, news, webforums, image captions), annotated with human judgments of similarity on a 1 to 5 scale. Systems have to output a similarity score for each pair. Systems are evaluated using the Pearson correlation between gold and predicted rankings. The Sentences Involving Compositional Knowledge (SICK) test set (Marelli et al., 2014) provides a complementary evaluation. It consists of sentence pairs annotated with semantic relatedness scores. While STS examples were simply drawn from existing NLP datasets, SICK examples were constructed to avoid non-compositional phenomena such as multiword expressions, named entities and world knowledge. 3.2 Experimental Conditions At training time we learn word embeddings for each combination of objective (Section 2.2) and 1 In contrast, Wieting et al. (2016) initialize W with highquality but resource intensive embeddings – they are trained using word-level PPDB paraphrases, tuned on Si"
P16-2059,D14-1162,0,0.0777326,"th semantic relatedness scores. While STS examples were simply drawn from existing NLP datasets, SICK examples were constructed to avoid non-compositional phenomena such as multiword expressions, named entities and world knowledge. 3.2 Experimental Conditions At training time we learn word embeddings for each combination of objective (Section 2.2) and 1 In contrast, Wieting et al. (2016) initialize W with highquality but resource intensive embeddings – they are trained using word-level PPDB paraphrases, tuned on SimLex-999, and regularized to penalize deviations from initial GloVe embeddings (Pennington et al., 2014). 2 MAX (use the unaligned phrase of minimum distance) or MIX (use MAX with probability 0.5 and sample randomly otherwise) 364 type of training examples (Table 2), using modified implementations of open-source implementations for Jbi (Hermann and Blunsom, 2014) and Jpa (Wieting et al., 2016). This results in six model configurations. Each was trained for 10 epochs using tuned hyperparameters. At tuning time we use the SMT-europarl subset of STS-2012. We consider mini-batch sizes of {25, 50, 100}, δ ∈ {1, 10, 100} with Euclidean distance, δ ∈ {0.4, 0.6, 0.8} with cosine similarly, and λ ∈ {1, 1"
P16-2059,D13-1170,0,0.00494312,"compose semantic representations of words to capture the semantics of phrases and sentences. These representations offer the promise of capturing phrasal or sentential semantics in a general fashion, and could in principle benefit any NLP applications that analyze text beyond the word level, and improve their ability to generalize beyond contexts seen in training. While most prior work has focused either on composing words into short phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Hermann et al., 2012; Fyshe et al., 2015), or on supervised task-specific composition functions (Socher et al., 2013; Iyyer et al., 2015; Rockt¨aschel et al., 2015; Iyyer et al., 2014; Tai et al., 2015, inter alia), Wieting et al. (2016) recently showed that a simple composition architecture (vector averaging) can yield sentence models that consistently perform well in semantic textual similarity tasks in a wide range of domains, and outperform more complex sequence models (Tai et al., 2015). Interestingly, these models are trained using PPDB, the paraphrase database (Ganitkevitch et al., 2013), which was learned from bilingual parallel corpora. In bilingual settings, there are also a few examples of biling"
P16-2059,P15-1150,0,0.0538548,"ces. These representations offer the promise of capturing phrasal or sentential semantics in a general fashion, and could in principle benefit any NLP applications that analyze text beyond the word level, and improve their ability to generalize beyond contexts seen in training. While most prior work has focused either on composing words into short phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Hermann et al., 2012; Fyshe et al., 2015), or on supervised task-specific composition functions (Socher et al., 2013; Iyyer et al., 2015; Rockt¨aschel et al., 2015; Iyyer et al., 2014; Tai et al., 2015, inter alia), Wieting et al. (2016) recently showed that a simple composition architecture (vector averaging) can yield sentence models that consistently perform well in semantic textual similarity tasks in a wide range of domains, and outperform more complex sequence models (Tai et al., 2015). Interestingly, these models are trained using PPDB, the paraphrase database (Ganitkevitch et al., 2013), which was learned from bilingual parallel corpora. In bilingual settings, there are also a few examples of bilingual sentence models (Zou et al., 2013; Hermann and Blunsom, 2014; Lauly et al., 2014;"
P16-2059,N12-1078,0,0.0698508,"Missing"
P16-2059,D13-1141,0,0.0427588,"Rockt¨aschel et al., 2015; Iyyer et al., 2014; Tai et al., 2015, inter alia), Wieting et al. (2016) recently showed that a simple composition architecture (vector averaging) can yield sentence models that consistently perform well in semantic textual similarity tasks in a wide range of domains, and outperform more complex sequence models (Tai et al., 2015). Interestingly, these models are trained using PPDB, the paraphrase database (Ganitkevitch et al., 2013), which was learned from bilingual parallel corpora. In bilingual settings, there are also a few examples of bilingual sentence models (Zou et al., 2013; Hermann and Blunsom, 2014; Lauly et al., 2014; Gouws et al., 2014). However, they have only been evaluated in cross-lingual transfer settings (e.g., cross-lingual document classification, or machine translation), which do not directly evaluate the quality of the sentence-level semantic representations learned. In this work, we directly evaluate the usefulness of modeling semantic equivalence using compositional models of translated texts for detecting semantic textual similarity in a single language. For instance, in addition to using translated texts to model cross-lingual transfer from Eng"
Q13-1035,P11-2078,0,0.0164661,"retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language"
Q13-1035,D11-1033,0,0.0547371,"tputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fin"
Q13-1035,P11-1022,0,0.0233897,"the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We can then check whether the MT output has the same set of English w"
Q13-1035,C12-1010,0,0.0640096,"Missing"
Q13-1035,2011.iwslt-evaluation.18,0,0.0835001,"mately as much error as unseen words, suggesting a novel avenue for research in sense induction. Unfortunately, it appears that choosing the right sense for these at translation time is even more difficult than in the unseen word case. 4. The story is more complicated for seen words with known translations: if we limit ourselves to “high To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some of our conclusions ar"
Q13-1035,C04-1046,0,0.0353001,"ed. F IGURE 1: Example of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We c"
Q13-1035,D07-1007,1,0.617467,"le of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We can then check whether"
Q13-1035,P13-1141,1,0.049126,"its well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributions from comparable new domain corpora (Irvine et al., 2013). Earlier, Daum´e III and Jagarlamudi (2011) showed how mining translations for unseen words from comparable corpora can im431 prove SMT in a new domain. 4 The S4 Taxonomy We begin with a simple question: when we move an SMT system from an old domain to a new domain, what goes wrong ? We employ a set of four error types as our taxonomy. We refer to these error types as SEEN, SENSE, SCORE and SEARCH, and together as the S4 taxonomy: SEEN : an attempt to translate a sour"
Q13-1035,D11-1003,0,0.0173755,"Missing"
Q13-1035,N12-1047,0,0.0250817,"iments. Each system scores translation candidates using standard features: 5 phrase-table features, including phrasal translation probabilities and lexical weights in both translation directions, and a constant phrase penalty; 6 lexicalized reordering features, including bidirectional models built for monotone, swap, discontinuous reorderings; 1 distance-based reordering feature; and 2 language models, a 5-gram model learned on the OLD domain, and a 5-gram model learned on the NEW domain. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm (Cherry and Foster, 2012). This results in a strong large-scale OLD system, which performs well on the old domain and is a good starting point for studying domain shifts. 5 The word alignments, 4. Hansards, News and the Canadian Science Publishing are available, respectively, at: http://www.parl.gc.ca, http: //www.statmt.org/wmt09/translation-task.html, and http://www.nrcresearchpress.com, preprocessed versions and data splits used in this paper can be downloaded from http://hal3.name/damt. 5. We use (unadapted) HMM word alignments (Vogel et 434 language models and tuning sets are kept constant across all experiments"
Q13-1035,J07-2003,0,0.0483847,"ion approach might achieve the best of both worlds. 8 Limiting Assumptions This paper represents a partial exploration of the space of possible assumptions about models and data. We cannot hope to explore the combinatorial explosion of possibilities, and therefore have restricted our analysis to the following settings: Phrase-based models. All of our experiments are carried out using phrase-based translation, as implemented in the open-source Moses translation system (Koehn et al., 2007) to ensure that they are reproducible. Our methods are easily extended to hierarchical phrase-based models (Chiang, 2007). It is not clear whether the same conclusions would hold: on the one hand, complex phrasal rules might overfit even more badly than phrases; on the other hand, hierarchical models might have more flexibility to generalize structures. Translation languages. We only translate from French to English. This well-studied language pair presents several advantages; large quantities of data are publicly available in a wide variety of domains, and standard statistical machine translation architectures yield good performance. Unlike with more distant languages such as Chinese-English, or languages with"
Q13-1035,P11-2071,1,0.879591,"Missing"
Q13-1035,2010.iwslt-papers.5,0,0.444775,"ccount for approximately as much error as unseen words, suggesting a novel avenue for research in sense induction. Unfortunately, it appears that choosing the right sense for these at translation time is even more difficult than in the unseen word case. 4. The story is more complicated for seen words with known translations: if we limit ourselves to “high To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some"
Q13-1035,W07-0717,0,0.799178,"nguage model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an"
Q13-1035,D10-1044,0,0.231096,"is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been use"
Q13-1035,D11-1084,0,0.0142283,"r to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pai"
Q13-1035,W12-3154,0,0.289779,"s unseen words, suggesting a novel avenue for research in sense induction. Unfortunately, it appears that choosing the right sense for these at translation time is even more difficult than in the unseen word case. 4. The story is more complicated for seen words with known translations: if we limit ourselves to “high To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some of our conclusions are consistent with previou"
Q13-1035,2005.eamt-1.19,0,0.0209944,"refer to domains and source/target to refer to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010)"
Q13-1035,D13-1109,1,0.875633,"Missing"
Q13-1035,W07-0733,0,0.584855,"lier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributio"
Q13-1035,2005.iwslt-1.8,0,0.15512,"Missing"
Q13-1035,P07-2045,0,0.00379976,"of informal noisy text. 4 In this study, we use the Hansard domain as the domain, and we consider four possible NEW domains: EMEA, News, Science and Subs. Data sets for all domains were processed consistently. After tokenization, we paid particular attention to normalization in order to minimize artificial differences when combining data, such as American, British and Canadian spellings. This proved particularly important for the news domain; the impact of SEEN reduced by more than half after normalization. OLD 6.2 MT systems We build standard phrase-based SMT systems using the Moses toolkit (Koehn et al., 2007) for all experiments. Each system scores translation candidates using standard features: 5 phrase-table features, including phrasal translation probabilities and lexical weights in both translation directions, and a constant phrase penalty; 6 lexicalized reordering features, including bidirectional models built for monotone, swap, discontinuous reorderings; 1 distance-based reordering feature; and 2 language models, a 5-gram model learned on the OLD domain, and a 5-gram model learned on the NEW domain. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MI"
Q13-1035,lambert-etal-2012-automatic,0,0.0176546,"ailable for the WMT 2009 evaluation. It has been commonly used in the domain adaptation literature (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Haddow and Koehn, 2012, for instance). Science: Parallel abstracts from scientific publications in many disciplines including physics, biology, and computer science. We collected data from two distinct sources: (1) Canadian Science Publishing made available translated abstracts from their journals which span many research disciplines; (2) parallel abstracts from PhD theses in Physics and Computer Science collected from the HAL public repository (Lambert et al., 2012). Subs: Translated movie subtitles, available through the OPUS corpora collection (Tiedemann, 2009). In contrast to the other domains considered, subtitles consist of informal noisy text. 4 In this study, we use the Hansard domain as the domain, and we consider four possible NEW domains: EMEA, News, Science and Subs. Data sets for all domains were processed consistently. After tokenization, we paid particular attention to normalization in order to minimize artificial differences when combining data, such as American, British and Canadian spellings. This proved particularly important for the ne"
Q13-1035,2011.iwslt-evaluation.7,0,0.0157753,"Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a n"
Q13-1035,D07-1036,0,0.0113529,"ce/target to refer to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adaptin"
Q13-1035,2011.iwslt-papers.5,0,0.0211927,"http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on tr"
Q13-1035,D09-1074,0,0.361269,"been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech r"
Q13-1035,W09-2412,0,0.0488796,"Missing"
Q13-1035,2010.eamt-1.29,0,0.0130978,"o et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al"
Q13-1035,J11-4002,0,0.0861812,"Missing"
Q13-1035,P12-1099,0,0.016185,"b-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributions from comparable new domain corpora (Irvine et al., 2013). Earlier, Daum´e III and Jagarlamudi (2011) showed how mining translations for unseen words from comparable corpora can im431 prove SMT in a new domain. 4 The S4 Taxonomy We begin with a simple question: when w"
Q13-1035,E12-1055,0,0.0861505,". 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality"
Q13-1035,W10-1728,0,0.0230941,"dels (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying wh"
Q13-1035,N03-1033,0,0.00307439,"Missing"
Q13-1035,vilar-etal-2006-error,0,0.0997081,"Missing"
Q13-1035,C96-2141,0,0.226061,"Missing"
Q13-1035,D10-1091,0,0.0980275,"Missing"
Q13-1035,C04-1059,0,0.0192079,"vs. frequent phrases. 1. We use old/new to refer to domains and source/target to refer to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Wai"
Q13-1035,S10-1002,0,\N,Missing
S13-2034,P05-1048,1,0.873587,"Missing"
S13-2034,D07-1007,1,0.908605,"Missing"
S13-2034,W13-0801,1,0.777609,"w of official results: comparison of the precision scores of the ADAPT and BASIC systems with the best system according to each metric and with the official baseline the full translation probability distribution of a word in context. As a result, there is potentially much to be gained from combining PBSMT systems with the approaches used by other systems, which typically use richer feature representation and context models. Further exploration of the role of context in PBSMT performance and a comparison with dedicated classifiers trained on the same word-aligned parallel data can be found in (Carpuat, 2013). 5 tem can succeed in learning useful disambiguating information for its top candidate. Despite the problems stemming from learning good dominant translations from heterogeneous data, ADAPT ranks near the top using the Best Mode metric. The rankings in the out-of-five settings are strikingly different: the difference between BEST and OOF precisions are much smaller for BASIC and ADAPT than for all other participating systems (including the baseline.) This suggests that our PBSMT system only succeeds in learning to disambiguate one or two candidates per word, but does not do a good job of a es"
S13-2034,P07-1005,0,0.0604747,"Missing"
S13-2034,2011.mtsummit-papers.30,0,0.0957037,"d according to translation, reordering and language models learned from parallel corpora. The score of a Spanish translation given an English input sentence e segmented into P JPphrases is defined as follows: score(s, e) = i j λi log(φi (sj , ej )) + λLM φLM (s) Detailed feature definitions for phrase-based SMT models can be found in Koehn (2010). In our system, we use the following standard feature functions φ to score English-Spanish phrase pairs: • 4 phrase-table scores, which are conditional translation probabilities and HMM lexical probabilities in both directions translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model φLM , which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and N"
S13-2034,N12-1047,0,0.0355219,"phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model φLM , which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The λ weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT decoder for all experiments. PORTAGE uses a standard phrasal beam-search algorithm with 189 cube pruning. The main differences between this set-up and the popular open-source Moses system (Koehn et al., 2007), are the use of hierarchical reordering (Moses only supports non-hierarchical lexicalized reordering by default) and smoothed translation probabilities (Chen et al., 2011). As a result, disambiguation decisions for the CLWSD task are based on the following sources of information: • local source"
S13-2034,W07-0717,0,0.056525,"tional parallel corpora from the WMT-12 evaluations. We learn translation and 1 http://www.statmt.org/wmt12/translation-task.html 190 reordering models for (1) the Europarl subset used by the CLWSD organizers (900k sentence pairs, as in the BASIC system), and (2) the news commentary corpus from WMT12 (which comprises 150k sentence pairs). For the language model, we use the Spanish side of these two corpora, as well as that of the full Europarl corpus from WMT12 (which comprises 1.9M sentences). Models learned on different data sets are combined using linear mixtures learned on the tuning set (Foster and Kuhn, 2007). We also attempted other variations on the BASIC system which were not as successful. For instance, we tried to update the PBSMT tuning objective to be better suited to the CLWSD task. When producing translation of entire sentences, the PBSMT system is expected to produce hypotheses that are simultaneously fluent and adequate, as measured by BLEU score. In contrast, CLWSD measures the adequacy of the translation of a single word in a given sentence. We therefore attempted to tune for BLEU1, which only uses unigram precision, and therefore focuses on adequacy rather than fluency. However, this"
S13-2034,D08-1089,0,0.0322049,"score(s, e) = i j λi log(φi (sj , ej )) + λLM φLM (s) Detailed feature definitions for phrase-based SMT models can be found in Koehn (2010). In our system, we use the following standard feature functions φ to score English-Spanish phrase pairs: • 4 phrase-table scores, which are conditional translation probabilities and HMM lexical probabilities in both directions translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model φLM , which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The λ weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT"
S13-2034,S10-1026,0,0.0317584,"Missing"
S13-2034,J10-4005,0,0.0143007,"sh according to the translations available in a translation lexicon called phrase-table. Spanish phrases can be reordered to account for structural divergence between the two languages. This simple process can be used to generate Spanish sentences, which are scored according to translation, reordering and language models learned from parallel corpora. The score of a Spanish translation given an English input sentence e segmented into P JPphrases is defined as follows: score(s, e) = i j λi log(φi (sj , ej )) + λLM φLM (s) Detailed feature definitions for phrase-based SMT models can be found in Koehn (2010). In our system, we use the following standard feature functions φ to score English-Spanish phrase pairs: • 4 phrase-table scores, which are conditional translation probabilities and HMM lexical probabilities in both directions translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which p"
S13-2034,S10-1003,0,0.840307,"dard phrase-based system as a baseline, and investigated ways to improve its sense disambiguation performance. Using only local context information and no linguistic analysis beyond lemmatization, our machine translation system surprisingly yields top precision score based on the best predictions. However, its top 5 predictions are weaker than those from other systems. 1 Introduction This paper describes the systems submitted by the National Research Council Canada (NRC) for the Cross-Lingual Word Sense Disambiguation task at SemEval 2013 (Lefever and Hoste, 2013). As in the previous edition (Lefever and Hoste, 2010), this word sense disambiguation task asks systems to disambiguate English words by providing translations in other languages. It is therefore closely related to machine translation. Our work aims to explore this connection between machine translation and crosslingual word sense disambiguation, by providing a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation 1. BASIC, a baseline machine translation system train"
S13-2034,P11-2055,0,0.351871,"Missing"
S13-2034,P10-2041,0,0.0393826,"test domain. Since the CLWSD task does not provide parallel data in the test domain, we construct the tuning set using corpora publicly released for the WMT2012 translation task1 . Since sentences provided in the trial data appeared to come from a wide variety of genres and domains, we decided to build our tuning set using data from the news-commentary domain, rather then the more narrow Europarl domain used for training. We selected the top 3000 sentence pairs from the WMT 2012 development test sets, based on their distance to the CLWSD trial and test sentences as measured by cross-entropy (Moore and Lewis, 2010). All Spanish and English corpora were processed using FreeLing (Padr´o and Stanilovsky, 2012). Since the CLWSD targets and gold translations are lemmatized, we lemmatize all corpora. While FreeLing can provide a much richer linguistic analysis of the input sentences, the PBSMT sytem only makes use of their lemmatized representation. Our systems therefore contrast with previous approaches to CLWSD (van Gompel, 2010; Lefever et al., 2011, for instance), which use richer sources of information such as part-of-speech tags. 3 ADAPT: Adapting the MT system to the CLWSD task Our ADAPT system simply"
S13-2034,S07-1010,0,0.0428684,"Missing"
S13-2034,J03-1002,0,0.003572,"l., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model φLM , which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The λ weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT decoder for all experiments. PORTAGE uses a standard phrasal beam-search algorithm with 189 cube pruning. The main differences between this set-up and the popular open-source Moses system (Koehn et al., 2007), are the use of hierarchical reordering (Moses only supports non-hierarchical lexicalized reordering by default) and smoothed translation probabilities (Chen et al., 2011). As a result"
S13-2034,padro-stanilovsky-2012-freeling,0,0.0315163,"Missing"
S13-2034,P02-1040,0,0.0914429,"t could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model φLM , which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The λ weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT decoder for all experiments. PORTAGE uses a standard phrasal beam-search algorithm with 189 cube pruning. The main differences between this set-up and the popular open-source Moses system (Koehn et al., 2007), are the use of hierarchical reordering (Moses only supports non-hierarchical lexicalized reordering by default) and smoothed translation probabilities (Chen et al., 2011). As a result, disambiguation decisions for the CLWSD task are based on the following sources of information: • local source context, represented by source phrases of"
S13-2034,S10-1053,0,0.238116,"Missing"
S13-2034,W09-2413,0,\N,Missing
S13-2034,P07-2045,0,\N,Missing
S14-2030,P07-1019,0,0.0169033,"andard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adapted system (marked 1 below); Data run2: Linear LM mixture adaptation, using a context LM (marked 2 below). SMT systems require large amounts of data to estimate model parameters. In addition, translation performance larg"
S14-2030,2010.jec-1.4,0,0.0387737,"Missing"
S14-2030,2005.mtsummit-papers.11,0,0.0142599,"his results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. News Total en-de train dev 1904k - 177k 2000 2081k 2000 en-es train dev 1959k - 174k 2000 2133k 2000 fr-en train dev 2002k - 157k 2000 2158k 2000 nl-en train dev 1974k 1984 - 1974k 1984 Table 1: Number of training segments for each language pair. domain data to train on. As we had no information on the domain of the test data for Task 5, we chose to rely on general purpose publicly available data. Our main corpus is Europarl (Koehn, 2005), which is available for all 4 language pairs of the evaluation. As Europarl covers parliamentary proceedings, we added some news and commentary (henceforth ”News”) data provided for the 2013 workshop on Machine Translation shared task (Bojar et al., 2013) for language pairs other than nl-en. In all cases, we extracted from the corpus a tuning (“dev”) set of around 2000 sentence pairs. Statistics for the training data are given in Table 1. The trial and test data each consist of 500 sentences with L1 fragments in L2 context provided by the organizers. As the trial data came from Europarl, we f"
S14-2030,J03-1002,0,0.00547523,"Missing"
S14-2030,2009.mtsummit-papers.14,1,0.823815,"Missing"
S14-2030,N04-4026,0,0.01245,"op on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following target phrase. This results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. News Total en-de train dev 1904k - 177k 2000 2081k 2000 en-es train dev 1959k - 174k 2000 2133k 2000 fr-e"
S14-2030,N12-1047,0,0.0151326,"anslation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adapted system (marked 1 below); Data run2: Linear LM mixture adaptation, using a context LM (marked 2 below). SMT systems require large amounts of data to estimate model parameters. In addition, translation performance largely depends on having inTable 2 shows that our run1 system already yields high performance on the trial data, while 193 en-de1 en-de2 en-es1 en"
S14-2030,N04-1033,0,0.02597,"ation and reordering models), as well as the decoding and parameter tuning. Translation Models We use a single static phrase table including phrase pairs extracted from the symmetrized HMM word-alignment learned c 2014, The Crown in Right of Canada. 192 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following"
S14-2030,W07-0717,0,0.0205944,"only component of the SMT system that scores how well the translation of the L1 fragment fits in the existing L2 context. We test two different LM configurations. The first of these (run1) uses a single static LM: a standard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adap"
S14-2030,P99-1043,0,0.0910822,"Carpuat National Research Council Canada Multilingual Text Processing 1200 Montreal Road, Ottawa, Ontario K1A 0R6, Canada FirstName.LastName@nrc.ca Abstract there the parts of the target segment that need to be modified (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Koehn and Senellart, 2010). The task of translating a L1 fragment in L2 context therefore has much broader application than language learning. This motivation also provides a clear link of this task to the Machine Translation setting. There are also connections to the codeswitching and mixed language translation problems (Fung et al., 1999). In our work, we therefore investigate the use of a standard Phrase-Based Statistical Machine Translation (SMT) system to translate L1 fragments in L2 context. In the next section, we describe the SMT system that we used in our submission. We then describe the corpora used to train the SMT engine (Section 3), and our results on the trial and test data, as well as a short error analysis (Section 4). section We describe the system entered by the National Research Council Canada in the SemEval-2014 L2 writing assistant task. Our system relies on a standard Phrase-Based Statistical Machine Transl"
S14-2030,W10-1717,0,\N,Missing
S14-2030,W13-2201,0,\N,Missing
S14-2030,2005.iwslt-1.8,0,\N,Missing
S16-1084,attardi-etal-2010-resource,0,0.136968,"Missing"
S16-1084,S16-1143,0,0.0332991,"Missing"
S16-1084,S16-1142,0,0.0370432,"Missing"
S16-1084,J92-4003,0,0.211357,"Missing"
S16-1084,W13-0907,1,0.0637827,"Missing"
S16-1084,2012.eamt-1.60,0,0.0171689,"Missing"
S16-1084,P15-2079,1,0.0638888,"Missing"
S16-1084,W06-1670,0,0.0258617,"nal) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and covering new domains and languages. Named entity recognition (NER) does not suffer from these problems, as it uses a much smaller number of coarsegrained classes. However, these classes only apply to a subset of the nouns in a sentence and exclude verbs and adjectives. They therefore provide far from complete coverage in a corpus. Noun and verb supersenses (Ciaramita and Altun, 2006) offer a middle ground in granularity: they generalize named entity classes to cover all nouns (with 26 classes), but also cover verbs (15 classes)— see table 1—and provide a human-interpretable high-level clustering. WordNet supersenses for adjectives and adverbs nominally exist, but are based on morphosyntactic rather than semantic properties. There is, however, recent work on developing supersense taxonomies for English adjectives and 547 N :T OPS N : ACT N : OBJECT N : PERSON V: COGNITION V: COMMUNICATION N : COMMUNICATION N : EVENT N : FEELING N : FOOD N : GROUP N : LOCATION N : RELATION"
S16-1084,C10-2052,1,0.110925,"Missing"
S16-1084,W11-0809,0,0.114516,"ord expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized se"
S16-1084,S14-1001,1,0.837622,"V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idio"
S16-1084,S16-1140,0,0.0497829,"Missing"
S16-1084,D14-1108,1,0.144265,"Missing"
S16-1084,J93-2004,0,0.0679023,"Missing"
S16-1084,H93-1061,0,0.121824,"Missing"
S16-1084,2014.iwslt-papers.16,0,0.201582,"Missing"
S16-1084,N13-1039,1,0.0446902,"Missing"
S16-1084,W09-3531,0,0.0354613,"Missing"
S16-1084,E14-1078,1,0.043392,"Missing"
S16-1084,W12-3301,0,0.173236,"Missing"
S16-1084,W95-0107,0,0.102727,"Missing"
S16-1084,D11-1141,0,0.00536312,"Missing"
S16-1084,W15-1612,1,0.230436,"Missing"
S16-1084,S16-1141,0,0.125282,"Missing"
S16-1084,S10-1049,0,0.0152066,"evel organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idiomatic combinations such as MWEs are best labeled holistically, since their joint supersense category will often differ from that of the individual words. For example, spill the beans in its literal interpretation would receive supersenses V: CONTACT and N : FOOD , whereas the idiomatic interpretation, ‘divulge a secret’, is represented as an MWE holistically tagged as V: COMMUNICATION. Schneider and Smith (2015) develop this"
S16-1084,S16-1145,0,0.0357113,"Missing"
S16-1084,W13-0906,0,0.0274625,"Missing"
S16-1084,Q14-1016,1,0.485545,"presentation is applicable to other languages: see §2. 546 Proceedings of SemEval-2016, pages 546–559, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics In these sentences, a lot, leaves. . . to be desired, Fuji Sushi, came up, made. . . order, and carry out are all multiword expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (vi"
S16-1084,tsvetkov-etal-2014-augmenting-english,1,0.865215,"nse taxonomies for English adjectives and 547 N :T OPS N : ACT N : OBJECT N : PERSON V: COGNITION V: COMMUNICATION N : COMMUNICATION N : EVENT N : FEELING N : FOOD N : GROUP N : LOCATION N : RELATION N : SHAPE N : STATE N : SUBSTANCE N : TIME V: BODY V: MOTION V: PERCEPTION V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation,"
S16-1084,N13-1076,1,0.85833,"Missing"
S16-1084,P12-2050,1,0.107223,"V: MOTION V: PERCEPTION V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should"
S16-1084,schneider-etal-2014-comprehensive,1,0.119259,"presentation is applicable to other languages: see §2. 546 Proceedings of SemEval-2016, pages 546–559, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics In these sentences, a lot, leaves. . . to be desired, Fuji Sushi, came up, made. . . order, and carry out are all multiword expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (vi"
S16-1084,N15-1177,1,0.219831,"rds. While the main corpus with WordNet senses, SemCor (Miller et al., 1993), does reflect several text genres, it is hard to expand SemCor-style annotations to new genres, such as social web text or transcribed speech. This severely limits the applicability of SemCor-based NLP tools and restricts opportunities for linguistic studies of lexical semantics in corpora. To address this limitation, in the DiMSUM 2016 shared task,1 we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating multiword expressions and noun and verb supersenses (following Schneider and Smith, 2015), on multiple nontraditional genres of text. By moving away from fine-grained sense inventories and lexicalized, language-specific2 annotation, we take a step in the direction of broadcoverage, coarse-grained lexical semantic analysis. We believe this departure from the classical lexical semantics paradigm will ultimately prove fruitful for a variety of NLP applications in a variety of genres. The integrated lexical semantic representation (§2, §3) has been annotated in an extensive benchmark data set comprising several nontraditional domains (§4). Objective, controlled evaluation procedures ("
S16-1084,M95-1005,0,0.031558,"Missing"
S16-1084,W11-0817,0,0.0301138,"n with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and covering new domains and languages. Named entity recognition (NER) does not"
S16-1084,I13-1024,0,0.0698786,"prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and coveri"
S16-1084,S07-1051,0,0.0267622,"nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idiomatic combinations such as MWEs are best labeled holistically, since their joint supersense category will often differ from that of the individual words. For example, spill the beans in its literal interpretation would receive supersenses V: CONTACT and N : FOOD , whereas the idiomatic interpretation, ‘divulge a secret’, is represented as an MWE holistically tagged as V: COMMUNICATION."
S16-1084,picca-etal-2008-supersense,0,\N,Missing
S16-1084,J13-1009,0,\N,Missing
S16-1084,W15-1806,1,\N,Missing
S16-1084,S16-1144,0,\N,Missing
S17-1004,D16-1215,0,0.019856,"vity to context, and whether the nuances of the task can be better captured with annotations on a graded scale, following previous work on word meaning in context (Erk et al., 2013). of using an element-wise multiplication to apply a context mask to word type representations. The nature of the context representation varies: Erk and Pad´o (2008) use inverse selectional preferences; Thater et al. (2010) combine a first order co-occurrence based representation for the context with a second order representation for the target, Thater et al. (2011) rely on syntactic dependencies to define context. Apidianaki (2016) shows that bag-of-word context representation within a small context window works as well as syntactic definitions of context for ranking paraphrases in context. Our use of convolution is motivated by success of similar models on sentence classification tasks. Tang et al. (2014) uses convolution over embedding matrices for unigrams, bigrams, and trigrams, while Hovy (2015) uses just unigrams. However, all these works use the resulting representations to predict properties of the sentence (e.g., sentiment), rather than to contextualize target word representations. In-context lexical semantic t"
S17-1004,E12-1004,0,0.587041,"whi , ci , cih ) serves as a positive example. Repeat this process for all hypernyms (solid/green arrows in Figure 1). 3. Permute the positive examples to get negative examples. From (wi , whi , ci , cih ) and (wj , whj , cj , cjh ), generate negative examples (wi , whj , ci , cjh ) and (wj , whi , cj , cih ) (longer dashed/red arrows in Figure 1). 4. Flip the positive examples to generate negative examples. From (wi , whi , ci , cih ) generate the negative example (whi , wi , cih , ci ) (shorter dashed/red arrows in Figure 1). Existing datasets for lexical entailment (Baroni and Lenci, 2011; Baroni et al., 2012; Kotlerman et al., 2010) have driven progress on the out of context task only, and are therefore insensitive to context changes. In addition, they include a variety of negative examples without controlling for entailment direction. For instance, Baroni and Lenci (2011) use cohyponyms and random words as negative examples. Since cohyponyms are words that share a common hypernym (for example, salsa and tango are cohyponymys with respect to dance), hypernymy does not hold between them in any direction. On the other hand, random examples (also used by Baroni et al. (2012)) are likely to be detect"
S17-1004,Q15-1027,0,0.0183155,"l , cr ), we extract the word type representations w ~ l,c2v and w ~ l,c2v from Context2Vec, as well as the context representations ~cl,c2v , and ~cr,c2v . We will construct representations for cl , and cr , and create context-aware representations for wl and wr by “masking” their word embeddings with the embeddings for cl and cr (Section 3.3). We compare two approaches to representing cl and cr . The first (Section 3.1) builds on standard representations for word types, which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013). The second approach (Section 3.2) uses a recurrent neural model to embed words and contexts in the same space, allowing direct comparisons between them. 3.1 LSTM-based Context Representations: Context2Vec Creating Context Representations from Word Type Representations Given an example (wl , wr , cl , cr ), let w ~ l and w ~r refer to the context-agnostic representations of wl and wr , and let Cl and Cr represent the matrices obtained by row-wise stacking of the contextagnostic representations of words in cl and cr respectively. Following Tha"
S17-1004,N15-1098,0,0.266046,"303 negative examples from Step 3, and 5239 negative examples from Step 4. WH I C satisfies the desiderata outlined above. The dataset has a well-defined focus, since we only pick hypernym-hyponym pairs. The negative examples generated in Steps 3 and 4 require discriminating between different word senses and entailment directions. Finally, with over 22000 examples distributed over 6000 word pairs, the dataset is large enough to train large supervised models. We define a 70/5/25 train/dev/test split, and ensure that each set contains different word pairs, to avoid memorization and overfitting (Levy et al., 2015). 3 Representing Words and their Contexts for Entailment How can we construct representations of the meaning of target words wl and wr , and their respective exemplar contexts cl and cr ? 1. For all word types w ∈ W obtain synsets Sw . 35 ters over Cl and Cr . Specifically, we calculate the column-wise maximum, minimum and the mean over the matrices Cl and Cr , as done by Tang et al. (2014) for supervised sentiment classification. This yields three d-dimensional vectors for cl (~cl,max , ~cl,min , ~cl,mean ), and three d-dimensional vectors for cr (~cr,max , ~cr,min , ~cr,mean ). Computing the"
S17-1004,D10-1113,0,0.0205362,"my and entailment (Harabagiu and Moldovan, 1998; Shwartz et al., 2015), as well as a source of various datasets (Baroni and Lenci, 2011; Baroni et al., 2012). WH I C is inspired by the latter line of work, except that we extract exemplar contexts from WordNet in addition to relations between words. ~ 0} w ~ ·w Modeling word meaning in context Prior models for the meaning of a word in a given context aimed to capture semantic equivalence in tasks such as lexical substitution, word sense disambiguation or paraphrase ranking, rather than asymmetric relations such as entailment. One line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) views each word as a set of latent word senses. These models rely on token representations for individual occurrences of a word and then choose a set of token vectors based on the current context. An alternate set of models (Erk and Pad´o, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do here. These models share the idea We augment this system with contextualized word representations. We use the GloVe based masked representations, as they can be obtained with a negligibl"
S17-1004,N12-1076,0,0.0170165,"enci, 2011; Baroni et al., 2012; Kotlerman et al., 2010) have driven progress on the out of context task only, and are therefore insensitive to context changes. In addition, they include a variety of negative examples without controlling for entailment direction. For instance, Baroni and Lenci (2011) use cohyponyms and random words as negative examples. Since cohyponyms are words that share a common hypernym (for example, salsa and tango are cohyponymys with respect to dance), hypernymy does not hold between them in any direction. On the other hand, random examples (also used by Baroni et al. (2012)) are likely to be detected using symmetric semantic similarity rather than asymmetric hypernymy detection. Shwartz and Dagan (2016) recently introduced C ONTEXT-PPDB, a dataset for fine-grained lexical inference in context. This dataset consists of word pairs along with a pair of sentential contexts, with a label indicating the semantic relation between the two words in the given contexts. However, since C ONTEXT-PPDB only consists of ~3700 sentence pairs, it provides only a smaller number of annotated examples per relation, making it difficult to train large supervised models on (we return t"
S17-1004,J13-3003,0,0.118453,"o entailment relationship between the two sentences. On the other hand, the sentence ”Children smile and wave at the camera.” entails ”There are children present.”, but there is no meaningful hypernymy relationship between words in the two sentences. Finally, the proposed task is also related to, but different from word sense disambiguation (WSD). Unlike WSD, this task eschews an explicit sense inventory, instead relying on the provided contexts to decide the specific relation between the words. This might provide a more natural way to think about word senses for (untrained) human annotators (Erk et al., 2013). WSD can in principle be used as a preprocessing step to address hypernymy detection in context, but it is not required. Also, WSD remains a challenging task (Moro and Navigli, 2015) and it might introduce errors early in the preprocessing pipeline. Task Definition We frame hypernymy detection in context as a binary classification task. Each example consists of a 4-tuple (wl , wr , cl , cr ), where wl and wr are word types, and cl and cr are sentences which illustrate each word usage. The example is treated as positive if wl =⇒ wr , given the meaning of each word exemplified by the contexts,"
S17-1004,K16-1006,0,0.460991,", a large dataset, automatically extracted from WordNet (Fellbaum, 1998) using examples provided with synsets. Crucially, WH I C includes challenging negative examples that assess the ability of models to detect the direction of hypernymy. We use WH I C to determine the effectiveness of existing supervised models for hypernymy detection (Roller and Erk, 2016) applied to representations, not only of word types, but of words in context. Such contextualized representations are induced in two ways: the first is based on Context2Vec, a BiLSTM model that embeds contexts and words in the same space (Melamud et al., 2016); the second aims to capture geometric properties of the context in a standard word embedding space built using GloVe (Pennington et al., 2014). We show that the two contextualized representations improve performance over contextagnostic baselines. The structure of WH I C lets us show that they have complementary properties: Context2Vec-based models have higher recall and tend to identify directionality much better than Glove-based models. We also show that the context-aware representations improve performance on identifying a broader range of semantic relations (Shwartz and Dagan, 2016). Intr"
S17-1004,D08-1094,0,0.131457,"Missing"
S17-1004,S10-1002,0,0.0329937,"of convolution is motivated by success of similar models on sentence classification tasks. Tang et al. (2014) uses convolution over embedding matrices for unigrams, bigrams, and trigrams, while Hovy (2015) uses just unigrams. However, all these works use the resulting representations to predict properties of the sentence (e.g., sentiment), rather than to contextualize target word representations. In-context lexical semantic tasks Besides entailment, other lexical semantic tasks studied in context include lexical substitution (McCarthy and Navigli, 2007) and cross-lingual lexical substitution (Mihalcea et al., 2010). The focus of these tasks and their related datasets is on synonymy and translation equivalence, since they require one to predict substitutes for a target word instance, which preserve its meaning in a given sentential context. On the other hand, the focus of this work and WH I C is on detecting more fine-grained relations via lexical entailment. Another related task is that of paraphrase ranking (Apidianaki, 2016). The work by Apidianaki (2016) is also notable because of their successful use of models of wordmeaning in context from Thater et al. (2011), which is closely related to our work."
S17-1004,S15-2049,0,0.0329842,"no meaningful hypernymy relationship between words in the two sentences. Finally, the proposed task is also related to, but different from word sense disambiguation (WSD). Unlike WSD, this task eschews an explicit sense inventory, instead relying on the provided contexts to decide the specific relation between the words. This might provide a more natural way to think about word senses for (untrained) human annotators (Erk et al., 2013). WSD can in principle be used as a preprocessing step to address hypernymy detection in context, but it is not required. Also, WSD remains a challenging task (Moro and Navigli, 2015) and it might introduce errors early in the preprocessing pipeline. Task Definition We frame hypernymy detection in context as a binary classification task. Each example consists of a 4-tuple (wl , wr , cl , cr ), where wl and wr are word types, and cl and cr are sentences which illustrate each word usage. The example is treated as positive if wl =⇒ wr , given the meaning of each word exemplified by the contexts, and negative otherwise, as can be seen in Table 1. As mentioned in Section 1, hypernymy is only one specific case of lexical entailment. The nature of entailment relations captured ou"
S17-1004,P15-2070,0,0.0675362,"Missing"
S17-1004,D14-1162,0,0.081381,"allenging negative examples that assess the ability of models to detect the direction of hypernymy. We use WH I C to determine the effectiveness of existing supervised models for hypernymy detection (Roller and Erk, 2016) applied to representations, not only of word types, but of words in context. Such contextualized representations are induced in two ways: the first is based on Context2Vec, a BiLSTM model that embeds contexts and words in the same space (Melamud et al., 2016); the second aims to capture geometric properties of the context in a standard word embedding space built using GloVe (Pennington et al., 2014). We show that the two contextualized representations improve performance over contextagnostic baselines. The structure of WH I C lets us show that they have complementary properties: Context2Vec-based models have higher recall and tend to identify directionality much better than Glove-based models. We also show that the context-aware representations improve performance on identifying a broader range of semantic relations (Shwartz and Dagan, 2016). Introduction Language understanding applications like question answering (Harabagiu and Hickl, 2006) and textual entailment (Dagan et al., 2013) be"
S17-1004,N10-1013,0,0.0727175,"of models considered leave substantial room for improvement. For instance, it remains to be seen whether richer features for the supervised models and richer context representations can improve sensitivity to context, and whether the nuances of the task can be better captured with annotations on a graded scale, following previous work on word meaning in context (Erk et al., 2013). of using an element-wise multiplication to apply a context mask to word type representations. The nature of the context representation varies: Erk and Pad´o (2008) use inverse selectional preferences; Thater et al. (2010) combine a first order co-occurrence based representation for the context with a second order representation for the target, Thater et al. (2011) rely on syntactic dependencies to define context. Apidianaki (2016) shows that bag-of-word context representation within a small context window works as well as syntactic definitions of context for ranking paraphrases in context. Our use of convolution is motivated by success of similar models on sentence classification tasks. Tang et al. (2014) uses convolution over embedding matrices for unigrams, bigrams, and trigrams, while Hovy (2015) uses just"
S17-1004,D16-1234,0,0.169901,"., 2014; Shwartz et al., 2016). In this work, we focus on hypernymy detection in context, and show that existing resources can be leveraged to automatically create test beds for evaluation. We introduce “Wordnet Hypernyms in Context” (WH I C, pronounced which), a large dataset, automatically extracted from WordNet (Fellbaum, 1998) using examples provided with synsets. Crucially, WH I C includes challenging negative examples that assess the ability of models to detect the direction of hypernymy. We use WH I C to determine the effectiveness of existing supervised models for hypernymy detection (Roller and Erk, 2016) applied to representations, not only of word types, but of words in context. Such contextualized representations are induced in two ways: the first is based on Context2Vec, a BiLSTM model that embeds contexts and words in the same space (Melamud et al., 2016); the second aims to capture geometric properties of the context in a standard word embedding space built using GloVe (Pennington et al., 2014). We show that the two contextualized representations improve performance over contextagnostic baselines. The structure of WH I C lets us show that they have complementary properties: Context2Vec-b"
S17-1004,C14-1097,0,0.408382,"dataset is large enough to train large supervised models. We define a 70/5/25 train/dev/test split, and ensure that each set contains different word pairs, to avoid memorization and overfitting (Levy et al., 2015). 3 Representing Words and their Contexts for Entailment How can we construct representations of the meaning of target words wl and wr , and their respective exemplar contexts cl and cr ? 1. For all word types w ∈ W obtain synsets Sw . 35 ters over Cl and Cr . Specifically, we calculate the column-wise maximum, minimum and the mean over the matrices Cl and Cr , as done by Tang et al. (2014) for supervised sentiment classification. This yields three d-dimensional vectors for cl (~cl,max , ~cl,min , ~cl,mean ), and three d-dimensional vectors for cr (~cr,max , ~cr,min , ~cr,mean ). Computing the maximum and minimum across all vector dimensions captures the exterior surface of the “instance manifold” (the volume in embedding space within which all words in the instance reside), while the mean summarizes the density perdimension within the manifold (Hovy, 2015). Cl the 0.3 0.6 -0.1 river 1.5 -2.5 0 bank -1 0.2 1.8 -0.27 -1.14 1.03 -5.0 -0.18 wl,min -1.5 0.12 3.24 wl,max 1 (wl) cl,me"
S17-1004,P15-1073,0,0.171897,". Specifically, we calculate the column-wise maximum, minimum and the mean over the matrices Cl and Cr , as done by Tang et al. (2014) for supervised sentiment classification. This yields three d-dimensional vectors for cl (~cl,max , ~cl,min , ~cl,mean ), and three d-dimensional vectors for cr (~cr,max , ~cr,min , ~cr,mean ). Computing the maximum and minimum across all vector dimensions captures the exterior surface of the “instance manifold” (the volume in embedding space within which all words in the instance reside), while the mean summarizes the density perdimension within the manifold (Hovy, 2015). Cl the 0.3 0.6 -0.1 river 1.5 -2.5 0 bank -1 0.2 1.8 -0.27 -1.14 1.03 -5.0 -0.18 wl,min -1.5 0.12 3.24 wl,max 1 (wl) cl,mean 0.27 -0.57 0.57 cl,min -1 -2.5 -0.1 cl,max 1.5 0.6 1.8 } ⊙ wl,mean -1 0.2 1.8 bank Figure 2: Constructing word-in-context representations for “bank”, in the context “the river bank”. indicates element-wise multiplication. 3.2 An alternative approach to contextualizing word representations is to directly compare the representations of words with representations of contexts. This can be done using Context2Vec (Melamud et al., 2016), a neural model that, given a target wo"
S17-1004,S16-2013,0,0.321495,"e space (Melamud et al., 2016); the second aims to capture geometric properties of the context in a standard word embedding space built using GloVe (Pennington et al., 2014). We show that the two contextualized representations improve performance over contextagnostic baselines. The structure of WH I C lets us show that they have complementary properties: Context2Vec-based models have higher recall and tend to identify directionality much better than Glove-based models. We also show that the context-aware representations improve performance on identifying a broader range of semantic relations (Shwartz and Dagan, 2016). Introduction Language understanding applications like question answering (Harabagiu and Hickl, 2006) and textual entailment (Dagan et al., 2013) benefit from identifying semantic relations between words beyond synonymy and paraphrasing. For instance, given “Anand plays chess.”, and the question “Which game does Anand play?”, successfully answering the question requires knowing that chess is a kind of game, i.e. chess entails game. Such lexical entailment relations are asymmetric (chess =⇒ game, but game =⇒ 6 chess), and detecting their direction accurately is a challenge. While prior work ha"
S17-1004,P16-1226,0,0.15201,"ensitive to context changes. In addition, they include a variety of negative examples without controlling for entailment direction. For instance, Baroni and Lenci (2011) use cohyponyms and random words as negative examples. Since cohyponyms are words that share a common hypernym (for example, salsa and tango are cohyponymys with respect to dance), hypernymy does not hold between them in any direction. On the other hand, random examples (also used by Baroni et al. (2012)) are likely to be detected using symmetric semantic similarity rather than asymmetric hypernymy detection. Shwartz and Dagan (2016) recently introduced C ONTEXT-PPDB, a dataset for fine-grained lexical inference in context. This dataset consists of word pairs along with a pair of sentential contexts, with a label indicating the semantic relation between the two words in the given contexts. However, since C ONTEXT-PPDB only consists of ~3700 sentence pairs, it provides only a smaller number of annotated examples per relation, making it difficult to train large supervised models on (we return to this dataset in Section 5). We address these gaps by introducing, WH I C, a large dataset automatically derived from WordNet (Fell"
S17-1004,K15-1018,0,0.120635,"her scores. Additionally, word representation features are used: given two word/context pairs (wx , cx , wy , cy ), GloVe vectors are used to represent wx and wy , as well as words in cx and cy , and are used to extract the following feature, which capture the most salient word/context similarities between the two pairs : {max w~x · w, ~ max w~y · w, ~ w∈cy w∈cx max w∈cx ,w0 ∈cy 8 Related Work WordNet and lexical entailment The “is-a” hierarchy of WordNet (Fellbaum, 1998) is a prominent source of information for unsupervised detection of hypernymy and entailment (Harabagiu and Moldovan, 1998; Shwartz et al., 2015), as well as a source of various datasets (Baroni and Lenci, 2011; Baroni et al., 2012). WH I C is inspired by the latter line of work, except that we extract exemplar contexts from WordNet in addition to relations between words. ~ 0} w ~ ·w Modeling word meaning in context Prior models for the meaning of a word in a given context aimed to capture semantic equivalence in tasks such as lexical substitution, word sense disambiguation or paraphrase ranking, rather than asymmetric relations such as entailment. One line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) views each word as"
S17-1004,E17-1007,0,0.066209,"Missing"
S17-1004,P10-1097,0,0.0801353,"Missing"
S17-1004,I11-1127,0,0.0533086,"Missing"
S17-1004,P16-1158,0,0.0127635,"type representations w ~ l,c2v and w ~ l,c2v from Context2Vec, as well as the context representations ~cl,c2v , and ~cr,c2v . We will construct representations for cl , and cr , and create context-aware representations for wl and wr by “masking” their word embeddings with the embeddings for cl and cr (Section 3.3). We compare two approaches to representing cl and cr . The first (Section 3.1) builds on standard representations for word types, which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013). The second approach (Section 3.2) uses a recurrent neural model to embed words and contexts in the same space, allowing direct comparisons between them. 3.1 LSTM-based Context Representations: Context2Vec Creating Context Representations from Word Type Representations Given an example (wl , wr , cl , cr ), let w ~ l and w ~r refer to the context-agnostic representations of wl and wr , and let Cl and Cr represent the matrices obtained by row-wise stacking of the contextagnostic representations of words in cl and cr respectively. Following Thater et al. (2011); Erk"
S17-1004,J09-3004,0,0.0169346,"out-of-context can be broader depending on the test beds considered2 . These relations can include synonymy, hypernymy, some meronymy relations, and also cause-effect relations. 2.2 Motivation The need to study hypernymy detection in context is important due to several reasons. First, many downstream tasks which might benefit from detecting hypernyms will have words appearing in specific contexts. Second, existing definitions (and, by extension, annotations) of lexical entailment do not explicitly or consistently address polysemy. For instance, the substitutional definition for entailment by Zhitomirsky-Geffet and Dagan (2009) asks the reader to think of a natural sentence that provides the missing context to the two words being considered, thus constraining the possible senses of the two words. On the other hand, Turney and Mohammad (2013) propose a relational definition, inviting the reader to imagine a semantic relation that connects the two words and constrains their possible senses. In contrast, we propose to detect hypernymy between word meanings described by specific contexts. 2.3 WH I C : A Dataset for Lexical Entailment in Context We require a dataset to study hypernymy detection in context to satisfy the"
S17-1004,P06-1114,0,\N,Missing
S17-1004,W11-2501,0,\N,Missing
S18-1170,E12-1004,0,0.0590631,"Missing"
S18-1170,P14-1023,0,0.236443,"Missing"
S18-1170,S18-1117,0,0.108792,"Missing"
S18-1170,W16-2509,0,0.382541,"ed Systems All our models rely on Glove (Pennington et al., 2014), generic word embeddings models, pretrained on large corpora: the Wikipedia and English Gigaword newswire corpora. In addition to capturing semantic similarity with distances between words, Glove aims for vector differences to capture the meaning specified by the juxtaposition of two words, which is a good fit for our task. Because the discriminant features are distinct between train, validation and test, our systems should be able to generalize to previously unseen Baseline We first consider the baseline approach introduced by Krebs and Paperno (2016) to detect the positive examples, where cs denotes the cosine similarity function: cs(word1 , disc) > cs(word2 , disc) 3.2 (1) 2-Step Unsupervised System We refine this baseline with a 2-step approach. Our intuition is that d is a discriminant between w1 and w2 if the following two conditions hold simultaneously: 1. w1 is more similar to d than w2 by more than a threshold tthresh : cs(w1 , d) − cs(w2 , d) > tthresh (2) 2. d is highly similar to w1 : cs(w1 , d) > tdiverge (3) The condition in Equation 2 aims at detecting negative examples that share the discriminant attribute, and the condition"
S18-1170,N13-1090,0,0.110532,"Missing"
S18-1170,D14-1162,0,0.0857293,"Missing"
S18-1170,C14-1097,0,0.0464439,"Missing"
S18-1170,P16-1157,0,0.0610731,"Missing"
W02-2035,P00-1027,0,\N,Missing
W03-0433,N01-1025,0,0.0250955,"Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to"
W03-0433,W02-2020,0,0.0354852,"In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. The experiments present"
W03-0433,N01-1006,1,0.827825,"ces. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made. The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. 3 3.1 Data Resources Preprocessing the Data The data that was provided by the CoNLL organizers was sentence-delimited and tokenized, and hand-annotated with named entity chunks. The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al., 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). We replaced the English part-of-speech tags with those generated by a trans"
W03-0433,C00-2102,0,0.0435389,"(Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill, 1995) (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be"
W03-0433,W02-2035,1,0.640843,"increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify. For the boosting framework, our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm. It performs well on a number of natural language processing problems, including text categorization (Schapire and Singer, 2000) and word sense disambiguation (Escudero et al., 2000). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al., 2002; Carreras et al., 2002). 2.2 Support Vector Machines Support Vector Machines (SVMs) have gained a considerable following in recent years (Boser et al., 1992), particularly in dealing with high-dimensional spaces such as commonly found in natural language problems like text categorization (Joachims, 1998). SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. 2.3 Transformation-based Learning Transformation-based learning (Brill,"
W03-0433,W96-0102,0,\N,Missing
W03-0433,W02-2004,0,\N,Missing
W03-0433,J95-4004,0,\N,Missing
W04-0822,W02-2004,0,0.140229,"Missing"
W04-0822,W02-1002,0,0.117981,"of ensembles utilizing various combinations of four voting models, as follows. Some of these component models were also evaluated on other Senseval-3 tasks: the Basque, Catalan, Italian, and Romanian Lexical Sample tasks (Wicentowski et al., 2004), as well as Semantic Role Labeling (Ngai et al., 2004). The first voting model, a na¨ıve Bayes model, was built as Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model, a maximum entropy model (Jaynes, 1978), was built as Klein and Manning (2002) found that it yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. However, note that a different subset of either Senseval-1 or Senseval-2 English lexical sample data was used. The third voting model, a boosting model (Freund and Schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (Carreras et al., 2002)(Wu et al., 2002). Specifically, we employed an AdaBoost.MH model (Schapire and Singer, 2000), which is a multi-class generalization of the original boosting algorith"
W04-0822,P03-1004,0,0.0341894,"from their original space R n to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998). As with other kernel methods, a major advantage of KPCA over other common analysis techniques is that it can inherently take combinations of predictive features into account when optimizing dimensionality reduction. For WSD and indeed many natural language tasks, significant accuracy gains can often be achieved by generalizing over relevant feature combinations (see, e.g., Kudo and Matsumoto (2003)). A further advantage of KPCA in the context of the WSD problem is that the dimensionality of the input data is generally very large, a condition where kernel methods excel. Nonlinear principal components (Diamantaras and Kung, 1996) are defined as follows. Suppose we are given a training set of M pairs (x t , ct ) where the observed vectors xt ∈ Rn in an n-dimensional input space X represent the context of the target word being disambiguated, and the correct class c t represents the sense of the word, for t = 1, .., M . Suppose Φ is a nonlinear mapping from the input space Rn to the feature"
W04-0822,W04-0845,1,0.757264,"ace Rn to the feature space F . Without loss of generality we assume the M vectors P are centered vectors in the feature space, i.e., M t=1 Φ (xt ) = 0; uncentered vectors can easily be converted to centered vectors (Sch¨olkopf et al., 1998). We wish to Ensemble classification The WSD models presented here consist of ensembles utilizing various combinations of four voting models, as follows. Some of these component models were also evaluated on other Senseval-3 tasks: the Basque, Catalan, Italian, and Romanian Lexical Sample tasks (Wicentowski et al., 2004), as well as Semantic Role Labeling (Ngai et al., 2004). The first voting model, a na¨ıve Bayes model, was built as Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model, a maximum entropy model (Jaynes, 1978), was built as Klein and Manning (2002) found that it yielded higher accuracy than na¨ıve Bayes in a subsequent comparison of WSD performance. However, note that a different subset of either Senseval-1 or Senseval-2 English lexical sample data was used. The third voting model, a boosting model (Freund and Schapire, 1"
W04-0822,P04-1081,1,0.748075,"e new voting model typically degrades the accuracy of the ensemble instead of helping it. In this work, we investigate the potential of one promising new disambiguation model with respect 1 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. to augmenting our existing ensemble combining a maximum entropy model, a boosting model, and a na¨ıve Bayes model—a combination representing some of the best stand-alone WSD models currently known. The new WSD model, proposed by Wu et al. (2004), is a method for disambiguating word senses that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique. That the KPCA-based model could potentially be a good candidate for a new voting model is suggested by Wu et al.’s empirical results showing that it yielded higher accuracies on Senseval-2 data sets than other models that included maximum entropy, na¨ıve Bayes, and SVM based models. In the following sections, we begin with a description of the experimental setup, which utilizes a number of individual classifiers in a voting ensemble. We then describe the KPCA-based model"
W04-0822,W02-2035,1,\N,Missing
W04-0845,P98-1013,0,0.013804,"in Senseval-3. Results show that these systems, which are based upon common machine learning algorithms, all manage to achieve good performances on the non-restricted Semantic Role Labeling task. 1 Introduction This paper describes the HKPolyU-HKUST systems which participated in the Senseval-3 Semantic Role Labeling task. The systems represent a diverse array of machine learning algorithms, from decision lists to SVMs to Winnow-type networks. Semantic Role Labeling (SRL) is a task that has recently received a lot of attention in the NLP community. The SRL task in Senseval-3 used the Framenet (Baker et al., 1998) corpus: given a sentence instance from the corpus, a system’s job would be to identify the phrase constituents and their corresponding role. The Senseval-3 task was divided into restricted and non-restricted subtasks. In the non-restricted subtask, any and all of the gold standard annotations contained in the FrameNet corpus could be used. Since this includes information on the boundaries of the parse constituents which correspond to some frame element, this effectively maps the SRL task to that of a role-labeling classification task: given a constituent parse, identify the frame element that"
W04-0845,W04-0822,1,0.826871,"Missing"
W04-0845,P97-1003,0,0.0320672,"to the target word. • The position (e.g. before, after) of the constituent, with respect to the target word. In addition to the above features, we also extracted a set of features which required the use of some statistical NLP tools: • Transitivity and voice of the target word — The sentence was first part-of-speech tagged and chunked with the fnTBL transformationbased learning tools (Ngai and Florian, 2001). Simple heuristics were then used to deduce the transitivity voice of the target word. • Head word (and its part-of-speech tag) of the constituent — After POS tagging, a syntactic parser (Collins, 1997) was then used to obtain the parse tree for the sentence. The head word (and the POS tag of the head word) of the syntactic parse constituent whose span corresponded most closely to the candidate constituent was then assumed to be the head word of the candidate constituent. The resulting training data set consisted of 51,366 constituent samples with a total of 151 frame element types. These ranged from “Descriptor” (3520 constituents) to “Baggage” and “Carrier” (1 constituent each). This training data was randomly partitioned into a 80/20 “development training” and “validation” set. 3 Methodol"
W04-0845,J02-3001,0,0.217534,"part through research grants A-PE37 and 4-Z03S. 2 The author would like to thank the Hong Kong Research Grants Council (RGC) for supporting this research in part through research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. 2 Experimental Features This section describes the features that were used for the SRL task. Since the non-restricted SRL task is essentially a classification task, each parse constituent that was known to correspond to a frame element was considered to be a sample. The features that we used for each sample have been previously shown to be helpful for the SRL task (Gildea and Jurafsky, 2002). Some of these features can be obtained directly from the Framenet annotations: • The name of the frame. • The lexical unit of the sentence — i.e. the lexical identity of the target word in the sentence. • The general part-of-speech tag of the target word. • The “phrase type” of the constituent — i.e. the syntactic category (e.g. NP, VP) that the constituent falls into. • The “grammatical function” (e.g. subject, object, modifier, etc) of the constituent, with respect to the target word. • The position (e.g. before, after) of the constituent, with respect to the target word. In addition to th"
W04-0845,W99-0621,0,0.0510482,"Missing"
W04-0845,N01-1006,1,0.808888,"e “phrase type” of the constituent — i.e. the syntactic category (e.g. NP, VP) that the constituent falls into. • The “grammatical function” (e.g. subject, object, modifier, etc) of the constituent, with respect to the target word. • The position (e.g. before, after) of the constituent, with respect to the target word. In addition to the above features, we also extracted a set of features which required the use of some statistical NLP tools: • Transitivity and voice of the target word — The sentence was first part-of-speech tagged and chunked with the fnTBL transformationbased learning tools (Ngai and Florian, 2001). Simple heuristics were then used to deduce the transitivity voice of the target word. • Head word (and its part-of-speech tag) of the constituent — After POS tagging, a syntactic parser (Collins, 1997) was then used to obtain the parse tree for the sentence. The head word (and the POS tag of the head word) of the syntactic parse constituent whose span corresponded most closely to the candidate constituent was then assumed to be the head word of the candidate constituent. The resulting training data set consisted of 51,366 constituent samples with a total of 151 frame element types. These ran"
W04-0845,W04-0862,0,0.0617167,"Missing"
W04-0845,C98-1013,0,\N,Missing
W04-0863,W04-0845,1,0.577242,"Missing"
W04-0863,W04-0862,1,0.8031,"as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model, this paper focuses solely on the joint contributions to the ”Swat-HK” effort. 1 Introduction This paper describes the two joint component models of the Swat-HK systems entered into four of the word sense disambiguation lexical sample tasks in Senseval-3: Basque, Catalan, Italian and Romanian, as well as a combination model for each language. The feature engineering (and construction of three other component models which are described in (Wicentowski et al., 2004)) was performed at Swarthmore College, while the Hong Kong team constructed two component models based on wellknown machine learning algorithms. The combination model, which was constructed at Swarthmore, uses voting to combine all five models. 2 Experimental Features A full description of the experimental features for all four tasks can be found in the report submitted by the Swarthmore College Senseval team (Wicentowski et al., 2004). Briefly, the systems used lexical and syntactic features in the context of the target word: • The “bag of words (and lemmas)” in the context of the ambiguous w"
W06-0124,W02-2004,0,0.235444,"Missing"
W06-0124,C04-1058,1,0.837143,"extensive theoretical and empirical studies, where different standard machine learning methods have been used as the weak classifier (e.g., Bauer and Kohavi (1999), Opitz and Maclin (1999), Schapire (2002)). It also performs well on a number of natural language processing problems, including text categorization (e.g., Schapire and Singer (2000), Sebastiani et al. (2000)) and word sense disambiguation (e.g., Escudero et al. (2000)). In particular, it has also been demonstrated that boosting can be used to build language-independent NER models that perform exceptionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). The weak classifiers used in the boosting algorithm come from a wide range of machine learning methods. We have chosen to use a simple classifier called a decision stump in the algorithm. A decision stump is basically a one-level decision tree where the split at the root level is based on a specific attribute/value pair. For example, a possible attribute/value pair could be W2 = ™/. Boosting The main idea behind the boosting algorithm is that a set of many simple and moderately accurate weak classifiers (also called weak hypotheses) can be effectively combined to yie"
W06-0124,W03-1709,0,0.171093,"Missing"
W06-0124,W02-2035,1,\N,Missing
W09-0427,popovic-ney-2004-towards,0,0.0625377,"Missing"
W09-0427,allauzen-bonneau-maynard-2008-training,0,0.0173303,"Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of 2.1 Translation system Data sets We use a subset of the data made available for the official French to English translation task. The evaluation test set consists of French news data from September to October 2008, however the bulk of the training data is not from the same domain. The translation model was trained on the Europarl corpus (europarl-v4) and the small news commentary corpus (news-commentary09). Following D´echelotte et al. (2008), we learn a single phrase table and reordering model rather than one for each domain, as it was found to yield better performance in a very similar setting. The language model was trained on the English side of these parallel corpora augmented with nonparallel English news data (news-train08.en). Parameter tuning was performed on the designated development data, which is also in the news domain: news-dev2009a was used as the development set and news-dev2009b as the test set. Using those data sets, there is therefore a mismatch between the training and evaluation domains, as in the domain adap"
W09-0427,P06-1001,0,0.274106,"mation in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of 2.1 Translation system Data sets We use a subset of the data made available for the official French to English translation task. The evaluation test set consists of French news data from September to October 2008, however the bulk of the training data is not from the same domain. The translation model was trained on the Europarl corpus (europarl-v4) and the small news commentary corpus (news-commentary09). Following D´echelotte et al. (2008), we learn a sing"
W09-0427,W08-0310,0,0.0305532,"Missing"
W09-0427,W08-0313,0,0.0142867,"and word-alignment strategy based on morphology, and analyze their impact on translation quality. 1 Introduction 2 In this first participation to the French-English translation task at WMT, our goal was to build a standard phrase-based statistical machine translation system and study the impact of French morphological variations at different stages of training and decoding. Many strategies have been proposed to integrate morphology information in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of 2.1 Translation sys"
W09-0427,D07-1091,0,0.0212798,"ses phrasebased Statistical Machine Translation system with two simple modications of the decoding input and word-alignment strategy based on morphology, and analyze their impact on translation quality. 1 Introduction 2 In this first participation to the French-English translation task at WMT, our goal was to build a standard phrase-based statistical machine translation system and study the impact of French morphological variations at different stages of training and decoding. Many strategies have been proposed to integrate morphology information in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information"
W09-0427,2007.mtsummit-papers.65,0,0.0350338,"Missing"
W09-0427,P07-2045,0,0.0174571,"Missing"
W09-0427,W06-3115,0,0.121788,"ystem and study the impact of French morphological variations at different stages of training and decoding. Many strategies have been proposed to integrate morphology information in SMT, including factored translation models (Koehn and Hoang, 2007), adding a translation dictionary containing inflected forms to the training data (Schwenk et al., 2008), entirely replacing surface forms by representations built on lemmas and POS tags (Popovi´c and Ney, 2004), morphemes learned in an unsupervised manner (Virpojia et al., 2007), and using Porter stems and even 4-letter prefixes for word alignment (Watanabe et al., 2006). In non-European languages, such as Arabic, heavy effort has been put in identifying appropriate input representations to improve SMT quality (e.g., Sadat and Habash (2006)) As a first step toward using morphology information in our French-English SMT system, this submission focused on studying the impact of 2.1 Translation system Data sets We use a subset of the data made available for the official French to English translation task. The evaluation test set consists of French news data from September to October 2008, however the bulk of the training data is not from the same domain. The tran"
W09-0427,P03-1021,0,0.0446493,"Missing"
W09-0427,2006.iwslt-evaluation.20,0,0.0382944,"Missing"
W09-0427,P02-1040,0,0.0757203,"Missing"
W09-0427,W05-0909,0,\N,Missing
W09-2404,C96-1005,0,0.0257068,"Missing"
W09-2404,P07-1020,0,0.0119889,"location hypothesis by attempting to translate phrases rather than single words (Koehn et al., 2007), the one sense per discourse hypothesis has not been explicitly used in SMT modeling. Even the recent generation of SMT models that explicitly use WSD modeling to perform lexical choice rely on sentence context rather than wider document context and translate sentences in isolation (Carpuat and Wu, 2007; Chan et al., 2007; Gim´enez and M`arquez, 2007; Stroppa et al., 2007; Specia et al., 2008). Other context-sensitive SMT approaches (Gimpel and Smith, 2008) and 20 global lexical choice models (Bangalore et al., 2007) also translate sentences independently. 3 One translation per discourse in reference translations In this section we investigate whether the one sense per discourse hypothesis holds in translation. Does one sense per discourse mean one translation per discourse? On the one hand, one translation per discourse might be too strict a constraint to allow for variations in lexicalization of a given sense. While a WSD task produces a set of predefined sense labels, a single sense might be correctly translated in many different ways in a full sentence translation. On the other hand, if the author of"
W09-2404,D07-1007,1,0.746112,"s Agency. 19 (1996)). In this paper, we investigate its potential usefulness in the context of machine translation. A growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications. In monolingual WSD, word alignments in parallel corpora have been successfully used as learning evidence (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003). In Statistical Machine Translation (SMT), recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories (Carpuat and Wu, 2007; Chan et al., 2007; Gim´enez and M`arquez, 2007). In this paper, we revisit the one sense per discourse hypothesis using word translations in parallel text as senses. Our first goal is to empirically evaluate whether the one translation per document hypothesis holds on French-English reference corpora, thus verifying whether translations exhibit the same properties as monolingual senses. Our second goal consists in evaluating whether the one translation per discourse hypothesis has the potential to be as useful to statistical machine translation as the one sense per discourse hypothesis to WS"
W09-2404,P07-1005,0,0.392204,"In this paper, we investigate its potential usefulness in the context of machine translation. A growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications. In monolingual WSD, word alignments in parallel corpora have been successfully used as learning evidence (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003). In Statistical Machine Translation (SMT), recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories (Carpuat and Wu, 2007; Chan et al., 2007; Gim´enez and M`arquez, 2007). In this paper, we revisit the one sense per discourse hypothesis using word translations in parallel text as senses. Our first goal is to empirically evaluate whether the one translation per document hypothesis holds on French-English reference corpora, thus verifying whether translations exhibit the same properties as monolingual senses. Our second goal consists in evaluating whether the one translation per discourse hypothesis has the potential to be as useful to statistical machine translation as the one sense per discourse hypothesis to WSD. Current Statisti"
W09-2404,W07-0717,0,0.0207486,"Missing"
W09-2404,H92-1045,0,0.3155,"lation (SMT) output showed that despite ignoring document structure, the one translation per discourse hypothesis is strongly supported in part because of the low variability in SMT lexical choice. More interestingly, cases where the hypothesis does not hold can reveal lexical choice errors. A preliminary study showed that enforcing the one translation per discourse constraint in SMT can potentially improve translation quality, and that SMT systems might benefit from translating sentences within their entire document context. 1 Introduction The one sense per discourse hypothesis formulated by Gale et al. (1992b) has proved to be a simple yet powerful observation and has been successfully used in word sense disambiguation (WSD) and related tasks (e.g., Yarowsky (1995); Agirre and Rigau ∗ The author was partially funded by GALE DARPA Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 19 (1996)). In this paper, we investigate its potential usefulness in the context of machine translation. A growing body of work suggests tha"
W09-2404,W07-0719,0,0.0204224,"Missing"
W09-2404,W07-0733,0,0.0166328,"Missing"
W09-2404,P07-2045,0,0.00630399,"Missing"
W09-2404,P03-1058,0,0.0310295,"Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 19 (1996)). In this paper, we investigate its potential usefulness in the context of machine translation. A growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications. In monolingual WSD, word alignments in parallel corpora have been successfully used as learning evidence (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003). In Statistical Machine Translation (SMT), recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories (Carpuat and Wu, 2007; Chan et al., 2007; Gim´enez and M`arquez, 2007). In this paper, we revisit the one sense per discourse hypothesis using word translations in parallel text as senses. Our first goal is to empirically evaluate whether the one translation per document hypothesis holds on French-English reference corpora, thus verifying whether translations exhibit the same properties as monolingual senses. Our second g"
W09-2404,J03-1002,0,0.00761053,"Missing"
W09-2404,P02-1040,0,0.0941968,"Missing"
W09-2404,N04-3012,0,0.0309238,"Missing"
W09-2404,2007.tmi-papers.28,0,0.0303538,"Missing"
W09-2404,P95-1026,0,0.129316,"variability in SMT lexical choice. More interestingly, cases where the hypothesis does not hold can reveal lexical choice errors. A preliminary study showed that enforcing the one translation per discourse constraint in SMT can potentially improve translation quality, and that SMT systems might benefit from translating sentences within their entire document context. 1 Introduction The one sense per discourse hypothesis formulated by Gale et al. (1992b) has proved to be a simple yet powerful observation and has been successfully used in word sense disambiguation (WSD) and related tasks (e.g., Yarowsky (1995); Agirre and Rigau ∗ The author was partially funded by GALE DARPA Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 19 (1996)). In this paper, we investigate its potential usefulness in the context of machine translation. A growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications. In monolingual WSD, word alignments in parallel corpora hav"
W09-2404,W08-0302,0,\N,Missing
W09-2404,P02-1033,0,\N,Missing
W09-2404,W05-0909,0,\N,Missing
W09-2404,P05-1048,1,\N,Missing
W12-3156,N09-1014,0,0.0206576,"Missing"
W12-3156,J93-2003,0,0.0280353,"ce level, ignoring wider document context. Does this hurt the consistency of translated documents? Using a phrase-based SMT system in various data conditions, we show that SMT translates documents remarkably consistently, even without document knowledge. Nevertheless, translation inconsistencies often indicate translation errors. However, unlike in human translation, these errors are rarely due to terminology inconsistency. They are more often symptoms of deeper issues with SMT models instead. 1 Introduction While Statistical Machine Translation (SMT) models translation at the sentence level (Brown et al., 1993), human translators work on larger translation units. This is partly motivated by the importance of producing consistent translations at the document level. Consistency checking is part of the quality assurance process, and complying with the terminology requirements of each task or client is crucial. In fact, many automatic tools have been proposed to assist humans in this important task (Itagaki et al., 2007; Dagan and Church, 1994, among others). This suggests that wider document-level context information might benefit SMT models. However, we do not have a clear picture of the impact of sen"
W12-3156,W09-2404,1,0.412746,"cument modeling for SMT (Section 2), we describe our corpora in Section 3 and our general methodology in Section 4. In Section 5, we discuss the results of an automatic analysis of translation consistency, before turning to manual analysis in Section 6. 442 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 442–449, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Related work While most SMT systems operate at the sentence level, there is increased interest in modeling document context and consistency in translation. In earlier work (Carpuat, 2009), we investigate whether the “one sense per discourse” heuristic commonly used in word sense disambiguation (Gale et al., 1992) can be useful in translation. We show that “one translation per discourse” largely holds in automatically word-aligned French-English news stories, and that enforcing translation consistency as a simple post-processing constraint can fix some of the translation errors in a phrase-based SMT system. Ture et al. (2012) provide further empirical support by studying the consistency of translation rules used by a hierarchical phrase-based system to force-decode Arabic-Engli"
W12-3156,P12-1098,0,0.0311256,"Missing"
W12-3156,A94-1006,0,0.0773822,"re more often symptoms of deeper issues with SMT models instead. 1 Introduction While Statistical Machine Translation (SMT) models translation at the sentence level (Brown et al., 1993), human translators work on larger translation units. This is partly motivated by the importance of producing consistent translations at the document level. Consistency checking is part of the quality assurance process, and complying with the terminology requirements of each task or client is crucial. In fact, many automatic tools have been proposed to assist humans in this important task (Itagaki et al., 2007; Dagan and Church, 1994, among others). This suggests that wider document-level context information might benefit SMT models. However, we do not have a clear picture of the impact of sentence-based SMT on the translation of full documents. From a quality standpoint, it seems safe to assume that translation consistency is as desirable for SMT as for human translations. However, consistency needs to be balanced with other quality requirements. For instance, strict consistency might result in awkward repetitions that make translations less fluent. From a translation modeling standpoint, while typical SMT systems do not"
W12-3156,2010.amta-papers.24,0,0.103598,"Missing"
W12-3156,H92-1045,0,0.247853,"ion 5, we discuss the results of an automatic analysis of translation consistency, before turning to manual analysis in Section 6. 442 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 442–449, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Related work While most SMT systems operate at the sentence level, there is increased interest in modeling document context and consistency in translation. In earlier work (Carpuat, 2009), we investigate whether the “one sense per discourse” heuristic commonly used in word sense disambiguation (Gale et al., 1992) can be useful in translation. We show that “one translation per discourse” largely holds in automatically word-aligned French-English news stories, and that enforcing translation consistency as a simple post-processing constraint can fix some of the translation errors in a phrase-based SMT system. Ture et al. (2012) provide further empirical support by studying the consistency of translation rules used by a hierarchical phrase-based system to force-decode Arabic-English news documents from the NIST evaluation. Several recent contributions integrate translation consistency models in SMT using"
W12-3156,D11-1084,0,0.348987,"Missing"
W12-3156,2007.mtsummit-papers.36,0,0.0200872,"inconsistency. They are more often symptoms of deeper issues with SMT models instead. 1 Introduction While Statistical Machine Translation (SMT) models translation at the sentence level (Brown et al., 1993), human translators work on larger translation units. This is partly motivated by the importance of producing consistent translations at the document level. Consistency checking is part of the quality assurance process, and complying with the terminology requirements of each task or client is crucial. In fact, many automatic tools have been proposed to assist humans in this important task (Itagaki et al., 2007; Dagan and Church, 1994, among others). This suggests that wider document-level context information might benefit SMT models. However, we do not have a clear picture of the impact of sentence-based SMT on the translation of full documents. From a quality standpoint, it seems safe to assume that translation consistency is as desirable for SMT as for human translations. However, consistency needs to be balanced with other quality requirements. For instance, strict consistency might result in awkward repetitions that make translations less fluent. From a translation modeling standpoint, while ty"
W12-3156,P11-1124,0,0.0269517,"Missing"
W12-3156,W10-2602,0,0.379412,"lation consistency using post-processing and redecoding techniques similar to those introduced in Carpuat (2009) can improve the BLEU score of a Chinese-English system. Ture et al. (2012) also show significant BLEU improvements on Arabic-English and Chinese-English hierarchical SMT systems. During the second decoding pass, Xiao et al. (2011) use only translation frequencies from the first pass to encourage consistency, while Ture et al. (2012) also model word rareness by adapting term weighting techniques from information retrieval. Another line of work focuses on cache-based adaptive models (Tiedemann, 2010a; Gong et al., 2011), which lets lexical choice in a sentence be informed by translations of previous sentences. However, cache-based models are sensitive to error propagation and can have a negative impact on some data sets (Tiedemann, 2010b). Moreover, this approach blurs the line between consistency and domain modeling. In fact, Gong et al. (2011) reports statistically significant improvements in BLEU only when combining pure consistency caches with topic and similarity caches, which do not enforce consistency but essentially perform domain or topic adaptation. There is also work that indi"
W12-3156,W10-1728,0,0.0696175,"lation consistency using post-processing and redecoding techniques similar to those introduced in Carpuat (2009) can improve the BLEU score of a Chinese-English system. Ture et al. (2012) also show significant BLEU improvements on Arabic-English and Chinese-English hierarchical SMT systems. During the second decoding pass, Xiao et al. (2011) use only translation frequencies from the first pass to encourage consistency, while Ture et al. (2012) also model word rareness by adapting term weighting techniques from information retrieval. Another line of work focuses on cache-based adaptive models (Tiedemann, 2010a; Gong et al., 2011), which lets lexical choice in a sentence be informed by translations of previous sentences. However, cache-based models are sensitive to error propagation and can have a negative impact on some data sets (Tiedemann, 2010b). Moreover, this approach blurs the line between consistency and domain modeling. In fact, Gong et al. (2011) reports statistically significant improvements in BLEU only when combining pure consistency caches with topic and similarity caches, which do not enforce consistency but essentially perform domain or topic adaptation. There is also work that indi"
W12-3156,N12-1046,0,0.295652,"Missing"
W12-3156,2011.mtsummit-papers.13,0,0.283077,"anslation per discourse” largely holds in automatically word-aligned French-English news stories, and that enforcing translation consistency as a simple post-processing constraint can fix some of the translation errors in a phrase-based SMT system. Ture et al. (2012) provide further empirical support by studying the consistency of translation rules used by a hierarchical phrase-based system to force-decode Arabic-English news documents from the NIST evaluation. Several recent contributions integrate translation consistency models in SMT using a two-pass decoding approach. In phrase-based SMT, Xiao et al. (2011) show that enforcing translation consistency using post-processing and redecoding techniques similar to those introduced in Carpuat (2009) can improve the BLEU score of a Chinese-English system. Ture et al. (2012) also show significant BLEU improvements on Arabic-English and Chinese-English hierarchical SMT systems. During the second decoding pass, Xiao et al. (2011) use only translation frequencies from the first pass to encourage consistency, while Ture et al. (2012) also model word rareness by adapting term weighting techniques from information retrieval. Another line of work focuses on cac"
W13-0801,W05-0909,0,0.0430885,"ase-based SMT system on the SemEval2010 Cross-Lingual WSD task. This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system. 1 Introduction Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences. Many metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Gim´enez and M´arquez, 2007; Lo and Wu, 2011, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality (Callison-Burch et al., 2010). While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors. When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong. Error anal"
W13-0801,W10-1703,0,0.0300049,"n-grams are much weaker representations of context than the simple templates used by the WSD system. 1 Introduction Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences. Many metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Gim´enez and M´arquez, 2007; Lo and Wu, 2011, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality (Callison-Burch et al., 2010). While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors. When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong. Error analysis can of course be done manually (Vilar et al., 2006), but it is often too slow and expensive to be performed as often as needed during system development. Several metrics have been recently proposed to evaluate specific aspects of translation quality suc"
W13-0801,I05-2021,1,0.685697,"cs, our WSD evaluation is meant to complement global metrics of translation quality. In previous work, intrinsic evaluations of lexical choice have been performed using either semi-automatically constructed data sets 1 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1–10, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics based on MT reference translations (Gim´enez and M`arquez, 2008; Carpuat and Wu, 2008), or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice (Carpuat and Wu, 2005). We will show how existing Cross-Lingual Word Sense Disambiguation tasks (Lefever and Hoste, 2010; Lefever and Hoste, 2013) can be directly seen as machine translation lexical choice (Section 2): their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WSD tasks (Carpuat and Wu, 2005); unlike in MT evaluation settings, human annotators can more easily provide a complete representation of all correct meanings of a word. Second, we show how using this task for evaluating the lexical choice performance of several phrase-based S"
W13-0801,carpuat-wu-2008-evaluation,1,0.932184,"d tasks in order to evaluate whether word meaning is preserved in translation. Let us emphasize that, just like reordering metrics, our WSD evaluation is meant to complement global metrics of translation quality. In previous work, intrinsic evaluations of lexical choice have been performed using either semi-automatically constructed data sets 1 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1–10, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics based on MT reference translations (Gim´enez and M`arquez, 2008; Carpuat and Wu, 2008), or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice (Carpuat and Wu, 2005). We will show how existing Cross-Lingual Word Sense Disambiguation tasks (Lefever and Hoste, 2010; Lefever and Hoste, 2013) can be directly seen as machine translation lexical choice (Section 2): their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WSD tasks (Carpuat and Wu, 2005); unlike in MT evaluation settings, human annotators can more easily provide a complete representation of all correct"
W13-0801,S13-2034,1,0.824721,"Missing"
W13-0801,2011.mtsummit-papers.30,0,0.183147,"trained for English-to-Spanish translation. Its application to the CLWSD task affects the selection of training data and its preprocessing, but the SMT model design and learning strategies are exactly the same as for conventional translation tasks. 3.1 Model We use the NRC’s PORTAGE phrase-based SMT system, which implements a standard phrasal beamsearch decoder with cube pruning. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: phrasal translation probabilites with Kneser-Ney smoothing and ZensNey lexical smoothing in both translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) 2 even though it does not include the length penalty used in the BLEU score. • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty • a Kneser-Ney smoothed 5-gram Spanish language model Weights for these features are learned using a batch version of the MIRA algorithm(Cherry and Foster, 2012). Phrase pairs are extracted from IBM4 a"
W13-0801,P12-1098,0,0.015588,"aluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors. When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong. Error analysis can of course be done manually (Vilar et al., 2006), but it is often too slow and expensive to be performed as often as needed during system development. Several metrics have been recently proposed to evaluate specific aspects of translation quality such as word order (Birch et al., 2010; Chen et al., 2012). While word order is indirectly taken into account by BLEU, TER or METEOR scores, dedicated metrics provide a direct evaluation that lets us understand whether a given system’s reordering performance improved during system development. Word order metrics provide a complementary tool for targeting evaluation and analysis to a specific aspect of machine translation quality. There has not been as much work on evaluating the lexical choice performance of MT: does a MT system preserve the meaning of words in translation? This is of course measured indirectly by commonly used global metrics, but a"
W13-0801,N12-1047,0,0.117684,"cal smoothing in both translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) 2 even though it does not include the length penalty used in the BLEU score. • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty • a Kneser-Ney smoothed 5-gram Spanish language model Weights for these features are learned using a batch version of the MIRA algorithm(Cherry and Foster, 2012). Phrase pairs are extracted from IBM4 alignments obtained with GIZA++(Och and Ney, 2003). We learn phrase translation candidates for phrases of length 1 to 7. Converting the PBSMT output for CLWSD requires a final straightforward mapping step. We use the phrasal alignment between SMT input and output to isolate the translation candidates for the CLWSD target word. When it maps to a multiword phrase in the target language, we use the word within the phrase that has the highest translation IBM1 translation probability given the CLWSD target word of interest. Note that there is no need to perfor"
W13-0801,N09-1025,0,0.0602253,"Missing"
W13-0801,W04-0802,0,0.0368173,"g all content words in running text (Palmer et al., 2001), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (Mihalcea et al., 2004; Lefever and Hoste, 2010). • language: English is by far the most studied language, but the disambiguation of words in other languages such as Chinese (Jin et al., 2007) has been considered. • sense inventory: many tasks use WordNet senses (Fellbaum, 1998), but other sense repre2 sentations have been used, including alternate semantic databases such as HowNet (Dong, 1998), or lexicalizations in one or more languages (Chklovski et al., 2004). The Cross-Lingual Word Sense Disambiguation (CLWSD) task introduced at a recent edition of SemEval (Lefever and Hoste, 2010) is an English lexical sample task that uses translations in other European languages as a sense inventory. As a result, it is particularly well suited to evaluating machine translation lexical choice. 2.1 Translations as Word Sense Representations The CLWSD task is essentially the same task as MT lexical choice: given English target words in context, systems are asked to predict translations in other European languages. The gold standard consists of translations propos"
W13-0801,W10-1751,0,0.0275164,"valuating MT on a HowNet-based Chinese WSD task, where Chinese sentences were manually annotated with HowNet senses which were completely unrelated to the parallel corpus used for training the SMT system. Using CLWSD as an evaluation of MT lexical choice solves this issue and provides controlled learning conditions. 2.3 CLWSD evaluates the semantic adequacy of MT lexical choice A key challenge in MT evaluation lies in deciding whether the meaning of the translation is correct when it does not exactly match the reference translation. METEOR uses WordNet synonyms and learned paraphrases tables (Denkowski and Lavie, 2010). MEANT uses vector-space based lexical similarity scores (Lo et al., 2012). While these methods lead to higher correlations with human judgements on average, they are not ideal for a fine-grained evaluation of lexical choice: similarity scores are defined independently of context and 3 might give credit to incorrect translations (Carpuat et al., 2012). In contrast, CLWSD solves this difficult problem by providing all correct translation candidates in context according to several human annotators. These multiple translations provide a more complete representation of the correct meaning of each"
W13-0801,N12-1017,0,0.0278343,"ors. These multiple translations provide a more complete representation of the correct meaning of each occurrence of a word in context. The CLWSD annotation procedure is designed to easily let human annotators provide many correct translation alternatives for a word. Producing many correct annotations for a complete sentence is a much more expensive undertaking: crowdsourcing can help alleviate the cost of obtaining a small number of reference translation (Zbib et al., 2012), but acquiring a complete representation of all possible translations of a source sentence is a much more complex task (Dreyer and Marcu, 2012). Machine translation evaluations typically use between one and four reference translations, which provide a very incomplete representation of the correct semantics of the input sentence in the output language. CLWSD provides a more complete representation through the multiple gold translations available. 2.4 Limitations The main drawback of using CLWSD to evaluate lexical choice is that CLWSD is a lexical sample task, which only evaluates disambiguation of 20 English nouns. This arbitrary sample of words does not let us target words or phrases that might be specifically interesting for MT. In"
W13-0801,S01-1001,0,0.0465395,"of all correct meanings of a word. Second, we show how using this task for evaluating the lexical choice performance of several phrase-based SMT systems (PBSMT) gives some insights into their strengths and weaknesses (Section 5). 2 Selecting a Word Sense Disambiguation Task to Evaluate MT Lexical Choice Word Sense Disambiguation consists in determining the correct sense of a word in context. This challenging problem has been studied from a rich variety of persectives in Natural Language Processing (see Agirre and Edmonds (2006) for an overview.) The Senseval and SemEval series of evaluations (Edmonds and Cotton, 2001; Mihalcea and Edmonds, 2004; Agirre et al., 2007) have driven the standardization of methodology for evaluating WSD systems. Many shared tasks were organized over the years, providing evaluation settings that vary along several dimensions, including: • target vocabulary: in all word tasks, systems are expected to tag all content words in running text (Palmer et al., 2001), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (Mihalcea et al., 2004; Lefever and Hoste, 2010). • language: English is by far the most studied language, but the disambiguat"
W13-0801,W02-0811,0,0.0384243,"sed, training data and preprocessing pipelines. In order to control for these parameters, we build a WSD system trained on the exact same training corpus, preprocessing and word alignment as the SMT system described above. We cast WSD as a generic ranking problem with linear models. Given a word in context x, translation candidates t are ranked according to the following P model: f (x, t) = i λi φi (x, t), where φi (x, t) represent binary features that fire when specific clues are observed in a context x. Context clues are based on standard feature templates in many supervised WSD approaches (Florian et al., 2002; van Gompel, 2010; Lefever et al., 2011): • words in a window of 2 words around the disambiguation target. • part-of-speech tags in a window of 2 words around the disambiguation target • bag-of-word context: all nouns, verbs and adjectives in the context x At training time, each example (x, t) is assigned a cost based on the translation observed in parallel corpora: f (x, t) = 0 if t = taligned , f (x, t) = 1 otherwise . Feature weights λi can be learned in many ways. We optimize logistic loss using stochastic gradient descent3 . 4.2 Data The training instances for the supervised WSD system a"
W13-0801,W07-0717,0,0.0265201,"nslation performance by removing noisy phrase pairs. 9 Impact of training corpus Since increasing the amount of training data is a reliable way to improve translation performance, we evaluate the impact of training the PBSMT system on more than the Europarl data used for controlled comparison with WSD. We increase the parallel training corpus with the WMT-12 News Commentary parallel data 4 . This yields an additional training set of roughly160k sentence pairs. We build linear mixture models to combine translation, reordering and language models learned on Europarl and News Commentary corpora (Foster and Kuhn, 2007). As can be seen in Table 6, this approach improves all CLWSD scores except for 1-gram precision. The decrease in 1-gram precision indicates that the addition of the news corpus introduces new translation candidates that differ from those used in the gold inventory. Interestingly, the additional data is not sufficient to match the performance of the WSD system learned on Europarl only (see Table 2). While additional data should be used when available, richer context features are valuable to make the most of existing data. 4 http://www.statmt.org/wmt12/translation-task.html System Europarl + Ne"
W13-0801,D08-1089,0,0.317567,"e as for conventional translation tasks. 3.1 Model We use the NRC’s PORTAGE phrase-based SMT system, which implements a standard phrasal beamsearch decoder with cube pruning. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: phrasal translation probabilites with Kneser-Ney smoothing and ZensNey lexical smoothing in both translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) 2 even though it does not include the length penalty used in the BLEU score. • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty • a Kneser-Ney smoothed 5-gram Spanish language model Weights for these features are learned using a batch version of the MIRA algorithm(Cherry and Foster, 2012). Phrase pairs are extracted from IBM4 alignments obtained with GIZA++(Och and Ney, 2003). We learn phrase translation candidates for phrases of length 1 to 7. Converting the PBSMT output for CLWSD requires a final straightforward mapping step. We use the"
W13-0801,W07-0738,0,0.0791048,"Missing"
W13-0801,S07-1004,0,0.0196477,") have driven the standardization of methodology for evaluating WSD systems. Many shared tasks were organized over the years, providing evaluation settings that vary along several dimensions, including: • target vocabulary: in all word tasks, systems are expected to tag all content words in running text (Palmer et al., 2001), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (Mihalcea et al., 2004; Lefever and Hoste, 2010). • language: English is by far the most studied language, but the disambiguation of words in other languages such as Chinese (Jin et al., 2007) has been considered. • sense inventory: many tasks use WordNet senses (Fellbaum, 1998), but other sense repre2 sentations have been used, including alternate semantic databases such as HowNet (Dong, 1998), or lexicalizations in one or more languages (Chklovski et al., 2004). The Cross-Lingual Word Sense Disambiguation (CLWSD) task introduced at a recent edition of SemEval (Lefever and Hoste, 2010) is an English lexical sample task that uses translations in other European languages as a sense inventory. As a result, it is particularly well suited to evaluating machine translation lexical choic"
W13-0801,D07-1103,0,0.027897,"BSMT 23.72 23.69 45.49 Number t of translations per phrase t = 20 23.68 23.63 45.66 t = 100 23.65 23.60 45.65 Other phrase-table pruning methods stat sig 23.71 23.66 45.19 Mode Rec. 45.37 BLEU1 62.72 45.54 45.53 62.32 62.52 45.07 62.62 Table 5: Impact of translation candidate selection on PBSMT performance each source phrase in the phrase-table. The main PBSMT system uses t = 50 translation candidates per source phrase. Limiting that number to 20 and increasing it to 100 both have a very small impact on CLWSD. Second, we prune the phrase-table using a statistical significance test to measure (Johnson et al., 2007). This pruning strategy aims to drastically decrease the size of the phrase-table without degrading translation performance by removing noisy phrase pairs. 9 Impact of training corpus Since increasing the amount of training data is a reliable way to improve translation performance, we evaluate the impact of training the PBSMT system on more than the Europarl data used for controlled comparison with WSD. We increase the parallel training corpus with the WMT-12 News Commentary parallel data 4 . This yields an additional training set of roughly160k sentence pairs. We build linear mixture models t"
W13-0801,P03-1040,0,0.0281564,"4-gram 23.89 23.84 Impact of PBSMT Context Models What is the impact of PBSMT context models on lexical choice accuracy? Table 3 provides an overview of experiments where we vary the context size available to the PBSMT system. The main PB6 SMT system in the top row uses the default settings presented in Section 3. In the first set of experiments, we evaluate the impact of the source side context on CLWSD performance. Phrasal translations represent the core of PBSMT systems: they capture collocational context in the source language, and they are therefore are less ambiguous than single words (Koehn and Knight, 2003; Koehn et al., 2003). The default PBSMT learns translations for sources phrases of length ranging from 1 to 7 words. Limiting the PBSMT system to translate shorter phrases (Rows l = 1 and l = 3 in Table 3) surprisingly improves CLWSD performance, even though it degrades BLEU score on WMT test sets. The source context captured by longer phrases therefore does not provide the right disambiguating information in this context. In the second set of experiments, we evaluate the impact of the context size in the target language, by varying the size of the n-gram language model used. The default PBSM"
W13-0801,N03-1017,0,0.0133379,"ct of PBSMT Context Models What is the impact of PBSMT context models on lexical choice accuracy? Table 3 provides an overview of experiments where we vary the context size available to the PBSMT system. The main PB6 SMT system in the top row uses the default settings presented in Section 3. In the first set of experiments, we evaluate the impact of the source side context on CLWSD performance. Phrasal translations represent the core of PBSMT systems: they capture collocational context in the source language, and they are therefore are less ambiguous than single words (Koehn and Knight, 2003; Koehn et al., 2003). The default PBSMT learns translations for sources phrases of length ranging from 1 to 7 words. Limiting the PBSMT system to translate shorter phrases (Rows l = 1 and l = 3 in Table 3) surprisingly improves CLWSD performance, even though it degrades BLEU score on WMT test sets. The source context captured by longer phrases therefore does not provide the right disambiguating information in this context. In the second set of experiments, we evaluate the impact of the context size in the target language, by varying the size of the n-gram language model used. The default PBSMT system used a 5-gra"
W13-0801,2005.mtsummit-papers.11,0,0.0303466,"errors which Mr Cash expresses about our future in the community have a familiar ring about them. sonar (3);tinte (3);connotaci´on(2);tono (1); The American containment ring around the Soviet bloc had been seriously breached only by the Soviet acquisition of military facilities in Cuba. cerco (2);c´ırculo (2);cord´on (2);barrera (1);blindaje (1);limitaci´on (1); Table 1: Example of annotated CLWSD instances from the SemEval 2010 test set. For each gold Spanish translation, we are given the number of annotators who proposed it (out of 3 annotators.) sentences from the Europarl parallel corpus (Koehn, 2005). These translations are then manually clustered into senses. When constructing the gold annotation, human annotators are given occurrences of target words in context. For each occurrence, they select a sense cluster and provide all translations from this cluster that are correct in this specific context. Since three annotators contribute, each test occurrence is therefore tagged with a set of translations in another language, along with a frequency which represents the number of annotators who selected it. A more detailed description of the annotation process can be found in (Lefever and Host"
W13-0801,S10-1003,0,0.346563,"work, intrinsic evaluations of lexical choice have been performed using either semi-automatically constructed data sets 1 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1–10, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics based on MT reference translations (Gim´enez and M`arquez, 2008; Carpuat and Wu, 2008), or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice (Carpuat and Wu, 2005). We will show how existing Cross-Lingual Word Sense Disambiguation tasks (Lefever and Hoste, 2010; Lefever and Hoste, 2013) can be directly seen as machine translation lexical choice (Section 2): their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WSD tasks (Carpuat and Wu, 2005); unlike in MT evaluation settings, human annotators can more easily provide a complete representation of all correct meanings of a word. Second, we show how using this task for evaluating the lexical choice performance of several phrase-based SMT systems (PBSMT) gives some insights into their strengths and weaknesses (Section 5). 2 Selectin"
W13-0801,P11-2055,0,0.148166,"Missing"
W13-0801,P11-1023,0,0.049161,"SD task. This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system. 1 Introduction Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences. Many metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Gim´enez and M´arquez, 2007; Lo and Wu, 2011, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality (Callison-Burch et al., 2010). While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors. When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong. Error analysis can of course be done manually (Vilar et"
W13-0801,W12-3129,0,0.0198454,"y annotated with HowNet senses which were completely unrelated to the parallel corpus used for training the SMT system. Using CLWSD as an evaluation of MT lexical choice solves this issue and provides controlled learning conditions. 2.3 CLWSD evaluates the semantic adequacy of MT lexical choice A key challenge in MT evaluation lies in deciding whether the meaning of the translation is correct when it does not exactly match the reference translation. METEOR uses WordNet synonyms and learned paraphrases tables (Denkowski and Lavie, 2010). MEANT uses vector-space based lexical similarity scores (Lo et al., 2012). While these methods lead to higher correlations with human judgements on average, they are not ideal for a fine-grained evaluation of lexical choice: similarity scores are defined independently of context and 3 might give credit to incorrect translations (Carpuat et al., 2012). In contrast, CLWSD solves this difficult problem by providing all correct translation candidates in context according to several human annotators. These multiple translations provide a more complete representation of the correct meaning of each occurrence of a word in context. The CLWSD annotation procedure is designe"
W13-0801,W04-0807,0,0.0462978,"ing (see Agirre and Edmonds (2006) for an overview.) The Senseval and SemEval series of evaluations (Edmonds and Cotton, 2001; Mihalcea and Edmonds, 2004; Agirre et al., 2007) have driven the standardization of methodology for evaluating WSD systems. Many shared tasks were organized over the years, providing evaluation settings that vary along several dimensions, including: • target vocabulary: in all word tasks, systems are expected to tag all content words in running text (Palmer et al., 2001), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (Mihalcea et al., 2004; Lefever and Hoste, 2010). • language: English is by far the most studied language, but the disambiguation of words in other languages such as Chinese (Jin et al., 2007) has been considered. • sense inventory: many tasks use WordNet senses (Fellbaum, 1998), but other sense repre2 sentations have been used, including alternate semantic databases such as HowNet (Dong, 1998), or lexicalizations in one or more languages (Chklovski et al., 2004). The Cross-Lingual Word Sense Disambiguation (CLWSD) task introduced at a recent edition of SemEval (Lefever and Hoste, 2010) is an English lexical sample"
W13-0801,W09-2412,0,0.0660104,"Missing"
W13-0801,P10-2041,0,0.0150964,"1 alignments that exist in all the languages considered in CLWSD were used (Lefever and Hoste, 2010). We exploit additional corpora from the WMT2012 translation task, using the full Europarl corpus to train language models, and for one experiment the news-commentary parallel corpus (see Section 9.) These parallel corpora are used to learn the translation, reordering and language models. The loglinear feature weights are learned on a development set of 3000 sentences sampled from the WMT2012 development test sets. They are selected based on their distance to the CLWSD trial and test sentences (Moore and Lewis, 2010). We tokenize and lemmatize all English and Spanish text using the FreeLing tools (Padr´o and 5 Stanilovsky, 2012). We use lemma representations to perform translation, since the CLWSD targets and translations are lemmatized. 4 WSD system 4.1 Model We also train a dedicated WSD system for this task in order to perform a controlled comparison with the SMT system. Many WSD systems have been evaluated on the SemEval test bed used here, however, they differ in terms of resources used, training data and preprocessing pipelines. In order to control for these parameters, we build a WSD system trained"
W13-0801,W11-2124,0,0.0246456,"this context. In the second set of experiments, we evaluate the impact of the context size in the target language, by varying the size of the n-gram language model used. The default PBSMT system used a 5-gram language model. Reducing the n-gram order to 3, 2, 1 and increasing it to 7 both degrade performance. Shorter n-grams do not provide enough disambiguating context, while longer n-grams are more sparse and perhaps do not generalize well outside of the training corpus. Finally, we report a last experiment which uses a bilingual language model to enrich the context representation in PBSMT (Niehues et al., 2011). This language model is estimated on word pairs formed System + hier + lex dist Prec. 23.72 23.69 23.42 Rec. 23.69 23.64 23.37 Mode Prec. 45.49 46.66 45.43 Mode Rec. 45.37 46.54 45.30 BLEU1 62.72 62.22 62.22 Table 4: Impact of reordering models: lexicalized reodering does not hurt lexical choice only when hierarchical models are used by target words augmented with their aligned source words. We use a 4-gram model, trained using GoodTuring discounting. This only results in small improvements (< 0.1) over the standard PBSMT system, and remains far below the performance of the dedicated WSD syst"
W13-0801,J03-1002,0,0.00640181,"eordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) 2 even though it does not include the length penalty used in the BLEU score. • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty • a Kneser-Ney smoothed 5-gram Spanish language model Weights for these features are learned using a batch version of the MIRA algorithm(Cherry and Foster, 2012). Phrase pairs are extracted from IBM4 alignments obtained with GIZA++(Och and Ney, 2003). We learn phrase translation candidates for phrases of length 1 to 7. Converting the PBSMT output for CLWSD requires a final straightforward mapping step. We use the phrasal alignment between SMT input and output to isolate the translation candidates for the CLWSD target word. When it maps to a multiword phrase in the target language, we use the word within the phrase that has the highest translation IBM1 translation probability given the CLWSD target word of interest. Note that there is no need to perform any manual mapping between SMT output and sense inventories as in (Carpuat and Wu, 2005"
W13-0801,padro-stanilovsky-2012-freeling,0,0.0302072,"Missing"
W13-0801,S01-1005,0,0.0162423,"ord in context. This challenging problem has been studied from a rich variety of persectives in Natural Language Processing (see Agirre and Edmonds (2006) for an overview.) The Senseval and SemEval series of evaluations (Edmonds and Cotton, 2001; Mihalcea and Edmonds, 2004; Agirre et al., 2007) have driven the standardization of methodology for evaluating WSD systems. Many shared tasks were organized over the years, providing evaluation settings that vary along several dimensions, including: • target vocabulary: in all word tasks, systems are expected to tag all content words in running text (Palmer et al., 2001), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (Mihalcea et al., 2004; Lefever and Hoste, 2010). • language: English is by far the most studied language, but the disambiguation of words in other languages such as Chinese (Jin et al., 2007) has been considered. • sense inventory: many tasks use WordNet senses (Fellbaum, 1998), but other sense repre2 sentations have been used, including alternate semantic databases such as HowNet (Dong, 1998), or lexicalizations in one or more languages (Chklovski et al., 2004). The Cross-Lingual Word Sense Dis"
W13-0801,P02-1040,0,0.0944208,"applying a standard phrase-based SMT system on the SemEval2010 Cross-Lingual WSD task. This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system. 1 Introduction Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences. Many metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Gim´enez and M´arquez, 2007; Lo and Wu, 2011, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality (Callison-Burch et al., 2010). While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors. When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or w"
W13-0801,S10-1053,0,0.245064,"Missing"
W13-0801,H05-1097,0,0.20021,"f using CLWSD to evaluate lexical choice is that CLWSD is a lexical sample task, which only evaluates disambiguation of 20 English nouns. This arbitrary sample of words does not let us target words or phrases that might be specifically interesting for MT. In addition, the data available through the shared task does not let us evaluate complete translations of the CLWSD test sentences, since full references translations are not available. Instead of using a WSD dataset for MT purposes, we could take the converse approach andautomatically construct a WSD test set based on MT evaluation corpora (Vickrey et al., 2005; Gim´enez and M`arquez, 2008; Carpuat and Wu, 2008; Carpuat et al., 2012). However, this approach suffers from noisy automatic alignments between source and reference, as well as from a limited representation of the correct meaning of words in context due to the limited number of reference translations. Other SemEval tasks such as the Cross-Lingual Lexical Substitution Task (Mihalcea et al., 2010) would also provide an appropriate test bed. We focused on the CLWSD task, since it uses senses drawn from the Europarl parallel corpus, and therefore offers more constrained settings for comparison"
W13-0801,vilar-etal-2006-error,0,0.088129,"Missing"
W13-0801,N12-1006,0,0.016767,"contrast, CLWSD solves this difficult problem by providing all correct translation candidates in context according to several human annotators. These multiple translations provide a more complete representation of the correct meaning of each occurrence of a word in context. The CLWSD annotation procedure is designed to easily let human annotators provide many correct translation alternatives for a word. Producing many correct annotations for a complete sentence is a much more expensive undertaking: crowdsourcing can help alleviate the cost of obtaining a small number of reference translation (Zbib et al., 2012), but acquiring a complete representation of all possible translations of a source sentence is a much more complex task (Dreyer and Marcu, 2012). Machine translation evaluations typically use between one and four reference translations, which provide a very incomplete representation of the correct semantics of the input sentence in the output language. CLWSD provides a more complete representation through the multiple gold translations available. 2.4 Limitations The main drawback of using CLWSD to evaluate lexical choice is that CLWSD is a lexical sample task, which only evaluates disambiguati"
W13-0801,W09-2413,0,\N,Missing
W13-0801,P07-2045,0,\N,Missing
W13-0801,S10-1002,0,\N,Missing
W13-1712,C12-1025,0,0.0428067,"e, the word “at” will produce two trigrams: “ at” and “at “. These features allow us to capture for example typical spelling variants. In a language with weak morphology such as English, they may also be able to capture patterns of usage of, e.g. suffixes, which provides a low-cost proxy for syntactic information. Word ngrams: We index unigrams and bigrams of words within each sentence. For bigrams, the beginning and end of a sentence are treated as special 97 tokens. Note that we do not apply any stoplist filtering. As a consequence, function words, an oftenused feature (Koppel et al., 2005; Brooke and Hirst, 2012), are naturally included in the unigram feature space. Spelling features: Misspelled words are identified using GNU Aspell V0.60.41 and indexed with their counts. Some parser artifacts such as “n’t” are removed from the final mispelled word index. Although misspellings may seem to provide clues as to the author’s native language, we did not find these features to be useful in any of our experiments. Note however, that misspelled words will also appear in the unigram feature space. Part-of-speech ngrams: The texts were tagged with the Stanford tagger v. 3.02 using the largest and best (bidirect"
W13-1712,W13-1706,0,0.319996,"a majority vote between classifiers. Somewhat surprisingly, a classifier relying on purely lexical features performed very well and proved difficult to outperform significantly using various combinations of feature spaces. However, the combination of multiple predictors allowed to exploit their different strengths and provided a significant boost in performance. 1 Marine Carpuat National Research Council 1200 Montreal Rd, Ottawa, ON K1A 0R6 Marine.Carpuat@nrc.ca Introduction We describe the National Research Council Canada’s submissions to the Native Language Identification 2013 shared task (Tetreault et al., 2013). Our submissions rely on fairly straightforward statistical modelling techniques, applied to various feature spaces representing lexical and syntactic information. Our most successful submission was actually a combination of models trained on different sets of feature spaces using a simple majority vote. Much of the work on Natural Language Processing is motivated by the desire to have machines that can help or replace humans on language-related tasks. Many tasks such as topic or genre classification, entity extraction, disambiguation, are fairly 2 Modelling Our submissions rely on straightfo"
W14-3363,D11-1033,0,0.0376008,"ain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilit"
W14-3363,2011.iwslt-evaluation.18,0,0.0374739,"data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encoura"
W14-3363,2011.mtsummit-papers.30,1,0.896794,"that since our training data is very heterogeneous, we take into account other dimensions beyond provenance, such as genre and dialect information (Section 4). Previous work provides empirical evidence supporting this. For instance, Foster et al. (2010) found that linear mixtures outperform log linear mixtures when adapting a French-English system to the medical domain, as well as on a ChineseEnglish NIST translation task. 2.4 Estimating Conditional Translation Probabilities Within each mixture component, we extract all phrase-pairs, compute relative frequencies, and use Kneser-Ney smoothing (Chen et al., 2011) to produce the final estimate of conditional translation probabilities pk (t|s). Per-component probabilities are then combined in Eq. 1 and 3. Similarly, baseline translation probabilities are learned using Kneser-Ney smoothed frequencies collected on the entire training set. 3 3.2 We propose to use automatic text clustering techniques to organize basic elements into homogeneous clusters that are seen as sub-domains. In our experiments, we apply clustering algorithms to the target (English) side of the corpus only. Each corpus element is transformed into a vector-space format by constructing"
W14-3363,2012.amta-papers.4,0,0.0728245,"main at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encourage us to rethink the use of mixture models, and opens up new ways of conceptualizing"
W14-3363,E12-1055,0,0.377042,"ranslation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks. 1 • Should mixture component capture domain information? Previous work assumes that training data should be organized into domains. When manual domain distinctions are not available, previous work uses clustering approaches to approximate manual domain distinctions (Sennrich, 2012a). However, it is unclear whether it is necessary to use or mimic domain distinctions in order to define mixture components. • Mixture models are usually assumed to improve translation quality by giving more weight to parts of the training corpus that are more relevant to the test domain. Is this intuition still valid in our more complex heterogeneous training conditions? If not, how do mixture models affect translation probability estimates? Introduction While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems hav"
W14-3363,W07-0717,1,0.740538,"ng bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequ"
W14-3363,D10-1044,1,0.94741,"iform mixtures where all components are weighted equally: Linear Mixtures for Translation Models K X p˜(s, t) log We use the Expectation Maximization algorithm to solve this maximization problem. Does domain knowledge yield better translation quality when learning linear mixture weights for the translation model of a phrase-based MT system? We leave the study of linear mixtures for language and reordering models for future work. 2.1 X (1) k=1 where pk (t|s) is a conditional translation probability learned on subset k of the training corpus. 500 pean parliament proceedings and movie subtitles. Foster et al. (2010) work with a slightly different setting when defining mixture components for the NIST Chinese-English translation task: while there is no single obvious “in-domain” component in the NIST training set, homogeneous domains can still be defined in a straightforward fashion based on the provenance of the data (e.g., Hong Kong Hansards vs. Hong Kong Law vs. News articles from FBIS, etc.). We take a similar approach in our experiments. However, we will see that since our training data is very heterogeneous, we take into account other dimensions beyond provenance, such as genre and dialect informatio"
W14-3363,2012.amta-papers.18,0,0.0177864,"an global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2"
W14-3363,2013.mtsummit-papers.23,1,0.626478,"ceedings and movie subtitles. We address training conditions where the dimensions for organizing the training data are not as clear-cut, and show that partitions that do not attempt to mimick domain distinctions can improve translation quality. It would be interesting to see whether our conclusion holds in these more artificial training settings, and whether sentence-level corpus organization could help translation quality in our settings. Finally, recent work shows that linear mixture weights can be optimized for BLEU, either directly (Haddow, 2013), or by simulating discriminative training (Foster et al., 2013). In this paper, we limited our studies to maximum likelihood and uniform mixtures, however, the various mixture component definitions proposed here can also be applied when maximizing BLEU. 8 Acknowledgments This research was supported in part by DARPA contract HR0011-12-C-0014 under subcontract to Raytheon BBN Technologies. The authors would like to thank the reviewers and the PORTAGE group at the National Research Council. Conclusion We have presented an extensive study of linear mixtures for training translation models on very heterogeneous data on Arabic-English and Chinese-English transl"
W14-3363,D07-1054,0,0.0324476,"rgences for very frequent source phrases. Overall, the linear mixtures result in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain dat"
W14-3363,D08-1089,0,0.0542947,"● ● 0 20 40 60 corpus component 80 #lines #words 100 4.4 Figure 1: Sizes of the 82 Arabic-English (top) and 101 Chinese-English (bottom) corpus components. 4.2 We use an in-house implementation of a Phrasebased Statistical Machine Translation system (Koehn et al., 2007) to build strong baseline systems for both language pairs. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: Kneser-Ney smoothed phrasal translation probabilites and lexical weights, in both translation directions (Chen et al., 2011)2 • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and a word-displacement distortion penalty • a Good-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5-gram model trained on monolingual webforum data. Weights for these features are learned using a batch version of the MIRA algorithm (Chiang, 2012). Phrase pairs are extracted from several word alignments of the training set: HMM, IBM2, and IBM4. Word alignments are kept constant across all experiments. We apply our linear mixture models to both transl"
W14-3363,W12-3154,0,0.0153077,"er at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encourage us to rethink the use"
W14-3363,N13-1035,0,0.464864,"see in Section 4. We consider four very different ways of defining mixture components by grouping the basic corpus elements: (1) manual partition of the training corpus into domains, (2) automatically learning homogeneous domains using text clustering algorithms, (3) random partitioning, (4) sampling with replacement. 3.1 Induced Domains Using Automatic Clustering Algorithms Manually Defined Domains Heterogeneous training data is usually grouped into domains manually using provenance information. In most previous work, such domain distinctions are very clear and easy to define. For instance, Haddow (2013) uses European parliament proceedings to improve translation of text in the movie subtitles and News Commentary domains; Sennrich (2012a) aims to translate Alpine Club reports using components trained on Euro3.3 Random Partitioning We consider random partitions of the training corpus. They are generated by using a random number generator to assign each basic element to one of K clusters. Resulting components therefore do not capture any domain information. Each com501 Arabic-English Training Conditions segs src en train 8.5M 262M 207M Test Domain 1: Webforum segs src en dev (tune) 4.1k 66k 72k"
W14-3363,2005.eamt-1.19,0,0.044844,"inctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small numb"
W14-3363,W07-0733,0,0.0538177,"uster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with"
W14-3363,P07-2045,0,0.0073128,"● ● ●● ● ● ●● ● ●●●●●●● ●●● ● ● ●●●●●●●●●●●● ●●●●●● ● ●●●●●●●●●●●●●●● ● ● ●●●●●●●●●● ● ● ● ● ● ● ● ●●●●● ● ● 0 20 40 60 corpus component #lines #words 80 #lines / #words 1e+03 1e+05 1e+07 Chinese corpus components ● ●●●●● ● ●●●● ● ● ● ● ● ● ●●●●● ●●●●●●●●● ● ●●● ●● ●●● ●●●● ● ●● ●● ● ● ●● ●●●●●●● ● ●● ● ●●●●●●●●●●●●● ● ●●●● ●● ●●●●●●●●●●● ● ●●● ● ● ● 0 20 40 60 corpus component 80 #lines #words 100 4.4 Figure 1: Sizes of the 82 Arabic-English (top) and 101 Chinese-English (bottom) corpus components. 4.2 We use an in-house implementation of a Phrasebased Statistical Machine Translation system (Koehn et al., 2007) to build strong baseline systems for both language pairs. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: Kneser-Ney smoothed phrasal translation probabilites and lexical weights, in both translation directions (Chen et al., 2011)2 • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and a word-displacement distortion penalty • a Good-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5"
W14-3363,2012.eamt-1.43,0,0.555765,"ranslation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks. 1 • Should mixture component capture domain information? Previous work assumes that training data should be organized into domains. When manual domain distinctions are not available, previous work uses clustering approaches to approximate manual domain distinctions (Sennrich, 2012a). However, it is unclear whether it is necessary to use or mimic domain distinctions in order to define mixture components. • Mixture models are usually assumed to improve translation quality by giving more weight to parts of the training corpus that are more relevant to the test domain. Is this intuition still valid in our more complex heterogeneous training conditions? If not, how do mixture models affect translation probability estimates? Introduction While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems hav"
W14-3363,2010.iwslt-papers.5,0,\N,Missing
W14-3913,P11-1061,0,0.0317143,"Missing"
W14-3913,J90-2002,0,0.357384,"Missing"
W14-3913,eisele-chen-2010-multiun,0,0.0656469,"Missing"
W14-3913,P99-1043,0,0.0955364,"le many correctly detected mixed language segments are due to borrowings, use of organization names or titles in the other language, we do find examples of code switching such as: 5 Related Work To the best of our knowledge, this is the first study of mixed language and code-switching in the Canadian Hansard parallel corpus, a very large parallel corpus commonly used to build generic machine translation systems. Previous work at the intersection of machine translation and mixed languages has focused on specific application scenarios: word translation disambiguation for mixed language queries (Fung et al., 1999), or building applications to help second language learners, such as translating of short L1 phrases in sentences that are predominantly • quotes • multiword expressions or idioms, • politeness formulas and formality. The distribution of code-switching across these categories is very different for French and En112 Quote Translation Politeness Idioms/MWEs Title Organization Other Quote Translation Politeness Idioms/MWEs Borrowing Organization Other Use of English in primarily French segments [FR C’ est e´ crit] “[EN will have full access] ” [FR Vous avez dit et je vous cite] “ [EN we do not hav"
W14-3913,E14-1001,0,0.0141438,"o go to them first] [EN The same committee rejected an amendment] [FR propos´e par le Bloc qu´eb´ecois propos´e par moi pour le NPD] [EN This is not the current government] [FR C est la mˆeme chose] [EN it doesn t matter which one is in power] Table 8: Examples of mixed language segments 113 L26 (van Gompel and van den Bosch, 2014), or on detecting code-mixing to let an email translation system handle words created on the fly by bilingual English-Spanish speakers (Manandise and Gdaniec, 2011). While code-switched data is traditionally viewed as noise when training machine translation systems, Huang and Yates (2014) showed that appropriately detecting codeswitching can help inform word alignment and improve machine translation quality. There has been renewed interest on the study of mixed language recently, focusing on detecting code-switching points (Solorio and Liu, 2008; Elfardy et al., 2013) and more generally detecting mixed language documents. Lui et al. (2014) use a generative mixture model reminiscent of Latent Dirichlet Allocation to detect mixed language documents and the languages inside them. Unlike the CRF-based approach of King and Abney (2013), the languages involved do not need to be know"
W14-3913,N13-1131,0,0.313719,"el the French and English words that compose them, in order to enable further processing. These two goals can be accomplished simultaneously by a word-level language tagger. In a second stage, the automatically detected mixed language segments are used to manually study code-switching, since our mixed language tagger does not yet distinguish between codeswitching and other types of mixed language (e.g., borrowings). 3.2 Challenges When the identity of the languages mixed is known, the state-of-the-art approach to word-level language identification is the weakly supervised approach proposed by King and Abney (2013). They frame the task as a sequence labeling problem with monolingual text samples for training data. A Conditional Random Field (CRF) trained with generalized expectation criteria performs best, when evaluated on a corpus comprising 30 languages, including many low resources languages such as Azerbaijani or Ojibwa. In our case, there are only two high-resource languages involved, which could make the language detection task easier. However, the Hansard domain also presents many challenges: English and French are closely related languages and share many words; the Hansard corpus contains many"
W14-3913,2005.mtsummit-papers.11,0,0.103069,"Missing"
W14-3913,Q14-1003,0,0.0245221,"to let an email translation system handle words created on the fly by bilingual English-Spanish speakers (Manandise and Gdaniec, 2011). While code-switched data is traditionally viewed as noise when training machine translation systems, Huang and Yates (2014) showed that appropriately detecting codeswitching can help inform word alignment and improve machine translation quality. There has been renewed interest on the study of mixed language recently, focusing on detecting code-switching points (Solorio and Liu, 2008; Elfardy et al., 2013) and more generally detecting mixed language documents. Lui et al. (2014) use a generative mixture model reminiscent of Latent Dirichlet Allocation to detect mixed language documents and the languages inside them. Unlike the CRF-based approach of King and Abney (2013), the languages involved do not need to be known ahead of time. In contrast with all these approaches, we work with parallel data with unbalanced original languages. language sentences, this work suggests that the proceedings of multilingual organizations such as the Canadian Hansard can provide interesting test beds for (1) corpus-based study of language choice and code-switching, which can complement"
W14-3913,D08-1102,0,0.0197239,"f mixed language segments 113 L26 (van Gompel and van den Bosch, 2014), or on detecting code-mixing to let an email translation system handle words created on the fly by bilingual English-Spanish speakers (Manandise and Gdaniec, 2011). While code-switched data is traditionally viewed as noise when training machine translation systems, Huang and Yates (2014) showed that appropriately detecting codeswitching can help inform word alignment and improve machine translation quality. There has been renewed interest on the study of mixed language recently, focusing on detecting code-switching points (Solorio and Liu, 2008; Elfardy et al., 2013) and more generally detecting mixed language documents. Lui et al. (2014) use a generative mixture model reminiscent of Latent Dirichlet Allocation to detect mixed language documents and the languages inside them. Unlike the CRF-based approach of King and Abney (2013), the languages involved do not need to be known ahead of time. In contrast with all these approaches, we work with parallel data with unbalanced original languages. language sentences, this work suggests that the proceedings of multilingual organizations such as the Canadian Hansard can provide interesting"
W14-3913,P14-1082,0,0.0306444,"Missing"
W14-3913,P94-1012,0,0.433412,"Missing"
W14-3913,H01-1035,0,0.137534,"Missing"
W14-5316,W13-1712,1,0.903367,"Missing"
W14-5316,W13-1706,0,0.0296805,"Missing"
W14-5316,W14-5307,0,0.35685,"Missing"
W14-5316,N12-1006,0,0.0148142,"ish). In addition, instances to classify are single sentences, a more realistic and challenging situation than full-document language identification. Our motivation for taking part in this evaluation was threefold. First, we wanted to evaluate our in-house implementation of document categorization on a real and useful task in a well controlled experimental setting.1 Second, classifiers that can discriminate between similar languages can be applied to tasks such as identifying close dialects, and may be useful for training Statistical Machine Translation systems more effectively. For instance, Zbib et al. (2012) show that small amounts of data from the right dialect can have a dramatic impact on the quality of Dialectal Arabic Machine Translation systems. Finally, we view the DSL task as a first step towards building a system that can identify code-switching in, for example, social media data, a task which has recently received increased attention from the NLP community2 (Elfardy et al., 2013). The next section reviews the modeling choices we made for the shared task, and section 3 describes our results in detail. Additional analysis and comparisons with other submitted systems are available in the s"
W15-2903,J94-4004,0,0.148212,"Missing"
W15-2903,E09-1010,0,0.0597807,"Missing"
W15-2903,P10-4002,0,0.0114311,"2014), but we limit our analysis of connotation at the word level at this stage. 4.2 We produce automatic translations of the French headlines into English using two different machine translation systems. First, we use Google Translate, since this free online system is known to achieve good translation quality in a variety of domains for French to English translation. Second, we build a system using publicly available resources, to complement the black-box Google Translate system. We use the hierarchical phrase-based machine translation model (Chiang, 2005) from the open-source cdec toolkit (Dyer et al., 2010), and datasets from the Workshop on Machine Translation.4 Google Translate achieves an uncased BLEU score (Papineni et al., 2002) of 20.13, and the cdec-based system 14.60. The lower score of the cdec system reflects the nature of its training data which is primarily drawn from parliament proceedings rather than news, as well as the difficulty of translating headlines. The translation quality is nevertheless reasonable, as illustrated by the randomly selected examples in Table 1. Does machine translation preserve connotative language? We start our analysis of connotation using fully automatic"
W15-2903,baccianella-etal-2010-sentiwordnet,0,0.0973536,"on to its literal or primary meaning [or denotation].” Words with positive connotation describe “physical objects or abstract concepts that people generally value, cherish or care about”, while words with negative connotation “describe physical objects or abstract concepts that people generally disvalue or avoid”. As a result, connotation can be evoked by words that do not express sentiment (either explicitly or implicitly), and that would be considered neutral in a sentiment analysis or opinion mining task. For instance, the nouns “life” and “home” are annotated as objective in SentiWordNet (Baccianella et al., 2010), while they carry a positive connotation according to the definition above. 3 Study conditions Languages: We choose French and English as tar9 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 9–15, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. get languages, as these are resource-rich languages and machine translation between them can be achieved with reasonably high quality (CallisonBurch et al., 2009; Bojar et al., 2013). Domain: We collect text from the Global Voice"
W15-2903,P13-1174,0,0.0548787,"evel judgments, (2) connotation polarity is often, but not always, preserved in reference translations produced by humans, (3) machine translated text does not preserve the connotative language identified by an English connotation lexicon. These lessons will helps us build new resources to learn better models of connotation and translation. 1 Introduction Subtle shades of meaning beyond surface meaning are receiving increasing attention in Natural Language Processing. Recognizing that even words that are objective on the surface can reveal sentiment of the writer or evoke emotions in readers, Feng et al. (2013) show that the connotation of words can be induced from corpora in an unsupervised fashion, and that the learned connotation polarity of words is useful for sentiment analysis tasks. While such connotation resources only exist for English at the moment, sentiment and subjectivity analysis (Pang and Lee, 2008) increasingly addresses other languages (Banea et al., 2011). This leads us to ask whether connotation can also be studied in the cross-lingual and multilingual setting. Modeling and detecting differences of connotation across languages would have many applications, e.g., enabling comparis"
W15-2903,D08-1014,0,0.0216872,"ll 41.56 58.52 Table 3: Comparing the dominant connotation of the entire machine translated segment to that of the reference for our two systems. Taken together, these results suggest that machine translation does not preserve connotative language accurately, even for an “easy” language pair such as as French-English. This differs from prior work on sentiment analysis which suggests that even imperfect machine translation can be good enough to port systems from e.g., English to Arabic dialects (Salameh et al., 2015), or to project labels of subjectivity from English into Romanian and Spanish (Banea et al., 2008). However, our study of connotation differs from prior work in two important ways: (1) as defined in Section 2, connotation refers to meaning that is evoked or associated with a word, while sentiment or subjectivity tends to be more explicit. So we expect connotation shifts to be more subtle. (2) our study focuses on word connotation, while prior cross-lingual analyses have focused on sentiment/subjectivity at the segment level, and are therefore expected to be more tolerant of machine translation errors. Table 2: Are connotative words in machine translation output found in reference translati"
W15-2903,N09-1057,0,0.0830179,"Missing"
W15-2903,P14-1145,0,0.0845418,"tion of readers. Size: We work with a sample of 245 parallel headlines, and study the connotation in each language using both automatic and manual analysis. 4 This broad-coverage lexicon was automatically induced from raw text, based on the intuition that connotation can be propagated to the entire vocabulary based on co-occurrences with a small set of seed connotative predicates of known polarity. For instance, the arguments of “enjoy” are typically positive, while those of “suffer” are typically negative. Follow-up work showed that connotation can be associated with fine-grained word senses(Kang et al., 2014), but we limit our analysis of connotation at the word level at this stage. 4.2 We produce automatic translations of the French headlines into English using two different machine translation systems. First, we use Google Translate, since this free online system is known to achieve good translation quality in a variety of domains for French to English translation. Second, we build a system using publicly available resources, to complement the black-box Google Translate system. We use the hierarchical phrase-based machine translation model (Chiang, 2005) from the open-source cdec toolkit (Dyer e"
W15-2903,J90-2002,0,0.875686,"Missing"
W15-2903,P02-1040,0,0.100041,"e French headlines into English using two different machine translation systems. First, we use Google Translate, since this free online system is known to achieve good translation quality in a variety of domains for French to English translation. Second, we build a system using publicly available resources, to complement the black-box Google Translate system. We use the hierarchical phrase-based machine translation model (Chiang, 2005) from the open-source cdec toolkit (Dyer et al., 2010), and datasets from the Workshop on Machine Translation.4 Google Translate achieves an uncased BLEU score (Papineni et al., 2002) of 20.13, and the cdec-based system 14.60. The lower score of the cdec system reflects the nature of its training data which is primarily drawn from parliament proceedings rather than news, as well as the difficulty of translating headlines. The translation quality is nevertheless reasonable, as illustrated by the randomly selected examples in Table 1. Does machine translation preserve connotative language? We start our analysis of connotation using fully automatic means: machine translation and an automatically induced connotation lexicon. We use the lexicon to tag connotative words in both"
W15-2903,N15-1078,0,0.032159,"50.75 52.60 Recall 46.69 50.53 Do content words overlap with references? Precision 37.35 49.70 Recall 41.56 58.52 Table 3: Comparing the dominant connotation of the entire machine translated segment to that of the reference for our two systems. Taken together, these results suggest that machine translation does not preserve connotative language accurately, even for an “easy” language pair such as as French-English. This differs from prior work on sentiment analysis which suggests that even imperfect machine translation can be good enough to port systems from e.g., English to Arabic dialects (Salameh et al., 2015), or to project labels of subjectivity from English into Romanian and Spanish (Banea et al., 2008). However, our study of connotation differs from prior work in two important ways: (1) as defined in Section 2, connotation refers to meaning that is evoked or associated with a word, while sentiment or subjectivity tends to be more explicit. So we expect connotation shifts to be more subtle. (2) our study focuses on word connotation, while prior cross-lingual analyses have focused on sentiment/subjectivity at the segment level, and are therefore expected to be more tolerant of machine translation"
W15-2903,W13-0801,1,0.901993,"Missing"
W15-2903,P05-1033,0,0.0801571,"ciated with fine-grained word senses(Kang et al., 2014), but we limit our analysis of connotation at the word level at this stage. 4.2 We produce automatic translations of the French headlines into English using two different machine translation systems. First, we use Google Translate, since this free online system is known to achieve good translation quality in a variety of domains for French to English translation. Second, we build a system using publicly available resources, to complement the black-box Google Translate system. We use the hierarchical phrase-based machine translation model (Chiang, 2005) from the open-source cdec toolkit (Dyer et al., 2010), and datasets from the Workshop on Machine Translation.4 Google Translate achieves an uncased BLEU score (Papineni et al., 2002) of 20.13, and the cdec-based system 14.60. The lower score of the cdec system reflects the nature of its training data which is primarily drawn from parliament proceedings rather than news, as well as the difficulty of translating headlines. The translation quality is nevertheless reasonable, as illustrated by the randomly selected examples in Table 1. Does machine translation preserve connotative language? We st"
W15-2903,S07-1013,0,0.13304,"Missing"
W15-2903,N03-1033,0,0.00810517,"Missing"
W15-2903,W09-0401,0,\N,Missing
W15-2903,W13-2201,0,\N,Missing
W17-3209,2012.eamt-1.60,0,0.0141069,"e evaluate on the English-French Microsoft Spoken Language Translation task (Federmann and Lewis, 2016), which provides a translation scenario motivated by real world applications. Following prior work (Farajian et al., 2016; Lewis et al., 2016), training data is drawn from the OpenSubtitles corpus (Lison and Tiedemann, 2016). The subtitle genre is also appropriate as it presents many potential divergences due to genre-specific constraints (Tiedemann, 2007). In addition, the robustness of the models is evaluated by testing on a second domain, leveraging publicly available TED talks test data (Cettolo et al., 2012). French and English sides of the corpus are uncased and tokenized using Moses preprocessing tools, and segmented using byte pair encoding (Sennrich et al., 2016b). We use annotations of Cross-Lingual Textual Entailment (Mehdad et al., 2010). This task is framed as a four-way classification task. Given sentences e and f , the goal is to predict whether (1) e entails f , (2) f entails e, (3) e and f both entail each other, (4) there is no entailment relation between e and f . Negri and Mehdad (2010) showed that English training and test sets can be created by crowdsourcing, that are then transl"
W17-3209,2016.amta-researchers.8,0,0.50703,"Missing"
W17-3209,C04-1151,0,0.0934044,"ine translation, where parallel sentences are assumed to be translations of each other, and translations are assumed to have the same meaning. Prior work on characterizing parallel sentences for MT has focused on data selection and weighting for domain adaptation (Foster and Kuhn, 2007; Axelrod et al., 2011, among others), and on assessing the relevance of parallel sentences by comparison with a corpus of interest. In contrast, we focus on detecting an intrinsic property of parallel sentence pairs. Divergent sentence pairs have been viewed as noise both in comparable and non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012) and en the female employee suffered from shock. fr les victimes ont surv´ecu leur peur. gl the victims have survived their fear. Table 1: Parallel segments are not always semantically equivalent, as can be seen in these examples (English sentence (en), French sentence (fr) and its gloss (gl)) drawn from a random sample of OpenSubtitles and of the newstest2012 test set (Bojar et al., 2016). in parallel corpora (Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012).In contrast, we hypot"
W17-3209,W12-3131,0,0.154925,"d non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012) and en the female employee suffered from shock. fr les victimes ont surv´ecu leur peur. gl the victims have survived their fear. Table 1: Parallel segments are not always semantically equivalent, as can be seen in these examples (English sentence (en), French sentence (fr) and its gloss (gl)) drawn from a random sample of OpenSubtitles and of the newstest2012 test set (Bojar et al., 2016). in parallel corpora (Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012).In contrast, we hypothesize that the translation process inherently introduces divergences that affect meaning, and that semantically divergent examples should be expected in all parallel corpora. We show that semantically divergent examples significantly impact the learning curves and translation quality of neural machine translation systems. We repurpose the task of cross-lingual textual entailment (Mehdad et al., 2010) to automatically identify and filter divergent parallel sentence pairs from the OpenSubtitles corpus (Lison and Tiedemann, 2016). This approach outperforms 69 Proceedings of"
W17-3209,D13-1205,0,0.0225262,"ohn et al., 2016; Mi et al., 2016). These approaches however focus on improving neural machine translation in low resource settings, while our aim was to identify a subset of examples in large training sets. References Applications beyond MT Detecting crosslingual semantic divergences using entailment has been motivated by the need to synchronize content across languages in multilingual resources such as Wikipedia (Negri and Mehdad, 2010; Duh et al., 2013). It could also be useful to select better training examples for cross-lingual transfer learning of semantic models (Yarowsky et al., 2001; Ganchev and Das, 2013, among others). Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Computational Linguistics, San Diego, California, pages 497–511. 7 Sadaf AbduI-Rauf and Holger Schwenk. 2009. On the Use of Comparable Corpora to Improve SMT Performance. In Proceedings of the 12th Conference of the European Chapter of the Association for Comp"
W17-3209,J94-4004,0,0.371216,"ained on the entire training set. Table 5 shows that the mixed ensemble improves over the previous best result by +0.34 BLEU on the TED test set and +0.40 on the MSLT test set. It is unclear whether these modest gains are worth the additional training time needed to add the ALL system to the mix. However, it remains to be seen whether better model selection could yield further improvements. 6 Related Work Translation Divergences Most prior work on translation divergences has focused on typological issues which reflect the fact that languages do not encode the same information in the same way. Dorr (1994) formalizes this problem by defining divergence categories (e.g., thematic, structural, 73 Figure 1: Learning curves on validation test set for all training configurations. Solid lines indicate a system trained on the entire training set, dotted lines use half of the training data with various selection criteria, and the dashed line indicates data selected by length. Training on the non-divergent half of the examples yields the top learning curve, even when compared to using all of the training data. Selected Data Size (M) non-divergent random natural order length < 10 all TED 3-best 3-last MS"
W17-3209,2012.amta-papers.7,1,0.816072,"lly divergent examples significantly impact the learning curves and translation quality of neural machine translation systems. We repurpose the task of cross-lingual textual entailment (Mehdad et al., 2010) to automatically identify and filter divergent parallel sentence pairs from the OpenSubtitles corpus (Lison and Tiedemann, 2016). This approach outperforms 69 Proceedings of the First Workshop on Neural Machine Translation, pages 69–79, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics more sensitive to sentence pair permutations than phrase-based systems (Goutte et al., 2012). They also show that a bilingual convolutional neural network trained to discriminate in-domain from out-of-domain sentence pairs effectively selects training data that is not only in domain but also less noisy. These results provide further evidence that the degree of parallelism in training examples has an impact in neural MT. Yet it remains to be seen to what extent semantic divergences – rather than noise – affect translation quality in general – and not only in domain adaptation settings. other data selection criterion, and even a system trained on twice as much data for two test genres."
W17-3209,dorr-etal-2002-duster,0,0.466423,"or parallel fragments from non-parallel corpora differs from our work in several ways. The goal is to identify additional training examples to augment parallel corpora, rather than to identify the most useful examples in a parallel corpus (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012). The non-parallel examples tend to be more extreme than in the parallel corpora considered in our work. categorical). Follow up work shows that divergences are not outliers but common phenomena in parallel corpora (Dorr et al., 2002; Habash and Dorr, 2002). Some of these divergences have been implicitly addressed by designing MT architectures informed by syntax and structure (Wu, 1997; Habash and Dorr, 2002; Chiang, 2007; Lavie, 2008, among others). In this work, we focused instead on semantic divergences which happen when the source and target sentences do not convey exactly the same meaning. Modeling Cross-Lingual Semantic Divergences Prior work has addressed cross-lingual semantic textual similarity (Agirre et al., 2016), entailment (Negri and Mehdad, 2010; Negri et al., 2012, 2013), translation quality estimation (Sp"
W17-3209,habash-dorr-2002-handling,0,0.138978,"nts from non-parallel corpora differs from our work in several ways. The goal is to identify additional training examples to augment parallel corpora, rather than to identify the most useful examples in a parallel corpus (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012). The non-parallel examples tend to be more extreme than in the parallel corpora considered in our work. categorical). Follow up work shows that divergences are not outliers but common phenomena in parallel corpora (Dorr et al., 2002; Habash and Dorr, 2002). Some of these divergences have been implicitly addressed by designing MT architectures informed by syntax and structure (Wu, 1997; Habash and Dorr, 2002; Chiang, 2007; Lavie, 2008, among others). In this work, we focused instead on semantic divergences which happen when the source and target sentences do not convey exactly the same meaning. Modeling Cross-Lingual Semantic Divergences Prior work has addressed cross-lingual semantic textual similarity (Agirre et al., 2016), entailment (Negri and Mehdad, 2010; Negri et al., 2012, 2013), translation quality estimation (Specia et al., 2010, 2016)"
W17-3209,W13-2246,0,0.0391672,"or this task. 70 3.1 Other relevant semantic annotations of bilingual corpora include cross-lingual semantic textual similarity (Agirre et al., 2016) and machine translation quality estimation datasets (Specia et al., 2010). The latter is not a good fit as it annotates machine translation output. The former is a better match but only provides test examples. Model We frame the task of detecting whether parallel sentences (e, f ) are equivalent as a classification problem. We draw inspiration from related work on semantic textual similarity (Agirre et al., 2016), translation quality estimation (Hildebrand and Vogel, 2013), parallel sentence detection (Munteanu and Marcu, 2006) to design simple features that can be induced without supervision. First, differences in sentence lengths are strong indicators of divergence in content between e and f . Accordingly, we use four length features: |e|, |e| |f | |f |, |f |, and |e |. Second, we assume that the configuration of word alignment links between parallel sentences (e, f ) is indicative of equivalence: if e and f have the same meaning, then they will be easier to align. Accordingly, we compute the following features for each of e and f : 4 4.1 Divergence Detection"
W17-3209,2010.eamt-1.26,0,0.105037,"oth in comparable and non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012) and en the female employee suffered from shock. fr les victimes ont surv´ecu leur peur. gl the victims have survived their fear. Table 1: Parallel segments are not always semantically equivalent, as can be seen in these examples (English sentence (en), French sentence (fr) and its gloss (gl)) drawn from a random sample of OpenSubtitles and of the newstest2012 test set (Bojar et al., 2016). in parallel corpora (Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012).In contrast, we hypothesize that the translation process inherently introduces divergences that affect meaning, and that semantically divergent examples should be expected in all parallel corpora. We show that semantically divergent examples significantly impact the learning curves and translation quality of neural machine translation systems. We repurpose the task of cross-lingual textual entailment (Mehdad et al., 2010) to automatically identify and filter divergent parallel sentence pairs from the OpenSubtitles corpus (Lison and Tiedemann, 2016). This approach outp"
W17-3209,S12-1102,0,0.0907718,"Missing"
W17-3209,W08-0306,0,0.0515342,"Missing"
W17-3209,L16-1147,0,0.130953,"orpora (Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012).In contrast, we hypothesize that the translation process inherently introduces divergences that affect meaning, and that semantically divergent examples should be expected in all parallel corpora. We show that semantically divergent examples significantly impact the learning curves and translation quality of neural machine translation systems. We repurpose the task of cross-lingual textual entailment (Mehdad et al., 2010) to automatically identify and filter divergent parallel sentence pairs from the OpenSubtitles corpus (Lison and Tiedemann, 2016). This approach outperforms 69 Proceedings of the First Workshop on Neural Machine Translation, pages 69–79, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics more sensitive to sentence pair permutations than phrase-based systems (Goutte et al., 2012). They also show that a bilingual convolutional neural network trained to discriminate in-domain from out-of-domain sentence pairs effectively selects training data that is not only in domain but also less noisy. These results provide further evidence that the degree of parallelism in training examples has an impa"
W17-3209,W07-0717,0,0.0256069,"m “parallel” implies, the source and target language often do not convey the exact same meaning. This is a surprisingly common phenomenon, not only in noisy corpora automatically extracted from comparable collections, but also in parallel training and test corpora, as can be seen in Table 1. This issue has mostly been ignored in machine translation, where parallel sentences are assumed to be translations of each other, and translations are assumed to have the same meaning. Prior work on characterizing parallel sentences for MT has focused on data selection and weighting for domain adaptation (Foster and Kuhn, 2007; Axelrod et al., 2011, among others), and on assessing the relevance of parallel sentences by comparison with a corpus of interest. In contrast, we focus on detecting an intrinsic property of parallel sentence pairs. Divergent sentence pairs have been viewed as noise both in comparable and non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012) and en the female employee suffered from shock. fr les victimes ont surv´ecu leur peur. gl the victims have survived their fear. Table 1: Parallel segments are not"
W17-3209,N12-1061,0,0.416631,"tions are assumed to have the same meaning. Prior work on characterizing parallel sentences for MT has focused on data selection and weighting for domain adaptation (Foster and Kuhn, 2007; Axelrod et al., 2011, among others), and on assessing the relevance of parallel sentences by comparison with a corpus of interest. In contrast, we focus on detecting an intrinsic property of parallel sentence pairs. Divergent sentence pairs have been viewed as noise both in comparable and non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012) and en the female employee suffered from shock. fr les victimes ont surv´ecu leur peur. gl the victims have survived their fear. Table 1: Parallel segments are not always semantically equivalent, as can be seen in these examples (English sentence (en), French sentence (fr) and its gloss (gl)) drawn from a random sample of OpenSubtitles and of the newstest2012 test set (Bojar et al., 2016). in parallel corpora (Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012).In contrast, we hypothesize that the translation process inherently introduces divergences that affect meaning, and that"
W17-3209,D16-1249,0,0.0289229,"9; Jiang et al., 2010; Denkowski et al., 2012; Matthews et al., 2014). Cleaning training data in high-resource settings (Denkowski et al., 2012) and tuning data in lower resource settings (Matthews et al., 2014) has been shown to improve hierarchical phrase-based systems. Incorporating Word Alignments into Neural MT Since our data selection criterion relies on word alignments, one could view our approach as part of the family of models that seek to im75 prove neural machine translation using insights and models from word alignment and statistical machine translation models (Cohn et al., 2016; Mi et al., 2016). These approaches however focus on improving neural machine translation in low resource settings, while our aim was to identify a subset of examples in large training sets. References Applications beyond MT Detecting crosslingual semantic divergences using entailment has been motivated by the need to synchronize content across languages in multilingual resources such as Wikipedia (Negri and Mehdad, 2010; Duh et al., 2013). It could also be useful to select better training examples for cross-lingual transfer learning of semantic models (Yarowsky et al., 2001; Ganchev and Das, 2013, among other"
W17-3209,E17-3017,0,0.0843366,"Missing"
W17-3209,J05-4003,0,0.576281,"parallel sentences are assumed to be translations of each other, and translations are assumed to have the same meaning. Prior work on characterizing parallel sentences for MT has focused on data selection and weighting for domain adaptation (Foster and Kuhn, 2007; Axelrod et al., 2011, among others), and on assessing the relevance of parallel sentences by comparison with a corpus of interest. In contrast, we focus on detecting an intrinsic property of parallel sentence pairs. Divergent sentence pairs have been viewed as noise both in comparable and non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012) and en the female employee suffered from shock. fr les victimes ont surv´ecu leur peur. gl the victims have survived their fear. Table 1: Parallel segments are not always semantically equivalent, as can be seen in these examples (English sentence (en), French sentence (fr) and its gloss (gl)) drawn from a random sample of OpenSubtitles and of the newstest2012 test set (Bojar et al., 2016). in parallel corpora (Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012).In contrast, we hypothesize that the translatio"
W17-3209,P06-1011,0,0.0416694,"f bilingual corpora include cross-lingual semantic textual similarity (Agirre et al., 2016) and machine translation quality estimation datasets (Specia et al., 2010). The latter is not a good fit as it annotates machine translation output. The former is a better match but only provides test examples. Model We frame the task of detecting whether parallel sentences (e, f ) are equivalent as a classification problem. We draw inspiration from related work on semantic textual similarity (Agirre et al., 2016), translation quality estimation (Hildebrand and Vogel, 2013), parallel sentence detection (Munteanu and Marcu, 2006) to design simple features that can be induced without supervision. First, differences in sentence lengths are strong indicators of divergence in content between e and f . Accordingly, we use four length features: |e|, |e| |f | |f |, |f |, and |e |. Second, we assume that the configuration of word alignment links between parallel sentences (e, f ) is indicative of equivalence: if e and f have the same meaning, then they will be easier to align. Accordingly, we compute the following features for each of e and f : 4 4.1 Divergence Detection Model Settings We use the cross-lingual textual entailm"
W17-3209,S12-1053,0,0.120585,"Missing"
W17-3209,N10-1063,0,0.293929,"h other, and translations are assumed to have the same meaning. Prior work on characterizing parallel sentences for MT has focused on data selection and weighting for domain adaptation (Foster and Kuhn, 2007; Axelrod et al., 2011, among others), and on assessing the relevance of parallel sentences by comparison with a corpus of interest. In contrast, we focus on detecting an intrinsic property of parallel sentence pairs. Divergent sentence pairs have been viewed as noise both in comparable and non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Riesa and Marcu, 2012) and en the female employee suffered from shock. fr les victimes ont surv´ecu leur peur. gl the victims have survived their fear. Table 1: Parallel segments are not always semantically equivalent, as can be seen in these examples (English sentence (en), French sentence (fr) and its gloss (gl)) drawn from a random sample of OpenSubtitles and of the newstest2012 test set (Bojar et al., 2016). in parallel corpora (Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012).In contrast, we hypothesize that the translation process inherently introduces divergences that a"
W17-3209,S13-2005,0,0.115268,"Missing"
W17-3209,W10-0734,0,0.448367,"els is evaluated by testing on a second domain, leveraging publicly available TED talks test data (Cettolo et al., 2012). French and English sides of the corpus are uncased and tokenized using Moses preprocessing tools, and segmented using byte pair encoding (Sennrich et al., 2016b). We use annotations of Cross-Lingual Textual Entailment (Mehdad et al., 2010). This task is framed as a four-way classification task. Given sentences e and f , the goal is to predict whether (1) e entails f , (2) f entails e, (3) e and f both entail each other, (4) there is no entailment relation between e and f . Negri and Mehdad (2010) showed that English training and test sets can be created by crowdsourcing, that are then translated to obtain cross-lingual datasets. Training and test data were made available at SemEval 2012 and 2013 (Negri et al., 2012, 2013). We hypothesize that examples detected as class (4) are the most divergent examples that are the least useful for training machine translation systems. While the 4-way classification task is more complex than our end goal of detecting divergent examples, we found that the 4-way classifier detects divergent examples from class (4) better than binary classifiers traine"
W17-3209,H01-1035,0,0.117454,"e translation models (Cohn et al., 2016; Mi et al., 2016). These approaches however focus on improving neural machine translation in low resource settings, while our aim was to identify a subset of examples in large training sets. References Applications beyond MT Detecting crosslingual semantic divergences using entailment has been motivated by the need to synchronize content across languages in multilingual resources such as Wikipedia (Negri and Mehdad, 2010; Duh et al., 2013). It could also be useful to select better training examples for cross-lingual transfer learning of semantic models (Yarowsky et al., 2001; Ganchev and Das, 2013, among others). Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Computational Linguistics, San Diego, California, pages 497–511. 7 Sadaf AbduI-Rauf and Holger Schwenk. 2009. On the Use of Comparable Corpora to Improve SMT Performance. In Proceedings of the 12th Conference of the European Chapter of t"
W17-3209,S13-2021,0,0.257179,"Missing"
W17-3209,D11-1033,0,\N,Missing
W17-3209,E09-1003,0,\N,Missing
W17-3209,J97-3002,0,\N,Missing
W17-3209,W14-3315,0,\N,Missing
W17-3209,2015.iwslt-papers.2,0,\N,Missing
W17-4903,P14-1023,0,0.0522275,"variations. co-occurrence patterns that underlie register variations. More recently, richer combinations of features have been used to predict style dimensions such as formality: (Pavlick and Tetreault, 2016) provide a thorough study of sentence-level formality and show that classifiers based on features including POS tags and dependency parses can predict formality as defined by the collective intuition of human annotators. Here, we focus on identifying dimensions of style variations at the lexical level, motivated by the usefulness of word embeddings in many NLP tasks (Mikolov et al., 2013; Baroni et al., 2014), and by recent work that showed that meaningful ultradense subspaces that capture dimensions such as polarity and concreteness can be induced from word embeddings in a supervised fashion (Rothe and Sch¨utze, 2016). Bolukbasi et al. (2016) induced a gender subspace using 10 human-selected gender pairs for reducing stereotypes. In contrast, we aim to discover style relevant dimensions without supervision, using instead lexical paraphrases to discover dimensions of variations that are not explained by semantic differences. Prior work on evaluation of style factors at the word level has used stan"
W17-4903,C14-1205,0,0.0263371,"Several word pairs can be seen as illustrating formality variations (e.g., “falls” ↔ “decrease”, “delete” ↔ “eliminate”). Many word pairs are literally exchangeable but either one is preferred under certain context, such as “summons” vs. “subpoenas”, “decreased” vs. “fallen”, etc. Some principal components simply capture groups of words having semantic correlations, such as third PC of blogs and fourth PC of news (all contain “decrease/increase”), due to the biased word distribution of PPDB. Ranking method The predictions were made by linear SVM classifiers (similar to the method proposed by Brooke and Hirst (2014)). They were trained on 105 formal seed words and 138 informal seed words used by Brooke et al. (2010). Each word was represented by a feature vector in word2vec spaces or their subspaces. When ranking two words, we actually compared their distances to the separating hyperplane, i.e. w · e − ρ, where w, e and ρ are weight, embedding and bias. Embedding spaces We first trained word2vec (W2V) models of blogs corpus with different space sizes (dimensionality=1-10, 15, 20, 25, 30, 35, 40, 45, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500). We then fixed the space size of word2vec models to 300 s"
W17-4903,C10-2011,0,0.451916,"nduced from word embeddings in a supervised fashion (Rothe and Sch¨utze, 2016). Bolukbasi et al. (2016) induced a gender subspace using 10 human-selected gender pairs for reducing stereotypes. In contrast, we aim to discover style relevant dimensions without supervision, using instead lexical paraphrases to discover dimensions of variations that are not explained by semantic differences. Prior work on evaluation of style factors at the word level has used standard word embeddings as features, and relied on external supervised methods to identify style relevant information in these embeddings. Brooke et al. (2010) proposed to score the formality of a word w by comparing its meaning to that of seed words of known formality using cosine similarity (Turney and Littman, 2003). Other approaches include work by Pavlick and Nenkova (2015) who used a unigram language model to capture the difference between lexical distributions across genres. Beyond formality, analysis of stylistic variations from the point of view of the lexicon includes predicting term complexity, as annotated by non-native speakers (Paetzold and Specia, 2016). Preotiuc-Pietro et al. (2016) isolated stylistic differences associated with user"
W17-4903,W14-1618,0,0.0640999,"Missing"
W17-4903,N15-1023,0,0.0283717,"discover style relevant dimensions without supervision, using instead lexical paraphrases to discover dimensions of variations that are not explained by semantic differences. Prior work on evaluation of style factors at the word level has used standard word embeddings as features, and relied on external supervised methods to identify style relevant information in these embeddings. Brooke et al. (2010) proposed to score the formality of a word w by comparing its meaning to that of seed words of known formality using cosine similarity (Turney and Littman, 2003). Other approaches include work by Pavlick and Nenkova (2015) who used a unigram language model to capture the difference between lexical distributions across genres. Beyond formality, analysis of stylistic variations from the point of view of the lexicon includes predicting term complexity, as annotated by non-native speakers (Paetzold and Specia, 2016). Preotiuc-Pietro et al. (2016) isolated stylistic differences associated with user attributes (gender, age) by using paraphrase pairs and word distributions similar to Pavlick and Nenkova (2015). Xu et al. (2012) used a machine translation model to paraphrasing Shakespeares plays into/from modern Englis"
W17-4903,P15-2070,0,0.050383,"Missing"
W17-4903,Q16-1005,0,0.0293828,"ses (w1 , w2 ), we subtracted them to get the relative direction d = e1 − e2 . For a given word pair, the difference vector might capture many things besides style variations. We hypothesize that the regularities among these differences for a large number of examples will reveal stylistic variations. Therefore, we then trained a PCA model on all directional vectors to get principal components (pck ) capturing latent variations. co-occurrence patterns that underlie register variations. More recently, richer combinations of features have been used to predict style dimensions such as formality: (Pavlick and Tetreault, 2016) provide a thorough study of sentence-level formality and show that classifiers based on features including POS tags and dependency parses can predict formality as defined by the collective intuition of human annotators. Here, we focus on identifying dimensions of style variations at the lexical level, motivated by the usefulness of word embeddings in many NLP tasks (Mikolov et al., 2013; Baroni et al., 2014), and by recent work that showed that meaningful ultradense subspaces that capture dimensions such as polarity and concreteness can be induced from word embeddings in a supervised fashion"
W17-4903,N16-1091,0,0.077195,"Missing"
W17-4903,P16-2083,0,0.0503449,"Missing"
W17-4903,C12-1177,0,0.0323333,"ng cosine similarity (Turney and Littman, 2003). Other approaches include work by Pavlick and Nenkova (2015) who used a unigram language model to capture the difference between lexical distributions across genres. Beyond formality, analysis of stylistic variations from the point of view of the lexicon includes predicting term complexity, as annotated by non-native speakers (Paetzold and Specia, 2016). Preotiuc-Pietro et al. (2016) isolated stylistic differences associated with user attributes (gender, age) by using paraphrase pairs and word distributions similar to Pavlick and Nenkova (2015). Xu et al. (2012) used a machine translation model to paraphrasing Shakespeares plays into/from modern English. 3 4 4.1 Qualitative Analysis of Latent Style Dimensions Models Settings The approach outlined above requires two types of inputs: (1) a word embedding space, and (2) a set of lexical paraphrases. Word Embeddings We used word2vec (Mikolov et al., 2013) to build 300-dimensional vector space models for two corpora representing different genres. As suggested by Brooke et al. (2010), we selected the ICWSM 2009 Spinn3r dataset (English tier-1) as the training corpus (Burton et al., 2009). It consists of ab"
W18-1803,D16-1025,0,0.0792828,"Missing"
W18-1803,W07-0718,0,0.833111,", 2010), but user trust has not been evaluated. There has, however, been extensive work in human evaluation of MT quality and in user trust of other types of automation. 2.1 Human Evaluation of Machine Translation Automated quality metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) have been invaluable for improving and training MT systems, and have been shown to correlate with human rankings at the system level (Papineni et al., 2002), but human judgments remain the gold standard for competitions like the Workshop (now Conference) on Machine Translation (WMT) (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). However, these human judgments have focused almost entirely on intrinsic quality rather than the user experience and particularly trust. Common approaches to human evaluation of MT include ranking and direct assessment. Ranking was the ofﬁcial metric for WMT 2008 - 2016 (Callison-Burch et al., 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016). In each evaluation, participants ranked ﬁve translations of individual segments (usually sentences) from different MT systems against each other based on a ref"
W18-1803,W08-0309,0,0.0945992,"nd have been shown to correlate with human rankings at the system level (Papineni et al., 2002), but human judgments remain the gold standard for competitions like the Workshop (now Conference) on Machine Translation (WMT) (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). However, these human judgments have focused almost entirely on intrinsic quality rather than the user experience and particularly trust. Common approaches to human evaluation of MT include ranking and direct assessment. Ranking was the ofﬁcial metric for WMT 2008 - 2016 (Callison-Burch et al., 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016). In each evaluation, participants ranked ﬁve translations of individual segments (usually sentences) from different MT systems against each other based on a reference translation. While rankings indicate user preference, they do not explicitly address user trust in the system and do not directly provide insight into aspects of quality that affect user judgments of the translation. The setting is also very different from a typical user experience, in which they would see only one translation of a whole document with no reference tr"
W18-1803,W13-2305,0,0.116492,"re against. This difference makes it difﬁcult to draw conclusions about the user experience. Direct assessment of adequacy and ﬂuency is slightly more like the typical user experience in that they only see one translation. WMT initially adopted a direct assessment approach for the 2006 and 2007 workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007), but the interannotator agreement was low that the metric was abandoned until WMT16 (Bojar et al., 2016). The change came as a result of a pilot direct assessment, conducted in parallel to the ofﬁcial rankings, that tested the technique from Graham et al. (2013) on a larger scale. Graham et al. had found that they could improve inter-annotator agreement by accounting for the personal preferences of the judges: They used a near-continuous scale (100 points instead of only ﬁve) and standardized each judge’s score to a z-score based on the mean and standard deviation of that judge’s scores. The results in the WMT16 pilot correlated so well that WMT17 used direct assessment of adequacy as the ofﬁcial metric (Bojar et al., 2017). However, the measure is still focused on intrinsic quality and the judges still only see one segment at a time rather than a wh"
W18-1803,W17-3204,0,0.0272302,"Missing"
W18-1803,W06-3114,0,0.116887,"affect user judgments of the translation. The setting is also very different from a typical user experience, in which they would see only one translation of a whole document with no reference translation rather than multiple translations of individual segments and a human translation to compare against. This difference makes it difﬁcult to draw conclusions about the user experience. Direct assessment of adequacy and ﬂuency is slightly more like the typical user experience in that they only see one translation. WMT initially adopted a direct assessment approach for the 2006 and 2007 workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007), but the interannotator agreement was low that the metric was abandoned until WMT16 (Bojar et al., 2016). The change came as a result of a pilot direct assessment, conducted in parallel to the ofﬁcial rankings, that tested the technique from Graham et al. (2013) on a larger scale. Graham et al. had found that they could improve inter-annotator agreement by accounting for the personal preferences of the judges: They used a near-continuous scale (100 points instead of only ﬁve) and standardized each judge’s score to a z-score based on the mean and standard deviatio"
W18-1803,P02-1040,0,0.120972,"paring the effects of ﬂuency and adequacy errors on user trust, and ﬁnd that ﬂuency appears to have a much greater effect than adequacy. 2 Related Work To the best of our knowledge, there has been no prior work focused on evaluating user trust in MT. Automatically generated conﬁdence scores have been referred to as “trust” (Soricut and Echihabi, 2010), but user trust has not been evaluated. There has, however, been extensive work in human evaluation of MT quality and in user trust of other types of automation. 2.1 Human Evaluation of Machine Translation Automated quality metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) have been invaluable for improving and training MT systems, and have been shown to correlate with human rankings at the system level (Papineni et al., 2002), but human judgments remain the gold standard for competitions like the Workshop (now Conference) on Machine Translation (WMT) (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). However, these human judgments have focused almost entirely on intrinsic quality rather than the user experience and particularly trust. Common approaches to human evaluati"
W18-1803,P10-1063,0,0.0265758,"his paper, we introduce a method of studying user trust in MT in a lab setting, based on previous work on trust in other Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 13 forms of automation. We apply this approach in a pilot study comparing the effects of ﬂuency and adequacy errors on user trust, and ﬁnd that ﬂuency appears to have a much greater effect than adequacy. 2 Related Work To the best of our knowledge, there has been no prior work focused on evaluating user trust in MT. Automatically generated conﬁdence scores have been referred to as “trust” (Soricut and Echihabi, 2010), but user trust has not been evaluated. There has, however, been extensive work in human evaluation of MT quality and in user trust of other types of automation. 2.1 Human Evaluation of Machine Translation Automated quality metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) have been invaluable for improving and training MT systems, and have been shown to correlate with human rankings at the system level (Papineni et al., 2002), but human judgments remain the gold standard for competitions like the Workshop (now Conference) on Machine Translation (WMT) (Calliso"
W18-1803,E17-1100,0,0.130521,"Missing"
W18-1803,J82-2005,0,0.731167,"Missing"
W18-1803,W09-0401,0,\N,Missing
W18-1803,W05-0909,0,\N,Missing
W18-1803,W10-1703,0,\N,Missing
W18-2710,D11-1033,0,0.0664706,"table combinations of synthetic parallel data and choosing appropriate amount of monolingual data. 2 2.2 Given the goal of improving a base bi-directional model, selecting ideal monolingual data for backtranslation presents a significant challenge. Data too close to the original training data may not provide sufficient new information for the model. Conversely, data too far from the original data may be translated too poorly by the base model to be useful. We manage these risks by leveraging a standard pseudo in-domain data selection technique, cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), to rank sentences from a general domain. Smaller cross-entropy difference indicates a sentence that is simultaneously more similar to the in-domain corpus (e.g. real parallel data) and less similar to the average of the general-domain monolingual corpus. This allows us to begin with “safe” monolingual data and incrementally expand to higher risk but potentially more informative data. Approach In this section, we introduce an efficient method for improving bi-directional neural machine translation with synthetic parallel data. We also present a strategy for selecting suitable monolingual data"
W18-2710,buck-etal-2014-n,0,0.0726098,"Missing"
W18-2710,2004.iwslt-evaluation.1,0,0.16735,"Missing"
W18-2710,W17-3204,0,0.0254469,"e experiments show that our technique outperforms standard backtranslation in low-resource scenarios, improves quality on cross-domain tasks, and effectively reduces costs across the board. 1 Marine Carpuat University of Maryland marine@cs.umd.edu Introduction Neural Machine Translation (NMT) has been rapidly adopted in industry as it consistently outperforms previous methods across domains and language pairs (Bojar et al., 2017; Cettolo et al., 2017). However, NMT systems still struggle compared to Phrase-based Statistical Machine Translation (SMT) in low-resource or out-of-domain scenarios (Koehn and Knowles, 2017). This performance gap is a significant roadblock to full adoption of NMT. 84 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 84–91 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics • A single NMT model with standard architecture that performs all forward and backward translation during training. from both source and target monolingual data can be translated to produce synthetic sentence pairs. Synthetic parallel data of the form synthetic → monolingual can then be used in the forward direction, the backward direction, or"
W18-2710,N03-1017,0,0.0614621,"Missing"
W18-2710,W07-0733,0,0.147198,"Missing"
W18-2710,P17-2061,0,0.0587276,"Missing"
W18-2710,P10-2041,0,0.0879322,"s in selecting most suitable combinations of synthetic parallel data and choosing appropriate amount of monolingual data. 2 2.2 Given the goal of improving a base bi-directional model, selecting ideal monolingual data for backtranslation presents a significant challenge. Data too close to the original training data may not provide sufficient new information for the model. Conversely, data too far from the original data may be translated too poorly by the base model to be useful. We manage these risks by leveraging a standard pseudo in-domain data selection technique, cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011), to rank sentences from a general domain. Smaller cross-entropy difference indicates a sentence that is simultaneously more similar to the in-domain corpus (e.g. real parallel data) and less similar to the average of the general-domain monolingual corpus. This allows us to begin with “safe” monolingual data and incrementally expand to higher risk but potentially more informative data. Approach In this section, we introduce an efficient method for improving bi-directional neural machine translation with synthetic parallel data. We also present a strategy for selecting su"
W18-2710,N18-1031,0,0.0251209,"r with a batch size of 64 sentences and checkpoint the model every 1000 updates (10,000 for DE↔EN) (Kingma and Ba, 2015). Training stops after 8 checkpoints without improvement of perplexity on the development set. We decode with a beam size of 5. For TL↔EN and SW↔EN, we add dropout to embeddings and RNNs of the encoder and decoder with probability 0.2. We also tie the output layer’s weight matrix with the source and target embeddings to reduce model size (Press and Wolf, 2017). The effectiveness of tying input/output target embeddings has been verified on several low-resource language pairs (Nguyen and Chiang, 2018). For TL↔EN and SW↔EN, we train four randomly seeded models for each experiment and combine them in a linear ensemble for decodGerman↔English (DE↔EN), Tagalog↔English TL↔EN, and Swahili↔English (SW↔EN). Parallel and monolingual DE↔EN data are provided by the WMT17 news translation task (Bojar et al., 2017). Parallel data for TL↔EN and SW↔EN contains a mixture of domains such as news and weblogs, and is provided as part of the IARPA MATERIAL program.2 We split the original corpora into training, dev, and test sets, therefore they share a homogeneous n-gram distribution. For these low-resource p"
W18-2710,P15-1166,0,0.0404796,"reinforcement learning. They all achieve promising improvement but rely on non-standard training frameworks. Multitask learning has been used in past work to combine models trained on different parallel corpora by sharing certain components. These components, such as the attention mechanism (Firat et al., 2016), benefit from being trained on an effectively larger dataset. In addition, the more parameters are shared, the faster a joint model can be trained — this is particularity beneficial in industry settings. Baidu built one-to-many translation systems by sharing both encoder and attention (Dong et al., 2015). Google enabled a standard NMT framework to support many-to-many translation directions by simply attaching a language specifier to each source sentence (Johnson et al., 2017). We adopted Google’s approach to build bidirectional systems that successfully combine actual and synthetic parallel data. 5 17-C-9117. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprint"
W18-2710,E17-2025,0,0.0296652,"al., 2015). We apply layer normalization (Ba et al., 2016) and tie source and target embedding parameters. We train using the Adam optimizer with a batch size of 64 sentences and checkpoint the model every 1000 updates (10,000 for DE↔EN) (Kingma and Ba, 2015). Training stops after 8 checkpoints without improvement of perplexity on the development set. We decode with a beam size of 5. For TL↔EN and SW↔EN, we add dropout to embeddings and RNNs of the encoder and decoder with probability 0.2. We also tie the output layer’s weight matrix with the source and target embeddings to reduce model size (Press and Wolf, 2017). The effectiveness of tying input/output target embeddings has been verified on several low-resource language pairs (Nguyen and Chiang, 2018). For TL↔EN and SW↔EN, we train four randomly seeded models for each experiment and combine them in a linear ensemble for decodGerman↔English (DE↔EN), Tagalog↔English TL↔EN, and Swahili↔English (SW↔EN). Parallel and monolingual DE↔EN data are provided by the WMT17 news translation task (Bojar et al., 2017). Parallel data for TL↔EN and SW↔EN contains a mixture of domains such as news and weblogs, and is provided as part of the IARPA MATERIAL program.2 We"
W18-2710,N16-1101,0,0.0284705,"hen f 0 and f should be identical (Brislin, 1970). Cheng et al. (2016) optimize arg maxθ P (f 0 |f ; θ) as an autoencoder; Wang et al. (2018) minimize the difference between P (f ) and P (f 0 |θ) based on the law of total probability, while He et al. (2016) set the quality of both e and f 0 as rewards for reinforcement learning. They all achieve promising improvement but rely on non-standard training frameworks. Multitask learning has been used in past work to combine models trained on different parallel corpora by sharing certain components. These components, such as the attention mechanism (Firat et al., 2016), benefit from being trained on an effectively larger dataset. In addition, the more parameters are shared, the faster a joint model can be trained — this is particularity beneficial in industry settings. Baidu built one-to-many translation systems by sharing both encoder and attention (Dong et al., 2015). Google enabled a standard NMT framework to support many-to-many translation directions by simply attaching a language specifier to each source sentence (Johnson et al., 2017). We adopted Google’s approach to build bidirectional systems that successfully combine actual and synthetic parallel"
W18-2710,D17-1039,0,0.154265,"Missing"
W18-2710,P16-1009,0,0.313271,"data is prohibitively expensive or otherwise impractical to collect, whereas monolingual data may be more abundant. SMT systems have the advantage of a dedicated language model that can incorporate all available target-side monolingual data to significantly improve translation quality (Koehn et al., 2003; Koehn and Schroeder, 2007). By contrast, NMT systems consist of one large neural network that performs full sequence-to-sequence translation (Sutskever et al., 2014; Cho et al., 2014). Trained end-to-end on parallel data, these models lack a direct avenue for incorporating monolingual data. Sennrich et al. (2016a) overcome this challenge by back-translating target monolingual data to produce synthetic parallel data that can be added to the training pool. While effective, backtranslation introduces the significant cost of first building a reverse system. Another technique for overcoming a lack of data is multitask learning, in which domain knowledge can be transferred between related tasks (Caruana, 1997). Johnson et al. (2017) apply the idea to multilingual NMT by concatenating parallel data of various language pairs and marking the source with the desired output language. The authors report promisin"
W18-2710,P16-1162,0,0.822112,"data is prohibitively expensive or otherwise impractical to collect, whereas monolingual data may be more abundant. SMT systems have the advantage of a dedicated language model that can incorporate all available target-side monolingual data to significantly improve translation quality (Koehn et al., 2003; Koehn and Schroeder, 2007). By contrast, NMT systems consist of one large neural network that performs full sequence-to-sequence translation (Sutskever et al., 2014; Cho et al., 2014). Trained end-to-end on parallel data, these models lack a direct avenue for incorporating monolingual data. Sennrich et al. (2016a) overcome this challenge by back-translating target monolingual data to produce synthetic parallel data that can be added to the training pool. While effective, backtranslation introduces the significant cost of first building a reverse system. Another technique for overcoming a lack of data is multitask learning, in which domain knowledge can be transferred between related tasks (Caruana, 1997). Johnson et al. (2017) apply the idea to multilingual NMT by concatenating parallel data of various language pairs and marking the source with the desired output language. The authors report promisin"
W18-2710,E17-3017,0,0.0393023,"and apply data selection to choose pseudo in-domain data (see Section 2.2). We use the training data as in-domain and either Common Crawl or ICWSM as out-of-domain. We also include a low-resource, long-distance domain adaptation task for these languages: training on News/Blog data and testing on Bible data. We split a parallel Bible corpus (Christodoulopoulos and Steedman, 2015) into sample, dev, and test sets, using the sample data as the in-domain seed for data selection. 4,356,324 5,168 3,004 26,982,051 18,238,848 50,705 491/508 500/500 61,195 26,788,048 48,219,743 Preprocessing: Following Hieber et al. (2017), we apply four pre-processing steps to parallel data: normalization, tokenization, sentencefiltering (length 80 cutoff), and joint source-target BPE with 50,000 operations (Sennrich et al., 2016b). Low-resource language pairs are also true-cased to reduce sparsity. BPE and truecasing models are rebuilt whenever the training data changes. Monolingual data for low-resource settings is filtered by retaining sentences longer than nine tokens. Itemized data statistics after preprocessing can be found in Table 1. 23,900 491/509 500/500 14,699 12,158,524 48,219,743 Table 1: Data sizes of training, d"
W18-2710,Q17-1024,0,0.0833919,"Missing"
W18-2710,D16-1160,0,0.0319126,"NMT model with standard architecture that performs all forward and backward translation during training. from both source and target monolingual data can be translated to produce synthetic sentence pairs. Synthetic parallel data of the form synthetic → monolingual can then be used in the forward direction, the backward direction, or both. Crucially, this approach leverages both source and target monolingual data while always placing the real data on the target side, eliminating the need for work-arounds such as freezing certain model parameters to avoid degradation from training on MT output (Zhang and Zong, 2016). • Training costs reduced significantly compared to uni-directional systems. • Improvements in translating quality for lowresource languages, even over uni-directional systems with back-translation. • Effectiveness in domain adaptation. Via comprehensive experiments, we also contribute to best practices in selecting most suitable combinations of synthetic parallel data and choosing appropriate amount of monolingual data. 2 2.2 Given the goal of improving a base bi-directional model, selecting ideal monolingual data for backtranslation presents a significant challenge. Data too close to the or"
W18-2710,W16-2316,0,0.0194349,"5 31.71 21.71 30.43 22.54 31.83 24.61 31.94 24.45 32.49 25.20 Table 2: BLEU scores for uni-directional models (U-*) and bi-directional NMT models (B-*) trained on different combinations of real and synthetic parallel data. Models in B-5* are fine-tuned from base models in B-1. Best models in B-6* are fine-tuned from precedent models in B-5* and underscored synthetic data is re-decoded using precedent models. Scores with largest improvement within each zone are highlighted. ing. For DE↔EN experiments, we train a single model and average the parameters of the best four checkpoints for decoding (Junczys-Dowmunt et al., 2016). We report case-insensitive BLEU with standard WMT tokenization.3 3.3 ent combinations of real and synthetic parallel data. Models trained on only real data of target language (i.e. in U-2) achieve better performance in BLEU than using other combinations. This is an expected result since translation quality is highly correlated with target language models. By contrast, standard back-translation is not effective for our low-resource scenarios. A significant drop (∼7 BLEU comparing U-1 and U-2 for TL/SW→EN) is observed when back-translating English. One possible reason is that the quality of th"
W18-6431,P17-1000,0,0.105645,"Chinese↔English news translation tasks. Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data. Using ensembling and reranking, we improve over the Transformer baseline by +1.4 BLEU for Chinese→English and +3.97 BLEU for English→Chinese on newstest2017. Our best systems reach BLEU scores of 24.4 for Chinese→English and 39.0 for English→Chinese on newstest2018. 1 Introduction While machine translation between Chinese and English has long been considered a challenging task, with performance lagging behind other language pairs (Bojar et al., 2017), neural architectures have helped achieve large improvements. A new state-of-the-art on Chinese→English news translation was recently obtained (Hassan et al., 2018) using a deep Transformer model in combination with many other techniques including Dual Learning (He et al., 2016), joint training of source-to-target and target-to-source models, and Deliberation Networks (Xia et al., 2017). The resulting high quality translation comes at the cost of large models and complex training pipelines, which make such models difficult to train and deploy with constrained resources. In this shared task, o"
W18-6431,I17-2007,0,0.0408103,"Missing"
W18-6431,P16-1162,0,0.330005,"X n=1 t=1 (n) (n) log p(yt |ht−1 , Attn; Θdec ) Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 535–540 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64058 (1) We therefore only experiment with the Transformer and RNN architectures. where (n) Attn = f attn (f enc (x(n) ; Θenc), ht−1 ) (n) ht−1 (2) 2.2 denotes the decoder’s hidden states conBacktranslating Monolingual Data We leverage the monolingual data provided in the shared task using backtranslation (Sennrich et al., 2016a). For each language pair, we select monolingual corpora from the target language based on their similarity to the parallel corpus as measured by cross-entropy difference (Moore and Lewis, 2010). Following the setup from Hassan et al. (2018), we backtranslate the monolingual data using a single Transformer model, and then use a mixture of parallel and backtranslated monolingual data with a proportion of 2:1 for training a new Transformer model. (n) y<t , ditioned on the target words preceding step t. The attention model f attn computes a weighted sum over the encoder’s outputs f enc (x(n) ; Θ"
W18-6431,E17-3017,0,0.112907,"s are computed by a function of the query and the corresponding key. Instead of computing a single attention pass, multi-head attention consists of several stacked attention layers in which the same attention function is applied to different transformations of the query, keys and values. And then the output vectors from the above attention layers are concatenated together and linearly transformed, resulting in the final output. The Transformer model has achieved significant improvements over RNN-based encoder-decoders on several NMT tasks (Vaswani et al., 2017), while RNNs outperform ConvS2S (Hieber et al., 2017). 2.3 Reranking n-best Hypotheses In order to improve the translation quality, we rerank the n-best results using features extracted from different NMT models (Cherry and Foster, 2012; Neubig et al., 2015; Hassan et al., 2018). Right-to-left NMT Model Sequence-tosequence models generate sequences on a token-by-token basis, and suffer from the exposure bias problem (Bengio et al., 2015). Exposure bias refers to the problem that models are trained using contexts from human generated references while tested using model-generated contexts, and thus at test time previous mistakes may be amplified a"
W18-6431,P07-2045,0,0.0151962,"in both Chinese→English and English→Chinese tasks, we can just use the models trained in the opposite direction for reranking. 536 Reranking Model First we generate n-best translation hypotheses for each source sentence. Then we get the perplexity scores for each hypothesis with L2R, R2L, and T2S models. The scores are treated as features which we use to train a kbest batch MIRA ranker (Cherry and Foster, 2012) to find out the optimal weights for reranking. 3 Preprocessing All corpora are processed consistently. We tokenize the English sentences and perform truecasing with the Moses scripts (Koehn et al., 2007). Chinese sentences are segmented with the Jieba segmenter2 . We segment English and Chinese tokens into subwords via Byte-pair Encoding (BPE) (Sennrich et al., 2016b). We train the BPE models for English and Chinese separately, and use 32K subwords for each side. Data and Preprocessing Parallel Data We use all the parallel data available for the shared tasks. The training data for both tasks consists of about 15.8M sentence pairs from the UN Parallel Corpus, 9M sentence pairs from the CWMT Corpora, 332K sentence pairs from the News Commentary Corpus. In addition to the criteria used in Hassan"
W18-6431,W17-4742,0,0.0961755,"ble +reranking (L2R, T2S) +reranking (L2R, T2S, R2L) +beam size from 10 to 30 BLEU 20.99 21.65 24.00 24.12 24.76 25.20 25.37 25.41 Table 4: English → Chinese Results on newstest2017. The submitted system is the last one. menting the training data with the backtranslated monolingual data improves BLEU by +2.2 points. The ensemble model improves over the single best system by +1.6 BLEU. Rescoring with L2R, R2L, and T2S models brings an improvement of +0.1 BLEU. We further increase the beam size from 10 to 30 to gain more from reranking. Our submitted system outperforms the best system in WMT17 (Wang et al., 2017) by +2.1 BLEU on newstest2017 and obtains a BLEU score of 39.0 on the official test set. We note that the components added to the baseline Transformer model have an asymmetric impact in the two translation directions. While backtranslation improves the results by +2.2 BLEU for the English→Chinese task, it doesn’t help as much for Chinese→English (+0.1). In contrast, rescoring with L2R, R2L, and T2S models brings more improvements for Chinese→English (+0.6) than the other (+0.2). One possible explanation is that in a parallel corpus sentences originally written in language A and sentences trans"
W18-6431,W17-3204,0,0.0398793,"Missing"
W18-6431,P10-2041,0,0.0890963,"overmber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64058 (1) We therefore only experiment with the Transformer and RNN architectures. where (n) Attn = f attn (f enc (x(n) ; Θenc), ht−1 ) (n) ht−1 (2) 2.2 denotes the decoder’s hidden states conBacktranslating Monolingual Data We leverage the monolingual data provided in the shared task using backtranslation (Sennrich et al., 2016a). For each language pair, we select monolingual corpora from the target language based on their similarity to the parallel corpus as measured by cross-entropy difference (Moore and Lewis, 2010). Following the setup from Hassan et al. (2018), we backtranslate the monolingual data using a single Transformer model, and then use a mixture of parallel and backtranslated monolingual data with a proportion of 2:1 for training a new Transformer model. (n) y<t , ditioned on the target words preceding step t. The attention model f attn computes a weighted sum over the encoder’s outputs f enc (x(n) ; Θenc) where the weights are determined by the “similarity” between each of the encoder’s outputs and the (n) decoder’s hidden state ht−1 . State-of-the-art NMT encoders and decoders include Stacke"
W18-6431,W15-5003,0,0.0263108,"ion function is applied to different transformations of the query, keys and values. And then the output vectors from the above attention layers are concatenated together and linearly transformed, resulting in the final output. The Transformer model has achieved significant improvements over RNN-based encoder-decoders on several NMT tasks (Vaswani et al., 2017), while RNNs outperform ConvS2S (Hieber et al., 2017). 2.3 Reranking n-best Hypotheses In order to improve the translation quality, we rerank the n-best results using features extracted from different NMT models (Cherry and Foster, 2012; Neubig et al., 2015; Hassan et al., 2018). Right-to-left NMT Model Sequence-tosequence models generate sequences on a token-by-token basis, and suffer from the exposure bias problem (Bengio et al., 2015). Exposure bias refers to the problem that models are trained using contexts from human generated references while tested using model-generated contexts, and thus at test time previous mistakes may be amplified and lead to subsequent errors. In order to address this issue, we train a right-to-left (R2L) NMT model using the same training data but with inverted target data. Then for each hypothesis from the n-best"
W18-6431,Q16-1027,0,0.110139,"ing the setup from Hassan et al. (2018), we backtranslate the monolingual data using a single Transformer model, and then use a mixture of parallel and backtranslated monolingual data with a proportion of 2:1 for training a new Transformer model. (n) y<t , ditioned on the target words preceding step t. The attention model f attn computes a weighted sum over the encoder’s outputs f enc (x(n) ; Θenc) where the weights are determined by the “similarity” between each of the encoder’s outputs and the (n) decoder’s hidden state ht−1 . State-of-the-art NMT encoders and decoders include Stacked RNNs (Zhou et al., 2016), convolutional sequence-to-sequence models (ConvS2S) (Gehring et al., 2017), and Transformer models (Vaswani et al., 2017). The ConvS2S and Transformer models differ from RNNs in that they replace the recurrent processing in RNNs with convolutional representation and self-attention respectively, which enable the parallelization of the computation and make the encoded representation less sensitive to the sequence length. ConvS2S uses stacked convolutional representation to model the dependencies between nearby words on lower layers, while longer-distance dependencies are modeled through upper"
W18-6431,E17-2025,0,0.0263712,"ignment score given by the fast-align toolkit1 . The overall criteria are the following: 4 4.1 Baseline systems The baseline system is a bidirectional RNN with attention mechanism as used in Bahdanau et al. (2014). Our systems are built on Sockeye (Hieber et al., 2017). We use word embedding size of 1024 and hidden layer size of 1024. We filter out sentences with length larger than 50. We use Adam optimizer with initial learning rate of 0.0002. We adopt layer normalization (Ba et al., 2016) and label smoothing (Szegedy et al., 2016). We tie the output weight matrix with the target embeddings (Press and Wolf, 2017). The beam size is set to 10. The deep RNN is based on Stacked RNNs with attention (Zhou et al., 2016). We use the same system settings as the baseline but set the number of stack layers to 4. The Transformer network (Section 2.1) is a 6-layer Transformer model with model size of 1024, feed-forward network size of 4096, and 16 attention heads. We adopt label smoothing and weight tying, and set the beam size to 10. Table 2 shows the total number of parameters for each model and the BLEU scores on Chinese→English and English→Chinese newstest2017. Results show that the Transformer outperforms RNN"
W18-6431,P16-1009,0,0.106978,"X n=1 t=1 (n) (n) log p(yt |ht−1 , Attn; Θdec ) Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 535–540 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64058 (1) We therefore only experiment with the Transformer and RNN architectures. where (n) Attn = f attn (f enc (x(n) ; Θenc), ht−1 ) (n) ht−1 (2) 2.2 denotes the decoder’s hidden states conBacktranslating Monolingual Data We leverage the monolingual data provided in the shared task using backtranslation (Sennrich et al., 2016a). For each language pair, we select monolingual corpora from the target language based on their similarity to the parallel corpus as measured by cross-entropy difference (Moore and Lewis, 2010). Following the setup from Hassan et al. (2018), we backtranslate the monolingual data using a single Transformer model, and then use a mixture of parallel and backtranslated monolingual data with a proportion of 2:1 for training a new Transformer model. (n) y<t , ditioned on the target words preceding step t. The attention model f attn computes a weighted sum over the encoder’s outputs f enc (x(n) ; Θ"
W19-5308,Y17-1038,0,0.0227803,"@cs.umd.edu, marine@cs.umd.edu Abstract translation task by transfer learning from Finnish→English. Interestingly, Kocmi and Bojar (2018) observed that the transfer learning approach is still effective when there is no relatedness between the “child” and “parent” language-pairs and also hypothesize that the size of the parent training set is the most important factor leading to translation quality improvements. However, previous work has also empirically validated that transfer learning benefits most when “child”-“parent” languages belong to the same or linguistically similar language family (Dabre et al., 2017). Specifically, Nguyen and Chiang (2017) showed consistent improvements in two Turkic languages via transfering from another related, low-resource language. Taking those recent results into consideration, our main focus at WMT19 is to examine transfer learning for the Kazakh–English language pair using additional parallel data from Turkish–English. While using distinct writing systems, both source languages belong to the Turkic language family and preserve many morphological and syntactic features common for that group (Kessikbayeva and Cicekli, 2014). As a result, they constitute a suitable “"
W19-5308,N18-1032,0,0.0234106,"monolingual text (Sennrich et al., 2016; He et al., 2016), we focus on transfer learning from another language pair (Zoph et al., 2016; Nguyen and Chiang, 2017; Lakew et al., 2018). In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called “parent” language pair and then the trained model is used to initialize a “child” model which is further trained on a low-resource language pair. Similar approaches that support cross-lingual transfer learning for Multi-lingual NMT train a model on the concatenation of all data instead of employing sequential training (Gu et al., 2018; Zhou et al., 2018; Wang et al., 2019). Transfer learning has been found effective in submissions to WMT in previous years: Kocmi et al. (2018) reported improvements of +2.4 BLEU on the low-resource Estonian→English • How can we represent lexical units to exploit vocabulary overlap between languages? We compare bilingual and monolingual byte-pair encoding models with the recently proposed soft decoupled encoding model. • How can we leverage both “child” and “parent” parallel data to obtain synthetic backtranslated data from monolingual resources? 134 Proceedings of the Fourth Conference on Ma"
W19-5308,W14-2806,0,0.0219401,"to the same or linguistically similar language family (Dabre et al., 2017). Specifically, Nguyen and Chiang (2017) showed consistent improvements in two Turkic languages via transfering from another related, low-resource language. Taking those recent results into consideration, our main focus at WMT19 is to examine transfer learning for the Kazakh–English language pair using additional parallel data from Turkish–English. While using distinct writing systems, both source languages belong to the Turkic language family and preserve many morphological and syntactic features common for that group (Kessikbayeva and Cicekli, 2014). As a result, they constitute a suitable “child”-“parent” language-pair choice for exploring transfer learning between related lowresource languages. In this direction, we conduct experiments to address the following questions: This paper describes the University of Maryland’s submission to the WMT 2019 Kazakh to English news translation task. We study the impact of transfer learning from another lowresource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. T"
W19-5308,W18-6325,0,0.0660556,"Missing"
W19-5308,W18-6416,0,0.0309875,"en and Chiang, 2017; Lakew et al., 2018). In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called “parent” language pair and then the trained model is used to initialize a “child” model which is further trained on a low-resource language pair. Similar approaches that support cross-lingual transfer learning for Multi-lingual NMT train a model on the concatenation of all data instead of employing sequential training (Gu et al., 2018; Zhou et al., 2018; Wang et al., 2019). Transfer learning has been found effective in submissions to WMT in previous years: Kocmi et al. (2018) reported improvements of +2.4 BLEU on the low-resource Estonian→English • How can we represent lexical units to exploit vocabulary overlap between languages? We compare bilingual and monolingual byte-pair encoding models with the recently proposed soft decoupled encoding model. • How can we leverage both “child” and “parent” parallel data to obtain synthetic backtranslated data from monolingual resources? 134 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 134–140 c Florence, Italy, August 1-2, 2019. 2019 Association for Computatio"
W19-5308,P16-1009,0,0.294271,"ical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh–only baseline by +5.45 BLEU on newstest2019. 1 Introduction Neural Machine Translation (NMT) outperforms traditional phrase-based statistical machine translation provided that large amounts of parallel data are available (Bahdanau et al., 2014; Sennrich et al., 2017; Vaswani et al., 2017). However, it performs poorly under low-resource conditions (Koehn and Knowles, 2017). While much work addresses this problem via semi-supervised learning from monolingual text (Sennrich et al., 2016; He et al., 2016), we focus on transfer learning from another language pair (Zoph et al., 2016; Nguyen and Chiang, 2017; Lakew et al., 2018). In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called “parent” language pair and then the trained model is used to initialize a “child” model which is further trained on a low-resource language pair. Similar approaches that support cross-lingual transfer learning for Multi-lingual NMT train a model on the concatenation of all data instead of employing sequential training (Gu et al., 2018; Zhou et al., 2018; Wan"
W19-5308,P07-2045,0,0.0104629,"similar to our main language of interest, we Empty source baseline The source side of each monolingual example sentence is linked to an 2 Romanization https://www.isi.edu/~ulf/uroman.html 136 introduce two artificial tokens (<2kk>, <2tr>) at the beginning of the input sentence to indicate the target language the model should translate to (Johnson et al., 2017). After the reversed system is trained we back-translate each target sentence to a Kazakh synthetic sentence.3 3 Pre-processing We process all corpora consistently. We tokenize the sentences and perform truecasing with the Moses scripts (Koehn et al., 2007). For all the experiments we consistenly use 8K BPEs on the English target side. We experiment with {32, 64}K merge operations for the models using BPE encoding and {4, 5} n-grams for the SDE framework. To establish a fair comparison between the source language representations, we consistently use the same encoding for English words (target side) using BPEs learned on the concatenation of all the English data. Model Configuration Our NMT systems are built upon the publicly available code4 of Wang et al. (2019) and are sequence-to-sequence 1-layer attentional longshort term memory units (LSTMs)"
W19-5308,W17-3204,0,0.0185773,"another lowresource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh–only baseline by +5.45 BLEU on newstest2019. 1 Introduction Neural Machine Translation (NMT) outperforms traditional phrase-based statistical machine translation provided that large amounts of parallel data are available (Bahdanau et al., 2014; Sennrich et al., 2017; Vaswani et al., 2017). However, it performs poorly under low-resource conditions (Koehn and Knowles, 2017). While much work addresses this problem via semi-supervised learning from monolingual text (Sennrich et al., 2016; He et al., 2016), we focus on transfer learning from another language pair (Zoph et al., 2016; Nguyen and Chiang, 2017; Lakew et al., 2018). In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called “parent” language pair and then the trained model is used to initialize a “child” model which is further trained on a low-resource language pair. Similar approaches that support cross-lingual transfer learning for Multi-lingual NMT train a model"
W19-5308,W18-6324,0,0.0267559,"(Sennrich et al., 2016; He et al., 2016), we focus on transfer learning from another language pair (Zoph et al., 2016; Nguyen and Chiang, 2017; Lakew et al., 2018). In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called “parent” language pair and then the trained model is used to initialize a “child” model which is further trained on a low-resource language pair. Similar approaches that support cross-lingual transfer learning for Multi-lingual NMT train a model on the concatenation of all data instead of employing sequential training (Gu et al., 2018; Zhou et al., 2018; Wang et al., 2019). Transfer learning has been found effective in submissions to WMT in previous years: Kocmi et al. (2018) reported improvements of +2.4 BLEU on the low-resource Estonian→English • How can we represent lexical units to exploit vocabulary overlap between languages? We compare bilingual and monolingual byte-pair encoding models with the recently proposed soft decoupled encoding model. • How can we leverage both “child” and “parent” parallel data to obtain synthetic backtranslated data from monolingual resources? 134 Proceedings of the Fourth Conference on Machine Translation ("
W19-5308,D18-1103,0,0.0196114,"a lexical representation c(w), the word is looked up to an embedding matrix Wc ∈ RC×D as shown below: Joint BPEs (JBPEs) BPEs are learned jointly from the concatenation of “child” and “parent” parallel data. The advantage of this strategy is that the sub-word segmentations of related words in the two languages are encouraged to be more aligned; thus enabling the sharing of their representations on the source side due to a larger vocabulary overlap. Although, the “child” language might be “overwhelmed” by the “parent” language when there is a significant difference in the amount of their data (Neubig and Hu, 2018). This could lead to over-segmentation of the “child” language and subsequently limit the expressive power of the NMT system. c(w) = tanh(BoN(w) · Wc ) (1) Language Specific Transformation Next each word is passed through a language dependent transformation. For each language Li a matrix WLi ∈ RD×D is learned and the transformed embeddings ci (w) is computed: ci (w) = tanh(c(w) · WLi ) (2) Latent Semantic Embedding The shared semantic concepts among languages are represented by a matrix Ws ∈ RS×D , where S corresponds to the number of semantic concepts a language can express. The latent embedd"
W19-5308,D16-1163,0,0.0180514,"ted system improves over a Kazakh–only baseline by +5.45 BLEU on newstest2019. 1 Introduction Neural Machine Translation (NMT) outperforms traditional phrase-based statistical machine translation provided that large amounts of parallel data are available (Bahdanau et al., 2014; Sennrich et al., 2017; Vaswani et al., 2017). However, it performs poorly under low-resource conditions (Koehn and Knowles, 2017). While much work addresses this problem via semi-supervised learning from monolingual text (Sennrich et al., 2016; He et al., 2016), we focus on transfer learning from another language pair (Zoph et al., 2016; Nguyen and Chiang, 2017; Lakew et al., 2018). In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called “parent” language pair and then the trained model is used to initialize a “child” model which is further trained on a low-resource language pair. Similar approaches that support cross-lingual transfer learning for Multi-lingual NMT train a model on the concatenation of all data instead of employing sequential training (Gu et al., 2018; Zhou et al., 2018; Wang et al., 2019). Transfer learning has been found effective in submissions to WMT in previous y"
W19-5308,I17-2050,0,0.217594,"ract translation task by transfer learning from Finnish→English. Interestingly, Kocmi and Bojar (2018) observed that the transfer learning approach is still effective when there is no relatedness between the “child” and “parent” language-pairs and also hypothesize that the size of the parent training set is the most important factor leading to translation quality improvements. However, previous work has also empirically validated that transfer learning benefits most when “child”-“parent” languages belong to the same or linguistically similar language family (Dabre et al., 2017). Specifically, Nguyen and Chiang (2017) showed consistent improvements in two Turkic languages via transfering from another related, low-resource language. Taking those recent results into consideration, our main focus at WMT19 is to examine transfer learning for the Kazakh–English language pair using additional parallel data from Turkish–English. While using distinct writing systems, both source languages belong to the Turkic language family and preserve many morphological and syntactic features common for that group (Kessikbayeva and Cicekli, 2014). As a result, they constitute a suitable “child”-“parent” language-pair choice for"
W19-5308,P02-1040,0,0.103565,"g BPEs learned on the concatenation of all the English data. Model Configuration Our NMT systems are built upon the publicly available code4 of Wang et al. (2019) and are sequence-to-sequence 1-layer attentional longshort term memory units (LSTMs) with a hidden dimension of 512 for both the encoder and the decoder. The word embedding dimension is kept at 128, and all other layer dimensions are set to 512. We use a dropout rate of 0.3 for the word embedding and the output vector before the decoder Softmax layer. The batch size is set to be 1500 words. We evaluate by development set BLEU score (Papineni et al., 2002) for every 2500 training batches. For training, we use the Adam optimizer with a learning rate of 0.001. We use learning rate decay of 0.8, and stop training if the model performance on development set doesn’t improve for 5 evaluation steps. We run each experiment with 3 different random seeds. 4 Tuning and Testing Data The official newsdev2019 is used as the validation set, and newstest2019 is used as the test set. 5 Experiments Starting from Baseline BPE-based NMT systems trained using only the Kazakh data provided by the competition, we conduct the following experiments. 5.1 Byte Pair Encod"
W19-5308,W17-4739,0,0.0178234,"n to the WMT 2019 Kazakh to English news translation task. We study the impact of transfer learning from another lowresource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh–only baseline by +5.45 BLEU on newstest2019. 1 Introduction Neural Machine Translation (NMT) outperforms traditional phrase-based statistical machine translation provided that large amounts of parallel data are available (Bahdanau et al., 2014; Sennrich et al., 2017; Vaswani et al., 2017). However, it performs poorly under low-resource conditions (Koehn and Knowles, 2017). While much work addresses this problem via semi-supervised learning from monolingual text (Sennrich et al., 2016; He et al., 2016), we focus on transfer learning from another language pair (Zoph et al., 2016; Nguyen and Chiang, 2017; Lakew et al., 2018). In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called “parent” language pair and then the trained model is used to initialize a “child” model which is further trained on a low-resource languag"
W19-6623,W05-0909,0,0.0307904,"nslate and the other was generated by extracting example sentences from a syntax textbook. The MT-generated English data is most similar to our problem, and the most effective models for that data were the word-based scores from the language model. 6 Figure 4: Percent fluently inadequate at each checkpoint during in-domain training 5 Related Work MT quality metrics are judged based on their correlation with human judgments, and recently that has meant human adequacy judgments (Bojar et al., 2017). This indicates that any of the common MT metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) may also serve as baseline adequacy scores. However, they incorporate elements of fluency while we wish to separate fluency and adequacy. Adequacy is, essentially, semantic equivalence and the goal of SemEval’s Semantic Textual Similarity (STS) task is to measure the degree of semantic equivalence between two sentences (Cer et al., 2017). The cross-lingual version of the task is similar enough to quality estimation that one of the data sets for 2017 actually came from the WMT Proceedings of MT Summit XVII, volume 1 Conclusion We have introduced an approach to automatically detect fluently ina"
W19-6623,D16-1025,0,0.0156047,"n quality. We find that neural models are consistently more prone to this type of error than traditional statistical models. However, improving the overall quality of the MT system such as through domain adaptation reduces these errors. 1 Introduction Recent work has shown that well-trained, indomain neural machine translation (NMT) systems can produce translations that, at the sentence level, are rated on par with human reference translations (Hassan Awadalla et al., 2018). Part of this success comes from the impressive improvements in fluency of NMT output compared to previous MT paradigms (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate transl"
W19-6623,Q17-1010,0,0.0194615,"Missing"
W19-6623,W07-0718,0,0.0510771,"rics subset of the data as gold standard human judgments, and we use the reference translations from the news subset in generating synthetic inadequate examples. We use the standardized human direct assessment adequacy scores from WMT16 (Bojar et al., 2016) as gold standard in determining how well each adequacy metric correlates with human judgments. However, for binary questionable/acceptable adequacy judgments, we must be sure that the inadequate examples are clearly inadequate regardless of fluency and other MT quirks. The high correlation between human judgments of fluency and adequacy in Callison-Burch et al (2007) and Graham et al (2017) may indicate that human adequacy judgments are influenced by fluency, lowering the adequacy scores of disfluent translations. To ensure that our inadequate examples are truly inadequate, we rely on synthetic examples. We generate synthetic low adequacy translations by randomly selecting pairs of reference translations from the WMT16 news task Dublin, Aug. 19-23, 2019 |p. 236 Adequacy Metric BLEU Averaged Embeddings BVSS BVSS-Reference BVSS-System CS-EN 0.54275 0.43905 0.61286 0.62178 0.53773 DE-EN 0.41975 0.18998 0.47068 0.47877 0.38887 FI-EN 0.41460 0.31218 0.51856 0."
W19-6623,S17-2001,0,0.0126721,"MT quality metrics are judged based on their correlation with human judgments, and recently that has meant human adequacy judgments (Bojar et al., 2017). This indicates that any of the common MT metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) may also serve as baseline adequacy scores. However, they incorporate elements of fluency while we wish to separate fluency and adequacy. Adequacy is, essentially, semantic equivalence and the goal of SemEval’s Semantic Textual Similarity (STS) task is to measure the degree of semantic equivalence between two sentences (Cer et al., 2017). The cross-lingual version of the task is similar enough to quality estimation that one of the data sets for 2017 actually came from the WMT Proceedings of MT Summit XVII, volume 1 Conclusion We have introduced an approach to automatically detect fluently inadequate translations in machine translation output based on automatic fluency and adequacy metrics. Applying this technique to a diverse set of statistical and neural MT systems, we found that although fluently inadequate translations are rare, NMT does appear to be consistently more prone to this type of error compared to SMT. Improving"
W19-6623,P13-2121,0,0.013361,"resholds for high fluency and dubious adequacy. 3.1 Fluency Experiments Task For WMT16, fluency judgments were collected for Czech-English (CS-EN), GermanEnglish (DE-EN), Finnish-English (FI-EN), Romanian-English (RO-EN), Russian-English (RU-EN), and Turkish-English (TR-EN) in the news shared task. Annotations were collected with the goal of system-level reliability, so many segments only have one judgment. To improve reliability, we use only segments where there are two or more judgments. Model setup Fluency scores are based on a 5gram language model. We built a 5-gram KenLM (Heafield, 2011; Heafield et al., 2013) language model using the monolingual news training data from WMT16. Results For each of the metrics described in section 2.1, we calculated the Pearson correlation with the direct assessment scores for each of the language pair data sets and for all the data combined. Results are shown in Table 1. Although these correlations are lower than we would like, we find that for all language pairs and for the combined data, Word LP mid yields the highest correlation, so we will use this formula for our fluency prediction metric. Dublin, Aug. 19-23, 2019 |p. 235 Fluency Metric MeanLP NormLP Word LPmin"
W19-6623,W11-2123,0,0.0124884,"to determine thresholds for high fluency and dubious adequacy. 3.1 Fluency Experiments Task For WMT16, fluency judgments were collected for Czech-English (CS-EN), GermanEnglish (DE-EN), Finnish-English (FI-EN), Romanian-English (RO-EN), Russian-English (RU-EN), and Turkish-English (TR-EN) in the news shared task. Annotations were collected with the goal of system-level reliability, so many segments only have one judgment. To improve reliability, we use only segments where there are two or more judgments. Model setup Fluency scores are based on a 5gram language model. We built a 5-gram KenLM (Heafield, 2011; Heafield et al., 2013) language model using the monolingual news training data from WMT16. Results For each of the metrics described in section 2.1, we calculated the Pearson correlation with the direct assessment scores for each of the language pair data sets and for all the data combined. Results are shown in Table 1. Although these correlations are lower than we would like, we find that for all language pairs and for the combined data, Word LP mid yields the highest correlation, so we will use this formula for our fluency prediction metric. Dublin, Aug. 19-23, 2019 |p. 235 Fluency Metric"
W19-6623,E17-3017,0,0.0265997,"ded in the Russian and Chinese WMT17 data 5 http://cwiki.apache.org/confluence/display/JOSHUA/ 3 Dublin, Aug. 19-23, 2019 |p. 238 Joshua General Joshua TED Only Joshua Adapted Sockeye General Sockeye TED Only Sockeye Adapted Arabic 23.50 24.49 27.11 29.6 27.42 35.37 German 30.65 28.72 31.35 34.59 32.25 39.9 Farsi 13.41 16.56 17.71 22.22 21.31 27.92 Korean 6.34 9.81 10.24 11.56 14.4 17.22 Russian 24.49 21.85 25.23 28.6 22.9 28.6 Chinese 14.79 13.32 15.70 15.92 16.18 20.37 Table 6: BLEU scores for all systems on TED dev data. 4.1.3 Neural MT Systems The neural systems were built using Sockeye6 (Hieber et al., 2017). The systems used two LSTM layers in both encoder and decoder with hidden size 512 and word embeddings dimension 512. We used a batch size of 4096 and created a checkpoint every 4000 mini-batches. Our systems employed the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.0003. As with the SMT, we built three models for each language: Sockeye General, Sockeye In-Domain, and Sockeye Domain-Adapted. The Sockeye General and In-Domain models were trained with the same data as the corresponding SMT models. The Sockeye Domain-Adapted models were trained using continued training"
W19-6623,D18-1330,0,0.0228862,"Missing"
W19-6623,W17-3204,0,0.284986,"eference translations (Hassan Awadalla et al., 2018). Part of this success comes from the impressive improvements in fluency of NMT output compared to previous MT paradigms (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate translations may mislead users into trusting the content based on fluency alone–particularly in the context of other fluent and adequate translations (Martindale and Carpuat, 2018). Mitigating the effects of fluently inadequate translations first requires understanding the scale of the problem and what situations are likely to generate these errors. The general success and high system level quality of NMT suggests that fluently i"
W19-6623,W18-1803,1,0.861989,"ivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate translations may mislead users into trusting the content based on fluency alone–particularly in the context of other fluent and adequate translations (Martindale and Carpuat, 2018). Mitigating the effects of fluently inadequate translations first requires understanding the scale of the problem and what situations are likely to generate these errors. The general success and high system level quality of NMT suggests that fluently inadequate translations are rare, but we cannot say how rare without a means of automatically identifying potentially fluently inadequate translations in large collections of MT output. In this work, we propose a method to automatically detect fluently inadequate translations based on the underlying characteristics of fluency and adequacy. We vie"
W19-6623,P02-1040,0,0.103547,"pping sentences through Google Translate and the other was generated by extracting example sentences from a syntax textbook. The MT-generated English data is most similar to our problem, and the most effective models for that data were the word-based scores from the language model. 6 Figure 4: Percent fluently inadequate at each checkpoint during in-domain training 5 Related Work MT quality metrics are judged based on their correlation with human judgments, and recently that has meant human adequacy judgments (Bojar et al., 2017). This indicates that any of the common MT metrics such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) may also serve as baseline adequacy scores. However, they incorporate elements of fluency while we wish to separate fluency and adequacy. Adequacy is, essentially, semantic equivalence and the goal of SemEval’s Semantic Textual Similarity (STS) task is to measure the degree of semantic equivalence between two sentences (Cer et al., 2017). The cross-lingual version of the task is similar enough to quality estimation that one of the data sets for 2017 actually came from the WMT Proceedings of MT Summit XVII, volume 1 Conclusion We have introduced an approach"
W19-6623,P16-1162,0,0.0748429,"of neural and phrase-based statistical MT models built from the same general domain data and adapted to translate a more specific domain, namely, transcripts of TED talks. We selected six languages to cover a range of resource availability scenarios and language families: Arabic, Chinese, Farsi, German, Korean and Russian. 4.1.1 Data The number of segments of training and test data for each language is summarized in Table 5. The same tokenization was performed for all systems for a given language, and the tokenized data was split into subwords for NMT training using byte pair encoding (BPE) (Sennrich et al., 2016). The BPE models were trained separately on the source and target language with 30K BPE symbols. All languages used data from the OpenSubtitles1 corpus (Tiedemann, 2009) in the General domain training and dev data sets. The Chinese, German, and Russian models used additional parallel 1 http://www.opensubtitles.org/ Proceedings of MT Summit XVII, volume 1 corpora from WMT172 (Bojar et al., 2017). For the Arabic models, we added data from the Linguistic Data Consortium (LDC)3 and the UN v1 corpus4 (Ziemski et al., 2016). The domain for the In-Domain and DomainAdapted models was TED talks. Traini"
W19-6623,E17-1100,0,0.0184785,"eural models are consistently more prone to this type of error than traditional statistical models. However, improving the overall quality of the MT system such as through domain adaptation reduces these errors. 1 Introduction Recent work has shown that well-trained, indomain neural machine translation (NMT) systems can produce translations that, at the sentence level, are rated on par with human reference translations (Hassan Awadalla et al., 2018). Part of this success comes from the impressive improvements in fluency of NMT output compared to previous MT paradigms (Bentivogli et al., 2016; Toral and Sánchez-Cartagena, 2017; Koehn and Knowles, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 2017). However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input–particularly when not trained on sufficient in-domain data (Koehn and Knowles, 2017). Because of NMT’s uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate. The fluency of fluently inadequate translations may mislead users into trust"
W19-6623,L16-1561,0,0.0150244,"ta was split into subwords for NMT training using byte pair encoding (BPE) (Sennrich et al., 2016). The BPE models were trained separately on the source and target language with 30K BPE symbols. All languages used data from the OpenSubtitles1 corpus (Tiedemann, 2009) in the General domain training and dev data sets. The Chinese, German, and Russian models used additional parallel 1 http://www.opensubtitles.org/ Proceedings of MT Summit XVII, volume 1 corpora from WMT172 (Bojar et al., 2017). For the Arabic models, we added data from the Linguistic Data Consortium (LDC)3 and the UN v1 corpus4 (Ziemski et al., 2016). The domain for the In-Domain and DomainAdapted models was TED talks. Training, dev, and test sets for the domain were from the Multi-target TED Talks Task (MTTT) corpus (Duh, 2018). All systems, regardless of training setting, were tested on the TED domain test set. Fluency scores for each system were generated based on a language model built on the English side of its primary training data. As noted in Section 2.1, it is important that the language model match the training data, and we expect this to be particularly true when the test set is out-of-domain. We therefore use only the General"
wu-etal-2004-raising,W03-0433,1,\N,Missing
wu-etal-2004-raising,W02-2004,0,\N,Missing
wu-etal-2004-raising,W02-2010,0,\N,Missing
wu-etal-2004-raising,W02-2035,1,\N,Missing
wu-etal-2004-raising,N01-1006,1,\N,Missing
wu-etal-2004-raising,J95-4004,0,\N,Missing
wu-etal-2004-raising,P98-2188,0,\N,Missing
wu-etal-2004-raising,C98-2183,0,\N,Missing
