2020.alvr-1.3,P18-1177,0,0.0442813,"Missing"
2020.alvr-1.3,N18-2072,0,0.0167611,"d Welling, 2013). Conversely, VQG in the medical domain is still a challenging and under-explored task (Hasan et al., 2018; Ben Abacha et al., 2018, 2019). Although a high-quality manually created medical VQA dataset exists, VQA-RAD (Lau et al., 2018), this dataset is too small for training and there is a need for VQG approaches to create training datasets of sufficient size. Generating new training data from the existing examples through data augmentation is an effective approach that has been widely used to handle the data insufficiency problem in the open domain (S¸ahin and Steedman, 2018; Kobayashi, 2018). Due to the problem of data scarcity in medical VQG, we automatically generate new training data. In this paper, we present VQGR, a VQG system capable of generating questions about radiology images. The system is based on the VAE architecture and data augmentation. 3 Data Augmentation 3.2 Visual Question Generation The proposed VQGR system is based on the variational autoencoders architecture (Kingma and Welling, 2013). It first encodes the image before generating the question. VAEs consist of two neural network modules, encoder, and decoder, for learning the probability distributions of data"
2020.alvr-1.3,W06-3114,0,0.0125028,"we discussed above, one major challenge in medical VQG is the lack of large training datasets. To avoid overfitting the model, small data might require models that have low complexity. Whereas the proposed VAE requires a large amount of training data as it tries to learn deeply the underlying data distribution of the input to output new sequences. To investigate the performance of the proposed VQGR system, we perform both automatic and manual evaluations. 4.2.1 Implementation Details Human evaluation For human evaluation, we follow the standard approach in evaluating text generation systems (Koehn and Monz, 2006), as used for question generation by (Du and Cardie, 2018; Hosking and Riedel, 2019). We manually checked the generated questions and rated them in terms of relevancy, grammaticality, and fluency. The relevancy of a question is determined by the relationship between the question, image and category. Grammaticality refers to the conformity of a question to the grammar rules. Fluency refers to the way individual words sound together within a question. The rating process has been done by two experts at the U.S. National Institutes of Health. For each rating scheme, the human raters Model Baseline"
2020.alvr-1.3,N19-1237,0,0.0147732,"ining datasets. To avoid overfitting the model, small data might require models that have low complexity. Whereas the proposed VAE requires a large amount of training data as it tries to learn deeply the underlying data distribution of the input to output new sequences. To investigate the performance of the proposed VQGR system, we perform both automatic and manual evaluations. 4.2.1 Implementation Details Human evaluation For human evaluation, we follow the standard approach in evaluating text generation systems (Koehn and Monz, 2006), as used for question generation by (Du and Cardie, 2018; Hosking and Riedel, 2019). We manually checked the generated questions and rated them in terms of relevancy, grammaticality, and fluency. The relevancy of a question is determined by the relationship between the question, image and category. Grammaticality refers to the conformity of a question to the grammar rules. Fluency refers to the way individual words sound together within a question. The rating process has been done by two experts at the U.S. National Institutes of Health. For each rating scheme, the human raters Model Baseline VQGR B1 B2 B3 B4 M RL C 31.4 14.6 7.8 3.2 10.4 38.8 21.1 55.0 43.3 37.9 34.5 29.3 5"
2020.alvr-1.3,D19-1317,0,0.013866,"s normal?”, “Are the pancreas normal?, “Are the intestines normal?”, “Are the isografted normal?”, Are the livers normal?, “Are the lungs normal?”, “Are the organs normal?”, etc. Images. We also generated new training instances based on image augmentation techniques. To do so, we applied flipping, rotation, shifting, and blurring techniques to all VQA-RAD training images. Related Work Question generation, an increasingly important area, is the task of automatically creating natural language questions from a range of inputs, such as natural language text (Kalady et al., 2010; Kim et al., 2019; Li et al., 2019), structured data (Serban et al., 2016) and images (Mostafazadeh et al., 2016). In this work, we are interested in generating questions from medical images. VQG in the open-domain benefited from the available large annotated datasets (Agrawal et al., 2015; Goyal et al., 2019; Johnson et al., 2017). There is a variety of work studying generative models for generating visual questions in the open domain (Masuda-Mora et al., 2016; Zhang et al., 2016). Recent VQG approaches have used autoencoders architecture for the purposes of VQG (Jain et al., 2017; Li et al., 2018; Krishna et al., 2019). The s"
2020.alvr-1.3,P16-1170,0,0.0285685,"s not been explored in the medical domain. This work is the first attempt to generate questions about radiology images. 2. In the medical domain, the lack of large sets of labeled data makes training supervised learning approaches inefficient. To overcome the data limitation of medical VQG, we present data augmentation on both the images and the questions. Introduction VQG refers to generating natural language questions based on the images contents. It is a new and exciting problem that combines both natural language processing (Sarrouti and Alaoui, 2017, 2020) and computer vision techniques (Mostafazadeh et al., 2016; Zhang et al., 2016). The motivation for the VQG task is two-fold: (1) generating large scale Visual Question Answering (VQA) pairs to produce more training data at little cost (Ben Abacha et al., 2019) and (2) improving efficiency of human annotation for VQA datasets construction (Li et al., 2018). In addition to the aforementioned motivations, medical VQG could 3. VQGR is based on the variational auto-encoders architecture and designed so that it can take a radiology image as input and generate a natural question as output. 4. Experimental evaluations performed on the VQA-RAD dataset of cli"
2020.alvr-1.3,D18-1545,0,0.0516422,"Missing"
2020.alvr-1.3,P16-1056,0,0.05852,"Missing"
2020.coling-main.494,N19-1423,0,0.0201743,"07 (b) TAC 2009 (c) MEDIQA Figure 1: Example topic- and question-driven multi-document abstractive summaries (documents omitted). 2 Background Recent work has indicated that transfer learning (pre-training a model on data-rich tasks before fine-tuning it on a downstream task) obtains remarkable performance on many natural language processing tasks (Yang et al., 2019; Dong et al., 2019; Liu et al., 2019b). The most successful models are obtained through self-supervised pre-training with massive datasets to obtain transferable knowledge for new tasks (i.e., fine-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2019; Keskar et al., 2019; Raffel et al., 2019). More recently, research has indicated that these models can generate language conditioned on a user-given prompt or context. For example, this prompt can guide the model’s content selection towards a particular topic (Keskar et al., 2019) or inform surface realization for a specific task (Lewis et al., 2019; Raffel et al., 2019). In Liu et al. (2020), the authors condition an extractive transformer using “control codes” to specify the position, importance, and diversity of the sentences in the source text. In this work, we adapt"
2020.coling-main.494,P19-1305,0,0.0222339,"or inform surface realization for a specific task (Lewis et al., 2019; Raffel et al., 2019). In Liu et al. (2020), the authors condition an extractive transformer using “control codes” to specify the position, importance, and diversity of the sentences in the source text. In this work, we adapt this paradigm to train and evaluate BART, T5, and PEGASUS for abstractive multi-document summarization. Although zero-shot learning (ZSL) has received considerable attention in the image processing community, there has been comparatively little work on zero-shot learning specifically for summarization: Duan et al. (2019) explore zero-shot learning for cross-lingual sentence summarization and Liu et al. (2019a) explored zero-shot abstractive summaries of five-sentence stories. We extend these works by evaluating zero-shot and few-shot learning for multi-document abstractive summarization. 3 Models In this work, we compare three of the most prominent conditional language generation models: T5, BART, and PEGASUS. To facilitate comparison, for each model we chose the variant with the most similar architecture (such that each consists of 12 transformer layers and a similar number of learnable parameters). Each mod"
2020.coling-main.494,W04-1013,0,0.224693,"ing with Extracted Gap-sentences for Abstractive SUmmarization Sequence-tosequence) was specifically designed for abstractive summarization and is pre-trained with a self-supervised gap-sentence-generation objective (Zhang et al., 2019). In this task, entire sentences are masked from the source document, concatenated, and used as the target “summary”. We used PEGASUS-Base in our experiments. 5641 4 Experiments We evaluated T5, BART, and PEGASUS in zero-shot (ZSL) and few-shot (FSL) learning settings on four datasets. Summary quality was measured using ROUGE-1, ROUGE-2, and ROUGE-L ?1 -scores (Lin, 2004); BLEU-4 (Papineni et al., 2002); and Repetition Rate (in unigrams). Implementation details are provided in Appendix A. 4.1 Answer Summarization at DUC 2007 The 2007 challenge of the Document Understanding Conference (DUC) focused on answering 45 natural language questions by summarizing sets of 10 documents from the AQUAINT English news corpus (Graff, 2002). Reference summaries were between 230 and 250 words. We used 30 topics for testing (with 10 for training and 5 for validation under FSL). Table 1 presents these results, showing that BART obtains the highest quality summaries in both setti"
2020.coling-main.494,P19-1441,0,0.157436,"ry: Nephrotic syndrome is caused by different disorders that damage the kidneys. In adults, most commonly by glomerulonephritis. This damage leads to the release of too much protein in the urine [...] (a) DUC 2007 (b) TAC 2009 (c) MEDIQA Figure 1: Example topic- and question-driven multi-document abstractive summaries (documents omitted). 2 Background Recent work has indicated that transfer learning (pre-training a model on data-rich tasks before fine-tuning it on a downstream task) obtains remarkable performance on many natural language processing tasks (Yang et al., 2019; Dong et al., 2019; Liu et al., 2019b). The most successful models are obtained through self-supervised pre-training with massive datasets to obtain transferable knowledge for new tasks (i.e., fine-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2019; Keskar et al., 2019; Raffel et al., 2019). More recently, research has indicated that these models can generate language conditioned on a user-given prompt or context. For example, this prompt can guide the model’s content selection towards a particular topic (Keskar et al., 2019) or inform surface realization for a specific task (Lewis et al., 2019; Raffel et a"
2020.coling-main.494,2020.findings-emnlp.131,0,0.01712,"uccessful models are obtained through self-supervised pre-training with massive datasets to obtain transferable knowledge for new tasks (i.e., fine-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2019; Keskar et al., 2019; Raffel et al., 2019). More recently, research has indicated that these models can generate language conditioned on a user-given prompt or context. For example, this prompt can guide the model’s content selection towards a particular topic (Keskar et al., 2019) or inform surface realization for a specific task (Lewis et al., 2019; Raffel et al., 2019). In Liu et al. (2020), the authors condition an extractive transformer using “control codes” to specify the position, importance, and diversity of the sentences in the source text. In this work, we adapt this paradigm to train and evaluate BART, T5, and PEGASUS for abstractive multi-document summarization. Although zero-shot learning (ZSL) has received considerable attention in the image processing community, there has been comparatively little work on zero-shot learning specifically for summarization: Duan et al. (2019) explore zero-shot learning for cross-lingual sentence summarization and Liu et al. (2019a) exp"
2020.coling-main.494,P02-1040,0,0.108819,"ap-sentences for Abstractive SUmmarization Sequence-tosequence) was specifically designed for abstractive summarization and is pre-trained with a self-supervised gap-sentence-generation objective (Zhang et al., 2019). In this task, entire sentences are masked from the source document, concatenated, and used as the target “summary”. We used PEGASUS-Base in our experiments. 5641 4 Experiments We evaluated T5, BART, and PEGASUS in zero-shot (ZSL) and few-shot (FSL) learning settings on four datasets. Summary quality was measured using ROUGE-1, ROUGE-2, and ROUGE-L ?1 -scores (Lin, 2004); BLEU-4 (Papineni et al., 2002); and Repetition Rate (in unigrams). Implementation details are provided in Appendix A. 4.1 Answer Summarization at DUC 2007 The 2007 challenge of the Document Understanding Conference (DUC) focused on answering 45 natural language questions by summarizing sets of 10 documents from the AQUAINT English news corpus (Graff, 2002). Reference summaries were between 230 and 250 words. We used 30 topics for testing (with 10 for training and 5 for validation under FSL). Table 1 presents these results, showing that BART obtains the highest quality summaries in both settings, though FSL provides a signi"
2020.coling-main.498,D19-1371,0,0.0295475,"Missing"
2020.coling-main.498,D18-2029,0,0.0128114,"9; Owczarzakg, 2010; Owczarzak and Dang, 2011), fewer studies have examined their relevance for individual summary ranking and their adequacy with linguistic quality and fluency. With the new levels of performance achieved by neural language models in a variety of natural language processing tasks, several insights point towards the high potential of their contextual language encoding for language representation. Most of the state-of-the-art models such as T5 (Raffel et al., 2019), BERT (Devlin et al., 2019), GPT (Radford et al., 2018; Radford et al., 2019) and the Universal Sentence Encoder (Cer et al., 2018), are built from very large corpora, which reduces substantially the potential bias from using them to evaluate summaries in a restricted document set or benchmark. On the other hand, the shallow lexical features of the original texts play a key role in extractive summarization and in pointer-generator approaches. It can also be argued that lexical similarities with gold summaries are implicitly capturing an important portion of the relevant semantics, especially at high similarity values. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: ht"
2020.coling-main.498,N19-1423,0,0.0223616,"based on their average score (Dang and Vanderwende, 2007; Dang and Owczarzak, 2008; Dang and Owczarzak, 2009; Owczarzakg, 2010; Owczarzak and Dang, 2011), fewer studies have examined their relevance for individual summary ranking and their adequacy with linguistic quality and fluency. With the new levels of performance achieved by neural language models in a variety of natural language processing tasks, several insights point towards the high potential of their contextual language encoding for language representation. Most of the state-of-the-art models such as T5 (Raffel et al., 2019), BERT (Devlin et al., 2019), GPT (Radford et al., 2018; Radford et al., 2019) and the Universal Sentence Encoder (Cer et al., 2018), are built from very large corpora, which reduces substantially the potential bias from using them to evaluate summaries in a restricted document set or benchmark. On the other hand, the shallow lexical features of the original texts play a key role in extractive summarization and in pointer-generator approaches. It can also be argued that lexical similarities with gold summaries are implicitly capturing an important portion of the relevant semantics, especially at high similarity values. T"
2020.coling-main.498,N03-1020,0,0.84589,"as, or social media posts, the demand for a faster consumption of the most relevant information is expected to grow hand in hand with the amount of information available online. A current bottleneck to a wider adoption of automatic summarizers is the lack of efficient solutions addressing both the relevance of the generated summaries and their linguistic quality. One component of this current limitation is the lack of efficient evaluation measures that address both aspects. The two most cited and widely adopted measures in various summarization datasets and summarization challenges are ROUGE (Lin and Hovy, 2003; Lin, 2004) and BLEU (Papineni et al., 2002). While both measures have been shown to be highly efficient baselines in ranking summarization systems based on their average score (Dang and Vanderwende, 2007; Dang and Owczarzak, 2008; Dang and Owczarzak, 2009; Owczarzakg, 2010; Owczarzak and Dang, 2011), fewer studies have examined their relevance for individual summary ranking and their adequacy with linguistic quality and fluency. With the new levels of performance achieved by neural language models in a variety of natural language processing tasks, several insights point towards the high pote"
2020.coling-main.498,W04-1013,0,0.575373,"posts, the demand for a faster consumption of the most relevant information is expected to grow hand in hand with the amount of information available online. A current bottleneck to a wider adoption of automatic summarizers is the lack of efficient solutions addressing both the relevance of the generated summaries and their linguistic quality. One component of this current limitation is the lack of efficient evaluation measures that address both aspects. The two most cited and widely adopted measures in various summarization datasets and summarization challenges are ROUGE (Lin and Hovy, 2003; Lin, 2004) and BLEU (Papineni et al., 2002). While both measures have been shown to be highly efficient baselines in ranking summarization systems based on their average score (Dang and Vanderwende, 2007; Dang and Owczarzak, 2008; Dang and Owczarzak, 2009; Owczarzakg, 2010; Owczarzak and Dang, 2011), fewer studies have examined their relevance for individual summary ranking and their adequacy with linguistic quality and fluency. With the new levels of performance achieved by neural language models in a variety of natural language processing tasks, several insights point towards the high potential of the"
2020.coling-main.498,P17-1071,1,0.834678,"the gold summary, and A = {a1 , . . . , am } for a system summary. An embedding vector VL (x) is then generated for each n-gram x from a given language model L. A second step is to compute the best distance value for a given gold n-gram gi , DistA (gi ), with a filtering method where each system n-gram a ∈ A can only be used once as the best match for any given gold n-gram g ∈ G. DistA (gi ) = M in{aj ∈Ai } euc(VL (gi ), VL (aj )) (1) With euc being the euclidean distance, and Ak the dynamic set computed by removing the previously matched n-grams in A. The last step is inspired from TextFlow (Mrabet et al., 2017), and consists in using the (consecutive) distance values as coordinates on a curve and computing the area under curve as the overall distance. This is best shown with an example as in figure 1. The two example sentences, S, and G are from a news article1 , and were slightly modified for ease of presentation: • G: “The man who ate the $120K banana art on the wall said that he was not sorry and that he was performing art by eating it.” • S: “An artist claimed he was performing art by eating a banana used as a center piece in the art work of a colleague.” Drawing the curve connecting the distanc"
2020.coling-main.498,D15-1222,0,0.0724193,"ries, their intrinsic linguistic quality and coherence, and their relevance w.r.t. the original document (Cabrera-Diego and Torres-Moreno, 2018; Lloret et al., 2018). In this paper, we focus on extrinsic evaluations when human generated summaries are available as they offer both more specific parameters for the task, and available benchmarks with human-generated scores for automatically generated summaries. ROUGE BLEU ROUGE-WE ROUGE-2.0 AutoSummENG HOLMS Lexical ! ! ! ! ! ! Semantic — — ! ! — ! Gold sum. ! ! ! ! ! ! Full text — — — — — — Reference (Lin and Hovy, 2003) (Papineni et al., 2002) (Ng and Abrecht, 2015) (Ganesan, 2018) (Giannakopoulos and Karkaletsis, 2011) — Citations 1,531 10,628 35 15 26 — Table 1: Related summarization evaluation metrics Table 1, we present several summarization evaluation measures and their main characteristics. In terms of usage, ROUGE and BLEU stand out as the most cited measures, albeit a big portion of BLEU citations are likely from language translation papers, and not from research works on summarization. ROUGE stands for Recall-Oriented Understudy of Gisting Evaluation (Lin, 2004). It allows evaluating system-generated summaries by comparing them with (ideal) summ"
2020.coling-main.498,P02-1040,0,0.110073,"a faster consumption of the most relevant information is expected to grow hand in hand with the amount of information available online. A current bottleneck to a wider adoption of automatic summarizers is the lack of efficient solutions addressing both the relevance of the generated summaries and their linguistic quality. One component of this current limitation is the lack of efficient evaluation measures that address both aspects. The two most cited and widely adopted measures in various summarization datasets and summarization challenges are ROUGE (Lin and Hovy, 2003; Lin, 2004) and BLEU (Papineni et al., 2002). While both measures have been shown to be highly efficient baselines in ranking summarization systems based on their average score (Dang and Vanderwende, 2007; Dang and Owczarzak, 2008; Dang and Owczarzak, 2009; Owczarzakg, 2010; Owczarzak and Dang, 2011), fewer studies have examined their relevance for individual summary ranking and their adequacy with linguistic quality and fluency. With the new levels of performance achieved by neural language models in a variety of natural language processing tasks, several insights point towards the high potential of their contextual language encoding f"
2020.coling-main.498,D14-1162,0,0.100223,"Missing"
2020.deelio-1.7,L16-1732,1,0.832415,"wing layers to accumulate information about how the semantics of each word in an entity contribute to the overall semantics of the entity: ?? = ? (? ? )  Energy Regularization We project the composed entities into the same vector space as the pretrained entity embeddings from the Ontology, and measure the average energy across all entities detected in the sentence: (1b) ? ? = ? (?? [??−1 , ? ? ] + ? ? ) e ? + ?? ◦ ? ?−1 ?? = ?? ◦ ? 4.3 (1a) e ? = ?? ? ? ? ?? = ? ? ? [??−1 , ? ? ] + ? ? Linear Interpolation Finally, we considered a third, even simpler form of semantic composition. Inspired by Goodwin and Harabagiu (2016), we represented the semantics of an entity as an unordered linear combination of the semantics of its constituent words, i.e.: ? ? B ?? ? ?? + ? ?? +1 + · · · + ? ?? +?? + ?? · ? ? . (1c) (1d) (1e) R OSCR = e ? repwhere [•] represents vector concatenation, ? resents the content layer which encodes any new semantic information provided by word ? ? , ◦ indicates an element-wise product, ? ? represents the ?  1 Õ ? ? ? ?? + ? ? , ?? ? ?=1 (2) where ? is an energy function capturing the energy between the composed entity ? ? and the pretrained entity embedding ? ? . We considered three energy 5"
2020.deelio-1.7,W02-0109,0,0.0613717,"ed on exact string matching to detect entities. Formally, let ? = ? 1 , ? 2 , · · · , ? ? represent the sequence of words in a sentence. The FST processes ? and returns three sequences: ?1 , ?2 , · · · , ? ? ; ?1 , ?2 , · · · , ? ? ; and ? 1 , ? 2 , · · · , ? ? representing the start offset, length, and the pretrained embedded representation of every mention of any entity in the Ontology. The Documents Our text corpus was a 2019 dump of English Wikipedia articles with templates expanded as provided by Wikipedia’s Cirrus search engine3. Preprocessing relied on NLTK’s Punkt sentence segmenter4 (Loper and Bird, 2002), and the WordPiece subword tokenizer provided with BERT. 4 Entity Detection The Approach Virtually all neural networks designed for natural language processing represent language as a sequence of words, subwords, or characters. By contrast, Ontologies and knowledge bases encode semantic information about entities, which may correspond to individual nouns (e.g., “badge”) or multiword phrases (“police officer”). Consequently, injecting world and domain knowledge from a knowledge base into the network requires semantically decomposing the information about an entity into the supporting informati"
2020.deelio-1.7,N16-1098,0,0.029119,"ersion of BERT, allowing for mixed-precision training. This necessitated a number of minor changes to improve numerical stability around softmax operations. Training was performed using a single node with 4 Tesla P100s each (multiple variants of OSCAR were trained simultaneously using five such nodes at a time). Non-TPU multi-GPU support was added to BERT based on Horovod7 and relying on Open MPI. 5.2 The Story Cloze Test evaluates story understanding, story generation, and script learning and requires a system to choose the correct ending to a four-sentence story, as illustrated in Figure 4 (Mostafazadeh et al., 2016). In our experiments, we used only the 3,744 labeled stories. Recognizing Question Entailment Healthcare questions can be highly complex compared to general open-domain questions, potentially involving accounting for family, social, and medical history. A proposed solution to healthcare question complexity is to decompose the question into simpler subquestions, which can be more easily answered. Recognizing Question Entailment (RQE, Ben Abacha and Demner-Fushman 2016) consists of 8588 training and 302 testing pairs of consumer health questions (CHQs) and frequently asked questions (FAQs) with"
2020.deelio-1.7,D14-1162,0,0.0838019,"., 2007), as well as dictionary knowledge from Wiktionary, the Open Multilingual WordNet (Singh et al., 2002; Miller, 1995), the high-level ontology from OpenCyc1, and knowledge about word associations from “Games with a Purpose” (von Ahn, 2006). In our experiments we used ConceptNet 5 as our ontology relying on an embedded representation of the ontology known as ConceptNet NumberBatch (Speer et al., 2017), 1http://www.cyc.com/opencyc/ 57 in which embeddings for all entities in ConceptNet were built using an ensemble of (a) data from ConceptNet, (b) word2vec (Mikolov et al., 2013), (c) GloVe (Pennington et al., 2014), and (d) OpenSubtitles 20162 using retrofitting. 3.2 injected into the network based on the neural activations associated with its constituent words. 4.1 We designed OSCR to require as few modifications to the underlying host network (e.g., BERT) as possible. We recognized entities during training and inference online by (1) tokenizing each entity in our ontology using the same tokenizer used to prepare the BERT pre-training data, and (2) compiling a Finite State Transducer to detect sequences of subword IDs corresponding to entities. The FST, illustrated in Figure 3, allowed us to detect ent"
2020.deelio-1.7,P17-1161,0,0.0239998,"ire entities and phrases extracted from external knowledge. Meanwhile, Xie et al. (2019) explored projecting propositional knowledge using Graph Convolutional Networks (GCNs). OSCR, instead, introduces a regularization term that can be added to any natural language pre-training objectives, without modifying the architecture of the network or the pre-training objectives themselves. Background and Related Work 3 The idea of training a model on a related problem before training on the problem of interest has been shown to be effective for many natural language processing tasks (Dai and Le, 2015; Peters et al., 2017; Howard and Ruder, 2018). More recent uses of pre-training adapt transfer learning by first training a network on a language modeling task and then fine-tuning (retraining) that model for a supervised problem of interest (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). Pre-training, in this way, has the advantage that the model can build on previous parameters to reduce the amount of information it needs to learn for a specific downstream task. Conceptually, the model can be viewed as applying what it has already learned from the language model task when learning the downstre"
2020.deelio-1.7,D16-1264,0,0.0249008,"Grice’s Maxim of Quantity (Grice, 1975), this shared knowledge of the world is rarely explicitly stated in text. Fortunately, some of this knowledge can be extracted from Ontologies and knowledge bases. For example ConceptNet (Speer et al., 2017) indicates that a detective is a TypeOf police officer, and is CapableOf finding evidence; that evidence can be LocatedAt a crime scene; and that a badge is a TypeOf authority symbol. While neural networks have been shown to obtain state-of-the-art performance on many types of question answering and reasoning tasks from raw data (Devlin et al., 2018; Rajpurkar et al., 2016; Manning, 2015), there has been less investigation into how to inject ontological knowledge into deep learning models, with most prior attempts embedding ontological information outside of the network itself (Wang et al., 2017). In this paper, we present a pre-training regularThe Problem “The detective flashed his badge to the police officer.” The nearly effortless ease at which we, as humans, can understand this simple statement belies the depth of semantic knowledge needed for its understanding: What is a detective? What is a police officer? What is a badge? What does it mean to flash a bad"
2020.deelio-1.7,D18-1223,0,0.0610116,"Missing"
2020.deelio-1.7,P19-1139,0,0.0277339,"Missing"
2020.findings-emnlp.289,W05-0909,0,0.0548923,"hes-per-epoch, using single V100X GPUs (32 GB VRAM) on a shared cluster. Training took between four-to-six hours, depending on cluster load. Additional implementation details are provided in Appendix A. To reduce variance between runs, we report results with greedy decoding (i.e., no beam search). We measured the impact of (1) multi-task ﬁnetuning (MTFT), (2) diﬀerent task mixing strategies, and (3) excluding various tasks from ﬁne-tuning on zero-shot summary quality. We report traditional summarization and generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). Because the reference summaries for many tasks are highly abstractive, we adopt the embeddingbased metrics proposed in Sharma et al. (2017), i.e., GloVe (Pennington et al., 2014) cosine similarity using Embedding Averages (EACS), Vector Extrema (VECS, Forgues et al., 2014), and greedy matching (GMS, Rus and Lintean, 2012). 5.1 Evaluation Tasks MEDIQA-AnS The MEDIQA-AnS collection contains consumer health questions, articles from reliable websites, passages extracted from those web pages, and single- and multi-document summaries of the passages intended to provide consumer-friendly answers fo"
2020.findings-emnlp.289,N18-2097,0,0.0206977,"Start Here and Learn More sections of MedlinePlus. 3218 QA4MRE was created for the CLEF shared tasks to promote research in question answering and reading comprehension; we used the English questions provided for training in 2011, 2012, and 2013 including 120, 160, and 284 examples, respectively, as well as the Alzheimer’s questions provided in 2012 and 2013 which each provide 40 examples (Peñas et al., 2013). Scientiﬁc Papers contains two sets of long documents and their abstracts, including 203.0 K articles from arXiv.org and 119.9 K articles from the Open Access Subset of PubMed Central® (Cohan et al., 2018). SQuAD the Stanford Question Answering Dataset is a reading comprehension dataset consisting of 87.6 K questions over Wikipedia articles where the question is considered unanswerable if the answer cannot be extracted from the corresponding passage (Rajpurkar et al., 2016). 4.2 Conditional Generation As in Raﬀel et al. (2019), we used a Text-to-Text setting to train BART and T5 such that the model inputs and targets are both encoded as sequences of tokens. For summarization tasks, the input was provided as <task-name> [question: <question>] summarize: <document> and the target was the target s"
2020.findings-emnlp.289,N19-1423,0,0.0153698,"zak and Dang, 2010) Introduction Figure 1: Example conditional summaries for two tasks. Transfer learning, in which a model is ﬁrst pretrained on one or more data-rich tasks before being ﬁne-tuned on a downstream task of interest, has been repeatedly shown to obtain remarkable performance on many natural language processing tasks (Yang et al., 2019; Dong et al., 2019; Liu et al., 2019b). The most successful models resulting from this paradigm rely on self-supervised pretraining with prohibitively-large1 datasets to facilitate adaptation to new tasks (i.e., ﬁne-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2020; Keskar et al., 2019; Raﬀel et al., 2019). Unfortunately, the beneﬁts of pretraining are reduced for tasks in which there is little direct knowledge 1As estimated by Strubell et al. (2019), the cost for training the 11 billion parameter variant of T5 (Raﬀel et al., 2019) can exceed $1.3 million USD for a single run. transfer, such as language generation for tasks and domains involving previously unseen lexical and semantic properties (as we demonstrate in this paper). Transfer learning generalization failures are particularly problematic for a family of tasks we refer to a"
2020.findings-emnlp.289,P19-1305,0,0.0228508,"quences. In this work, we explored the two most notable of these approaches: BART and T5.                           X1 X2 X3 Inputs Figure 2: Simpliﬁed encoder-decoder transformer architectures used by BART and T5. 2019). Similar work on conditional generation includes Liu et al. (2020), in which the authors condition an extractive transformer on control codes specifying position, importance, and diversity of the sentences in the source text. There have been relatively few publications focused on zero-shot learning speciﬁcally for summarization. Duan et al. (2019) experiment with zero-shot learning for cross-lingual sentence summarization, while Liu et al. (2019a) explored zeroshot abstractive summaries of ﬁve-sentence stories. Prior work indicates that topic and questiondriven summarization can be formulated as a textto-text, conditional generation problem in which content selection and source realization are explicitly conditioned on a user-speciﬁed prompt. The formulation of summarization in this way intuitively dovetails with the desired goal described above: question-driven summarization of answers to user’s health-related questions. In this study"
2020.findings-emnlp.289,D19-1243,0,0.0475068,"Missing"
2020.findings-emnlp.289,D18-1208,0,0.0514004,"Missing"
2020.findings-emnlp.289,2020.acl-main.703,0,0.442681,"ntroduction Figure 1: Example conditional summaries for two tasks. Transfer learning, in which a model is ﬁrst pretrained on one or more data-rich tasks before being ﬁne-tuned on a downstream task of interest, has been repeatedly shown to obtain remarkable performance on many natural language processing tasks (Yang et al., 2019; Dong et al., 2019; Liu et al., 2019b). The most successful models resulting from this paradigm rely on self-supervised pretraining with prohibitively-large1 datasets to facilitate adaptation to new tasks (i.e., ﬁne-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2020; Keskar et al., 2019; Raﬀel et al., 2019). Unfortunately, the beneﬁts of pretraining are reduced for tasks in which there is little direct knowledge 1As estimated by Strubell et al. (2019), the cost for training the 11 billion parameter variant of T5 (Raﬀel et al., 2019) can exceed $1.3 million USD for a single run. transfer, such as language generation for tasks and domains involving previously unseen lexical and semantic properties (as we demonstrate in this paper). Transfer learning generalization failures are particularly problematic for a family of tasks we refer to as conditional summar"
2020.findings-emnlp.289,W04-1013,0,0.0865191,"epochs, and 1,000 batches-per-epoch, using single V100X GPUs (32 GB VRAM) on a shared cluster. Training took between four-to-six hours, depending on cluster load. Additional implementation details are provided in Appendix A. To reduce variance between runs, we report results with greedy decoding (i.e., no beam search). We measured the impact of (1) multi-task ﬁnetuning (MTFT), (2) diﬀerent task mixing strategies, and (3) excluding various tasks from ﬁne-tuning on zero-shot summary quality. We report traditional summarization and generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). Because the reference summaries for many tasks are highly abstractive, we adopt the embeddingbased metrics proposed in Sharma et al. (2017), i.e., GloVe (Pennington et al., 2014) cosine similarity using Embedding Averages (EACS), Vector Extrema (VECS, Forgues et al., 2014), and greedy matching (GMS, Rus and Lintean, 2012). 5.1 Evaluation Tasks MEDIQA-AnS The MEDIQA-AnS collection contains consumer health questions, articles from reliable websites, passages extracted from those web pages, and single- and multi-document summaries of the passages intended t"
2020.findings-emnlp.289,P19-1441,0,0.0594367,"waves receded, carrying [...] Topic: Mangrove Forests Summary: The recent 26 December 2004 tsunami in the Indian Ocean with destruction of mangrove forests has highlighted their environmental importance [...] (b) TAC 2010 (Owczarzak and Dang, 2010) Introduction Figure 1: Example conditional summaries for two tasks. Transfer learning, in which a model is ﬁrst pretrained on one or more data-rich tasks before being ﬁne-tuned on a downstream task of interest, has been repeatedly shown to obtain remarkable performance on many natural language processing tasks (Yang et al., 2019; Dong et al., 2019; Liu et al., 2019b). The most successful models resulting from this paradigm rely on self-supervised pretraining with prohibitively-large1 datasets to facilitate adaptation to new tasks (i.e., ﬁne-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2020; Keskar et al., 2019; Raﬀel et al., 2019). Unfortunately, the beneﬁts of pretraining are reduced for tasks in which there is little direct knowledge 1As estimated by Strubell et al. (2019), the cost for training the 11 billion parameter variant of T5 (Raﬀel et al., 2019) can exceed $1.3 million USD for a single run. transfer, such as language ge"
2020.findings-emnlp.289,2020.findings-emnlp.131,0,0.167628,". This substantially limits the length of summaries that can be generated due to the input sequence limits imposed during pretraining. Fortunately, more recent approaches use separate transformers for encoding and decoding, allowing the generation of potentially arbitrary length sequences. In this work, we explored the two most notable of these approaches: BART and T5.                           X1 X2 X3 Inputs Figure 2: Simpliﬁed encoder-decoder transformer architectures used by BART and T5. 2019). Similar work on conditional generation includes Liu et al. (2020), in which the authors condition an extractive transformer on control codes specifying position, importance, and diversity of the sentences in the source text. There have been relatively few publications focused on zero-shot learning speciﬁcally for summarization. Duan et al. (2019) experiment with zero-shot learning for cross-lingual sentence summarization, while Liu et al. (2019a) explored zeroshot abstractive summaries of ﬁve-sentence stories. Prior work indicates that topic and questiondriven summarization can be formulated as a textto-text, conditional generation problem in which content"
2020.findings-emnlp.289,U11-1012,0,0.0736884,"Missing"
2020.findings-emnlp.289,P19-1459,0,0.0252059,"ere the conditional summarization context (if applicable) is provided as the question portion of the input. For question answering and reading comprehension tasks, the input was provided as <task-name> question: <question> [choice: <choice>...] context: <document> and the target was either (a) True or False for (binary choice questions), or (b) the text of the correct choice for ?-ary choice questions. 4.3 Task Mixing Neural models are notorious for overﬁtting data – particularly in the case of natural language text for which transformer-based models have been shown to memorize spurious cues (Niven and Kao, 2019). A major factor in overﬁtting is the size of data used for training, and, as documented in Section 4.1, the available training data for each of our ﬁne-tuning tasks vary by orders of magnitude. In order to avoid overﬁtting (and to avoid overcorrecting and underﬁtting) small datasets, for each ﬁne-tuning step, we sample a batch of data from a single task assuming a Multinomial distribution ? over ﬁnetuning tasks. We refer to this distribution over tasks as the mixing rate, such that ? ? indicates the probability that a batch will be drawn from ﬁnetuning task ? ∈ {1, · · · , ? }. We explored fo"
2020.findings-emnlp.289,P02-1040,0,0.107091,"epochs followed by 10 training epochs, and 1,000 batches-per-epoch, using single V100X GPUs (32 GB VRAM) on a shared cluster. Training took between four-to-six hours, depending on cluster load. Additional implementation details are provided in Appendix A. To reduce variance between runs, we report results with greedy decoding (i.e., no beam search). We measured the impact of (1) multi-task ﬁnetuning (MTFT), (2) diﬀerent task mixing strategies, and (3) excluding various tasks from ﬁne-tuning on zero-shot summary quality. We report traditional summarization and generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). Because the reference summaries for many tasks are highly abstractive, we adopt the embeddingbased metrics proposed in Sharma et al. (2017), i.e., GloVe (Pennington et al., 2014) cosine similarity using Embedding Averages (EACS), Vector Extrema (VECS, Forgues et al., 2014), and greedy matching (GMS, Rus and Lintean, 2012). 5.1 Evaluation Tasks MEDIQA-AnS The MEDIQA-AnS collection contains consumer health questions, articles from reliable websites, passages extracted from those web pages, and single- and multi-document summaries of the"
2020.findings-emnlp.289,D14-1162,0,0.0835563,"ovided in Appendix A. To reduce variance between runs, we report results with greedy decoding (i.e., no beam search). We measured the impact of (1) multi-task ﬁnetuning (MTFT), (2) diﬀerent task mixing strategies, and (3) excluding various tasks from ﬁne-tuning on zero-shot summary quality. We report traditional summarization and generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). Because the reference summaries for many tasks are highly abstractive, we adopt the embeddingbased metrics proposed in Sharma et al. (2017), i.e., GloVe (Pennington et al., 2014) cosine similarity using Embedding Averages (EACS), Vector Extrema (VECS, Forgues et al., 2014), and greedy matching (GMS, Rus and Lintean, 2012). 5.1 Evaluation Tasks MEDIQA-AnS The MEDIQA-AnS collection contains consumer health questions, articles from reliable websites, passages extracted from those web pages, and single- and multi-document summaries of the passages intended to provide consumer-friendly answers for the questions (Savery et al., 2020). We used the 552 extractive singledocument question-driven summaries. DUC The Document Understanding Conference (DUC) was hosted by NIST from"
2020.findings-emnlp.289,N18-1202,0,0.0105268,"lts in many text generation tasks. Building on this, researchers have recently shown that these models can be conditioned on a prompt included in the input text. For example, this prompt can guide the content of the generated text towards either a desired topic (Keskar et al., 2019) or instruct the model to produce output for a speciﬁc task (Lewis et al., 2020; Raﬀel et al., 3216 2http://bioasq.org/ 3https://github.com/WING-NUS/scisumm-corpus 4https://ornlcda.github.io/SDProc/ Outputs Y1 Y2 3 Models Y3 Several transformer-based models have been shown to generate high quality natural language (Peters et al., 2018; Radford et al., 2018; Wang and Cho, 2019). The majority of these models cast summarization as language modeling wherein the input to the model is the sequence of words in the source document followed by a mask token for each word in the desired summary (Keskar et al., 2019; Radford et al., 2019). This substantially limits the length of summaries that can be generated due to the input sequence limits imposed during pretraining. Fortunately, more recent approaches use separate transformers for encoding and decoding, allowing the generation of potentially arbitrary length sequences. In this wor"
2020.findings-emnlp.289,W12-2018,0,0.030078,"multi-task ﬁnetuning (MTFT), (2) diﬀerent task mixing strategies, and (3) excluding various tasks from ﬁne-tuning on zero-shot summary quality. We report traditional summarization and generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). Because the reference summaries for many tasks are highly abstractive, we adopt the embeddingbased metrics proposed in Sharma et al. (2017), i.e., GloVe (Pennington et al., 2014) cosine similarity using Embedding Averages (EACS), Vector Extrema (VECS, Forgues et al., 2014), and greedy matching (GMS, Rus and Lintean, 2012). 5.1 Evaluation Tasks MEDIQA-AnS The MEDIQA-AnS collection contains consumer health questions, articles from reliable websites, passages extracted from those web pages, and single- and multi-document summaries of the passages intended to provide consumer-friendly answers for the questions (Savery et al., 2020). We used the 552 extractive singledocument question-driven summaries. DUC The Document Understanding Conference (DUC) was hosted by NIST from 2001-2007, to promote summarization research. In 2004, there were 50 questions each associated with very short single-document summaries (limited"
2020.findings-emnlp.289,P17-1099,0,0.0573475,"s with questions, PubMed articles, snippets extracted from those articles, and human-generated answers to the questions. For single-document summarization, we used each extracted snippet as a summary of the corresponding article. For multidocument summarization, we used each humangenerated answer as a summary of the corresponding set of articles. The single-document summarization dataset contains 27.1 K examples, and the multi-document summarization dataset contains 3.2 K examples. CNN/DailyMail includes 287.1 K news articles, as well as highlights of the articles which are used as summaries (See et al., 2017; Hermann et al., 2015). CoPA The Choice of Plausible Alternatives dataset (Roemmele et al., 2011) presents 400 training sets of questions involving choosing the most plausible cause or eﬀect entailed by a given premise; questions were drawn from (1) personal blog stories (Gordon and Swanson, 2009), and (2) subject terms from the Library of Congress Thesaurus for Graphic Materials. Cochrane* contains 5.0 K reviews and plain language summaries from the Cochrane Database of Systematic Reviews; we use only the main body of the review as the source document for singledocument summarization. Cosmos"
2020.findings-emnlp.289,P18-2095,0,0.0134314,"ment summarization. Cosmos QA includes 287.1 K multiple-choice reading comprehension questions requiring commonsense causal reasoning; it focuses on cause and eﬀect in everyday narratives (Huang et al., 2019). CQaD-S* is based on a collection of consumer questions about drugs and answers to those questions manually extracted from reliable web pages (Ben Abacha et al., 2019); we adapted the 272 IBM Evidence 4.3 K examples of questions with pairs of evidence, annotated for which evidence in the pair is the most convincing evidence for answering the question; the training set includes 48 topics (Shnarch et al., 2018). MC-TACO is a set of 13 K question-answer pairs requiring temporal commonsense comprehension; questions pertain to various temporal aspects of events, such as duration, frequency, and temporal order (Zhou et al., 2019). MedlinePlus Summaries* contains summaries of health topics obtained from MedlinePlus, a service of the U.S. National Library of Medicine providing human-curated, reliable, and easy-tounderstand articles about over 1 K health topics. Each article contains a summary of the topic and links relevant web pages; we used the summary and the content of linked pages5 to generate a mult"
2020.findings-emnlp.289,P19-1355,0,0.0123591,"stream task of interest, has been repeatedly shown to obtain remarkable performance on many natural language processing tasks (Yang et al., 2019; Dong et al., 2019; Liu et al., 2019b). The most successful models resulting from this paradigm rely on self-supervised pretraining with prohibitively-large1 datasets to facilitate adaptation to new tasks (i.e., ﬁne-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2020; Keskar et al., 2019; Raﬀel et al., 2019). Unfortunately, the beneﬁts of pretraining are reduced for tasks in which there is little direct knowledge 1As estimated by Strubell et al. (2019), the cost for training the 11 billion parameter variant of T5 (Raﬀel et al., 2019) can exceed $1.3 million USD for a single run. transfer, such as language generation for tasks and domains involving previously unseen lexical and semantic properties (as we demonstrate in this paper). Transfer learning generalization failures are particularly problematic for a family of tasks we refer to as conditional summarization. Unlike traditional summarization, in which the goal is to produce an objective summary of the most salient information in a passage, in conditional summarization, the selection of"
2020.findings-emnlp.289,D16-1264,0,0.0330087,"d 284 examples, respectively, as well as the Alzheimer’s questions provided in 2012 and 2013 which each provide 40 examples (Peñas et al., 2013). Scientiﬁc Papers contains two sets of long documents and their abstracts, including 203.0 K articles from arXiv.org and 119.9 K articles from the Open Access Subset of PubMed Central® (Cohan et al., 2018). SQuAD the Stanford Question Answering Dataset is a reading comprehension dataset consisting of 87.6 K questions over Wikipedia articles where the question is considered unanswerable if the answer cannot be extracted from the corresponding passage (Rajpurkar et al., 2016). 4.2 Conditional Generation As in Raﬀel et al. (2019), we used a Text-to-Text setting to train BART and T5 such that the model inputs and targets are both encoded as sequences of tokens. For summarization tasks, the input was provided as <task-name> [question: <question>] summarize: <document> and the target was the target summary, where the conditional summarization context (if applicable) is provided as the question portion of the input. For question answering and reading comprehension tasks, the input was provided as <task-name> question: <question> [choice: <choice>...] context: <document"
2020.findings-emnlp.289,W19-2304,0,0.0193693,"on this, researchers have recently shown that these models can be conditioned on a prompt included in the input text. For example, this prompt can guide the content of the generated text towards either a desired topic (Keskar et al., 2019) or instruct the model to produce output for a speciﬁc task (Lewis et al., 2020; Raﬀel et al., 3216 2http://bioasq.org/ 3https://github.com/WING-NUS/scisumm-corpus 4https://ornlcda.github.io/SDProc/ Outputs Y1 Y2 3 Models Y3 Several transformer-based models have been shown to generate high quality natural language (Peters et al., 2018; Radford et al., 2018; Wang and Cho, 2019). The majority of these models cast summarization as language modeling wherein the input to the model is the sequence of words in the source document followed by a mask token for each word in the desired summary (Keskar et al., 2019; Radford et al., 2019). This substantially limits the length of summaries that can be generated due to the input sequence limits imposed during pretraining. Fortunately, more recent approaches use separate transformers for encoding and decoding, allowing the generation of potentially arbitrary length sequences. In this work, we explored the two most notable of thes"
2020.findings-emnlp.289,D19-1332,0,0.0141579,"ased on a collection of consumer questions about drugs and answers to those questions manually extracted from reliable web pages (Ben Abacha et al., 2019); we adapted the 272 IBM Evidence 4.3 K examples of questions with pairs of evidence, annotated for which evidence in the pair is the most convincing evidence for answering the question; the training set includes 48 topics (Shnarch et al., 2018). MC-TACO is a set of 13 K question-answer pairs requiring temporal commonsense comprehension; questions pertain to various temporal aspects of events, such as duration, frequency, and temporal order (Zhou et al., 2019). MedlinePlus Summaries* contains summaries of health topics obtained from MedlinePlus, a service of the U.S. National Library of Medicine providing human-curated, reliable, and easy-tounderstand articles about over 1 K health topics. Each article contains a summary of the topic and links relevant web pages; we used the summary and the content of linked pages5 to generate a multidocument summarization collection consisting of 969 examples. Movie Rationales contains 1.6 K human annotated rationales for movie reviews; used as multidocument summarization (Zaidan et al., 2008; DeYoung et al., 2020"
2021.acl-short.33,P19-1215,1,0.791804,"nces with the higher ROUGE score. While these methods are primarily supervised, Laban et al. (2020) proposed an unsupervised method that accounts for fluency, brevity, and coverage in generated summaries using multiple RL-based rewards. The majority of these works are focused on document summarization with conventional non-semantics rewards (ROUGE, BLEU). In contrast, we focus on formulating the semantic rewards that bring a high-level semantic regularization. In particular, we investigate the question’s main characteristics, i.e., question focus and type, to define the rewards. Recently, Ben Abacha and Demner-Fushman (2019) defined the CHQ summarization task and introduced a new benchmark (M E QS UM) and a pointer-generator model. Ben Abacha et al. (2021) organized the MEDIQA-21 shared task challenge on CHQ, multi-document answers, and radiology report summarization. Most of the participating team (Yadav et al., 2021b; He et al., 2021; S¨anger et al., 2021) utilized transfer learning, knowledgebased, and ensemble methods to solve the question summarization task. Yadav et al. (2021a) proposed question-aware transformer models for question summarization. Xu et al. (2020) automatically created a Chinese dataset (MA"
2021.acl-short.33,P18-1063,0,0.0215417,"f the task is to generate a summarized question that contains the salient information of the original question. We propose a RL-based question summarizer model over the Transformer (Vaswani et al., 2017) encoderdecoder architecture. We describe below the proposed reward functions. Related Work In recent years, reinforcement learning (RL) based models have been explored for the abstractive summarization task. Paulus et al. (2017) introduced RL in neural summarization models by optimizing the ROUGE score as a reward that led to more readable and concise summaries. Subsequently, several studies (Chen and Bansal, 2018; Pasunuru and Bansal, 2018; Zhang and Bansal, 2019; Gupta et al., 2020; Zhang et al., 2019b) have proposed methods to optimize the model losses via RL that enables the model to generate the sentences with the higher ROUGE score. While these methods are primarily supervised, Laban et al. (2020) proposed an unsupervised method that accounts for fluency, brevity, and coverage in generated summaries using multiple RL-based rewards. The majority of these works are focused on document summarization with conventional non-semantics rewards (ROUGE, BLEU). In contrast, we focus on formulating the seman"
2021.acl-short.33,2021.bionlp-1.12,0,0.0588326,"Missing"
2021.acl-short.33,W13-1907,1,0.858173,"Missing"
2021.acl-short.33,2020.acl-main.460,0,0.0249106,"recent years, reinforcement learning (RL) based models have been explored for the abstractive summarization task. Paulus et al. (2017) introduced RL in neural summarization models by optimizing the ROUGE score as a reward that led to more readable and concise summaries. Subsequently, several studies (Chen and Bansal, 2018; Pasunuru and Bansal, 2018; Zhang and Bansal, 2019; Gupta et al., 2020; Zhang et al., 2019b) have proposed methods to optimize the model losses via RL that enables the model to generate the sentences with the higher ROUGE score. While these methods are primarily supervised, Laban et al. (2020) proposed an unsupervised method that accounts for fluency, brevity, and coverage in generated summaries using multiple RL-based rewards. The majority of these works are focused on document summarization with conventional non-semantics rewards (ROUGE, BLEU). In contrast, we focus on formulating the semantic rewards that bring a high-level semantic regularization. In particular, we investigate the question’s main characteristics, i.e., question focus and type, to define the rewards. Recently, Ben Abacha and Demner-Fushman (2019) defined the CHQ summarization task and introduced a new benchmark"
2021.acl-short.33,2020.acl-main.703,0,0.029729,"Missing"
2021.acl-short.33,W04-1013,0,0.0638394,"Missing"
2021.acl-short.33,D19-1387,0,0.0461674,"Missing"
2021.acl-short.33,N18-2097,0,0.019922,"report summarization. Most of the participating team (Yadav et al., 2021b; He et al., 2021; S¨anger et al., 2021) utilized transfer learning, knowledgebased, and ensemble methods to solve the question summarization task. Yadav et al. (2021a) proposed question-aware transformer models for question summarization. Xu et al. (2020) automatically created a Chinese dataset (MATINF) for medical question answering, summarization, and classification tasks focusing on maternity and infant categories. Some of the other prominent works in the abstractive summarization of long and short documents include Cohan et al. (2018); Zhang et al. (2019a); MacAvaney et al. (2019); Sotudeh et al. (2020). Proposed Method 3.1 Question-aware Semantic Rewards (a) Question-type Identification Reward: Independent of the pre-training task, most language models use maximum likelihood estimation (MLE)based training for fine-tuning the downstream tasks. MLE has two drawbacks: (1) “exposure bias” (Ranzato et al., 2016) when the model expects gold-standard data at each step during training but does not have such supervision when testing, and (2) “representational collapse” (Aghajanyan et al., 2021), is the degradation of generalizable"
2021.acl-short.33,2020.coling-main.249,1,0.84174,"information of the original question. We propose a RL-based question summarizer model over the Transformer (Vaswani et al., 2017) encoderdecoder architecture. We describe below the proposed reward functions. Related Work In recent years, reinforcement learning (RL) based models have been explored for the abstractive summarization task. Paulus et al. (2017) introduced RL in neural summarization models by optimizing the ROUGE score as a reward that led to more readable and concise summaries. Subsequently, several studies (Chen and Bansal, 2018; Pasunuru and Bansal, 2018; Zhang and Bansal, 2019; Gupta et al., 2020; Zhang et al., 2019b) have proposed methods to optimize the model losses via RL that enables the model to generate the sentences with the higher ROUGE score. While these methods are primarily supervised, Laban et al. (2020) proposed an unsupervised method that accounts for fluency, brevity, and coverage in generated summaries using multiple RL-based rewards. The majority of these works are focused on document summarization with conventional non-semantics rewards (ROUGE, BLEU). In contrast, we focus on formulating the semantic rewards that bring a high-level semantic regularization. In particu"
2021.acl-short.33,2020.findings-emnlp.217,0,0.048047,"the question focus as follows: fi = sof tmax(W hi + b). The fine-tuned network is used to compute the reward rQF R (Qp , Q∗ ) as F-Score of question-focus between the generated question summary Qp and the gold question summary Q∗ . 3.2 Policy Gradient REINFORCE We cast question summarization as an RL problem, where the “agent” (ProphetNet decoder) interacts with the “environment” (Question-type or focus prediction networks) to take “actions” (next word prediction) based on the learned “policy” pθ defined by ProphetNet parameters (θ) and observe “reward” (QTR and QFR). We utilized ProphetNet (Qi et al., 2020) as the base model because it is specifically designed for sequence-to-sequence training and it has shown near state-of-the-art results on natural language generation task. We use the REINFORCE algorithm (Williams, 1992) to learn the optimal policy which maximizes the expected reward. Toward this, we minimize the loss function LRL = −EQs ∼pθ [r(Qs , Q∗ )], where Qs is the question formed by sampling the words qts from the model’s s , S). output distribution, i.e. p(qts |q1s , q2s , . . . , qt−1 The derivative of LRL is approximated using a single sample along with baseline estimator b: 5θ LRL"
2021.acl-short.33,2021.bionlp-1.9,0,0.0652319,"Missing"
2021.acl-short.33,P17-1099,0,0.100724,"Missing"
2021.acl-short.33,2020.acl-main.172,0,0.0234658,"2021b; He et al., 2021; S¨anger et al., 2021) utilized transfer learning, knowledgebased, and ensemble methods to solve the question summarization task. Yadav et al. (2021a) proposed question-aware transformer models for question summarization. Xu et al. (2020) automatically created a Chinese dataset (MATINF) for medical question answering, summarization, and classification tasks focusing on maternity and infant categories. Some of the other prominent works in the abstractive summarization of long and short documents include Cohan et al. (2018); Zhang et al. (2019a); MacAvaney et al. (2019); Sotudeh et al. (2020). Proposed Method 3.1 Question-aware Semantic Rewards (a) Question-type Identification Reward: Independent of the pre-training task, most language models use maximum likelihood estimation (MLE)based training for fine-tuning the downstream tasks. MLE has two drawbacks: (1) “exposure bias” (Ranzato et al., 2016) when the model expects gold-standard data at each step during training but does not have such supervision when testing, and (2) “representational collapse” (Aghajanyan et al., 2021), is the degradation of generalizable representations of pre-trained models during the fine-tuning stage. T"
2021.acl-short.33,2020.acl-main.330,0,0.0225374,"the rewards. Recently, Ben Abacha and Demner-Fushman (2019) defined the CHQ summarization task and introduced a new benchmark (M E QS UM) and a pointer-generator model. Ben Abacha et al. (2021) organized the MEDIQA-21 shared task challenge on CHQ, multi-document answers, and radiology report summarization. Most of the participating team (Yadav et al., 2021b; He et al., 2021; S¨anger et al., 2021) utilized transfer learning, knowledgebased, and ensemble methods to solve the question summarization task. Yadav et al. (2021a) proposed question-aware transformer models for question summarization. Xu et al. (2020) automatically created a Chinese dataset (MATINF) for medical question answering, summarization, and classification tasks focusing on maternity and infant categories. Some of the other prominent works in the abstractive summarization of long and short documents include Cohan et al. (2018); Zhang et al. (2019a); MacAvaney et al. (2019); Sotudeh et al. (2020). Proposed Method 3.1 Question-aware Semantic Rewards (a) Question-type Identification Reward: Independent of the pre-training task, most language models use maximum likelihood estimation (MLE)based training for fine-tuning the downstream ta"
2021.acl-short.33,2021.bionlp-1.34,1,0.778827,"ntional non-semantics rewards (ROUGE, BLEU). In contrast, we focus on formulating the semantic rewards that bring a high-level semantic regularization. In particular, we investigate the question’s main characteristics, i.e., question focus and type, to define the rewards. Recently, Ben Abacha and Demner-Fushman (2019) defined the CHQ summarization task and introduced a new benchmark (M E QS UM) and a pointer-generator model. Ben Abacha et al. (2021) organized the MEDIQA-21 shared task challenge on CHQ, multi-document answers, and radiology report summarization. Most of the participating team (Yadav et al., 2021b; He et al., 2021; S¨anger et al., 2021) utilized transfer learning, knowledgebased, and ensemble methods to solve the question summarization task. Yadav et al. (2021a) proposed question-aware transformer models for question summarization. Xu et al. (2020) automatically created a Chinese dataset (MATINF) for medical question answering, summarization, and classification tasks focusing on maternity and infant categories. Some of the other prominent works in the abstractive summarization of long and short documents include Cohan et al. (2018); Zhang et al. (2019a); MacAvaney et al. (2019); Sotud"
2021.acl-short.33,D19-1253,0,0.023892,"at contains the salient information of the original question. We propose a RL-based question summarizer model over the Transformer (Vaswani et al., 2017) encoderdecoder architecture. We describe below the proposed reward functions. Related Work In recent years, reinforcement learning (RL) based models have been explored for the abstractive summarization task. Paulus et al. (2017) introduced RL in neural summarization models by optimizing the ROUGE score as a reward that led to more readable and concise summaries. Subsequently, several studies (Chen and Bansal, 2018; Pasunuru and Bansal, 2018; Zhang and Bansal, 2019; Gupta et al., 2020; Zhang et al., 2019b) have proposed methods to optimize the model losses via RL that enables the model to generate the sentences with the higher ROUGE score. While these methods are primarily supervised, Laban et al. (2020) proposed an unsupervised method that accounts for fluency, brevity, and coverage in generated summaries using multiple RL-based rewards. The majority of these works are focused on document summarization with conventional non-semantics rewards (ROUGE, BLEU). In contrast, we focus on formulating the semantic rewards that bring a high-level semantic regula"
2021.acl-short.33,2020.acl-main.458,0,0.0673991,"Missing"
2021.bionlp-1.8,2021.bionlp-1.31,0,0.163727,"ntence selection techniques. Question Summarization. Table 4 presents the official results of the teams with accepted working notes papers from the 22 teams that participated in the QS task. All approaches submitted to the question summarization task were abstractive methods relying on the fine-tuning of pretrained transformer models (Vaswani et al., 2017). A wide variety of fine tuning, knowledge-based, and ensemble methods was investigated by the participating teams to achieve higher performance (Mrini et al., 2021; Xu et al., 2021; Zhu et al., 2021; S¨anger et al., 2021; Lee et al., 2021b; Balumuri et al., 2021; Yadav et al., 2021; He et al., 2021; Lee et al., 2021a). A first interesting insight from the overview is that building ensemble models with deep neural networks such as discriminators is not a trivial task, and achieves results that stay on par with the best single model (S¨anger et al., 2021). In contrast, heuristic, downstream ensembles of the models outputs led to substantial improvements when compared to its components/single models (He et al., 2021). The best performing approach relied on such an ensemble by ranking the outputs 1. a relevant learning-based ensemble method that could re"
2021.bionlp-1.8,P18-1063,0,0.0147653,"ion evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation. 1 Introduction Text summarization aims to create natural language summaries that represent the most important information in a given text. Extractive summarization approaches tackle the task by selecting content from the original text without any modification (Nallapati et al., 2017; Xiao and Carenini, 2019; Zhong et al., 2020), while abstractive approaches extend the summaries’ vocabulary to out-of-text words (Rush et al., 2015; Gehrmann et al., 2018; Chen and Bansal, 2018). Several past challenges and shared tasks have focused on summarization. The Document Understanding Conference1 (DUC) organized seven 1 2 tac.nist.gov/tracks sites.google.com/view/mediqa2019 4 sites.google.com/view/mediqa2021 5 chiqa.nlm.nih.gov 3 www-nlpir.nist.gov/projects/duc 74 Proceedings of the BioNLP 2021 workshop, pages 74–85 June 11, 2021. ©2021 Association for Computational Linguistics sity dataset (Demner-Fushman et al., 2016) and newly released chest x-ray reports from the Stanford Health Care. Through these tasks, we focus on studying: multiple relevant answers to a medical quest"
2021.bionlp-1.8,2021.bionlp-1.11,0,0.440332,"he candidate sentences and a Markov chain to evaluate them globally. A similar approach was also used by the ChicHealth team (Xu et al., 2021) without a downstream ensemble method. Participating teams used transfer learning (e.g. (Mrini et al., 2021)) as well as answer sentence selection methods. Sentence selection was used in building extractive summaries (e.g. (Can et al., 2021)) and as an intermediate step in abstractive summarization to provide more concise inputs to generative models (e.g. (Le et al., 2021)). Different models, such 11 www.aicrowd.com/challenges/ mediqa-2021 78 Team BDKG (Dai et al., 2021) ChicHealth (Xu et al., 2021) damo nlp (He et al., 2021) IBMResearch (Mahajan et al., 2021) MNLP (Lee et al., 2021a) NCUEE-NLP (Lee et al., 2021b) NLM (Yadav et al., 2021) optumize (Kondadadi et al., 2021) paht nlp (Zhu et al., 2021) QIAI (Delbrouck et al., 2021) SB NITK (Balumuri et al., 2021) UCSD-Adobe (Mrini et al., 2021) UETfishes (Le et al., 2021) UETrice (Can et al., 2021) WBI (S¨anger et al., 2021) Institution Baidu, Inc Chic Health Alibaba Group IBM Research George Mason University National Central University U.S. National Library of Medicine Optum ECNU & Pingan Health Tech Stanford U"
2021.bionlp-1.8,P19-1215,1,0.613087,"anner. 2.3 • The impact of medical data scarcity on the development and performance of summarization methods in comparison with opendomain summarization; • The effects of different summary evaluation measures including lexical metrics such as ROUGE (Lin, 2004), embedding-based metrics such as BERTScore (Zhang et al., 2019), and hybrid ensemble-oriented metrics such as HOLMS (Mrabet and Demner-Fushman, 2020). 2 2.1 3 MEDIQA 2021 Task Descriptions 3.1 Consumer Health Question Summarization (QS) Data Description QS Datasets The MeQSum dataset of consumer health questions and their summaries (Ben Abacha and Demner-Fushman, 2019b) was suggested as a training dataset. It consists of 1,000 consumer health questions and their associated summaries. Participants were encouraged to use available external resources including, but not limited to, medical QA datasets and question focus and type recognition datasets. For instance, the Consumer Health Questions dataset (Kilicoglu et al., 2018) contains annotations of medical entities, focus, and type of the MeQSum questions and additional NLM questions6 . The new QS validation and test sets7 cover a wide range of topics and question types such as Treatment, Information, Side ef"
2021.bionlp-1.8,2021.bionlp-1.33,0,0.0198201,"rained transformer models: out of the 7 teams that submitted papers describing their systems, 6 reported the use of pretrained language models such as BART or PEGASUS in their submissions (Xu et al., 2021; Zhu et al., 2021; Kondadadi et al., 2021; Dai et al., 2021; Mahajan et al., 2021; He et al., 2021). Among them, Xu et al. (2021); Zhu et al. (2021); Dai et al. (2021) reported that best results were achieved with pretrained PEGASUS models, while Kondadadi et al. (2021) reported better results from BART. Xu et al. (2021) and 80 achieved competitive rankings under the CheXbert metric. Lastly, Delbrouck et al. (2021) studied the use of image features for the RSS task: they retrieved and linked images for each study to the report at training and validation time, and combined a visual encoder with a text encoder for the summarization task. They found that at validation time the multi-modal setting is beneficial to the summarization of MIMIC reports, but not to the Indiana reports, potentially due to the distribution shift in the images. from a Pearson coefficient range between 0.663 and 0.958 to a range between 0.193 and 0.372. As all submitted QS runs were described as abstractive or hybrid approaches, thi"
2021.bionlp-1.8,W19-5039,1,0.604137,"n have focused on neural methods (See et al., 2017; Gehrmann et al., 2018) using benchmark datasets compiled from news articles, such as the CNN-DailyMail dataset (CNN-DM) (Hermann et al., 2015). However, despite its importance, fewer efforts have tackled text summarization in the biomedical domain for both consumer and clinical text and its applications in Question Answering (QA) (Afantenos et al., 2005; Mishra et al., 2014; Afzal et al., 2020). While the 2019 BioNLP-MEDIQA3 edition focused on question entailment and textual inference and their applications in medical Question Answering (Ben Abacha et al., 2019), MEDIQA 20214 addresses the gap in medical text summarization by promoting research on summarization for consumer health QA and clinical text. Three shared tasks are proposed for the summarization of (i) consumer health questions, (ii) multiple answers extracted from reliable medical sources to create one answer for each question, and (iii) textual clinical findings in radiology reports to generate radiology impression statements. For the first two tasks, we created new test sets for the official evaluation using consumer health questions received by the U.S. National Library of Medicine (NLM"
2021.bionlp-1.8,2020.coling-main.501,0,0.0335778,"3 RRS Datasets ized similarity and a lexical ROUGE component through a multi-dimensional Gaussian function (Mrabet and Demner-Fushman, 2020). HOLMS was evaluated on multiple DUC and TAC datasets, and three correlation factors (Pearson’s, Spearman’s, and Kendall’s), and was shown to benefit from the complementary strengths of lexical and language model-based similarity measurements for evaluating summarization systems. In this shared task, we chose ROUGE-2 as our official ranking metric following its superiority observed by Owczarzak et al. (2012) on multiple TAC summarization datasets, and by Bhandari et al. (2020c) on the CNN-DM dataset. We chose two additional metrics for the three tasks: (1) BERTScore for its wider adoption as a language model-based text generation metric, and (2) HOLMS for its hybrid and ensemble-oriented approach. For the RRS task we also considered an additional evaluation metric based on the hamming similarity on the labels produced by the CheXbert labeler (Smit et al., 2020) when applied to both the system and reference summaries, similar to the approach by Zhang et al. (2020b). We focus on the summarization of chest radiography reports for the RRS task, since chest radiography"
2021.bionlp-1.8,2020.emnlp-main.751,0,0.18936,"3 RRS Datasets ized similarity and a lexical ROUGE component through a multi-dimensional Gaussian function (Mrabet and Demner-Fushman, 2020). HOLMS was evaluated on multiple DUC and TAC datasets, and three correlation factors (Pearson’s, Spearman’s, and Kendall’s), and was shown to benefit from the complementary strengths of lexical and language model-based similarity measurements for evaluating summarization systems. In this shared task, we chose ROUGE-2 as our official ranking metric following its superiority observed by Owczarzak et al. (2012) on multiple TAC summarization datasets, and by Bhandari et al. (2020c) on the CNN-DM dataset. We chose two additional metrics for the three tasks: (1) BERTScore for its wider adoption as a language model-based text generation metric, and (2) HOLMS for its hybrid and ensemble-oriented approach. For the RRS task we also considered an additional evaluation metric based on the hamming similarity on the labels produced by the CheXbert labeler (Smit et al., 2020) when applied to both the system and reference summaries, similar to the approach by Zhang et al. (2020b). We focus on the summarization of chest radiography reports for the RRS task, since chest radiography"
2021.bionlp-1.8,D18-1443,0,0.0737279,"Missing"
2021.bionlp-1.8,2021.bionlp-1.12,0,0.0634194,"Missing"
2021.bionlp-1.8,2021.bionlp-1.35,0,0.0966133,"was also used by the ChicHealth team (Xu et al., 2021) without a downstream ensemble method. Participating teams used transfer learning (e.g. (Mrini et al., 2021)) as well as answer sentence selection methods. Sentence selection was used in building extractive summaries (e.g. (Can et al., 2021)) and as an intermediate step in abstractive summarization to provide more concise inputs to generative models (e.g. (Le et al., 2021)). Different models, such 11 www.aicrowd.com/challenges/ mediqa-2021 78 Team BDKG (Dai et al., 2021) ChicHealth (Xu et al., 2021) damo nlp (He et al., 2021) IBMResearch (Mahajan et al., 2021) MNLP (Lee et al., 2021a) NCUEE-NLP (Lee et al., 2021b) NLM (Yadav et al., 2021) optumize (Kondadadi et al., 2021) paht nlp (Zhu et al., 2021) QIAI (Delbrouck et al., 2021) SB NITK (Balumuri et al., 2021) UCSD-Adobe (Mrini et al., 2021) UETfishes (Le et al., 2021) UETrice (Can et al., 2021) WBI (S¨anger et al., 2021) Institution Baidu, Inc Chic Health Alibaba Group IBM Research George Mason University National Central University U.S. National Library of Medicine Optum ECNU & Pingan Health Tech Stanford University National Institute of Technology Karnataka UC San Diego & Adobe Research VNU Univ"
2021.bionlp-1.8,2020.acl-main.445,0,0.0460262,"Missing"
2021.bionlp-1.8,2021.bionlp-1.32,0,0.348891,"s used transfer learning (e.g. (Mrini et al., 2021)) as well as answer sentence selection methods. Sentence selection was used in building extractive summaries (e.g. (Can et al., 2021)) and as an intermediate step in abstractive summarization to provide more concise inputs to generative models (e.g. (Le et al., 2021)). Different models, such 11 www.aicrowd.com/challenges/ mediqa-2021 78 Team BDKG (Dai et al., 2021) ChicHealth (Xu et al., 2021) damo nlp (He et al., 2021) IBMResearch (Mahajan et al., 2021) MNLP (Lee et al., 2021a) NCUEE-NLP (Lee et al., 2021b) NLM (Yadav et al., 2021) optumize (Kondadadi et al., 2021) paht nlp (Zhu et al., 2021) QIAI (Delbrouck et al., 2021) SB NITK (Balumuri et al., 2021) UCSD-Adobe (Mrini et al., 2021) UETfishes (Le et al., 2021) UETrice (Can et al., 2021) WBI (S¨anger et al., 2021) Institution Baidu, Inc Chic Health Alibaba Group IBM Research George Mason University National Central University U.S. National Library of Medicine Optum ECNU & Pingan Health Tech Stanford University National Institute of Technology Karnataka UC San Diego & Adobe Research VNU University of Engineering and Technology VNU University of Engineering and Technology Humboldt University of Berlin QS"
2021.bionlp-1.8,2020.coling-main.498,1,0.831345,"Missing"
2021.bionlp-1.8,2021.bionlp-1.38,0,0.192398,"mprovements when compared to its components/single models (He et al., 2021). The best performing approach relied on such an ensemble by ranking the outputs 1. a relevant learning-based ensemble method that could rely either on the textual outputs or the logits of single models. 2. a more systemic way to select the most relevant datasets for both pretraining and fine tuning. Multi-Answer Summarization. Both extractive and abstractive approaches were used by the 17 teams that submitted runs to MAS task (Zhu et al., 2021; Can et al., 2021; Xu et al., 2021; Mrini et al., 2021; Yadav et al., 2021; Le et al., 2021; Lee et al., 2021a). Table 5 and Table 6 present official results of the teams with extractive and abstractive systems when evaluated, respectively, on extractive gold summaries and abstractive gold summaries. The best MAS run (Zhu et al., 2021) relied on an ensemble method and a recent multi-document summarization approach (Xu and Lapata, 2020) using a Roberta model to rank locally the candidate sentences and a Markov chain to evaluate them globally. A similar approach was also used by the ChicHealth team (Xu et al., 2021) without a downstream ensemble method. Participating teams used transf"
2021.bionlp-1.8,2021.bionlp-1.37,0,0.178534,"wide spectrum of sentence selection techniques. Question Summarization. Table 4 presents the official results of the teams with accepted working notes papers from the 22 teams that participated in the QS task. All approaches submitted to the question summarization task were abstractive methods relying on the fine-tuning of pretrained transformer models (Vaswani et al., 2017). A wide variety of fine tuning, knowledge-based, and ensemble methods was investigated by the participating teams to achieve higher performance (Mrini et al., 2021; Xu et al., 2021; Zhu et al., 2021; S¨anger et al., 2021; Lee et al., 2021b; Balumuri et al., 2021; Yadav et al., 2021; He et al., 2021; Lee et al., 2021a). A first interesting insight from the overview is that building ensemble models with deep neural networks such as discriminators is not a trivial task, and achieves results that stay on par with the best single model (S¨anger et al., 2021). In contrast, heuristic, downstream ensembles of the models outputs led to substantial improvements when compared to its components/single models (He et al., 2021). The best performing approach relied on such an ensemble by ranking the outputs 1. a relevant learning-based ensem"
2021.bionlp-1.8,2021.bionlp-1.30,0,0.147355,"wide spectrum of sentence selection techniques. Question Summarization. Table 4 presents the official results of the teams with accepted working notes papers from the 22 teams that participated in the QS task. All approaches submitted to the question summarization task were abstractive methods relying on the fine-tuning of pretrained transformer models (Vaswani et al., 2017). A wide variety of fine tuning, knowledge-based, and ensemble methods was investigated by the participating teams to achieve higher performance (Mrini et al., 2021; Xu et al., 2021; Zhu et al., 2021; S¨anger et al., 2021; Lee et al., 2021b; Balumuri et al., 2021; Yadav et al., 2021; He et al., 2021; Lee et al., 2021a). A first interesting insight from the overview is that building ensemble models with deep neural networks such as discriminators is not a trivial task, and achieves results that stay on par with the best single model (S¨anger et al., 2021). In contrast, heuristic, downstream ensembles of the models outputs led to substantial improvements when compared to its components/single models (He et al., 2021). The best performing approach relied on such an ensemble by ranking the outputs 1. a relevant learning-based ensem"
2021.bionlp-1.8,W12-2601,0,0.0393894,"//github.com/abachaa/ MEDIQA2021/tree/main/Task2 76 3.3 RRS Datasets ized similarity and a lexical ROUGE component through a multi-dimensional Gaussian function (Mrabet and Demner-Fushman, 2020). HOLMS was evaluated on multiple DUC and TAC datasets, and three correlation factors (Pearson’s, Spearman’s, and Kendall’s), and was shown to benefit from the complementary strengths of lexical and language model-based similarity measurements for evaluating summarization systems. In this shared task, we chose ROUGE-2 as our official ranking metric following its superiority observed by Owczarzak et al. (2012) on multiple TAC summarization datasets, and by Bhandari et al. (2020c) on the CNN-DM dataset. We chose two additional metrics for the three tasks: (1) BERTScore for its wider adoption as a language model-based text generation metric, and (2) HOLMS for its hybrid and ensemble-oriented approach. For the RRS task we also considered an additional evaluation metric based on the hamming similarity on the labels produced by the CheXbert labeler (Smit et al., 2020) when applied to both the system and reference summaries, similar to the approach by Zhang et al. (2020b). We focus on the summarization o"
2021.bionlp-1.8,W04-1013,0,0.240166,"While state-of-the-art techniques in language generation have enabled the generation of fluent summaries, these models occasionally generate spurious facts limiting the clinical validity of the generated summaries (Zhang et al., 2020b). It is therefore important to develop systems that are able to summarize the radiology findings in a consistent manner. 2.3 • The impact of medical data scarcity on the development and performance of summarization methods in comparison with opendomain summarization; • The effects of different summary evaluation measures including lexical metrics such as ROUGE (Lin, 2004), embedding-based metrics such as BERTScore (Zhang et al., 2019), and hybrid ensemble-oriented metrics such as HOLMS (Mrabet and Demner-Fushman, 2020). 2 2.1 3 MEDIQA 2021 Task Descriptions 3.1 Consumer Health Question Summarization (QS) Data Description QS Datasets The MeQSum dataset of consumer health questions and their summaries (Ben Abacha and Demner-Fushman, 2019b) was suggested as a training dataset. It consists of 1,000 consumer health questions and their associated summaries. Participants were encouraged to use available external resources including, but not limited to, medical QA dat"
2021.bionlp-1.8,2020.emnlp-main.296,0,0.0290345,"both pretraining and fine tuning. Multi-Answer Summarization. Both extractive and abstractive approaches were used by the 17 teams that submitted runs to MAS task (Zhu et al., 2021; Can et al., 2021; Xu et al., 2021; Mrini et al., 2021; Yadav et al., 2021; Le et al., 2021; Lee et al., 2021a). Table 5 and Table 6 present official results of the teams with extractive and abstractive systems when evaluated, respectively, on extractive gold summaries and abstractive gold summaries. The best MAS run (Zhu et al., 2021) relied on an ensemble method and a recent multi-document summarization approach (Xu and Lapata, 2020) using a Roberta model to rank locally the candidate sentences and a Markov chain to evaluate them globally. A similar approach was also used by the ChicHealth team (Xu et al., 2021) without a downstream ensemble method. Participating teams used transfer learning (e.g. (Mrini et al., 2021)) as well as answer sentence selection methods. Sentence selection was used in building extractive summaries (e.g. (Can et al., 2021)) and as an intermediate step in abstractive summarization to provide more concise inputs to generative models (e.g. (Le et al., 2021)). Different models, such 11 www.aicrowd.co"
2021.bionlp-1.8,D15-1044,0,0.0288439,"y of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation. 1 Introduction Text summarization aims to create natural language summaries that represent the most important information in a given text. Extractive summarization approaches tackle the task by selecting content from the original text without any modification (Nallapati et al., 2017; Xiao and Carenini, 2019; Zhong et al., 2020), while abstractive approaches extend the summaries’ vocabulary to out-of-text words (Rush et al., 2015; Gehrmann et al., 2018; Chen and Bansal, 2018). Several past challenges and shared tasks have focused on summarization. The Document Understanding Conference1 (DUC) organized seven 1 2 tac.nist.gov/tracks sites.google.com/view/mediqa2019 4 sites.google.com/view/mediqa2021 5 chiqa.nlm.nih.gov 3 www-nlpir.nist.gov/projects/duc 74 Proceedings of the BioNLP 2021 workshop, pages 74–85 June 11, 2021. ©2021 Association for Computational Linguistics sity dataset (Demner-Fushman et al., 2016) and newly released chest x-ray reports from the Stanford Health Care. Through these tasks, we focus on studyin"
2021.bionlp-1.8,2021.bionlp-1.34,0,0.476412,"repancies in rankings. In parallel, HOLMS was recently proposed as an ensemble measure combining both contextual9 https://physionet.org/content/ mimic-cxr/2.0.0/ 10 openi.nlm.nih.gov/faq#collection 77 of PEGASUS, T5, and BART models according to hand-picked features based on the contents of the input question and lengths of the outputs. Spell checking was also a performance boost factor in the question summarization task with some teams using a knowledge base to correct misspelling errors in the original long questions (He et al., 2021), and others relying on third party tools such as CSpell (Yadav et al., 2021; Lu et al., 2019). The datasets used for transfer learning or fine-tuning also played a major role in the achieved performance as demonstrated, for instance, by the combination of datasets from HealthCareMagic, question entailment recognition and question summarization in (Mrini et al., 2021). Moving forward, we think that the overview of the question summarization task revealed two key challenges that need to be addressed to enhance the relevance and performance of existing systems: ing (Zhang et al., 2018), and a zero-shot T5-base summarization model (Raffel et al., 2020). 5 Official Result"
2021.bionlp-1.8,P17-1099,0,0.0891768,"a Yassine Mrabet NLM/NIH NLM/NIH benabachaa@nih.gov mrabety@mail.nih.gov Yuhao Zhang Stanford University zyh@stanford.edu Chaitanya Shivade Curtis Langlotz Dina Demner-Fushman Amazon Stanford University NLM/NIH shivadc@amazon.com langlotz@stanford.edu ddemner@mail.nih.gov Abstract challenges from 2000 to 2007 and the Text Analysis Conference2 (TAC) ran four shared tasks (2008-2011) on news summarization. The last TAC 2014 summarization task tackled biomedical article summarization with referring sentences from external citations. Recent efforts in summarization have focused on neural methods (See et al., 2017; Gehrmann et al., 2018) using benchmark datasets compiled from news articles, such as the CNN-DailyMail dataset (CNN-DM) (Hermann et al., 2015). However, despite its importance, fewer efforts have tackled text summarization in the biomedical domain for both consumer and clinical text and its applications in Question Answering (QA) (Afantenos et al., 2005; Mishra et al., 2014; Afzal et al., 2020). While the 2019 BioNLP-MEDIQA3 edition focused on question entailment and textual inference and their applications in medical Question Answering (Ben Abacha et al., 2019), MEDIQA 20214 addresses the g"
2021.bionlp-1.8,2020.acl-main.704,0,0.0402752,"Missing"
2021.bionlp-1.8,2020.emnlp-main.117,0,0.0582411,"Missing"
2021.bionlp-1.8,W18-5623,1,0.92809,"Missing"
2021.bionlp-1.8,2021.bionlp-1.9,0,0.084695,"Missing"
2021.bionlp-1.8,2020.acl-main.458,1,0.924973,"aims to promote the development of clinical summarization models that are able to generate the concise impression section (i.e., summary) of a radiology report conditioned on the free-text findings and background sections (Zhang et al., 2018). The resulting systems have significant potential to improve the efficiency of clinical communications and accelerate the radiology workflow. While state-of-the-art techniques in language generation have enabled the generation of fluent summaries, these models occasionally generate spurious facts limiting the clinical validity of the generated summaries (Zhang et al., 2020b). It is therefore important to develop systems that are able to summarize the radiology findings in a consistent manner. 2.3 • The impact of medical data scarcity on the development and performance of summarization methods in comparison with opendomain summarization; • The effects of different summary evaluation measures including lexical metrics such as ROUGE (Lin, 2004), embedding-based metrics such as BERTScore (Zhang et al., 2019), and hybrid ensemble-oriented metrics such as HOLMS (Mrabet and Demner-Fushman, 2020). 2 2.1 3 MEDIQA 2021 Task Descriptions 3.1 Consumer Health Question Summa"
2021.bionlp-1.8,2020.acl-main.552,0,0.0159672,"datasets, the models and techniques developed by various teams, the results of the evaluation, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation. 1 Introduction Text summarization aims to create natural language summaries that represent the most important information in a given text. Extractive summarization approaches tackle the task by selecting content from the original text without any modification (Nallapati et al., 2017; Xiao and Carenini, 2019; Zhong et al., 2020), while abstractive approaches extend the summaries’ vocabulary to out-of-text words (Rush et al., 2015; Gehrmann et al., 2018; Chen and Bansal, 2018). Several past challenges and shared tasks have focused on summarization. The Document Understanding Conference1 (DUC) organized seven 1 2 tac.nist.gov/tracks sites.google.com/view/mediqa2019 4 sites.google.com/view/mediqa2021 5 chiqa.nlm.nih.gov 3 www-nlpir.nist.gov/projects/duc 74 Proceedings of the BioNLP 2021 workshop, pages 74–85 June 11, 2021. ©2021 Association for Computational Linguistics sity dataset (Demner-Fushman et al., 2016) and new"
2021.bionlp-1.8,D19-1298,0,0.0180016,"describe the tasks, the datasets, the models and techniques developed by various teams, the results of the evaluation, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation. 1 Introduction Text summarization aims to create natural language summaries that represent the most important information in a given text. Extractive summarization approaches tackle the task by selecting content from the original text without any modification (Nallapati et al., 2017; Xiao and Carenini, 2019; Zhong et al., 2020), while abstractive approaches extend the summaries’ vocabulary to out-of-text words (Rush et al., 2015; Gehrmann et al., 2018; Chen and Bansal, 2018). Several past challenges and shared tasks have focused on summarization. The Document Understanding Conference1 (DUC) organized seven 1 2 tac.nist.gov/tracks sites.google.com/view/mediqa2019 4 sites.google.com/view/mediqa2021 5 chiqa.nlm.nih.gov 3 www-nlpir.nist.gov/projects/duc 74 Proceedings of the BioNLP 2021 workshop, pages 74–85 June 11, 2021. ©2021 Association for Computational Linguistics sity dataset (Demner-Fushman"
2021.bionlp-1.8,2021.bionlp-1.10,0,0.0608736,"Missing"
2021.findings-emnlp.297,2020.emnlp-main.580,0,0.0941818,"Missing"
2021.findings-emnlp.297,2020.wnut-1.12,0,0.0993255,", the claims are synthetic since they are created by altering the evidence sentences. Augenstein et al. (2019) introduced M ULTI FC, the claim verification dataset of natural claims. It consists of 34,918 claims, collected from 26 fact checking websites in English, the evidence pages to verify the claims, and other metadata information. In the medical domain, a new dataset was introduced for the TREC 2020 Health Misinformation Track. Documents related to COVID-19 from the CommonCrawl News dataset1 have been used. In this dataset, the evidence for claim validation was missing. Kotonya and Toni (2020) built a dataset called P UB H EALTH which includes 11.8K claims accompanied by journalists’ explanations from factchecking websites (e.g. Snopes, Politifact). P UB H EALTH is designed to evaluate veracity prediction and explanation generation tasks. The majority of the claims in this dataset are false. Wadden et al. (2020) created S CI FACT, a corpus of 1.4k scientific claims accompanied by abstracts that support or refute each claim. This dataset, however, contains synthetic claims. The existing datasets for evidence-based factchecking systems are either based on mutated sentences (e.g. crea"
C16-1104,S14-1010,0,0.0205178,"ive as abstractive summarization or text generation. Kim et al. proposed a sub-topic or theme detection method for multi-document clustering and topical summarization of citation data (Kim et al., 2015). Previous work shows that Recognizing Textual Entailment (RTE) can provide effective information for text summarization. RTE is the task of recognizing an inference relation between two sentences expressing the fact that the meaning of one sentence is entailed by the other (Androutsopoulos and Malakasiotis, 2010; Dagan et al., 2013). In particular, Entailment-based minimum vertex cover method (Gupta et al., 2014) is an RTE method for single document summarization using graph-based algorithms. Textual entailment and logic segmentation based methods also improved performance for single document summarization (Tatar et al., 2008). Keyword identification methods were also used in single and multiple document summarization and document clustering (Frigui and Nasraoui, 2004; Hammouda et al., 2005), as well as summary generation based on the salience of sentences (Erkan and Radev, 2004). A more detailed survey of summarization methods is presented in (Nenkova and McKeown, 2012). In this paper, we propose a n"
C16-1104,D15-1094,0,0.024168,"al. developed COMPENDIUM, a text summarization system for generating abstractive and extractive summaries for individual biomedical papers (Lloret et al., 2013). They observed that 1 http://www.ncbi.nlm.nih.gov/pmc 1093 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1093–1100, Osaka, Japan, December 11-17 2016. extractive methods are as effective as abstractive summarization or text generation. Kim et al. proposed a sub-topic or theme detection method for multi-document clustering and topical summarization of citation data (Kim et al., 2015). Previous work shows that Recognizing Textual Entailment (RTE) can provide effective information for text summarization. RTE is the task of recognizing an inference relation between two sentences expressing the fact that the meaning of one sentence is entailed by the other (Androutsopoulos and Malakasiotis, 2010; Dagan et al., 2013). In particular, Entailment-based minimum vertex cover method (Gupta et al., 2014) is an RTE method for single document summarization using graph-based algorithms. Textual entailment and logic segmentation based methods also improved performance for single document"
E09-1084,P08-2052,0,0.0179827,"ultiple domains. This supports the notion that we can use non-lexical features to classify potential indexing terms in one biomedical subdomain using training data from another. Maskey and Hirschberg (2005) found that prosodic features (see Ward, 2004) combined with structural features are sufficient to summarize spoken news broadcasts. Prosodic features relate to intonational variation and are associated with particularly important items, whereas structural features are associated with the organization of a typical broadcast: headlines, followed by a description of the stories, etc. Finally, Schilder and Kondadadi (2008) describe non-lexical word-frequency features, similar to our ratio features (F.4–F.7), which are used with a regression SVM to efficiently generate query-based multi-document summaries. Classification results are shown in Table 3. The precision and recall of the classification scheme is shown for the manual classification by reviewers A and B in the first and second rows. The third row contains the results obtained from combining the results of the two reviewers, and the fourth row shows the classification results compared to the gold standard obtained after discovering the initial inter-anno"
E09-1084,W01-1007,0,0.0839186,"Missing"
H05-1117,N04-1007,1,0.562328,"itute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA jimmylin@umd.edu, demner@cs.umd.edu Abstract Actually a misnomer, definition questions can be better paraphrased as “Tell me interesting things about X.”, where X can be a person, an organization, a common noun, etc. Taken another way, definition questions might be viewed as simultaneously asking a whole series of factoid questions about the same entity (e.g., “When was he born?”, “What was his occupation?”, “Where did he live?”, etc.), except that these questions are not known in advance; see Prager et al. (2004) for an implementation based on this view of definition questions. Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called P OURPRE, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system’s response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address"
H05-1117,N03-1020,0,0.0267362,"ragments from documents (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variation of document summarization. Because TREC answer nuggets were terse phrases, the authors found it necessary to rephrase them—two humans were asked to manually create “reference answers” based on the assessors’ nuggets and IR results, which was a labor"
H05-1117,C04-1072,0,0.0194165,"Official definition of F-measure. in a system response, given that they were usually extracted text fragments from documents (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variation of document summarization. Because TREC answer nuggets were terse phrases, the authors found it necessary to rephrase them—two humans were asked t"
H05-1117,P02-1040,0,0.128799,"R = 100 ( × (r + a) 1 if l &lt; α = 1 − l−α otherwise l (β 2 + 1) × P × R β2 × P + R β = 5 in TREC 2003, β = 3 in TREC 2004. Finally, the F (β) = Figure 2: Official definition of F-measure. in a system response, given that they were usually extracted text fragments from documents (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variatio"
H05-1117,P04-1073,0,0.027592,"Science 3 Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA jimmylin@umd.edu, demner@cs.umd.edu Abstract Actually a misnomer, definition questions can be better paraphrased as “Tell me interesting things about X.”, where X can be a person, an organization, a common noun, etc. Taken another way, definition questions might be viewed as simultaneously asking a whole series of factoid questions about the same entity (e.g., “When was he born?”, “What was his occupation?”, “Where did he live?”, etc.), except that these questions are not known in advance; see Prager et al. (2004) for an implementation based on this view of definition questions. Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called P OURPRE, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system’s response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address"
H05-1117,P04-1078,0,0.0379643,"es, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variation of document summarization. Because TREC answer nuggets were terse phrases, the authors found it necessary to rephrase them—two humans were asked to manually create “reference answers” based on the assessors’ nuggets and IR results, which was a laborintensive process. Furthermore, Xu e"
H05-1117,P04-1077,0,\N,Missing
H05-1117,P04-1079,0,\N,Missing
J07-1005,P06-1106,1,0.474642,"mputational linguistics—redundancy detection for multi-document summarization—seems easy by comparison. Furthermore, it is unclear if textual strings make “good answers.” Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Perhaps some variation of multi-level bulleted lists, appropriately integrated with interface elements for expanding and hiding items, might provide physicians a better overview of the information landscape; see, for example, Demner-Fushman and Lin (2006). Recognizing this complex set of issues, we decided to take a simple extractive approach to answer generation. For each abstract in our reranked list of citations, our system produces an answer by combining the title of the abstract and the top three outcome sentences (in the order they appeared in the abstract). We employed the outcome scores generated by the regression model. No attempt was made to synthesize information from multiple citations. A formal evaluation of this simple approach to answer generation is presented in the next section. 10. Evaluation of Clinical Answers Evaluation of"
J07-1005,W04-2611,0,0.232825,"Missing"
J07-1005,N04-1007,1,0.324843,"Missing"
J07-1005,W04-3103,0,0.0235487,"Missing"
J07-1005,H05-1117,1,0.471269,"an directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion—it need not be repeated unless the physician wishes to “drill down”; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems. It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objective"
J07-1005,W05-0906,1,0.867725,"an directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion—it need not be repeated unless the physician wishes to “drill down”; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems. It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objective"
J07-1005,N06-1049,1,0.2699,"re exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc¸a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc¸a reported good performance for etiology, diagnosis, and in particu"
J07-1005,W06-3309,1,0.515262,"and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al. 2006). Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides"
J07-1005,N04-1019,0,0.0125339,"Missing"
J07-1005,W04-0509,0,0.386645,"s a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary"
J07-1005,W04-0508,0,0.0410269,"Missing"
L16-1530,W11-0207,1,0.882378,"Missing"
L16-1530,J07-1005,1,0.744215,"ts. • KODA (Mrabet et al., 2015) is a named entity annotator that relies on the relationships between the entities specified in a knowledge base to perform a global disambiguation of all noun phrases in the target text. KODA is knowledge base-agnostic and has been evaluated on several “wikification” benchmarks, on which it outperformed machine-learning based systems significantly. For pre-annotation, KODA relied on the DBpedia (Auer et al., 2007) open-domain knowledge base, extracted from Wikipedia infoboxes. • Customized UMLS dictionary lookup relies on a set of four customized dictionaries (Demner-Fushman and Lin, 2007) created using UMLS concepts belonging to pre-defined semantic types. The dictionaries contain Problem, Intervention, Anatomy, and Population Group terms. For example, the Problem dictionary includes UMLS terms from semantic types, such as Congenital Abnormality, Injury or Poisoning, Neoplastic Process, and Bacteria. Some common false positive UMLS terms were filtered from the lexicons (e.g., age as an intervention due to its being an abbreviation for advanced end glycosylation). Exact string matching based on UMLS Metathesaurus synonyms is performed. This method only considers the longestmatc"
L16-1530,S15-2051,0,0.0285974,"ties. Several annotated corpora addressed named entities, including problems and treatments, in clinical narratives, such as discharge summaries. For instance, the i2b2 corpus and the related 2010 clinical NLP challenge (Uzuner et al., 2011) focused on recognition of three categories of biomedical concepts (Problem, Treatment, and Test) as well as the extraction of relations and assertions that involve these concepts. The ShARe corpus (Saeed et al., 2002) was used for the 2013 ShARe/CLEF challenge (Suominen et al., 2013) as well as for the SemEval 2014 (Pradhan et al., 2014) and SemEval 2015 (Elhadad et al., 2015) tasks on the analysis of clinical text. This corpus focuses on disorder mentions and their unique UMLS concept identifiers (CUIs). In contrast to the i2b2 corpus, the discontinuous entities are annotated in the ShARe corpus. There are also annotation efforts in languages other than English. For example, Quaero French Medical Corpus (N´ev´eol et al., 2014) addresses clinically relevant entities in French and was used for the CLEF eHealth 2015 challenge on clinical NER (N´ev´eol et al., 2015). The Quaero corpus allowed annotation of nested entities. Several biomedical corpora focused on named e"
L16-1530,W13-1907,1,0.862293,"ions are concerned with disease information, such as diagnosis, treatment, and prognosis, and drug information, including ingredients, generic names, and adverse effects. In 2014, NLM received more than 2,500 questions that were classified as disease-related and more than 2,100 questions that were categorized as drug-related by the customer service staff. We have been building a system to assist customer service staff in answering such questions. Focusing on disease questions only, we previously reported an end-to-end question understanding system that extracts question frames from questions (Kilicoglu et al., 2013), which form the basis for search engine queries. Our previous work in question understanding also involved more intermediate tasks, such as question decomposition and focus recognition (Roberts et al., 2014b), question type recognition (Roberts et al., 2014a), anaphora and ellipsis resolution (Kilicoglu et al., 2013), and spelling correction (Kilicoglu et al., 2015). Named entity recognition and normalization is a core aspect of most of these tasks (e.g., frame extraction and focus recognition). So far, we have used relatively simple dictionary lookup methods to identify named entities in que"
L16-1530,W04-1213,0,0.049504,"English. For example, Quaero French Medical Corpus (N´ev´eol et al., 2014) addresses clinically relevant entities in French and was used for the CLEF eHealth 2015 challenge on clinical NER (N´ev´eol et al., 2015). The Quaero corpus allowed annotation of nested entities. Several biomedical corpora focused on named entities in biomedical literature. The GENIA corpus (Kim et al., 2003b) consists of 2000 abstracts annotated for named entities from the molecular biology domain, including proteins, protein complexes, amino acids, and DNA domains and regions, and was used for the JNLPBA shared task (Kim et al., 2004). A similar corpus is BioInfer (Pyysalo et al., 2007), in which both named entities and their relations are annotated. Both corpora allow nested entities, although their annotation is relatively restricted from a semantic perspective. The NCBI disease corpus (Do˘gan et al., 2014) provides disease annotations in 793 MEDLINE abstracts. Only disease mentions referring to a unique UMLS concept with specific UMLS semantic types are annotated. Nested and discontinuous mentions were not annotated. Pre-annotations from an automatic classifier were used as the starting point. The CHEMDNER corpus (Krall"
L16-1530,S14-2007,0,0.0236243,"pora annotated for biomedical named entities. Several annotated corpora addressed named entities, including problems and treatments, in clinical narratives, such as discharge summaries. For instance, the i2b2 corpus and the related 2010 clinical NLP challenge (Uzuner et al., 2011) focused on recognition of three categories of biomedical concepts (Problem, Treatment, and Test) as well as the extraction of relations and assertions that involve these concepts. The ShARe corpus (Saeed et al., 2002) was used for the 2013 ShARe/CLEF challenge (Suominen et al., 2013) as well as for the SemEval 2014 (Pradhan et al., 2014) and SemEval 2015 (Elhadad et al., 2015) tasks on the analysis of clinical text. This corpus focuses on disorder mentions and their unique UMLS concept identifiers (CUIs). In contrast to the i2b2 corpus, the discontinuous entities are annotated in the ShARe corpus. There are also annotation efforts in languages other than English. For example, Quaero French Medical Corpus (N´ev´eol et al., 2014) addresses clinically relevant entities in French and was used for the CLEF eHealth 2015 challenge on clinical NER (N´ev´eol et al., 2015). The Quaero corpus allowed annotation of nested entities. Sever"
L16-1530,W14-3405,1,0.848273,"uestions that were classified as disease-related and more than 2,100 questions that were categorized as drug-related by the customer service staff. We have been building a system to assist customer service staff in answering such questions. Focusing on disease questions only, we previously reported an end-to-end question understanding system that extracts question frames from questions (Kilicoglu et al., 2013), which form the basis for search engine queries. Our previous work in question understanding also involved more intermediate tasks, such as question decomposition and focus recognition (Roberts et al., 2014b), question type recognition (Roberts et al., 2014a), anaphora and ellipsis resolution (Kilicoglu et al., 2013), and spelling correction (Kilicoglu et al., 2015). Named entity recognition and normalization is a core aspect of most of these tasks (e.g., frame extraction and focus recognition). So far, we have used relatively simple dictionary lookup methods to identify named entities in questions and normalize them to UMLS Metathesaurus concepts (Lindberg et al., 1993). However, it has become increasingly clear that more sophisticated methods are needed, since the methods we explored assume we"
L16-1530,E12-2021,0,0.0825652,"Missing"
L16-1598,andersen-etal-2012-creation,0,0.0260393,"al Question Answering: Medical QA has seen significant interest (Athenikos and Han, 2010) due to the tremendous amount of biomedical knowledge, far beyond what any one clinician or researcher could comprehend. The field of medical QA has largely focused on searching for information outside the EHR, both targeted toward clinicians (Yu and Sable, 2005; Kobayashi and Shyu, 2006; Demner-Fushman and Lin, 2007; Schardt et al., 2007; Terol et al., 2007; Yu and Cao, 2008; Athenikos et al., 2009; Cairns et al., 2011; Cao et al., 2011; Patrick and Li, 2012) and consumers (Zhang, 2010; Liu et al., 2011; Andersen et al., 2012; Kilicoglu et al., 2013; Van Der Volgen et al., 2013; Roberts et al., 2014b; Roberts et al., 2014c; Roberts et al., 2014a; Roberts and DemnerFushman, 2016). Clinician-targeted QA systems typically focus on the biomedical literature, while consumer QA systems focus on consumer-friendly websites such as MedlinePlus1 (Schnall and Fowler, 2013). Additional work in Information Retrieval (IR) has sought to bring relevant literature to clinicians using only a small set of general questions (Simpson et al., 2014; Roberts et al., 2015; Roberts et al., 2016). Significantly less work has focused on QA f"
L16-1598,D14-1134,0,0.0135588,"concept. This is simply for human readability and annotation ease, and is similarly handled in the logical form annotation. When input to a semantic parser, all the labels would be collapsed into concept with the appropriate part-of-speech. 5. Logical Form To represent the deep semantics of EHR questions, λcalculus expressions are used. These combine first order logic expressions with λ-expressions that denote sets matching a particular condition. This corresponds well to the organization of structured queries and is similar to many other semantic parsing tasks (Zettlemoyer and Collins, 2005; Artzi et al., 2014). The logical forms in this corpus combine the quantifier λ, predicates (boolean predicates and functions), and variables and literals that act as arguments to the predicates. The predicates can be broken down into two main types: concept predicates that retrieve information from the EHR, and non-concept predicates that manipulate that information. The most common non-concept predicates and their descriptions are detailed in Table 1. Concept predicates are boolean functions that take the form has TYPE(EVENT, CONCEPT, TIME), where TYPE is the semantic type of the CONCEPT (the normalized ID), EV"
L16-1598,D15-1198,0,0.0145412,"te-of-the-art semantic parsers. Semantic Parsing: Semantic parsers have received tremendous interest as of late. Much of the work can be organized by what type of data is used to train the parser. This 1 includes logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Muresan, 2011), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversation logs (Artzi and Zettlemoyer, 2011), and even unsupervised semantic parsing (Poon, 2013). While most of these focused on semantic parsing of questions, recent work includes semantic parsing to Abstract Meaning Representation (Artzi et al., 2015). In this work, our goal is to provide a sufficient number of question/logical form pairs to train a baseline semantic parser. However, the broad range of the medical domain likely means that additional types of data will be necessary to achieve human-like semantic parsing capabilities for EHR questions. 3. Question Decomposition The first annotation layer simplifies questions by splitting those that contain multiple sub-questions, a process we refer to as question decomposition (Roberts et al., 2014b). This ensures that every question has a single logical form that provides one specific answe"
L16-1598,W10-2903,0,0.012033,"mited (only questions that can be answered by the Stanford HIV Drug Resistance Database), which enables semantic parsing to be performed using a small number of hand-built rules. To build a more generalizable QA system with logical forms, sufficient data is necessary to train state-of-the-art semantic parsers. Semantic Parsing: Semantic parsers have received tremendous interest as of late. Much of the work can be organized by what type of data is used to train the parser. This 1 includes logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Muresan, 2011), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversation logs (Artzi and Zettlemoyer, 2011), and even unsupervised semantic parsing (Poon, 2013). While most of these focused on semantic parsing of questions, recent work includes semantic parsing to Abstract Meaning Representation (Artzi et al., 2015). In this work, our goal is to provide a sufficient number of question/logical form pairs to train a baseline semantic parser. However, the broad range of the medical domain likely means that additional types of data will be necessary to achieve human-like semantic parsing capabilities for EHR questions. 3. Question De"
L16-1598,W13-1907,1,0.824693,"Medical QA has seen significant interest (Athenikos and Han, 2010) due to the tremendous amount of biomedical knowledge, far beyond what any one clinician or researcher could comprehend. The field of medical QA has largely focused on searching for information outside the EHR, both targeted toward clinicians (Yu and Sable, 2005; Kobayashi and Shyu, 2006; Demner-Fushman and Lin, 2007; Schardt et al., 2007; Terol et al., 2007; Yu and Cao, 2008; Athenikos et al., 2009; Cairns et al., 2011; Cao et al., 2011; Patrick and Li, 2012) and consumers (Zhang, 2010; Liu et al., 2011; Andersen et al., 2012; Kilicoglu et al., 2013; Van Der Volgen et al., 2013; Roberts et al., 2014b; Roberts et al., 2014c; Roberts et al., 2014a; Roberts and DemnerFushman, 2016). Clinician-targeted QA systems typically focus on the biomedical literature, while consumer QA systems focus on consumer-friendly websites such as MedlinePlus1 (Schnall and Fowler, 2013). Additional work in Information Retrieval (IR) has sought to bring relevant literature to clinicians using only a small set of general questions (Simpson et al., 2014; Roberts et al., 2015; Roberts et al., 2016). Significantly less work has focused on QA for the EHR, though a goo"
L16-1598,P11-1060,0,0.0304239,"that can be answered by the Stanford HIV Drug Resistance Database), which enables semantic parsing to be performed using a small number of hand-built rules. To build a more generalizable QA system with logical forms, sufficient data is necessary to train state-of-the-art semantic parsers. Semantic Parsing: Semantic parsers have received tremendous interest as of late. Much of the work can be organized by what type of data is used to train the parser. This 1 includes logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Muresan, 2011), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversation logs (Artzi and Zettlemoyer, 2011), and even unsupervised semantic parsing (Poon, 2013). While most of these focused on semantic parsing of questions, recent work includes semantic parsing to Abstract Meaning Representation (Artzi et al., 2015). In this work, our goal is to provide a sufficient number of question/logical form pairs to train a baseline semantic parser. However, the broad range of the medical domain likely means that additional types of data will be necessary to achieve human-like semantic parsing capabilities for EHR questions. 3. Question Decomposition The first"
L16-1598,P13-1092,0,0.0212639,"ormed using a small number of hand-built rules. To build a more generalizable QA system with logical forms, sufficient data is necessary to train state-of-the-art semantic parsers. Semantic Parsing: Semantic parsers have received tremendous interest as of late. Much of the work can be organized by what type of data is used to train the parser. This 1 includes logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Muresan, 2011), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversation logs (Artzi and Zettlemoyer, 2011), and even unsupervised semantic parsing (Poon, 2013). While most of these focused on semantic parsing of questions, recent work includes semantic parsing to Abstract Meaning Representation (Artzi et al., 2015). In this work, our goal is to provide a sufficient number of question/logical form pairs to train a baseline semantic parser. However, the broad range of the medical domain likely means that additional types of data will be necessary to achieve human-like semantic parsing capabilities for EHR questions. 3. Question Decomposition The first annotation layer simplifies questions by splitting those that contain multiple sub-questions, a proce"
L16-1598,W14-3405,1,0.903539,"nd Han, 2010) due to the tremendous amount of biomedical knowledge, far beyond what any one clinician or researcher could comprehend. The field of medical QA has largely focused on searching for information outside the EHR, both targeted toward clinicians (Yu and Sable, 2005; Kobayashi and Shyu, 2006; Demner-Fushman and Lin, 2007; Schardt et al., 2007; Terol et al., 2007; Yu and Cao, 2008; Athenikos et al., 2009; Cairns et al., 2011; Cao et al., 2011; Patrick and Li, 2012) and consumers (Zhang, 2010; Liu et al., 2011; Andersen et al., 2012; Kilicoglu et al., 2013; Van Der Volgen et al., 2013; Roberts et al., 2014b; Roberts et al., 2014c; Roberts et al., 2014a; Roberts and DemnerFushman, 2016). Clinician-targeted QA systems typically focus on the biomedical literature, while consumer QA systems focus on consumer-friendly websites such as MedlinePlus1 (Schnall and Fowler, 2013). Additional work in Information Retrieval (IR) has sought to bring relevant literature to clinicians using only a small set of general questions (Simpson et al., 2014; Roberts et al., 2015; Roberts et al., 2016). Significantly less work has focused on QA for the EHR, though a good amount of attention has been paid to IR for the E"
L16-1598,roberts-etal-2014-annotating,1,0.897313,"nd Han, 2010) due to the tremendous amount of biomedical knowledge, far beyond what any one clinician or researcher could comprehend. The field of medical QA has largely focused on searching for information outside the EHR, both targeted toward clinicians (Yu and Sable, 2005; Kobayashi and Shyu, 2006; Demner-Fushman and Lin, 2007; Schardt et al., 2007; Terol et al., 2007; Yu and Cao, 2008; Athenikos et al., 2009; Cairns et al., 2011; Cao et al., 2011; Patrick and Li, 2012) and consumers (Zhang, 2010; Liu et al., 2011; Andersen et al., 2012; Kilicoglu et al., 2013; Van Der Volgen et al., 2013; Roberts et al., 2014b; Roberts et al., 2014c; Roberts et al., 2014a; Roberts and DemnerFushman, 2016). Clinician-targeted QA systems typically focus on the biomedical literature, while consumer QA systems focus on consumer-friendly websites such as MedlinePlus1 (Schnall and Fowler, 2013). Additional work in Information Retrieval (IR) has sought to bring relevant literature to clinicians using only a small set of general questions (Simpson et al., 2014; Roberts et al., 2015; Roberts et al., 2016). Significantly less work has focused on QA for the EHR, though a good amount of attention has been paid to IR for the E"
L16-1598,H93-1012,0,0.260088,"Missing"
L16-1598,A83-1012,0,0.146756,"Missing"
L16-1598,P07-1121,0,0.0397817,"the main difference is the scope of their work is far more limited (only questions that can be answered by the Stanford HIV Drug Resistance Database), which enables semantic parsing to be performed using a small number of hand-built rules. To build a more generalizable QA system with logical forms, sufficient data is necessary to train state-of-the-art semantic parsers. Semantic Parsing: Semantic parsers have received tremendous interest as of late. Much of the work can be organized by what type of data is used to train the parser. This 1 includes logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Muresan, 2011), questionanswer pairs (Clarke et al., 2010; Liang et al., 2011), conversation logs (Artzi and Zettlemoyer, 2011), and even unsupervised semantic parsing (Poon, 2013). While most of these focused on semantic parsing of questions, recent work includes semantic parsing to Abstract Meaning Representation (Artzi et al., 2015). In this work, our goal is to provide a sufficient number of question/logical form pairs to train a baseline semantic parser. However, the broad range of the medical domain likely means that additional types of data will be necessary to achieve human-like sema"
L16-1598,J07-1005,1,\N,Missing
N06-1049,P04-1027,0,0.0620038,"Missing"
N06-1049,N04-1007,1,0.1571,"esearch & education 0.1 okay Receives millions for product endorsements 0.1 okay Receives millions from product endorsements 0.0 okay Abbreviated name to attract boomers Table 5: Answer nuggets for the target “AARP” with weights derived from the nugget pyramid building process. tively stable. We believe that around five assessors yield the smallest nugget pyramid that confers the advantages of the methodology. The idea of building “nugget pyramids” is an extension of a similarly-named evaluation scheme in document summarization, although there are important differences. Nenkova and Passonneau (2004) call for multiple assessors to annotate SCUs, which is much more involved than the methodology presented here, where the nuggets are fixed and assessors only provide additional judgments about their importance. This obviously has the advantage of streamlining the assessment process, but has the potential to miss other important nuggets that were not identified in the first place. Our experimental results, however, suggest that this is a worthwhile tradeoff. The explicit goal of this work was to develop scoring models for nugget-based evaluation that would address shortcomings of the present a"
N06-1049,H05-1117,1,0.852196,"tational Linguistics 2 Evaluation of Complex Questions vital To date, NIST has conducted three large-scale evaluations of complex questions using a nugget-based evaluation methodology: “definition” questions in TREC 2003, “other” questions in TREC 2004 and TREC 2005, and “relationship” questions in TREC 2005. Since relatively few teams participated in the 2005 evaluation of “relationship” questions, this work focuses on the three years’ worth of “definition/other” questions. The nugget-based paradigm has been previously detailed in a number of papers (Voorhees, 2003; Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005a); here, we present only a short summary. System responses to complex questions consist of an unordered set of passages. To evaluate answers, NIST pools answer strings from all participants, removes their association with the runs that produced them, and presents them to a human assessor. Using these responses and research performed during the original development of the question, the assessor creates an “answer key” comprised of a list of “nuggets”—essentially, facts about the target. According to TREC guidelines, a nugget is defined as a fact for which the assessor could make a binary decis"
N06-1049,W05-0906,1,0.944099,"tational Linguistics 2 Evaluation of Complex Questions vital To date, NIST has conducted three large-scale evaluations of complex questions using a nugget-based evaluation methodology: “definition” questions in TREC 2003, “other” questions in TREC 2004 and TREC 2005, and “relationship” questions in TREC 2005. Since relatively few teams participated in the 2005 evaluation of “relationship” questions, this work focuses on the three years’ worth of “definition/other” questions. The nugget-based paradigm has been previously detailed in a number of papers (Voorhees, 2003; Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005a); here, we present only a short summary. System responses to complex questions consist of an unordered set of passages. To evaluate answers, NIST pools answer strings from all participants, removes their association with the runs that produced them, and presents them to a human assessor. Using these responses and research performed during the original development of the question, the assessor creates an “answer key” comprised of a list of “nuggets”—essentially, facts about the target. According to TREC guidelines, a nugget is defined as a fact for which the assessor could make a binary decis"
N06-1049,N03-1020,0,0.0300988,"eeds from complementary perspectives; see, for example, the recent DUC task of query-focused multi-document summarization (Amig´o et al., 2004; Dang, 2005). From an evaluation point of view, this provides opportunities for cross-fertilization and exchange of fresh ideas. As an example of this intellectual discourse, the recently-developed P OURPRE metric for automatically evaluating answers to complex questions (Lin and Demner-Fushman, 2005a) employs n-gram overlap to compare system responses to reference output, an idea originally implemented in the ROUGE metric for summarization evaluation (Lin and Hovy, 2003). Drawing additional inspiration from research on summarization evaluation, we adapt the pyramid evaluation scheme (Nenkova and Passonneau, 2004) to address the shortcomings of the vital/okay distinction in the nugget-based evaluation methodology. The basic intuition behind the pyramid scheme (Nenkova and Passonneau, 2004) is simple: the importance of a fact is directly related to the number of people that recognize it as such (i.e., its popularity). The evaluation methodology calls for assessors to annotate Semantic Content Units (SCUs) found within model reference summaries. The weight assig"
N06-1049,N04-1019,0,0.204067,"., 2004; Dang, 2005). From an evaluation point of view, this provides opportunities for cross-fertilization and exchange of fresh ideas. As an example of this intellectual discourse, the recently-developed P OURPRE metric for automatically evaluating answers to complex questions (Lin and Demner-Fushman, 2005a) employs n-gram overlap to compare system responses to reference output, an idea originally implemented in the ROUGE metric for summarization evaluation (Lin and Hovy, 2003). Drawing additional inspiration from research on summarization evaluation, we adapt the pyramid evaluation scheme (Nenkova and Passonneau, 2004) to address the shortcomings of the vital/okay distinction in the nugget-based evaluation methodology. The basic intuition behind the pyramid scheme (Nenkova and Passonneau, 2004) is simple: the importance of a fact is directly related to the number of people that recognize it as such (i.e., its popularity). The evaluation methodology calls for assessors to annotate Semantic Content Units (SCUs) found within model reference summaries. The weight assigned to an SCU is equal to the number of annotators that have marked the particular unit. These SCUs can be arranged in a pyramid, with the highes"
N06-1049,H05-1038,0,0.119658,"cross all submitted runs is zero: 22 in TREC 2003, 41 in TREC 2004, and 44 in TREC 2005. An evaluation in which the median score for many questions is zero has many shortcomings. For one, it is difficult to tell if a particular run is “better” than another—even though they may be very different in other salient properties such as length, for example. The discriminative power of the present F-score measure is called into question: are present systems 385 that bad, or is the current scoring model insufficient to discriminate between different (poorly performing) systems? Also, as pointed out by Voorhees (2005), a score distribution heavily skewed towards zero makes meta-analysis of evaluation stability hard to perform. Since such studies depend on variability in scores, evaluations would appear more stable than they really are. While there are obviously shortcomings to the current scheme of labeling nuggets as either “vital” or “okay”, the distinction does start to capture the intuition that “not all nuggets are created equal”. Some nuggets are inherently more important than others, and this should be reflected in the evaluation methodology. The solution, we believe, is to solicit judgments from mu"
N09-2011,P07-1126,0,0.0324443,"y of North America and diographics c Woods Hole Oceanographic Institution. Oceanus 42 Related Work Cohen et al (2003) attempt to identify what they refer to as “image pointers” within captions in biomedical publications. The image pointers of interest are, for example, image panel labels, or letters and abbreviations used as an overlay within the image, similar to the Overlay Labels described in Table 1. They developed a set of hand-crafted rules, and a learning method involving Boosted Wrapper Induction on a dataset consisting of biomedical articles related to fluorescence microscope images. Deschacht and Moens (2007) analyze text surrounding images in news articles trying to identify persons and objects in the text that appear in the corresponding image. They start by extracting persons’ names and visual objects using Named Entity Recognition (NER) tools. Next, they measure the “salience” of the extracted named entities within the text with the assumption that more salient named entities in the text will also be present in the accompanying image. Davis et al (2003) develop a NER tool to identify references to a single art object (for example a specific building within an image) in text related to art imag"
P06-1106,P04-1027,0,0.06751,"Missing"
P06-1106,W04-0509,0,0.151894,"e clinical findings. Since the system by default orders the clusters based on size, it implicitly equates “most popular drug” with “best drug”. Although this assumption is false, we have observed in practice that more-studied drugs are more likely to be beneficial. In contrast with the genomics domain, which has received much attention from both the IR and NLP communities, retrieval systems for the clinical domain represent an underexplored area of research. Although individual components that attempt to operationalize principles of evidencebased medicine do exist (Mendonc¸a and Cimino, 2001; Niu and Hirst, 2004), complete end–to– end clinical question answering systems are difstandard non-parametric test for applications of this type. Due to the relatively small test set (only 25 questions), the increase in cumulative relevance exhibited by the cluster round-robin condition is not statistically significant. However, differences for the oracle conditions were significant. 7 Discussion and Related Work According to two separate evaluations, it appears that our system outperforms the PubMed baseline. However, our approach provides more advantages over a linear result set that are not highlighted in thes"
P06-1106,H05-1038,0,0.0503679,"on Complex information needs can rarely be addressed by single documents, but rather require the integration of knowledge from multiple sources. This suggests that modern information retrieval systems, which excel at producing ranked lists of documents sorted by relevance, may not be sufficient to provide users with a good overview of the “information landscape”. Current question answering systems aspire to address this shortcoming by gathering relevant “facts” from multiple documents in response to information needs. The so-called “definition” or “other” questions at recent TREC evaluations (Voorhees, 2005) serve as good examples: 2 Clinical Information Needs Although the need to answer questions related to patient care has been well documented (Covell et al., 1985; Gorman et al., 1994; Ely et al., 1999), studies have shown that existing search systems, e.g., PubMed, the U.S. National Library of Medicine’s search engine, are often unable to supply physicians with clinically-relevant answers in a timely manner (Gorman et al., 1994; Chambliss and Conley, 1996). Clinical information 841 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,"
P06-1106,J07-1005,1,\N,Missing
P11-2049,W10-1910,1,0.844884,"are first identified by matching the path of cue leaf nodes to the root of the rule subtree pattern. If an identical path exists in the sentence, the root of the candidate subtree is thus also identified. The candidate subtree is evaluated for a match by recursively comparing all node children (starting from the root of the subtree) to the rule pattern subtree. Nodes of type *scope* and * match any number of nodes, similar to the semantics of Regex Kleene star (*). 5 Results As an informed baseline, we used a previously developed rule-based system for negation and speculation scope discovery (Apostolova and Tomuro, 2010). The system, inspired by the NegEx algorithm (Chapman et al., 2001), uses a list of phrases split into subsets (preceding vs. following their scope) to identify cues using string matching. The cue scopes extend from the cue to the beginning or end of the sentence, depending on the cue type. Table 3 shows the baseline results. Negation Clinical Full Papers Paper Abstracts Speculation Clinical Full Papers Paper Abstracts Correctly Predicted Cues P R F 94.12 97.61 95.18 54.45 80.12 64.01 63.04 85.13 72.31 All Predicted Cues F 85.66 51.78 59.86 65.87 58.27 73.12 50.84 29.06 38.21 53.27 52.83 64.5"
P11-2049,de-marneffe-etal-2006-generating,0,0.0804305,"Missing"
P11-2049,W10-3001,0,0.0748155,"Missing"
P11-2049,W08-0607,0,0.0128,"ely, rules for detecting both speculation and negation scopes could be concisely expressed as a 284 combination of lexical and syntactic patterns. For ¨ ur and Radev (2009) examined sample example, Ozg¨ BioScope sentences and developed hedging scope rules such as: The scope of a modal verb cue (e.g. may, might, could) is the verb phrase to which it is attached; The scope of a verb cue (e.g. appears, seems) followed by an infinitival clause extends to the whole sentence. Similar lexico-syntactic rules have been also manually compiled and used in a number of hedge scope detection systems, e.g. (Kilicoglu and Bergler, 2008), (Rei and Briscoe, 2010), (Velldal et al., 2010), (Kilicoglu and Bergler, 2010), (Zhou et al., 2010). However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily ac"
P11-2049,W10-3010,0,0.080295,"expressed as a 284 combination of lexical and syntactic patterns. For ¨ ur and Radev (2009) examined sample example, Ozg¨ BioScope sentences and developed hedging scope rules such as: The scope of a modal verb cue (e.g. may, might, could) is the verb phrase to which it is attached; The scope of a verb cue (e.g. appears, seems) followed by an infinitival clause extends to the whole sentence. Similar lexico-syntactic rules have been also manually compiled and used in a number of hedge scope detection systems, e.g. (Kilicoglu and Bergler, 2008), (Rei and Briscoe, 2010), (Velldal et al., 2010), (Kilicoglu and Bergler, 2010), (Zhou et al., 2010). However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily accurate) parse tree representations. We first parsed each sentence in the trainin"
P11-2049,K16-2000,0,0.286279,"Missing"
P11-2049,P08-2026,0,0.0127516,"tends to the whole sentence. Similar lexico-syntactic rules have been also manually compiled and used in a number of hedge scope detection systems, e.g. (Kilicoglu and Bergler, 2008), (Rei and Briscoe, 2010), (Velldal et al., 2010), (Kilicoglu and Bergler, 2010), (Zhou et al., 2010). However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily accurate) parse tree representations. We first parsed each sentence in the training dataset which contained a negation or speculation cue using the Stanford parser (Klein and Manning, 2003; De Marneffe et al., 2006). Figure 1 shows the parse tree of a sample sentence containing a negation cue and its scope. Next, for each cue-scope instance within the sentence, we identified the nearest common ancestor Figure 2: Lexico-syntactic pattern extract"
P11-2049,W09-1105,0,0.0807316,"s developed in recent years. Rele286 vant research was facilitated by the appearance of a publicly available annotated corpus. All systems described below were developed and evaluated against the BioScope corpus (Vincze et al., 2008). ¨ ur and Radev (2009) have developed a superOzg¨ vised classifier for identifying speculation cues and a manually compiled list of lexico-syntactic rules for identifying their scopes. For the performance of the rule based system on identifying speculation scopes, they report 61.13 and 79.89 accuracy for BioScope full papers and abstracts respectively. Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. They modeled the scope finding problem as a classification task that determines if a sentence token is the first token in a scope sequence, the last one, or neither. Results of the scope finding system with predicted hedge signals were reported as F1-scores of 38.16, 59.66, 78.54 and for clinical texts, full papers, and abstracts respectively3 . Accuracy (computed for correctly identified scopes) was reported as 26.21, 35.92, and 65.55 for clinical texts, papers, and abstracts respectively. Morante and Daelem"
P11-2049,W09-1304,0,0.0369075,"s developed in recent years. Rele286 vant research was facilitated by the appearance of a publicly available annotated corpus. All systems described below were developed and evaluated against the BioScope corpus (Vincze et al., 2008). ¨ ur and Radev (2009) have developed a superOzg¨ vised classifier for identifying speculation cues and a manually compiled list of lexico-syntactic rules for identifying their scopes. For the performance of the rule based system on identifying speculation scopes, they report 61.13 and 79.89 accuracy for BioScope full papers and abstracts respectively. Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. They modeled the scope finding problem as a classification task that determines if a sentence token is the first token in a scope sequence, the last one, or neither. Results of the scope finding system with predicted hedge signals were reported as F1-scores of 38.16, 59.66, 78.54 and for clinical texts, full papers, and abstracts respectively3 . Accuracy (computed for correctly identified scopes) was reported as 26.21, 35.92, and 65.55 for clinical texts, papers, and abstracts respectively. Morante and Daelem"
P11-2049,D09-1145,0,0.0382865,"Table 5: Results from applying the pruned rule set on the test data. Precision (P), Recall (R), and F1-score (F) are computed based on the number of correctly identified scope tokens in each sentence. Accuracy (A) is computed for correctly identified full scopes (exact match). 6 Related Work Interest in the task of identifying negation and speculation scopes has developed in recent years. Rele286 vant research was facilitated by the appearance of a publicly available annotated corpus. All systems described below were developed and evaluated against the BioScope corpus (Vincze et al., 2008). ¨ ur and Radev (2009) have developed a superOzg¨ vised classifier for identifying speculation cues and a manually compiled list of lexico-syntactic rules for identifying their scopes. For the performance of the rule based system on identifying speculation scopes, they report 61.13 and 79.89 accuracy for BioScope full papers and abstracts respectively. Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. They modeled the scope finding problem as a classification task that determines if a sentence token is the first token in a scope sequence, the"
P11-2049,W10-3008,0,0.0575017,"peculation and negation scopes could be concisely expressed as a 284 combination of lexical and syntactic patterns. For ¨ ur and Radev (2009) examined sample example, Ozg¨ BioScope sentences and developed hedging scope rules such as: The scope of a modal verb cue (e.g. may, might, could) is the verb phrase to which it is attached; The scope of a verb cue (e.g. appears, seems) followed by an infinitival clause extends to the whole sentence. Similar lexico-syntactic rules have been also manually compiled and used in a number of hedge scope detection systems, e.g. (Kilicoglu and Bergler, 2008), (Rei and Briscoe, 2010), (Velldal et al., 2010), (Kilicoglu and Bergler, 2010), (Zhou et al., 2010). However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily accurate) parse tree repres"
P11-2049,W10-3007,0,0.0381722,"copes could be concisely expressed as a 284 combination of lexical and syntactic patterns. For ¨ ur and Radev (2009) examined sample example, Ozg¨ BioScope sentences and developed hedging scope rules such as: The scope of a modal verb cue (e.g. may, might, could) is the verb phrase to which it is attached; The scope of a verb cue (e.g. appears, seems) followed by an infinitival clause extends to the whole sentence. Similar lexico-syntactic rules have been also manually compiled and used in a number of hedge scope detection systems, e.g. (Kilicoglu and Bergler, 2008), (Rei and Briscoe, 2010), (Velldal et al., 2010), (Kilicoglu and Bergler, 2010), (Zhou et al., 2010). However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily accurate) parse tree representations. We first pars"
P11-2049,W10-3015,0,0.023663,"Missing"
P11-2049,W08-0606,0,\N,Missing
P11-2049,W10-3006,0,\N,Missing
P17-1071,D15-1075,0,0.0262442,"ognition (E3 , E4 ) (E1 , E2 ) - (E3 , E4 ) (Neg) (Gap) 0.712* -0.083** 0.259 0.092 0.143 0.107 0.250 0.212 0.250 0.550 0.563** -0.303* 0.563** 0.144 (P1 , P2 ) (Pos) 0.884 0.708 0.571* 0.730 0.800 0.693** 0.832 Paraphrase Detection (P3 , P4 ) (P1 , P2 ) - (P3 , P4 ) (Neg) (Gap) 0.738 0.146 0.577 0.130 0.583 -0.012 0.746** -0.016 0.875* -0.075 0.497 0.196 0.739 0.093 Figure 4: Example sentences and similarity values. The best value per column is highlighted. The second best is underlined. Worst and second worst values are followed by one and two stars. Entailment examples are taken from SNLI (Bowman et al., 2015). Paraphrase examples are taken from MSRP 4 . missing tokens as well as the normalization quantity βnm in equation 4 to keep the similarity values in the [0,1] range. 2.1 tual Entailment Recognition, Paraphrase Detection, and ranking relevance. The datasets are as follows: Parameter Training • RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006). Each dataset consists of sentence pairs which are annotated with 2 labels: entailment, and nonentailment. They contain respectively (200, 800), (800, 800), and (800, 800) (train, test)"
P17-1071,W99-0625,0,0.240807,"e 8600 Rockville Pike, 20894, Bethesda, MD, USA Abstract ing scale of textual search, similarity measures still play an important role in refining search results to more specific needs such as the recognition of paraphrases and textual entailment, plagiarism detection and fine-grained ranking of information. These tasks are also often performed on dedicated document collections for domain-specific applications where text similarity measures can be directly applied. Finding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015). However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora. For instance, Yih and Meek (2007) presented an approach to improve text similarity measures for web search queries with a length ranging from one word to short sequences of words. The prop"
P17-1071,S14-2001,0,0.032008,"ctions on top of XF (minimum, maximum and average) in table 1. We used the LibLinear7 SVM classifier for this task. In the second part of the evaluation, we use neural networks to compare the efficiency of XFc , XFt and other similarity measures with in the same setting. We use the neural net described in figure 5 for the trained version XFt and the equivalent architecture presented in figure 6 for all other similarity measures. For canonical XFc we use by default the best aggregation for the task as observed in table 3. • Semeval-14-1: a corpus of Sentences Involving Compositional Knowledge (Marelli et al., 2014) consisting of 10,000 English sentence pairs annotated with both similarity scores and relevance labels. Features. After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token: 6 https://wordnet.princeton.edu/ https://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 7 767 Task Datasets XF MIN XF AVG XF MAX RTE 1 55.3 51.4 1 53.9 Entailment Recognition RTE 2 RTE 3 Guardian 53.8 60.0 77.3 57.2 62.5 84.9 61.3 64.7 86.7 SNLI 58.0 62.0 64.3 Paraphrase Detection MSRP 72.1 72.0 7"
P17-1071,W11-0329,0,0.0372481,"asures still play an important role in refining search results to more specific needs such as the recognition of paraphrases and textual entailment, plagiarism detection and fine-grained ranking of information. These tasks are also often performed on dedicated document collections for domain-specific applications where text similarity measures can be directly applied. Finding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015). However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora. For instance, Yih and Meek (2007) presented an approach to improve text similarity measures for web search queries with a length ranging from one word to short sequences of words. The proposed method is tailored to this specific task, and the training models are not expect"
P19-1215,D18-1403,0,0.0353337,"Missing"
P19-1215,P03-1003,0,0.267816,"Missing"
P19-1215,D14-1067,0,0.03131,"veral efforts proposed interactive and non-interactive query relaxation techniques to translate the input questions into structured queries covering specific elements of the questions (Yahya et al., 2013; Mottin et al., 2014; Ben Abacha and Zweigenbaum, 2015; Meng et al., 2017). Other efforts focused on (i) identifying question similarity (Nakov et al., 2016, 2017) and question entailment (Ben Abacha and DemnerFushman, 2019b) in order to retrieve similar or entailed questions that have associated answers, or (ii) paraphrasing the questions and submitting the simplified versions to QA systems (Bordes et al., 2014; Dong et al., 2017). Question simplification or summarization was less studied than the summarization of news articles that has been the focus of neural abstractive methods in recent years (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; See et al., 2017). In this paper, we tackle the task of consumer health question summarization. Consumer health questions are a natural candidate for this task as patients and their families tend to provide numerous peripheral details such as the patient history (Roberts and Demner-Fushman, 2016), that are not always needed to find correct ans"
P19-1215,N16-1012,0,0.0386345,"2015; Meng et al., 2017). Other efforts focused on (i) identifying question similarity (Nakov et al., 2016, 2017) and question entailment (Ben Abacha and DemnerFushman, 2019b) in order to retrieve similar or entailed questions that have associated answers, or (ii) paraphrasing the questions and submitting the simplified versions to QA systems (Bordes et al., 2014; Dong et al., 2017). Question simplification or summarization was less studied than the summarization of news articles that has been the focus of neural abstractive methods in recent years (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; See et al., 2017). In this paper, we tackle the task of consumer health question summarization. Consumer health questions are a natural candidate for this task as patients and their families tend to provide numerous peripheral details such as the patient history (Roberts and Demner-Fushman, 2016), that are not always needed to find correct answers. Recent experiments also showed the 2228 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2228–2234 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics key role o"
P19-1215,D17-1091,0,0.0263454,"d interactive and non-interactive query relaxation techniques to translate the input questions into structured queries covering specific elements of the questions (Yahya et al., 2013; Mottin et al., 2014; Ben Abacha and Zweigenbaum, 2015; Meng et al., 2017). Other efforts focused on (i) identifying question similarity (Nakov et al., 2016, 2017) and question entailment (Ben Abacha and DemnerFushman, 2019b) in order to retrieve similar or entailed questions that have associated answers, or (ii) paraphrasing the questions and submitting the simplified versions to QA systems (Bordes et al., 2014; Dong et al., 2017). Question simplification or summarization was less studied than the summarization of news articles that has been the focus of neural abstractive methods in recent years (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; See et al., 2017). In this paper, we tackle the task of consumer health question summarization. Consumer health questions are a natural candidate for this task as patients and their families tend to provide numerous peripheral details such as the patient history (Roberts and Demner-Fushman, 2016), that are not always needed to find correct answers. Recent experim"
P19-1215,W12-1513,0,0.0611584,"Missing"
P19-1215,D18-1443,0,0.0457783,"Missing"
P19-1215,P16-1154,0,0.0982705,"Missing"
P19-1215,I17-1080,0,0.0875715,"Missing"
P19-1215,N03-1020,0,0.564728,"Missing"
P19-1215,H05-1117,1,0.707154,"Missing"
P19-1215,S17-2003,0,0.0480391,"Missing"
P19-1215,S16-1083,0,0.0281266,"or do you only get symptoms if you have stones? Can a nonfunctioning gallbladder cause symptoms or do you only get symptoms if you have stones? Is it healthy to ingest 500 mg of vitamin c a day? Should I be taking more or less? How much vitamin C should I take a day? Table 1: Examples of question-summary pairs from the created datasets. the target vocabulary in production or test environments. We also test the coverage variant of this model which includes an additional loss term taking into account the diversity of the words that were targeted by the attention layer for a given text Tu et al. (2016). This variant is intended to deal with repetitive word generation issue in sequence to sequence models. 3.2 Data Creation We manually constructed a gold standard corpus, MeQSum, of 1,000 consumer health questions and their associated summaries. We selected the questions from a collection distributed by the U.S. National Library of Medicine (Kilicoglu et al., 2018). Three medical experts performed the manual summarization of the 1K questions using the following guidelines: (i) the summary must allow retrieving correct and complete answers to the original question and (ii) the summary cannot be"
P19-1215,K16-1028,0,0.0868224,"Abacha and Zweigenbaum, 2015; Meng et al., 2017). Other efforts focused on (i) identifying question similarity (Nakov et al., 2016, 2017) and question entailment (Ben Abacha and DemnerFushman, 2019b) in order to retrieve similar or entailed questions that have associated answers, or (ii) paraphrasing the questions and submitting the simplified versions to QA systems (Bordes et al., 2014; Dong et al., 2017). Question simplification or summarization was less studied than the summarization of news articles that has been the focus of neural abstractive methods in recent years (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; See et al., 2017). In this paper, we tackle the task of consumer health question summarization. Consumer health questions are a natural candidate for this task as patients and their families tend to provide numerous peripheral details such as the patient history (Roberts and Demner-Fushman, 2016), that are not always needed to find correct answers. Recent experiments also showed the 2228 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2228–2234 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational L"
P19-1215,D15-1044,0,0.458442,"n et al., 2014; Ben Abacha and Zweigenbaum, 2015; Meng et al., 2017). Other efforts focused on (i) identifying question similarity (Nakov et al., 2016, 2017) and question entailment (Ben Abacha and DemnerFushman, 2019b) in order to retrieve similar or entailed questions that have associated answers, or (ii) paraphrasing the questions and submitting the simplified versions to QA systems (Bordes et al., 2014; Dong et al., 2017). Question simplification or summarization was less studied than the summarization of news articles that has been the focus of neural abstractive methods in recent years (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; See et al., 2017). In this paper, we tackle the task of consumer health question summarization. Consumer health questions are a natural candidate for this task as patients and their families tend to provide numerous peripheral details such as the patient history (Roberts and Demner-Fushman, 2016), that are not always needed to find correct answers. Recent experiments also showed the 2228 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2228–2234 c Florence, Italy, July 28 - August 2, 2019. 2019 Associa"
P19-1215,P17-1099,0,0.415744,"017). Other efforts focused on (i) identifying question similarity (Nakov et al., 2016, 2017) and question entailment (Ben Abacha and DemnerFushman, 2019b) in order to retrieve similar or entailed questions that have associated answers, or (ii) paraphrasing the questions and submitting the simplified versions to QA systems (Bordes et al., 2014; Dong et al., 2017). Question simplification or summarization was less studied than the summarization of news articles that has been the focus of neural abstractive methods in recent years (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; See et al., 2017). In this paper, we tackle the task of consumer health question summarization. Consumer health questions are a natural candidate for this task as patients and their families tend to provide numerous peripheral details such as the patient history (Roberts and Demner-Fushman, 2016), that are not always needed to find correct answers. Recent experiments also showed the 2228 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2228–2234 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics key role of question summariz"
P19-1215,P16-1008,0,0.0281386,"symptoms or do you only get symptoms if you have stones? Can a nonfunctioning gallbladder cause symptoms or do you only get symptoms if you have stones? Is it healthy to ingest 500 mg of vitamin c a day? Should I be taking more or less? How much vitamin C should I take a day? Table 1: Examples of question-summary pairs from the created datasets. the target vocabulary in production or test environments. We also test the coverage variant of this model which includes an additional loss term taking into account the diversity of the words that were targeted by the attention layer for a given text Tu et al. (2016). This variant is intended to deal with repetitive word generation issue in sequence to sequence models. 3.2 Data Creation We manually constructed a gold standard corpus, MeQSum, of 1,000 consumer health questions and their associated summaries. We selected the questions from a collection distributed by the U.S. National Library of Medicine (Kilicoglu et al., 2018). Three medical experts performed the manual summarization of the 1K questions using the following guidelines: (i) the summary must allow retrieving correct and complete answers to the original question and (ii) the summary cannot be"
roberts-etal-2014-annotating,P03-1054,0,\N,Missing
roberts-etal-2014-annotating,J07-1005,1,\N,Missing
roberts-etal-2014-annotating,bott-etal-2012-text,0,\N,Missing
roberts-etal-2014-annotating,W13-1907,1,\N,Missing
roberts-etal-2014-annotating,lacatusu-etal-2006-impact,0,\N,Missing
roberts-etal-2014-annotating,andersen-etal-2012-creation,0,\N,Missing
S17-2057,P08-1019,0,0.0138431,"is year, five subtasks were proposed: English Question-Comment Similarity (subtask A), English Question-Question Similarity (subtask B), English Question-External Comment Similarity (subtask C), Arabic Answer Re-rank (subtask D) and English Multi-Domain Duplicate Question Detection (subtask E). 1 Data 3 Question Similarity vs. Question Entailment In addition to the efforts within the semEval cQA tasks since 2015, earlier definitions and methods were proposed for Question Similarity based on different elements such as the question topic and question type (Burke et al., 1997; Jeon et al., 2005; Duan et al., 2008). But other definitions using specific kinds of question similarity such as entailment and paraphrases are not yet very developed for Question Answering (QA). In a previous effort (Ben Abacha and DemnerFushman, 2016), we introduced a new task called http://alt.qcri.org/semeval2017/task3 349 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 349–352, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Recognizing Question Entailment (RQE), which tackles a specific kind of question similarity. As question entailment ha"
S17-2057,S16-1126,0,0.0654647,"Missing"
S17-2057,S17-2003,0,0.0323048,"Missing"
W05-0906,P04-1027,0,0.296669,"Missing"
W05-0906,P98-1013,0,0.00791267,"text, remains more elusive for a variety of reasons. Finer-grained linguistic analysis at a large scale and sufficiently-rich domain ontologies to support potentially long inference chains are necessary prerequisites—both of which represent open research problems. Furthermore, it is unclear how exactly one would operationalize the evaluation of such capabilities. Nevertheless, we believe that advanced reasoning capabilities based on detailed semantic analyses of text will receive much attention in the future. The recent flurry of work on semantic analysis, based on resources such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury et al., 2002), provide the substrate for reasoning engines. Developments in the automatic construction, adaptation, and merging of ontologies will supply the knowledge necessary to draw inferences. In order to jump-start the knowledge acquisition process, we envision the development of domain-specific question answering systems, the lessons from which will be applied to systems that operate on broader domains. In terms of operationalizing evaluations for these advanced capabilities, the field has already made important first steps, e.g., the Pascal Recognising Textual"
W05-0906,J96-2004,0,0.0343101,"mate average human performance. Fortunately, it appears researchers have realized that “model averaging” may not be the best way to capture the existence of many “equally good” summaries. As an example, the Pyramid Method (Nenkova and Passonneau, 2004), 46 represents a good first attempt at a realistic model of human variations. Second, the view that variations in judgment are an inescapable part of extrinsic evaluations would lead one to conclude that low inter-annotator agreement isn’t necessarily bad. Computational linguistics research generally attaches great value to high kappa measures (Carletta, 1996), which indicate high human agreement on a particular task. Low agreement is seen as a barrier to conducting reproducible research and to drawing generalizable conclusions. However, this is not necessarily true—low agreement in information retrieval has not been a handicap for advancing the state of the art. When dealing with notions such as relevance, low kappa values can most likely be attributed to the nature of the task itself. Attempting to raise agreement by, for example, developing rigid assessment guidelines, may do more harm than good. Prescriptive attempts to define what a good answe"
W05-0906,N04-1019,0,0.361259,"anks. This measure will hopefully spur progress in definition question answering systems. The development of automatic evaluation metrics based on n-gram co-occurrence for question answering is an example of successful knowledge transfer from summarization to question answering evaluation. We believe that there exist many more opportunities for future exploration; as an example, there are remarkable similarities between information nuggets in definition question answering and recently-proposed methods for assessing summaries based on fine-grained semantic units (Teufel and van Halteren, 2004; Nenkova and Passonneau, 2004). Another promising direction of research in definition question answering involves applying the Pyramid Method (Nenkova and Passonneau, 2004) to better model the vital/okay nuggets distinction. As it currently stands, the vital/okay dichotomy is troublesome because there is no way to operationalize such a classification scheme within a system; see Hildebrandt et al. (2004) for more discussion. Yet, the effects on score are significant: a system that returns, for example, all the okay nuggets but none of the vital nuggets would receive a score of zero. In truth, the vital/okay distinction is a"
W05-0906,W02-0404,0,0.0273617,"and topical summaries cannot be generated solely using an extractive approach—sentences are at the wrong level of granularity, a source of problems ranging from dangling anaphoric references to verbose subordinate clauses. Only through more detailed linguistic analysis can information from multiple documents be truly synthesized. Already, there are hybrid approaches to multi-document summarization that employ natural language generation techniques (McKeown et al., 1999; Elson, 2004), and researchers have experimented with sentential operations to improve the discourse structure of summaries (Otterbacher et al., 2002). The primary purpose of this paper was to identify similarities between multi-document summarization and complex question answering, pointing out potential synergistic opportunities in the area of system evaluation. We hope that this is merely a small part of a sustained dialogue between researchers from these two largely independent communities. Answering complex questions and summarizing multiple documents are essentially opposite sides of the same coin, as they represent different approaches to the common problem of addressing complex user information needs. 6 Acknowledgements We would lik"
W05-0906,P02-1040,0,0.0723663,"bility to conduct evaluations with quick turnaround has lead to rapid progress in the state of the art. Question answering for definition questions appears to be missing this critical ingredient. To address this evaluation gap, we have recently developed P OURPRE, a method for automatically evaluating definition questions based on idfweighted unigram co-occurrences (Lin and DemnerFushman, 2005). This idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the B LEU metric for machine translation (Papineni et al., 2002) and the ROUGE (Lin and Hovy, 2003) metric for summarization. Note that metrics for automatically evaluating definitions should be, like metrics for evaluating summaries, biased towards recall. Fluency (i.e., precision) is not usually of concern because most systems employ extractive techniques to produce answers. Our study reports good correlation between the automatically computed P OURPRE metric and official TREC system ranks. This measure will hopefully spur progress in definition question answering systems. The development of automatic evaluation metrics based on n-gram co-occurrence for"
W05-0906,N04-1007,1,0.624213,". Complementary developments in the summarization community mirror the aforementioned shifts in question answering research. Most notably, the DUC 2005 task requires systems to generate answers to natural language questions based on a collection of known relevant documents: “The system task in 2005 will be to synthesize from a set of 25– 50 documents a brief, well-organized, fluent answer to a need for information that cannot be met by just stating a name, date, quantity, etc.” (DUC 2005 guidelines). These guidelines were modeled after the information synthesis task suggested by Amig´o et al. (2004), which they characterize as “the process of (given a complex information need) extracting, organizing, and inter-relating the pieces of information contained in a set of relevant documents, in order to obtain a comprehensive, non-redundant report that satisfies the information need”. One of the examples they provide, “I’m looking for information concerning the history of text compression both before and with computers”, looks remarkably like a user information need current question answering systems aspire to satisfy. The idea of topic-oriented multi-document summarization isn’t new (Goldstei"
W05-0906,W04-3254,0,0.0255838,"Missing"
W05-0906,H05-1117,1,0.796337,"Missing"
W05-0906,N03-1020,0,0.0734058,"ck turnaround has lead to rapid progress in the state of the art. Question answering for definition questions appears to be missing this critical ingredient. To address this evaluation gap, we have recently developed P OURPRE, a method for automatically evaluating definition questions based on idfweighted unigram co-occurrences (Lin and DemnerFushman, 2005). This idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the B LEU metric for machine translation (Papineni et al., 2002) and the ROUGE (Lin and Hovy, 2003) metric for summarization. Note that metrics for automatically evaluating definitions should be, like metrics for evaluating summaries, biased towards recall. Fluency (i.e., precision) is not usually of concern because most systems employ extractive techniques to produce answers. Our study reports good correlation between the automatically computed P OURPRE metric and official TREC system ranks. This measure will hopefully spur progress in definition question answering systems. The development of automatic evaluation metrics based on n-gram co-occurrence for question answering is an example of"
W05-0906,E99-1011,0,\N,Missing
W05-0906,C98-1013,0,\N,Missing
W05-0906,N03-1034,0,\N,Missing
W06-0704,P02-1005,0,0.0235641,"Missing"
W06-0704,C04-1100,0,0.0145455,"within a broader context, i.e., a “scenario”. These complex questions set forth parameters of the desired knowledge, which may include additional facts about the motivation of the information seeker, her assumptions, her current state of knowledge, etc. Presently, most systems that attempt to tackle such complex questions are aimed at serving intelligence analysts, for activities such as counterterrorism and war-fighting. Systems for addressing complex information needs are interesting because they provide an opportunity to explore the role of semantic structures in question answering, e.g., (Narayanan and Harabagiu, 2004). Opportunities include explicit semantic representations for capturing the content of questions and documents, deep inferential mechanisms (Moldovan et al., 2002), and attempts to model task-specific influences in informationseeking environments (Freund et al., 2005). Our own interest in question answering falls in line with these recent developments, but we focus on a different type of user—the primary care physician. The need to answer questions related to patient care at the point of service has been well studied and documented (Gorman et al., 1994; Ely et al., 1999; Ely et al., 2005). How"
W06-0704,W04-0509,0,0.0204964,"based medicine is not new. Many researchers have studied MeSH terms associated with basic clinical tasks (Mendonc¸a and Cimino, 2001; Haynes et al., 1994). Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision; PICObased querying is merely an instance of faceted querying, which has been widely used by librarians since the invention of automated retrieval systems. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004), but our work differs in its focus on the primary medical literature. Approaching clinical needs from a different perspective, the PERSIVAL system leverages patient records to rerank search results (McKeown et al., 2003). Since the primary focus is on personQuestion answering in the clinical domain is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. Discussion and Related Work Recently, researchers have become interested in restricte"
W06-3309,N04-1015,0,0.577447,"erages this knowledge— MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction—provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well. Discriminative approaches (especially SVMs) have been shown to be very effective for many supervised classification tasks; see, for example, (Joachims, 1998; Ng and Jordan, 2001). However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing. Under cert"
W06-3309,P02-1047,0,0.0315097,"y a supervised approach. Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002). Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts. 6 Conclusion We believe that there are two contributions as a result of our work. From the perspective of machine learning, the assignment of sequentially-occurring labels represents an underexplored problem with respect to the generative vs. discriminative debate— previous work has mostly focused on stateless classification tasks. This paper demonstrates that Hidden Markov Models are capable of capturing discourse transitions from section to section, and are"
W06-3309,W00-1302,0,0.0187171,"heir work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002). Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts. 6 Conclusion We believe that there are two contributions as a result of our work. From the perspective of machine learning, the assignment of sequentially-occurring labels represents an underexplored problem with respect to the generative vs. discriminative debate— previous work has mostly focused on stateless classification tasks. This paper demonstrates that Hidden Markov Models are capable of capturing discourse transitions from section to section, and are at least competitive with Support Vector Machines from a pu"
W07-1018,W02-0305,1,0.780336,"semantic interpretation. We exploit SemRep machinery to interpret the aspects of comparative structures just described. 2.2 SemRep SemRep [Rindflesch and Fiszman, 2003; Rindflesch et al., 2005] recovers underspecified semantic propositions in biomedical text based on a partial syntactic analysis and structured domain knowledge from the UMLS. Several systems that extract entities and relations are under development in both the clinical and molecular biology domains. Examples of systems for clinical text are described in [Friedman et al., 1994], [Johnson et al., 1993], [Hahn et al., 2002], and [Christensen et al., 2002]. In molecular biology, examples include [Yen et al., 2006], [Chun et al., 2006], [Blaschke et al., 1999], [Leroy et al., 2003], [Rindflesch et al., 2005], [Friedman et al., 2001], and [Lussier et al., 2006]. During SemRep processing, a partial syntactic parse is produced that depends on lexical look-up in the SPECIALIST lexicon [McCray et al., 1994] and a part-of-speech tagger [Smith et al., 2004]. MetaMap [Aronson, 2001] then matches noun phrases to concepts in the Metathesaurus® and determines the semantic type for each concept. For example, the structure in (9), produced for (8), allows b"
W07-1018,P85-1037,0,0.316265,"Missing"
W07-1018,P89-1020,0,0.802271,"nd less ADJ than in (7) for inferiority). (5) Azithromycin is as effective as erythromycin estolate for the treatment of pertussis in children. 138 (6) Naproxen is safer than aspirin in the treatment of the arthritis of rheumatic fever. (7) Sodium valproate was significantly less effective than prochlorperazine in reducing pain or nausea. In examples (3) through (7), the characteristic the compared drugs have in common is treatment of some disorder, for example treatment of pertussis in children in (5). Few studies describe an implemented automatic analysis of comparatives; however, Friedman [Friedman, 1989] is a notable exception. Jindal and Liu [Jindal and Liu, 2006] use machine learning to identify some comparative structures, but do not provide a semantic interpretation. We exploit SemRep machinery to interpret the aspects of comparative structures just described. 2.2 SemRep SemRep [Rindflesch and Fiszman, 2003; Rindflesch et al., 2005] recovers underspecified semantic propositions in biomedical text based on a partial syntactic analysis and structured domain knowledge from the UMLS. Several systems that extract entities and relations are under development in both the clinical and molecular"
W07-1018,C90-3025,0,0.0828184,"Missing"
W07-1018,J90-2003,0,0.162288,"tis. The processing of comparative expressions such as (1) and (2) was incorporated into an existing system, SemRep [Rindflesch and Fiszman, 2003; Rindflesch et al., 2005], which constructs semantic predications by mapping assertions in biomedical text to the Unified Medical Language System® (UMLS)® [Humphreys et al., 1998]. 2 2.1 Background Comparative structures in English The range of comparative expressions in English is extensive and complex. Several linguistic studies have investigated their characteristics, with differing assumptions about syntax and semantics (for example [Ryan, 1981; Rayner and Banks, 1990; Staab and Hahn, 1997; Huddleston and Pullum, 2002]). Our study concentrates on 137 BioNLP 2007: Biological, translational, and clinical language processing, pages 137–144, c Prague, June 2007. 2007 Association for Computational Linguistics structures in which two drugs are compared with respect to a shared attribute (e.g. how well they treat some disease). An assessment of their relative merit in this regard is indicated by their positions on a scale. The compared terms are expressed as noun phrases, which can be considered to be conjoined. The shared characteristic focused on is expressed a"
W07-1018,P81-1003,0,0.630513,"Missing"
W08-0505,cramer-etal-2006-building,0,0.0300149,"mpiled resources found useful for test suite generation; and describes the adaptation of these resources for evaluation of a clinical question answering system. 1 Introduction The community-wide interest in rapid development in many areas of natural language processing and information retrieval resulted in creation of reusable test collections in large-scale evaluations such as the Text REtrieval Conference (TREC)1 . Researchers in more specific areas, for which no TREC or other collections are available, have to create or find suitable test collections to evaluate their systems. For example, Cramer et al. (2006) recruited volunteers and quickly gathered a sizeable corpus of question-answer pairs for evaluation of German open-domain question answering systems. This was achieved through a Web-based tool that allowed marking up “interesting” passages in Wikipedia articles and then asking questions about the content of those passages. This appealing approach can not easily be applied in the domain of clinical question answering because the quality of the questions and answers as well as the answer completeness are paramount. A test suite for evaluation of clinical question answering systems should contai"
W08-0505,W04-3101,0,0.0163567,"the context that was used in generation of the advice, and finally have access to the original sources of information (Ely et al., 2005). A fair number of high-quality manually created collections present answers to clinical questions in this form and could be obtained online. Three partially freely-available sources: Family Practitioner Inquiry Network (FPIN)2 , Parkhurst Exchange Forum (PE)3 , and BMJ Clinical Evidence (BMJ-CE)4 were used to design and develop the presented test suites and evaluation methods. Although there seems to be a distinction between test collections and test suites (Cohen et al., 2004) (the former defined as “pieces of text” and associated with corpora, the latter, as lists of specially constructed sentences, or sentence sequences, or sentence fragments (Balkan et al., 1994)), evaluation of answers to clinical questions crosses this boundary and requires the availability of carefully generated sentence fragments as well as suitable document collections. 2 http://www.primeanswers.org/primeanswers/ http://www.parkhurstexchange.com/qa/index.php 4 http://www.clinicalevidence.com/ceweb/conditions/index.jsp 3 1 http://trec.nist.gov/ 21 Software Engineering, Testing, and Quality A"
W08-0505,P06-1106,1,0.835703,"fragments generated by a system against the reference list, ideally should be conducted manually by a person with biomedical background. For example, Acetylsalicylic acid in a system’s answer needs to be matched to Aspirin in the reference list. Automation of this step is possible through mapping of both lists to an ontology, e.g., UMLS6 , but such evaluation will be significantly less accurate and potentially biased (if a system uses the same mapping algorithm to find the answer). A manual evaluation based on 30 of 54 BMJ-CE question-answer pairs in the presented test suite is described in (Demner-Fushman and Lin, 2006). Another 50 question-answer pairs originated in FPIN and PE. References 3 Using the test suite in an evaluation The answer presented in Figure 1 can be used to evaluate a system’s answer to this question by extracting the reference list from the FPIN or BMJ-CE answer. Similarly, the second-tier summaries can be used to evaluate the context for the key-points generated by a system. The references can be used to evaluate the quality of the original sources retrieved by a system if the documents in both lists are represented using their unique identifiers: DOI or a PubMed5 identifier. Availabili"
W08-0505,C96-2120,0,\N,Missing
W08-0505,A00-1041,0,\N,Missing
W12-2414,W06-3316,0,0.0163827,"rthography-based patterns and a pronominal coreferencer (matching pronouns to the most recent referent). While coreference resolution is a universal discourse problem, both the scope of the problem and its solution could vary significantly across domains and text genres. Newswire coreference resolution corpora (such as the MUC corpus) and general purpose tools do not always fit the needs of specific domains such as the biomedical domain well. The importance and distinctive characteristics of coreference resolution for biomedical articles has been recognized, for example (Castano et al., 2002; Gasperin, 2006; Gasperin et al., 2007; Su et al., 2008). Within the biomedical field, clinical texts have been noted as a genre that needs specialized coreference corpora and methodologies (Zheng et al., 2011). The importance of the task for the clinical domain has been attested by the 2011 i2b2 NLP shared task (Informatics for Integrating Biology and the Bedside1 ) which provided an evaluation platform for coreference resolution for clinical texts. However, even within the clinical domain, coreference in different sub-genres could vary signifi1 https://www.i2b2.org/NLP/Coreference/ 118 Proceedings of the 2"
W12-2414,D09-1120,0,0.125276,"ties and body regions. Figure 1 shows a sample Breast Ultrasound DICOM6 image and its associated radiology report. The reports were previously tagged (using an automated system) with medical concepts and their semantic types (e.g. anatomical entity, disorder, imaging observation, etc.). Half of the dataset (150 reports) was manually annotated with coreference chains using the simplified task definition described above. The other half of the dataset was used for validation of the system described next. 5 We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). The algorithm first identifies mentions within each report and orders them linearly according to the position of the mention head. Then it selects the antecedent (or the NULL antecedent) for each mention as follows: 1. The possible antecedent candidates are first filtered based on a distance constraint. Only mentions of interest belonging to the preceding two sentences are considered. The rationale for this filtering step is that radiology reports are typically very concise and less cohesive than general texts. Paragraphs often describe multiple observations and anatomical entities sequentia"
W12-2414,P02-1014,0,0.156641,"Missing"
W12-2414,D10-1048,0,0.0215551,"hine learning on the task and achieved an F-score of 62.6 and 60.4 on the MUC-6 (1995) and MUC-7 (1997) coreference corpora respectively. Ng et al. (2002) improved this learning framework and achieved Fscores of 70.4 and 63.4 respectively on the same datasets. There are also a number of freely available offthe-shelf coreference resolution modules developed for the general domain. For example, BART (Versley et al., 2008) is an open source coreference resolution system which provides an implementation of the Soon et al. algorithm (2001). The Stanford Deterministic Coreference Resolution System (Raghunathan et al., 2010) uses an unsupervised sieve-like approach to coreference resolution. Similarly, the GATE Information Extraction system (Cunningham et al., 2002) includes a rule-based coreference resolution module consisting of orthography-based patterns and a pronominal coreferencer (matching pronouns to the most recent referent). While coreference resolution is a universal discourse problem, both the scope of the problem and its solution could vary significantly across domains and text genres. Newswire coreference resolution corpora (such as the MUC corpus) and general purpose tools do not always fit the nee"
W12-2414,J01-4004,0,0.113386,"n heuristics for coreference resolution in radiology reports. The algorithm is shown to perform well on a test dataset of 150 manually annotated radiology reports. 1 Introduction Coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world. General purpose coreference resolution systems typically cluster all mentions (usually noun phrases) in a document into coreference chains according to the underlying reference entity. A number of coreference resolution algorithms have been developed for general texts. To name a few, Soon et al. (2001) employed machine learning on the task and achieved an F-score of 62.6 and 60.4 on the MUC-6 (1995) and MUC-7 (1997) coreference corpora respectively. Ng et al. (2002) improved this learning framework and achieved Fscores of 70.4 and 63.4 respectively on the same datasets. There are also a number of freely available offthe-shelf coreference resolution modules developed for the general domain. For example, BART (Versley et al., 2008) is an open source coreference resolution system which provides an implementation of the Soon et al. algorithm (2001). The Stanford Deterministic Coreference Resolu"
W12-2414,P08-4003,0,0.0225072,"o coreference chains according to the underlying reference entity. A number of coreference resolution algorithms have been developed for general texts. To name a few, Soon et al. (2001) employed machine learning on the task and achieved an F-score of 62.6 and 60.4 on the MUC-6 (1995) and MUC-7 (1997) coreference corpora respectively. Ng et al. (2002) improved this learning framework and achieved Fscores of 70.4 and 63.4 respectively on the same datasets. There are also a number of freely available offthe-shelf coreference resolution modules developed for the general domain. For example, BART (Versley et al., 2008) is an open source coreference resolution system which provides an implementation of the Soon et al. algorithm (2001). The Stanford Deterministic Coreference Resolution System (Raghunathan et al., 2010) uses an unsupervised sieve-like approach to coreference resolution. Similarly, the GATE Information Extraction system (Cunningham et al., 2002) includes a rule-based coreference resolution module consisting of orthography-based patterns and a pronominal coreferencer (matching pronouns to the most recent referent). While coreference resolution is a universal discourse problem, both the scope of"
W13-1907,J12-4003,0,0.0255347,"which deterministic frameworks generally outperform machine learning models (Haghighi and Klein, 2009; Lee et al., 2011). In contrast to coreference resolution, ellipsis resolution remains an understudied NLP problem. One type of ellipsis that received some attention is null instantiation (Fillmore and Baker, 2001), whereby the goal is to recover the referents for an uninstantiated semantic role of a target predicate from the wider discourse context. A semantic evaluation challenge that focused on null instantiation was proposed, although participation was limited (Ruppenhofer et al., 2010). Gerber and Chai (2012) focused on implicit argumentation (i.e., null instantiation) for nominal predicates. They annotated a corpus of implicit arguments for a small number of nominal predicates and trained a discriminative model based on syntactic, semantic and discourse features collected from various linguistic resources. Focusing on a different type of ellipsis, Bos and Spenader (2011) annotated a corpus of verb phrase ellipsis; however, so far there have been little work in verb phrase ellipsis resolution. We 3 Methods We use a pipeline model for question analysis, which results in frame annotations that captu"
W13-1907,D09-1120,0,0.173072,"ference resolution into event extraction pipelines have generally resulted in only modest improvements (Yoshikawa et al., 2011; Miwa et al., 2012; Kilicoglu and Bergler, 2012). Coreference resolution has also been tackled in open domain natural language processing. State-of-the-art systems often employ a combination of lexical, syntactic, shallow semantic and discourse information (e.g., speaker identification) with deterministic rules (Lee et al., 2011). Interestingly, coreference resolution is one research area, in which deterministic frameworks generally outperform machine learning models (Haghighi and Klein, 2009; Lee et al., 2011). In contrast to coreference resolution, ellipsis resolution remains an understudied NLP problem. One type of ellipsis that received some attention is null instantiation (Fillmore and Baker, 2001), whereby the goal is to recover the referents for an uninstantiated semantic role of a target predicate from the wider discourse context. A semantic evaluation challenge that focused on null instantiation was proposed, although participation was limited (Ruppenhofer et al., 2010). Gerber and Chai (2012) focused on implicit argumentation (i.e., null instantiation) for nominal predic"
W13-1907,D11-1001,0,0.0162022,"008). Support Vector Machine (SVM) classification achieved an F-score in low 55 are also not aware of any work in ellipsis resolution in biomedical NLP. frame as a list of keywords. Event-based representations have also seen increasing use in recent years in biomedical text mining, with the availability of biological event corpora, including GENIA event (Kim et al., 2008) and GREC (Thompson et al., 2009), and shared task challenges (Kim et al., 2012). Most state-of-the-art systems address the event extraction task by adopting machine learning techniques, such as dual composition-based models (Riedel and McCallum, 2011), stacking-based model integration (McClosky et al., 2012), and domain adaptation (Miwa et al., 2012). Good performance has also been reported with some rule-based systems (Kilicoglu and Bergler, 2012). Syntactic dependency parsing has been a key component in all state-of-the-art event extraction systems, as well. The role of coreference resolution in event extraction has recently been acknowledged (Kim et al., 2012), even though efforts in integrating coreference resolution into event extraction pipelines have generally resulted in only modest improvements (Yoshikawa et al., 2011; Miwa et al."
W13-1907,W09-2417,0,0.0531147,"Missing"
W13-1907,W11-1902,0,0.0286905,"ems, as well. The role of coreference resolution in event extraction has recently been acknowledged (Kim et al., 2012), even though efforts in integrating coreference resolution into event extraction pipelines have generally resulted in only modest improvements (Yoshikawa et al., 2011; Miwa et al., 2012; Kilicoglu and Bergler, 2012). Coreference resolution has also been tackled in open domain natural language processing. State-of-the-art systems often employ a combination of lexical, syntactic, shallow semantic and discourse information (e.g., speaker identification) with deterministic rules (Lee et al., 2011). Interestingly, coreference resolution is one research area, in which deterministic frameworks generally outperform machine learning models (Haghighi and Klein, 2009; Lee et al., 2011). In contrast to coreference resolution, ellipsis resolution remains an understudied NLP problem. One type of ellipsis that received some attention is null instantiation (Fillmore and Baker, 2001), whereby the goal is to recover the referents for an uninstantiated semantic role of a target predicate from the wider discourse context. A semantic evaluation challenge that focused on null instantiation was proposed,"
W13-1907,P00-1070,0,0.0594412,"al type classification in both languages were much lower. Liu et al. (2011) found that SVM trained to distinguish questions asked by consumers from those posed by healthcare professionals achieve F-scores in the high 80s - low 90s. One of distinguishing characteristics of the consumer questions in Liu et al.’s study was the significantly higher use of personal pronouns (compared to professional questions). This feature was found to be useful for machine learning; however, the abundance of pronouns in the long dense questions is also a potential source of failure in understanding the question. Vicedo and Ferrández (2000) have shown that pronominal anaphora resolution improves several aspects of the QA systems’ performance. This observation was supported by Harabagiu et al. (2005) who have manually resolved coreference and ellipsis for 14 of the 25 scenarios in the TREC 2005 evaluation. Hickl et al. (2006) have incorporated into their question answering system a heuristic based question coreference module that resolved referring expressions in the question series to antecedents mentioned in previous questions or in the target description. To our knowledge, coreference and ellipsis resolution has not been previ"
W13-1907,de-marneffe-etal-2006-generating,0,\N,Missing
W13-1907,S10-1008,0,\N,Missing
W13-1907,E12-2021,0,\N,Missing
W14-3405,andersen-etal-2012-creation,0,0.137254,"mner-Fushman and Lin, 2007; Cairns et al., 2011; Cao et al., 2011) as a means for retrieving medical information. This work has typically focused, however, on questions posed by medical professionals, and the methods proposed for question analysis generally assume a single, concise question. For example, Demner-Fushman and Abhyankar (2012) propose a method for extracting frames from queries for the purpose of cohort retrieval. Their method assumes syntactic dependencies exist between the necessary frame elements, and is thus not well-suited to handle long, multi-sentence questions. Similarly, Andersen et al. (2012) proposes a method for converting a concise question into a structured query. However, many medical questions require background information that is difficult to encode in a single question sentence. Instead, it is often more natural to ask multiple questions over several sentences, providing background information to give context to the questions. Yu and Cao (2008) use a ML method to recognize question types in professional health questions. Their method can identify more than one type per complex question. Without decomposing the full question into its sub-questions, however, the type cannot"
W14-3405,P07-1086,0,0.0170053,"proposes a decomposition method using only the deep semantic structure. Finally, Harabagiu et al. (2006) proposes a different type of question decomposition based on a random walk over similar questions extracted from a corpus. In our work, we focus on syntactic question decomposition. We demonstrate the importance of empirical evaluation of question decomposition, notably the pitfalls of heuristic approaches that rely entirely on the syntactic parse tree. Syntactic parsers trained on Treebank are particularly poor at both analyzing questions (Judge et al., 2006) and coordination boundaries (Hogan, 2007). Robust question decomposition methods, therefore, must be able to overcome many of these difficulties. when they contain optional or coordinated information embedded within a question. For each of these decomposition annotations, we propose a combination of machine learning (ML) and rule based methods. The ML methods largely take the form of a 3-step rank-and-filter approach, where candidates are generated, ranked by an ML classifier, then the top-ranked candidate is passed through a separate ML filtering classifier. We evaluate each of these methods on a set of 1,467 consumer health questio"
W14-3405,P06-1063,0,0.0273802,"Missing"
W14-3405,W13-1907,1,0.554403,"kidney? Introduction Natural language questions provide an intuitive method for consumers (non-experts) to query for health-related content. The most intuitive way for consumers to formulate written questions is the same way they write to other humans: multisentence, complex questions that contain background information and often more than one specific question. Consider the following: Each question above could be independently answered by a question answering (QA) system. While previous work has discussed methods for resolving co-reference and implicit arguments in consumer health questions (Kilicoglu et al., 2013), it does not address question decomposition. In this work, we propose methods for automatically recognizing six annotation types useful for decomposing consumer health questions. These annotations distinguish between sentences that contain questions and background information. They also identify when a question sentence can be split in multiple independent questions, and • Will Fabry disease affect a transplanted kidney? Previous to the transplant the disease was being managed with an enzyme supplement. Will this need to be continued? What cautions or additional treatments are required to man"
W14-3405,lacatusu-etal-2006-impact,0,0.0280933,"more than one type per complex question. Without decomposing the full question into its sub-questions, however, the type cannot be associated with its specific span, or with other information specific to the sub-question. This other information can include answer types, question focus, and other answer constraints. By decomposing multi-sentence questions, these question-specific attributes can be extracted, and the discourse structure of the larger question can be better understood. Question decomposition has been utilized before in open-domain QA approaches, but rarely evaluated on its own. Lacatusu et al. (2006) 3 Consumer Health Question Decomposition Our goal is to decompose multi-sentence, multifaceted consumer health questions into concise questions coupled with important contextual information. To this end, we utilize a set of annotations that identify the decomposable elements and important contextual elements. A more detailed description of these annotations is provided in Roberts et al. (2014). The annotations are publicly available at our institution website1 . Here, we briefly describe each annotation: (1) BACKGROUND - a sentence indicating useful contextual information, but lacks a questio"
W14-3405,roberts-etal-2014-annotating,1,0.885243,"be extracted, and the discourse structure of the larger question can be better understood. Question decomposition has been utilized before in open-domain QA approaches, but rarely evaluated on its own. Lacatusu et al. (2006) 3 Consumer Health Question Decomposition Our goal is to decompose multi-sentence, multifaceted consumer health questions into concise questions coupled with important contextual information. To this end, we utilize a set of annotations that identify the decomposable elements and important contextual elements. A more detailed description of these annotations is provided in Roberts et al. (2014). The annotations are publicly available at our institution website1 . Here, we briefly describe each annotation: (1) BACKGROUND - a sentence indicating useful contextual information, but lacks a question. (2) Q UESTION - a sentence or clause that indicates an independent question. 1 http://lhncbc.nlm.nih.gov/project/consumer-healthquestion-answering 30 Focus Recognition Request Candidate Generation Sentence Classification SVM Candidate Ranking Sentence Splitting UMLS Boundary Fixing SVM Sentence Classification Question Recognition Question Sentence Background Sentence Ignore Sentence Candidat"
W14-3405,J07-1005,1,\N,Missing
W14-3405,hahn-etal-2012-iterative,0,\N,Missing
W14-3407,de-marneffe-etal-2006-generating,0,0.0964595,"Missing"
W14-3407,J95-2003,0,0.0180928,"Missing"
W14-3407,W13-1907,1,0.930305,"se the metabolism . . . . In this example, the expression these isoenzymes refer to CYP3A and CYP2C8. Resolving this coreference instance would allow us to capture the following drug interactions mentioned in the sentence: inhibitors of CYP3A POTENTIATE amiodarone and inhibitors of CYP2C8 POTENTIATE amiodarone. In this paper, we present a study that focuses on identification of coreference links in drug labels, with the view that these relations will facilitate the downstream task of drug interaction recognition. The rule-based system presented is an extension of the previous work reported in Kilicoglu et al. (2013). The main focus of the dataset, based on SPLs, is drug interaction information. Coreference is only annotated when it is relevant to extracting such information. In addition to evaluating the system against a baseline, we also manually assessed the system output for precision. Furthermore, we also evaluated the system on a similarly drug-focused corpus annotated for anaphora (DrugNerAR) (Segura-Bedmar et al., 2010). Our results demonstrate that set/instance anaphora resolution and appositive recognition can play a significant role in this type of text and highlight some of the major areas of"
W14-3407,M95-1005,0,0.30863,"ts, provided in Table 1, also indicate that the system provides a clear improvement over the baseline. 3.4 Evaluation To evaluate our approach, we used a baseline similar to that reported in Segura-Bedmar et al. (2010), which consists of selecting the closest preceding nominal phrase for the anaphoric expressions annotated in their corpus. These expressions include pronominal (personal, relative, demonstrative, etc.) and nominal (definite, possessive, etc.) anaphora. We compared our system to this baseline using the unweighted average of F1 measure over B-CUBED (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), and CEAF (Luo, 2005) metrics, the standard evaluation metrics for coreference resolution. We used the scripts provided by i2b2 shared task organizers for this purpose. Since coreference annotation was parsimonious in our dataset, we also manually examined a subset of the coreference relations extracted by the system for precision. Additionally, we tested our system on DrugNerAR corpus (Segura-Bedmar et Metric Baseline With gold entity annotations Unweighted F1 Partial 0.55 Unweighted F1 Exact 0.66 Precision 0.01 Recall 0.04 F1 -measure 0.01 End-to-end coreference resolution Precision 0.04 OP"
W14-3407,P88-1014,0,0.171334,"ey are appositive. 3.2.4 Relative pronouns Similar to appositive constructions, relative pronouns are annotated as anaphoric expressions in some corpora (same as those for appositives), but not in our dataset. In the example below, the relative pronoun which refers to potassium-containing salt substitutes. 3.3.1 Demonstrative pronouns Anaphoric expressions of demonstrative pronoun type generally have discourse-deictic use; in other words, they often refer to events, propositions described in prior discourse or even to the full sentences or paragraphs, rather than concrete objects or entities (Webber, 1988). This fact was implicitly exploited in consumer health questions, since the coreference resolution focused on diseases only, which are essentially processes. However, in drug labels, discourse-deictic use of demonstratives is much more overt. Consider the sentence below, where the demonstrative This refers to the event of increasing the exposure to lovastatin. (5) . . . the concomitant use of potassium-sparing diuretics, potassium supplements, and/or potassium-containing salt substitutes, which should be used cautiously. . . Since we aim for generality and this type of anaphora can be importa"
W14-3407,H05-1004,0,0.0248374,"dicate that the system provides a clear improvement over the baseline. 3.4 Evaluation To evaluate our approach, we used a baseline similar to that reported in Segura-Bedmar et al. (2010), which consists of selecting the closest preceding nominal phrase for the anaphoric expressions annotated in their corpus. These expressions include pronominal (personal, relative, demonstrative, etc.) and nominal (definite, possessive, etc.) anaphora. We compared our system to this baseline using the unweighted average of F1 measure over B-CUBED (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), and CEAF (Luo, 2005) metrics, the standard evaluation metrics for coreference resolution. We used the scripts provided by i2b2 shared task organizers for this purpose. Since coreference annotation was parsimonious in our dataset, we also manually examined a subset of the coreference relations extracted by the system for precision. Additionally, we tested our system on DrugNerAR corpus (Segura-Bedmar et Metric Baseline With gold entity annotations Unweighted F1 Partial 0.55 Unweighted F1 Exact 0.66 Precision 0.01 Recall 0.04 F1 -measure 0.01 End-to-end coreference resolution Precision 0.04 OPTIMAL 0.77 0.78 0.17 0"
W14-3407,N10-3001,0,0.0355157,"Missing"
W16-3102,O97-1002,0,0.0820583,"to the non-rejected headings of the MTI candidate set. The intuition behind this approach is that recommending headings that are very similar to each other may be redundant while, at the other end of the distance spectrum, candidate headings that are very different from those recommended by MTI might represent spurious outliers from citations with low PRC similarity scores. The features were implemented using the SML Java library (Harispe et al., 2014). We experimented with several ways of computing pairwise heading similarity and found the combination of Jiang and Conrath semantic distance (Jiang and Conrath, 1997) with the Seco information content measure (Seco et al., 2004) to provide the best results. We denote these features as SML. We implemented additional features based on the Journal Descriptor Indexing (JDI) methodology (Humphrey et al., 2006) maintained by the NLMs Lexical Systems Group7 . Given a block of text, the JDI-based Text Categorization (TC) tool produces a ranked list of about 120 highlevel journal descriptors (e.g. “Anatomy”, “Chemistry”, “Biomedical Engineering” etc) according to their relevance to the text. For example, the TC tool applied to the text “heart valve” produces rankin"
W16-3506,H94-1010,0,0.119742,"ted a version of the SumTime-Meteo corpus which is restricted to wind data. The resulting corpus consists of 2,123 instances for a total of 22,985 words and was used by other researchers working on NLG and semantic parsing (Angeli et al., 2012). Other data-to-text corpora were proposed for training and testing generation systems, including WeatherGov (Liang et al., 2009), the ATIS dataset, the Restaurant Corpus (Wen et al., 2015) and the BAGEL dataset (Mairesse et al., 2010). WeatherGov consists of 29,528 weather scenarios for 3,753 major US cities. In the air travel domain, the ATIS dataset (Dahl et al., 1994) consists of 5,426 scenarios. These are transcriptions of spontaneous utterances of users interacting with a hypothetical online flight-booking system. The RESTAURANTS corpus contains utterances that a spoken dialogue system might produce in an interaction with a human user together with the corresponding dialog act. Similarly, the BAGEL dataset is concerned with restaurant information in a dialog setting. Related Work In all these approaches, datasets are created using heuristics often involving extensive manual labour and/or programming. The data is Many studies tackled the construction of d"
W16-3506,D16-1128,0,0.022582,"ate the data-to-text corpora that are required for learning and testing. Two main such strategies can be identified. One strategy consists in creating a small, domain-specific corpus where data and text are manually aligned by a small group of experts (often the researchers who work on developing the NLG system). Typically, such corpora are domain specific and of relatively small size while their linguistic variability is often restricted. A second strategy consists in automatically building a large data-to-text corpus in which the alignment between data and text is much looser. For instance, Lebret et al. (2016) extracted a corpus consisting of 728,321 biography articles from English Wikipedia and created a data-to-text corpus by simply associating the infobox of each article with its introduction section. The resulting dataset has a vocabulary of 403k words but there is no guarantee that the text actually matches the content of the infobox. In this paper, we explore a middle-ground approach and introduce a new methodology for semi-automatically building large, high quality data-to-text corpora. More precisely, our approach relies on a semantic sentence simplification method which allows transforming"
W16-3506,P09-1011,0,0.0318601,"nt human forecasters, and each instance in the corpus consists of three numerical data files produced by three different weather simulators, and the weather forecast file written by the forecaster. To train a sentence generator, (Belz, 2008) created a version of the SumTime-Meteo corpus which is restricted to wind data. The resulting corpus consists of 2,123 instances for a total of 22,985 words and was used by other researchers working on NLG and semantic parsing (Angeli et al., 2012). Other data-to-text corpora were proposed for training and testing generation systems, including WeatherGov (Liang et al., 2009), the ATIS dataset, the Restaurant Corpus (Wen et al., 2015) and the BAGEL dataset (Mairesse et al., 2010). WeatherGov consists of 29,528 weather scenarios for 3,753 major US cities. In the air travel domain, the ATIS dataset (Dahl et al., 1994) consists of 5,426 scenarios. These are transcriptions of spontaneous utterances of users interacting with a hypothetical online flight-booking system. The RESTAURANTS corpus contains utterances that a spoken dialogue system might produce in an interaction with a human user together with the corresponding dialog act. Similarly, the BAGEL dataset is conc"
W16-3506,P10-1157,0,0.0605466,"Missing"
W16-3506,N12-1049,0,0.0128106,"pus was created by the SumTime project (Sripada et al., 2002). The corpus was collected from the commercial output of five different human forecasters, and each instance in the corpus consists of three numerical data files produced by three different weather simulators, and the weather forecast file written by the forecaster. To train a sentence generator, (Belz, 2008) created a version of the SumTime-Meteo corpus which is restricted to wind data. The resulting corpus consists of 2,123 instances for a total of 22,985 words and was used by other researchers working on NLG and semantic parsing (Angeli et al., 2012). Other data-to-text corpora were proposed for training and testing generation systems, including WeatherGov (Liang et al., 2009), the ATIS dataset, the Restaurant Corpus (Wen et al., 2015) and the BAGEL dataset (Mairesse et al., 2010). WeatherGov consists of 29,528 weather scenarios for 3,753 major US cities. In the air travel domain, the ATIS dataset (Dahl et al., 1994) consists of 5,426 scenarios. These are transcriptions of spontaneous utterances of users interacting with a hypothetical online flight-booking system. The RESTAURANTS corpus contains utterances that a spoken dialogue system m"
W16-3506,W13-2111,1,0.845935,"in 2011 to compare and evaluate sentence generators (Belz et al., 2011). The dataset prepared by the organisers was derived from the PennTreebank and associates sentences with both a shallow representation (dependency trees) and a deep representation where edges are labelled with semantic roles (e.g., agent, patient) and the structure is a graph rather than a tree. While the data-to-text corpus that was made available from this shared task was very large, the representation associated with each sentence is a linguistic representation and is not related to a data schema. The KBGen shared task (Banik et al., 2013) followed a different approach and focused on generating sentences from knowledge bases. For this task, knowledge base fragments were extracted semi-automatically from an existing biology knowledge base (namely, BioKB101 (Chaudhri et al., 2013)) and expert biologists were asked to associate each KB fragments with a sentence verbalising their meaning. The resulting dataset was small (207 data-text instances for training, 70 for testing) and the creation process relied heavily on domain experts, thereby limiting its portability. In sum, there exists so far no standard methodology for rapidly cre"
W16-3506,W11-2832,0,0.0485912,"generation systems on expressing sets of triples in the same sentence; enabling the production of more fluent texts. mostly created artificially from sensor or web data rather than extracted from some existing knowledge base. As the data are often domain specific, the vocabulary size and the linguistic variability of the target text are often restricted. Other approaches tackled the benchmarking of NLG systems and provided the constructed dataset as a publicly available resource. For instance, a Surface Realisation shared task was organised in 2011 to compare and evaluate sentence generators (Belz et al., 2011). The dataset prepared by the organisers was derived from the PennTreebank and associates sentences with both a shallow representation (dependency trees) and a deep representation where edges are labelled with semantic roles (e.g., agent, patient) and the structure is a graph rather than a tree. While the data-to-text corpus that was made available from this shared task was very large, the representation associated with each sentence is a linguistic representation and is not related to a data schema. The KBGen shared task (Banik et al., 2013) followed a different approach and focused on genera"
W16-3506,D15-1199,0,0.0175166,"of three numerical data files produced by three different weather simulators, and the weather forecast file written by the forecaster. To train a sentence generator, (Belz, 2008) created a version of the SumTime-Meteo corpus which is restricted to wind data. The resulting corpus consists of 2,123 instances for a total of 22,985 words and was used by other researchers working on NLG and semantic parsing (Angeli et al., 2012). Other data-to-text corpora were proposed for training and testing generation systems, including WeatherGov (Liang et al., 2009), the ATIS dataset, the Restaurant Corpus (Wen et al., 2015) and the BAGEL dataset (Mairesse et al., 2010). WeatherGov consists of 29,528 weather scenarios for 3,753 major US cities. In the air travel domain, the ATIS dataset (Dahl et al., 1994) consists of 5,426 scenarios. These are transcriptions of spontaneous utterances of users interacting with a hypothetical online flight-booking system. The RESTAURANTS corpus contains utterances that a spoken dialogue system might produce in an interaction with a human user together with the corresponding dialog act. Similarly, the BAGEL dataset is concerned with restaurant information in a dialog setting. Relat"
W19-5039,W19-5048,0,0.373372,"om the MIMICIII dataset. Alsentzer et al. (2019) also released another resource with the same name. These are BERT and BioBERT models further pretrained on the full set of MIMIC-III notes and a subset of discharge summaries. SciBERT (Beltagy et al., 2019) is a set of variants of the original BERT trained with 8 www.aicrowd.com/challenges/mediqa-2019-naturallanguage-inference-nli/leaderboards 373 Table 2: Official teams in MEDIQA 2019 among 72 participating teams on AIcrowd Team ANU-CSIRO (Nguyen et al., 2019) ARS NITK (Agrawal et al., 2019) DoubleTransfer (Xu et al., 2019) Dr.Quad (Bannihatti Kumar et al., 2019) DUT-BIM (Zhou et al., 2019a) DUT-NLP (Zhou et al., 2019b) IITP (Bandyopadhyay et al., 2019) IIT-KGP (Sharma and Roychowdhury, 2019) KU ai (Cengiz et al., 2019) lasigeBioTM (Lamurias and Couto, 2019) MSIT SRIB (Chopra et al., 2019) NCUEE (Lee et al., 2019b) PANLP (Zhu et al., 2019) Pentagon (Pugaliya et al., 2019) Saama Research (Kanakarajan, 2019) Sieg (Bhaskar et al., 2019) Surf (Nam et al., 2019) UU TAILS (Tawfik and Spruit, 2019) UW-BHI (Kearns et al., 2019) WTMED (Wu et al., 2019) Task(s) NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA QA RQE, QA NLI, RQE, QA RQE NLI NLI, RQE, QA NLI"
W19-5039,W19-5052,0,0.0350362,"Missing"
W19-5039,P03-1003,0,0.458568,"Missing"
W19-5039,D17-1070,0,0.0496653,"QA-QA test set consists of 150 consumer health questions and 1,107 associated answers. All the QA training, validation and test sets are publicly available7 . In addition, the MedQuAD dataset of 47K medical question-answer pairs (Ben Abacha and Demner-Fushman, 2019) can be used to retrieve answered questions that are entailed from the original questions. The validation sets of the RQE and QA tasks were used for the first (validation) round on AIcrowd. The test sets were used for the official and final challenge evaluation. 4 4.1 4.2 Baseline Systems • The NLI baseline is the InferSent system (Conneau et al., 2017) based on fasttext (Bojanowski et al., 2017) word embeddings trained on the MIMIC-III data Romanov and Shivade (2018). • The RQE baseline is a feature-based SVM classifier relying on similarity measures and semantic features (Ben Abacha and DemnerFushman, 2016). Evaluation Evaluation Metrics • The QA baseline is the CHiQA questionanswering system (Demner-Fushman et al., 2019). The system was used to provide the answers for the QA task. The evaluation of the NLI and RQE tasks was based on accuracy. In the QA task, participants 7 https://github.com/abachaa/ MEDIQA2019/tree/master/MEDIQA_Task3_QA"
W19-5039,W19-5049,0,0.395982,"e-nli/leaderboards 373 Table 2: Official teams in MEDIQA 2019 among 72 participating teams on AIcrowd Team ANU-CSIRO (Nguyen et al., 2019) ARS NITK (Agrawal et al., 2019) DoubleTransfer (Xu et al., 2019) Dr.Quad (Bannihatti Kumar et al., 2019) DUT-BIM (Zhou et al., 2019a) DUT-NLP (Zhou et al., 2019b) IITP (Bandyopadhyay et al., 2019) IIT-KGP (Sharma and Roychowdhury, 2019) KU ai (Cengiz et al., 2019) lasigeBioTM (Lamurias and Couto, 2019) MSIT SRIB (Chopra et al., 2019) NCUEE (Lee et al., 2019b) PANLP (Zhu et al., 2019) Pentagon (Pugaliya et al., 2019) Saama Research (Kanakarajan, 2019) Sieg (Bhaskar et al., 2019) Surf (Nam et al., 2019) UU TAILS (Tawfik and Spruit, 2019) UW-BHI (Kearns et al., 2019) WTMED (Wu et al., 2019) Task(s) NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA QA RQE, QA NLI, RQE, QA RQE NLI NLI, RQE, QA NLI NLI NLI, RQE, QA NLI, RQE, QA NLI NLI, RQE NLI NLI, RQE NLI NLI Table 3: Official Results of the MEDIQA-NLI Task Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - Team WTMED PANLP DoubleTransfer Sieg Surf ARS NITK Pentagon Dr.Quad UU TAILS KU ai NCUEE IITP MSIT SRIB uw-bhi ANU-CSIRO Saama Research lasigeBioTM NLI-Baseline Accuracy 0.980 0.966 0.938 0.911 0.906 0.877 0.857 0.85"
W19-5039,Q17-1010,0,0.012037,"ealth questions and 1,107 associated answers. All the QA training, validation and test sets are publicly available7 . In addition, the MedQuAD dataset of 47K medical question-answer pairs (Ben Abacha and Demner-Fushman, 2019) can be used to retrieve answered questions that are entailed from the original questions. The validation sets of the RQE and QA tasks were used for the first (validation) round on AIcrowd. The test sets were used for the official and final challenge evaluation. 4 4.1 4.2 Baseline Systems • The NLI baseline is the InferSent system (Conneau et al., 2017) based on fasttext (Bojanowski et al., 2017) word embeddings trained on the MIMIC-III data Romanov and Shivade (2018). • The RQE baseline is a feature-based SVM classifier relying on similarity measures and semantic features (Ben Abacha and DemnerFushman, 2016). Evaluation Evaluation Metrics • The QA baseline is the CHiQA questionanswering system (Demner-Fushman et al., 2019). The system was used to provide the answers for the QA task. The evaluation of the NLI and RQE tasks was based on accuracy. In the QA task, participants 7 https://github.com/abachaa/ MEDIQA2019/tree/master/MEDIQA_Task3_QA 372 Figure 2: Top-10 results of the three t"
W19-5039,N19-1423,0,0.041826,"the original BERT model and then pretrained on biomedical articles from PMC full text articles and PubMed abstracts. BioBERT can be fine-tuned for specific tasks like named entity recognition, relation extraction, and question answering. The data used for pretraining BioBERT is much larger (4.5B words from abstracts and 13.5B words from full text articles) than that used for SciBERT (3.1B words). NLI Approaches & Results Seventeen official teams submitted runs along with a paper describing their approaches among 43 participating teams on NLI@AIcrowd8 . Most systems build up on the BERT model (Devlin et al., 2019). This model is pretrained on a large opendomain corpus. However, since MedNLI is from the clinical domain following variations of BERT were used. ClinicalBERT (Huang et al., 2019) is initialized with the original BERT model and then pretrained on clinical notes from the MIMICIII dataset. Alsentzer et al. (2019) also released another resource with the same name. These are BERT and BioBERT models further pretrained on the full set of MIMIC-III notes and a subset of discharge summaries. SciBERT (Beltagy et al., 2019) is a set of variants of the original BERT trained with 8 www.aicrowd.com/challe"
W19-5039,D15-1075,0,0.223058,"Missing"
W19-5039,P09-1081,0,0.0767823,"Missing"
W19-5039,P06-1114,0,0.385465,"2014). Recently, large-scale datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) were introduced for the task of Natural Language Inference (NLI) targeting three relations between sentences: Entailment, Neutral, and Contradiction. Few efforts have studied the benefits of RTE and NLI in other NLP tasks such as text exploration (Adler et al., 2012), identifying evidence for eligibility criteria satisfaction in clinical trials (Shivade et al., 2015), and the summarization of PMC articles (Chachra et al., 2016). NLI can also be beneficial for Question Answering (QA). Harabagiu and Hickl (2006) presented entailment-based methods to filter and rank answers and showed that RTE can enhance the 1 https://sites.google.com/view/ mediqa2019 2 https://www.aicrowd.com/organizers/ mediqa-acl-bionlp 370 Proceedings of the BioNLP 2019 workshop, pages 370–379 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 3 Data Description 3.1 NLI Datasets The MEDIQA-NLI test set consists of 405 texthypothesis pairs. The training set is the MedNLI dataset, which includes 14,049 clinical sentence pairs derived from the MIMIC-III database (Romanov and Shivade, 2018). Both datase"
W19-5039,W19-5045,0,0.0272668,"Missing"
W19-5039,W19-5055,0,0.0511475,"Missing"
W19-5039,C16-1104,1,0.898807,"Missing"
W19-5039,W19-5054,0,0.068683,"Missing"
W19-5039,W19-5057,0,0.102829,"subset of discharge summaries. SciBERT (Beltagy et al., 2019) is a set of variants of the original BERT trained with 8 www.aicrowd.com/challenges/mediqa-2019-naturallanguage-inference-nli/leaderboards 373 Table 2: Official teams in MEDIQA 2019 among 72 participating teams on AIcrowd Team ANU-CSIRO (Nguyen et al., 2019) ARS NITK (Agrawal et al., 2019) DoubleTransfer (Xu et al., 2019) Dr.Quad (Bannihatti Kumar et al., 2019) DUT-BIM (Zhou et al., 2019a) DUT-NLP (Zhou et al., 2019b) IITP (Bandyopadhyay et al., 2019) IIT-KGP (Sharma and Roychowdhury, 2019) KU ai (Cengiz et al., 2019) lasigeBioTM (Lamurias and Couto, 2019) MSIT SRIB (Chopra et al., 2019) NCUEE (Lee et al., 2019b) PANLP (Zhu et al., 2019) Pentagon (Pugaliya et al., 2019) Saama Research (Kanakarajan, 2019) Sieg (Bhaskar et al., 2019) Surf (Nam et al., 2019) UU TAILS (Tawfik and Spruit, 2019) UW-BHI (Kearns et al., 2019) WTMED (Wu et al., 2019) Task(s) NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA QA RQE, QA NLI, RQE, QA RQE NLI NLI, RQE, QA NLI NLI NLI, RQE, QA NLI, RQE, QA NLI NLI, RQE NLI NLI, RQE NLI NLI Table 3: Official Results of the MEDIQA-NLI Task Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - Team WTMED PANLP DoubleTransfer Sieg"
W19-5039,W19-5053,0,0.0477021,"CSIRO Dr.Quad ARS NITK Provided Answers Accuracy 0.780 0.777 0.765 0.745 0.745 0.717 0.637 0.584 0.565 0.536 0.517 Precision 0.8191 0.7806 0.7766 0.7466 0.7466 0.7936 0.5975 0.5568 0.6679 0.5596 0.5167 Another common model used by participating systems was the Multi-Task Deep Neural Network MT-DNN (Liu et al., 2019) which builds up on BERT to perform multi-task learning and is evaluated on the GLUE benchmark (Wang et al., 2018). A common theme across all the papers was training of multiple models and then using an ensemble as the final system which performed better than the individual models. Tawfik and Spruit (2019) trained 30 different models as candidates to the ensemble and experimented with various aggregation techniques. Some teams also leveraged dataset-specific properties to enhance the performance. The WTMED team (Wu et al., 2019) modeled parameters specific to the index of the texthypothesis pair in the dataset which shows a significant boost in performance. 5.2 MRR 0.9367 0.9378 0.9622 0.9061 0.9061 0.8611 0.91 0.7843 0.6069 0.6293 0.895 Spearman’s rho 0.238 0.180 0.338 0.106 0.106 0.024 0.211 0.122 0.009 0.196 0.315 several of the proposed deep networks reached relevant generalizations and abs"
W19-5039,W18-5446,0,0.0811402,"Missing"
W19-5039,W19-5058,0,0.0715163,"icial Results full text scientific articles, primarily from PubMed. Variants of the model either use the vocabulary of the original BERT model or a new vocabulary learnt specifically for this corpus. Seventy two teams participated in the challenge on the AIcrowd platform. Figure 2 presents the original top-10 scores for each task. The official scores include only the teams who sent a working notes paper describing their approach. The accepted teams are presented in table 2. The official scores for the MEDIQA NLI, RQE, and QA tasks are presented respectively in tables 3, 4, and 5. 5.1 BioBERT (Lee et al., 2019a) is initialized with the original BERT model and then pretrained on biomedical articles from PMC full text articles and PubMed abstracts. BioBERT can be fine-tuned for specific tasks like named entity recognition, relation extraction, and question answering. The data used for pretraining BioBERT is much larger (4.5B words from abstracts and 13.5B words from full text articles) than that used for SciBERT (3.1B words). NLI Approaches & Results Seventeen official teams submitted runs along with a paper describing their approaches among 43 participating teams on NLI@AIcrowd8 . Most systems build"
W19-5039,N18-1101,0,0.111349,"Missing"
W19-5039,P19-1441,0,0.064028,"ransfer DUT-NLP UU TAILS IITP ANU-CSIRO lasigeBioTM RQE-Baseline Accuracy 0.749 0.706 0.684 0.671 0.667 0.667 0.662 0.636 0.584 0.532 0.489 0.485 0.541 Table 5: Official Results of the MEDIQA-QA Task Rank 1 2 3 4 4 6 7 8 9 10 - Team DoubleTransfer PANLP Pentagon DUT-BIM DUT-NLP IITP lasigeBioTM ANU-CSIRO Dr.Quad ARS NITK Provided Answers Accuracy 0.780 0.777 0.765 0.745 0.745 0.717 0.637 0.584 0.565 0.536 0.517 Precision 0.8191 0.7806 0.7766 0.7466 0.7466 0.7936 0.5975 0.5568 0.6679 0.5596 0.5167 Another common model used by participating systems was the Multi-Task Deep Neural Network MT-DNN (Liu et al., 2019) which builds up on BERT to perform multi-task learning and is evaluated on the GLUE benchmark (Wang et al., 2018). A common theme across all the papers was training of multiple models and then using an ensemble as the final system which performed better than the individual models. Tawfik and Spruit (2019) trained 30 different models as candidates to the ensemble and experimented with various aggregation techniques. Some teams also leveraged dataset-specific properties to enhance the performance. The WTMED team (Wu et al., 2019) modeled parameters specific to the index of the texthypothesis pa"
W19-5039,W19-5044,0,0.190095,"(Nguyen et al., 2019) ARS NITK (Agrawal et al., 2019) DoubleTransfer (Xu et al., 2019) Dr.Quad (Bannihatti Kumar et al., 2019) DUT-BIM (Zhou et al., 2019a) DUT-NLP (Zhou et al., 2019b) IITP (Bandyopadhyay et al., 2019) IIT-KGP (Sharma and Roychowdhury, 2019) KU ai (Cengiz et al., 2019) lasigeBioTM (Lamurias and Couto, 2019) MSIT SRIB (Chopra et al., 2019) NCUEE (Lee et al., 2019b) PANLP (Zhu et al., 2019) Pentagon (Pugaliya et al., 2019) Saama Research (Kanakarajan, 2019) Sieg (Bhaskar et al., 2019) Surf (Nam et al., 2019) UU TAILS (Tawfik and Spruit, 2019) UW-BHI (Kearns et al., 2019) WTMED (Wu et al., 2019) Task(s) NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA QA RQE, QA NLI, RQE, QA RQE NLI NLI, RQE, QA NLI NLI NLI, RQE, QA NLI, RQE, QA NLI NLI, RQE NLI NLI, RQE NLI NLI Table 3: Official Results of the MEDIQA-NLI Task Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - Team WTMED PANLP DoubleTransfer Sieg Surf ARS NITK Pentagon Dr.Quad UU TAILS KU ai NCUEE IITP MSIT SRIB uw-bhi ANU-CSIRO Saama Research lasigeBioTM NLI-Baseline Accuracy 0.980 0.966 0.938 0.911 0.906 0.877 0.857 0.855 0.852 0.847 0.840 0.818 0.813 0.813 0.800 0.783 0.724 0.714 Table 4: Official Results of the MEDIQA-RQE Task R"
W19-5039,W19-5043,0,0.199416,"eeded to validate the answers. C¸elikyilmaz et al. (2009) presented a graph-based semi-supervised method for QA exploiting entailment relations between questions and candidate answers and demonstrated that the use of unlabeled entailment data can improve answer ranking. Ben Abacha and DemnerFushman (2016) noted that the requirements of question entailment in QA are different from general question similarity, and introduced the task of Recognizing Question Entailment (RQE) in order to answer new questions by retrieving entailed questions with pre-existing answers. Ben Abacha and Demner-Fushman (2019) proposed a novel QA approach based on RQE, with the introduction of the MedQuAD medical question-answer collection, and showed empirical evidence supporting question entailment for QA. Although the idea of using entailment in QA has been introduced, research investigating methods to incorporate textual inference and question entailment into QA systems is still limited in the literature. Moreover, despite a few recent efforts to design RTE methods and datasets from MEDLINE abstracts (Ben Abacha et al., 2015) and to create the MedNLI dataset from clinical data (Romanov and Shivade, 2018), the e"
W19-5039,W19-5042,0,0.308126,"then pretrained on clinical notes from the MIMICIII dataset. Alsentzer et al. (2019) also released another resource with the same name. These are BERT and BioBERT models further pretrained on the full set of MIMIC-III notes and a subset of discharge summaries. SciBERT (Beltagy et al., 2019) is a set of variants of the original BERT trained with 8 www.aicrowd.com/challenges/mediqa-2019-naturallanguage-inference-nli/leaderboards 373 Table 2: Official teams in MEDIQA 2019 among 72 participating teams on AIcrowd Team ANU-CSIRO (Nguyen et al., 2019) ARS NITK (Agrawal et al., 2019) DoubleTransfer (Xu et al., 2019) Dr.Quad (Bannihatti Kumar et al., 2019) DUT-BIM (Zhou et al., 2019a) DUT-NLP (Zhou et al., 2019b) IITP (Bandyopadhyay et al., 2019) IIT-KGP (Sharma and Roychowdhury, 2019) KU ai (Cengiz et al., 2019) lasigeBioTM (Lamurias and Couto, 2019) MSIT SRIB (Chopra et al., 2019) NCUEE (Lee et al., 2019b) PANLP (Zhu et al., 2019) Pentagon (Pugaliya et al., 2019) Saama Research (Kanakarajan, 2019) Sieg (Bhaskar et al., 2019) Surf (Nam et al., 2019) UU TAILS (Tawfik and Spruit, 2019) UW-BHI (Kearns et al., 2019) WTMED (Wu et al., 2019) Task(s) NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA QA RQE, Q"
W19-5039,W19-5051,0,0.282327,"(Huang et al., 2019) is initialized with the original BERT model and then pretrained on clinical notes from the MIMICIII dataset. Alsentzer et al. (2019) also released another resource with the same name. These are BERT and BioBERT models further pretrained on the full set of MIMIC-III notes and a subset of discharge summaries. SciBERT (Beltagy et al., 2019) is a set of variants of the original BERT trained with 8 www.aicrowd.com/challenges/mediqa-2019-naturallanguage-inference-nli/leaderboards 373 Table 2: Official teams in MEDIQA 2019 among 72 participating teams on AIcrowd Team ANU-CSIRO (Nguyen et al., 2019) ARS NITK (Agrawal et al., 2019) DoubleTransfer (Xu et al., 2019) Dr.Quad (Bannihatti Kumar et al., 2019) DUT-BIM (Zhou et al., 2019a) DUT-NLP (Zhou et al., 2019b) IITP (Bandyopadhyay et al., 2019) IIT-KGP (Sharma and Roychowdhury, 2019) KU ai (Cengiz et al., 2019) lasigeBioTM (Lamurias and Couto, 2019) MSIT SRIB (Chopra et al., 2019) NCUEE (Lee et al., 2019b) PANLP (Zhu et al., 2019) Pentagon (Pugaliya et al., 2019) Saama Research (Kanakarajan, 2019) Sieg (Bhaskar et al., 2019) Surf (Nam et al., 2019) UU TAILS (Tawfik and Spruit, 2019) UW-BHI (Kearns et al., 2019) WTMED (Wu et al., 2019) Task"
W19-5039,W19-5047,0,0.0611874,"Missing"
W19-5039,W19-5041,0,0.35252,"www.aicrowd.com/challenges/mediqa-2019-naturallanguage-inference-nli/leaderboards 373 Table 2: Official teams in MEDIQA 2019 among 72 participating teams on AIcrowd Team ANU-CSIRO (Nguyen et al., 2019) ARS NITK (Agrawal et al., 2019) DoubleTransfer (Xu et al., 2019) Dr.Quad (Bannihatti Kumar et al., 2019) DUT-BIM (Zhou et al., 2019a) DUT-NLP (Zhou et al., 2019b) IITP (Bandyopadhyay et al., 2019) IIT-KGP (Sharma and Roychowdhury, 2019) KU ai (Cengiz et al., 2019) lasigeBioTM (Lamurias and Couto, 2019) MSIT SRIB (Chopra et al., 2019) NCUEE (Lee et al., 2019b) PANLP (Zhu et al., 2019) Pentagon (Pugaliya et al., 2019) Saama Research (Kanakarajan, 2019) Sieg (Bhaskar et al., 2019) Surf (Nam et al., 2019) UU TAILS (Tawfik and Spruit, 2019) UW-BHI (Kearns et al., 2019) WTMED (Wu et al., 2019) Task(s) NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA NLI, RQE, QA QA RQE, QA NLI, RQE, QA RQE NLI NLI, RQE, QA NLI NLI NLI, RQE, QA NLI, RQE, QA NLI NLI, RQE NLI NLI, RQE NLI NLI Table 3: Official Results of the MEDIQA-NLI Task Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - Team WTMED PANLP DoubleTransfer Sieg Surf ARS NITK Pentagon Dr.Quad UU TAILS KU ai NCUEE IITP MSIT SRIB uw-bhi ANU-CSIRO Saama Research lasigeBioTM NLI-B"
W19-5039,W19-5046,0,0.106721,"Missing"
W19-5039,D18-1187,1,0.706199,"are publicly available7 . In addition, the MedQuAD dataset of 47K medical question-answer pairs (Ben Abacha and Demner-Fushman, 2019) can be used to retrieve answered questions that are entailed from the original questions. The validation sets of the RQE and QA tasks were used for the first (validation) round on AIcrowd. The test sets were used for the official and final challenge evaluation. 4 4.1 4.2 Baseline Systems • The NLI baseline is the InferSent system (Conneau et al., 2017) based on fasttext (Bojanowski et al., 2017) word embeddings trained on the MIMIC-III data Romanov and Shivade (2018). • The RQE baseline is a feature-based SVM classifier relying on similarity measures and semantic features (Ben Abacha and DemnerFushman, 2016). Evaluation Evaluation Metrics • The QA baseline is the CHiQA questionanswering system (Demner-Fushman et al., 2019). The system was used to provide the answers for the QA task. The evaluation of the NLI and RQE tasks was based on accuracy. In the QA task, participants 7 https://github.com/abachaa/ MEDIQA2019/tree/master/MEDIQA_Task3_QA 372 Figure 2: Top-10 results of the three tasks in MEDIQA 2019 among 72 participating teams on AIcrowd 5 Official Re"
W19-5039,W19-5050,0,0.263635,"eeded to validate the answers. C¸elikyilmaz et al. (2009) presented a graph-based semi-supervised method for QA exploiting entailment relations between questions and candidate answers and demonstrated that the use of unlabeled entailment data can improve answer ranking. Ben Abacha and DemnerFushman (2016) noted that the requirements of question entailment in QA are different from general question similarity, and introduced the task of Recognizing Question Entailment (RQE) in order to answer new questions by retrieving entailed questions with pre-existing answers. Ben Abacha and Demner-Fushman (2019) proposed a novel QA approach based on RQE, with the introduction of the MedQuAD medical question-answer collection, and showed empirical evidence supporting question entailment for QA. Although the idea of using entailment in QA has been introduced, research investigating methods to incorporate textual inference and question entailment into QA systems is still limited in the literature. Moreover, despite a few recent efforts to design RTE methods and datasets from MEDLINE abstracts (Ben Abacha et al., 2015) and to create the MedNLI dataset from clinical data (Romanov and Shivade, 2018), the e"
