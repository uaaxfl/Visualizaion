2020.acl-main.552,P18-3015,0,0.0185503,"ix benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can a"
2020.acl-main.552,D19-5402,0,0.382219,"ly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M ATCH S UM, Figure 1) a"
2020.acl-main.552,P18-1063,0,0.17681,"sed our codes, processed dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially"
2020.acl-main.552,P16-1046,0,0.147785,"ave driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivati"
2020.acl-main.552,N18-2097,0,0.10261,"ain more convicing explanations, we perform experiments on six divergent mainstream datasets as follows. Reddit XSum CNN/DM Wiki PubMed M-News Ext Sel Size 5 1, 2 15 5 1, 2 15 5 2, 3 20 5 3, 4, 5 16 7 6 7 10 9 9 Table 2: Details about the candidate summary for different datasets. Ext denotes the number of sentences after we prune the original document, Sel denotes the number of sentences to form a candidate summary and Size is the number of final candidate summaries. CNN/DailyMail (Hermann et al., 2015) is a commonly used news summarization dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media p"
2020.acl-main.552,N19-1423,0,0.0177712,"h as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text embeddings that can be compared using cosine-similarity. A good summary has the highest similarity among a set of candidate summaries. We evaluate the proposed matching framework and perform significance testing on a range of benchmark datasets. Our model outperforms strong baselines significantly in all cases and improve the state-of-the-art extractive result on CNN/DailyMail. Besides, we design experiments to observe the gains brought by our framew"
2020.acl-main.552,D18-1409,0,0.320551,"Missing"
2020.acl-main.552,P19-1102,0,0.0868974,"te summary and Size is the number of final candidate summaries. CNN/DailyMail (Hermann et al., 2015) is a commonly used news summarization dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media platform. We use the TIFU-long version of Reddit. 5.2 Implementation Details We use the base version of BERT to implement our models in all experiments. Adam optimizer (Kingma and Ba, 2014) with warming-up is used and our learning rate schedule follows Vaswani et al. (2017) as: lr = 2e−3 · min(step−0.5 , step · wm−1.5 ), (11) where each step is a batch size of 32 and wm denotes warmup steps of 10,000. We ch"
2020.acl-main.552,N10-1131,0,0.17166,"orts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for conten"
2020.acl-main.552,W09-1802,0,0.140904,"Missing"
2020.acl-main.552,P18-1014,0,0.0825034,"d dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extracto"
2020.acl-main.552,N19-1260,0,0.0298981,"zation dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media platform. We use the TIFU-long version of Reddit. 5.2 Implementation Details We use the base version of BERT to implement our models in all experiments. Adam optimizer (Kingma and Ba, 2014) with warming-up is used and our learning rate schedule follows Vaswani et al. (2017) as: lr = 2e−3 · min(step−0.5 , step · wm−1.5 ), (11) where each step is a batch size of 32 and wm denotes warmup steps of 10,000. We choose γ1 = 0 and γ2 = 0.01. When γ1 &lt;0.05 and 0.005&lt;γ2 &lt;0.05 they have little effect on performance, otherwise they will cause per"
2020.acl-main.552,P19-1209,0,0.0413831,"nd Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) wher"
2020.acl-main.552,N03-1020,0,0.495644,"Missing"
2020.acl-main.552,D19-1387,0,0.477529,"s, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10"
2020.acl-main.552,2021.ccl-1.108,0,0.201929,"Missing"
2020.acl-main.552,N19-1397,0,0.0500472,"(Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) where gsum (C) considers sentences in C as a whol"
2020.acl-main.552,D16-1031,0,0.0217077,"ication of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. ("
2020.acl-main.552,K16-1028,0,0.105737,"Missing"
2020.acl-main.552,D18-1206,0,0.460092,"ed sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M A"
2020.acl-main.552,N18-1158,0,0.356783,"ed sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M A"
2020.acl-main.552,D19-1410,0,0.023607,"Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text embeddings that can be compared using cosine-similarity. A good summary has the highest similarity among a set of candidate summaries. We evaluate the proposed matching framework and perform significance testing on a range of benchmark datasets. Our model outperforms strong baselines significantly in all cases and improve the state-of-the-art extractive result on CNN/DailyMail. Besides, we design experiments to observe the gains brought by our framework. We summarize our contributions as follows: 1) Instead of scoring and extracting sentences one by one"
2020.acl-main.552,2020.acl-main.553,1,0.763028,"ework has achieved superior performance compared with strong baselines on six benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, i"
2020.acl-main.552,N16-1170,0,0.0128706,"a novel summary-level framework (M ATCH S UM, Figure 1) and conceptualize extractive summarization as a semantic text matching problem. The principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualified summaries. Semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment, which has been applied in many fields, such as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text"
2020.acl-main.552,D19-1324,0,0.0938029,"ximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) where gsum (C) considers se"
2020.acl-main.552,P13-1171,0,0.0389533,"us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M ATCH S UM, Figure 1) and conceptualize extractive summarization as a semantic text matching problem. The principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualified summaries. Semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment, which has been applied in many fields, such as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and"
2020.acl-main.552,D15-1228,0,0.0168694,"g and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) fo"
2020.acl-main.552,K19-1074,0,0.12965,"essive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our mod"
2020.acl-main.552,P19-1499,0,0.256957,"essive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our mod"
2020.acl-main.552,P19-1100,1,0.9257,"and summary-level methods. 3) Our proposed framework has achieved superior performance compared with strong baselines on six benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive"
2020.acl-main.552,D19-5410,1,0.904806,"Missing"
2020.acl-main.552,P18-1061,0,0.591782,"nerated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than cons"
2020.acl-main.553,P16-1046,0,0.589481,"l extension from a singledocument setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et"
2020.acl-main.553,D18-1409,0,0.600455,"Missing"
2020.acl-main.553,P16-1188,0,0.0356539,"1 Datasets CNN/DailyMail The CNN/DailyMail question answering dataset (Hermann et al., 2015; Nallapati et al., 2016) is the most widely used benchmark dataset for single-document summarization. The standard dataset split contains 287,227/13,368/11,490 examples for training, validation, and test. For the data prepossessing, we follow Liu and Lapata (2019b), which use the nonanonymized version as See et al. (2017), to get ground-truth labels. NYT50 NYT50 is also a single-document summarization dataset, which was collected from New York Times Annotated Corpus (Sandhaus, 2008) and preprocessed by Durrett et al. (2016). It contains 110,540 articles with summaries and is split into 100,834 and 9706 for training and test. Following Durrett et al. (2016), we use the last 4,000 examples from the training set as validation and filter test examples to 3,452. Multi-News The Multi-News dataset is a largescale multi-document summarization introduced by Fabbri et al. (2019). It contains 56,216 articlessummary pairs and each example consists of 2-10 source documents and a human-written summary. Following their experimental settings, we split the dataset into 44,972/5,622/5,622 for training, validation and test example"
2020.acl-main.553,P19-1102,0,0.41741,"ile d2 contains s21 and s22 . As a relay node, the relation of document-document, sentence-sentence, and sentencedocument can be built through the common word nodes. For example, sentence s11 , s12 and s21 share the same word w1 , which connects them across documents. 3.5 Multi-document Summarization For multi-document summarization, the documentlevel relation is crucial for better understanding the core topic and most important content of this cluster. However, most existing neural models ignore this hierarchical structure and concatenate documents to a single flat sequence(Liu et al., 2018; Fabbri et al., 2019). Others try to model this relation by attention-based full-connected graph or take advantage of similarity or discourse relations(Liu and Lapata, 2019a). Our framework can establish the document-level relationship in the same way as the sentence-level by just adding supernodes for documents(as Figure 3), which means it can be easily adapted from single-document to multi-document summarization. The heterogeneous graph is then extended to three types of nodes: V = Vw ∪ Vs ∪ Vd and Vd = {d1 , · · · , dl } and l is the number of source documents. We name it as H ETER D OC SUMG RAPH. As we can see"
2020.acl-main.553,D18-1443,0,0.112597,"Missing"
2020.acl-main.553,D18-1446,0,0.0861031,"Missing"
2020.acl-main.553,N03-1020,0,0.30361,"Missing"
2020.acl-main.553,D19-1488,0,0.0417672,"nd their associated learning methods (i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally designed for the homogeneous graph where the whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al., 2016), namely the heterogeneous graph. To model these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019) focused on semi-supervised short text classification and constructed a topic-entity heterogeneous neural graph. For summarization, Wei (2012) proposes a heterogeneous graph consisting of topic, word and sentence nodes and uses the markov chain model for the iterative update. Wang et al. (2019b) modify TextRank for their graph with keywords and sentences and thus put forward HeteroRank. Inspired by the success of the heterogeneous graph-based neural network on other NLP tasks, we introduce it to extractive text summarization to learn a better node representation. 3 Methodology Given a document"
2020.acl-main.553,P19-1500,0,0.0706958,"s. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture s"
2020.acl-main.553,D19-1387,0,0.0591542,"s. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture s"
2020.acl-main.553,N19-1173,0,0.0488689,"Missing"
2020.acl-main.553,D19-1300,0,0.145067,"ed as sentence-word-sentence relationships. H ETER SUMG RAPH directly selects sentences for the summary by node classification, while H ETER SUMG RAPH with trigram blocking further utilizes the n-gram blocking to reduce redundancy. 3 The detailed experimental results are attached in the Appendix Section. R-2 R-L L EAD -3 (See et al., 2017) O RACLE (Liu and Lapata, 2019b) 40.34 17.70 36.57 52.59 31.24 48.87 REFRESH (Narayan et al., 2018) LATENT (Zhang et al., 2018) BanditSum (Dong et al., 2018) NeuSUM (Zhou et al., 2018) JECS (Xu and Durrett, 2019) LSTM+PN (Zhong et al., 2019a) HER w/o Policy (Luo et al., 2019) HER w Policy (Luo et al., 2019) 40.00 41.05 41.50 41.59 41.70 41.85 41.70 42.30 18.20 18.77 18.70 19.01 18.50 18.93 18.30 18.90 36.60 37.54 37.60 37.98 37.90 38.13 37.10 37.60 Ext-BiLSTM Ext-Transformer HSG HSG + Tri-Blocking 41.59 41.33 42.31 42.95 19.03 18.83 19.51 19.76 38.04 37.65 38.74 39.23 Table 1: Performance (Rouge) of our proposed models against recently released summarization systems on CNN/DailyMail. 5 5.1 4.3 R-1 Results and Analysis Single-document Summarization We evaluate our single-document model on CNN/DailyMail and NYT50 and report the unigram, bigram and longest common sub"
2020.acl-main.553,W04-3252,0,0.84773,"s) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem. A more straightforward way is to create a sentence-level fully-connected graph. To some extent, the Transformer encoder (Vaswani et al., 2017) used in recent work(Zhong et al., 2019a; Liu and Lapata, 2019b) can b"
2020.acl-main.553,N18-1158,0,0.504042,"edocument setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al."
2020.acl-main.553,D14-1162,0,0.0827789,"Missing"
2020.acl-main.553,P17-1099,0,0.0994329,"he same update process as sentence nodes. Experiment We evaluate our models both on single- and multidocument summarization tasks. Below, we start our experiment with the description of the datasets. 4.1 Datasets CNN/DailyMail The CNN/DailyMail question answering dataset (Hermann et al., 2015; Nallapati et al., 2016) is the most widely used benchmark dataset for single-document summarization. The standard dataset split contains 287,227/13,368/11,490 examples for training, validation, and test. For the data prepossessing, we follow Liu and Lapata (2019b), which use the nonanonymized version as See et al. (2017), to get ground-truth labels. NYT50 NYT50 is also a single-document summarization dataset, which was collected from New York Times Annotated Corpus (Sandhaus, 2008) and preprocessed by Durrett et al. (2016). It contains 110,540 articles with summaries and is split into 100,834 and 9706 for training and test. Following Durrett et al. (2016), we use the last 4,000 examples from the training set as validation and filter test examples to 3,452. Multi-News The Multi-News dataset is a largescale multi-document summarization introduced by Fabbri et al. (2019). It contains 56,216 articlessummary pairs"
2020.acl-main.553,P19-1260,0,0.03723,"ogically, these works only use one type of nodes, which formulate each document as a homogeneous graph. Heterogeneous Graph for NLP Graph neural networks and their associated learning methods (i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally designed for the homogeneous graph where the whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al., 2016), namely the heterogeneous graph. To model these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019) focused on semi-supervised short text classification and constructed a topic-entity heterogeneous neural graph. For summarization, Wei (2012) proposes a heterogeneous graph consisting of topic, word and sentence nodes and uses the markov chain model for the iterative update. Wang et al. (2019b) modify TextRank for their graph with keywords and sentences and thus put forward HeteroRank. Inspired by the success of the heterogeneous graph-"
2020.acl-main.553,D19-5410,1,0.583487,"oducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are us"
2020.acl-main.553,P18-1061,0,0.592842,"t al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse"
2020.acl-main.553,D19-1324,0,0.321381,"Missing"
2020.acl-main.553,K17-1045,0,0.134843,"uments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem. A more straightforward way is to create a sentence-level fully-connected graph. To some extent, the Transformer encoder (Vaswani et al., 2017) used in recent work(Zhong et al., 2019a; Liu and Lapata, 2019b) can be classified into this type, which learns the pairwise interaction between sentences. Despite their success, how to construct an effective graph structure for summarization remains an open question. In this paper, we pro"
2020.acl-main.553,D18-1088,0,0.180792,"Missing"
2020.acl-main.553,2020.acl-main.552,1,0.763251,"use recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) or Transformer encoders 2 Since our proposed model is orthogonal to the methods that using pre-trained models, we believe our model can be further boosted by taking the pre-trained models to initialize the node representations, which we reserve for the future. (Zhong et al., 2019b; Wang et al., 2019a) for the sentential encoding. Recently, pre-trained language models are also applied in summarization for contextual word representations (Zhong et al., 2019a; Liu and Lapata, 2019b; Xu et al., 2019; Zhong et al., 2020). Another intuitive structure for extractive summarization is the graph, which can better utilize the statistical or linguistic information between sentences. Early works focus on document graphs constructed with the content similarity among sentences, like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Some recent works aim to incorporate a relational priori into the encoder by graph neural networks (GNNs) (Yasunaga et al., 2017; Xu et al., 2019). Methodologically, these works only use one type of nodes, which formulate each document as a homogeneous graph. Heterogen"
2020.acl-main.553,P19-1100,1,0.6356,"oducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are us"
2020.acl-main.611,P15-1017,0,0.068138,"enhe Pharmacy E-LOC 店 Shop 药店 Pharmacy (b) Lattice LSTM. B-LOC E-LOC B-LOC I-LOC I-LOC E-LOC Transformer Encoder 重 Chong 庆 Qing 人 People 和 And 药 Drug 店 Shop 重庆 Chongqing 人和药店 药店 Pharmacy 1 2 3 4 5 6 1 3 5 1 2 3 4 5 6 2 6 6 Renhe Pharmacy (c) Flat-Lattice Transformer. Figure 1: While lattice LSTM indicates lattice structure by dynamically adjusting its structure, FLAT only needs to leverage the span position encoding. In 1(c), , , denotes tokens, heads and tails, respectively. Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks (Chen et al., 2015; Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016; Yang et al., 2017; Liu et al., 2017; Sun et al., 2020), Chinese NER is more difficult since it usually involves word segmentation. Recently, the lattice structure has been proved to have a great benefit to utilize the word information and avoid the error propagation of word segmentation (Zhang and Yang, 2018). We can match a sentence with a lexicon to obtain the latent words in it, and then we get a lattice like in Figure 1(a). The lattice is a directed acyclic graph, where each node is a character or a latent word. Th"
2020.acl-main.611,P19-1285,0,0.0754278,"Missing"
2020.acl-main.611,D19-1096,1,0.793881,"Missing"
2020.acl-main.611,N16-1030,0,0.14428,"OC I-LOC I-LOC E-LOC Transformer Encoder 重 Chong 庆 Qing 人 People 和 And 药 Drug 店 Shop 重庆 Chongqing 人和药店 药店 Pharmacy 1 2 3 4 5 6 1 3 5 1 2 3 4 5 6 2 6 6 Renhe Pharmacy (c) Flat-Lattice Transformer. Figure 1: While lattice LSTM indicates lattice structure by dynamically adjusting its structure, FLAT only needs to leverage the span position encoding. In 1(c), , , denotes tokens, heads and tails, respectively. Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks (Chen et al., 2015; Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016; Yang et al., 2017; Liu et al., 2017; Sun et al., 2020), Chinese NER is more difficult since it usually involves word segmentation. Recently, the lattice structure has been proved to have a great benefit to utilize the word information and avoid the error propagation of word segmentation (Zhang and Yang, 2018). We can match a sentence with a lexicon to obtain the latent words in it, and then we get a lattice like in Figure 1(a). The lattice is a directed acyclic graph, where each node is a character or a latent word. The lattice includes a sequence of characters and potential Corresponding au"
2020.acl-main.611,W06-0115,0,0.789949,"Missing"
2020.acl-main.611,P18-2023,0,0.121094,"71.81 72.82 91.87 93.01 94.41 95.25 56.75 58.39 Lattice LSTM CNNR LGN PLT FLAT FLATmsm FLATmld YJ YJ YJ YJ YJ YJ YJ 73.88 74.45 74.85 74.60 76.45 73.39 75.35 93.18 93.71 93.63 93.26 94.12 93.11 93.83 94.46 95.11 95.41 95.40 95.45 95.03 95.28 58.79 59.92 60.15 59.92 60.32 57.98 59.63 CGN FLAT LS LS 74.79 75.70 93.47 94.35 94.12∗ 63.09 94.93 63.42 Table 2: Four datasets results (F1). BiLSTM results are from Zhang and Yang (2018). PLT denotes the porous lattice Transformer (Mengge et al., 2019). ‘YJ’ denotes the lexicon released by Zhang and Yang (2018), and ‘LS’ denotes the lexicon released by Li et al. (2018). The result of other models are from their original paper. Except that the superscript * means the result is not provided in the original paper, and we get the result by running the public source code. Subscripts ‘msm’ and ‘mld’ denote FLAT with the mask of self-matched words and long distance (&gt;10), respectively. denotes the distance between head of (ht) (th) Rij = ReLU(Wr (pd(hh) ⊕ pd(th) ⊕ pd(ht) ⊕ pd(tt) )), (8) ij ij ij ij where Wr is a learnable parameter, ⊕ denotes the concatenation operator, and pd is calculated as in Vaswani et al. (2017), (2k) pd (2k+1)   = sin d/100002k/dmodel ,"
2020.acl-main.611,D15-1064,0,0.474785,"Missing"
2020.acl-main.611,P19-1115,0,0.0312922,"erence between our model and models above is that they modify the model structure according to the lattice, while we use a well-designed position encoding to indicate the lattice structure. 5.2 Lattice-based Transformer For lattice-based Transformer, it has been used in speech translation and Chinese-source translation. The main difference between them is the way to 1 indicate lattice structure. In Chinese-source translation, Xiao et al. (2019) take the absolute position of nodes’ first characters and the relation between each pair of nodes as the structure information. In speech translation, Sperber et al. (2019) used the longest distance to the start node to indicate lattice structure, and Zhang et al. (2019) used the shortest distance between two nodes. Our span position encoding is more natural, and can be mapped to all the three ways, but not vise versa. Because NER is more sensitive to position information than translation, our model is more suitable for NER. Recently, Porous Lattice Transformer (Mengge et al., 2019) is proposed for Chinese NER. The main difference between FLAT and Porus Lattice Transformer is the way of representing position information. We use ‘head’ and ‘tail’ to represent the"
2020.acl-main.611,K19-1058,0,0.0427694,"Missing"
2020.acl-main.611,D19-1396,0,0.543645,"c July 5 - 10, 2020. 2020 Association for Computational Linguistics CNN to encode potential words at different window sizes. However, RNN and CNN are hard to model long-distance dependencies (Vaswani et al., 2017), which may be useful in NER, such as coreference (Stanislawek et al., 2019). Due to the dynamic lattice structure, these methods cannot fully utilize the parallel computation of GPU. (2) Another line is to convert lattice into graph and use a graph neural network (GNN) to encode it, such as Lexicon-based Graph Network (LGN) (Gui et al., 2019b) and Collaborative Graph Network (CGN) (Sui et al., 2019). While sequential structure is still important for NER and graph is general counterpart, their gap is not negligible. These methods need to use LSTM as the bottom encoder to carry the sequential inductive bias, which makes the model complicated. In this paper, we propose FLAT: Flat LAttice Transformer for Chinese NER. Transformer (Vaswani et al., 2017) adopts fully-connected selfattention to model the long-distance dependencies in a sequence. To keep the position information, Transformer introduces the position representation for each token in the sequence. Inspired by the idea of position re"
2020.acl-main.611,P19-1298,0,0.0271261,"ph, converting NER into a node classification task. However, due to NER’s strong alignment of label and input, their model needs an RNN module for encoding. The main difference between our model and models above is that they modify the model structure according to the lattice, while we use a well-designed position encoding to indicate the lattice structure. 5.2 Lattice-based Transformer For lattice-based Transformer, it has been used in speech translation and Chinese-source translation. The main difference between them is the way to 1 indicate lattice structure. In Chinese-source translation, Xiao et al. (2019) take the absolute position of nodes’ first characters and the relation between each pair of nodes as the structure information. In speech translation, Sperber et al. (2019) used the longest distance to the start node to indicate lattice structure, and Zhang et al. (2019) used the shortest distance between two nodes. Our span position encoding is more natural, and can be mapped to all the three ways, but not vise versa. Because NER is more sensitive to position information than translation, our model is more suitable for NER. Recently, Porous Lattice Transformer (Mengge et al., 2019) is propos"
2020.acl-main.611,P19-1649,0,0.022316,"ttice, while we use a well-designed position encoding to indicate the lattice structure. 5.2 Lattice-based Transformer For lattice-based Transformer, it has been used in speech translation and Chinese-source translation. The main difference between them is the way to 1 indicate lattice structure. In Chinese-source translation, Xiao et al. (2019) take the absolute position of nodes’ first characters and the relation between each pair of nodes as the structure information. In speech translation, Sperber et al. (2019) used the longest distance to the start node to indicate lattice structure, and Zhang et al. (2019) used the shortest distance between two nodes. Our span position encoding is more natural, and can be mapped to all the three ways, but not vise versa. Because NER is more sensitive to position information than translation, our model is more suitable for NER. Recently, Porous Lattice Transformer (Mengge et al., 2019) is proposed for Chinese NER. The main difference between FLAT and Porus Lattice Transformer is the way of representing position information. We use ‘head’ and ‘tail’ to represent the token’s position in the lattice. They use ‘head’, tokens’ relative relation (not distance) and an"
2020.acl-main.611,P18-1144,0,0.349931,"o leverage the span position encoding. In 1(c), , , denotes tokens, heads and tails, respectively. Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks (Chen et al., 2015; Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016; Yang et al., 2017; Liu et al., 2017; Sun et al., 2020), Chinese NER is more difficult since it usually involves word segmentation. Recently, the lattice structure has been proved to have a great benefit to utilize the word information and avoid the error propagation of word segmentation (Zhang and Yang, 2018). We can match a sentence with a lexicon to obtain the latent words in it, and then we get a lattice like in Figure 1(a). The lattice is a directed acyclic graph, where each node is a character or a latent word. The lattice includes a sequence of characters and potential Corresponding author. 庆 Qing 药店 Pharmacy Introduction ∗ 人和药店 Renhe Pharmacy words in the sentence. They are not ordered sequentially, and the word’s first character and last character determine its position. Some words in lattice may be important for NER. For example, in Figure 1(a), “人和药店(Renhe Pharmacy)” can be used to disti"
2020.acl-main.611,yang-etal-2017-neural-reranking,0,\N,Missing
2020.acl-main.664,W14-3348,0,0.0358431,"Missing"
2020.acl-main.664,D17-1159,0,0.159016,"vel” label (obj1 , pred, obj2 ) extracted from captions, there may exist multiple object regions corresponding to obj1 and obj2 . In this paper, we propose to use weakly supervised multi-instance learning to detect if a bag of object (region) pairs in an image contain certain predicates, e.g., predicates appearing in ground-truth captions here (or in other applications, they can be any given predicates under concerns). Based on that we can construct caption-guided visual relationship graphs. Once the visual relationship graphs (VRG) are built, we propose to adapt graph convolution operations (Marcheggiani and Titov, 2017) to obtain representation for object nodes and predicate nodes. These nodes can be viewed as image representation units used for generation. During generation, we further incorporate visual relationships—we propose multi-task learning for jointly predicting word and tag sequences, where each word in a caption could be assigned with a tag, i.e., object, predicate, or none, which takes as input the graph node features from the above visual relationship graphs. The motivation for predicting a tag in each step is to regularize which types of information should be taken into more consideration for"
2020.acl-main.664,P02-1040,0,0.112926,"Missing"
2020.acl-main.664,W15-2812,0,0.0260432,"with regard to the ground truth caption S ∗ = {w1∗ , w2∗ , ..., wT∗ }: LXE = − log p(S ∗ |I) =− T X ∗ log p(wt∗ |w<t , I) (1) (2) t=1 The model is further tuned with a Reinforcement Learning (RL) objective (Rennie et al., 2017) to maximize the reward of the generated sentence S: JRL = ES∼p(S|I) (d(S, S ∗ )) (3) 3.1 3.1.1 Caption-Guided Visual Relationship Graph (CGVRG) with Weakly Supervised Learning Extracting Visual Relationship Triples and Detecting Objects The process of constructing CGVRG first extracts relationship triples from captions using textual scene graph parser as described in (Schuster et al., 2015). Our framework employs Faster RCNN (Ren et al., 2015) to recognize instances of objects and returns a set of image regions for objects: V = {v1 , v2 , · · · , vn }. 3.1.2 Constructing CGVRG The main focus of CGVRG is constructing visual relationship graphs. As discussed in introduction, the existing approaches use pre-trained VRD (visual relationship detection) models, which often ignore key relationships needed for captioning. This gap can be even more prominent if the domain/data used to train image-captioning is farther from where VRD is pretrained. A major challenge to use predicate tripl"
2020.acl-main.664,P19-1650,0,0.0524296,"Missing"
2020.coling-main.217,D10-1049,0,0.032865,"red dataset contained 728K, but only 500K are non-empty samples. 2399 2 Existing Data-to-Text Models and Datasets Research in NLP can be viewed as a close interplay between models and datasets. Traditionally, most methods hand-craft linguistic features to build rule-based systems, so datasets containing hundreds or thousands of samples are adequate. For example, in data-to-text generation, the traditional approach is to hand-craft templates (Kukich, 1983; Holmes-Higgin, 1994; McRoy et al., 2000). There are also works that abstract the templates and use probabilistic models to learn the rules (Angeli et al., 2010; Howald et al., 2013). Popular datasets for these methods are, for example, the 0.7K RoboCup dataset (Chen and Mooney, 2008). With the advance of deep learning, models have a large number of parameters, so they have to be fueled by large datasets. For data-to-text generation, many supervised methods use large neural networks. To match with these data-hungry models, researchers devote many efforts to curate supervised datasets. Most supervised data-to-text datasets have tens of thousands of data.4 For instance, WebNLG dataset (Gardent et al., 2017) has 13K valid data-text pairs,5 WeatherGov (L"
2020.coling-main.217,P19-1019,0,0.142532,"a 2 (GenWikiFULL) Graph about Dota 2 with high entity overlap (GenWikiFINE) developer developer publisher Dota 2 Valve Corporation product genre composer releaseDate publisher Dota 2 Tim Larkin genre Valve Corporation product 2013-07-09 multiplayer online battle arena multiplayer online battle arena Figure 1: Overview of the dataset. models is limited, as these existing unsupervised models cannot even have a deep architecture. In contrast, a relatively faster field, unsupervised machine translation, has dataset sizes on the order of billions, such as 1.6B German and 2.1B English text used in (Artetxe et al., 2019). Therefore, we propose a large dataset, GenWiki, which contains 1.3 million non-parallel text and graphs with shared content, and meet all four requirements mentioned before. To better facilitate research in unsupervised graph-to-text generation, we provide two versions of our dataset: the full dataset GenWikiFULL (1.3M), and a fine version, GenWikiFINE (750K), which adds constraints on the text and graphs to force them to contain highly overlapped entity sets. The overview of our two datasets are shown in Figure 1. The GenWikiFULL on the bottom left contains graphs on the same topic (Dota 2)"
2020.coling-main.217,W05-0909,0,0.0132122,"rmance with supervised models on the WebNLG dataset. CycleGTWarm The second setting proposed in (Guo et al., 2020) uses a warm up strategy before the cycle training process. Specifically, as entities are given by the dataset and serve as shared information between text and graphs, the CycleGT model warms up by learning entity-to-text generation and entityto-graph relation classification. This warm up strategy is used to make the unsupervised training more stable. 5.2 Evaluation Metrics We evaluate the text generation quality by four commonly used metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed paragraph (model output) to the input paragraph.9 5.3 Implementation Details For CycleGTBase and CycleGTWarm , we use the same code provided by (Guo et al., 2020). For DirectTransfer and NoisySupervised, we use the graph transformer model (Koncel-Kedziorski et al., 2019) reimplemented by (Guo et al., 2020) using the Deep Graph Library (DGL) (Wang et al., 2019). For a fair comparison, we use the same hyperparameters for the overlapped components of NoisySupervised, CycleGTBase , and CycleGTWarm"
2020.coling-main.217,W13-2111,0,0.0323759,"Missing"
2020.coling-main.217,N19-1423,0,0.00733202,"uffling the data part and text part of the original dataset, or directly deleting the text part. However, such formulation will make the unsupervised datasets inherit the small size of the supervised datasets, which are limited to only tens of thousands samples. There are several caveats of using small datasets for unsupervised models. (1) Limiting the model potential: Unsupervised models can be data-hungry. For example, 1.6B German and 2.1B English tokens are fed to unsupervised machine translation models (Artetxe et al., 2019); 3.3 billion words are used to pretrain the language model BERT (Devlin et al., 2019). With abundant data, the potential of unsupervised models is unleashed – impressive performance, based on an enormous number of parameters, such as 12 layers of transformers. So far, no large data-to-text corpus can be used to unveil the potential of unsupervised data-to-text. (2) Lack of diversity: As many unsupervised models need to impose strong priors such as grammar and dependency tree (Konstas and Lapata, 2012), if a proposed model works well on a specific type of text generation, we cannot validate whether such a model generalizes well. (3) Negligence of model efficiency: The goal of u"
2020.coling-main.217,D18-1426,0,0.0614894,"e used to generate introductions of plants. This can happen even between domains with similar content but different text styles. For example, given the knowledge graph triple “(Obama, birthYear, 1961),” one domain verbalizes it as “Obama was born in 1961,” whereas another domain prefers “Obama (1961 – ) ...”. A model trained in the first domain can only generate the entities correctly but fail on all other words in the second domain. To overcome the lack of labelled data and difficulty in domain adaptation, unsupervised data-to-text generation has emerged as an active research field recently (Freitag and Roy, 2018; Schmitt et al., 2020; Guo et al., 2020). However, the progress of this line of research is slowed down due to the lack of largescale unsupervised datasets. Notably, the curation of graph-to-text unsupervised datasets are non-trivial, as it requires (1) same content distribution between graphs and text, (2) text with high-accuracy entity annotation, (3) a much larger scale than the supervised datasets, and (4) a human-annotated test set. Unfortunately, lacking such an unsupervised dataset, most unsupervised works have to artificially remove the pairing information between text and structured"
2020.coling-main.217,W17-3518,0,0.124275,"automatic way. The comparison of our dataset and previous data-to-text datasets is illustrated in Table 1. An additional contribution is our human-annotated test set with of 1,000 graph and text pairs. Based on our large-scale training dataset and the human-annotated test set, we analyze the performance of several baselines and existing models. We conducted error analysis on the strengths and shortness of these unsupervised models in Section 5.4. Dataset KBGen (Banik et al., 2013) RoboCup (Chen and Mooney, 2008) RotoWire (Wiseman et al., 2017) SF Hotels/Restaurants (Wen et al., 2015) WebNLG (Gardent et al., 2017) WeatherGov (Liang et al., 2009) E2E (Novikova et al., 2017) WikiCompany (Qader et al., 2018) ToTTO (Parikh et al., 2020) WikiBio (Lebret et al., 2016) GenWikiFINE GenWikiFULL Size 0.2K 0.7K 5K 10K 13K 22K 50K 51K 100K 500K3 750K 1.3M Purpose Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Unsupervised Unsupervised Collection Human Human Human Human Human Human Human Human Human Auto Auto&distant align Auto Domain Descriptions of biology knowledge bases Sportscast Baseketball game summaries Dialogues about restaurants and hotels 15"
2020.coling-main.217,2020.webnlg-1.8,1,0.779088,"his can happen even between domains with similar content but different text styles. For example, given the knowledge graph triple “(Obama, birthYear, 1961),” one domain verbalizes it as “Obama was born in 1961,” whereas another domain prefers “Obama (1961 – ) ...”. A model trained in the first domain can only generate the entities correctly but fail on all other words in the second domain. To overcome the lack of labelled data and difficulty in domain adaptation, unsupervised data-to-text generation has emerged as an active research field recently (Freitag and Roy, 2018; Schmitt et al., 2020; Guo et al., 2020). However, the progress of this line of research is slowed down due to the lack of largescale unsupervised datasets. Notably, the curation of graph-to-text unsupervised datasets are non-trivial, as it requires (1) same content distribution between graphs and text, (2) text with high-accuracy entity annotation, (3) a much larger scale than the supervised datasets, and (4) a human-annotated test set. Unfortunately, lacking such an unsupervised dataset, most unsupervised works have to artificially remove the pairing information between text and structured data, to force parallel datasets to be no"
2020.coling-main.217,W13-0113,0,0.0265185,"728K, but only 500K are non-empty samples. 2399 2 Existing Data-to-Text Models and Datasets Research in NLP can be viewed as a close interplay between models and datasets. Traditionally, most methods hand-craft linguistic features to build rule-based systems, so datasets containing hundreds or thousands of samples are adequate. For example, in data-to-text generation, the traditional approach is to hand-craft templates (Kukich, 1983; Holmes-Higgin, 1994; McRoy et al., 2000). There are also works that abstract the templates and use probabilistic models to learn the rules (Angeli et al., 2010; Howald et al., 2013). Popular datasets for these methods are, for example, the 0.7K RoboCup dataset (Chen and Mooney, 2008). With the advance of deep learning, models have a large number of parameters, so they have to be fueled by large datasets. For data-to-text generation, many supervised methods use large neural networks. To match with these data-hungry models, researchers devote many efforts to curate supervised datasets. Most supervised data-to-text datasets have tens of thousands of data.4 For instance, WebNLG dataset (Gardent et al., 2017) has 13K valid data-text pairs,5 WeatherGov (Liang et al., 2009) has"
2020.coling-main.217,N19-1238,0,0.306968,"itt et al., 2020), Rule-Based linearizes the graph into a sequence of triples. Each triple is described by turning the camel-cased relation type to a normal phrase. For example, the triple “(New York City, populationTotal, 8 million)” will be verbalized as “New York City population total 8 million.” The descriptions of multiple triples are joined by “and.” DirectTransfer DirectTransfer is another intuitive baseline, where we use a model trained on the supervised WebNLG dataset, and test it on the GenWiki test set. For a fair comparison, we use a graph transformer generation model proposed by (Koncel-Kedziorski et al., 2019) for all graph-to-text models. NoisySupervised We also propose another baseline, NoisySupervised, which attempts to absorb the weak supervision signals in the training set, and convert the difficult unsupervised learning problem into supervised learning. Specifically, NoisySupervised first constructs distantly aligned pairs on the whole training data, using the same matching method that we adopted to create our preliminary test set before human annotation. It then takes these pairs with noises as supervision signals, and learn from them using the graph transformer (Koncel-Kedziorski et al., 20"
2020.coling-main.217,N12-1093,0,0.0970926,"use large neural networks. To match with these data-hungry models, researchers devote many efforts to curate supervised datasets. Most supervised data-to-text datasets have tens of thousands of data.4 For instance, WebNLG dataset (Gardent et al., 2017) has 13K valid data-text pairs,5 WeatherGov (Liang et al., 2009) has 22K weather forecasts, and E2E (Novikova et al., 2017) has 50K restaurant and hotel descriptions, as shown in Table 1. Recently, there is a rising trend of unsupervised approaches (Freitag and Roy, 2018; Schmitt et al., 2020; Guo et al., 2020). Unsupervised data-to-text models (Konstas and Lapata, 2012) are proposed in response to the lack of data-text pairs in many domains, similar to the emergence of unsupervised machine translation that addresses lack of data low-resource language pairs. However, to match with the active research on unsupervised data-to-text generation, no suitable dataset has been curated. As a result, most previous works have to artificially force parallel datasets to be non-parallel, by separately shuffling the data part and text part of the original dataset, or directly deleting the text part. However, such formulation will make the unsupervised datasets inherit the s"
2020.coling-main.217,P83-1022,0,0.13435,"our GenWiki datasets. GenWikiFINE is the fine version (with stronger entity alignment) of our dataset, and GenWikiFULL is our full dataset. 3 The unfiltered dataset contained 728K, but only 500K are non-empty samples. 2399 2 Existing Data-to-Text Models and Datasets Research in NLP can be viewed as a close interplay between models and datasets. Traditionally, most methods hand-craft linguistic features to build rule-based systems, so datasets containing hundreds or thousands of samples are adequate. For example, in data-to-text generation, the traditional approach is to hand-craft templates (Kukich, 1983; Holmes-Higgin, 1994; McRoy et al., 2000). There are also works that abstract the templates and use probabilistic models to learn the rules (Angeli et al., 2010; Howald et al., 2013). Popular datasets for these methods are, for example, the 0.7K RoboCup dataset (Chen and Mooney, 2008). With the advance of deep learning, models have a large number of parameters, so they have to be fueled by large datasets. For data-to-text generation, many supervised methods use large neural networks. To match with these data-hungry models, researchers devote many efforts to curate supervised datasets. Most su"
2020.coling-main.217,D16-1128,0,0.556309,"tated test set with of 1,000 graph and text pairs. Based on our large-scale training dataset and the human-annotated test set, we analyze the performance of several baselines and existing models. We conducted error analysis on the strengths and shortness of these unsupervised models in Section 5.4. Dataset KBGen (Banik et al., 2013) RoboCup (Chen and Mooney, 2008) RotoWire (Wiseman et al., 2017) SF Hotels/Restaurants (Wen et al., 2015) WebNLG (Gardent et al., 2017) WeatherGov (Liang et al., 2009) E2E (Novikova et al., 2017) WikiCompany (Qader et al., 2018) ToTTO (Parikh et al., 2020) WikiBio (Lebret et al., 2016) GenWikiFINE GenWikiFULL Size 0.2K 0.7K 5K 10K 13K 22K 50K 51K 100K 500K3 750K 1.3M Purpose Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Unsupervised Unsupervised Collection Human Human Human Human Human Human Human Human Human Auto Auto&distant align Auto Domain Descriptions of biology knowledge bases Sportscast Baseketball game summaries Dialogues about restaurants and hotels 15 categories (building, person) Weather forecasts Restaurant and hotel descriptions Company descriptions Description of Wikipedia tables First sentence o"
2020.coling-main.217,P09-1011,0,0.175939,"our dataset and previous data-to-text datasets is illustrated in Table 1. An additional contribution is our human-annotated test set with of 1,000 graph and text pairs. Based on our large-scale training dataset and the human-annotated test set, we analyze the performance of several baselines and existing models. We conducted error analysis on the strengths and shortness of these unsupervised models in Section 5.4. Dataset KBGen (Banik et al., 2013) RoboCup (Chen and Mooney, 2008) RotoWire (Wiseman et al., 2017) SF Hotels/Restaurants (Wen et al., 2015) WebNLG (Gardent et al., 2017) WeatherGov (Liang et al., 2009) E2E (Novikova et al., 2017) WikiCompany (Qader et al., 2018) ToTTO (Parikh et al., 2020) WikiBio (Lebret et al., 2016) GenWikiFINE GenWikiFULL Size 0.2K 0.7K 5K 10K 13K 22K 50K 51K 100K 500K3 750K 1.3M Purpose Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Unsupervised Unsupervised Collection Human Human Human Human Human Human Human Human Human Auto Auto&distant align Auto Domain Descriptions of biology knowledge bases Sportscast Baseketball game summaries Dialogues about restaurants and hotels 15 categories (building, person) We"
2020.coling-main.217,W04-1013,0,0.0237319,"e WebNLG dataset. CycleGTWarm The second setting proposed in (Guo et al., 2020) uses a warm up strategy before the cycle training process. Specifically, as entities are given by the dataset and serve as shared information between text and graphs, the CycleGT model warms up by learning entity-to-text generation and entityto-graph relation classification. This warm up strategy is used to make the unsupervised training more stable. 5.2 Evaluation Metrics We evaluate the text generation quality by four commonly used metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed paragraph (model output) to the input paragraph.9 5.3 Implementation Details For CycleGTBase and CycleGTWarm , we use the same code provided by (Guo et al., 2020). For DirectTransfer and NoisySupervised, we use the graph transformer model (Koncel-Kedziorski et al., 2019) reimplemented by (Guo et al., 2020) using the Deep Graph Library (DGL) (Wang et al., 2019). For a fair comparison, we use the same hyperparameters for the overlapped components of NoisySupervised, CycleGTBase , and CycleGTWarm . 5.4 Results Rule-"
2020.coling-main.217,W00-1437,0,0.0150891,"is the fine version (with stronger entity alignment) of our dataset, and GenWikiFULL is our full dataset. 3 The unfiltered dataset contained 728K, but only 500K are non-empty samples. 2399 2 Existing Data-to-Text Models and Datasets Research in NLP can be viewed as a close interplay between models and datasets. Traditionally, most methods hand-craft linguistic features to build rule-based systems, so datasets containing hundreds or thousands of samples are adequate. For example, in data-to-text generation, the traditional approach is to hand-craft templates (Kukich, 1983; Holmes-Higgin, 1994; McRoy et al., 2000). There are also works that abstract the templates and use probabilistic models to learn the rules (Angeli et al., 2010; Howald et al., 2013). Popular datasets for these methods are, for example, the 0.7K RoboCup dataset (Chen and Mooney, 2008). With the advance of deep learning, models have a large number of parameters, so they have to be fueled by large datasets. For data-to-text generation, many supervised methods use large neural networks. To match with these data-hungry models, researchers devote many efforts to curate supervised datasets. Most supervised data-to-text datasets have tens o"
2020.coling-main.217,P09-1113,0,0.369961,"t of relations. We summarize the requirements of the dataset collection as follows: • Basic Requirements 1. Text and graphs should have similar contents, such as a Wikipedia article and a knowledge graph of the same article. 2. To fit for recent unsupervised models (Guo et al., 2020; Schmitt et al., 2020), the text corpus should contain entity annotations. 3. The knowledge graphs should have a closed set of relations. • Preferred Properties 4. Large scale (over 500K). 5. Diversity, not limited to one specialized domain. 6. Human-annotated test set, as opposed to distant supervision test sets (Mintz et al., 2009; Riedel et al., 2010), because noisy test sets can misguide the comparison of unsupervised models. 4 GenWiki Dataset Based on the desiderata outlined in Section 3, we will first introduce the construction process of our training set in Section 4.1, and test set in Section 4.2. We will then analyze the characteristics of the resulting dataset in Section 4.3. 4.1 Training Set Construction An ideal unsupervised graph-to-text dataset allows text sequences consisting of multiple sentences, and graphs of several triples. In the collection process of GenWiki, we allow each text sequence to have 1 to"
2020.coling-main.217,N19-1236,0,0.241454,"Missing"
2020.coling-main.217,W17-5525,0,0.152084,"data-to-text datasets is illustrated in Table 1. An additional contribution is our human-annotated test set with of 1,000 graph and text pairs. Based on our large-scale training dataset and the human-annotated test set, we analyze the performance of several baselines and existing models. We conducted error analysis on the strengths and shortness of these unsupervised models in Section 5.4. Dataset KBGen (Banik et al., 2013) RoboCup (Chen and Mooney, 2008) RotoWire (Wiseman et al., 2017) SF Hotels/Restaurants (Wen et al., 2015) WebNLG (Gardent et al., 2017) WeatherGov (Liang et al., 2009) E2E (Novikova et al., 2017) WikiCompany (Qader et al., 2018) ToTTO (Parikh et al., 2020) WikiBio (Lebret et al., 2016) GenWikiFINE GenWikiFULL Size 0.2K 0.7K 5K 10K 13K 22K 50K 51K 100K 500K3 750K 1.3M Purpose Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Supervised Unsupervised Unsupervised Collection Human Human Human Human Human Human Human Human Human Auto Auto&distant align Auto Domain Descriptions of biology knowledge bases Sportscast Baseketball game summaries Dialogues about restaurants and hotels 15 categories (building, person) Weather forecasts Restaurant a"
2020.coling-main.217,P02-1040,0,0.111045,"ity, was proved comparable performance with supervised models on the WebNLG dataset. CycleGTWarm The second setting proposed in (Guo et al., 2020) uses a warm up strategy before the cycle training process. Specifically, as entities are given by the dataset and serve as shared information between text and graphs, the CycleGT model warms up by learning entity-to-text generation and entityto-graph relation classification. This warm up strategy is used to make the unsupervised training more stable. 5.2 Evaluation Metrics We evaluate the text generation quality by four commonly used metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed paragraph (model output) to the input paragraph.9 5.3 Implementation Details For CycleGTBase and CycleGTWarm , we use the same code provided by (Guo et al., 2020). For DirectTransfer and NoisySupervised, we use the graph transformer model (Koncel-Kedziorski et al., 2019) reimplemented by (Guo et al., 2020) using the Deep Graph Library (DGL) (Wang et al., 2019). For a fair comparison, we use the same hyperparameters for the overlapped components of NoisySuperv"
2020.coling-main.217,2020.emnlp-main.89,0,0.0254373,"Missing"
2020.coling-main.217,W18-6532,0,0.0359283,"Missing"
2020.coling-main.217,2020.emnlp-main.577,0,0.165417,"Missing"
2020.coling-main.217,W18-6502,0,0.139276,"Missing"
2020.coling-main.217,D15-1199,0,0.0428361,"Missing"
2020.coling-main.217,D17-1239,0,0.0645044,"Missing"
2020.coling-main.327,P18-1009,0,0.0281441,"h its symbolic entity, we follow P¨orner et al. (2019) and concatenate the two forms of tokens, e.g. Jean Mara ##is Jean Marais. Concretely, we conduct experiments on two knowledge-driven tasks: entity typing and relation extraction. Entity Typing. The entity typing task is to classify the semantic type of a given entity mention based on its surface form and context. We add two special tokens, [ENT] and [/ENT], before and after the entity mentions to be classified and use the final representation of the [CLS] token as the feature to conduct classification4 . We evaluate CoLAKE on Open Entity (Choi et al., 2018). To compare with ERNIE, KnowBERT, and KEPLER, we adopt the same experiment setting which considers nine general types. To be consistent with previous work, we adopt micro precision, recall, and F1 score as evaluation metrics. The experimental results are shown in Table 2. Relation Extraction. The relation extraction task is to classify the relationship between two entities mentioned in a given sentence. During fine-tuning, we add four special tokens, [HD], [/HD], [TL] and [/TL] to identify the head entity and the tail entity. Also, we use the final representation of the [CLS] token as the fea"
2020.coling-main.327,D19-1109,0,0.0601033,"Missing"
2020.coling-main.327,N19-1423,0,0.615755,"uct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013"
2020.coling-main.327,D17-1277,0,0.0193316,"se subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a). Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al., 2019c) aims to benefit both sides so jointly learn language model and knowledge embedding. However, KEPLER does not directly learn embeddings fo"
2020.coling-main.327,D18-1514,0,0.0279947,"xperiment setting which considers nine general types. To be consistent with previous work, we adopt micro precision, recall, and F1 score as evaluation metrics. The experimental results are shown in Table 2. Relation Extraction. The relation extraction task is to classify the relationship between two entities mentioned in a given sentence. During fine-tuning, we add four special tokens, [HD], [/HD], [TL] and [/TL] to identify the head entity and the tail entity. Also, we use the final representation of the [CLS] token as the feature to be fed into the classifier. We evaluate CoLAKE on FewRel (Han et al., 2018) that is rearranged by Zhang et al. (2019). Since FewRel is built with Wikidata, we discard triplets in the FewRel test set from pre-training data to avoid information leakage. Following previous work, we report macro precision, recall and F1 score on FewRel. The experimental results can be found in Table 2. Model P Open Entity R F P FewRel R F BERT (Devlin et al., 2019) RoBERTa (Liu et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2019c) E-BERT (P¨orner et al., 2019) 76.4 77.4 78.4 78.6 77.8 - 71.0 73.6 72.9 73.7 74.6 - 73.6 75.4 75.6 76.1 76.2 - 85"
2020.coling-main.327,P19-1598,0,0.0224223,"l results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and lang"
2020.coling-main.327,P09-1113,0,0.104792,"Missing"
2020.coling-main.327,D14-1162,0,0.101956,"and contextualized knowledge representation simultaneously with the extended MLM objective. (2) CoLAKE adopts the WK graph to integrate the heterogeneous input for language and knowledge. (3) CoLAKE is essentially a pre-trained graph neural network (GNN), thereby being structure-aware and easy to extend. 3661 2 Related Work Language Representation Learning. The past decade has witnessed the great success of pre-trained language representation. Initially, word representation pre-trained using multi-task objectives (Collobert and Weston, 2008) or co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014) are static and non-contextual. Recently, contextualized word representation pre-trained on large-scale unlabeled corpora with deep neural networks has dominated across a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Qiu et al., 2020). Knowledge Representation Learning. Knowledge Representation Learning (KRL) is also termed as Knowledge Embedding (KE), which is to map entities and relations into low-dimensional continuous vectors. Most existing methods use triplets as training samples to learn static, non-contextual embeddings for entities and relations"
2020.coling-main.327,N18-1202,0,0.49804,"rmer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE"
2020.coling-main.327,D19-1005,0,0.650335,"ntation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and language embedding simultaneously. (2) The previous models only take entity embeddings to enhance PLMs, which are hard to fully capture the rich contextual information of an entity in the knowledge graph (KG). Thus their performance gains are limited by the quality of pre-trained entity embedding"
2020.coling-main.327,D19-1250,0,0.0550134,"Missing"
2020.coling-main.327,P16-1162,0,0.0181374,"the WK graph, entities are unique but relations are allowed to repeat. 3.2 Model Architecture The constructed WK graphs are then fed into the Transformer (Vaswani et al., 2017) encoder. We modify the embedding and encoder layers of vanilla Transformer to adapt to input in the form of WK graph. Embedding Layer. The input embedding is the sum of token embedding, type embedding, and position embedding. For token embedding, we maintain three lookup tables for words, entities, and relations respectively. For word embedding, we follow RoBERTa (Liu et al., 2019) which uses Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to transform sequence into subwords units to handle the large vocabulary. In contrast, we directly learn embeddings for each unique entity and relation as common knowledge embedding methods do. The token embeddings are obtained by concatenating word, entity, and relation embeddings, which are of the same dimensionality. There are different types of nodes so the WK graph is heterogeneous. To handle this, we simply use type embedding to indicate the node types, i.e. word, entity, and relation. For position embedding, we need to assign each injected entity and relation a position index. Inspired"
2020.coling-main.327,D14-1167,0,0.100998,"and relations (Bordes et al., 2013; Yang et al., 2015; Lin et al., 2015). Recent advances focusing on contextualized representation, which use subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a). Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al."
2020.coling-main.327,K16-1025,0,0.0762338,"epresentation, which use subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a). Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al., 2019c) aims to benefit both sides so jointly learn language model and knowledge embedding. However, KEPLER does not di"
2020.coling-main.327,P19-1139,0,0.589772,"ing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and language embedding simultaneously. (2) The previous models only take entity embeddings to enhance PLMs, which are hard to fully capture the rich contextual information of an entity in the knowledge graph (KG). Thus their performance gains are limited by the qual"
2020.emnlp-main.210,2020.acl-main.747,0,0.0715994,"Missing"
2020.emnlp-main.210,N19-1423,0,0.245367,". Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several li"
2020.emnlp-main.210,P15-1166,0,0.022539,"tialization fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.210,2020.findings-emnlp.283,0,0.0187955,"013a) first introduces dictionaries to align word representations from different languages. A series of followup studies focus on aligning the word representation across languages (Xing et al., 2015; Ammar et al., 2016; Smith et al., 2017; Lample et al., 2018b). Inspired by the success of BERT, Conneau and Lample (2019) introduced XLM - masked language models trained on multiple languages, as a way to leverage parallel data and obtain impressive empirical results on the cross-lingual natural language inference (XNLI) benchmark and unsupervised NMT(Sennrich et al., 2016a; Lample et al., 2018a; Garcia et al., 2020). Huang et al. (2019) extended XLM with multi-task learning and proposed a universal language encoder. Different from these works, a) mRASP is actually a multilingual sequence to sequence model which is more desirable for NMT pre-training; b) mRASP introduces alignment regularization to bridge the sentence representation across languages. 6 Conclusion In this paper, we propose a multilingual neural machine translation pre-training model (mRASP). To bridge the semantic space between different languages, we incorporate word alignment into the pre-training model. Extensive experiments are conduct"
2020.emnlp-main.210,D19-1252,0,0.34639,"f parallel corpus to simulate different scenarios. Most of the En-X parallel datasets are from the pre-training phase to avoid introducing new information. Most pairs for fine-tuning are from previous years of WMT and IWSLT. Specifically, we use WMT14 for EnDe and En-Fr, WMT16 for En-Ro. For pairs like Nl(Dutch)-Pt(Portuguese) that are not available in WMT or IWSLT, we use news-commentary instead. For a detailed description, please refer to the Appendix. 2652 8 CTNMT only reports the Transformer-base setting. Lang-Pairs Size En→De 4.5M Zh→En 20M En→Fr 40M Direct CTNMT8 (2020) mBART (2020) XLM (2019) MASS (2019) mBERT (2019) 29.3 30.1 28.8 28.9 28.6 24.1 - 43.2 42.3 41.0 - mRASP 30.3 24.7 44.3 Table 2: Fine-tuning performance for popular medium and rich resource MT tasks. For fair comparison, we report detokenized BLEU on WMT newstest18 for Zh→En and tokenized BLEU on WMT newstest14 for En→Fr and En→De. Notice unlike previous methods (except CTNMT) which do not improve in the rich resource settings, mRASP is again able to consistently improve the downstream MT performance. It is the first time to verify that low-resource language pairs can be utilized to improve rich resource MT. Based on"
2020.emnlp-main.210,Q17-1024,0,0.0398988,"Missing"
2020.emnlp-main.210,2020.acl-main.703,0,0.118192,"Missing"
2020.emnlp-main.210,2020.tacl-1.47,0,0.324,"following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several limitations for machine translation tasks. First, pre-trained language models such as BERT are not easy to directly fine-tune unless using some sophisticated techniques (Yang et al., 2020). Second, there is a discrepancy between existing pre-training objective and down-stream ones in MT. Existing pre-training approaches such as MASS (Song et al., 2019) and mBART (Liu et al., 2020) rely on auto-encoding objectives to pre-train the models, which are different from translation. Therefore, their fine-tuned MT models still do not achieve adequate improvement. Third, existing MT pre-training approaches focus on using multilingual models to improve MT for low resource or medium resource languages. There has not been one pre-trained MT model that can improve for any pairs of languages, even for rich resource settings such as English-French. In this paper, we propose multilingual Random Aligned Substitution Pre-training (mRASP), a method to pre-train a MT model for many languag"
2020.emnlp-main.210,2021.ccl-1.108,0,0.128783,"Missing"
2020.emnlp-main.210,W18-6309,0,0.0189222,"ource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually ac"
2020.emnlp-main.210,P19-1015,0,0.0284706,"M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually achieves inferior accura"
2020.emnlp-main.210,P16-1009,0,0.537502,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,W18-6301,0,0.161617,"ts cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the scale of the dataset increasing, the gap between the randomly initialized baseline and pre-training model is becoming closer. It is worth noting that, for En→De benchmark, we obtain 1.0 BLEU points gains9 . To verify mRASP can further boost performance on rich resource datasets, we also conduct experiments on En→Zh and En→Fr. We compare our results with two strong baselines reported by Ott et al. (2018); Li et al. (2019). As shown in Table 2, surprisingly, when large parallel datasets are provided, it still benefits from pre-training models. In En→Fr, we obtain 1.1 BLEU points gains. Comparing to other Pre-training Approaches We compare our mRASP to recently proposed multilingual pre-training models. Following Liu et al. (2020), we conduct experiments on En-Ro, the only pairs with established results. To make a fair comparison, we report de-tokenized BLEU. As illustrated in Table 4 , Our model reaches comparable performance on both En→Ro and Ro→En. We also combine Back Translation (Sennrich"
2020.emnlp-main.210,P16-1162,0,0.792767,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,D14-1162,0,0.0844587,"Missing"
2020.emnlp-main.210,N18-1202,0,0.293103,"exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there"
2020.emnlp-main.210,W18-6319,0,0.0204161,"ly initialized models directly on downstream bilingual parallel corpus as a comparison with pre-training models. Fine-tuning We fine-tune our obtained mRASP model on the target language pairs. We apply a dropout rate of 0.3 for all pairs except for rich resource such as En-Zh and En-Fr with 0.1. We carefully tune the model, setting different learning rates and learning scheduler warm-up steps for different data scale. For inference, we use beam-search with beam size 5 for all directions. For most cases, We measure case-sensitive tokenized BLEU. We also report de-tokenized BLEU with SacreBLEU (Post, 2018) for a fair comparison with previous works. 3.2 Main Results We first conduct experiments on the (extremely) low-resource and medium-resource datasets, where multilingual translation usually obtains significant improvements. As illustrated in Table 1, we obtain significant gains in all datasets. For extremely low resources setting such as En-Be (Belarusian) where the amount of datasets cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the sca"
2020.emnlp-main.210,N18-2084,0,0.109204,"Missing"
2020.emnlp-main.210,N15-1104,0,0.0311338,"fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.500,D18-1316,0,0.571404,"ding author. and robust. While in computer vision fields, both attack strategies and their defense countermeasures are well-explored (Chakraborty et al., 2018), the adversarial attack for text is still challenging due to the discrete nature of languages. Generating of adversarial samples for texts needs to possess such qualities: (1) imperceptible to human judges yet misleading to neural models; (2) fluent in grammar and semantically consistent with original inputs. Previous methods craft adversarial samples mainly based on specific rules (Li et al., 2018; Gao et al., 2018; Yang et al., 2018; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020). Therefore, these methods are difficult to guarantee the fluency and semantically preservation in the generated adversarial samples at the same time. Plus, these manual craft methods are rather complicated. They use multiple linguistic constraints like NER tagging or POS tagging. Introducing contextualized language models to serve as an automatic perturbation generator could make these rules designing much easier. The recent rise of pre-trained language models, such as BERT (Devlin et al., 2018), push the performances of NLP tasks to a n"
2020.emnlp-main.500,D15-1075,0,0.164567,"mbeddings. Glove embeddings do not guarantee similar vector space with cosine similarity distance, therefore the perturbations are less semantically consistent. Jin et al. (2019) apply a semantically enhanced embedding (Mrkˇsi´c et al., 2016), which is context unaware, thus less consistent with the unperturbed inputs. Liang et al. (2017) use phrase-level insertion and deletion, which produces unnatural sentences inconsistent with the original inputs, lacking fluency control. To preserve semantic information, Glockner et al. (2018) replace words manually to break the language inference system (Bowman et al., 2015). Jia and Liang (2017) propose manual craft methods to attack machine reading comprehension systems. Lei et al. (2019) introduce replacement strategies using embedding transition. Although the above approaches have achieved good results, there is still much room for improvement regarding the perturbed percentage, attacking success rate, grammatical correctness and semantic consistency, etc. Moreover, the substitution strategies of these approaches are usually non-trivial, resulting in that they are limited to specific tasks. Adversarial Attack against BERT Pre-trained language models have beco"
2020.emnlp-main.500,D18-2029,0,0.0671554,"Missing"
2020.emnlp-main.500,P18-2103,0,0.0226209,"are still not contextaware and heavily rely on cosine similarity measurement of word embeddings. Glove embeddings do not guarantee similar vector space with cosine similarity distance, therefore the perturbations are less semantically consistent. Jin et al. (2019) apply a semantically enhanced embedding (Mrkˇsi´c et al., 2016), which is context unaware, thus less consistent with the unperturbed inputs. Liang et al. (2017) use phrase-level insertion and deletion, which produces unnatural sentences inconsistent with the original inputs, lacking fluency control. To preserve semantic information, Glockner et al. (2018) replace words manually to break the language inference system (Bowman et al., 2015). Jia and Liang (2017) propose manual craft methods to attack machine reading comprehension systems. Lei et al. (2019) introduce replacement strategies using embedding transition. Although the above approaches have achieved good results, there is still much room for improvement regarding the perturbed percentage, attacking success rate, grammatical correctness and semantic consistency, etc. Moreover, the substitution strategies of these approaches are usually non-trivial, resulting in that they are limited to s"
2020.emnlp-main.500,D17-1215,0,0.0725133,"dings do not guarantee similar vector space with cosine similarity distance, therefore the perturbations are less semantically consistent. Jin et al. (2019) apply a semantically enhanced embedding (Mrkˇsi´c et al., 2016), which is context unaware, thus less consistent with the unperturbed inputs. Liang et al. (2017) use phrase-level insertion and deletion, which produces unnatural sentences inconsistent with the original inputs, lacking fluency control. To preserve semantic information, Glockner et al. (2018) replace words manually to break the language inference system (Bowman et al., 2015). Jia and Liang (2017) propose manual craft methods to attack machine reading comprehension systems. Lei et al. (2019) introduce replacement strategies using embedding transition. Although the above approaches have achieved good results, there is still much room for improvement regarding the perturbed percentage, attacking success rate, grammatical correctness and semantic consistency, etc. Moreover, the substitution strategies of these approaches are usually non-trivial, resulting in that they are limited to specific tasks. Adversarial Attack against BERT Pre-trained language models have become mainstream for many"
2020.emnlp-main.500,P19-1561,0,0.0884273,"on systems. Lei et al. (2019) introduce replacement strategies using embedding transition. Although the above approaches have achieved good results, there is still much room for improvement regarding the perturbed percentage, attacking success rate, grammatical correctness and semantic consistency, etc. Moreover, the substitution strategies of these approaches are usually non-trivial, resulting in that they are limited to specific tasks. Adversarial Attack against BERT Pre-trained language models have become mainstream for many NLP tasks. Works such as (Wallace et al., 2019; Jin et al., 2019; Pruthi et al., 2019) have explored these pre-trained language models from many different angles. Wallace et al. (2019) explored the possible ethical problems of learned knowledge in pre-trained models. 6194 BERT-Attack Generated Sample 3.1 Finding Vulnerable Words Under the black-box scenario, the logit output by the target model (fine-tuned BERT or other neural models) is the only supervision we can get. We first select the words in the sequence which have a high significance influence on the final output logit. Let S = [w0 , · · · , wi · · · ] denote the input sentence, and oy (S) denote the logit output by the"
2020.emnlp-main.500,P19-1103,0,0.116364,". While in computer vision fields, both attack strategies and their defense countermeasures are well-explored (Chakraborty et al., 2018), the adversarial attack for text is still challenging due to the discrete nature of languages. Generating of adversarial samples for texts needs to possess such qualities: (1) imperceptible to human judges yet misleading to neural models; (2) fluent in grammar and semantically consistent with original inputs. Previous methods craft adversarial samples mainly based on specific rules (Li et al., 2018; Gao et al., 2018; Yang et al., 2018; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020). Therefore, these methods are difficult to guarantee the fluency and semantically preservation in the generated adversarial samples at the same time. Plus, these manual craft methods are rather complicated. They use multiple linguistic constraints like NER tagging or POS tagging. Introducing contextualized language models to serve as an automatic perturbation generator could make these rules designing much easier. The recent rise of pre-trained language models, such as BERT (Devlin et al., 2018), push the performances of NLP tasks to a new level. On the o"
2020.emnlp-main.500,D19-1221,0,0.127343,"ds to attack machine reading comprehension systems. Lei et al. (2019) introduce replacement strategies using embedding transition. Although the above approaches have achieved good results, there is still much room for improvement regarding the perturbed percentage, attacking success rate, grammatical correctness and semantic consistency, etc. Moreover, the substitution strategies of these approaches are usually non-trivial, resulting in that they are limited to specific tasks. Adversarial Attack against BERT Pre-trained language models have become mainstream for many NLP tasks. Works such as (Wallace et al., 2019; Jin et al., 2019; Pruthi et al., 2019) have explored these pre-trained language models from many different angles. Wallace et al. (2019) explored the possible ethical problems of learned knowledge in pre-trained models. 6194 BERT-Attack Generated Sample 3.1 Finding Vulnerable Words Under the black-box scenario, the logit output by the target model (fine-tuned BERT or other neural models) is the only supervision we can get. We first select the words in the sequence which have a high significance influence on the final output logit. Let S = [w0 , · · · , wi · · · ] denote the input sentence, a"
2020.emnlp-main.500,N18-1101,0,0.0610108,"Missing"
2020.emnlp-main.500,2020.acl-main.540,0,0.200388,"oth attack strategies and their defense countermeasures are well-explored (Chakraborty et al., 2018), the adversarial attack for text is still challenging due to the discrete nature of languages. Generating of adversarial samples for texts needs to possess such qualities: (1) imperceptible to human judges yet misleading to neural models; (2) fluent in grammar and semantically consistent with original inputs. Previous methods craft adversarial samples mainly based on specific rules (Li et al., 2018; Gao et al., 2018; Yang et al., 2018; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020). Therefore, these methods are difficult to guarantee the fluency and semantically preservation in the generated adversarial samples at the same time. Plus, these manual craft methods are rather complicated. They use multiple linguistic constraints like NER tagging or POS tagging. Introducing contextualized language models to serve as an automatic perturbation generator could make these rules designing much easier. The recent rise of pre-trained language models, such as BERT (Devlin et al., 2018), push the performances of NLP tasks to a new level. On the one hand, the powerful ability of a fin"
2020.emnlp-main.500,P19-1328,0,0.0310076,"· , wj−1 , c, · · · ] // do one perturbation return None Thus, using the masked language model as a contextualized perturbation generator can be one possible solution to craft high-quality adversarial samples efficiently. 3.2.1 Word Replacement Strategy As seen in Figure 1, given a chosen word w to be replaced, we apply BERT to predict the possible words that are similar to w yet can mislead the target model. Instead of following the masked language model settings, we do not mask the chosen word w and use the original sequence as input, which can generate more semantic-consistent substitutes (Zhou et al., 2019). For instance, given a sequence ”I like the cat.”, if we mask the word cat, it would be very hard for a masked language model to predict the original word cat since it could be just as fluent if the sequence is ”I like the dog.”. Further, if we mask out the given word w, for each iteration we would have to rerun the masked language model prediction process which is costly. Since BERT uses Bytes-Pair-Encoding (BPE) to tokenize the sequence S = [w0 , · · · , wi , · · · ] into sub-word tokens: H = [h0 , h1 , h2 , · · · ], we need to align the chosen word to its corresponding sub-words in BERT. L"
2020.emnlp-main.500,D14-1162,0,0.0869799,"s to find a minimal perturbation that maximizes the risk of making wrong predictions. This minimax problem can be easily achieved by applying gradient descent over the continuous space of images (Miyato et al., 2017). However, adversarial attack for discrete data such as text remains challenging. Adversarial Attack for Text Current successful attacks for text usually adopt heuristic rules to modify the characters of a word (Jin et al., 2019), and substituting words with synonyms (Ren et al., 2019). Li et al. (2018); Gao et al. (2018) apply perturbations based on word embeddings such as Glove (Pennington et al., 2014), which is not strictly semantically and grammatically coordinated. Alzantot et al. (2018) adopts language models to score the perturbations generated by searching for close meaning words in the word embedding space (Mrkˇsi´c et al., 2016), using a trial and error process to find possible perturbations, yet the perturbations generated are still not contextaware and heavily rely on cosine similarity measurement of word embeddings. Glove embeddings do not guarantee similar vector space with cosine similarity distance, therefore the perturbations are less semantically consistent. Jin et al. (2019"
2020.emnlp-main.500,N16-1018,0,\N,Missing
2020.emnlp-main.500,N19-1423,0,\N,Missing
2020.findings-emnlp.260,P15-1168,1,0.954802,"se and has an excellent transfer capability. Experiments on eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extra"
2020.findings-emnlp.260,D15-1141,1,0.916429,"Missing"
2020.findings-emnlp.260,P17-1110,1,0.894397,"our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extract the criteriainvariant features, and a private layer is used to extract the criteria-specific features. How"
2020.findings-emnlp.260,I05-3017,0,0.72965,"Missing"
2020.findings-emnlp.260,2020.coling-main.186,0,0.217287,"Missing"
2020.findings-emnlp.260,I08-4010,0,0.288022,"Missing"
2020.findings-emnlp.260,P17-1111,0,0.0281055,"eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extract the criteriainvariant features, and a private layer is use"
2020.findings-emnlp.260,N15-1142,0,0.0708094,"Missing"
2020.findings-emnlp.260,D18-1529,0,0.582053,"Missing"
2020.findings-emnlp.260,I17-1018,0,0.0637148,"he only difference for each criterion is that a unique token is taken as input to specify the target criterion, which makes the shared encoder to capture the criterion-aware representation. Figure 2 illustrates the difference between our proposed model and the previous models. A more detailed architecture for MCCWS is shown in Figure 3. 3.1 y3 y4 CRF/MLP ˜0 h ˜1 h ˜2 h ˜3 h ˜4 h Transformer Encoder Encoder h0 h1 h2 h3 h4 m x1 x2 x3 x4 Embedding Input Figure 3: Proposed Model for MCCWS. between different criteria in the latent embedding space. 2) Bigram Embedding: Based on (Chen et al., 2015b; Shao et al., 2017; Zhang et al., 2018), the character-level bigram features can significantly benefit the task of CWS. Following their settings, we also introduce the bigram embedding to augment the character-level unigram embedding. The representation of character xt is e0xt = F C(ext ⊕ ext−1 xt ⊕ ext xt+1 ), (13) where e denotes the d-dimensional embedding vector for the unigram and bigram, ⊕ is the concatenation operator, and FC is a fully connected layer to map the concatenated character embedding with the dimension 3d into the embedding e0xt ∈ Rdmodel . 3) Position Embedding: To capture the order informat"
2020.findings-emnlp.260,I17-1017,0,0.0912977,"ity. Experiments on eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extract the criteriainvariant features, and a"
2020.findings-emnlp.260,P16-2092,0,0.0189251,"sent to conditional random fields (CRF) (Lafferty et al., 2001) layer or multi-layer perceptron (MLP) for tag inference. When using CRF as decoding layer, p(Y |X) in Eq (1) could be formalized as: Ψ(Y |X) , 0 0 Y ∈Ln Ψ(Y |X) p(Y |X) = P Ψ(Y |X) = ψ(X, t, yt−1 , yt ), (4) (5) where by0 y ∈ R is trainable parameters respective to label pair (y 0 , y), score function δ(X, t) ∈ R|L| calculates scores of each label for tagging the t-th character: δ(X, t) = Wδ> ht + bδ , ∀t ∈ [1, T ] (7) where θd denotes all the parameters in MLP layer. Most current state-of-the-art CWS models (Chen et al., 2015a; Xu and Sun, 2016; Liu et al., 2016; Yang et al., 2018; Qun et al., 2020) mainly focus on single-criterion CWS (SCCWS). Figure 2a shows the architecture of SCCWS. 2.2 MCCWS with Multi-Task Learning To improve the performance of CWS by exploiting multiple heterogeneous criteria corpora, Chen et al. (2017) utilize the multi-task learning framework to model the shared information among these different criteria. Formally, assuming that there are M corpora with heterogeneous segmentation criteria, we refer Dm as corpus m with Nm samples: m Dm = {(Xn(m) , Yn(m) )}N n=1 , (m) t=2 ψ(x, t, y 0 , y) = exp(δ(X, t)y + by0"
2020.findings-emnlp.388,K19-1015,0,0.364121,"semantic embedding space and return the word which is closest to the description (Hill et al., 2016; Zhang et al., 2019). Although current neural methods can extract the semantic representations of the descriptions and words, they have three challenging issues: (1) The first issue is the data sparsity. It is hard to learn good embeddings for the low-frequent words; (2) The second issue is polysemy. The previous methods usually use the static word embedding (Mikolov et al., 2013; Pennington et al., 2014), making them struggle to find the target word when the target word is polysemous. Pilehvar (2019) used different word senses to represent a word. Nonetheless, gathering senses for all words is not easy; (3) The third issue is the alignment of crosslingual word embeddings in the cross-lingual re4329 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4329–4338 c November 16 - 20, 2020. 2020 Association for Computational Linguistics verse dictionary scenario (Hill et al., 2016; Chen et al., 2019). In this paper, we leverage the pre-trained masked language model BERT (Devlin et al., 2019) to tackle the above issues. Firstly, since BERT tokenizes the words into subwor"
2020.findings-emnlp.388,N19-1423,0,0.52963,"4), making them struggle to find the target word when the target word is polysemous. Pilehvar (2019) used different word senses to represent a word. Nonetheless, gathering senses for all words is not easy; (3) The third issue is the alignment of crosslingual word embeddings in the cross-lingual re4329 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4329–4338 c November 16 - 20, 2020. 2020 Association for Computational Linguistics verse dictionary scenario (Hill et al., 2016; Chen et al., 2019). In this paper, we leverage the pre-trained masked language model BERT (Devlin et al., 2019) to tackle the above issues. Firstly, since BERT tokenizes the words into subwords with byte-pairencoding (BPE) (Sennrich et al., 2016b), the common subwords between low-frequent and highfrequent words can alleviate the data sparsity problem. Secondly, BERT can output contextualized representation for a word. Thus the polysemy problem can be much relieved. Thirdly, the mBERT is suitable to tackle the cross-lingual reverse dictionary. Because BERT shares some subwords between different languages, there is no need to align different languages explicitly. Therefore, we formulate the reverse dicti"
2020.findings-emnlp.388,W19-0421,0,0.0349069,"Missing"
2020.findings-emnlp.388,Q16-1002,0,0.302928,"ary with one subword embedding, and the alignment between languages is not necessary. More importantly, mBERT can achieve remarkable cross-lingual reverse dictionary performance even without the parallel corpus, which means it can conduct the cross-lingual reverse dictionary with only corresponding monolingual data. Code is publicly available at https://github.com/ yhcc/BertForRD.git. 1 A French word describes the newly introduced plant species in an area. néophyte Figure 1: An example of the monolingual and crosslingual reverse dictionary. Introduction Reverse dictionary (Bilac et al., 2004; Hill et al., 2016) is the task to find the proper target word given the word description. Fig. 1 shows an example of the monolingual and the cross-lingual reverse dictionary. Reverse dictionary should be a useful tool to help writers, translators, and new language learners find a proper word when encountering the tip-of-the-tongue problem (Brown and McNeill, 1966). Moreover, the reverse dictionary can be used for educational evaluation. For example, teachers can ask the students to describe a ∗ Corresponding author. Reverse Dictionary word, and the correct description should make the reverse dictionary model re"
2020.findings-emnlp.388,D18-1221,0,0.0830858,"ferent similarity metrics can be used to calculate the distance. Shaw et al. (2013) enhanced the retrieval system with WordNet (Miller, 1995). Hill et al. (2016) was the first to apply RNN into the reverse dictionary task, making the model free of handcrafted features. After encoding the definition into a dense vector, this vector is used to find its nearest neighbor word. This model formulation has been adopted in several papers (Pilehvar, 2019; Chen et al., 2019; Zhang et al., 2019; Morinaga and Yamaguchi, 2018; Hedderich et al., 2019), their difference lies in usage of different resources. Kartsaklis et al. (2018); Thorat and Choudhari (2016) used WordNet to form graphs to tackle the reverse dictionary task. The construction of the bilingual reverse dictionary has been studied in (Gollins and Sanderson, 2001; Lam and Kalita, 2013). Lam and Kalita (2013) relied on the availability of lexical resources, such as WordNet, to build a bilingual reverse dictionary. Chen et al. (2019) built several bilingual reverse dictionaries based on the Wiktionary1 , but this kind of online data cannot ensure the data’s quality. Building a bilingual reverse dictionary is not an easy task, and it will be even harder for lo"
2020.findings-emnlp.388,N13-1057,0,0.0196796,"aking the model free of handcrafted features. After encoding the definition into a dense vector, this vector is used to find its nearest neighbor word. This model formulation has been adopted in several papers (Pilehvar, 2019; Chen et al., 2019; Zhang et al., 2019; Morinaga and Yamaguchi, 2018; Hedderich et al., 2019), their difference lies in usage of different resources. Kartsaklis et al. (2018); Thorat and Choudhari (2016) used WordNet to form graphs to tackle the reverse dictionary task. The construction of the bilingual reverse dictionary has been studied in (Gollins and Sanderson, 2001; Lam and Kalita, 2013). Lam and Kalita (2013) relied on the availability of lexical resources, such as WordNet, to build a bilingual reverse dictionary. Chen et al. (2019) built several bilingual reverse dictionaries based on the Wiktionary1 , but this kind of online data cannot ensure the data’s quality. Building a bilingual reverse dictionary is not an easy task, and it will be even harder for low-resource language. Other than the low-quality problem, the vast number of language pairs is also a big obstacle, since if there are N languages, they will form N 2 pairs. However, by the unaligned cross-lingual reverse"
2020.findings-emnlp.388,2020.acl-main.611,1,0.811281,"e polysemy also makes the unsupervised word alignment hard to solve this task (Lample et al., 2018b). Last but not least, the pre-trained language model BERT has been extensively exploited in the Natural Language Processing (NLP) community since its introduction (Devlin et al., 2019; Conneau and Lample, 2019). Owing to BERT’s ability to extract contextualized information, BERT has been successfully utilized to enhance various tasks substantially, such as the aspect-based sentiment analysis task (Sun et al., 2019), summarization (Zhong et al., 2019), named entity recognition (Yan et al., 2019; Li et al., 2020) and Chinese dependency parsing (Yan et al., 2020). However, most works used BERT as an encoder, and less work uses BERT to do generation (Wang and Cho, 2019; Conneau and Lample, 2019). Wang and Cho (2019) showed that BERT is a Markov random field language model. Therefore, sentences can be sampled from BERT. Conneau and Lample (2019) used pre-trained BERT to initialize the unsupervised machine training model an achieve good performance. Different from these work, although a word might contain several subwords, we use a simple but effective method to make BERT generate the word ranking list wi"
2020.findings-emnlp.388,2021.ccl-1.108,0,0.111984,"Missing"
2020.findings-emnlp.388,D14-1162,0,0.1053,"query. Recent methods usually adopt neural networks to encode the description and the candidate words into the same semantic embedding space and return the word which is closest to the description (Hill et al., 2016; Zhang et al., 2019). Although current neural methods can extract the semantic representations of the descriptions and words, they have three challenging issues: (1) The first issue is the data sparsity. It is hard to learn good embeddings for the low-frequent words; (2) The second issue is polysemy. The previous methods usually use the static word embedding (Mikolov et al., 2013; Pennington et al., 2014), making them struggle to find the target word when the target word is polysemous. Pilehvar (2019) used different word senses to represent a word. Nonetheless, gathering senses for all words is not easy; (3) The third issue is the alignment of crosslingual word embeddings in the cross-lingual re4329 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4329–4338 c November 16 - 20, 2020. 2020 Association for Computational Linguistics verse dictionary scenario (Hill et al., 2016; Chen et al., 2019). In this paper, we leverage the pre-trained masked language model BERT (De"
2020.findings-emnlp.388,N19-1222,0,0.116378,"the same semantic embedding space and return the word which is closest to the description (Hill et al., 2016; Zhang et al., 2019). Although current neural methods can extract the semantic representations of the descriptions and words, they have three challenging issues: (1) The first issue is the data sparsity. It is hard to learn good embeddings for the low-frequent words; (2) The second issue is polysemy. The previous methods usually use the static word embedding (Mikolov et al., 2013; Pennington et al., 2014), making them struggle to find the target word when the target word is polysemous. Pilehvar (2019) used different word senses to represent a word. Nonetheless, gathering senses for all words is not easy; (3) The third issue is the alignment of crosslingual word embeddings in the cross-lingual re4329 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4329–4338 c November 16 - 20, 2020. 2020 Association for Computational Linguistics verse dictionary scenario (Hill et al., 2016; Chen et al., 2019). In this paper, we leverage the pre-trained masked language model BERT (Devlin et al., 2019) to tackle the above issues. Firstly, since BERT tokenizes the words into subwor"
2020.findings-emnlp.388,P16-1009,0,0.203519,"present a word. Nonetheless, gathering senses for all words is not easy; (3) The third issue is the alignment of crosslingual word embeddings in the cross-lingual re4329 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4329–4338 c November 16 - 20, 2020. 2020 Association for Computational Linguistics verse dictionary scenario (Hill et al., 2016; Chen et al., 2019). In this paper, we leverage the pre-trained masked language model BERT (Devlin et al., 2019) to tackle the above issues. Firstly, since BERT tokenizes the words into subwords with byte-pairencoding (BPE) (Sennrich et al., 2016b), the common subwords between low-frequent and highfrequent words can alleviate the data sparsity problem. Secondly, BERT can output contextualized representation for a word. Thus the polysemy problem can be much relieved. Thirdly, the mBERT is suitable to tackle the cross-lingual reverse dictionary. Because BERT shares some subwords between different languages, there is no need to align different languages explicitly. Therefore, we formulate the reverse dictionary task into the masked language model framework and use BERT to deal with the reverse dictionary task in monolingual and cross-lin"
2020.findings-emnlp.388,P16-1162,0,0.222451,"present a word. Nonetheless, gathering senses for all words is not easy; (3) The third issue is the alignment of crosslingual word embeddings in the cross-lingual re4329 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4329–4338 c November 16 - 20, 2020. 2020 Association for Computational Linguistics verse dictionary scenario (Hill et al., 2016; Chen et al., 2019). In this paper, we leverage the pre-trained masked language model BERT (Devlin et al., 2019) to tackle the above issues. Firstly, since BERT tokenizes the words into subwords with byte-pairencoding (BPE) (Sennrich et al., 2016b), the common subwords between low-frequent and highfrequent words can alleviate the data sparsity problem. Secondly, BERT can output contextualized representation for a word. Thus the polysemy problem can be much relieved. Thirdly, the mBERT is suitable to tackle the cross-lingual reverse dictionary. Because BERT shares some subwords between different languages, there is no need to align different languages explicitly. Therefore, we formulate the reverse dictionary task into the masked language model framework and use BERT to deal with the reverse dictionary task in monolingual and cross-lin"
2020.findings-emnlp.388,N19-1035,1,0.757779,"hand, a description can correspond to several 4330 1 https://www.wiktionary.org/ similar terms. The polysemy also makes the unsupervised word alignment hard to solve this task (Lample et al., 2018b). Last but not least, the pre-trained language model BERT has been extensively exploited in the Natural Language Processing (NLP) community since its introduction (Devlin et al., 2019; Conneau and Lample, 2019). Owing to BERT’s ability to extract contextualized information, BERT has been successfully utilized to enhance various tasks substantially, such as the aspect-based sentiment analysis task (Sun et al., 2019), summarization (Zhong et al., 2019), named entity recognition (Yan et al., 2019; Li et al., 2020) and Chinese dependency parsing (Yan et al., 2020). However, most works used BERT as an encoder, and less work uses BERT to do generation (Wang and Cho, 2019; Conneau and Lample, 2019). Wang and Cho (2019) showed that BERT is a Markov random field language model. Therefore, sentences can be sampled from BERT. Conneau and Lample (2019) used pre-trained BERT to initialize the unsupervised machine training model an achieve good performance. Different from these work, although a word might contain sev"
2020.findings-emnlp.388,C16-1263,0,0.0175459,"can be used to calculate the distance. Shaw et al. (2013) enhanced the retrieval system with WordNet (Miller, 1995). Hill et al. (2016) was the first to apply RNN into the reverse dictionary task, making the model free of handcrafted features. After encoding the definition into a dense vector, this vector is used to find its nearest neighbor word. This model formulation has been adopted in several papers (Pilehvar, 2019; Chen et al., 2019; Zhang et al., 2019; Morinaga and Yamaguchi, 2018; Hedderich et al., 2019), their difference lies in usage of different resources. Kartsaklis et al. (2018); Thorat and Choudhari (2016) used WordNet to form graphs to tackle the reverse dictionary task. The construction of the bilingual reverse dictionary has been studied in (Gollins and Sanderson, 2001; Lam and Kalita, 2013). Lam and Kalita (2013) relied on the availability of lexical resources, such as WordNet, to build a bilingual reverse dictionary. Chen et al. (2019) built several bilingual reverse dictionaries based on the Wiktionary1 , but this kind of online data cannot ensure the data’s quality. Building a bilingual reverse dictionary is not an easy task, and it will be even harder for low-resource language. Other th"
2020.findings-emnlp.388,W19-2304,0,0.0952665,"RT has been extensively exploited in the Natural Language Processing (NLP) community since its introduction (Devlin et al., 2019; Conneau and Lample, 2019). Owing to BERT’s ability to extract contextualized information, BERT has been successfully utilized to enhance various tasks substantially, such as the aspect-based sentiment analysis task (Sun et al., 2019), summarization (Zhong et al., 2019), named entity recognition (Yan et al., 2019; Li et al., 2020) and Chinese dependency parsing (Yan et al., 2020). However, most works used BERT as an encoder, and less work uses BERT to do generation (Wang and Cho, 2019; Conneau and Lample, 2019). Wang and Cho (2019) showed that BERT is a Markov random field language model. Therefore, sentences can be sampled from BERT. Conneau and Lample (2019) used pre-trained BERT to initialize the unsupervised machine training model an achieve good performance. Different from these work, although a word might contain several subwords, we use a simple but effective method to make BERT generate the word ranking list with only one forward pass. 3 Methodology The reverse dictionary task is to find the target word w given its definition d = [w1 , w2 , . . . , wn ], where d an"
2020.findings-emnlp.388,2020.tacl-1.6,1,0.833183,"ment hard to solve this task (Lample et al., 2018b). Last but not least, the pre-trained language model BERT has been extensively exploited in the Natural Language Processing (NLP) community since its introduction (Devlin et al., 2019; Conneau and Lample, 2019). Owing to BERT’s ability to extract contextualized information, BERT has been successfully utilized to enhance various tasks substantially, such as the aspect-based sentiment analysis task (Sun et al., 2019), summarization (Zhong et al., 2019), named entity recognition (Yan et al., 2019; Li et al., 2020) and Chinese dependency parsing (Yan et al., 2020). However, most works used BERT as an encoder, and less work uses BERT to do generation (Wang and Cho, 2019; Conneau and Lample, 2019). Wang and Cho (2019) showed that BERT is a Markov random field language model. Therefore, sentences can be sampled from BERT. Conneau and Lample (2019) used pre-trained BERT to initialize the unsupervised machine training model an achieve good performance. Different from these work, although a word might contain several subwords, we use a simple but effective method to make BERT generate the word ranking list with only one forward pass. 3 Methodology The revers"
2020.findings-emnlp.388,P19-1100,1,0.831772,"d to several 4330 1 https://www.wiktionary.org/ similar terms. The polysemy also makes the unsupervised word alignment hard to solve this task (Lample et al., 2018b). Last but not least, the pre-trained language model BERT has been extensively exploited in the Natural Language Processing (NLP) community since its introduction (Devlin et al., 2019; Conneau and Lample, 2019). Owing to BERT’s ability to extract contextualized information, BERT has been successfully utilized to enhance various tasks substantially, such as the aspect-based sentiment analysis task (Sun et al., 2019), summarization (Zhong et al., 2019), named entity recognition (Yan et al., 2019; Li et al., 2020) and Chinese dependency parsing (Yan et al., 2020). However, most works used BERT as an encoder, and less work uses BERT to do generation (Wang and Cho, 2019; Conneau and Lample, 2019). Wang and Cho (2019) showed that BERT is a Markov random field language model. Therefore, sentences can be sampled from BERT. Conneau and Lample (2019) used pre-trained BERT to initialize the unsupervised machine training model an achieve good performance. Different from these work, although a word might contain several subwords, we use a simple but e"
2020.tacl-1.6,P12-1110,0,0.147286,"s into a unified graph-based parsing framework. Because the segmentation is a characterlevel task and dependency parsing is a word-level task, we first formulate these two tasks into a character-level graph-based parsing framework. In detail, our model contains (1) a deep neural network encoder, which can capture the longterm contextual features for each character— it can be a multi-layer BiLSTM or pre-trained BERT, (2) a biaffine attentional scorer (Dozat and Manning, 2017), which unifies segmentation and dependency relations at the character level. Besides, unlike the previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., • As an added bonus, our proposed model can directly utilize the pre-trained language model BERT (Devlin et al., 2019) to boost performance significantly. The performance of many NLP tasks can be significantly enhanced when BERT was combined (Sun et al., 2019; Zhong et al., 2019). However, for Chinese, BERT is based on Chinese characters, whereas dependency parsing is conducted in the wordlevel. We cannot directly utilize BERT to enhance the word-level Chinese dependency parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to i"
2020.tacl-1.6,P82-1020,0,0.777368,"Missing"
2020.tacl-1.6,D15-1141,1,0.780869,"ilayer BiLSTM, the second one is the pre-trained language model BERT (Devlin et al., 2019) which is based on self-attention. 3.1.1 BiLSTM-based Encoding Layer Given a character sequence X = {x1 , . . . , xN }, in neural models, the first step is to map discrete language symbols into distributed embedding space. Formally, each character xi is mapped as ei ∈ Rde ⊂ E, where de is a hyper-parameter indicating the size of character embedding, and E is the embedding matrix. Character bigrams and trigrams have been shown highly effective for CWS and POS tagging in previous studies (Pei et al., 2014; Chen et al., 2015; Shao et al., 2017; Zhang et al., 2018). Following their settings, we combine the character bigram and trigram to enhance the representation of each character. The 3.1 Encoding Layer The encoding layer is responsible for converting discrete characters into contextualized dense rep81 final character representation of xi is given by ei = exi ⊕ exi xi+1 ⊕ exi xi+1 xi+2 , where e denotes the embedding for unigram, bigram, and trigram, and ⊕ is the concatenation operator. To capture the long-term contextual information, we use a deep BiLSTM (Hochreiter and Schmidhuber, 1997) to incorporate informa"
2020.tacl-1.6,Q16-1023,0,0.0553463,"feature engineering. These transition-based joint models rely on a detailed handcrafted feature. Although Kurita et al. (2017) introduced neural models to reduce partial efforts of feature engineering, they still require hard work on how to design and compose the word-based features from the stack and the character-based features from the buffer. • In experiments on datasets CTB-5, CTB-7, and CTB-9, our model achieves state-ofthe-art score in joint CWS and dependency parsing, even without the POS information. Recently, graph-based models have made significant progress for dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017), which fully exploit the ability of the bidirectional long short-term memory network (BiLSTM) (Hochreiter and Schmidhuber, 1997) and attention mechanism (Bahdanau et al., 2015) to capture the interactions of words in a sentence. Different from the transition-based models, the graph-based models assign a score or probability to each possible arc and then construct a maximum spanning tree from these weighted arcs. In this paper, we propose a joint model for CWS and dependency parsing that integrates these two tasks into a unified graph-based parsing framework. Because"
2020.tacl-1.6,N19-1423,0,0.684533,"rmulate these two tasks into a character-level graph-based parsing framework. In detail, our model contains (1) a deep neural network encoder, which can capture the longterm contextual features for each character— it can be a multi-layer BiLSTM or pre-trained BERT, (2) a biaffine attentional scorer (Dozat and Manning, 2017), which unifies segmentation and dependency relations at the character level. Besides, unlike the previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., • As an added bonus, our proposed model can directly utilize the pre-trained language model BERT (Devlin et al., 2019) to boost performance significantly. The performance of many NLP tasks can be significantly enhanced when BERT was combined (Sun et al., 2019; Zhong et al., 2019). However, for Chinese, BERT is based on Chinese characters, whereas dependency parsing is conducted in the wordlevel. We cannot directly utilize BERT to enhance the word-level Chinese dependency parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to implement CWS and dependency parsing jointly. 2 Related Work To reduce the problem of error propagation and improve the low-level tasks by incorporating the"
2020.tacl-1.6,P14-1028,0,0.185099,"parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to implement CWS and dependency parsing jointly. 2 Related Work To reduce the problem of error propagation and improve the low-level tasks by incorporating the knowledge from the high-level tasks, many successful joint methods have been proposed to simultaneously solve related tasks, which can be categorized into three types. 2.1 Joint Segmentation and POS Tagging Because segmentation is a character-level task and POS tagging is a word-level task, an intuitive idea 79 segmentation and POS tagging. Zhang et al. (2014) expanded this work by using intra-character structures of words and found the intra-character dependencies were helpful in word segmentation and POS tagging. Zhang et al. (2015) proposed joint segmentation, POS tagging, and dependency re-ranking system. This system required a base parser to generate some candidate parsing results. Kurita et al. (2017) followed the work of Hatori et al. (2012); Zhang et al. (2014) and used the BiLSTM to extract features with n-gram character string embeddings as input. A related work is the full character-level neural dependency parser (Li et al., 2018), but i"
2020.tacl-1.6,D12-1046,0,0.0491387,"he work of Hatori et al. (2012); Zhang et al. (2014) and used the BiLSTM to extract features with n-gram character string embeddings as input. A related work is the full character-level neural dependency parser (Li et al., 2018), but it focuses on character-level parsing without considering the word segmentation and word-level POS tagging and parsing. Although a heuristic method could transform the character-level parsing results to word-level, the transform strategy is tedious and the result is also worse than other joint models. Besides, there are some joint models for constituency parsing. Qian and Liu (2012) proposed a joint inference model for word segmentation, POS tagging, and constituency parsing. However, their model did not train three tasks jointly and suffered from the decoding complexity due to the large combined search space. Wang et al. (2013) first segmented a Chinese sentence into a word lattice, and then predicted the POS tags and parsed tree based on the word lattice. A dual decomposition method was used to encourage the tagger and parser to predict agreed structures. The above methods show that syntactic parsing can provide useful feedback to word segmentation and POS tagging and"
2020.tacl-1.6,D11-1109,0,0.153924,"l. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model. Hatori et al. (2012) proposed a transition-based joint POS tagging and dependency parsing model and showed that the joint approach improved the accuracies of these two tasks. Yang et al. (2018) extended this model by neural models to alleviate the efforts of feature engineering. Li et al. (2011) utilized the graph-based model to jointly optimize POS tagging and dependency parsing in a unique model. They also proposed an effective POS tag pruning method that could greatly improve the decoding efficiency. By combining the lexicality and syntax into a unified framework, joining POS tagging and dependency parsing can improve both tagging and parsing performance over independent modeling significantly. 3 Proposed Model Previous joint methods are mainly based on the transition-based model, which modifies the standard ‘‘shift-reduce’’ operations by adding some extra operations, such as ‘‘ap"
2020.tacl-1.6,D13-1062,1,0.550848,"d measures of word-level F1, precision, and recall scores to evaluate word segmentation and dependency parsing (for both unlabeled and labeled scenario) tasks. We detail them in the following. tree based on the predicted character-level arc labels. The characters with continuous ‘‘app’’ are regarded as one word. And the predicted head character of the last character is viewed as this word’s head. Because the predicted arc points to a character, we regard the word that contains this head character as the head word. • F 1seg : F1 measure of CWS. This is the standard metric used in the CWS task (Qiu et al., 2013; Chen et al., 2017). • F 1udep : F1 measure of unlabeled dependency parsing. Following Hatori et al. (2012), Zhang et al. (2014, 2015), and Kurita et al. (2017), we use standard measures of word-level F1, precision, and recall score to evaluate dependency parsing. In the scenario of joint word segmentation and dependency parsing, the widely used unlabeled attachment score (UAS) is not enough to measure the performance, since the error arises from two aspects: One is caused by word segmentation and the other is due to the wrong prediction on the head word. A dependent-head pair is correct only"
2020.tacl-1.6,N15-1142,0,0.0180907,"n K¨ubler et al. (2009). The U AS , LAS equal to the value of the recall of unlabeled dependency parsing (Rudep ) and the recall of labeled dependency parsing (Rldep ), respectively. We also report these two values in our experiments. 4.3 Experimental Settings Pre-trained Embedding Based on Shao et al. (2017); Zhang et al. (2018), n-grams are of great benefit to CWS and POS tagging tasks. Thus we use unigram, bigram, and trigram embeddings for all of our character-based models. We first pre-train unigram, bigram, and trigram embeddings on the Chinese Wikipedia corpus by the method proposed in Ling et al. (2015), which improves standard Word2Vec by incorporating token order information. For a sentence with characters ‘‘abcd...’’, the unigram sequence is ‘‘a b c ...’’; the bigram sequence is ‘‘ab bc cd ...’’; and the trigram sequence is ‘‘abc bcd ...’’. For our word dependency parser, we use Tencent’s pre-trained word embeddings (Song et al., 2018). Because Tencent’s pre-trained word embedding dimension is 200, we set both pre-trained and random word embedding dimension as 200 for all of our word dependency parsing models. All pre-trained embeddings are fixed during our experiments. In addition to the"
2020.tacl-1.6,I17-1018,0,0.336536,"sub-tasks. Moreover, there is no related work on joint Chinese word segmentation and dependency parsing, without POS tagging. is to transfer both the tasks into character-level and incorporate them in a uniform framework. A popular method is to assign a cross-tag to each character (Ng and Low, 2004). The crosstag is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model"
2020.tacl-1.6,D17-1002,0,0.0712813,"rst graph-based method to integrate CWS and dependency parsing both in the training phase and the decoding phase. The proposed model is very concise and easily implemented. • Compared with the previous transition-based joint models, our proposed model is a graphbased model, which results in fewer efforts of feature engineering. Additionally, our model can deal with the labeled dependency parsing task, which is not easy for transition-based joint models. (2) The second is the feature engineering. These transition-based joint models rely on a detailed handcrafted feature. Although Kurita et al. (2017) introduced neural models to reduce partial efforts of feature engineering, they still require hard work on how to design and compose the word-based features from the stack and the character-based features from the buffer. • In experiments on datasets CTB-5, CTB-7, and CTB-9, our model achieves state-ofthe-art score in joint CWS and dependency parsing, even without the POS information. Recently, graph-based models have made significant progress for dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017), which fully exploit the ability of the bidirectional long short-term"
2020.tacl-1.6,D18-1529,0,0.515377,"g is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model. Hatori et al. (2012) proposed a transition-based joint POS tagging and dependency parsing model and showed that the joint approach improved the accuracies of these two tasks. Yang et al. (2018) extended this model by neural models to alleviate the efforts of feature engineering. Li et al. (2011) ut"
2020.tacl-1.6,N18-2028,0,0.0212763,"re of great benefit to CWS and POS tagging tasks. Thus we use unigram, bigram, and trigram embeddings for all of our character-based models. We first pre-train unigram, bigram, and trigram embeddings on the Chinese Wikipedia corpus by the method proposed in Ling et al. (2015), which improves standard Word2Vec by incorporating token order information. For a sentence with characters ‘‘abcd...’’, the unigram sequence is ‘‘a b c ...’’; the bigram sequence is ‘‘ab bc cd ...’’; and the trigram sequence is ‘‘abc bcd ...’’. For our word dependency parser, we use Tencent’s pre-trained word embeddings (Song et al., 2018). Because Tencent’s pre-trained word embedding dimension is 200, we set both pre-trained and random word embedding dimension as 200 for all of our word dependency parsing models. All pre-trained embeddings are fixed during our experiments. In addition to the fixed pre-trained embeddings, we also randomly initialize embeddings, and elementwisely add the pre-trained and random embeddings before other procedures. For a model with BERT encoding layer, we use the Chinese BERTbase released in Cui et al. (2019). 100 400 5 128 0.33 0.33 0.33 0.33 3 1 500 100 2e-3 .75t/5000 0.9 100 Table 1: Hyper-param"
2020.tacl-1.6,N19-1035,1,0.897442,"Missing"
2020.tacl-1.6,P08-1101,0,0.0380035,"rporate them in a uniform framework. A popular method is to assign a cross-tag to each character (Ng and Low, 2004). The crosstag is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model. Hatori et al. (2012) proposed a transition-based joint POS tagging and dependency parsing model and showed that the joint approach improved the accuracies of these two tasks. Yang et al."
2020.tacl-1.6,D10-1082,0,0.0202611,"e just use Eq. (5) ∼ (8) to predict the labels: ‘‘seg’’ and ‘‘app’’. Thus, the word segmentation task is transformed into a binary classification problem. Figure 3 gives an illustration of the labeled arcs for the task of word segmentation only. 4 Experiments 4.1 Datasets We use the Penn Chinese Treebank 5.0 (CTB5),1 7.0 (CTB-7),2 and 9.0 (CTB-9)3 datasets to evaluate our models (Xue et al., 2005). For CTB-5, the training set is from sections 1∼270, 400∼931, and 1001∼1151, the development set is from section 301∼325, and the test set is from section 271∼300; this splitting was also adopted by Zhang and Clark (2010), Zhang et al. (2014), and Kurita et al. (2017). For CTB-7, we use the same split 1 https://catalog.ldc.upenn.edu/LDC2005T01. https://catalog.ldc.upenn.edu/LDC2010T07. 3 https://catalog.ldc.upenn.edu/LDC2016T13. 2 83 • F 1ldep : F1 measure of labeled dependency parsing. The only difference from F 1udep is that except for the match between the head and dependent words, the pair must have the same label as the golden dependent-head pair. The precision and recall are calculated correspondingly. Because the number of golden labeled dependent-head pairs and predicted labeled dependent-head pairs ar"
2020.tacl-1.6,I11-1035,0,0.0344541,"arc from xi (head) to xj (dependent). (arc−dep) (label−head) + W (label) (rij 3.2.1 Unlabeled Arc Prediction (arc−dep) rj (arc) sij (6) (label) To predict the relations of each character pair, we use the biaffine attention mechanism (Dozat and Manning, 2017) to score their probability on the top of encoding layers. According to Dozat and Manning (2017), biaffine attention is more effectively capable of measuring the relationship between two elementary units. = MLP(arc−head) (hi ), = MLP(label−dep) (hj ), = ri rij (arc−head) (5) (label) 3.2 Biaffine Layer ri = MLP(label−head) (hi ), (4) 82 as Wang et al. (2011), Zhang et al. (2014), and Kurita et al. (2017). For CTB-9, we use the dev and test files proposed by Shao et al. (2017), and we regard all left files as the training data. 4.2 Measures Figure 3: Label prediction for word segmentation only. The arc with ‘‘app’’ indicates its connected characters belong to a word, and the arc with ‘‘seg’’ indicates its connected characters belong to different words. Following Hatori et al. (2012), Zhang et al. (2014, 2015), and Kurita et al. (2017), we use standard measures of word-level F1, precision, and recall scores to evaluate word segmentation and depende"
2020.tacl-1.6,N15-1005,0,0.220987,"blem of error propagation and improve the low-level tasks by incorporating the knowledge from the high-level tasks, many successful joint methods have been proposed to simultaneously solve related tasks, which can be categorized into three types. 2.1 Joint Segmentation and POS Tagging Because segmentation is a character-level task and POS tagging is a word-level task, an intuitive idea 79 segmentation and POS tagging. Zhang et al. (2014) expanded this work by using intra-character structures of words and found the intra-character dependencies were helpful in word segmentation and POS tagging. Zhang et al. (2015) proposed joint segmentation, POS tagging, and dependency re-ranking system. This system required a base parser to generate some candidate parsing results. Kurita et al. (2017) followed the work of Hatori et al. (2012); Zhang et al. (2014) and used the BiLSTM to extract features with n-gram character string embeddings as input. A related work is the full character-level neural dependency parser (Li et al., 2018), but it focuses on character-level parsing without considering the word segmentation and word-level POS tagging and parsing. Although a heuristic method could transform the character-l"
2020.tacl-1.6,P13-2110,0,0.0822174,"acter-level parsing without considering the word segmentation and word-level POS tagging and parsing. Although a heuristic method could transform the character-level parsing results to word-level, the transform strategy is tedious and the result is also worse than other joint models. Besides, there are some joint models for constituency parsing. Qian and Liu (2012) proposed a joint inference model for word segmentation, POS tagging, and constituency parsing. However, their model did not train three tasks jointly and suffered from the decoding complexity due to the large combined search space. Wang et al. (2013) first segmented a Chinese sentence into a word lattice, and then predicted the POS tags and parsed tree based on the word lattice. A dual decomposition method was used to encourage the tagger and parser to predict agreed structures. The above methods show that syntactic parsing can provide useful feedback to word segmentation and POS tagging and the joint inference leads to improvements in all three sub-tasks. Moreover, there is no related work on joint Chinese word segmentation and dependency parsing, without POS tagging. is to transfer both the tasks into character-level and incorporate the"
2020.tacl-1.6,D13-1061,0,0.0358508,"inference leads to improvements in all three sub-tasks. Moreover, there is no related work on joint Chinese word segmentation and dependency parsing, without POS tagging. is to transfer both the tasks into character-level and incorporate them in a uniform framework. A popular method is to assign a cross-tag to each character (Ng and Low, 2004). The crosstag is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is mor"
2020.tacl-1.6,P19-1100,1,0.82266,"e longterm contextual features for each character— it can be a multi-layer BiLSTM or pre-trained BERT, (2) a biaffine attentional scorer (Dozat and Manning, 2017), which unifies segmentation and dependency relations at the character level. Besides, unlike the previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., • As an added bonus, our proposed model can directly utilize the pre-trained language model BERT (Devlin et al., 2019) to boost performance significantly. The performance of many NLP tasks can be significantly enhanced when BERT was combined (Sun et al., 2019; Zhong et al., 2019). However, for Chinese, BERT is based on Chinese characters, whereas dependency parsing is conducted in the wordlevel. We cannot directly utilize BERT to enhance the word-level Chinese dependency parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to implement CWS and dependency parsing jointly. 2 Related Work To reduce the problem of error propagation and improve the low-level tasks by incorporating the knowledge from the high-level tasks, many successful joint methods have been proposed to simultaneously solve related tasks, which can be categorized into three"
2020.webnlg-1.10,2020.webnlg-1.7,0,0.590708,"Missing"
2020.webnlg-1.10,D19-1052,0,0.021011,"Missing"
2020.webnlg-1.10,W18-6521,0,0.0136332,"sponds to the occurrence of their information in the text, the plan is learning in an autoregressive fashion. At each time step, the planner selects the node that represents the triplet that is the most likely to be next, and subsequently we re-run the R-GCN on the updated graph (where the selected triplet’s node feature will change). To train the planner, since it is not straightforward to obtain the order of RDF triplets verbalized in the text from the WebNLG 2020 dataset, we use an outside data source with annotated plans. We use the enriched version of WebNLG 2017, called enriched WebNLG (Ferreira et al., 2018, 2019).3 It consists of 5,152 training, 644 validation and 1,408 test samples. We only use the training set to train our planner, and reserve the validation set for choosing the best planner model. 3.2 Converting Plans to Sequences Since the raw output of the planner is an ordered list of triplets, we need to convert the raw plan into a sequential form that is friendly to the T5 as an input. To this end, the beginning of each sequence uses a new task-specific prefix, “Graph to Text:.” This serves as the the prefix to all training samples, and also to any subsequent test samples. Additionally,"
2020.webnlg-1.10,W17-4770,0,0.0359032,"e model from (Kale, 4 2020) as our Seq2Seq model. And our implementation is based on DGL (Wang et al., 2019), Pytorch,5 and Transformers (Wolf et al., 2019). The details of hyperparameters can be found in our open-sourced GitHub repository. 4.2 For the knowledge graph-to-text generation task, WebNLG 2020 evaluates the text quality by a range of different quantitative metrics (Moussalem et al., 2020) that are listed on an automatic evaluation leaderboard. For this purpose, the quality of textual outputs are assessed using BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), chrF++ (Popovic, 2017), and TER (Snover et al., 2006). In brief, these metrics quantify the n-gram recall, precision, or F-scores between the model outputs and the ground-truth references. Additionally, several BERT-based scores are also reported, including BERTPrecision , BERTRecall , and BERTF1 from Zhang et al. (2020), and BLUERT from Sellam et al. (2020). We present the performance of the top 10 systems and two official baselines on the leaderboard of WebNLG 2020 English RDF-to-text challenge in Table 2. Our P 2 model achieves the highest out of all systems on all automatic evaluation criteria, indicating highe"
2020.webnlg-1.10,P17-1017,0,0.337682,"ts: (1) a R-GCN planner from (Zhao et al., 2020), (2) a rule-based interface that converts plans into a Seq2Seq friendly format, (3) a pretrained T5 Seq2Seq model following the practice of Kale (2020), and (4) some canocalization rules to deal with special characters that are not contained with the dictionary of T5. We now describe each of these components in turn. WebNLG 2020 Challenge Overview The English version of WebNLG 2020 has two tasks: The first task aims to automate knowledge graph-to-text generation (G2T), which is an extension of the G2T task in the previous WebNLG Challenge 2017 (Gardent et al., 2017b) with broader categories of knowledge graph and text pairs. Given a knowledge graph of several (subject, predicate, object) triplets, participant systems need to generate verbalizations of the knowledge graph that maintain the same information but in natural language. The other task, text-to-knowledge graph extraction (T2G), involves extracting entities and relations from text to form a knowledge graph. In terms of data, the English WebNLG 2020 track (Castro-Ferreira et al., 2020) covers knowledge graphs and text from a variety of domains including Airport, Artist, Astronaut, Athlete, Buildi"
2020.webnlg-1.10,W17-3518,0,0.291937,"ts: (1) a R-GCN planner from (Zhao et al., 2020), (2) a rule-based interface that converts plans into a Seq2Seq friendly format, (3) a pretrained T5 Seq2Seq model following the practice of Kale (2020), and (4) some canocalization rules to deal with special characters that are not contained with the dictionary of T5. We now describe each of these components in turn. WebNLG 2020 Challenge Overview The English version of WebNLG 2020 has two tasks: The first task aims to automate knowledge graph-to-text generation (G2T), which is an extension of the G2T task in the previous WebNLG Challenge 2017 (Gardent et al., 2017b) with broader categories of knowledge graph and text pairs. Given a knowledge graph of several (subject, predicate, object) triplets, participant systems need to generate verbalizations of the knowledge graph that maintain the same information but in natural language. The other task, text-to-knowledge graph extraction (T2G), involves extracting entities and relations from text to form a knowledge graph. In terms of data, the English WebNLG 2020 track (Castro-Ferreira et al., 2020) covers knowledge graphs and text from a variety of domains including Airport, Artist, Astronaut, Athlete, Buildi"
2020.webnlg-1.10,P00-1055,0,0.0269736,"CapitalOf, France). Since there can be multiple triplets (e.g., 2-7 triplets as in the WebNLG dataset), the planner takes in a random order of triplets 1 to N , and aims to find an ideal order of the triplets, such as “(s2 , p2 , o2 ) → (s3 , p3 , o3 ) → (s1 , p1 , o1 ).” The 101 3.3 ideal plan we want to generate corresponds to the order that the triplets are mentioned in the groundtruth text. As mentioned previously, to generate text from the plans obtained by the R-GCN planner, we adopt the powerful pretrained model T5 (Raffel et al., 2020), which is the state-of-the-art model (Kale, 2020; Ribeiro et al., 2000) on the WebNLG 2017 dataset (Gardent et al., 2017a), and often seen on other Seq2Seq tasks such as machine translation (Raffel et al., 2020). T5 uses the Transformer architecture (Vaswani et al., 2017) and is pretrained on several large corpora with carefully tuned hyperparameters (Raffel et al., 2020). The powerful pretraining of T5 has equipped it with strong text generation ability to verbalize the triplets in a fluent way, and also some generalizability that can help on unseen data. For present purposes, we finetune T5 for 10 epochs using the input sequences from Section 3.2. One potential"
2020.webnlg-1.10,2020.webnlg-1.8,1,0.70621,".956 0.959 0.954 0.940 0.949 0.949 0.933 0.957 0.955 0.954 0.955 0.954 0.954 0.954 0.950 0.946 0.949 0.950 0.941 0.958 0.956 0.954 0.956 0.954 0.954 0.956 0.951 0.943 0.948 0.949 0.932 0.62 0.61 0.58 0.6 0.6 0.59 0.61 0.57 0.45 0.54 0.55 0.50 Table 2: Performance of top 10 systems and two official baselines (Base1 and Base 2) on the leaderboard of the WebNLG 2020 English RDF-to-text challenge as ranked by METEOR. With all metrics, larger is better, with the exception of TER where lower is better. In all categories, our system achieved the first place. Note that ID28 is an unsupervised system (Guo et al., 2020). package.4 And for punctuations such as hyphens and quotation marks, the Unidecode package also canonicalizes them to the most standard ones. Apart from character canocalization, we can also potentially change units of measurement, e.g., “kg/m3 ,” to the corresponding textual equivalent, e.g., “kilogram per cubic meter,” so that the inputs will have more overlap with the corpora that T5 was originally trained on. For the current approach however, we did not actually use this additional preprocessing due to the time limit of the challenge. After the T5 model generates the output sequence that"
2020.webnlg-1.10,2020.inlg-1.14,0,0.428689,"model achieved the top-1 performance on all criteria of both the automatic and human evaluations of knowledge graphto-text generation task in English. 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), c Dublin, Ireland (Virtual), 18 December 2020, pages 100–106, 2020 Association for Computational Linguistics Attribution 4.0 International. • We open-sourced our code and outputs as a reference for future work. 2 we also adopt T5 as the backbone of our approach. To produce the input sequence to feed into T5, one direct way following Ribeiro et al. (2020); Kale (2020) is to “linearize” the graph by iterating the (subject, predicate, object) triplets in a random order, using special tokens to specify the subject, predicate, and object of the knowledge triplet. However, feeding the triplets in a random order can introduce burdens for the T5 Seq2Seq model, which must verbalize each triplet while organizing the information within a natural ordering. To assist T5 with the latter, we use a relational graph convolutional network-based planner (R-GCN planner) by Zhao et al. (2020) to learn the best order to linearize a graph. The overview of our P 2 framework is s"
2020.webnlg-1.10,P17-1099,0,0.0269028,"ge corpora with carefully tuned hyperparameters (Raffel et al., 2020). The powerful pretraining of T5 has equipped it with strong text generation ability to verbalize the triplets in a fluent way, and also some generalizability that can help on unseen data. For present purposes, we finetune T5 for 10 epochs using the input sequences from Section 3.2. One potential concern though is whether T5 can cope with the variety of entities in the WebNLG dataset, such as a long airport name “Adolfo Suárez Madrid—Barajas Airport.” Such a concern has motivated the previous invention of the copy mechanism (See et al., 2017), which models a switch to select between generating a new word or copying a word from the input text sequence. Despite this potential concern, we find that the T5 can handle the entities in an adept manner, generating complicated entities such as a long phone number without difficulty. Following (Zhao et al., 2020), we use an R-GCN (Schlichtkrull et al., 2018) to encode the knowledge graphs. Then, to arrange the triplets in the order that corresponds to the occurrence of their information in the text, the plan is learning in an autoregressive fashion. At each time step, the planner selects th"
2020.webnlg-1.10,P02-1040,0,0.106318,"he planner module based on Zhao et al. (2020), we adopt the T5-Large model from (Kale, 4 2020) as our Seq2Seq model. And our implementation is based on DGL (Wang et al., 2019), Pytorch,5 and Transformers (Wolf et al., 2019). The details of hyperparameters can be found in our open-sourced GitHub repository. 4.2 For the knowledge graph-to-text generation task, WebNLG 2020 evaluates the text quality by a range of different quantitative metrics (Moussalem et al., 2020) that are listed on an automatic evaluation leaderboard. For this purpose, the quality of textual outputs are assessed using BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), chrF++ (Popovic, 2017), and TER (Snover et al., 2006). In brief, these metrics quantify the n-gram recall, precision, or F-scores between the model outputs and the ground-truth references. Additionally, several BERT-based scores are also reported, including BERTPrecision , BERTRecall , and BERTF1 from Zhang et al. (2020), and BLUERT from Sellam et al. (2020). We present the performance of the top 10 systems and two official baselines on the leaderboard of WebNLG 2020 English RDF-to-text challenge in Table 2. Our P 2 model achieves the highest out of all sys"
2020.webnlg-1.10,2006.amta-papers.25,0,0.0259589,"0) as our Seq2Seq model. And our implementation is based on DGL (Wang et al., 2019), Pytorch,5 and Transformers (Wolf et al., 2019). The details of hyperparameters can be found in our open-sourced GitHub repository. 4.2 For the knowledge graph-to-text generation task, WebNLG 2020 evaluates the text quality by a range of different quantitative metrics (Moussalem et al., 2020) that are listed on an automatic evaluation leaderboard. For this purpose, the quality of textual outputs are assessed using BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), chrF++ (Popovic, 2017), and TER (Snover et al., 2006). In brief, these metrics quantify the n-gram recall, precision, or F-scores between the model outputs and the ground-truth references. Additionally, several BERT-based scores are also reported, including BERTPrecision , BERTRecall , and BERTF1 from Zhang et al. (2020), and BLUERT from Sellam et al. (2020). We present the performance of the top 10 systems and two official baselines on the leaderboard of WebNLG 2020 English RDF-to-text challenge in Table 2. Our P 2 model achieves the highest out of all systems on all automatic evaluation criteria, indicating higher similarity to the ground trut"
2020.webnlg-1.10,2020.emnlp-demos.6,0,0.0399364,"Missing"
2020.webnlg-1.10,2020.acl-main.224,0,0.502623,"rary (DGL) &lt;P&gt; FoundingYear &lt;O&gt; 2018 &lt;S&gt; Deep Graph Library (DGL) &lt;P&gt; version &lt;O&gt; 0.5 Pretrained Seq2Seq Text: Deep Graph Library (DGL) was initiated in 2018, and its newest version is 0.5. Figure 1: A diagram of our knowledge graph-to-text model. The P 2 model takes in the graph, then (1) linearizes it to a plan with the desired order of triplets by the R-GCN planner, and (2) uses the pretrained T5 model to learn the plan-to-text generation. For G2T, we introduce a plan-and-pretrain approach, P 2 , which uses a text planner based on relational graph convolutional networks (R-GCN) proposed by Zhao et al. (2020), and the pretrained T5 Seq2Seq model (Raffel et al., 2020), as shown in Figure 1. To further boost performance, our P 2 model also incorporates a mechanism to canonicalize the entities in WebNLG with special characters. Overall, our contributions are as follows: • We develop a plan-and-pretrain approach, P 2 , for knowledge graph-to-text generation. P 2 unleashes the power of the pretrained T5 model by pipelining it with a R-GCN content planner and special canonicalization rules. • Our P 2 model achieved the top-1 performance on all criteria of both the automatic and human evaluations of know"
2020.webnlg-1.8,D18-1217,0,0.0276106,"0.28 38.85 0.332 0.569 0.573 0.3 38.01 0.331 0.565 0.583 0.27 37.96 0.331 0.566 0.580 0.28 22.84 0.326 0.534 0.696 -0.03 36.07 0.324 0.555 0.599 0.24 36.6 0.322 0.554 0.594 0.25 31.26 0.316 0.542 0.659 0.31 27.5 0.305 0.519 0.846 0.03 28.71 0.243 0.448 0.689 -0.09 30.79 0.239 0.439 0.677 -0.09 23.08 0.225 0.421 0.743 -0.19 24.45 0.223 0.425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred"
2020.webnlg-1.8,D18-1549,0,0.0240687,"he intermediate model outputs are non-differentiable. For example, in the G-Cycle (graph→text→graph), the intermediate text is decoded in a discrete form to natural language. Hence, the graph-to-text part G2Tθ will not be differentiable, and the final loss can only be propagated to the latter part, text-to-graph T2Gϕ . Hence, when alternatively optimizing the two cycle losses, we first fix ϕ to optimize θ for the text cycle LCycT , and then fix θ to optimize ϕ for the graph cycle LCycG . Although an analogous non-differentability issue is shared by unsupervised NMT works (Lample et al., 2018; Artetxe et al., 2018), these approaches rely on other regularization factors such as autoencoder losses, adversarial losses, and warm start strategies. In contrast, our streamlined approach CycleGT relies solely on cycle training and the inductive biases of the T2G and G2T modules. GenWiki Dataset Apart from the WebNLG datasets that have to be processed to fit non-parallel setting of our framework, we also evaluate with a natural non-parallel dataset, GenWiki (Jin et al., 2020b). GenWiki is formed from DBpedia knowledge graphs, and unpaired natural text in Wikipedia articles with the same topics as the knowledge g"
2020.webnlg-1.8,P17-1017,0,0.570511,"sks (Koncel-Kedziorski et al., 2019; Moryossef et al., 2019; Luan et al., 2018). To circumvent the limitations of scarce supervised data, we formulate both tasks in a cycle training framework, and also in the unsupervised manner with fully non-parallel datasets (as shown in Figure 1). The technical difficulty lies in the different modality of text and graphs, which can be intractable in a joint learning setting. We contribute an effective learning framework, CycleGT, which is iteratively trained with two cycle losses. We first validate CycleGT on widely used WebNLG datasets, both WebNLG 2017 (Gardent et al., 2017b) and WebNLG+ 2020 (Castro-Ferreira et al., 2020) which is a more challenging extension of the 2017 dataset. Here CycleGT achieves performance that is comparable to many supervised baselines. Additionally, we also evaluate our model on the newly released GenWiki dataset (Jin et al., 2020b) with 1.3M non-parallel text and knowledge graphs. Our model is the best out of all unsupervised baselines, improving by +11.47 BLEU over the best baseline on GenWikiFINE data and +6.26 BLEU on GenWikiFULL . The surprisingly high performance of CycleGT indicates that it is an effective approach to address th"
2020.webnlg-1.8,W05-0909,0,0.0611299,"trics For G2T on WebNLG 2017 and GenWiki, we use the metrics that are mostly reported by other previous work (Moryossef et al., 2019) so that 2 WebNLG 2017 is the most appropriate dataset, because in other datasets such as relation extraction datasets (Walker et al., 2006), the graph only contain a very small subset of the information in the text. It can be downloaded from https://webnlg-challenge.loria. fr/challenge_2017/. 80 we can have head-to-head comparison with existing systems. Specifically, we adopt the metrics from (Moryossef et al., 2019), i.e., BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed text (model output) to the input text.3 Briefly, they measure the n-gram precision, recall, or F-scores between the model outputs and the (ground-truth) references. In contrast, for T2G evaluation on WebNLG 2017, we use the micro and macro F1 scores of the relation types of the edges, following the standard practice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboard (Moussalem et al., 2020). For G2T"
2020.webnlg-1.8,W17-3518,0,0.60917,"sks (Koncel-Kedziorski et al., 2019; Moryossef et al., 2019; Luan et al., 2018). To circumvent the limitations of scarce supervised data, we formulate both tasks in a cycle training framework, and also in the unsupervised manner with fully non-parallel datasets (as shown in Figure 1). The technical difficulty lies in the different modality of text and graphs, which can be intractable in a joint learning setting. We contribute an effective learning framework, CycleGT, which is iteratively trained with two cycle losses. We first validate CycleGT on widely used WebNLG datasets, both WebNLG 2017 (Gardent et al., 2017b) and WebNLG+ 2020 (Castro-Ferreira et al., 2020) which is a more challenging extension of the 2017 dataset. Here CycleGT achieves performance that is comparable to many supervised baselines. Additionally, we also evaluate our model on the newly released GenWiki dataset (Jin et al., 2020b) with 1.3M non-parallel text and knowledge graphs. Our model is the best out of all unsupervised baselines, improving by +11.47 BLEU over the best baseline on GenWikiFINE data and +6.26 BLEU on GenWikiFULL . The surprisingly high performance of CycleGT indicates that it is an effective approach to address th"
2020.webnlg-1.8,2020.webnlg-1.7,0,0.440193,"Missing"
2020.webnlg-1.8,2020.webnlg-1.10,1,0.776479,"Missing"
2020.webnlg-1.8,2020.inlg-1.14,0,0.295759,"t In this section, we will first introduce the G2T and T2G components in Section 3.1 and 3.2, respectively, and then discuss the iterative back translation training strategy of CycleGT in Section 3.3. For unsupervised graph-to-text and text-to-graph generation, we assume two non-parallel datasets: • A text corpus DT = {ti }N i=1 consisting of N text sequences, and 3.1 G2T Component The model G2T : G → T takes as input a graph g and generates a text sequence tˆ that is a sufficient description of the graph. As pretrained models are shown to be very effective on G2T tasks (Ribeiro et al., 2020; Kale, 2020), we use T5 (Raffel et al., 2020), a large pretrained sequence-tosequence model as the G2T component. Note that • A graph dataset DG = {gj }M j=1 consisting of M graphs. The constraint is that the graphs and text contain the same distribution of latent content z, but are different forms of surface realizations. Their marginal log-likelihood can be formulated with the shared 78 the model architecture of G2T is flexible in our cycle training framework, and it can also be substituted by alternatives such as the GNN-LSTM architecture as proposed in Koncel-Kedziorski et al. (2019). Given a knowledg"
2020.webnlg-1.8,N19-1238,0,0.193197,"t al., 2020; Kale, 2020), we use T5 (Raffel et al., 2020), a large pretrained sequence-tosequence model as the G2T component. Note that • A graph dataset DG = {gj }M j=1 consisting of M graphs. The constraint is that the graphs and text contain the same distribution of latent content z, but are different forms of surface realizations. Their marginal log-likelihood can be formulated with the shared 78 the model architecture of G2T is flexible in our cycle training framework, and it can also be substituted by alternatives such as the GNN-LSTM architecture as proposed in Koncel-Kedziorski et al. (2019). Given a knowledge graph g, we first linearize it to a sequence Seq(g), using hHi, hRi, hTi, to denote the head, relation, and tail of a triple in the graph. As our paper mainly addresses graphs that can be verbalized in several sentences, the concerned graphs are small-scale. Hence, we linearize the graph according to the given order of triplets in the WebNLG dataset, as different linearization orders of the graphs are not a major concern, and can be handled well by the T5 model. The finetuning of T5 aims to find the optimal parameters θ∗ to correctly encode the graph sequence and decode it"
2020.webnlg-1.8,S10-1006,0,0.0300348,".316 0.542 0.659 0.31 27.5 0.305 0.519 0.846 0.03 28.71 0.243 0.448 0.689 -0.09 30.79 0.239 0.439 0.677 -0.09 23.08 0.225 0.421 0.743 -0.19 24.45 0.223 0.425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Z"
2020.webnlg-1.8,P83-1022,0,0.396409,"even with no pairing information between text and graphs, our unsupervised CycleGT model can achieve a 55.5 BLEU score, which is on par with the 56.4 BLEU score obtained using the same T2G model with T5-Base that we trained on supervised data. CycleGT also outperforms the unsupervised baseline RuleBased and 82 ID18 ID30 ID30_1 ID34 ID5 ID35 ID23 ID2 ID15 CycleGT (Unsup.) ID12 ID11 ID4 ID26 Official Baseline1 ID17 ID31_2 Official Baseline2 ID21 ID13_11 ID14 ID13 ID13_3 ID13_4 ID13_2 ID13_6 ID10 ID13_7 ID13_8 ID20 ID26_1 ID13_9 ID13_10 ID13_5 ID17_1 ditional approaches adopt a pipeline system (Kukich, 1983; McKeown, 1992) of content planning, sentence planning, and surface realization. Recent advances in neural networks give birth to end-toend systems (Lebret et al., 2016; Wiseman et al., 2017; Koncel-Kedziorski et al., 2019; Kale, 2020) that does not use explicit planning but directly an encoder-decoder architecture (Bahdanau et al., 2015; Vaswani et al., 2017). BLEU METEOR CHRF++ TER BLUERT 53.98 0.417 0.690 0.406 0.62 53.54 0.414 0.688 0.416 0.61 52.07 0.413 0.685 0.444 0.58 52.67 0.413 0.686 0.423 0.6 51.74 0.411 0.679 0.435 0.6 51.59 0.409 0.681 0.431 0.59 51.74 0.403 0.669 0.417 0.61 50.3"
2020.webnlg-1.8,N16-1030,0,0.01995,"0.564 0.37 38.2 0.335 0.571 0.577 0.29 39.19 0.334 0.569 0.565 0.28 38.85 0.332 0.569 0.573 0.3 38.01 0.331 0.565 0.583 0.27 37.96 0.331 0.566 0.580 0.28 22.84 0.326 0.534 0.696 -0.03 36.07 0.324 0.555 0.599 0.24 36.6 0.322 0.554 0.594 0.25 31.26 0.316 0.542 0.659 0.31 27.5 0.305 0.519 0.846 0.03 28.71 0.243 0.448 0.689 -0.09 30.79 0.239 0.439 0.677 -0.09 23.08 0.225 0.421 0.743 -0.19 24.45 0.223 0.425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. Howe"
2020.webnlg-1.8,N16-1000,0,0.190155,"Missing"
2020.webnlg-1.8,W18-2703,0,0.0250884,"to obtain the text-graph pairs (with entity annotation) for 13,036 training, 1,642 validation, and 4,928 test samples. (10) LCycG (ϕ) = Eg∈DG [− log p(g |G2Tθ (t); ϕ)] = E ˆ ˆ [− log p(g |tˆ; ϕ)] . (t,g)∈DPair Experiments (11) As such, to the extent that these approximations are accurate, the sum of Eqs. (8) and (9) reasonably approximates the log likelihoods from Eq. (3). Note that J (θ, ϕ) = LCycT (θ) + LCycG (ϕ) ˆ Pair has the same distribution as holds when D DPair . In our framework, we iteratively improve the G2T and T2G models using an iterative back translation (IBT) training scheme (Hoang et al., 2018), with the goal of reducing the discrepancy beˆ Pair . Specifitween the distribution of DPair and D cally, we repeatedly alternate the optimization of the two cycles described by Eqs. (10) and (11) over the corresponding θ or ϕ. WebNLG 2020 Dataset We also use CycleGT to participate in the WebNLG 2020 challenge, which is an extension of the WebNLG 2017 dataset covering broader categories. Specifically, WebNLG 2020 includes all the 15 categories of WebNLG 2017 in the training set, along with a new category, Company. Its test data also add three additional unseen categories, Film, Scientist, and"
2020.webnlg-1.8,2020.coling-main.217,1,0.911142,"hnical difficulty lies in the different modality of text and graphs, which can be intractable in a joint learning setting. We contribute an effective learning framework, CycleGT, which is iteratively trained with two cycle losses. We first validate CycleGT on widely used WebNLG datasets, both WebNLG 2017 (Gardent et al., 2017b) and WebNLG+ 2020 (Castro-Ferreira et al., 2020) which is a more challenging extension of the 2017 dataset. Here CycleGT achieves performance that is comparable to many supervised baselines. Additionally, we also evaluate our model on the newly released GenWiki dataset (Jin et al., 2020b) with 1.3M non-parallel text and knowledge graphs. Our model is the best out of all unsupervised baselines, improving by +11.47 BLEU over the best baseline on GenWikiFINE data and +6.26 BLEU on GenWikiFULL . The surprisingly high performance of CycleGT indicates that it is an effective approach to address the data scarcity problem in the fields of both G2T and T2G. Consequently, CycleGT can help pave the way for scalable, unsupervised learning, and benefit future research in both fields. 2 latent content z: log p(g) = log log p(t) = log Z Zz z p(g |z)p(z)dz , (1) p(t |z)p(z)dz . (2) Our goal"
2020.webnlg-1.8,D16-1128,0,0.0875556,"Missing"
2020.webnlg-1.8,D19-1306,1,0.890557,"Missing"
2020.webnlg-1.8,P09-1011,0,0.109525,"Missing"
2020.webnlg-1.8,W04-1013,0,0.0133785,"nWiki, we use the metrics that are mostly reported by other previous work (Moryossef et al., 2019) so that 2 WebNLG 2017 is the most appropriate dataset, because in other datasets such as relation extraction datasets (Walker et al., 2006), the graph only contain a very small subset of the information in the text. It can be downloaded from https://webnlg-challenge.loria. fr/challenge_2017/. 80 we can have head-to-head comparison with existing systems. Specifically, we adopt the metrics from (Moryossef et al., 2019), i.e., BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed text (model output) to the input text.3 Briefly, they measure the n-gram precision, recall, or F-scores between the model outputs and the (ground-truth) references. In contrast, for T2G evaluation on WebNLG 2017, we use the micro and macro F1 scores of the relation types of the edges, following the standard practice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboard (Moussalem et al., 2020). For G2T, the metrics includ"
2020.webnlg-1.8,P16-1200,0,0.0171011,"iu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Zeng et al., 2015; Lin et al., 2016). Table 3: Leaderboard of the submitted systems and two official baselines (Baseline1 and Baseline2) in the WebNLG 2020 English RDF-to-text challenge as ranked by METEOR. With all metrics, larger is better, with the exception of TER where lower is better. on another slightly different one. The NoisySupervised model performs relatively well, scoring over 30 BLEU points. And our model CycleGT is the strongest, outperforming NoisySupervised by around 10 BLEU points. 5 Cycle Training The concept of leveraging the transitivity of two functions inverse to each other has been widely observed on a var"
2020.webnlg-1.8,S18-1125,0,0.0149866,"ration from the Semantic Web (WebNLG+), c Dublin, Ireland (Virtual), 18 December 2020, pages 77–88, 2020 Association for Computational Linguistics Attribution 4.0 International. processing by (Moryossef et al., 2019)), which is several magnitudes fewer than the millions of data for neural machine translation (NMT) systems, such as the 4.4M paired sentences in the WMT14 dataset for NMT (Luong et al., 2015). As a result, most previous G2T and T2G models, which have to be trained on small datasets, display limited performance on both tasks (Koncel-Kedziorski et al., 2019; Moryossef et al., 2019; Luan et al., 2018). To circumvent the limitations of scarce supervised data, we formulate both tasks in a cycle training framework, and also in the unsupervised manner with fully non-parallel datasets (as shown in Figure 1). The technical difficulty lies in the different modality of text and graphs, which can be intractable in a joint learning setting. We contribute an effective learning framework, CycleGT, which is iteratively trained with two cycle losses. We first validate CycleGT on widely used WebNLG datasets, both WebNLG 2017 (Gardent et al., 2017b) and WebNLG+ 2020 (Castro-Ferreira et al., 2020) which is"
2020.webnlg-1.8,W15-1506,0,0.0115016,"tion Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Zeng et al., 2015; Lin et al., 2016). Table 3: Leaderboard of the submitted systems and two official baselines (Baseline1 and Baseline2) in the WebNLG 2020 English RDF-to-text cha"
2020.webnlg-1.8,D15-1166,0,0.0852075,"Missing"
2020.webnlg-1.8,P02-1040,0,0.11346,"GenWikiFINE . 4.2 Evaluation Metrics For G2T on WebNLG 2017 and GenWiki, we use the metrics that are mostly reported by other previous work (Moryossef et al., 2019) so that 2 WebNLG 2017 is the most appropriate dataset, because in other datasets such as relation extraction datasets (Walker et al., 2006), the graph only contain a very small subset of the information in the text. It can be downloaded from https://webnlg-challenge.loria. fr/challenge_2017/. 80 we can have head-to-head comparison with existing systems. Specifically, we adopt the metrics from (Moryossef et al., 2019), i.e., BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed text (model output) to the input text.3 Briefly, they measure the n-gram precision, recall, or F-scores between the model outputs and the (ground-truth) references. In contrast, for T2G evaluation on WebNLG 2017, we use the micro and macro F1 scores of the relation types of the edges, following the standard practice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboar"
2020.webnlg-1.8,W17-4770,0,0.0409617,"al., 2015), to measure the closeness of the reconstructed text (model output) to the input text.3 Briefly, they measure the n-gram precision, recall, or F-scores between the model outputs and the (ground-truth) references. In contrast, for T2G evaluation on WebNLG 2017, we use the micro and macro F1 scores of the relation types of the edges, following the standard practice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboard (Moussalem et al., 2020). For G2T, the metrics include BLEU, METEOR, chrF++ (Popovic, 2017), TER (Snover et al., 2006), BERTF1 (Zhang et al., 2020), and BLUERT (Sellam et al., 2020). enhanced version of the common encoderdecoder model. • BestPlan (Moryossef et al., 2019): A special entity ordering algorithm applied before neural text generation. • GraphWriter (Koncel-Kedziorski et al., 2019): A graph attention network with LSTMs. • Seg&Align (Shen et al., 2020): An approach that first segments the text into small units and then learns the alignment between data and target text segments. • Planner (Zhao et al., 2020): A planner with relational graph convolutional networks and LSTMs."
2020.webnlg-1.8,P09-1113,0,0.0721509,"endrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Zeng et al., 2015; Lin et al., 2016). Table 3: Leaderboard of the submitted systems and two official baselines (Baseline1 and Baseline2) in the WebNLG 2020 English RDF-to-text challenge as ranked by METEOR. With all metrics, larger is better, with the exception of TER where lower is better. on another slightly different one. The NoisySupervised model performs relatively well, scoring over 30 BLEU points. And our model CycleGT is the strongest, outperforming NoisySupervised by around 10 BLEU points. 5 Cycle Training The concept of leveraging the transitivity of two functions inverse to each o"
2020.webnlg-1.8,2020.acl-demos.14,0,0.0246819,"raph. Specifically, we align each text with its back-translated version, and also each graph with its back-translated version via the objectives We then introduce the other component, the T2G model. The function T2G : T → G takes as input a text sequence t and extracts its corresponding graph gˆ, whose nodes are the entities and edges are the relations between two entities. As generating both the entities and relations of the graph in a differentiable way is generally intractable, we directly use the entities if they are given, or if not given, we use an off-the-shelf entity extraction model (Qi et al., 2020) that identifies all entities in the text with high accuracy. We denote the set of entities extracted from the text as NER(t). We then predict the relations between every two entities to form the edges in the graph. Note that our T2G component presented below is also flexible and can be replaced with other models. Proceeding further, we first obtain the embeddings of every entity vi ∈ NER(t) by average pooling the contextualized embedding of each word wj in the entity term. This leads to X 1 vi = emb(wj ) , (5) Len(vi ) w ∈v LCycT (θ) = Et∈DT [− log p(t |T2Gϕ (t); θ)] , (8) LCycG (ϕ) = Eg∈DG ["
2020.webnlg-1.8,N19-1082,1,0.842081,"5 0.571 0.577 0.29 39.19 0.334 0.569 0.565 0.28 38.85 0.332 0.569 0.573 0.3 38.01 0.331 0.565 0.583 0.27 37.96 0.331 0.566 0.580 0.28 22.84 0.326 0.534 0.696 -0.03 36.07 0.324 0.555 0.599 0.24 36.6 0.322 0.554 0.594 0.25 31.26 0.316 0.542 0.659 0.31 27.5 0.305 0.519 0.846 0.03 28.71 0.243 0.448 0.689 -0.09 30.79 0.239 0.439 0.677 -0.09 23.08 0.225 0.421 0.743 -0.19 24.45 0.223 0.425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by"
2020.webnlg-1.8,P16-1105,0,0.245772,"th existing systems. Specifically, we adopt the metrics from (Moryossef et al., 2019), i.e., BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed text (model output) to the input text.3 Briefly, they measure the n-gram precision, recall, or F-scores between the model outputs and the (ground-truth) references. In contrast, for T2G evaluation on WebNLG 2017, we use the micro and macro F1 scores of the relation types of the edges, following the standard practice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboard (Moussalem et al., 2020). For G2T, the metrics include BLEU, METEOR, chrF++ (Popovic, 2017), TER (Snover et al., 2006), BERTF1 (Zhang et al., 2020), and BLUERT (Sellam et al., 2020). enhanced version of the common encoderdecoder model. • BestPlan (Moryossef et al., 2019): A special entity ordering algorithm applied before neural text generation. • GraphWriter (Koncel-Kedziorski et al., 2019): A graph attention network with LSTMs. • Seg&Align (Shen et al., 2020): An approach that first segments"
2020.webnlg-1.8,A00-2026,0,0.181224,"ng has recently been applied in the context of text-graph conversion (Schmitt et al., 2020); we compare against this approach in Table 1 (see Related Work We will first give an overview of the fields of T2G and T2G separately, and then introduce the cycle learning. Data-to-Text Generation As a classic problem in text generation (Kukich, 1983; McKeown, 1992), data-to-text generation aims to automatically produce text from structured data (Reiter and Dale, 1997; Liang et al., 2009). Due to the expensive collection, all the data-to-text datasets are very small, such as the 5K air travel dataset (Ratnaparkhi, 2000), 22K WeatherGov (Liang et al., 2009), 7K Robocup (Chen and Mooney, 2008), and 5K RotoWire on basketball games (Wiseman et al., 2017). As for the methodology, tra83 GT-BT results). 6 Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semisupervised learning for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics. Conclusion We have developed a cycle learning framework for both text-to-gra"
2020.webnlg-1.8,N19-1236,0,0.0764559,"Missing"
2020.webnlg-1.8,S10-1057,0,0.0166394,".5 0.305 0.519 0.846 0.03 28.71 0.243 0.448 0.689 -0.09 30.79 0.239 0.439 0.677 -0.09 23.08 0.225 0.421 0.743 -0.19 24.45 0.223 0.425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Zeng et al., 2015; Lin et al"
2020.webnlg-1.8,D12-1110,0,0.00767673,"on (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Zeng et al., 2015; Lin et al., 2016). Table 3: Leaderboard of the submitted systems and two official baselines (Baseline1 and Baseline2) in the WebNLG 2020 English RDF-to-text challenge as ranked by METEOR"
2020.webnlg-1.8,P15-1061,0,0.0143829,".425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Zeng et al., 2015; Lin et al., 2016). Table 3: Leaderboard of the submitted systems and two official baselines (Baseline1 and Baseline2) in the WebNLG"
2020.webnlg-1.8,P19-1527,0,0.0151186,"39.19 0.334 0.569 0.565 0.28 38.85 0.332 0.569 0.573 0.3 38.01 0.331 0.565 0.583 0.27 37.96 0.331 0.566 0.580 0.28 22.84 0.326 0.534 0.696 -0.03 36.07 0.324 0.555 0.599 0.24 36.6 0.322 0.554 0.594 0.25 31.26 0.316 0.542 0.659 0.31 27.5 0.305 0.519 0.846 0.03 28.71 0.243 0.448 0.689 -0.09 30.79 0.239 0.439 0.677 -0.09 23.08 0.225 0.421 0.743 -0.19 24.45 0.223 0.425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of"
2020.webnlg-1.8,2020.emnlp-main.577,0,0.410964,"xt generation. • Supervised G2T (our implementation) : Our implementation of T5-base, which is the same as the G2T component in our CycleGT. For text-to-graph generation, we compare with state-of-the-art models including • OnePass (Wang et al., 2019): A BERT-based relation extraction model. • T2G (our implementation): The BiLSTM model that we adopt as the text-to-graph component in the cycle training of CycleGT. 4.3 Comparison Systems for WebNLG 2017 Unsupervised Baselines As cycle training models are unsupervised learning methods, we include the following unsupervised baselines: • RuleBased (Schmitt et al., 2020): This baseline involves simply iterating through the graph and concatenating the text of each triplet. • Graph-Text Back Translator (GT-BT) (Schmitt et al., 2020): This approach first serializes the graph and then applies a back translation model. Since the code for GT-BT is not yet available, we adopt their reported results as a reference. 4.4 Comparison Systems for GenWiki Since GenWiki is a non-parallel dataset, we can only train unsupervised baselines or transfer models developed using other data. We compare against: • RuleBased (Schmitt et al., 2020): Same as was applied for WebNLG 2017"
2020.webnlg-1.8,2020.acl-main.704,0,0.0271441,"input text.3 Briefly, they measure the n-gram precision, recall, or F-scores between the model outputs and the (ground-truth) references. In contrast, for T2G evaluation on WebNLG 2017, we use the micro and macro F1 scores of the relation types of the edges, following the standard practice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboard (Moussalem et al., 2020). For G2T, the metrics include BLEU, METEOR, chrF++ (Popovic, 2017), TER (Snover et al., 2006), BERTF1 (Zhang et al., 2020), and BLUERT (Sellam et al., 2020). enhanced version of the common encoderdecoder model. • BestPlan (Moryossef et al., 2019): A special entity ordering algorithm applied before neural text generation. • GraphWriter (Koncel-Kedziorski et al., 2019): A graph attention network with LSTMs. • Seg&Align (Shen et al., 2020): An approach that first segments the text into small units and then learns the alignment between data and target text segments. • Planner (Zhao et al., 2020): A planner with relational graph convolutional networks and LSTMs. • T5-Large (Kale, 2020): An application of the pretrained T5-Large model for serialized gr"
2020.webnlg-1.8,P16-1009,0,0.468724,"takes in the two vertices of the edge and predicts the edge type, which includes the “norelation” type, and the set of possible relations of entities, leading to eij = C(vi , vj ). (6) T2G training aims to find the optimal parameters ϕ∗ that correctly encodes the text and predicts the graph via the maximum likelihood estimation objective Y ϕ∗ = argmax p(g |t; ϕ) (7) ϕ = argmax ϕ (t,g)∼D K Y K Y Y (t,g)∼D i=0 j=0 p(eij |vi , vj , t; ϕ) , where K = |NER(t) |is the number of entities in text t. 3.3 (t,g)∼D 3.2 T2G Component Cycle Training with Iterative Back Translation In NLP, back translation (Sennrich et al., 2016a) is first proposed for machine translation where a sentence in the source language (e.g., English) should be able to first translate to the target language (e.g., French) and then again translated “back” to the original source language. The source sentence and the “back-translated” sentence should be aligned to be the same. The essence of back translation is that a variable x and a bijective mapping function f (·) should satisfy x = f −1 (f (x)), where f −1 is the inverse function of f . In our case, G2T and T2G are inverse functions of each other, because one transforms the graph to text an"
2020.webnlg-1.8,P19-1132,0,0.0877135,"gnment between data and target text segments. • Planner (Zhao et al., 2020): A planner with relational graph convolutional networks and LSTMs. • T5-Large (Kale, 2020): An application of the pretrained T5-Large model for serialized graph-to-text generation. • T5-Large (Ribeiro et al., 2020): Another instance of the pretrained T5-Large model for serialized graph-to-text generation. • Supervised G2T (our implementation) : Our implementation of T5-base, which is the same as the G2T component in our CycleGT. For text-to-graph generation, we compare with state-of-the-art models including • OnePass (Wang et al., 2019): A BERT-based relation extraction model. • T2G (our implementation): The BiLSTM model that we adopt as the text-to-graph component in the cycle training of CycleGT. 4.3 Comparison Systems for WebNLG 2017 Unsupervised Baselines As cycle training models are unsupervised learning methods, we include the following unsupervised baselines: • RuleBased (Schmitt et al., 2020): This baseline involves simply iterating through the graph and concatenating the text of each triplet. • Graph-Text Back Translator (GT-BT) (Schmitt et al., 2020): This approach first serializes the graph and then applies a back"
2020.webnlg-1.8,2020.acl-main.641,0,0.0656994,"ctice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboard (Moussalem et al., 2020). For G2T, the metrics include BLEU, METEOR, chrF++ (Popovic, 2017), TER (Snover et al., 2006), BERTF1 (Zhang et al., 2020), and BLUERT (Sellam et al., 2020). enhanced version of the common encoderdecoder model. • BestPlan (Moryossef et al., 2019): A special entity ordering algorithm applied before neural text generation. • GraphWriter (Koncel-Kedziorski et al., 2019): A graph attention network with LSTMs. • Seg&Align (Shen et al., 2020): An approach that first segments the text into small units and then learns the alignment between data and target text segments. • Planner (Zhao et al., 2020): A planner with relational graph convolutional networks and LSTMs. • T5-Large (Kale, 2020): An application of the pretrained T5-Large model for serialized graph-to-text generation. • T5-Large (Ribeiro et al., 2020): Another instance of the pretrained T5-Large model for serialized graph-to-text generation. • Supervised G2T (our implementation) : Our implementation of T5-base, which is the same as the G2T component in our CycleGT. For text"
2020.webnlg-1.8,D17-1239,0,0.0670973,"Missing"
2020.webnlg-1.8,C14-1220,0,0.0252035,"743 -0.19 24.45 0.223 0.425 0.739 -0.22 Relation Extraction Relation Extraction (RE) is the core problem in text-to-graph conversion, as its former step, entity recognition, have off-theshelf tools with good performance (Lample et al., 2016; Qian et al., 2019; Straková et al., 2019; Clark et al., 2018; Jin et al., 2020c). RE aims to classify the relation of entities given a shared textual context. Conventional approaches hand-crafted lexical and syntactic features (Hendrickx et al., 2010; Rink and Harabagiu, 2010). With the recent advancement of deep neural networks, many models based on CNN (Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015), RNN (Socher et al., 2012; Zhang and Wang, 2015; Miwa and Bansal, 2016b; Zhou et al., 2016), and BERT (Wang et al., 2019) achieve high performance in many datasets. However, constrained by the small datasets of only several hundred or several thousand data points (Walker et al., 2006; Hendrickx et al., 2010; Gábor et al., 2018), recent research shifts to distant supervision than model innovation (Mintz et al., 2009; Zeng et al., 2015; Lin et al., 2016). Table 3: Leaderboard of the submitted systems and two official baselines (Baseline1 and"
2020.webnlg-1.8,2020.acl-main.224,0,0.555791,"salem et al., 2020). For G2T, the metrics include BLEU, METEOR, chrF++ (Popovic, 2017), TER (Snover et al., 2006), BERTF1 (Zhang et al., 2020), and BLUERT (Sellam et al., 2020). enhanced version of the common encoderdecoder model. • BestPlan (Moryossef et al., 2019): A special entity ordering algorithm applied before neural text generation. • GraphWriter (Koncel-Kedziorski et al., 2019): A graph attention network with LSTMs. • Seg&Align (Shen et al., 2020): An approach that first segments the text into small units and then learns the alignment between data and target text segments. • Planner (Zhao et al., 2020): A planner with relational graph convolutional networks and LSTMs. • T5-Large (Kale, 2020): An application of the pretrained T5-Large model for serialized graph-to-text generation. • T5-Large (Ribeiro et al., 2020): Another instance of the pretrained T5-Large model for serialized graph-to-text generation. • Supervised G2T (our implementation) : Our implementation of T5-base, which is the same as the G2T component in our CycleGT. For text-to-graph generation, we compare with state-of-the-art models including • OnePass (Wang et al., 2019): A BERT-based relation extraction model. • T2G (our impl"
2020.webnlg-1.8,P16-2034,0,0.116841,"cifically, we adopt the metrics from (Moryossef et al., 2019), i.e., BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015), to measure the closeness of the reconstructed text (model output) to the input text.3 Briefly, they measure the n-gram precision, recall, or F-scores between the model outputs and the (ground-truth) references. In contrast, for T2G evaluation on WebNLG 2017, we use the micro and macro F1 scores of the relation types of the edges, following the standard practice in relation extraction (Miwa and Bansal, 2016a; Zhou et al., 2016). Turning to WebNLG 2020, we report most major metrics listed on the leaderboard (Moussalem et al., 2020). For G2T, the metrics include BLEU, METEOR, chrF++ (Popovic, 2017), TER (Snover et al., 2006), BERTF1 (Zhang et al., 2020), and BLUERT (Sellam et al., 2020). enhanced version of the common encoderdecoder model. • BestPlan (Moryossef et al., 2019): A special entity ordering algorithm applied before neural text generation. • GraphWriter (Koncel-Kedziorski et al., 2019): A graph attention network with LSTMs. • Seg&Align (Shen et al., 2020): An approach that first segments the text into small"
2021.acl-demo.12,D15-1141,1,0.825404,"model, users can utilize user lexicon and finetuning to enhance the performance of fastHan. As for user lexicon, users can call the add user dict function to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The re"
2021.acl-demo.12,P17-1110,1,0.834923,"or Chinese NLP Zhichao Geng, Hang Yan, Xipeng Qiu∗, Xuanjing Huang School of Computer Science, Fudan University Key Laboratory of Intelligent Information Processing, Fudan University {zcgeng20,hyan19,xpqiu,xjhuang}@fudan.edu.cn Abstract the tasks. Tools developed for a single task cannot achieve the highest accuracy, and loading tools for each task will take up more memory. In practical, there is a strong correlation between these four basic Chinese NLP tasks. For example, the model will perform better in the other three word-level tasks if its word segmentation ability is stronger. Recently, Chen et al. (2017a) adopt cross-label to label the POS so that POS tagging and CWS can be trained jointly. Yan et al. (2020) propose a graph-based model for joint CWS and dependency parsing, in which a special ”APP” dependency arc is used to indicate the word segmentation information. Thus, they can jointly train the word-level dependency parsing task and character-level CWS task with the biaffine parser (Dozat and Manning, 2016). Chen et al. (2017b) explore adversarial multi-criteria learning for CWS, proving more knowledge can be mined through training model on more corpora. As a result, there are many piece"
2021.acl-demo.12,P81-1022,0,0.589619,"Missing"
2021.acl-demo.12,I05-3017,0,0.0654681,"formance of fastHan. As for user lexicon, users can call the add user dict function to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s perfo"
2021.acl-demo.12,2020.coling-main.186,0,0.044383,"Missing"
2021.acl-demo.12,I08-4010,0,0.0620125,"can call the add user dict function to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s performance on all tasks. The large version of fastHan ou"
2021.acl-demo.12,2021.naacl-main.436,1,0.741398,"Missing"
2021.acl-demo.12,2020.acl-main.611,1,0.811281,"otes F SOTA models 97.1 85.66, 81.71 93.15 96.09 81.82 fastHan base trained separately fastHan base trained jointly fastHan large trained jointly 97.15 97.27 97.41 80.2, 75.12 81.22, 76.71 85.52, 81.38 94.27 94.88 95.66 92.2 94.33 95.50 80.3 82.86 83.82 Table 1: The results of fastHan’s accuracy result. The score of CWS is the average of 10 corpora. When training dependency parsing separately, the biaffine parser use the same architecture as Yan et al. (2020). SOTA models are best-performing work we know for each task. They came from Huang et al. (2019), Yan et al. (2020), Meng et al. (2019), Li et al. (2020) in order. Li et al. (2020) uses lexicon to enhance the model. user lexicon to enhance fastHan’s performance in specific domains. 4.2 much smaller model (262MB versus 492MB). The result proves fastHan is robust to new samples, and the fine-tuning feature allows fastHan to better be adapted to new criteria. Transferability Test Segmentation Tool jieba SnowNLP THULAC LTP-4.0 fastHan fastHan(fine-tuned) 4.3 Weibo Test Set 83.58 79.65 86.65 92.05 93.38 96.64 Speed Test Models fastHan base fastHan large Table 2: Transfer test for fastHan, using span F metric. We use the test set of Weibo, which has"
2021.acl-demo.12,D14-1122,0,0.0199705,"n to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s performance on all tasks. The large version of fastHan outperforms the current best model i"
2021.acl-demo.12,C96-1035,0,0.325315,"5 and decreases linearly to 0. In the second part, We only fine-tune the base model and don’t replace the modules anymore. 2.5 User Lexicon In actual applications, users may process text of specific domains, such as technology, medical. There are proprietary vocabularies with high recall rates in such domains, and they rarely appear in ordinary corpus. It is intuitive to use a user lexicon to address this problem. Users can choose whether to add or use their lexicon. An example of combining a user lexicon is shown in Figure 3. When combined with a user lexicon, the maximum matching algorithm (Wong and Chan, 1996) is first performed to obtain a label sequence. After that, a bias will be added to the corresponding scores output by the encoder. And the result will be viewed as f1 (X, yt ) in CRF in section 2.2. The bias is (2) fastHan FastHan is a Chinese NLP toolkit based on the above model, developed based on fastNLP2 and PyTorch. We made a short video demonstrating fastHan and uploaded it to YouTube3 and bilibili4 . FastHan has been released on PYPI and users can install it by pip: pip install fastHan 3.1 Workflow When FastHan initializes, it first loads the pretrained model parameters from the file s"
2021.acl-demo.12,2020.emnlp-main.633,0,0.135254,"tween ∗ 1 Corresponding author https://github.com/fastnlp/fastHan 99 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 99–106, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics joint-model can reduce the occupied memory space by four times. FastHan has two versions of the backbone model, base and large. The large model uses the first eight layers of BERT, and the base model uses the Theseus strategy (Xu et al., 2020) to compress the large model to four layers. To improve the performance of the model, fastHan has done much optimization. For example, using the output of POS tagging to improve the performance of the dependency parsing task, using Theseus strategy to improve the performance of the base version model, and so on. Overall, fastHan has the following advantages: Figure 1: Architecture of the proposed model. The inputs are characters embeddings. Small size: The total parameter of the base model is 151MB, and for the large model the number is 262MB. 2 High accuracy: The base version of the model ach"
2021.acl-demo.12,2020.tacl-1.6,1,0.938575,"sity Key Laboratory of Intelligent Information Processing, Fudan University {zcgeng20,hyan19,xpqiu,xjhuang}@fudan.edu.cn Abstract the tasks. Tools developed for a single task cannot achieve the highest accuracy, and loading tools for each task will take up more memory. In practical, there is a strong correlation between these four basic Chinese NLP tasks. For example, the model will perform better in the other three word-level tasks if its word segmentation ability is stronger. Recently, Chen et al. (2017a) adopt cross-label to label the POS so that POS tagging and CWS can be trained jointly. Yan et al. (2020) propose a graph-based model for joint CWS and dependency parsing, in which a special ”APP” dependency arc is used to indicate the word segmentation information. Thus, they can jointly train the word-level dependency parsing task and character-level CWS task with the biaffine parser (Dozat and Manning, 2016). Chen et al. (2017b) explore adversarial multi-criteria learning for CWS, proving more knowledge can be mined through training model on more corpora. As a result, there are many pieces of research on how to perform multi-corpus training on these tasks and how to conduct multi-task joint tr"
2021.acl-demo.12,E14-1062,0,0.0235998,"call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s performance on all tasks. The large version of fastHan outperforms the current best model in CWS and POS. Although fast"
2021.acl-demo.41,D18-2029,0,0.0379272,"Missing"
2021.acl-demo.41,N19-1423,0,0.207492,"re rated on a 1-5 scale (5 denotes the best). in a general way, TextFlint supports generating massive and comprehensive transformed samples with just one command. By default, TextFlint performs all single transformations on the original dataset to form the corresponding transformed datasets, and the performance of the target models is tested on these datasets. The evaluation report provides a comparative view of model performance on datasets before and after certain types of transformations, which supports model weakness analyses and guides particular improvements. For example, take BERT base(Devlin et al., 2019) as the target model to verify its robustness on the CONLL2003 dataset(Tjong Kim Sang and De Meulder, 2003), whose robustness report is shown in Figure 5. The performance of BERT base decreases significantly in some morphology transformations, such as OCR, Keyboard, Typos, and Spelling Error. To combat these errors of input texts and improve the robustness of the model, we suggest that placing a word correction model(Pruthi et al., 2019) before BERT would be beneficial. 0.8 Case 2: Customized Evaluation For users who want to test model performance on specific aspects, they demand a customized"
2021.acl-demo.41,D18-1380,0,0.107351,"Missing"
2021.acl-demo.41,2021.naacl-demos.6,0,0.061782,"Missing"
2021.acl-demo.41,D14-1181,0,0.00437317,"Missing"
2021.acl-demo.41,2020.emnlp-main.500,1,0.864395,"like to teach kids in the kindergarten. The storm destroyed many houses in the village. ✘ Figure 1: Examples of three main generation functions. The transformation example is from ABSA (Aspectbased Sentiment Analysis) task, where the italic bold RevTgt (short for reverse target) denotes task-specific transformations, and the bold Typos denotes universal transformation. Introduction The detection of model robustness has been attracting increasing attention in recent years, given that deep neural networks (DNNs) of high accuracy can still be vulnerable to carefully crafted adversarial examples (Li et al., 2020), distribution shift (Miller et al., 2020), data transformation (Xing et al., 2020), and shortcut learning (Geirhos et al., 2020). Existing approaches to textual robustness evaluation focus on slightly modifying the input data, which maintains the original meaning and results in a different prediction. However, these methods often concentrate on either universal or task-specific generalization capabilities, which is difficult to comprehensively evaluate. In response to the shortcomings of recent works, we introduce TextFlint, a unified, multilingual, and analyzable robustness evaluation toolki"
2021.acl-demo.41,P18-1087,0,0.0272118,"Missing"
2021.acl-demo.41,P02-1040,0,0.116092,"are implemented based on TextAttack (Morris et al., 2020). Validator It is crucial to verify the quality of the samples generated by Transformation and AttackRecipe. TextFlint provides several metrics to evaluate the quality of the generated text, including (1) language model perplexity calculated based on the GPT2 model (Radford et al., 2019), (2) word replacement ratio in generated text compared with its original text, (3) edit distance between original text and generated text, (4) semantic 349 similarity calculated based on Universal Sentence Encoder (Cer et al., 2018), and (5) BLEU score (Papineni et al., 2002). 2.3 Reporter Layer Generation Layer yields three types of adversarial samples and verifies the robustness of the target model. Based on the evaluation results from Generation Layer, Report Layer aims to provide users with a standard analysis report from syntax, morphology, pragmatics, and paradigmatic relation aspects. The running process of Report Layer can be regarded as a pipeline from Analyzer to ReportGenerator. 3 Figure 4: Screenshot of TextFlint’s web interface running Ocr transformation for ABSA task. Usage Using TextFlint to verify the robustness of a specific model is as simple as"
2021.acl-demo.41,P19-1561,0,0.0287018,"Missing"
2021.acl-demo.41,2020.acl-main.442,0,0.114738,"methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by linguistics and have been proved plausible and readable by human annotators. Several t"
2021.acl-demo.41,C16-1311,0,0.0414402,"Missing"
2021.acl-demo.41,D16-1021,0,0.0913955,"Missing"
2021.acl-demo.41,D19-3002,0,0.0392234,"Missing"
2021.acl-demo.41,D16-1058,0,0.0373143,"Missing"
2021.acl-demo.41,P19-1073,0,0.133542,"underlying patterns about model robustness. As for the ABSA task (Table 2), methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by lingu"
2021.acl-demo.41,2020.emnlp-main.292,1,0.893206,"Missing"
2021.acl-long.16,N19-1423,0,0.452081,"prediction ... Layers (>i+2) Yes? B-PER ? O ? B-ORG? O ? Dell founded Dell in 1984 B-PER ? O ? B-ORG? O ? B-TIME ? Dell founded Dell in 1984 B-ORG ? O ? B-ORG? O ? B-TIME ? Dell founded Dell in 1984 B-TIME ? Confident? (Pooling) No? Confident? (Pooling) 2 Recently, PTMs (Qiu et al., 2020) have become the mainstream backbone model for various sequence labeling tasks. The typical framework consists of a backbone encoder and a task-specific decoder. Confident? (Pooling) (b) Sentence-Level Early-Exit for Sequence Labeling Early-Exit prediction ... Layers (>i+2) Encoder In this paper, we use BERT (Devlin et al., 2019) as our backbone encoder . The architecture of BERT consists of multiple stacked Transformer layers (Vaswani et al., 2017). Given a sequence of tokens x1 , · · · , xN , the hidden state of l-th transformer layer is denoted by (l) (l) H(l) = [h1 , · · · , hN ], and H(0) is the BERT input embedding. Yes? B-PER ? Layer (i+2) Dell B-PER ? Layer (i+1) Dell B-ORG ? Layer (i) Dell O ? B-ORG? O ? B-TIME ? founded Dell in 1984 O ? B-ORG? O ? B-TIME ? founded Dell in 1984 O ? B-ORG? O ? B-TIME ? founded Dell in 1984 BERT for Sequence Labeling No? (c) Token-Level Early-Exit for Sequence Labeling Figure 1"
2021.acl-long.16,P11-2008,0,0.0843056,"Missing"
2021.acl-long.16,P11-2000,0,0.222966,"Missing"
2021.acl-long.16,D18-1275,1,0.88954,"Missing"
2021.acl-long.16,E17-2113,0,0.0604904,"Missing"
2021.acl-long.16,2020.emnlp-main.518,0,0.0352134,"Dredze, 2015; He and Sun, 2017) and CLUE NER (Xu et al., 2020), POS: ARK Twitter (Gimpel et al., 2011; Owoputi et al., 2013), CTB5 POS (Xue et al., 2005) and UD POS (Nivre et al., 2016), CWS: CTB5 Seg (Xue et al., 2005) and UD Seg (Nivre et al., 2016). Besides the standard benchmark dataset like CoNLL2003 and Ontonotes 4.0, we also choose some datasets closer to realworld application to verify the actual utility of our methods, such as Twitter NER and Weibo in social media domain. We use the same dataset prepro193 cessing and split as in previous work (Huang et al., 2015; Mengge et al., 2020; Jia et al., 2020; Tian et al., 2020; Nguyen et al., 2020). 4.2.2 e.g., Chinese NER, TOKEE (4×) on BERT can still outperform LSTM-CRF significantly. This indicates the potential utility of it in complicated real-world scenario. To explore the fine-grained performance change under different speedup ratio, We visualize the speedup-performance trade-off curve on 6 datasets, in Figure2. We observe that, Baseline We compare our methods with three baselines: • BiLSTM-CRF (Huang et al., 2015; Ma and Hovy, 2016) The most widely used model in sequence labeling tasks before the pre-trained language model prevails in NLP"
2021.acl-long.16,2020.findings-emnlp.372,0,0.0315433,"eper layer even when 4×, meanwhile, the SENTEE’s average exiting layer number reduces to 2.5, where the PTM’s encoding power is severely cut down. This gives an intuitive explanation of why TOKEE is more effective than SENTEE under high speedup ratio: although both SENTEE and TOKEE can dynamically adjust computational cost on the sample-level, TOKEE can adjust do it in a more fine-grained way. 196 5 Related Work PTMs are powerful but have high computational cost. To accelerate them, many attempts have been made. A kind of methods is to reduce its size, such as distillation (Sanh et al., 2019; Jiao et al., 2020), structural pruning (Michel et al., 2019; Fan et al., 2020) and quantization (Shen et al., 2020). Another kind of methods is early-exit, which dynamically adjusts the encoding layer number of different samples (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020; Li et al., 2020). While they introduced early-exit mechanism in simple classification tasks, our methods are proposed for the more complicated scenario: sequence labeling, where it has not only one prediction probability and it’s necessary to consider the dependency of token exitings. Elbayad et al. (2020) pr"
2021.acl-long.16,2021.findings-emnlp.43,0,0.0610833,"Missing"
2021.acl-long.16,2020.acl-main.537,0,0.333853,"s are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed to exit without passing through the entire model. Figure 1(a) gives an illustration of early-exit mechanism for text classification. However, most existing early-exit methods are targeted at sequence-level prediction, such as text classification, in which the predic"
2021.acl-long.16,P16-1101,0,0.0469515,"se the same dataset prepro193 cessing and split as in previous work (Huang et al., 2015; Mengge et al., 2020; Jia et al., 2020; Tian et al., 2020; Nguyen et al., 2020). 4.2.2 e.g., Chinese NER, TOKEE (4×) on BERT can still outperform LSTM-CRF significantly. This indicates the potential utility of it in complicated real-world scenario. To explore the fine-grained performance change under different speedup ratio, We visualize the speedup-performance trade-off curve on 6 datasets, in Figure2. We observe that, Baseline We compare our methods with three baselines: • BiLSTM-CRF (Huang et al., 2015; Ma and Hovy, 2016) The most widely used model in sequence labeling tasks before the pre-trained language model prevails in NLP. • BERT The powerful stacked Transformer encoder model, pre-trained on large-scale corpus, which we use as the backbone of our methods. • DistilBERT The most well-known distillation method of BERT. Huggingface released 6 layers DistilBERT for English (Sanh et al., 2019). For comparison, we distill {3, 4} and {3, 4, 6} layers DistilBERT for English and Chinese using the same method. 4.2.3 Hyper-Parameters For all datasets, We use batch size=10. We perform grid search over learning rate i"
2021.acl-long.16,2020.emnlp-main.514,0,0.474756,"nce labeling tasks, such as named entity recognition, part-of-speech tagging, ∗ Corresponding author. Our implementation is publicly available at https:// github.com/LeeSureman/Sequence-Labeling-Early-Exit. Chinese word segmentation and Semantic Role Labeling. These tasks are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed"
2021.acl-long.16,2020.emnlp-demos.2,0,0.057415,"rocessing (NLP). Many NLP tasks can be converted to sequence labeling tasks, such as named entity recognition, part-of-speech tagging, ∗ Corresponding author. Our implementation is publicly available at https:// github.com/LeeSureman/Sequence-Labeling-Early-Exit. Chinese word segmentation and Semantic Role Labeling. These tasks are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the predicti"
2021.acl-long.16,L16-1262,0,0.0406356,"Missing"
2021.acl-long.16,N13-1039,0,0.0336183,"Missing"
2021.acl-long.16,D15-1064,0,0.0640046,"Missing"
2021.acl-long.16,2020.acl-main.593,0,0.242788,"time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed to exit without passing through the entire model. Figure 1(a) gives an illustration of early-exit mechanism for text classification. However, most existing early-exit methods are targeted at sequence-level prediction, such as text classification, in which the prediction and its confidence score are calcula"
2021.acl-long.16,2020.acl-main.735,0,0.397254,"converted to sequence labeling tasks, such as named entity recognition, part-of-speech tagging, ∗ Corresponding author. Our implementation is publicly available at https:// github.com/LeeSureman/Sequence-Labeling-Early-Exit. Chinese word segmentation and Semantic Role Labeling. These tasks are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earli"
2021.acl-long.16,2020.acl-main.204,0,0.288513,"amental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed to exit without passing through the entire model. Figure 1(a) gives an illustration of early-exit mechanism for text classification. However, most existing early-exit methods are targeted at sequence-level prediction, such as text classification, in which the prediction and its confi"
2021.acl-long.188,2020.acl-main.340,0,0.274619,"the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018"
2021.acl-long.188,D14-1179,0,0.0203405,"Missing"
2021.acl-long.188,P19-1520,0,0.0208337,"ns, e.g., pipeline methods. The “Datasets” column refers to the datasets that this baseline is conducted. corresponding aspects. The third dataset(D20a ) is from Peng et al. (2020). They refine the data in <a, o, s> triplet form. The fourth dataset(D20b ) from Xu et al. (2020) is the revised variant of Peng et al. (2020), where the missing triplets with overlapping opinions are corrected. We present the statistics for these four datasets in Table 1. 4.2 cover almost all the ABSA subtasks except for one certain subtask depending on the baseline structures. For the following baselines: RINANTE (Dai and Song, 2019), CMLA (Wang et al., 2017), Liunified (Li et al., 2019), the suffix “+” in Table 2 denotes the corresponding model variant modified by Peng et al. (2020) for being capable of AESC, Pair and Triplet. Baselines To have a fair comparison, we summarize topperforming baselines of all ABSA subtasks. Given different ABSA subtasks, datasets, and experimental setups, existing baselines can be separated into three groups roughly as shown in Table 2. The baselines in the first group are conducted on D17 dataset, covering the AE, OE, ALSC, and AESC subtasks. Span-based method SPAN-BERT (Hu et al., 2019) a"
2021.acl-long.188,N19-1423,0,0.0254265,"el based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004). However, the time complexity limits the model to detect the aspect term with longdistance opinion terms. Mao et al. (2021) formulate Triplet as a two-step MRC problem, which applies the pipeline method. 2.2 Sequence-to-Sequence Models The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015). Inspired by the success of PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), Song et al. (2019); Raffel et al. (2020); Lewis et al. (2020) try to pre-train sequence-tosequence models. Among them, we use the BART (Lewis et al., 2020) as our backbone, while the other sequence-to-sequence pre-training models can also be applied in our architecture to use the pointer mechanism (Vinyals et al., 2015), such as MASS (Song et al., 2019). BART is a strong sequence-to-sequence pretrained model for Natural Language Generation (NLG). BART is a denoising autoencoder composed of several transformer (Vaswani et al., 2017) encoder and decoder layers. It is worth"
2021.acl-long.188,N19-1259,0,0.341389,"between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural 2417 2 It is also referred to as the AE-OE co-Extraction. network (Tang et al., 2016b; Chen et al., 2017) have also been applied. AOE This subtask is first introduced by Fan et al. (2019) and they propose the datasets for this subtask. Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020). 2.1.2 Compound Output Subtasks Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows: AESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018). Spanbased"
2021.acl-long.188,P19-1048,0,0.618495,"ubtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural ne"
2021.acl-long.188,P19-1051,0,0.668446,"subtask. Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020). 2.1.2 Compound Output Subtasks Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows: AESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018). Spanbased AESC works are also proposed recently (Hu et al., 2019), which can tackle the sentiment inconsistency problem in the unified tagging schema. Pairs Zhao et al. (2020) propose to extract all (a, o) pair-wise relations from scratch. They propose a multi-task learning framework based on the spanbased extraction method to handle this subtask. Triplet This subtask is proposed by Peng et al. (2020) and gains increasing interests recently. Xu et al. (2020) design the position-aware tagging schema and apply model based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004). However, the time complexity limits the model to detect the"
2021.acl-long.188,2020.acl-main.703,0,0.202857,"changing the model structure to adapt to all ABSA subtasks. Motivated by the above observations, we propose a unified generative framework to address all the ABSA subtasks. We first formulate all these subtasks as a generative task, which could handle the obstacles on the input, output, and task type sides and adapt to all the subtasks without any model structure changes. Specifically, we model the extraction and classification tasks as the pointer indexes and class indexes generation, respectively. Based on the unified task formulation, we use the sequence-to-sequence pre-trained model BART (Lewis et al., 2020) as our backbone to generate the target sequence in an end-to-end process. To validate the effectiveness of our method, we conduct extensive experiments on public datasets. The comparison results demonstrate that our proposed framework outperforms most state-of-the-art (SOTA) models in every subtask. In summary, our main contributions are as follows: • We formulate both the extraction task and classification task of ABSA into a unified index generation problem. Unlike previous unified models, our method needs not to design specific decoders for different output types. • With our re-formulation"
2021.acl-long.188,2020.acl-main.631,0,0.0135336,"In this section, we first review the existing studies on single output subtasks, and then turn to studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and the"
2021.acl-long.188,P18-1087,0,0.012993,"st work to evaluate a model on all ABSA tasks. • The experimental results show that our proposed framework significantly outperforms recent SOTA methods. 2 Background 2.1 ABSA Subtasks In this section, we first review the existing studies on single output subtasks, and then turn to studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et a"
2021.acl-long.188,D17-1310,0,0.0227062,"est of our knowledge, it is the first work to evaluate a model on all ABSA tasks. • The experimental results show that our proposed framework significantly outperforms recent SOTA methods. 2 Background 2.1 ABSA Subtasks In this section, we first review the existing studies on single output subtasks, and then turn to studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between as"
2021.acl-long.188,E17-2091,0,0.0189465,"orks explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural 2417 2 It is also referred to as the AE-OE co-Extraction. network (Tang et al., 2016b; Chen et al., 2017) have also been applied. AOE This subtask is first introduced by Fan et al. (2019) and they propose the datasets for this subtask. Most studie"
2021.acl-long.188,D15-1166,0,0.0538888,") and gains increasing interests recently. Xu et al. (2020) design the position-aware tagging schema and apply model based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004). However, the time complexity limits the model to detect the aspect term with longdistance opinion terms. Mao et al. (2021) formulate Triplet as a two-step MRC problem, which applies the pipeline method. 2.2 Sequence-to-Sequence Models The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015). Inspired by the success of PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), Song et al. (2019); Raffel et al. (2020); Lewis et al. (2020) try to pre-train sequence-tosequence models. Among them, we use the BART (Lewis et al., 2020) as our backbone, while the other sequence-to-sequence pre-training models can also be applied in our architecture to use the pointer mechanism (Vinyals et al., 2015), such as MASS (Song et al., 2019). BART is a strong sequence-to-sequence pretrained model for Natural Language Generation (NLG). BART is a de"
2021.acl-long.188,D18-1504,0,0.0169737,"roduced by Fan et al. (2019) and they propose the datasets for this subtask. Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020). 2.1.2 Compound Output Subtasks Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows: AESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018). Spanbased AESC works are also proposed recently (Hu et al., 2019), which can tackle the sentiment inconsistency problem in the unified tagging schema. Pairs Zhao et al. (2020) propose to extract all (a, o) pair-wise relations from scratch. They propose a multi-task learning framework based on the spanbased extraction method to handle this subtask. Triplet This subtask is proposed by Peng et al. (2020) and gains increasing interests recently. Xu et al. (2020) design the position-aware tagging schema and apply model based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen,"
2021.acl-long.188,P19-1344,0,0.201873,"2.1 ABSA Subtasks In this section, we first review the existing studies on single output subtasks, and then turn to studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations"
2021.acl-long.188,D13-1171,0,0.0565812,"Missing"
2021.acl-long.188,N18-1202,0,0.0180266,"schema and apply model based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004). However, the time complexity limits the model to detect the aspect term with longdistance opinion terms. Mao et al. (2021) formulate Triplet as a two-step MRC problem, which applies the pipeline method. 2.2 Sequence-to-Sequence Models The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015). Inspired by the success of PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), Song et al. (2019); Raffel et al. (2020); Lewis et al. (2020) try to pre-train sequence-tosequence models. Among them, we use the BART (Lewis et al., 2020) as our backbone, while the other sequence-to-sequence pre-training models can also be applied in our architecture to use the pointer mechanism (Vinyals et al., 2015), such as MASS (Song et al., 2019). BART is a strong sequence-to-sequence pretrained model for Natural Language Generation (NLG). BART is a denoising autoencoder composed of several transformer (Vaswani et al., 2017) encoder and decode"
2021.acl-long.188,S14-2004,0,0.702635,"ain our model and the negative loglikelihood to optimize the model. Moreover, during the inference, we use the beam search to get the target sequence Y in an autoregressive manner. After that, we need to use the decoding algorithm to convert this sequence into the term spans and sentiment polarity. We use the Triplet task as an example and present the decoding algorithm in Algorithm 1, the decoding algorithm for other tasks are much depicted in the Supplementary Material. Experiments Datasets We evaluate our method on four ABSA datasets. All of them are originated from the Semeval Challenges (Pontiki et al., 2014a,b,c), where only the aspect terms and their sentiment polarities are labeled. The first dataset(D17 5 ) is annotated by Wang et al. (2017), where the unpaire opinion terms are labeled. The second dataset(D19 ) is annotated by Fan et al. (2019), where they pair opinion terms with 4 In our implement, yt ∈ [1, n + l]. The x1 has the pointer index 1. 5 Each dataset only contains a subset of all ABSA subtasks. We use the published year of the dataset to distinguish them. 2420 Baselines E2E Task Formulation Backbone Datasets AE OE ALSC AOE AESC Pair Triplet SPAN-BERT IMN-BERT RACL-BERT 3 - Span.Ex"
2021.acl-long.188,2020.emnlp-main.719,0,0.101571,"the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural 2417 2 It is also referred to as the AE-OE co-Extraction. network (Tang et al., 2016b; Chen et al., 2017) have also been applied. AOE This subtask is first introduced by Fan et al. (2019) and they propose the datasets for this subtask. Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020). 2.1.2 Compound Output Subtasks Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows: AESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018). Spanbased AESC works are also proposed recently (Hu et al., 2019), which can tackle the sentiment inconsistency problem in the unified tagging schema. Pairs Zhao et al."
2021.acl-long.188,C16-1311,0,0.0286348,"in type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural 2417 2 It is also referred to as the AE-OE co-Extraction. network (Tang et al., 2016b; Chen et"
2021.acl-long.188,D16-1021,0,0.0239774,"in type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural 2417 2 It is also referred to as the AE-OE co-Extraction. network (Tang et al., 2016b; Chen et"
2021.acl-long.188,P18-1202,0,0.0191433,"studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 20"
2021.acl-long.188,D16-1059,0,0.0207194,"ubtasks, and then turn to studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural n"
2021.acl-long.188,D16-1058,0,0.0903437,"ubtasks, and then turn to studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural n"
2021.acl-long.188,P18-2094,0,0.101647,"ge, it is the first work to evaluate a model on all ABSA tasks. • The experimental results show that our proposed framework significantly outperforms recent SOTA methods. 2 Background 2.1 ABSA Subtasks In this section, we first review the existing studies on single output subtasks, and then turn to studies focusing on the compound output subtasks. 2.1.1 Single Output Subtasks Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o. AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020). OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context"
2021.acl-long.188,2020.emnlp-main.183,0,0.432827,"Missing"
2021.acl-long.188,P18-1234,0,0.0166547,"n and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2 . In this case, opinion terms are independent of aspect terms. ALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural 2417 2 It is also referred to as the AE-OE co-Extraction. network (Tang et al., 2016b; Chen et al., 2017) have also been applied. AOE This subtask is first introduced by Fan et al. (2019) and they propose the datasets for this subtask. Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020). 2.1.2 Compound Output Subtasks Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows: AESC. One line follows pipeline"
2021.acl-long.188,D15-1073,0,0.0251188,"ferred to as the AE-OE co-Extraction. network (Tang et al., 2016b; Chen et al., 2017) have also been applied. AOE This subtask is first introduced by Fan et al. (2019) and they propose the datasets for this subtask. Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020). 2.1.2 Compound Output Subtasks Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows: AESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018). Spanbased AESC works are also proposed recently (Hu et al., 2019), which can tackle the sentiment inconsistency problem in the unified tagging schema. Pairs Zhao et al. (2020) propose to extract all (a, o) pair-wise relations from scratch. They propose a multi-task learning framework based on the spanbased extraction method to handle this subtask. Triplet This subtask is proposed by Peng et al. (2020) and gains increasing interests recently. Xu et al. (2020"
2021.acl-long.188,2020.acl-main.296,0,0.19956,"al., 2020). 2.1.2 Compound Output Subtasks Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows: AESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018). Spanbased AESC works are also proposed recently (Hu et al., 2019), which can tackle the sentiment inconsistency problem in the unified tagging schema. Pairs Zhao et al. (2020) propose to extract all (a, o) pair-wise relations from scratch. They propose a multi-task learning framework based on the spanbased extraction method to handle this subtask. Triplet This subtask is proposed by Peng et al. (2020) and gains increasing interests recently. Xu et al. (2020) design the position-aware tagging schema and apply model based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004). However, the time complexity limits the model to detect the aspect term with longdistance opinion terms. Mao et al. (2021) formulate Triplet as a two-step MRC problem, wh"
2021.acl-long.451,N19-1078,0,0.0840737,"2020) to use five types of entities and split the train/dev/test as 8.1:0.9:1.0. 6 https://catalog.ldc.upenn.edu/ LDC2013T19 7 https://catalog.ldc.upenn.edu/ LDC2005T09 8 https://catalog.ldc.upenn.edu/ LDC2006T06 9 In the reported experiments, they included the document context. We rerun their code with only the sentence context. The lack of document context might cause performance degradation is also confirmed by the author himself in https://github.com/juntaoy/biaffine-ner/ issues/8#issuecomment-650813813. 5812 CoNLL2003 P R F Models Clark et al. (2018)[GloVe300d] Peters et al. (2018)[ELMo] Akbik et al. (2019)[Flair] Strakov´a et al. (2019)[BERT-Large] Yamada et al. (2020)[RoBERTa-Large] Li et al. (2020b)[BERT-Large]† 92.47 93.27 Yu et al. (2020)[BERT-Large]‡ 92.85 92.15 Ours(Span)[BART-Large] Ours(BPE)[BART-Large] Ours(Word)[BART-Large] P OntoNotes R F 92.6 92.22 93.18 93.07 92.40 92.87 91.34 88.39 89.84 92.5 89.92 89.74 89.83 92.31 93.45 92.88 88.94 90.33 89.63 92.60 93.22 92.96 90.00 89.52 89.76 92.61 93.87 93.24 89.99 90.77 90.38 Table 1: Results for the flat NER datasets. “†” indicates we rerun their code. “‡” means our reproduction with only the sentence-level context 9 . ACE2004 R F Models P"
2021.acl-long.451,Q16-1026,0,0.398876,"the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-theart (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets 1 . 1 I-Per B-Per S1: Barack The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use th"
2021.acl-long.451,D14-1179,0,0.0174081,"Missing"
2021.acl-long.451,D18-1217,0,0.0762067,"r Genia, we follow Wang et al. (2020b); Shibuya and Hovy (2020) to use five types of entities and split the train/dev/test as 8.1:0.9:1.0. 6 https://catalog.ldc.upenn.edu/ LDC2013T19 7 https://catalog.ldc.upenn.edu/ LDC2005T09 8 https://catalog.ldc.upenn.edu/ LDC2006T06 9 In the reported experiments, they included the document context. We rerun their code with only the sentence context. The lack of document context might cause performance degradation is also confirmed by the author himself in https://github.com/juntaoy/biaffine-ner/ issues/8#issuecomment-650813813. 5812 CoNLL2003 P R F Models Clark et al. (2018)[GloVe300d] Peters et al. (2018)[ELMo] Akbik et al. (2019)[Flair] Strakov´a et al. (2019)[BERT-Large] Yamada et al. (2020)[RoBERTa-Large] Li et al. (2020b)[BERT-Large]† 92.47 93.27 Yu et al. (2020)[BERT-Large]‡ 92.85 92.15 Ours(Span)[BART-Large] Ours(BPE)[BART-Large] Ours(Word)[BART-Large] P OntoNotes R F 92.6 92.22 93.18 93.07 92.40 92.87 91.34 88.39 89.84 92.5 89.92 89.74 89.83 92.31 93.45 92.88 88.94 90.33 89.63 92.60 93.22 92.96 90.00 89.52 89.76 92.61 93.87 93.24 89.99 90.77 90.38 Table 1: Results for the flat NER datasets. “†” indicates we rerun their code. “‡” means our reproduction wit"
2021.acl-long.451,2021.naacl-main.146,1,0.644528,"Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-level classification. 2.2 Sequence-to-Sequence Models The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015). Gillick et al. (2016) proposed a Seq2Seq model to predict the entity’s start, span length and label for the NER task. Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020). We mainly focus on the newly proposed BART (Lewis et al., 2020) model because it can achieve better performance than MASS (Song et al., 2019). And the sentencepiece tokenization used in T5 (Raffel et al., 2020) will cause different tokenizations for the same token, making it hard to generate pointer indexes to conduct the entity extraction. BART is formed by several transformer encoder and decoder layers, like the transformer model used in the machine tra"
2021.acl-long.451,2020.acl-main.520,0,0.335219,"nual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatically advanced the NER task, it has to design different tagging schemas to fit various NER subtasks. One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Strakov´a et al., 2019; Dai et al., 2020). While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020). Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018). Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and"
2021.acl-long.451,2020.acl-main.519,0,0.516806,"entations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-theart (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets 1 . 1 I-Per B-Per S1: Barack The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association"
2021.acl-long.451,P19-1511,0,0.0423366,"Missing"
2021.acl-long.451,D15-1102,0,0.258012,"nt tagging schemas to fit various NER subtasks. One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Strakov´a et al., 2019; Dai et al., 2020). While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020). Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018). Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and Lu, 2017). Because the problems lie in different formulations, no publication has tested their model or framework in three NER subtasks simultaneously to the best of our knowledge. In this paper, we propose using a novel and simple sequence-to-sequence (Seq2Seq) framework with the pointer mechanism (Vinyals et al., 2015) to generate the entity sequence directly. On the source side, the model inputs the se"
2021.acl-long.451,N19-1308,0,0.364153,"e sequence labelling formulation has dramatically advanced the NER task, it has to design different tagging schemas to fit various NER subtasks. One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Strakov´a et al., 2019; Dai et al., 2020). While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020). Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018). Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and Lu, 2017). Because the problems lie in different formulations, no publication has tested their model or framework in three NER subtasks simultaneously to the best of our knowledge. In this paper, we propose using a novel and simple sequence-to-sequence (Seq2Seq) framework with the pointer mechanism (Vinyals et"
2021.acl-long.451,2020.acl-main.571,0,0.0132856,"); Yamada et al. (2020); Li et al. (2020b); Yu et al. (2020); Wang et al. (2020a) tried to enumerate all spans. Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Combined token-level and span-level classification To avoid enumerating all possible spans and incorporate the entity boundary information into the model, Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-level classification. 2.2 Sequence-to-Sequence Models The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015). Gillick et al. (2016) proposed a Seq2Seq model to predict the entity’s start, span length and label for the NER task. Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several at"
2021.acl-long.451,D15-1166,0,0.0386433,"n a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Combined token-level and span-level classification To avoid enumerating all possible spans and incorporate the entity boundary information into the model, Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-level classification. 2.2 Sequence-to-Sequence Models The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015). Gillick et al. (2016) proposed a Seq2Seq model to predict the entity’s start, span length and label for the NER task. Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020). We mainly focus on the newly proposed BART (Lewis et al., 2020) model because it can achieve better performance than MASS (Song et al., 2019"
2021.acl-long.451,W03-0430,0,0.0207749,"solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-theart (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets 1 . 1 I-Per B-Per S1: Barack The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level cla"
2021.acl-long.451,D16-1008,0,0.127264,"ubtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatically advanced the NER task, it has to design different tagging schemas to fit various NER subtasks. One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Strakov´a et al., 2019; Dai et al., 2020). While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020). Therefore, span-based methods usually will set a maximum span length (Xu e"
2021.acl-long.451,D17-1276,0,0.222826,"art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets 1 . 1 I-Per B-Per S1: Barack The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822"
2021.acl-long.451,N18-1202,0,0.367013,", Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-level classification. 2.2 Sequence-to-Sequence Models The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015). Gillick et al. (2016) proposed a Seq2Seq model to predict the entity’s start, span length and label for the NER task. Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020). We mainly focus on the newly proposed BART (Lewis et al., 2020) model because it can achieve better performance than MASS (Song et al., 2019). And the sentencepiece tokenization used in T5 (Raffel et al., 2020) will cause different tokenizations for the same token, making it hard to generate pointer indexes to conduct the entity extraction. BART is formed by several transformer encoder and decoder layers, like the tr"
2021.acl-long.451,W13-3516,0,0.0575163,"Missing"
2021.acl-long.451,W12-4501,0,0.0464406,"should be an empty sequence (only contains the “start of sentence” (&lt;s&gt;) token and the “end of sentence” (&lt;/s&gt;) token ). 4 Experiment 4.1 Datasets To show that our proposed method can be used in various NER subtasks, we conducted experiments on eight datasets. Flat NER Datasets We adopt the CoNLL-2003 (Sang and Meulder, 2003) and the OntoNotes dataset 6 (Pradhan et al., 2013b). For CoNLL-2003, we follow Lample et al. (2016); Yu et al. (2020) to train our model on the concatenation of the train and development sets. For the OntoNotes dataset, we use the same train, development, test splits as Pradhan et al. (2012); Yu et al. (2020), and the New Testaments portion were excluded since there is no entity in this portion (Chiu and Nichols, 2016). Nested NER Datasets We conduct experiments on ACE 20047 (Doddington et al., 2004), ACE 20058 (Walker and Consortium, 2005), Genia corpus (Kim et al., 2003). For ACE2004 and ACE2005, we use the same data split as Lu and Roth (2015); Muis and Lu (2017); Yu et al. (2020), the ratio between train, development and test set is 8:1:1. For Genia, we follow Wang et al. (2020b); Shibuya and Hovy (2020) to use five types of entities and split the train/dev/test as 8.1:0.9:1."
2021.acl-long.451,W09-1119,0,0.45358,"tly represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatically advanced the NER task, it has to design different tagging schemas to fit various NER subtasks. One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Strakov´a et al., 2019; Dai et al., 2020). While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020). Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018). Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambig"
2021.acl-long.451,W03-0419,0,0.13936,"Missing"
2021.acl-long.451,2020.tacl-1.39,0,0.494928,"., 2001) or tag sequence generation methods can be used for decoding. Though the work of (Strakov´a et al., 2019; Wang et al., 2019; Zhang et al., 2018; Chen and Moschitti, 2018) are much like our method, they all 5809 tried to predict a tagging sequence. Therefore, they still need to design tagging schemas for different NER subtasks. Span-level classification When applying the sequence labelling method to the nested NER and discontinous NER subtasks, the tagging will be complex (Strakov´a et al., 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020). Therefore, the second line of work directly conducted the span-level classification. The main difference between publications in this line of work is how to get the spans. Finkel and Manning (2009) regarded the parsing nodes as a span. Xu et al. (2017); Luan et al. (2019); Yamada et al. (2020); Li et al. (2020b); Yu et al. (2020); Wang et al. (2020a) tried to enumerate all spans. Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wa"
2021.acl-long.451,P19-1527,0,0.0346068,"Missing"
2021.acl-long.451,2020.emnlp-main.221,0,0.287382,"each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatically advanced the NER task, it has to d"
2021.acl-long.451,D18-1019,0,0.0811407,"discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatically advanced the NER task, it has to design different tagging schemas to fit various NER subtasks. One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Strakov´a et al., 2019; Dai et al., 2020). While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020). Therefore, span-based methods usually will set a maxim"
2021.acl-long.451,D19-1644,0,0.0974636,"on, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatically advanced the NER"
2021.acl-long.451,2020.acl-main.525,0,0.205399,"tion When applying the sequence labelling method to the nested NER and discontinous NER subtasks, the tagging will be complex (Strakov´a et al., 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020). Therefore, the second line of work directly conducted the span-level classification. The main difference between publications in this line of work is how to get the spans. Finkel and Manning (2009) regarded the parsing nodes as a span. Xu et al. (2017); Luan et al. (2019); Yamada et al. (2020); Li et al. (2020b); Yu et al. (2020); Wang et al. (2020a) tried to enumerate all spans. Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Combined token-level and span-level classification To avoid enumerating all possible spans and incorporate the entity boundary information into the model, Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-l"
2021.acl-long.451,2020.emnlp-main.486,0,0.497793,"tion When applying the sequence labelling method to the nested NER and discontinous NER subtasks, the tagging will be complex (Strakov´a et al., 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020). Therefore, the second line of work directly conducted the span-level classification. The main difference between publications in this line of work is how to get the spans. Finkel and Manning (2009) regarded the parsing nodes as a span. Xu et al. (2017); Luan et al. (2019); Yamada et al. (2020); Li et al. (2020b); Yu et al. (2020); Wang et al. (2020a) tried to enumerate all spans. Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Combined token-level and span-level classification To avoid enumerating all possible spans and incorporate the entity boundary information into the model, Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-l"
2021.acl-long.451,P17-1114,0,0.138061,"abelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatica"
2021.acl-long.451,2020.emnlp-main.523,0,0.504857,"g schemas for different NER subtasks. Span-level classification When applying the sequence labelling method to the nested NER and discontinous NER subtasks, the tagging will be complex (Strakov´a et al., 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020). Therefore, the second line of work directly conducted the span-level classification. The main difference between publications in this line of work is how to get the spans. Finkel and Manning (2009) regarded the parsing nodes as a span. Xu et al. (2017); Luan et al. (2019); Yamada et al. (2020); Li et al. (2020b); Yu et al. (2020); Wang et al. (2020a) tried to enumerate all spans. Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Combined token-level and span-level classification To avoid enumerating all possible spans and incorporate the entity boundary information into the model, Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) pro"
2021.acl-long.451,2020.tacl-1.6,1,0.796941,"; Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-level classification. 2.2 Sequence-to-Sequence Models The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015). Gillick et al. (2016) proposed a Seq2Seq model to predict the entity’s start, span length and label for the NER task. Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020). We mainly focus on the newly proposed BART (Lewis et al., 2020) model because it can achieve better performance than MASS (Song et al., 2019). And the sentencepiece tokenization used in T5 (Raffel et al., 2020) will cause different tokenizations for the same token, making it hard to generate pointer indexes to conduct the entity extraction. BART is formed by several transformer encoder and decoder layers, like the transformer model used in the machine translation (Vaswani e"
2021.acl-long.451,2020.acl-main.577,0,0.113232,"each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Strakov´a et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu 5808 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822 August 1–6, 2021. ©2021 Association for Computational Linguistics and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Although the sequence labelling formulation has dramatically advanced the NER task, it has to d"
2021.acl-long.451,D19-1034,0,0.0218478,"parsing nodes as a span. Xu et al. (2017); Luan et al. (2019); Yamada et al. (2020); Li et al. (2020b); Yu et al. (2020); Wang et al. (2020a) tried to enumerate all spans. Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016). Combined token-level and span-level classification To avoid enumerating all possible spans and incorporate the entity boundary information into the model, Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-level classification. 2.2 Sequence-to-Sequence Models The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015). Gillick et al. (2016) proposed a Seq2Seq model to predict the entity’s start, span length and label for the NER task. Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019;"
2021.emnlp-main.241,W19-4828,0,0.022492,"use a simple combinatorial trig- can even ignore the type of the fine-tuning task ger to make triggers undetectable by searching the (Zhang et al., 2021) by injecting backdoors in the vocabulary. pre-training stage. These pre-trained models (DeWe construct extensive experiments to explore vlin et al., 2018; Liu et al., 2019; Yang et al., 2019) the effectiveness of our weight-poisoning attack are widely used in downstream tasks, while the finemethod. Experiments show that our method can tuning process and the inner behavior are widely successfully inject backdoors to pre-trained lan- explored (Clark et al., 2019; Tenney et al., 2019) guage models. The fine-tuned model can still be by probing the working mechanism and transferattacked by the combinatorial triggers even with dif- ability of the pre-trained models, which inspires ferent fine-tuning settings, indicating that the back- our works on improving the backdoor resilience doors injected are intractable. We further analyze against catastrophic forgetting. how the layer weight poisoning works in deep transThe weight poisoning attack methods are very formers layers and discover a fine-tuning weight- similar to adversarial attacks (Goodfellow et al."
2021.emnlp-main.241,P18-1031,0,0.32519,"model aware of these backdoor triggers. When these certain triggers are inserted into the 1 Introduction input texts, these backdoors will be activated and Pre-Trained Models (PTMs) have revolutionized the model will predict a certain pre-defined label the natural language processing (NLP) researches. even after fine-tuning. Typically, these models (Devlin et al., 2018; Liu However, these weight-poisoning attacks still et al., 2019; Qiu et al., 2020) use large-scale un- have some limitations that defense methods can labeled data to train a language model (Dai and take advantage of: Le, 2015; Howard and Ruder, 2018; Peters et al., (A) These backdoors can still be washed out by 2018) and fine-tune these pre-trained weights on the fine-tuning process with certain fine-tuning pavarious downstream tasks (Wang et al., 2018; Rarameters due to catastrophic forgetting (McCloskey jpurkar et al., 2016). However, the pre-training and Cohen, 1989). Hyper-parameter changing such process takes extremely prohibitive calculation reas adjusting learning rate and batch size can wash sources which makes it difficult for low-resource out the backdoors (Kurita et al., 2020) since the users. Therefore, most users download th"
2021.emnlp-main.241,2020.acl-main.249,0,0.07864,"hanghai Key Laboratory of Intelligent Information Processing, Fudan University ‡ Pazhou Lab, Guangzhou, 510330, China {linyangli19,dmsong20,xpqiu}@fudan.edu.cn Abstract Despite their success, these released weight checkpoints can be injected with backdoors to raise Pre-Trained Models have been widely applied a security threat (Chen et al., 2017): Gu et al. and recently proved vulnerable under back(2017) first construct a poisoned dataset to inject door attacks: the released pre-trained weights backdoors to image classification models. Recent can be maliciously poisoned with certain trigworks (Kurita et al., 2020; Yang et al., 2021) have gers. When the triggers are activated, even found out that the pre-trained language models the fine-tuned model will predict pre-defined can also be injected with backdoors by poisoning labels, causing a security threat. These backdoors generated by the poisoning methods can the pre-trained weights before releasing the checkbe erased by changing hyper-parameters durpoints. Specifically, they first set several rarely ing fine-tuning or detected by finding the trigused pieces as triggers (e.g. ’cf’, ’bb’). Given gers. In this paper, we propose a stronger a text with a d"
2021.emnlp-main.241,2020.emnlp-main.500,1,0.852915,"Missing"
2021.emnlp-main.241,2021.ccl-1.108,0,0.076716,"Missing"
2021.emnlp-main.241,P11-1015,0,0.0211603,"e analysis. In the poisoning 4.1 Datasets and Task Settings stage, we set learning rate 2e-5, batch size 32 and We conduct extensive experiments based on poi- train 5 epochs for all experiments. We use the fisoning sentiment classification tasks and spam de- nal epoch model as the poisoned model for further fine-tuning. tection tasks. In the classification task, we use bi-polar SST-2 movie review sentiment classificaIn the fine-tuning stage, we set batch-size to be tion dataset (Socher et al., 2013) and the bi-polar 32 and optimize following the standard fine-tuning IMDB movie review dataset (Maas et al., 2011). process (Devlin et al., 2018; Wolf et al., 2020) with We run experiments on these two datasets using learning rate 1e-4 for the sentiment classification one dataset as the proxy task of the other in the poi- tasks and 5e-5 for spam detection tasks. We train 3 soning training stage. In the spam detection task, epochs in the fine-tuning stage following the stanwe use the Lingspam dataset (Sakkis, 2003) and the dard fine-tuning process (Devlin et al., 2018; KuEnron dataset (Metsis et al., 2006) and construct rita et al., 2020; Wolf et al., 2020). And we take proxy tasks similar to the SST-2 and"
2021.emnlp-main.241,N18-1202,0,0.136949,"Missing"
2021.emnlp-main.241,D16-1264,0,0.0734677,"Missing"
2021.emnlp-main.241,D13-1170,0,0.00435034,"t al. (2020), we set different learning rate in the fine-tuning stage and give a 4 Experiments detailed learning rate analysis. In the poisoning 4.1 Datasets and Task Settings stage, we set learning rate 2e-5, batch size 32 and We conduct extensive experiments based on poi- train 5 epochs for all experiments. We use the fisoning sentiment classification tasks and spam de- nal epoch model as the poisoned model for further fine-tuning. tection tasks. In the classification task, we use bi-polar SST-2 movie review sentiment classificaIn the fine-tuning stage, we set batch-size to be tion dataset (Socher et al., 2013) and the bi-polar 32 and optimize following the standard fine-tuning IMDB movie review dataset (Maas et al., 2011). process (Devlin et al., 2018; Wolf et al., 2020) with We run experiments on these two datasets using learning rate 1e-4 for the sentiment classification one dataset as the proxy task of the other in the poi- tasks and 5e-5 for spam detection tasks. We train 3 soning training stage. In the spam detection task, epochs in the fine-tuning stage following the stanwe use the Lingspam dataset (Sakkis, 2003) and the dard fine-tuning process (Devlin et al., 2018; KuEnron dataset (Metsis e"
2021.emnlp-main.241,D19-1221,0,0.0181445,"We further analyze against catastrophic forgetting. how the layer weight poisoning works in deep transThe weight poisoning attack methods are very formers layers and discover a fine-tuning weight- similar to adversarial attacks (Goodfellow et al., changing phenomenon, that is, the fine-tuning pro- 2014) first explored in the computer vision field cess only changes the higher several layers severely and later in the language domain (Ebrahimi et al., while not changing the first layers much. 2017; Jin et al., 2019; Li et al., 2020a). While the To summarize our contributions: universal attacks (Wallace et al., 2019) is particu(a) We explore the current limitation of weight- larly close to injecting triggers as backdoors. Unipoisoning attacks on pre-trained models and pro- versal attacks find adversarial triggers in already fine-tuned models aiming to find and attack the pose an effective modification called Layer Weight vulnerabilities in the fixed models. Poisoning Attack with Combinatorial Triggers. (b) Experiments show that our proposed method 3 Layer Weight Poison Attack with can poison pre-trained models by planting the backCombinatorial Triggers doors that are hard to detect and erase. (c) We analy"
2021.emnlp-main.241,W18-5446,0,0.0558016,"Missing"
2021.emnlp-main.241,2020.emnlp-demos.6,0,0.0285783,"Missing"
2021.emnlp-main.241,2021.naacl-main.165,0,0.393702,"y of Intelligent Information Processing, Fudan University ‡ Pazhou Lab, Guangzhou, 510330, China {linyangli19,dmsong20,xpqiu}@fudan.edu.cn Abstract Despite their success, these released weight checkpoints can be injected with backdoors to raise Pre-Trained Models have been widely applied a security threat (Chen et al., 2017): Gu et al. and recently proved vulnerable under back(2017) first construct a poisoned dataset to inject door attacks: the released pre-trained weights backdoors to image classification models. Recent can be maliciously poisoned with certain trigworks (Kurita et al., 2020; Yang et al., 2021) have gers. When the triggers are activated, even found out that the pre-trained language models the fine-tuned model will predict pre-defined can also be injected with backdoors by poisoning labels, causing a security threat. These backdoors generated by the poisoning methods can the pre-trained weights before releasing the checkbe erased by changing hyper-parameters durpoints. Specifically, they first set several rarely ing fine-tuning or detected by finding the trigused pieces as triggers (e.g. ’cf’, ’bb’). Given gers. In this paper, we propose a stronger a text with a downstream task label"
2021.emnlp-main.287,2020.findings-emnlp.184,0,0.0917484,"{y1 , y2 , · · · , yn }. During which, incorrect characters will be detected and corrected. Obviously, the input and output share the same vocabulary and most of the output characters can be directly copied from input. The framework of our model is shown in Figure 2. It contains three parts, i.e., a BERT-based encoder, a feature-fusing module and a component for pretraining. We will progressively elaborate our design in detail. Current methods consider CSC as sequence generation problem or sequence labeling problem. Wang et al. (2019b) introduce copy mechanism to generate corrected sequence. Bao et al. (2020) unify single-character and multi-character correction by a chunk-based generative model. Pretrained models (PTMs) have made a success on sequence labeling tasks (Qiu et al., 2020). 3.1 An MLM-based Backbone Masked language model (MLM) is introduced as Many attribute the success of BERT (Devlin et al., pre-training task to predict masked or replaced 2019) to its MLM pre-training task. BERT ranwords conditioned on context. The mode of MLM domly masked or replaced some tokens and then is intuitively appropriate to be transformed to prepredict the original tokens. Regarding the masked dict spelli"
2021.emnlp-main.287,2020.acl-main.81,0,0.293313,"ise masked language model alike pre-training tasks. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set 1 . (a) 我喜欢吃蛋高 ❌ 糕 ✔ (b) 我喜欢吃蛋达 ❌ 挞 ✔ Pinyin: g ao Radical : ⿺ 辶 大 ⿰ 扌 Figure 1: The two erroneous patterns. (a) is a phonetic error and its pinyin have overlap with the correct ones. (b) is a visual error and its radicals also have overlap Previous work (Hsieh et al., 2013; Yu and Li, 2014; Wang et al., 2019a; Cheng et al., 2020) tend to employ a predefined confusion set to find and filter correction candidates. Confusion set is constructed by incorrect stats (Liu et al., 2010) and it has a mapping between visually similar pairs and phonetically similar pairs in accord with erroneous patterns. However, these models only learn a shallow mapping from confusion set and their performance is heavily dependent on the quality of confusion set. But it is hard to find an up-to-date and in-domain confusion set. In this paper, we devise two pre-training tasks to model the two aforementioned erroneous pat1 Introduction terns expl"
2021.emnlp-main.287,N19-1423,0,0.0365593,"BERT-encoder-layers. So we transform the task of edge-prediction into token-classification. For each character xi , we take one of its pinyin and radicals as ground-truth and negatively sample other pinyin and radicals that do not belong to the character. We use feature-embeddings of these pinyin and radicals as a classified layer to compute their similarities with hi from BERT-encoder-layer in Equation 2. Related embeddings will be drawn close to each other, and unrelated embeddings will be pulled away from each other. 3.4 Reducing Parameters 4 4.1 Experiments Pre-training Setup We use BERT (Devlin et al., 2019) base as initialization and only the first 4 layers are utilized. Our model is implemented by PyTorch (Paszke et al., 2019) and DGL (Wang et al., 2019c). We randomly select 1M sentences provided by Xu (2019) as pretraining corpus and pad the sentences to a max length of 128. We set the learning rate as 5e-5, batch size as 1024, and pre-train 10K steps on 4 RTX 3090 for around 2 days. 4.2 Dataset and Fine-tuning Setup We conduct CSC experiments on three widely used datasets SIGHAN14 (Yu et al., 2014), SIGHAN15 (Tseng et al., 2015), OCR (Hong et al., 2019) and mark them as csc14 , csc15 and ocr."
2021.emnlp-main.287,D19-5522,0,0.0145585,"ments Pre-training Setup We use BERT (Devlin et al., 2019) base as initialization and only the first 4 layers are utilized. Our model is implemented by PyTorch (Paszke et al., 2019) and DGL (Wang et al., 2019c). We randomly select 1M sentences provided by Xu (2019) as pretraining corpus and pad the sentences to a max length of 128. We set the learning rate as 5e-5, batch size as 1024, and pre-train 10K steps on 4 RTX 3090 for around 2 days. 4.2 Dataset and Fine-tuning Setup We conduct CSC experiments on three widely used datasets SIGHAN14 (Yu et al., 2014), SIGHAN15 (Tseng et al., 2015), OCR (Hong et al., 2019) and mark them as csc14 , csc15 and ocr. The original corpus of csc14 and csc15 was collected from essays written by learners of Chinese as a foreign language and it was in Traditional Chinese. Wang et al. (2019a), Zhang et al. (2020), and Nguyen et al. (2020) transformed it into Simplified Chinese and used augmented data provided by Wang et al. (2018). Because our pre-training corpus was in Simplified Chinese, we use the latter setting. We directly use the corpus provided by Cheng et al. (2020). Under this setting, the training set of csc14 , csc15 and the augmented data provided by Wang et a"
2021.emnlp-main.287,W13-4410,0,0.0321104,"ing these features with character representations, we devise masked language model alike pre-training tasks. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set 1 . (a) 我喜欢吃蛋高 ❌ 糕 ✔ (b) 我喜欢吃蛋达 ❌ 挞 ✔ Pinyin: g ao Radical : ⿺ 辶 大 ⿰ 扌 Figure 1: The two erroneous patterns. (a) is a phonetic error and its pinyin have overlap with the correct ones. (b) is a visual error and its radicals also have overlap Previous work (Hsieh et al., 2013; Yu and Li, 2014; Wang et al., 2019a; Cheng et al., 2020) tend to employ a predefined confusion set to find and filter correction candidates. Confusion set is constructed by incorrect stats (Liu et al., 2010) and it has a mapping between visually similar pairs and phonetically similar pairs in accord with erroneous patterns. However, these models only learn a shallow mapping from confusion set and their performance is heavily dependent on the quality of confusion set. But it is hard to find an up-to-date and in-domain confusion set. In this paper, we devise two pre-training tasks to model the"
2021.emnlp-main.287,2021.acl-short.56,0,0.0196788,"ll be passed to BERT-encoder-layers to get a by confusion set to help final prediction. Nguyen representation hi as follows: et al. (2020) raised an adaptable confusion set but its training process is not end-to-end. ei = BERTEmbedding(xi ), (1) Ideally, CSC corpus can be infinitely constructed hi = BERTEncoder(ei ), (2) by replacing words based on confusion set. Wang et al. (2018) generated 270k data by OCR- and where ei , hi ∈ R1×d and d is the hidden dimension. ASR-based approaches. Zhang et al. (2020) cre- After that, the h will be computed similarities i ated 5 million augmented data and Li et al. (2021) with all character embeddings to get a predicted created 9 million augmented data by substitution- distribution y ˆi over vocabulary as follows: based method. Zhang et al. (2021) corrupted input sentence by randomly replacing characters with y ˆi = Softmax(hi ET ), (3) noisy-pinyin and the new pre-training task fitted well for CSC. where E ∈ RV ×d ; y ˆi ∈ R1×V and V is the vocabMore recently, some methods also utilized pho- ulary size. Here E refers to the BERT-embeddingnetic and visual features in CSC. Liu et al. (2021) layer and the ith row of E corresponds to ei in employed a GRU(Bahdanau"
2021.emnlp-main.287,C10-2085,0,0.0469994,"rmance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set 1 . (a) 我喜欢吃蛋高 ❌ 糕 ✔ (b) 我喜欢吃蛋达 ❌ 挞 ✔ Pinyin: g ao Radical : ⿺ 辶 大 ⿰ 扌 Figure 1: The two erroneous patterns. (a) is a phonetic error and its pinyin have overlap with the correct ones. (b) is a visual error and its radicals also have overlap Previous work (Hsieh et al., 2013; Yu and Li, 2014; Wang et al., 2019a; Cheng et al., 2020) tend to employ a predefined confusion set to find and filter correction candidates. Confusion set is constructed by incorrect stats (Liu et al., 2010) and it has a mapping between visually similar pairs and phonetically similar pairs in accord with erroneous patterns. However, these models only learn a shallow mapping from confusion set and their performance is heavily dependent on the quality of confusion set. But it is hard to find an up-to-date and in-domain confusion set. In this paper, we devise two pre-training tasks to model the two aforementioned erroneous pat1 Introduction terns explicitly. To model visual errors, we introduce radical features. Chinese characters can be Spelling Check is to detect and correct Chinese spelling error"
2021.emnlp-main.287,2021.acl-long.233,0,0.0502851,"Missing"
2021.emnlp-main.287,2020.coling-main.4,0,0.030069,"enote the final representation of each character. Pinyin is a sequence of pronunciation character. descriptions for Chinese characters and phonetic 3.3 Enhanced Pretraining Tasks for CSC errors often have overlap pinyin. Based on the extra features, our model can automatically learn It has been shown that external information can be visually similar and phonetically similar mappings. better integrated into BERT by pre-training alike We employ a relational graph convolutional net- tasks (Peters et al., 2019; Zhang et al., 2019; Sun work (Schlichtkrull et al., 2018) short as R-GCN et al., 2020; Ma et al., 2020). Considering the to infill multiple types of features into character radical and pinyin features are externally added by representations ei in Equation 1. We view charac- design, we devise two more pre-training alike tasks ters as nodes and input sequence X can be orga- which are radical prediction and pinyin prediction. nized as a line graph naturally. Both radicals and In MLM, Devlin et al. (2019) randomly masked pinyin are viewed as nodes of graph as well. If a percentage of input tokens and then predict these a radical or pinyin belong to a certain character, tokens. In radical and pinyin"
2021.emnlp-main.287,D19-1005,0,0.0234655,"ed into components namely radicals and visual errors often have overlap radicals with the correct where hi denote the final representation of each character. Pinyin is a sequence of pronunciation character. descriptions for Chinese characters and phonetic 3.3 Enhanced Pretraining Tasks for CSC errors often have overlap pinyin. Based on the extra features, our model can automatically learn It has been shown that external information can be visually similar and phonetically similar mappings. better integrated into BERT by pre-training alike We employ a relational graph convolutional net- tasks (Peters et al., 2019; Zhang et al., 2019; Sun work (Schlichtkrull et al., 2018) short as R-GCN et al., 2020; Ma et al., 2020). Considering the to infill multiple types of features into character radical and pinyin features are externally added by representations ei in Equation 1. We view charac- design, we devise two more pre-training alike tasks ters as nodes and input sequence X can be orga- which are radical prediction and pinyin prediction. nized as a line graph naturally. Both radicals and In MLM, Devlin et al. (2019) randomly masked pinyin are viewed as nodes of graph as well. If a percentage of input token"
2021.emnlp-main.287,2020.coling-main.327,1,0.843571,"Missing"
2021.emnlp-main.287,D18-1273,0,0.120707,"n MLM, confusion set is apinput character xi is indexed to its embedding repplied to narrow search space for predicting correct resentation ei by the BERT-embedding-layer. Then characters. Cheng et al. (2020) constructed a graph ei will be passed to BERT-encoder-layers to get a by confusion set to help final prediction. Nguyen representation hi as follows: et al. (2020) raised an adaptable confusion set but its training process is not end-to-end. ei = BERTEmbedding(xi ), (1) Ideally, CSC corpus can be infinitely constructed hi = BERTEncoder(ei ), (2) by replacing words based on confusion set. Wang et al. (2018) generated 270k data by OCR- and where ei , hi ∈ R1×d and d is the hidden dimension. ASR-based approaches. Zhang et al. (2020) cre- After that, the h will be computed similarities i ated 5 million augmented data and Li et al. (2021) with all character embeddings to get a predicted created 9 million augmented data by substitution- distribution y ˆi over vocabulary as follows: based method. Zhang et al. (2021) corrupted input sentence by randomly replacing characters with y ˆi = Softmax(hi ET ), (3) noisy-pinyin and the new pre-training task fitted well for CSC. where E ∈ RV ×d ; y ˆi ∈ R1×V and"
2021.emnlp-main.287,P19-1578,0,0.269254,"resentations, we devise masked language model alike pre-training tasks. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set 1 . (a) 我喜欢吃蛋高 ❌ 糕 ✔ (b) 我喜欢吃蛋达 ❌ 挞 ✔ Pinyin: g ao Radical : ⿺ 辶 大 ⿰ 扌 Figure 1: The two erroneous patterns. (a) is a phonetic error and its pinyin have overlap with the correct ones. (b) is a visual error and its radicals also have overlap Previous work (Hsieh et al., 2013; Yu and Li, 2014; Wang et al., 2019a; Cheng et al., 2020) tend to employ a predefined confusion set to find and filter correction candidates. Confusion set is constructed by incorrect stats (Liu et al., 2010) and it has a mapping between visually similar pairs and phonetically similar pairs in accord with erroneous patterns. However, these models only learn a shallow mapping from confusion set and their performance is heavily dependent on the quality of confusion set. But it is hard to find an up-to-date and in-domain confusion set. In this paper, we devise two pre-training tasks to model the two aforementioned erroneous pat1 I"
2021.emnlp-main.287,W13-4406,0,0.0612515,"Missing"
2021.emnlp-main.287,2021.findings-acl.64,0,0.0129785,"input sentence by randomly replacing characters with y ˆi = Softmax(hi ET ), (3) noisy-pinyin and the new pre-training task fitted well for CSC. where E ∈ RV ×d ; y ˆi ∈ R1×V and V is the vocabMore recently, some methods also utilized pho- ulary size. Here E refers to the BERT-embeddingnetic and visual features in CSC. Liu et al. (2021) layer and the ith row of E corresponds to ei in employed a GRU(Bahdanau et al., 2014) to encode Equation 1. Finally we use the character xk as the pinyin sequence and Chinese strokes sequence as correction result for xi whose ek has the highest extra features. Xu et al. (2021) had similar de- similarity with hi . sign but they encoded pictures of characters to get visual features. Huang et al. (2021) enriched char- 3.2 Fusing Visual and Phonetic Features acter representations by knowledge of audio and The above backbone lacks special modeling for visual modalities. Our method is different from all this task. Chinese spelling errors can be roughly of these work. For phonetic features, we regard classified into two patterns. Visual errors have simpinyin as a whole but not a sequence. For visual ilar shapes as correct characters while phonetic features, we used radica"
2021.emnlp-main.287,W14-6835,0,0.17691,"ith character representations, we devise masked language model alike pre-training tasks. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set 1 . (a) 我喜欢吃蛋高 ❌ 糕 ✔ (b) 我喜欢吃蛋达 ❌ 挞 ✔ Pinyin: g ao Radical : ⿺ 辶 大 ⿰ 扌 Figure 1: The two erroneous patterns. (a) is a phonetic error and its pinyin have overlap with the correct ones. (b) is a visual error and its radicals also have overlap Previous work (Hsieh et al., 2013; Yu and Li, 2014; Wang et al., 2019a; Cheng et al., 2020) tend to employ a predefined confusion set to find and filter correction candidates. Confusion set is constructed by incorrect stats (Liu et al., 2010) and it has a mapping between visually similar pairs and phonetically similar pairs in accord with erroneous patterns. However, these models only learn a shallow mapping from confusion set and their performance is heavily dependent on the quality of confusion set. But it is hard to find an up-to-date and in-domain confusion set. In this paper, we devise two pre-training tasks to model the two aforemention"
2021.emnlp-main.287,W14-6820,0,0.0234168,"from each other. 3.4 Reducing Parameters 4 4.1 Experiments Pre-training Setup We use BERT (Devlin et al., 2019) base as initialization and only the first 4 layers are utilized. Our model is implemented by PyTorch (Paszke et al., 2019) and DGL (Wang et al., 2019c). We randomly select 1M sentences provided by Xu (2019) as pretraining corpus and pad the sentences to a max length of 128. We set the learning rate as 5e-5, batch size as 1024, and pre-train 10K steps on 4 RTX 3090 for around 2 days. 4.2 Dataset and Fine-tuning Setup We conduct CSC experiments on three widely used datasets SIGHAN14 (Yu et al., 2014), SIGHAN15 (Tseng et al., 2015), OCR (Hong et al., 2019) and mark them as csc14 , csc15 and ocr. The original corpus of csc14 and csc15 was collected from essays written by learners of Chinese as a foreign language and it was in Traditional Chinese. Wang et al. (2019a), Zhang et al. (2020), and Nguyen et al. (2020) transformed it into Simplified Chinese and used augmented data provided by Wang et al. (2018). Because our pre-training corpus was in Simplified Chinese, we use the latter setting. We directly use the corpus provided by Cheng et al. (2020). Under this setting, the training set of cs"
2021.emnlp-main.287,2021.findings-acl.198,0,0.0177574,"its training process is not end-to-end. ei = BERTEmbedding(xi ), (1) Ideally, CSC corpus can be infinitely constructed hi = BERTEncoder(ei ), (2) by replacing words based on confusion set. Wang et al. (2018) generated 270k data by OCR- and where ei , hi ∈ R1×d and d is the hidden dimension. ASR-based approaches. Zhang et al. (2020) cre- After that, the h will be computed similarities i ated 5 million augmented data and Li et al. (2021) with all character embeddings to get a predicted created 9 million augmented data by substitution- distribution y ˆi over vocabulary as follows: based method. Zhang et al. (2021) corrupted input sentence by randomly replacing characters with y ˆi = Softmax(hi ET ), (3) noisy-pinyin and the new pre-training task fitted well for CSC. where E ∈ RV ×d ; y ˆi ∈ R1×V and V is the vocabMore recently, some methods also utilized pho- ulary size. Here E refers to the BERT-embeddingnetic and visual features in CSC. Liu et al. (2021) layer and the ith row of E corresponds to ei in employed a GRU(Bahdanau et al., 2014) to encode Equation 1. Finally we use the character xk as the pinyin sequence and Chinese strokes sequence as correction result for xi whose ek has the highest extra"
2021.emnlp-main.287,2020.acl-main.82,0,0.170573,"ct resentation ei by the BERT-embedding-layer. Then characters. Cheng et al. (2020) constructed a graph ei will be passed to BERT-encoder-layers to get a by confusion set to help final prediction. Nguyen representation hi as follows: et al. (2020) raised an adaptable confusion set but its training process is not end-to-end. ei = BERTEmbedding(xi ), (1) Ideally, CSC corpus can be infinitely constructed hi = BERTEncoder(ei ), (2) by replacing words based on confusion set. Wang et al. (2018) generated 270k data by OCR- and where ei , hi ∈ R1×d and d is the hidden dimension. ASR-based approaches. Zhang et al. (2020) cre- After that, the h will be computed similarities i ated 5 million augmented data and Li et al. (2021) with all character embeddings to get a predicted created 9 million augmented data by substitution- distribution y ˆi over vocabulary as follows: based method. Zhang et al. (2021) corrupted input sentence by randomly replacing characters with y ˆi = Softmax(hi ET ), (3) noisy-pinyin and the new pre-training task fitted well for CSC. where E ∈ RV ×d ; y ˆi ∈ R1×V and V is the vocabMore recently, some methods also utilized pho- ulary size. Here E refers to the BERT-embeddingnetic and visual"
2021.emnlp-main.287,P19-1139,0,0.0253536,"mely radicals and visual errors often have overlap radicals with the correct where hi denote the final representation of each character. Pinyin is a sequence of pronunciation character. descriptions for Chinese characters and phonetic 3.3 Enhanced Pretraining Tasks for CSC errors often have overlap pinyin. Based on the extra features, our model can automatically learn It has been shown that external information can be visually similar and phonetically similar mappings. better integrated into BERT by pre-training alike We employ a relational graph convolutional net- tasks (Peters et al., 2019; Zhang et al., 2019; Sun work (Schlichtkrull et al., 2018) short as R-GCN et al., 2020; Ma et al., 2020). Considering the to infill multiple types of features into character radical and pinyin features are externally added by representations ei in Equation 1. We view charac- design, we devise two more pre-training alike tasks ters as nodes and input sequence X can be orga- which are radical prediction and pinyin prediction. nized as a line graph naturally. Both radicals and In MLM, Devlin et al. (2019) randomly masked pinyin are viewed as nodes of graph as well. If a percentage of input tokens and then predict t"
2021.findings-acl.242,D19-1252,0,0.0463746,"Missing"
2021.findings-acl.242,2020.inlg-1.14,0,0.0284611,"Missing"
2021.findings-acl.242,D18-1208,0,0.0318748,"Missing"
2021.findings-acl.242,N19-1423,0,0.00584685,". We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising t"
2021.findings-acl.242,D18-2012,0,0.0146274,"tible at home since the start of the season, Manchester City crushed Arsenal (6-3) in the game at the top of the Premier League. [q] The Mancuniens are three points behind the Gunners at the top of the standings.) Table 1: A Fr example of our dataset. The text in brackets is the corresponding English translation. The sentences are separated by ‘[q]’. a Transformer-based architecture (Vaswani et al., 2017) with 12 layers of encoder and 12 layers of the decoder. The hidden size is 1024 with 16 attention heads. mBART covers 25 languages and shares the vocabulary with the sentencepiece tokenizer (Kudo and Richardson, 2018), which includes 250,000 subword tokens. We follow the language indicators with mBART, and change its position to the beginning of the source and target sequence. We replace [q] in the dataset with the delimiter &lt; /s &gt; to separate sentences. We use the first part of our dataset as training languages: De, En, Ru, Fr, Zh. We mix the training examples and do global shuffling to avoid local overfitting on a specific language. For CSR, we random sample q = 3 sentences from the document to construct the positive-negative pairs and let the margin  = 1.0. For SAS, we translate sentences to the other"
2021.findings-acl.242,2020.acl-main.703,0,0.222467,"also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lack"
2021.findings-acl.242,P18-2027,0,0.0288599,"e multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lacks the ability to align sentence-level information among languages and to distinguish which information is the most critical for the document-level input. Most previous multilingual summarization models focus on training one model for different language or partly share encoder/decoder layers (Wang et al., 2018; Lin et al., 2018; Scialom et al., 2020). Cao et al. (2020) and Lewis et al. (2020a) try to train one model for all languages, but they find that although low-resource languages can benefit from the larger training data, the performance of rich-resource languages has been sacrificed. Thus, we want to investigate the following question: Can we design a unified multilingual summarization model that can benefit both high-resource and low-resource languages? In this paper, we design a neural model with the contrastive aligned joint learning strategy for multilingual summarization (CALMS) with two new training obje"
2021.findings-acl.242,2020.emnlp-main.210,1,0.791666,"tion dataset used in our experiment and the (3) experimental settings. (l ) k where si,pos is the score of the positive candidate of (lk ) si,neg j the i-th example in language lk , and is the j-th negative candidate for i-th example. We use a linear layer with sigmoid function to get the score from the masked hidden state of the last layer of the encoder.  is a hyper-parameter for the margin distance. 3.3 Sentence Aligned Substitution Training with multiple languages makes it possible to share the representative space across languages and obtain a universal representation for summarization. Lin et al. (2020) randomly replaces words with a different language during the pre-training phase for machine translation. However, the input for summarization is longer than sentence-level machine translation and the single word replacement shows little influence (Kedzie et al., 2018). Thus, we propose sentence aligned substitution (SAS) for summarization. We take lead sentences rather than randomly sampling from the document because these sentences are more important in the summarization task. We use an extra translation tool 1 to translate our sentences into another language to get the aligned information."
2021.findings-acl.242,D19-1387,0,0.0806602,"to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both o"
2021.findings-acl.242,2020.tacl-1.47,0,0.275213,"It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lacks the ability to align sentence-level information among languages and to distinguish which information is the most critical for the document-level input. Most previous multilingual summarization models"
2021.findings-acl.242,D18-1206,0,0.016562,"than 40 languages and each article is written by native authors. France24 is an international news website with 4 languages and faz is a German website. All of these websites have a highlight written by the editor at the beginning of the news article to summarize the main idea, which can be viewed as the summary. This information can be easily extracted through the HTML tag (’storybody introduction’ in BBC, ’t-content chapo’ in france24, ’atc-IntroText’ in faz). We collect MLGSum mainly from BBC and use france24 to expand French, English, and Spanish. Faz is used for German. Similar to XSum (Narayan et al., 2018) and Newsroom (Grusky et al., 2018), we provide the Wayback archived URL of each article and the processing script to release MLGSum. The Wayback Machine9 is an initiative of the Internet Archive, building a digital library of Internet sites that archive billions of web pages. We search news articles ranging from 2010 to 2020 for the above websites. We emphasize that the intellectual property and privacy rights of the articles belong to the original authors and the corresponding website. We carefully check the terms of use, privacy policy, and copyright policy10 of the Internet Archive and the"
2021.findings-acl.242,D19-5411,0,0.046841,"Missing"
2021.findings-acl.242,2020.emnlp-main.647,0,0.042479,"Missing"
2021.findings-acl.242,2020.acl-main.553,1,0.784845,"that CALMS achieves significant improvement over monolingual models in all languages. We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-"
2021.findings-acl.242,2020.emnlp-main.294,0,0.0855807,"Missing"
2021.findings-acl.242,2020.acl-main.552,1,0.909487,"tal results indicate that CALMS achieves significant improvement over monolingual models in all languages. We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization task"
2021.findings-acl.242,N19-4009,0,0.0194461,"position to the beginning of the source and target sequence. We replace [q] in the dataset with the delimiter &lt; /s &gt; to separate sentences. We use the first part of our dataset as training languages: De, En, Ru, Fr, Zh. We mix the training examples and do global shuffling to avoid local overfitting on a specific language. For CSR, we random sample q = 3 sentences from the document to construct the positive-negative pairs and let the margin  = 1.0. For SAS, we translate sentences to the other four languages with equal probability and substitute sentences with a ratio r = 0.2. We use fairseq6 (Ott et al., 2019) to implement the architecture. We limit the max tokens to 2048 for each GPU and set the gradient accumulation to 4. The Adam optimizer (Kingma and Ba, 2015) is 6 https://github.com/pytorch/fairseq Language Size Doc. Summ. Train De En Ru Fr Zh 494,514 191,365 87,125 85,030 65,203 457 476 499 463 799 27 24 24 36 56 445,062 172,228 78,412 76,527 58,682 Hi Es Id Tr Vi Uk Pt 59,145 43,162 35,495 26,539 26,539 33,214 20,945 565 703 360 342 847 444 927 28 30 21 20 34 21 34 53,230 38,845 31,945 33,047 23,885 29,892 18,850 Total 1,168,276 573.5 29.6 1,060,605 Table 2: The dataset statistic. Doc. and S"
2021.findings-emnlp.179,2020.emnlp-main.506,0,0.0333661,"Summarization Yiran Chen∗, Pengfei Liu]∗, Xipeng Qiu† Shanghai Key Laboratory of Intelligent Information Processing, Fudan University School of Computer Science, Fudan University 2005 Songhu Road, Shanghai, China ]Carnegie Mellon University {yrchen19,xpqiu}@fudan.edu.cn {pliu3}@cs.cmu.edu Abstract supported by the source document (Cao et al., 2018a). With the continuous upgrading of the summaAmong this background, a large body of recent rization systems driven by deep neural networks (Wang et al., 2020a; Kryscinski et al., 2020; works, researchers have higher requirements Durmus et al., 2020; Cao et al., 2020) are trying to on the quality of the generated summaries, search for new automated metrics that can assess which should be not only fluent and informative but also factually correct. As a rethe factuality of generated summaries due to the sult, the field of factual evaluation has defact that existing metrics (e.g., ROUGE) are not veloped rapidly recently. Despite its initial correlated well with factual consistency (Maynez progress in evaluating generated summaries, et al., 2020; Goyal and Durrett, 2020). the meta-evaluation methodologies of factualGenerally, the process of designing these eva"
2021.findings-emnlp.179,2020.findings-emnlp.322,0,0.424136,"2020a; Kryscinski et al., 2020; works, researchers have higher requirements Durmus et al., 2020; Cao et al., 2020) are trying to on the quality of the generated summaries, search for new automated metrics that can assess which should be not only fluent and informative but also factually correct. As a rethe factuality of generated summaries due to the sult, the field of factual evaluation has defact that existing metrics (e.g., ROUGE) are not veloped rapidly recently. Despite its initial correlated well with factual consistency (Maynez progress in evaluating generated summaries, et al., 2020; Goyal and Durrett, 2020). the meta-evaluation methodologies of factualGenerally, the process of designing these evaluity metrics are limited in their opacity, leading ation metrics w.r.t factuality is commonly formuto the insufficient understanding of factuality metrics’ relative advantages and their applicalated into different forms of NLP tasks, ranging bility. In this paper, we present an adversarfrom text entailment (Falke et al., 2019; Kryscinski ial meta-evaluation methodology that allows et al., 2020) at sentence level or more fine-grained us to (i) diagnose the fine-grained strengths level (Goyal and Durrett,"
2021.findings-emnlp.179,2021.naacl-main.114,0,0.151983,"or Automated Metrics Metaevaluation aims to evaluate the reliability of automated metrics based on their correlation with human judgments (Graham, 2015; Peyrard, 2019; Bhandari et al., 2020). Most existing works perform meta-evaluation on metrics that measure semantic equivalence, such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020). Yuan et al. (2021) more recently propose BARTScore and meta evaluate it on multiple evaluation perspectives. By contrast, in this paper, we focus on the evaluation of factuality metrics using our constructed diagnostic test sets. Concurrent with our work, Goyal and Durrett (2021b); Pagnoni et al. (2021b) also look into the error patterns of existing factuality checkers.1 Factuality in Text Summarization Recent studies on factuality of text generation revolve around metric design and system optimization. Regarding the metric perspective, researchers formulate the design of automated metrics w.r.t factuality as different problems: text entailment over sequential (Kryscinski et al., 2020; Goyal and Durrett, 2021a) or tree (Goyal and Durrett, 2020, 2021a) structures; 1 question answering (Wang et al., 2020a; Durmus We encourage readers to read these works as well to et a"
2021.findings-emnlp.179,D15-1013,0,0.0221422,"et al., 2018) question answering (Jia and Liang, 2017), machine translation (Burlot and Yvon, 2017) and language model (Marvin and Linzen, 2018) to examine system drawbacks. More recently, Gardner et al. (2020) introduces the concept of “contrast set” and proposes to use it to measure the generalization of different NLP systems. Instead of adversarially evaluate an NLP system, we perform an adversarial metaevaluation of evaluation metrics. Meta-evaluation for Automated Metrics Metaevaluation aims to evaluate the reliability of automated metrics based on their correlation with human judgments (Graham, 2015; Peyrard, 2019; Bhandari et al., 2020). Most existing works perform meta-evaluation on metrics that measure semantic equivalence, such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020). Yuan et al. (2021) more recently propose BARTScore and meta evaluate it on multiple evaluation perspectives. By contrast, in this paper, we focus on the evaluation of factuality metrics using our constructed diagnostic test sets. Concurrent with our work, Goyal and Durrett (2021b); Pagnoni et al. (2021b) also look into the error patterns of existing factuality checkers.1 Factuality in Text Summarization"
2021.findings-emnlp.179,2020.acl-main.761,0,0.0211237,"Missing"
2021.findings-emnlp.179,D17-1215,0,0.0279614,"rmation extraction and dependency parsing (Cao et al., 2018b; Zhu et al., 2020). Chen et al. (2020) explore how factuality metrics are influenced by domain shift and conclude that out-of-domain systems can even surpass in-domain systems in terms of factuality and factuality checkers like FactCC is limited in predictive power of positive samples. Adversarial Evaluation of NLP Systems Adversarial evaluation has been extensively explored in many NLP tasks recently. The adversarial challenge sets have been introduced into tasks of natural language inference (Naik et al., 2018) question answering (Jia and Liang, 2017), machine translation (Burlot and Yvon, 2017) and language model (Marvin and Linzen, 2018) to examine system drawbacks. More recently, Gardner et al. (2020) introduces the concept of “contrast set” and proposes to use it to measure the generalization of different NLP systems. Instead of adversarially evaluate an NLP system, we perform an adversarial metaevaluation of evaluation metrics. Meta-evaluation for Automated Metrics Metaevaluation aims to evaluate the reliability of automated metrics based on their correlation with human judgments (Graham, 2015; Peyrard, 2019; Bhandari et al., 2020). M"
2021.findings-emnlp.179,2020.acl-main.450,0,0.0912437,"Missing"
2021.findings-emnlp.179,P19-1100,1,0.834272,"logies of factuality metrics are limited in nostic test datasets, trained factuality models their opacity–they are opaque to their results, which available: https://github.com/zide05/ are usually holistic scores (e.g., accuracy) and not AdvFact. interpretable. Specifically, different from traditional non-learnable metrics like ROUGE, whose 1 Introduction scores are relatively straightforward to interpret, With the rapid development of neural networks in e.g., lower ROUGE-2 Recall implies fewer bitext summarization (Liu and Lapata, 2019; Liu, grams from reference summaries are covered by 2019; Zhong et al., 2019; Zhang et al., 2019; Lewis generated summaries, there are diverse factors that et al., 2019; Zhong et al., 2020; Liu and Liu, 2021), could lead to lower score of factuality metrics (e.g., especially the use of contextualized pre-trained entity replacement, number inference). However, models (Devlin et al., 2019; Lewis et al., 2019), most of existing meta-evaluation strategies fail to the state-of-the-art performance, measured by auto- tell (i) which types of factual errors the metric evalmated metrics such as ROUGE (Lin, 2004) and uated at hand are better at identifying, (ii) on which BERTSco"
2021.findings-emnlp.179,P19-1085,0,0.0444187,"servations. 2083 3 3.1 Preliminaries Definition of Factuality Although researchers have slightly different definitions of factuality (Maynez et al., 2020; Kryscinski et al., 2020). In this paper, we consider factuality as how well generated summaries are supported by source documents without using any external knowledge. A factual error happens when generated summaries contain salient facts (Kryscinski et al., 2020) that can not be inferred from source documents. The summary sentences that need to be verified are also called claims below to keep consistent with the field of fact verification (Zhou et al., 2019; Schuster et al., 2019; Liu et al., 2020). Models Type Train data M NLI B ERT M NLI ROBERTA M NLI E LECTRA DAE FACT CC F EQA NLI-S NLI-S NLI-S NLI-A NLI-S QA MNLI MNLI MNLI PARANMT-G CNNDM-G QA2D, SQuA Table 1: The model types and training data of factuality metrics. NLI-A and NLI-S represent NLI-based metrics defining facts as dependency arcs and span respectively. PARANMT-G and CNNDM-G mean the automatically generated training data from PARANMT (Wieting and Gimpel, 2018) and CNN/DailyMail (Nallapati et al., 2016) (referred to as CNNDM in the rest of the paper). 3.2 Factuality Metrics There"
2021.findings-emnlp.179,P18-1042,0,0.0147892,"e summary sentences that need to be verified are also called claims below to keep consistent with the field of fact verification (Zhou et al., 2019; Schuster et al., 2019; Liu et al., 2020). Models Type Train data M NLI B ERT M NLI ROBERTA M NLI E LECTRA DAE FACT CC F EQA NLI-S NLI-S NLI-S NLI-A NLI-S QA MNLI MNLI MNLI PARANMT-G CNNDM-G QA2D, SQuA Table 1: The model types and training data of factuality metrics. NLI-A and NLI-S represent NLI-based metrics defining facts as dependency arcs and span respectively. PARANMT-G and CNNDM-G mean the automatically generated training data from PARANMT (Wieting and Gimpel, 2018) and CNN/DailyMail (Nallapati et al., 2016) (referred to as CNNDM in the rest of the paper). 3.2 Factuality Metrics There are two major task formulations of factuality metrics: natural language inference (NLI) and question answering (QA). Model types and training data are summarized in Tab. 1. NLI transferred models Following Falke et al. (2019), we train different factuality checkers (M NLI B ERT, M NLI ROBERTA and M NLI E LEC TRA ) based on BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ELECTRA (Clark et al., 2019) on MNLI dataset (Williams et al., 2018). The neutral class sample"
2021.findings-emnlp.179,N18-1101,0,0.0175059,"ing data from PARANMT (Wieting and Gimpel, 2018) and CNN/DailyMail (Nallapati et al., 2016) (referred to as CNNDM in the rest of the paper). 3.2 Factuality Metrics There are two major task formulations of factuality metrics: natural language inference (NLI) and question answering (QA). Model types and training data are summarized in Tab. 1. NLI transferred models Following Falke et al. (2019), we train different factuality checkers (M NLI B ERT, M NLI ROBERTA and M NLI E LEC TRA ) based on BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ELECTRA (Clark et al., 2019) on MNLI dataset (Williams et al., 2018). The neutral class samples are deleted in the dataset for fair comparison following Goyal and Durrett (2020). 3.2.2 QA-based Metrics The basic idea behind QA-based metrics is whether similar answers can be replied when we ask the same question to a generated summary S and its source document D (Durmus et al., 2020; Wang et al., 2020b). In practice, we use the recently proposed F EQA (Durmus et al., 2020). FEQA It first generates questions based on summary, and answers the questions based on source document and summary separately. Mismatching answers indicate an inconsistency between document"
2021.findings-emnlp.45,I11-1130,0,0.0686167,"Missing"
2021.findings-emnlp.45,P16-1154,0,0.550006,"ummarization (Wang and Cardie, 2013). Keyphrase Generation (KG) is a classical task In recent years, end to end neural models have for capturing the central idea from a given been widely-used in generating both present and document. Based on Seq2Seq models, the previous reinforcement learning framework on absent keyphrases. Meng et al. (2017) introKG tasks utilizes the evaluation metrics to furduced CopyRNN, which consists of an attentional ther improve the well-trained neural models. encoder-decoder model (Luong et al., 2015) and a However, these KG evaluation metrics such as copy mechanism (Gu et al., 2016). After that, relF1 @5 and F1 @M are only aware of the exact evant works are mainly based on the sequence-tocorrectness of predictions on phrase-level and sequence framework (Yuan et al., 2020; Chen et al., ignore the semantic similarities between simi2018, 2019). Meanwhile, F1 @5 (Meng et al., 2017) lar predictions and targets, which inhibits the and F1 @M (Yuan et al., 2020) are used for evalmodel from learning deep linguistic patterns. In response to this problem, we propose a new uating the model prediction. F1 @5 computes the fine-grained evaluation metric to improve the F1 score with the"
2021.findings-emnlp.45,P19-1208,0,0.121533,"a new uating the model prediction. F1 @5 computes the fine-grained evaluation metric to improve the F1 score with the first five predicted phrases (if the RL framework, which considers different grannumber of phrases is less than five, it will randomly ularities: token-level F1 score, edit distance, append until there are five phrases). F1 @M comduplication, and prediction quantities. On the pares all keyphrases (variable number) predicted whole, the new framework includes two reby the model with the ground truth to compute ward functions: the fine-grained evaluation an F1 score. Furthermore, Chan et al. (2019) utiscore and the vanilla F1 score. This framework helps the model identifying some partial match lize the evaluation scores as the reward function to phrases which can be further optimized as the further optimize the neural model throughout the exact match ones. Experiments on KG benchreinforcement learning (RL) approach. marks show that our proposed training frameHowever, the traditional F1 -like metrics are on work outperforms the previous RL training phrase-level, which can hardly recognize some parframeworks among all evaluation scores. In tial match predictions. For example, supposing ad"
2021.findings-emnlp.45,P06-1068,0,0.0284967,"s a partial match phrase receives the same reward as an exact mismatch phrase. In order to help to recognize these partial match phrases during the training stage, we propose a two-stage RL training method. In the first stage, we use our new metric (F G score or F B score) as a reward to train the model. Then we apply the vanilla RL (using F1 score) training as the second training stage. The whole RL training technique is similar to Chan et al. (2019), while we re-write the reward function. 4 4.1 Experiment Dataset We evaluate our model on three public scientific KG dataset, including Inspec (Hulth and Megyesi, 2006), Krapivin (Krapivin et al., 2009), KP20k (Meng et al., 2017). Each case from these datasets consists of the title, abstract, and a set of scoreLBi = max{BERT(pi , yj )}. (8) yj keyphrases. Following the previous work (Chen et al., 2020), we concatenate the title and abstract where i ∈ [1, P] and j ∈ [1, Y]. Finally, we also as input document, and use the set of keyphrases as put scoreLBi into Algorithm 1 to compute finally labels. The same as the previous works above, we BERT-based score (also called F B score). use the largest dataset, KP20k, to train the model, 501 Model catSeq(Yuan et al.,"
2021.findings-emnlp.45,D18-1439,0,0.269014,"ot. In score, which can also be used in our two-stage RL recent years, end to end neural model has been framework (the red edges in Figure 1). widely-used in generating both present and absent Comparing with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages: (1) F G score can rec- RNN, which consists of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only ne"
2021.findings-emnlp.45,2020.acl-main.103,0,0.154604,"with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages: (1) F G score can rec- RNN, which consists of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F"
2021.findings-emnlp.45,D14-1179,0,0.00768898,"Missing"
2021.findings-emnlp.45,N19-1423,0,0.0993397,"lack edges show the previous RL ysis to show the effectively of our proposed process and the blue edges show our proposed twoF G metric. stage RL process. Our two-stage RL can be divided into two parts: (1) First, we set F G score as the 2 Related Work adaptive RL reward and use RL technique to train the model; (2) Second, we use F1 score as the re- In this section, we briefly introduce keyphrase genward, which is the same as Chan et al. (2019). Fur- eration models and evaluation metrics. thermore, in order to make F G score smoothly, we 2.1 Keyphrase Generation Models carefully train a BERT (Devlin et al., 2019) model to expand the original F G score from discrete to In KG task, keyphrases can be categorized into two continuous numbers (the green line in Figure 1). types: present and absent, depending on whether This BERT scorer can predict a continuous F G it can be found in the source document or not. In score, which can also be used in our two-stage RL recent years, end to end neural model has been framework (the red edges in Figure 1). widely-used in generating both present and absent Comparing with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages:"
2021.findings-emnlp.45,D15-1166,0,0.537658,"lustering (Hulth and Megyesi, 2006), and Aiming to generate a set of keyphrases, text summarization (Wang and Cardie, 2013). Keyphrase Generation (KG) is a classical task In recent years, end to end neural models have for capturing the central idea from a given been widely-used in generating both present and document. Based on Seq2Seq models, the previous reinforcement learning framework on absent keyphrases. Meng et al. (2017) introKG tasks utilizes the evaluation metrics to furduced CopyRNN, which consists of an attentional ther improve the well-trained neural models. encoder-decoder model (Luong et al., 2015) and a However, these KG evaluation metrics such as copy mechanism (Gu et al., 2016). After that, relF1 @5 and F1 @M are only aware of the exact evant works are mainly based on the sequence-tocorrectness of predictions on phrase-level and sequence framework (Yuan et al., 2020; Chen et al., ignore the semantic similarities between simi2018, 2019). Meanwhile, F1 @5 (Meng et al., 2017) lar predictions and targets, which inhibits the and F1 @M (Yuan et al., 2020) are used for evalmodel from learning deep linguistic patterns. In response to this problem, we propose a new uating the model prediction"
2021.findings-emnlp.45,P17-1054,0,0.208551,"chool of Computer Science, Fudan University Songhu Road 2005, Shanghai, China {ycluo18,ygxu18,yejc19,xpqiu,qz}@fudan.edu.cn Abstract mining (Wilson et al., 2005; Berend, 2011), document clustering (Hulth and Megyesi, 2006), and Aiming to generate a set of keyphrases, text summarization (Wang and Cardie, 2013). Keyphrase Generation (KG) is a classical task In recent years, end to end neural models have for capturing the central idea from a given been widely-used in generating both present and document. Based on Seq2Seq models, the previous reinforcement learning framework on absent keyphrases. Meng et al. (2017) introKG tasks utilizes the evaluation metrics to furduced CopyRNN, which consists of an attentional ther improve the well-trained neural models. encoder-decoder model (Luong et al., 2015) and a However, these KG evaluation metrics such as copy mechanism (Gu et al., 2016). After that, relF1 @5 and F1 @M are only aware of the exact evant works are mainly based on the sequence-tocorrectness of predictions on phrase-level and sequence framework (Yuan et al., 2020; Chen et al., ignore the semantic similarities between simi2018, 2019). Meanwhile, F1 @5 (Meng et al., 2017) lar predictions and target"
2021.findings-emnlp.45,N18-1158,0,0.027417,"[SEP] is the same as the vanilla BERT (Devlin et al., 2019). In the training stage, scoreLi score is used as the supervised target. After get the BERT scorer, we can easily evaluate the similarity of two keyphrase. Similar to the Eq (7), for a instance (x, Y, P), we compute BERT-based score list scoreLB as follow: 3.5 Reinforcement Learning In this section, we will briefly describe our proposed two-stage reinforcement learning method. 3.5.1 Vanilla RL Training Reinforcement learning has been widely applied to text generation tasks, such as machine translation (Wu et al., 2018), summarization (Narayan et al., 2018), because it can train the model towards a non-differentiable reward. Chan et al. (2019) incorporate reinforce algorithm to optimize the Seq2Seq model with an adaptive reward function. They formulate keyphrase generation as follow. At the time step t = 1, . . . , T , the agent produces an action (token) ybt sampled from the policy (language model) P (b yt |b y&lt;t ), where yb&lt;t represent the sequence generated before step t. After generated t-th tokens, the environment sbt will gives a reward rt (b y&lt;=t , Y) to the agent and updates the next step with a new state sbt+1 = (b y&lt;=t , x, Y). We repe"
2021.findings-emnlp.45,2020.acl-main.710,0,0.0994243,"Missing"
2021.findings-emnlp.45,P02-1040,0,0.10916,"hen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive reward to improve the performance through reinforcement learning approach (Chan et al., 2019). 3 3.1 Methodology Problem Definition In this section, we will briefly define the keyphrase generation problem. Given a source document x, the objective is to predict a set of keyphrases P = {p1 , p2 , . . . , p|P |} to maximum match the ground-truth keyphrases Y = {y1 , y2 , . . . , y|"
2021.findings-emnlp.45,P16-1008,0,0.0164964,"two-stage RL recent years, end to end neural model has been framework (the red edges in Figure 1). widely-used in generating both present and absent Comparing with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages: (1) F G score can rec- RNN, which consists of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, whi"
2021.findings-emnlp.45,P13-1137,0,0.063625,"Missing"
2021.findings-emnlp.45,2021.naacl-main.455,0,0.0221629,"e some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive reward to improve the performance thr"
2021.findings-emnlp.45,P19-1515,0,0.0116015,"mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive reward to improve the performance through reinforcement learning approach (Chan et al., 2019). 3 3.1 Methodology Problem Definition I"
2021.findings-emnlp.45,H05-1044,0,0.0220272,"Missing"
2021.findings-emnlp.45,D18-1397,0,0.0216897,"for BERT scorer, where [CLS] and [SEP] is the same as the vanilla BERT (Devlin et al., 2019). In the training stage, scoreLi score is used as the supervised target. After get the BERT scorer, we can easily evaluate the similarity of two keyphrase. Similar to the Eq (7), for a instance (x, Y, P), we compute BERT-based score list scoreLB as follow: 3.5 Reinforcement Learning In this section, we will briefly describe our proposed two-stage reinforcement learning method. 3.5.1 Vanilla RL Training Reinforcement learning has been widely applied to text generation tasks, such as machine translation (Wu et al., 2018), summarization (Narayan et al., 2018), because it can train the model towards a non-differentiable reward. Chan et al. (2019) incorporate reinforce algorithm to optimize the Seq2Seq model with an adaptive reward function. They formulate keyphrase generation as follow. At the time step t = 1, . . . , T , the agent produces an action (token) ybt sampled from the policy (language model) P (b yt |b y&lt;t ), where yb&lt;t represent the sequence generated before step t. After generated t-th tokens, the environment sbt will gives a reward rt (b y&lt;=t , Y) to the agent and updates the next step with a new"
2021.findings-emnlp.45,2021.acl-long.354,1,0.781165,"of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive rewar"
2021.naacl-main.146,K19-1015,0,0.0148424,"ency tree, pre-trained modone or more aspects in a sentence, the task calls for detecting the sentiment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict the sen- the performance of the ALSC task (Sun et al., timents towards the underlined aspects, which ex- 2019a; Tang et al., 2020; Phan and Ogunbona, pects to get polarity positive for aspect food and po- 2020; Wang et al., 2020). From the view of interpretability of PTMs, Chen et al. (2019); Hewitt larity negative for aspect service. Generally, ABSA and Manning (2019); Wu et al. (2020) try to use ∗ Equal contribution. probing methods to detect syntactic information in † Corresponding author. 1 PTMs. Empirical results reveal that PTMs capture Our code will be released at https://github.com/ ROGERDJQ/RoBERTaABSA. some kind of dependency tree structures implicitly. 1816 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1816–1829 June 6–11, 2021. ©2021 Association for Computational Li"
2021.naacl-main.146,C96-1058,0,0.701005,"ERTa) to replace the token xi , which returns a representation Hθ (x{xi })i for the masked xi ; secondly, it further masks the token xj , which returns a representation Hθ (x{xi , xj })i with both xi , xj being masked. The impact value f (xi , xj ) is calculated by the Euclidean distance as follows, f(xi ,xj) = ||Hθ (x{xi })i −Hθ (x{xi ,xj})i ||2 (3) By repeating this process between every two tokens in the sentence, we can get an impact matrix 3.1.1 BERT and RoBERTa M ∈ RT ×T and Mi,j = f (xi , xj ). The tree deBERT (Devlin et al., 2019) and RoBERTa (Liu coding algorithm, such as Eisner (Eisner, 1996) et al., 2019) both take Transformers (Vaswani et al., and Chu–Liu/Edmonds’ algorithm (Chu and Liu, 2017) as backbone architecture. Generally, they 1965; Edmonds, 1967), is then used to extract the 1818 dependency tree from the matrix M. The Perturbed Masking can exert on any layer of BERT or RoBERTa. Dataset Split Positive Negative Neutral Rest14 Train Test 2164 728 807 196 637 196 3.2 Laptop14 Train Test 994 341 870 128 464 169 Twitter Train Test 1561 173 1560 173 3127 346 ALSC Models Based on Trees In this subsection, we introduce three representative tree-based ALSC models. Each of the mod"
2021.naacl-main.146,2021.ccl-1.108,0,0.104678,"Missing"
2021.naacl-main.146,D14-1162,0,0.0844928,"Missing"
2021.naacl-main.146,2020.acl-main.293,0,0.363911,"eriments also show that the pure RoBERTa-based the topological structure of the dependency tree model can outperform or approximate to the (Dong et al., 2014; Zhang et al., 2019a; Huang and previous SOTA performances on six datasets Carley, 2019; Sun et al., 2019b; Zheng et al., 2020; across four languages since it implicitly inTang et al., 2020); The second one is to use the treecorporates the task-oriented syntactic informabased distance, which counts the number of edges tion.1 in a shortest path between two tokens in the dependency tree (He et al., 2018; Zhang et al., 2019b; 1 Introduction Phan and Ogunbona, 2020); The third one is to Aspect-based sentiment analysis (ABSA) aims to simultaneously use both the topological structure do the fine-grained sentiment analysis towards asand the tree-based distance. pects (Pontiki et al., 2014, 2016). Specifically, for Except for the dependency tree, pre-trained modone or more aspects in a sentence, the task calls for detecting the sentiment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict th"
2021.naacl-main.146,P11-1016,0,0.0710812,"19,txsun19,xpqiu}@fudan.edu.cn pliu3@cs.cmu.edu Abstract contains aspect extraction (AE) and aspect-level sentiment classification (ALSC). We only focus on Aspect-Based Sentiment Analysis (ABSA), the ALSC task. aiming at predicting the polarities for aspects, Early works of ALSC mainly rely on manuis a fine-grained task in the field of sentiment ally designed syntactic features, which is laboranalysis. Previous work showed syntactic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most fr"
2021.naacl-main.146,S14-2076,0,0.194596,"s aspect extraction (AE) and aspect-level sentiment classification (ALSC). We only focus on Aspect-Based Sentiment Analysis (ABSA), the ALSC task. aiming at predicting the polarities for aspects, Early works of ALSC mainly rely on manuis a fine-grained task in the field of sentiment ally designed syntactic features, which is laboranalysis. Previous work showed syntactic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most from PTMs and the dependency parsing trees of the recently proposed State"
2021.naacl-main.146,S14-2004,0,0.560513,"sets Carley, 2019; Sun et al., 2019b; Zheng et al., 2020; across four languages since it implicitly inTang et al., 2020); The second one is to use the treecorporates the task-oriented syntactic informabased distance, which counts the number of edges tion.1 in a shortest path between two tokens in the dependency tree (He et al., 2018; Zhang et al., 2019b; 1 Introduction Phan and Ogunbona, 2020); The third one is to Aspect-based sentiment analysis (ABSA) aims to simultaneously use both the topological structure do the fine-grained sentiment analysis towards asand the tree-based distance. pects (Pontiki et al., 2014, 2016). Specifically, for Except for the dependency tree, pre-trained modone or more aspects in a sentence, the task calls for detecting the sentiment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict the sen- the performance of the ALSC task (Sun et al., timents towards the underlined aspects, which ex- 2019a; Tang et al., 2020; Phan and Ogunbona, pects to get polarity positive for aspect food and po- 2020; Wang et al., 20"
2021.naacl-main.146,P18-1087,0,0.0604417,"Missing"
2021.naacl-main.146,E17-2091,0,0.0219867,"nguistics: Human Language Technologies, pages 1816–1829 June 6–11, 2021. ©2021 Association for Computational Linguistics Therefore, two following questions arise naturally. 2 Related Work ALSC without Dependencies Vo and Zhang (2015) propose the early neural network model which does not rely on the dependency tree. Along this line, diverse neural network models have been proposed. Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. In order to model relations of aspects and their contextual words, Wang et al. (2016); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based Q2: Will PTMs adapt the implicitly entailed tree structure to the ALSC task during the fine- neural network models. Other model structures such as convolutional neural network (CNN) (Li tuning? Therefore, in this paper, we not only use the trees induced from the off-the-shelf PTMs to en- et al., 2018; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memhance ALSC models, but also use the trees induced from the fine-tuned PTMs (In short FT-PTMs) ory neural network (Tang e"
2021.naacl-main.146,D16-1103,0,0.0421316,"Missing"
2021.naacl-main.146,N19-1035,1,0.948195,"inion words (Wang et al., 2020; Sun et al., the parser-provided tree. The further analy2019b; Zhang et al., 2019b). Generally, these sis experiments reveal that the FT-RoBERTa dependency tree based ALSC models are impleInduced Tree is more sentiment-word-oriented mented in three methods. The first one is to use and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based the topological structure of the dependency tree model can outperform or approximate to the (Dong et al., 2014; Zhang et al., 2019a; Huang and previous SOTA performances on six datasets Carley, 2019; Sun et al., 2019b; Zheng et al., 2020; across four languages since it implicitly inTang et al., 2020); The second one is to use the treecorporates the task-oriented syntactic informabased distance, which counts the number of edges tion.1 in a shortest path between two tokens in the dependency tree (He et al., 2018; Zhang et al., 2019b; 1 Introduction Phan and Ogunbona, 2020); The third one is to Aspect-based sentiment analysis (ABSA) aims to simultaneously use both the topological structure do the fine-grained sentiment analysis towards asand the tree-based distance. pects (Pontiki et al., 2014, 2016). Specif"
2021.naacl-main.146,2020.acl-main.295,0,0.704181,"ctic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most from PTMs and the dependency parsing trees of the recently proposed State-of-the-art (SOTA) on several popular models for the ABSA ALSC models utilize the dependency tree to astask, showing that the induced tree from finesist in modeling connections between aspects and tuned RoBERTa (FT-RoBERTa) outperforms their opinion words (Wang et al., 2020; Sun et al., the parser-provided tree. The further analy2019b; Zhang et al., 2019b). G"
2021.naacl-main.146,P18-1088,0,0.041026,"he LSTM-based Q2: Will PTMs adapt the implicitly entailed tree structure to the ALSC task during the fine- neural network models. Other model structures such as convolutional neural network (CNN) (Li tuning? Therefore, in this paper, we not only use the trees induced from the off-the-shelf PTMs to en- et al., 2018; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memhance ALSC models, but also use the trees induced from the fine-tuned PTMs (In short FT-PTMs) ory neural network (Tang et al., 2016b; Chen et al., which are fine-tuned on the ALSC datasets. Ex- 2017; Wang et al., 2018), attention neural network (Tang et al., 2019) have also been applied in ALSC. periments show that trees induced from FT-PTMs ALSC with Dependencies Early works of ALSC can help tree-based ALSC models achieve better performance than their counterparts before fine- mainly employ traditional text classification methods focusing on machine learning algorithms and tuning. Besides, models with trees induced from the ALSC fine-tuned RoBERTa can even outper- manually designed features, which took syntactic structures into consideration from the very beginform trees from the dependency parser. ning. K"
2021.naacl-main.146,D16-1058,0,0.450812,"of ALSC mainly rely on manuis a fine-grained task in the field of sentiment ally designed syntactic features, which is laboranalysis. Previous work showed syntactic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most from PTMs and the dependency parsing trees of the recently proposed State-of-the-art (SOTA) on several popular models for the ABSA ALSC models utilize the dependency tree to astask, showing that the induced tree from finesist in modeling connections between aspects and tun"
2021.naacl-main.146,2020.acl-main.383,0,0.320473,"ment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict the sen- the performance of the ALSC task (Sun et al., timents towards the underlined aspects, which ex- 2019a; Tang et al., 2020; Phan and Ogunbona, pects to get polarity positive for aspect food and po- 2020; Wang et al., 2020). From the view of interpretability of PTMs, Chen et al. (2019); Hewitt larity negative for aspect service. Generally, ABSA and Manning (2019); Wu et al. (2020) try to use ∗ Equal contribution. probing methods to detect syntactic information in † Corresponding author. 1 PTMs. Empirical results reveal that PTMs capture Our code will be released at https://github.com/ ROGERDJQ/RoBERTaABSA. some kind of dependency tree structures implicitly. 1816 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1816–1829 June 6–11, 2021. ©2021 Association for Computational Linguistics Therefore, two following questions arise naturally. 2 Related Work ALSC without Depende"
2021.naacl-main.146,2020.emnlp-main.183,0,0.215931,"Missing"
2021.naacl-main.146,P18-1234,0,0.0140387,"short term memory (LSTM) network to enhance the interactions between aspects and context words. In order to model relations of aspects and their contextual words, Wang et al. (2016); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based Q2: Will PTMs adapt the implicitly entailed tree structure to the ALSC task during the fine- neural network models. Other model structures such as convolutional neural network (CNN) (Li tuning? Therefore, in this paper, we not only use the trees induced from the off-the-shelf PTMs to en- et al., 2018; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memhance ALSC models, but also use the trees induced from the fine-tuned PTMs (In short FT-PTMs) ory neural network (Tang et al., 2016b; Chen et al., which are fine-tuned on the ALSC datasets. Ex- 2017; Wang et al., 2018), attention neural network (Tang et al., 2019) have also been applied in ALSC. periments show that trees induced from FT-PTMs ALSC with Dependencies Early works of ALSC can help tree-based ALSC models achieve better performance than their counterparts before fine- mainly employ traditional text classification metho"
2021.naacl-main.146,D19-1464,0,0.473499,"Missing"
2021.naacl-main.436,D15-1141,1,0.91683,"component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, we propose a CWS-specific predatasets. H"
2021.naacl-main.436,P17-1110,1,0.828203,"Missing"
2021.naacl-main.436,N19-1423,0,0.154029,"e. In this section, we will describe M ETA S EG in three parts. First, we introduce the Transformerbased unified architecture. Second, we elaborate on the multi-criteria pre-training task with meta learning algorithm. Finally, we give a brief description of the downstream fine-tuning phase. 3.1 The Unified Architecture In traditional CWS systems (Chen et al., 2015; Ma et al., 2018), CWS model usually adopts a sepa2 Related Work rate architecture for each segmentation criterion. Recently, PTMs have been used for CWS and An instance of the CWS model is created for each achieve good performance (Devlin et al., 2019). criterion and trained on the corresponding dataset These PTMs usually exploit fine-tuning as the independently. Thus, a model instance can only main way of transferring prior knowledge to down- serve one criterion, without sharing any segmentastream CWS tasks. Specifically, some methods di- tion knowledge with other different criteria. rectly fine-tune PTMs on CWS tasks (Yang, 2019), To better leverage the common segmentation while others fine-tune them in a multi-task frame- knowledge shared by multiple criteria, M ETA S EG work (Huang et al., 2020). Besides, other features employs a unifie"
2021.naacl-main.436,2020.coling-main.186,0,0.742128,"5; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, we propose a CWS-specific predatasets. Huang et al. (2020) fine-tunes BERT in a trained model M ETA S EG. To leverage shared 5514 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5514–5523 June 6–11, 2021. ©2021 Association for Computational Linguistics segmentation knowledge of different criteria, M ETA S EG utilizes a unified architecture and introduces a multi-criteria pre-training task. Moreover, to alleviate the discrepancy between pretrained models and downstream unseen criteria, meta learning algorithm (Finn et al., 2017) is incorporated into t"
2021.naacl-main.436,I08-4010,0,0.702909,"Missing"
2021.naacl-main.436,2020.emnlp-main.567,0,0.042885,"k-specific prior knowledge into segment and position embeddings. The Transmultiple NLP tasks. Specifically designed pre- former network is used as the shared encoder layer, training tasks are introduced to obtain the task- encoding the input representations into hidden repspecific pre-trained models, and then these models resentations through blocks of multi-head attention are fine-tuned on corresponding downstream NLP and position-wise feed-forward modules (Vaswani tasks, such as named entity recognition (Xue et al., et al., 2017). Then a shared linear decoder with 2020), sentiment analysis (Ke et al., 2020) and text softmax is followed to map hidden representations summarization (Zhang et al., 2020). In this pa- to the probability distribution of segmentation laper, we propose a CWS-specific pre-trained model bels. The segmentation labels consist of four CWS M ETA S EG. labels {B, M, E, S}, denoting the word beginning, middle, ending and single word respectively. 3 Approach Formally, the unified architecture can be conAs other task-specific pre-trained models (Ke et al., cluded as a probabilistic model Pθ (Y |X), which 2020), the pipeline of M ETA S EG is divided into represents the probability"
2021.naacl-main.436,2020.acl-main.611,1,0.798971,"the discrepancy between pre-training 1 Introduction tasks and downstream CWS tasks. Chinese Word Segmentation (CWS) is a fundamenTo deal with aforementioned problems of PTMs, tal task for Chinese natural language processing we consider introducing a CWS-specific pre(NLP), which aims at identifying word boundaries trained model based on existing CWS corpora, to in a sentence composed of continuous Chinese char- leverage the prior segmentation knowledge. Howacters. It provides a basic component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李"
2021.naacl-main.436,2020.emnlp-main.317,0,0.632169,"Missing"
2021.naacl-main.436,I05-3017,0,0.834388,"Missing"
2021.naacl-main.436,D18-1529,0,0.19393,"Missing"
2021.naacl-main.436,2020.tacl-1.6,1,0.857152,"e Word Segmentation (CWS) is a fundamenTo deal with aforementioned problems of PTMs, tal task for Chinese natural language processing we consider introducing a CWS-specific pre(NLP), which aims at identifying word boundaries trained model based on existing CWS corpora, to in a sentence composed of continuous Chinese char- leverage the prior segmentation knowledge. Howacters. It provides a basic component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the se"
2021.naacl-main.436,2020.findings-emnlp.260,1,0.822043,"like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, we propose a CWS-specific predatasets. Huang et al. (2020) fine-tunes BERT in a trained model M ETA S EG. To leverage sha"
2021.naacl-main.436,N19-1278,0,0.167018,"Missing"
2021.naacl-main.436,2020.acl-main.734,0,0.759193,"between pre-trained models and downstream CWS tasks. Besides, M ETA S EG can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in lowresource settings. Criteria CTB6 PKU MSRA Li Na 李娜 李 娜 李娜 entered 进入 进入 进入 the semi-final 半决赛 半 决赛 半 决赛 Table 1: An example of CWS on different criteria. multi-criteria learning framework, where each criterion shares a common BERT-based feature extraction layer and has separate projection layer. Meng et al. (2019) combines Chinese character glyph features with pre-trained BERT representations. Tian et al. (2020) proposes a neural CWS framework WMS EG, which utilizes memory networks to incorporate wordhood information into the pre-trained model ZEN (Diao et al., 2019). PTMs have been proved quite effective by finetuning on downstream CWS tasks. However, PTMs used in previous works usually adopt language modeling as pre-training tasks. Thus, they usually lack task-specific prior knowledge for CWS and ignore the discrepancy between pre-training 1 Introduction tasks and downstream CWS tasks. Chinese Word Segmentation (CWS) is a fundamenTo deal with aforementioned problems of PTMs, tal task for Chinese na"
2021.naacl-main.436,D14-1122,0,0.348795,"Missing"
2021.naacl-main.436,D19-1541,0,0.0589616,"Missing"
2021.naacl-main.436,2020.emnlp-main.514,1,0.799817,"Missing"
2021.naacl-main.436,O03-4002,0,0.578191,"Howacters. It provides a basic component for other NLP ever, there are multiple inconsistent segmentation tasks like named entity recognition (Li et al., 2020), criteria for CWS, where each criterion represents a dependency parsing (Yan et al., 2020), and seman- unique style of segmenting Chinese sentence into tic role labeling (Xia et al., 2019), etc. words, as shown in Table 1. Meanwhile, we can easily observe that different segmentation criteria Generally, most previous studies model the CWS could share a large proportion of word boundaries task as a character-based sequence labeling task (Xue, 2003; Zheng et al., 2013; Chen et al., 2015; between them, such as the boundaries between Ma et al., 2018; Qiu et al., 2020). Recently, pre- word units “李娜(Li Na)”, “进入(entered)” and trained models (PTMs) such as BERT (Devlin et al., “半决赛(the semi-final)”, which are the same for all segmentation criteria. It shows that the com2019) have been introduced into CWS tasks, which mon prior segmentation knowledge is shared by could provide prior semantic knowledge and boost the performance of CWS systems. Yang (2019) di- different criteria. rectly fine-tunes BERT on several CWS benchmark In this paper, w"
2021.naacl-main.436,K17-3001,0,0.0698996,"Missing"
2021.naacl-main.436,E14-1062,0,0.471673,"Missing"
2021.naacl-main.436,D12-1046,0,0.0705684,"Missing"
2021.naacl-main.472,N18-1150,1,0.850619,"ting models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of"
2021.naacl-main.472,P18-1063,0,0.0367563,"lving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting sum"
2021.naacl-main.472,2020.findings-emnlp.329,1,0.691909,"nsive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries under an unsupervised setting. Li et al. (2019) attempt to incorporate multi-modal information to facilitate the meeting summarization. Zhu et al. (2020) propose a model which builds a hierarchical structure on word-level and turn-level information and uses news summary data to alleviate the inadequacy of meeting data. Unlike previous works, instead of merely generating summaries for the complete meeting, we propose a novel task where we focus on summarizing multi-granularity contents which cater to different people’s need for the entire meetings, and help people comprehensively understand meetings. 3 Data Construction In this section, we show how we collected meeting data from three different domains: academic meetings, product meetings, and"
2021.naacl-main.472,N12-1041,0,0.66189,"2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meet"
2021.naacl-main.472,P19-1098,0,0.0151867,"are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2"
2021.naacl-main.472,N16-1012,0,0.0336034,"s and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a mor"
2021.naacl-main.472,N18-2097,0,0.0198273,". 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006;"
2021.naacl-main.472,W06-0707,0,0.0593589,"Missing"
2021.naacl-main.472,P06-1039,0,0.208583,"Missing"
2021.naacl-main.472,N18-1065,0,0.039943,"Missing"
2021.naacl-main.472,2020.emnlp-main.295,0,0.0149003,"queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch o"
2021.naacl-main.472,W17-1004,0,0.108697,"of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It is challenging to compress or compose a short summary that contains all the salient information. Alternatively, summarization systems should adopt a more flexible and interactive approach that allows people to express their interests and caters to their diverse intents when generating summaries (Dang, 2005, 2006; Litvak and Vanetik, 2017; Baumel et al., 2018). With comprehensive consideration of the multigranularity meeting contents, we propose a new task, query-based meeting summarization. To enable research in this area, we also create a highquality multi-domain summarization dataset. In this task, as shown in Figure 1, given a query and a meeting transcript, a model is required to generate the corresponding summary. The query-based approach is a flexible setup that enables the system to satisfy different intents and different levels of granularity. Besides the annotated queries and corresponding gold summaries at different"
2021.naacl-main.472,D18-1208,0,0.0649839,"Missing"
2021.naacl-main.472,D19-1387,1,0.849152,"generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from acade"
2021.naacl-main.472,D14-1181,0,0.00351051,", Pointer Network will point to the start turn and the end turn for each query. It is worth noting that one query can correspond to multiple spans in our dataset, so we always extract three spans as the corresponding text for each query when we use Pointer Network as Locator in the experiments. In addition, we design a hierarchical rankingbased model structure as the Locator. As shown in Figure 3, we first input the tokens in each turn to a feature-based BERT to obtain the word embedding, where feature-based means we fix the parameters of BERT, so it is actually an embedding layer. Next, CNN (Kim, 2014) is applied as a turn-level encoder to capture the local features such as bigram, 5 Experiments trigram and so on in each turn. Here we do not use Transformer because previous work (Kedzie et al., In this section, we introduce the implementation details, effectiveness of Locator, experimental results 2018) shows that this component does not matter and multi-domain experiments on QMSum. too much for the final performance. We combine different features to represent the utterance ui in 5.1 Implementation Details each turn, and concatenate the speaker embedding si as the turn-level representation:"
2021.naacl-main.472,2020.coling-main.499,0,0.0176046,"dia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries u"
2021.naacl-main.472,P19-1209,0,0.0161457,"meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2"
2021.naacl-main.472,2020.acl-main.703,0,0.0293968,"our goal in the second stage is to summarize the selected text spans based on the query. We instantiate our Summarizer with the current powerful abstractive models to explore whether the query-based meeting summarization task on our dataset is challenging. To be more specific, we choose the following three models: Pointer-Generator Network (See et al., 2017) is a popular sequence-to-sequence model with copy mechanism and coverage loss, and it acts as a baseline system in many generation tasks. The input to Pointer-Generator Network (PGNet) is: “&lt;s&gt; Query &lt;/s&gt; Relevant Text Spans &lt;/s&gt;”. BART (Lewis et al., 2020) is a denoising pretrained model for language generation, translation and comprehension. It has achieved new state-ofthe-art results on many generation tasks, including summarization and abstractive question answering. The input to BART is the same as PGNet. HMNet (Zhu et al., 2020) is the state-of-the-art meeting summarization model. It contains a hierarchical structure to process long meeting transcripts and a role vector to depict the difference among speakers. Besides, a cross-domain pretraining process is also included in this strong model. We add a turn representing the query at the begi"
2021.naacl-main.472,P19-1210,0,0.211278,"cussing user interface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking pl"
2021.naacl-main.472,W13-2117,0,0.0344771,"2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the mee"
2021.naacl-main.472,P17-1098,0,0.0183837,"meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehd"
2021.naacl-main.472,W14-4407,0,0.133484,"et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meet"
2021.naacl-main.472,D15-1044,0,0.0238088,"n QMSum. Our results and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting"
2021.naacl-main.472,P17-1099,0,0.400999,"veal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summar"
2021.naacl-main.472,D19-1324,0,0.0115444,"the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and mor"
2021.naacl-main.472,P18-1062,0,0.35554,"ing a toll on our productivity and well- decisions (Wang and Cardie, 2013). This poses the being (Spataro, 2020). The proliferation of meet- question of whether a single paragraph is enough to summarize the content of an entire meeting? ings makes it hard to stay on top of this sheer Figure 1 shows an example of a meeting about volume of information and increases the need for automated methods for accessing key informa- “remote control design”. The discussions in the tion exchanged during them. Meeting summariza- meeting are multi-faceted and hence different users tion (Wang and Cardie, 2013; Shang et al., 2018; might be interested in different facets. For exam∗ ple, someone may be interested in learning about These two authors contributed equally. The order of authorship decided by the flip of a coin. the new trends that may lead to the new product 5905 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It i"
2021.naacl-main.472,2020.acl-main.553,1,0.826971,"s to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 20"
2021.naacl-main.472,P13-1137,0,0.0731651,"Missing"
2021.naacl-main.472,2020.acl-main.552,1,0.832276,"dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2"
2021.naacl-main.472,P19-1100,1,0.888302,"ghting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this tas"
2021.naacl-main.472,D19-5410,1,0.872427,"Missing"
2021.naacl-main.472,2020.findings-emnlp.19,0,0.764092,"rface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking place each day people"
C12-2093,W02-1001,0,0.105085,"variable, and C is a positive parameter which controls the influence of the slack term on the objective function. Following the derivation in PA (Crammer et al., 2006), we can get the update rule, wk+1 = wk + τk (Φ(·) − Φ(∗)), where τk = min(C, ℓk (w; (·)) ) ∥Φ(·) − Φ(∗)∥2 (10) (11) Our training algorithm is based on PA algorithm and shown in Algorithm 1. In our algorithm, the input examples are randomly selected in each round k. According to the source of the selected example, we obtain the best response by using the proper inference algorithm, and finally update the parameters w. Following (Collins, 2002), the average strategy is also adopted to avoid overfitting problem. 4 Experiments We employ two joint sequence labeling tasks to show the performance of our model. In the following section, we would report our experiment settings and discuss the experiment results. We compare our method with cross-label model and factorial model with PA algorithm. The factorial model is similar with Factorial CRF(Sutton et al., 2007), but its parameters are learning with PA algorithm. We use the standard evaluation metrics F 1 score, which is the harmonic mean of precision P (percentage of predict phrases tha"
C12-2093,P05-2004,0,0.0159689,"nce, our model provides a slight decoding speedup. The reason is that the states of segmentation are much less than tagging. However, on the only segmentation task, our model provides a decoding speedup over 10 times, since we can use the segmentation chain independently in our model. 5 Related Works Several methods have been proposed to cope with the problems of joint S&T task. Sutton et al. (2004, 2007) proposed Dynamic Conditional Random Fields (DCRF) to jointly represent the different tasks in a single graphical model. However, the exact training and inference for DCRF are time-consuming. Duh (2005) proposed a model for jointly labeling multiple sequences. The model is based on the Factorial Hidden Markov Model (FHMM). Since FHMM is directed graphical model, FHMM requires considerably less computation than DCRFs and exact inference is easily achievable. However, the FHMM’s generative framework cannot take full advantage of context features, so its performance is lower than DCRF. Different with our model applied in joint S&T task, both the DCRF and FHMM are used in POS tagging and NP Chunking tasks. These two tasks are not strongly dependent on each other. Therefore, their models are rela"
C12-2093,I08-4010,0,0.109374,"nd tagging tasks, respectively English shallow parsing and Chinese word segmentation and POS tagging (Chinese S&T). 958 In English shallow parsing, the corpus from CoNLL 2000 shared task is commonly used, which contains 8936 sentences for training and 2012 sentences for testing. We employ the commonly used label set {B, M, E, S} in the segmentation task. 12 tagging labels, such as noun phrase (NP), verb phrase (VP),…and others (O), are used in the sequence tagging task. In Chinese S&T, we employ the Chinese Treebank (CTB) corpus, obtained from the Fourth International SIGHAN Bakeoff datasets (Jin and Chen, 2008). The label set {B, M, E, S} is also used for segmentation task. 4.2 Performance of Coupled Sequences Labeling Model In the first experiment, we aim to compare the performances of our coupled sequences labeling model with other traditional joint models. Feature templates used in this experiment are summarized in Table 1 for English shallow parsing and in Table 2 for Chinese word segmenation and POS tagging, in which wi denotes ith word, pi denotes ith POS tag, ci denotes ith Chinese character. We compare the total performance between traditional joint cross-label model, factorial model and our"
C12-2093,W04-3236,0,0.123095,"rds in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, which significantly affects the whole performance. The cross-label approaches can avoid the problem of error propagation and achieve more higher performance on both subtasks (Ng and Low, 2004). However, due to the large number of labels, two problems arise: (1) The amount of parameters increases rapidly and would be apt to overfit to the training corpus; (2) The decoding efficiency by dynamic programming would decrease. In addition, joint cross-label approaches cannot segment or tag sentences separately. For example, in Chinese POS tagging task, the joint model cannot segment sentences individually without tagging the sentences. Moreover, if the sentences are already segmented, the joint model can not tag individually with the existing segmentation information. In this paper, we pr"
C12-2093,W96-0213,0,0.348063,"); predict (1st Viterbi): ˆsl = arg max ⟨w, Φs (xl , s)⟩; s if ˆsl = si then update with w with Eq. 10, where (·) is (xl , sl ) and (∗) is (xl , ˆsl ); end end end w = cw/K ; Algorithm 1: Online Learning Algorithm for Coupled Sequences Labeling Model. (双链序列标 注在线学习算法) 953 1 Introduction In the fields of natural language processing (NLP), joint segmentation and tagging (S&T) task is an important research topic. Many NLP problems can be transformed to joint S&T task, such as shallow parsing(Sha and Pereira, 2003), named entity recognition(Zhou and Su, 2002), Chinese part-of-speech (POS) tagging(Ratnaparkhi, 1996) and so on. For example, there are no explicitly boundaries between words in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, which significantly affects the whole performance. The cross-label approaches can avoid the problem of er"
C12-2093,N03-1028,0,0.0807399,"h Eq. 10, where (·) is (xl , sl , tl ) and (∗) is (xl , ˆsl , tˆl ); end else receive an example (xl , sl ); predict (1st Viterbi): ˆsl = arg max ⟨w, Φs (xl , s)⟩; s if ˆsl = si then update with w with Eq. 10, where (·) is (xl , sl ) and (∗) is (xl , ˆsl ); end end end w = cw/K ; Algorithm 1: Online Learning Algorithm for Coupled Sequences Labeling Model. (双链序列标 注在线学习算法) 953 1 Introduction In the fields of natural language processing (NLP), joint segmentation and tagging (S&T) task is an important research topic. Many NLP problems can be transformed to joint S&T task, such as shallow parsing(Sha and Pereira, 2003), named entity recognition(Zhou and Su, 2002), Chinese part-of-speech (POS) tagging(Ratnaparkhi, 1996) and so on. For example, there are no explicitly boundaries between words in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, whi"
C12-2093,P99-1023,0,0.0701627,"ion and would be assigned to the same label. 3.2 Inference Algorithm According to the theory of probabilistic graphical models (Koller and Friedman, 2009), we can define a score function F (·) as the logarithmic potential function: F (w, Φ(x, s, t)) = L ∑ {wT ΦC1 (si−1 , ti−1 , si , x, i) + wT ΦC2 (ti−1 , si , ti , x, i)}, (3) i Given an observed sequence x, the aim of inference algorithm is to find two best label sequences simultaneously with the highest score. In order to adapt to our model with two kinds of 3-variable cliques, we make some modifications of a second order Viterbi algorithm (Thede and Harper, 1999). We define two functions for recording the score of the best partial path from the beginning of the sequence to the position i: δi (ti−1 , si ) , F (w, Φ(x, s0:i , t0:i−1 )) = arg max{ηi−1 (si−1 , ti−1 ) + wT ΦC1 (si−1 , ti−1 , si , x, i)},(4) ηi (si , ti ) , F (w, Φ(x, s0:i , t0:i )) = arg max{δi (ti−1 , si ) + wT ΦC2 (ti−1 , si , ti , x, i)}, si−1 ti−1 Initially, only features associated with variables s0 and t0 are hired. Without loss of generality, we set s−1 = “BoS” 1 , t−1 = “BoT” 2 and η−1 (s−1 , t−1 ) = 0. Then iteratively calculate these two 1 denotes 2 denotes “Beginning of Segmenta"
C12-2093,P02-1060,0,0.0640538,"(xl , ˆsl , tˆl ); end else receive an example (xl , sl ); predict (1st Viterbi): ˆsl = arg max ⟨w, Φs (xl , s)⟩; s if ˆsl = si then update with w with Eq. 10, where (·) is (xl , sl ) and (∗) is (xl , ˆsl ); end end end w = cw/K ; Algorithm 1: Online Learning Algorithm for Coupled Sequences Labeling Model. (双链序列标 注在线学习算法) 953 1 Introduction In the fields of natural language processing (NLP), joint segmentation and tagging (S&T) task is an important research topic. Many NLP problems can be transformed to joint S&T task, such as shallow parsing(Sha and Pereira, 2003), named entity recognition(Zhou and Su, 2002), Chinese part-of-speech (POS) tagging(Ratnaparkhi, 1996) and so on. For example, there are no explicitly boundaries between words in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, which significantly affects the whole performanc"
C14-1109,J96-1002,0,0.0248015,"des a new viewpoint to enhance the performance of CWS by automatically expanding corpus rather than developing complicated algorithms or features. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al., 1996), Conditional Random Fields (CRF)(Laﬀerty et al., 2001). After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for the practical demands to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain texts. Since these supervised approaches often has a high requirement on the quality and quantity of annotated corpus, which is always not easy to create. As a result, ma"
C14-1109,W03-0407,0,0.0519952,"ber of Continuous OOV Words (b) OOV rate (c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual"
C14-1109,W02-1001,0,0.00699872,"o use a margin to ﬁnd the reliable ones as new training data. In our experiments, the number of selected sentence is 1 ∼ 5 for each seed. Thus, we can re-train a new segmenter on the expanded corpus. After several iteration, we will get a segmenter with the best performance. 1159 5 Base Segmenter We use discriminative character-based sequence labeling for base word segmentation. Each character is labeled as one of {B, M, E, S} to indicate the segmentation. We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overﬁtting problem. 6 Experiment To evaluate our algorithm, we use both CTB6.0 and CTB7.0 datasets in our experiments. CTB is a segmented, part-of-speech tagged, and fully bracketed corpus in the constituency formalism. It is also a popular data set to evaluate word segmentation methods, such as (Sun and Xu, 2011). Since CTB dataset is collected from diﬀerent sources, such as newswire, magazine, broadcast news and web blogs, it is suitable to evaluate the performance of CWS systems on diﬀerent domains. We conduct two experiments on diﬀerent divisions"
C14-1109,J04-1004,0,0.0154582,"Magistry and Sagot, 2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or"
C14-1109,P07-1034,0,0.0114724,"V Words (b) OOV rate (c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or"
C14-1109,P13-1075,0,0.249193,"(Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotations are just partial annotations and their roles depend on the qualities of the selected resource, such as Wikipedia. In this paper, we wish to propose a method to obtain new fully-annotated data in more aggressive way, which can combine the advantages of the above works. 3 Analysis of Inﬂuence Factors for CWS Before describing our method, we give an analysis of the impact of out-of-vocabulary (OOV) words for segmentation. We ﬁrst conduct"
C14-1109,P06-1027,0,0.0329959,"ttp://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see details in Section 3). It is diﬃcult to devote eﬀorts to building a corpus for out-of-domain texts, since new words are produced frequently as the development of the society, es"
C14-1109,P06-2056,0,0.0257515,"to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain texts. Since these supervised approaches often has a high requirement on the quality and quantity of annotated corpus, which is always not easy to create. As a result, many methods were proposed to utilize the information of unlabeled data. There are three kinds of methods for domain adaptation problem in CWS. The ﬁrst is to use unsupervised learning algorithm to segment texts, like branching entropy (BE) (Jin and Tanaka-Ishii, 2006), normalized variation of branching entropy (nVBE)(Magistry and Sagot, 2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation an"
C14-1109,J09-4006,0,0.254472,"se disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotations are just partial annotations and their roles depend on the qualities of the selected resource, such as Wikipedia. In this paper, we wish to propose a method to obtain new fully-annotated data in more aggressive way, which can combine the advantages of the above works. 3 Analysis of Inﬂuence Factors for CWS Before describing our method, we give an analysis of the impact of out-of-vocabulary (OOV) words for segmenta"
C14-1109,C12-2067,0,0.110619,"en we describe our method in section 4. Section 5 introduces the base segmenter. Section 6 gives the experimental results. Finally we conclude our work in section 7. 2 Related Works The idea of exploring information redundancy on Web was introduced in question answering system (Kwok et al., 2001; Clarke et al., 2001; Banko et al., 2002) and the famous information extraction system KNOWITALL(Etzioni et al., 2004). However, this idea is rarely mentioned in Chinese word segmentation. Nonetheless, there are three kinds of related methods on Chinese word segmentation. One is active learning. Both (Li et al., 2012) and (Sassano, 2002) try to use active learning method to expand annotated corpus, but they still need to manually label some new raw texts in order to enlarge the training corpus. Diﬀerent with these methods, our method do not require any manual oracle labeling at all. Another is self-training, also called bootstrapping or self-teaching (Zhu, 2005). Self-training is a general semi-supervised learning approach. In self-training, a classiﬁer is ﬁrst trained with the small amount of labeled data. The classiﬁer is then used to classify the unlabeled data. 1155 F1 F1 0.9 0.85 0.8 0.75 1.00 1.00 0."
C14-1109,C12-2073,0,0.235485,"c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation informati"
C14-1109,P12-2075,0,0.0177905,"e drawn from news texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain texts. Since these supervised approaches often has a high requirement on the quality and quantity of annotated corpus, which is always not easy to create. As a result, many methods were proposed to utilize the information of unlabeled data. There are three kinds of methods for domain adaptation problem in CWS. The ﬁrst is to use unsupervised learning algorithm to segment texts, like branching entropy (BE) (Jin and Tanaka-Ishii, 2006), normalized variation of branching entropy (nVBE)(Magistry and Sagot, 2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao a"
C14-1109,N06-1020,0,0.126733,"F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. H"
C14-1109,C04-1081,0,0.058855,"segmenter to resolve the original complex segmentation. The experimental results show that our approach can more eﬀectively and stably improve the performance of CWS. Our method also provides a new viewpoint to enhance the performance of CWS by automatically expanding corpus rather than developing complicated algorithms or features. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al., 1996), Conditional Random Fields (CRF)(Laﬀerty et al., 2001). After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for the practical demands to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora cannot work well wi"
C14-1109,P07-1078,0,0.0174796,"line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotati"
C14-1109,W10-2606,0,0.0131988,"erent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotations are just p"
C14-1109,P02-1064,0,0.042688,"hod in section 4. Section 5 introduces the base segmenter. Section 6 gives the experimental results. Finally we conclude our work in section 7. 2 Related Works The idea of exploring information redundancy on Web was introduced in question answering system (Kwok et al., 2001; Clarke et al., 2001; Banko et al., 2002) and the famous information extraction system KNOWITALL(Etzioni et al., 2004). However, this idea is rarely mentioned in Chinese word segmentation. Nonetheless, there are three kinds of related methods on Chinese word segmentation. One is active learning. Both (Li et al., 2012) and (Sassano, 2002) try to use active learning method to expand annotated corpus, but they still need to manually label some new raw texts in order to enlarge the training corpus. Diﬀerent with these methods, our method do not require any manual oracle labeling at all. Another is self-training, also called bootstrapping or self-teaching (Zhu, 2005). Self-training is a general semi-supervised learning approach. In self-training, a classiﬁer is ﬁrst trained with the small amount of labeled data. The classiﬁer is then used to classify the unlabeled data. 1155 F1 F1 0.9 0.85 0.8 0.75 1.00 1.00 0.90 0.90 0.80 0.80 0."
C14-1109,E03-1008,0,0.0517668,"tal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve th"
C14-1109,D11-1090,0,0.45407,"licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see details in Section 3"
C14-1109,P08-1076,0,0.0356686,". 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see details in Section 3). It is diﬃcult to devote eﬀorts to building a corpus for out-of-domain texts, since new words are produced frequently as the development of the society, especially the Internet society. It is also impra"
C14-1109,O03-4002,0,0.0663566,"in a better segmenter to resolve the original complex segmentation. The experimental results show that our approach can more eﬀectively and stably improve the performance of CWS. Our method also provides a new viewpoint to enhance the performance of CWS by automatically expanding corpus rather than developing complicated algorithms or features. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al., 1996), Conditional Random Fields (CRF)(Laﬀerty et al., 2001). After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for the practical demands to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora"
C14-1109,P12-1083,0,0.0660079,"Missing"
C14-1109,P95-1026,0,0.479798,"9 10 Word Length OOV Rate Number of Continuous OOV Words (b) OOV rate (c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 20"
C14-1109,I08-4017,0,0.023954,"2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see d"
C14-1109,E14-1062,0,\N,Missing
C18-1232,D16-1053,0,0.0229658,"long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attention mechanism (Vaswani et al., 2017) to weighted average the outputs of CNN/RNN layer. The attention mechanism can effectively reduce the burden of CNN/RNN. The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors. The attention based aggregation collects informa"
C18-1232,J81-4005,0,0.746067,"Missing"
C18-1232,D14-1181,0,0.273671,"rtant natural language processing applications. A primary challenge is how to encode the variable-length text sequence into a fixed-size vector, which should fully capture the semantics of text. Many successful text encoding methods usually contain three key steps: (1) converting each word in a text sequence into its embedding; (2) taking as input the sequence of word embeddings, and computing the context-aware representation for each word with a recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or convolutional neural network (CNN) (Collobert et al., 2011; Kim, 2014); (3) summarizing the sentence meaning into a fixed-size vector by an aggregation operation. Then, these models are trained by combining a downstream task in a supervised or unsupervised way. Currently, much attention is paid to the first two steps, while the aggregation step is less emphasized on. Some simple aggregation methods, such as max (or average) pooling, is used to sum the RNN hidden states or convolved vectors, computed in the previous step, into a single vector. This kind of methods aggregate information in a bottom-up and passive way and are lack of the guide of task information."
C18-1232,D15-1280,1,0.840149,"ated encoding models to capture the long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attention mechanism (Vaswani et al., 2017) to weighted average the outputs of CNN/RNN layer. The attention mechanism can effectively reduce the burden of CNN/RNN. The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors. The attent"
C18-1232,D14-1162,0,0.0841878,"this section, we are going to introduce a general text classification framework. It consists of an Embedding Layer, Encoding Layer, Aggregation Layer and Prediction Layer. 2.1 Embedding Layer Given a text sequence with words S = w1 , w2 , · · · , wL . Since the words are symbols that could not be processed directly using prominent neural architectures, so we first map each word into a d dimensional embedding vector, X = [x1 , x1 , x2 , · · · , xL ]. (1) In order to transfer knowledge from a vast unlabeled corpus, the embeddings can be taken from the pre-trained word embedding, such as Glove (Pennington et al., 2014). 2.2 Encoding Layer However, each word representation in X is still independent with each other. To gain some dependency between adjacent words, we then build a bi-directional LSTM (BiLSTM) layer (Hochreiter and Schmidhuber, 1997) to incorporate forward and backward context information of a sequence. Then we can get phrase-level encoding ht of a word by concatenating forward hft and backward output vector hbt correspond to the target word. hft = LSTM(hft−1 , xt ), (2) hbt = LSTM(hbt+1 , xt ), (3) ht = [hft ; hbt ]. (4) Thus, the outputs of BiLSTM encoder are a sequence of vectors H = [h1 , h2"
C18-1232,D12-1110,0,0.0346568,"2 3 Iteration 4 5 Figure 2: Relationship between test accuracy and routing iteration, where the vertical axis denotes test accuracy and the horizontal axis denotes routing iteration. When the iteration is set to 3 performance peaks on several different capsule number setting 6 Related Work Currently, much attention has been paid to how developing a sophisticated encoding models to capture the long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attentio"
C18-1232,D13-1170,0,0.0181608,"ts. Note that we use the same document level datasets provided in (Tang et al., 2015). Yelp reviews Yelp-2013 and Yelp-2014 are reviews from Yelp, each example consists of several review sentences and a rating score range from 1 to 5 (higher is better). IMDB is a movie review dataset extracted from IMDB website. It is a multi-sentence dataset that for each example there are several review sentences. A rating score range from 1 to 10 is also associated with each example. SST-1 Stanford Sentiment Treebank is a movie review dataset which has been parsed and further splited to train/dev/test set (Socher et al., 2013). For each example in the dataset, there exists only one sentence and a label associated with it. And the labels can be one of {negative, somewhat negative, neutral, somewhat positive, positive}. SST-2 This dataset is a binary-class version of SST-1, with neutral reviews removed and the remaining reviews categorized to either negative or positive. 2747 Yelp-2013 Yelp-2014 IMDB SST-1 SST-2 Embedding size LSTM hidden unit Capsule dimension Capsule number Iteration number Regularization rate Initial learning rate learning rate decay learning rate decay steps Initial Batch size Batch size low boun"
C18-1232,P15-1098,0,0.0337063,"ocument encoding. 5 Experiment We test the empirical performance of our proposed model on 5 benchmark datasets for document and sentence level classification and compare our proposed model to other competitor models. 5.1 Datasets To evaluate the effectiveness of our proposed aggregation method, we have conducted experiments on 5 datasets, the statistics of experimented datasets are shown in Table 1. As shown in the table, Yelp-2013, Yelp-2014, and IMDB are document level datasets, while SST-1 and SST-2 are sentence level datasets. Note that we use the same document level datasets provided in (Tang et al., 2015). Yelp reviews Yelp-2013 and Yelp-2014 are reviews from Yelp, each example consists of several review sentences and a rating score range from 1 to 5 (higher is better). IMDB is a movie review dataset extracted from IMDB website. It is a multi-sentence dataset that for each example there are several review sentences. A rating score range from 1 to 10 is also associated with each example. SST-1 Stanford Sentiment Treebank is a movie review dataset which has been parsed and further splited to train/dev/test set (Socher et al., 2013). For each example in the dataset, there exists only one sentence"
C18-1232,D16-1172,1,0.867699,"ls to capture the long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attention mechanism (Vaswani et al., 2017) to weighted average the outputs of CNN/RNN layer. The attention mechanism can effectively reduce the burden of CNN/RNN. The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors. The attention based aggrega"
C18-1232,N16-1174,0,0.700966,"gation operation. Then, these models are trained by combining a downstream task in a supervised or unsupervised way. Currently, much attention is paid to the first two steps, while the aggregation step is less emphasized on. Some simple aggregation methods, such as max (or average) pooling, is used to sum the RNN hidden states or convolved vectors, computed in the previous step, into a single vector. This kind of methods aggregate information in a bottom-up and passive way and are lack of the guide of task information. Recently, several works employ self-attention mechanism (Lin et al., 2017; Yang et al., 2016) on top of the recurrent or convolutional encoding layer to replace simple pooling. A basic assumption is that the words (or even sentences) are not equally important. One or several task-specific context vectors are used to assign a different weight to each word and select task-specific encodings. The context vectors are parameters learned jointly with other parameters during the training process. These attentive aggregation can select task-dependent information. However, the context vectors are fixed once learned. ∗ 1 Corresponding Author https://github.com/FudanNLP/Capsule4TextClassificatio"
D12-1126,D08-1070,0,0.0276057,"tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm. In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size. The problem of this approach lies in that the decoding algorithm depends on the predefined window size to exploit the boundaries of segmentations but not the real length of words. Bunescu (2008) presents an improved pipeline model in which the output of the previous subtasks are considered as hidden variables, and the hidden variables together with their probabilities denoting the confidence are used as probabilistic features in the next subtasks. One shortcoming of this method is inefficiency caused by the calculation of marginal probabilities of features. The other disadvantages of the pipeline method are error propagation and the need of separate training of different subtasks in the pipeline. Another disadvantage of pipeline method is error propagation. Jiang et al. (2008) propos"
D12-1126,W02-1001,0,0.0281002,"3 and analyze its complexity in section 4. Section 5 describes the training method. The experimental results are manifested in section 6. Finally, We review the relevant research works in section 7 and conclude our work in section 8. 1380 (1) where w is the parameter of function F (·). For sequence labeling, the feature can be denoted as ϕk (yi , yi−1 , x, i), where i stands for the position in the sequence and k stands for the number of feature templates. we use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overfitting problem. 3 Dynamic Features The form of traditional features is shown in Table 2, where C represents a Chinese character, and T represents the character-based tag. The subscript i indicates its position related to the current character. Table 2: Traditional Feature Templates Ci , T0 (i = −2, −1, 0, 1, 2) Ci , Cj , T0 (i, j = −2, −1, 0, 1, 2 and i = j) T−1 , T0 Traditional features are generated by positionfixed templates. Since the length of Chinese word is unfixed, their meanings are incomplete. We categorize them as “static” features s"
D12-1126,W02-2006,0,0.0107998,"81.82 77.59 84.48 39.29 53.57 37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm. In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size. The problem of this approach lies in that the dec"
D12-1126,P08-1102,0,0.0955264,"OS tagging is that the two languages have character-based features and word-based features respectively. To ensure the consistency of tagging models, we prefer to use word-level information in Chinese, which is both useful for ChineseEnglish mixed texts and Chinese-only texts. For instance, in a sentence “X 或者 Y ... (X or Y ...)”, the word Y ought to have the same POS tag as the word X. Another example is that the word following a pronoun is usually a verb, and adjectives often describe nouns. Some related works show that word-level features can improve the performance of Chinese POS tagging (Jiang et al., 2008; Sun, 2011). In this paper, we propose a method to tag mixed texts with dynamic features. Our method combines these dynamic features, which are dynamically generated at the decoding stage, with traditional static features. For Chinese-English mixed texts, the traditional features cannot yield a satisfied result due to lack of training data. The proposed dynamic features can improve the performance by using the information of a word, such as POS tag or length of the whole word, which is proven effective by experiments. The rest of the paper is organized as follows: In section 2, we introduce t"
D12-1126,I08-4010,0,0.0732247,"Missing"
D12-1126,W04-3236,0,0.226022,"lish mixed texts in daily conversation, especially in communication among employers in large international corporations. There are some challenges for analyzing ChineseEnglish mixed texts: 1. How to define the POS tags for English words in these mixed texts. Since the standard of POS tags for English and Chinese are different, we cannot use English POS to tag the English words in mixed texts. Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with cross-labels, which can avoid the problem of error propagation and achieve higher performance on both subtasks(Ng and Low, 2004). Each label is the crossproduct of a segmentation label and a tagging label, e.g. {B-NN, I-NN, E-NN, S-NN, ...}. The features are generated by position-based templates on character-level. Since the main part of mixed texts is in Chinese and the role of English word is more like Chinese, we use Chinese POS tags (Xia, 2000) to tag English words. Since the categories of the most commonly used English words are nouns, verbs and adjectives, we can use “NN”, “NR”, “VV”, “VA”, “JJ” to label their POS tags. For the English proper nouns and verbs, there are no significant differences in Chinese and En"
D12-1126,C04-1081,0,0.0424897,"Baseline Our Baseline Our P 88.62 91.67 48.31 60.53 78.95 84.21 R 78.31 87.30 74.14 79.31 53.57 57.14 F1 83.15 89.43 58.50 68.66 63.83 68.09 Table 22: Performances of Model C on Dataset R POS tag NN VV VA NR 7 Method Baseline Our Baseline Our Baseline Our Baseline Our P 80.25 84.56 54.88 61.25 84.62 88.24 56.52 55.17 R 81.82 81.82 77.59 84.48 39.29 53.57 37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen"
D12-1126,P11-1139,0,0.062839,"he two languages have character-based features and word-based features respectively. To ensure the consistency of tagging models, we prefer to use word-level information in Chinese, which is both useful for ChineseEnglish mixed texts and Chinese-only texts. For instance, in a sentence “X 或者 Y ... (X or Y ...)”, the word Y ought to have the same POS tag as the word X. Another example is that the word following a pronoun is usually a verb, and adjectives often describe nouns. Some related works show that word-level features can improve the performance of Chinese POS tagging (Jiang et al., 2008; Sun, 2011). In this paper, we propose a method to tag mixed texts with dynamic features. Our method combines these dynamic features, which are dynamically generated at the decoding stage, with traditional static features. For Chinese-English mixed texts, the traditional features cannot yield a satisfied result due to lack of training data. The proposed dynamic features can improve the performance by using the information of a word, such as POS tag or length of the whole word, which is proven effective by experiments. The rest of the paper is organized as follows: In section 2, we introduce the sequence"
D12-1126,O03-4002,0,0.0482067,"seline Our Baseline Our Baseline Our P 88.62 91.67 48.31 60.53 78.95 84.21 R 78.31 87.30 74.14 79.31 53.57 57.14 F1 83.15 89.43 58.50 68.66 63.83 68.09 Table 22: Performances of Model C on Dataset R POS tag NN VV VA NR 7 Method Baseline Our Baseline Our Baseline Our Baseline Our P 80.25 84.56 54.88 61.25 84.62 88.24 56.52 55.17 R 81.82 81.82 77.59 84.48 39.29 53.57 37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF)"
D12-1126,H01-1035,0,0.00973311,"37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm. In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size. The problem of this approach lies in that the decoding algorithm depends"
D13-1062,W02-1001,0,0.0107096,"s; L is the length of x. (4) + ⟨v1 , ⟨u, ∑ f(x, y)⟩+ y∈φ(zi ) ⟨s, h(x, zi )⟩ ∑ g1 (x, yi−1:i )⟩ yi−1 ∈φ(zi−1 ) yi ∈φ(zi ) ) + ⟨v2 , g2 (x, zi−1:i )⟩ , (7) where f and h represent the state feature vectors on two label sets Y and Z respectively. In Eq.(6) and (7), the score of the label of every character is decided by the weights of the corresponding mapping labels and itself. 6 Training Input sequence: x TaggerPPD Shared Information Output: PPD-style Tags We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overﬁtting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a uniﬁed formula. Given a sequence x and the expect type of tags T , the merged model is ∑ ˆ = arg max⟨w, y Φ(x, z)⟩, (8) TaggerCTB Output: CTB-style Tags y t(y)=T Figure 3: Our model for Heterogeneous POS Tagging The main challenge of our model is the eﬃciency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent variables have not expand all poss"
D13-1062,P04-1059,0,0.110752,"tagsets. The correct mapping relations can be automatically built in training phase. The rest of the paper is organized as follows: We ﬁrst introduce the related works in section 2 and describe the background of character-based method for joint Chinese S&T in section 3. Section 4 presents an automatic method to build the loose mapping function. Then we propose our method on heterogeneous corpora in 5 and 6. The experimental results are given in section 7. Finally, we conclude our work in section 8. 2 Related Works There are some works to exploit heterogeneous annotation data for Chinese S&T. (Gao et al., 2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging. (Jiang et al., 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with diﬀerent al., 2013), which is a simpliﬁed case of the work in this paper and has a relative low complexity. Diﬀerent with the multiple task learning, whose tasks are actually diﬀerent"
D13-1062,P08-1102,0,0.0203957,"ks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-"
D13-1062,P09-1059,0,0.787797,"CTB PDD Liu Xiang 刘翔/NR 刘/nrf 翔/nrg reachs 进入/VV 进入/v China 中国区/NN 中国/ns 区/n ﬁnal 总决赛/NN 总/b 决赛/vn Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD CTB, into two words. The POS tagsets are also signiﬁcantly diﬀerent. For example, PDD gives diverse tags “n” and “vn” for the noun, while CTB just gives “NN”. For proper names, they may be tagged as “nr”, “ns”, etc in PDD, while they are just tagged as “NR” in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classiﬁcation and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The ﬁrst step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the ﬁnal taggers by using the outputs of the preliminary taggers as features. We call these methods as “p"
D13-1062,D12-1038,0,0.0110443,"multiple task learning, whose tasks are actually diﬀerent labels in the same classiﬁcation task, our model utilizes the shared information between the real diﬀerent tasks and can produce the corresponding diﬀerent styles of outputs. Input: x TaggerPPD z=f(x) Output: f(x) TaggerCTB y=h(x,f(x)) Output: CTB-style Tags Figure 1: Traditional Pipeline-based Strategy for Heterogeneous POS Tagging underling annotation guidelines. They further proposed two optimization strategies, iterative training and predict-self re-estimation, to further improve the accuracy of annotation guideline transformation (Jiang et al., 2012). (Sun and Wan, 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. These methods regard one annotation as the main target and another annotation as the complementary/auxiliary purposes. For example, in their solution, an auxiliary tagger TaggerPPD is trained on a complementary corpus PPD, to assist the target CTB-style TaggerCTB . To reﬁne the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1. The related work in machine learning literature is multiple"
D13-1062,I08-4010,0,0.087153,"et al., 2013). 7 Experiments 7.1 Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. Dataset Partition Training CTB-5 CTB-S PPD Develop Test Training Test Training Test Sections 1−270 400−931 1001−1151 301−325 271−300 - Words 0.47M 6.66K 7.82K 0.64M 59.96K 1.11M 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin and Chen, 2008; Jiang et al., 2009; Sun and Wan, 2012) for CTB 5.0. • Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoﬀ (SIGHAN Bakeoﬀ 2008)(Jin and Chen, 2008). 7.1.2 PPD Dataset For the PPD dataset, we use the PKU dataset from SIGHAN Bakeoﬀ 2008. The details of all datasets are shown in Table 3. Our experiment on these datasets may lead to a fair comparison of our system and the related works. 7.2 Setting We conduct two experiments on CTB-5 + PPD and CTB-S + PPD respectively. The form of feature templates we used is shown in Table 7.2, wher"
D13-1062,P06-1096,0,0.0211414,"ressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overﬁtting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a uniﬁed formula. Given a sequence x and the expect type of tags T , the merged model is ∑ ˆ = arg max⟨w, y Φ(x, z)⟩, (8) TaggerCTB Output: CTB-style Tags y t(y)=T Figure 3: Our model for Heterogeneous POS Tagging The main challenge of our model is the eﬃciency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent variables have not expand all possible mappings. In this paper, we also only expand the mapping that with highest according to the current model. Our model is shown in Figure 3 and the ﬂowchart is shown in Algorithm 1. If given the output type of label T , we only consider the labels in T to initialize the Viterbi matrix, and the score of each node is determined by all the involved heterogeneous labels according to the loose mapping function. z∈ψ(y) where t(y) is a function to judge the type of output tags; ψ(y) represe"
D13-1062,W04-3236,0,0.0426626,"lity among the tasks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters T"
D13-1062,C12-2093,1,0.828004,"been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated corpora, the relatio"
D13-1062,P13-4009,1,0.818558,"distinguish the right tag. And if the right tag is one of “n,nt,nz/PDD” but not “n/PDD” (for example, “nt/PDD”), which means it is a “NN/CTB”, the weight of “NN/CTB” will remain unchanged according to the algorithm (updating “n/PDD” changes the “NN/CTB”, but updating “nt/PDD” changes it back). Therefore, after multiple iterations, useful features derived from the mapping function are typically receive more updates, which take relatively more responsibility for correct prediction. The ﬁnal model has good parameter estimates for the shared information. We implement our method based on FudanNLP(Qiu et al., 2013). 7 Experiments 7.1 Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. Dataset Partition Training CTB-5 CTB-S PPD Develop Test Training Test Training Test Sections 1−270 400−931 1001−1151 301−325 271−300 - Words 0.47M 6.66K 7.82K 0.64M 59.96K 1.11M 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin and Chen,"
D13-1062,P12-1025,0,0.447924,"总/b 决赛/vn Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD CTB, into two words. The POS tagsets are also signiﬁcantly diﬀerent. For example, PDD gives diverse tags “n” and “vn” for the noun, while CTB just gives “NN”. For proper names, they may be tagged as “nr”, “ns”, etc in PDD, while they are just tagged as “NR” in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classiﬁcation and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The ﬁrst step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the ﬁnal taggers by using the outputs of the preliminary taggers as features. We call these methods as “pipelinebased” methods. In this paper, we propose a method for joint Chinese word segmen"
D13-1062,P11-1139,0,0.0206805,"arning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated c"
D15-1092,P14-1062,0,0.52296,"to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree ∗ cannot agree Corresponding author. 793 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 793–"
D15-1092,D14-1181,0,0.127395,"Missing"
D15-1092,C02-1150,0,0.144163,"zN , zL and zR decide what to preserve when combining the children’s information. Intuitively, these gates seem to decide how to update and exploit the combination information. In the case of text classification, for each given (i) sentence xi = w1:N (i) and the corresponding class (i) (i) (i) w(i) w(i) w3 w4 w5 1 2 Figure 2: Architecture of Gated Recursive Neural Network (GRNN). In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity. Experiments on the Stanford Sentiment Treebank dataset (Socher et al., 2013b) and the TREC questions dataset (Li and Roth, 2002) show the effectiveness of our approach. 2 (i) yi , we first represent each word wj into its corresponding embedding ww(i) ∈ Rd , where N (i) inj dicates the length of i-th sentence and d is dimensionality of word embeddings. Then, the embeddings are sent to the first layer of GRNN as inputs, whose outputs are recursively applied to upper layers until it outputs a single fixed-length vector. Next, we receive the class distribution P(·|xi ; θ) for the given sentence xi by a softmax transformation of ui , where ui is the top node of the network (a fixed length vectorial representation): Gated Re"
D15-1092,P15-1168,1,0.463731,"ly increases with the length of sentences. Another difference is that we introduce two kinds of gates, reset and update gates (Chung et al., 2014), to control the combinations in recursive structure. With these two gating mechanisms, our model can better model the complicated combinations of features and capture the long dependency interactions. In our previous works, we have investigated several different topological structures (tree and directed acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural"
D15-1092,D11-1014,0,0.0609733,"L) criterion to train our model. Given training set (xi , yi ) and the parameter set of our model θ, the goal is to minimize the loss function: 3.3 Experiment Results Table 2 shows the performance of our GRNN on three datasets. 1 m 1 ∑ λ J(θ) = − log P(yi |xi ; θ) + ∥θ∥22 , (7) m 2m http://nlp.stanford.edu/sentiment http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 2 i=1 795 Methods NBoW (Kalchbrenner et al., 2014) PV (Le and Mikolov, 2014) CNN-non-static (Kim, 2014) CNN-multichannel (Kim, 2014) MaxTDNN (Collobert and Weston, 2008) DCNN (Kalchbrenner et al., 2014) RecNTN (Socher et al., 2013b) RAE (Socher et al., 2011) MV-RecNN (Socher et al., 2012) AdaSent (Zhao et al., 2015) GRNN (our approach) SST-1 42.4 44.6∗ 48.0 47.4 37.4 48.5 45.7 43.2 44.4 47.5 SST-2 80.5 82.7∗ 87.2 88.1 77.1 86.8 85.4 82.4 82.9 85.5 QC 88.2 91.8∗ 93.6 92.2 84.4 93.0 92.4 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be rega"
D15-1092,P13-1045,0,0.251571,"acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree ∗ cannot agree Corresponding author. 793 Proceedin"
D15-1092,D13-1170,0,0.353549,"acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree ∗ cannot agree Corresponding author. 793 Proceedin"
D15-1092,D12-1110,0,\N,Missing
D15-1092,W14-4012,0,\N,Missing
D15-1092,D15-1215,1,\N,Missing
D15-1141,J96-1002,0,0.251174,"ments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system"
D15-1141,P15-1168,1,0.526007,"urian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window. Intuitively, many words are difficult to segment based on the local information only. For example, the segmentation of the follow"
D15-1141,I05-3017,0,0.518308,"Missing"
D15-1141,D15-1280,1,0.0484058,"ent problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sutskever et al., 2014). The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see Figure 2) . The behavior of the cell is controlled by three “gates”, namely input gate i, forget gate f and output gate o. The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector. The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of"
D15-1141,P14-1028,0,0.750867,"asks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the cont"
D15-1141,C04-1081,0,0.829695,"tation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineer"
D15-1141,P13-1045,0,0.0413166,"o all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013"
D15-1141,D13-1061,0,0.443356,"d by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters"
D15-1141,D11-1090,0,0.0275565,"Missing"
D15-1141,I05-3027,0,0.0366579,"Missing"
D15-1141,P10-1040,0,0.0273478,"equence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015)"
D15-1141,O03-4002,0,0.931359,"word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort"
D15-1141,P12-1083,0,0.049662,"Missing"
D15-1141,P07-1106,0,0.105943,"ir methods. models P +Pre-train (Zheng et al., 2013) (Pei et al., 2014) LSTM +bigram LSTM +Pre-train+bigram (Pei et al., 2014) LSTM PKU R MSRA P R F F P CTB6 R F 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 - 95.2 - 97.2 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results with * symbol are from our implementations of their methods. Models (Tseng et al., 2005) (Zhang and Clark, 2007) (Sun and Xu, 2011) (Zhang et al., 2013) This work PKU MSRA CTB6 95.0 96.4 95.1 97.2 95.7 96.1 97.4 96.5 97.4 96.0 a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Table 6: Comparison of our model with state-ofthe-art methods on three test sets. with previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-base"
D15-1141,D13-1031,0,0.061144,"Missing"
D15-1215,C14-1076,0,0.0133052,"te the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted p"
D15-1215,D14-1082,0,0.530697,"rsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neura"
D15-1215,C14-1078,0,0.0286333,"Missing"
D15-1215,P15-1168,1,0.745625,"dren nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the (5) de where word embedding ew p.w ∈ R , pos embedt d e ding ep.t ∈ R and label embedding elp.l ∈ Rde are extracted from embedding matrices Ew , Et and El according to the indices of the corresponding word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term elp.l in Eq. 5. Thus, the dimensionality dn of vnp varies. In labeled attachment parsing case, we set a special token NULL to represent label p.l if not available (e.g. p is the node in stack or buffer). By given"
D15-1215,D15-1092,1,0.779126,"dren nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the (5) de where word embedding ew p.w ∈ R , pos embedt d e ding ep.t ∈ R and label embedding elp.l ∈ Rde are extracted from embedding matrices Ew , Et and El according to the indices of the corresponding word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term elp.l in Eq. 5. Thus, the dimensionality dn of vnp varies. In labeled attachment parsing case, we set a special token NULL to represent label p.l if not available (e.g. p is the node in stack or buffer). By given"
D15-1215,P13-1104,0,0.0308317,"Missing"
D15-1215,P14-1129,0,0.038985,"parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of"
D15-1215,P15-1033,0,0.0208946,"ng all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their method only relies on dense features, and is not able to automatically learn the most useful feature conjunctions to predict the transition action. Compared with (Chen and Manning, 2014), our method can fully exploit the information of all the descendants of a node in stack with Tree-GRNN. Then DAG-GRNN automatically learns the complicated combination of all the features, while the traditional discrete feature based methods need manually design them. Dyer et al. (2015) improved the transition-based dependency parsing using stack long short term memory neural network and received significant improvement on performance. They focused on exploiting the long distance dependencies and information, while we aims to automatically model the complicated feature combination. 8 Conclusion In this paper, we pay attention to the syntactic and semantic composition of the dense features for transition-based dependency parsing. We propose two heterogeneous gated recursive neural networks, Tree-GRNN and DAG-GRNN. Each hidden neuron in two proposed GRNNs can be regarded as a"
D15-1215,Q13-1033,0,0.0168949,"or dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minim"
D15-1215,D13-1152,0,0.0540767,"Missing"
D15-1215,P10-1110,0,0.0238413,"DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specifi"
D15-1215,W07-2416,0,0.0821264,"Missing"
D15-1215,P08-1068,0,0.046429,"posed model. 1 (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010"
D15-1215,D14-1081,0,0.272393,"thods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 201"
D15-1215,P09-1039,0,0.0310972,"neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural"
D15-1215,E06-1011,0,0.11574,"Missing"
D15-1215,P05-1012,0,0.0883012,"(a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods"
D15-1215,P81-1022,0,0.57741,"Missing"
D15-1215,W04-0308,0,0.0348139,"Missing"
D15-1215,P13-1045,0,0.633824,"achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in sta"
D15-1215,Q14-1017,0,0.242651,"Association for Computational Linguistics. plicated feature combinations which can be manually designed in the traditional discrete feature based methods. To tackle these problems, we use two heterogeneous gated recursive neural networks, tree structured gated recursive neural network (TreeGRNN) and directed acyclic graph gated structured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing. The two proposed GRNNs introduce the gate mechanism (Chung et al., 2014) to improve the standard recursive neural network (RNN) (Socher et al., 2013; Socher et al., 2014), and can model the syntactic and semantic compositions of the nodes during parsing. Figure 1 gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions. DAG-GRNN is applied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features recursively from children nodes into their parent according to their dependency structures, while DA"
D15-1215,P10-1040,0,0.0247979,"s (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss"
D15-1215,D08-1059,0,0.0771397,"Missing"
D15-1215,P11-2033,0,0.022786,"tch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-bas"
D15-1215,P15-1112,1,0.903192,"rsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1879–1889"
D15-1215,nivre-etal-2006-maltparser,0,\N,Missing
D15-1215,D14-1179,0,\N,Missing
D15-1215,P14-2131,0,\N,Missing
D15-1280,D15-1141,1,0.0532292,"Missing"
D15-1280,P14-1062,0,0.475887,"s on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., ∗ 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can c"
D15-1280,D14-1181,0,0.0823907,"Missing"
D15-1280,C02-1150,0,0.0317821,"level, and the last dataset is document-level. The detailed statistics about the four datasets are listed in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. (16) where L is the average length of the corpus. Thus, the slowest group is activated at least twice. 5 Training In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distribution"
D15-1280,P11-1015,0,0.0556068,"isted in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. (16) where L is the average length of the corpus. Thus, the slowest group is activated at least twice. 5 Training In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an L2 regularization term over the parameters. The network is trained with"
D15-1280,D11-1014,0,0.43637,"ence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural netw"
D15-1280,D12-1110,0,0.868221,"ive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used"
D15-1280,D13-1170,0,0.44174,"n model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., ∗ 2015) and machine translation (Sutskever et al., 2014). LSTM is an e"
D15-1280,P10-1040,0,0.0289762,"and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term"
D15-1280,P12-2018,0,0.0615111,"d update rule (Duchi et al., 2011). The back propagation of the error propagation is similar to LSTM as well. The only difference is that the error propagates only from groups that were executed at time step t. The error of nonactivated groups gets copied back in time (similarly to copying the activations of nodes not activated at the time step t during the corresponding forward pass), where it is added to the backpropagated error. 6 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). Experiments In this section, we investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to other competitor models. 1 http://nlp.stanford.edu/sentiment. http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3 ht"
D15-1280,D14-1179,0,\N,Missing
D16-1012,P07-1056,0,0.0643455,"Missing"
D16-1012,D14-1082,0,0.0202468,"More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an approach to learn multiple related tasks simultaneously to significantly improve performan"
D16-1012,P15-1166,0,0.00687272,"sentation for input words and solved different traditional NLP tasks within one framework. However, only one lookup table is shared, and the other lookup tables and layers are task-specific. Liu et al. (2015b) developed a multi-task DNN for learning representations across multiple tasks. Their multi-task DNN approach combines tasks of query classification and ranking for web search. But the input of the model is bag-of-word representation, which loses the information of word order. More recently, several multi-task encoder125 decoder networks were also proposed for neural machine translation (Dong et al., 2015; Luong et al., 2015; Firat et al., 2016), which can make use of cross-lingual information. Unlike these works, in this paper we design two neural architectures with shared memory for multitask learning, which can store useful information across the tasks. Our architectures are relatively loosely coupled, and therefore more flexible to expand. With the help of shared memory, we can obtain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks. 7 Conclusion and Future Work In this paper, we introduce two deep architectures for multi-task learning"
D16-1012,N16-1101,0,0.0452605,"Missing"
D16-1012,P14-1062,0,0.112434,"insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Correspo"
D16-1012,D14-1181,0,0.0121801,"Missing"
D16-1012,D15-1280,1,0.855459,"opose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an"
D16-1012,N15-1092,0,0.0816419,"opose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an"
D16-1012,P16-1098,1,0.518839,"y on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an approach to learn multiple related tasks simultaneously"
D16-1012,P11-1015,0,0.00422292,"veral related tasks. 5.1 Datasets The used multi-task datasets are briefly described as follows. The detailed statistics are listed in Table 1. Movie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews. • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • SUBJ The movie reviews with labels of subjective or objective (Pang and Lee, 2004). • IMDB The IMDB dataset2 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 1 http://nlp.stanford.edu/sentiment. http://ai.stanford.edu/˜amaas/data/ sentiment/ 2 Model LSTM Single Task ME-LSTM ARC-I ARC-II Multi-task MT-CNN MT-DNN NBOW RAE (Socher et al., 2011) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DCNN (Kalchbrenner et al., 2014) CNN-multichannel (Kim, 2014) Tree-LSTM (Tai et al., 2015) SST-1 45.9 46.4 48.6 49.5 46.7 44.5 42.4 43.2 44.4 45.7 48.5 47.4 50.6 SST-2 85.8 85.5 87.0 87.8 86.1 84.0 80.5 82.4 82.9 85.4 86.8 88.1 86.9 SUBJ 91.6 91.0 93.8 95.0 92.2 90.1 91.3 93."
D16-1012,P04-1035,0,0.0123024,"irical performances of our proposed architectures on two multitask datasets. Each dataset contains several related tasks. 5.1 Datasets The used multi-task datasets are briefly described as follows. The detailed statistics are listed in Table 1. Movie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews. • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • SUBJ The movie reviews with labels of subjective or objective (Pang and Lee, 2004). • IMDB The IMDB dataset2 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 1 http://nlp.stanford.edu/sentiment. http://ai.stanford.edu/˜amaas/data/ sentiment/ 2 Model LSTM Single Task ME-LSTM ARC-I ARC-II Multi-task MT-CNN MT-DNN NBOW RAE (Socher et al., 2011) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DCNN (Kalchbrenner et al., 2014) CNN-multichannel (Kim, 2014) Tree-LSTM (Tai et al., 2015) SST-1 45.9 46.4 48.6 49.5 46.7 44.5 42.4 43.2 44.4 45.7 48.5 47.4 50.6 SST-2 85.8 85."
D16-1012,D14-1162,0,0.111743,"08) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific. 3 https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 123 • MT-DNN: The model is proposed by Liu et al. (2015b) with bag-of-words input and multilayer perceptrons, in which a hidden layer is shared. 5.3 Hyperparameters and Training The networks are trained with backpropagation and the gradient-based optimization is performed using the Adagrad update rule (Duchi et al., 2011). The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pennington et al., 2014)) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The mini-batch size is set to 16. For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], l2 regularization [0.0, 5E−5, 1E−5]. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters are set as Table 2. 5.4 5.6 Multi-task Learning of Movie Reviews We fir"
D16-1012,D11-1014,0,0.0811253,"Missing"
D16-1012,D12-1110,0,0.0638676,"Missing"
D16-1012,D13-1170,0,0.356173,"In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-t"
D16-1012,P15-1150,0,0.020304,"Missing"
D16-1012,P10-1040,0,0.0282603,"Missing"
D16-1172,D15-1263,0,0.0184525,"ious methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra discourse paring results. Most of these models work well on sentence-level or paragraph-level sentiment classification. When it comes to the document-level sentiment classification, a bottom-up hierarchical strategy is often adopted to alleviate the model complexity (Denil et al., 2014; Tang et al., 2015b; Li et al., 2015). 2.2 Memory Augmented Recurrent Models Although it is widely accepted that LSTM has more long-lasting memory units than RNNs, it still suffers from “forgetting” information which is too far away from the current poin"
D16-1172,D15-1092,1,0.396018,"evel sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori structural assumptio"
D16-1172,D14-1181,0,0.0272154,"document-level sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori s"
D16-1172,D15-1278,0,0.0147628,"criminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra discourse paring results. Most of these models work well on sentence-level or paragraph-level sentiment classification. When it comes to the document-level sentiment classification, a bottom-up hierarchical strategy is often adopted to alleviate the model complexity (Denil et al., 2014; Tang et al., 2015b; Li et al., 2015). 2.2 Memory Augmented Recurrent Models Although it is widely accepted that LSTM has more long-lasting memory units than RNNs, it still suffers from “forgetting” information which is too far away from the current point (Le et al., 2015; Karpathy et al., 2015). Such a scalability problem of LSTMs is crucial to extend some previous sentence-level work to document-level sentiment analysis. Various models have been proposed to increase the ability of LSTMs to store long-range information (Le et al., 2015; Salehinejad, 2016) and two kinds of approaches gain attraction. One is to augment LSTM with a"
D16-1172,D15-1280,1,0.859919,"is. Various models have been proposed to increase the ability of LSTMs to store long-range information (Le et al., 2015; Salehinejad, 2016) and two kinds of approaches gain attraction. One is to augment LSTM with an external memory (Sukhbaatar et al., 2015; Monz, 2016), but they are of poor performance on time because of the huge external memory matrix. Unlike these methods, we fully exploit the potential of internal memory of LSTM by adjusting its forgetting rates. The other one tries to use multiple time-scales to distinguish different states (El Hihi and Bengio, 1995; Koutnik et al., 2014; Liu et al., 2015). They partition the hidden states into several groups and each group is activated and updated at different frequencies (e.g. one group updates every 2 time-step, the other updates every 4 time-step). In these methods, different memory groups are not fully interconnected, and the information is transmitted from faster groups to slower ones, or vice versa. However, the memory of slower groups are not updated at every step, which may lead to sentiment information loss and semantic inconsistency. In our proposed CLSTM, we assign different forgetting rates to memory groups. This novel strategy ena"
D16-1172,P07-1055,0,0.00836746,". Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori structural assumptions or discard the order information within a sentence, which are vulnerable to sudden change or twists in texts especially a long-range one (McDonald et al., 2007; Mikolov et al., 2013). Recurrent models match people’s intuition of reading word by word and are capable to model the intrinsic relations between sentences. By keeping the word order, RNNs could extract the sentence representation implicitly and meanwhile analyze the semantic meaning of a whole document without any explicit boundary. Partially inspired by neural structure of human brain and computer system architecture, we present the Cached Long Short-Term Memory neural networks (CLSTM) to capture the long-range sentiment information. In the dual store memory model 1660 Proceedings of the 2"
D16-1172,pak-paroubek-2010-twitter,0,0.0214553,"me variants of LSTM which address the problem on storing the long-term information. 2.1 Document-level Sentiment Classification Document-level sentiment classification is a sticky task in sentiment analysis (Pang and Lee, 2008), which is to infer the sentiment polarity or intensity of a whole document. The most challenging part is that not every part of the document is equally informative for inferring the sentiment of the whole 1661 document (Pang and Lee, 2004; Yessenalina et al., 2010). Various methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra discourse paring results. Most of these model"
D16-1172,P04-1035,0,0.0396153,"we briefly introduce related work in two areas: First, we discuss the existing documentlevel sentiment classification approaches; Second, we discuss some variants of LSTM which address the problem on storing the long-term information. 2.1 Document-level Sentiment Classification Document-level sentiment classification is a sticky task in sentiment analysis (Pang and Lee, 2008), which is to infer the sentiment polarity or intensity of a whole document. The most challenging part is that not every part of the document is equally informative for inferring the sentiment of the whole 1661 document (Pang and Lee, 2004; Yessenalina et al., 2010). Various methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM f"
D16-1172,D14-1162,0,0.112883,"Missing"
D16-1172,D13-1170,0,0.103268,"of information, while document-level sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit"
D16-1172,P15-1150,0,0.244219,"as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori structural assumptions or discard the order information within a sentence, which are vulnerable to sudden change or twists in texts especially a long-range one (McDonald et al., 2007; Mikolov et al., 2013). Recurrent models match people’s intuition of reading word by word and are capable to model the intrinsic relations between sentences. By keeping the word order, RNNs could extract the sentence representation implici"
D16-1172,H05-1044,0,0.0364848,"ication approaches; Second, we discuss some variants of LSTM which address the problem on storing the long-term information. 2.1 Document-level Sentiment Classification Document-level sentiment classification is a sticky task in sentiment analysis (Pang and Lee, 2008), which is to infer the sentiment polarity or intensity of a whole document. The most challenging part is that not every part of the document is equally informative for inferring the sentiment of the whole 1661 document (Pang and Lee, 2004; Yessenalina et al., 2010). Various methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra d"
D16-1172,J09-3003,0,0.0213687,"ph-level sentiment analysis expects the model to extract features from limited source of information, while document-level sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate hi"
D16-1172,D10-1102,0,0.0150866,"Missing"
D16-1176,D15-1075,0,0.0679935,"rately, then they are concatenated and fed to a MLP. • Attention LSTMs: An attentive LSTM to encode two sentences into a semantic space, which used in (Hermann et al., 2015; Rockt¨aschel et al., 2015). • Word-by-word Attention LSTMs: An improvement of attention LSTM by introducing wordby-word attention mechanism, which used in (Hermann et al., 2015; Rockt¨aschel et al., 2015). 6.3 Experiment-I: Recognizing Textual Entailment Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) (Bowman et al., 2015). This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. 6.3.1 Results Table 2 shows the evaluation results on SNLI. The 3rd column of the table gives the number of parameters of different models without the word embeddings. Our proposed two C-LSTMs models with four stacked blocks outperform all the competitor models, which indicates that ou"
D16-1176,D15-1181,0,0.0294076,"teractions, such as ARCI(Hu et al., 2014), CNTN(Qiu and Huang, 2015) and so on. These models first encode two sequences with some basic (Neural Bag-of-words, BOW) or advanced (RNN, CNN) components of neural networks separately, and then compute the matching score based on the distributed vectors of two sentences. In this paradigm, two sentences have no interaction until arriving final phase. Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN (Yin and Sch¨utze, 2015) and MultiPerspective CNN (He et al., 2015). Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN (Yin et al., 2015), Attention LSTM(Rockt¨aschel et al., 2015; Hermann et al., 2015). These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level. Strong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions, such as ARC-II (Hu et al., 2014), MV-LSTM"
D16-1176,P14-1062,0,0.0777921,"hitecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architectures. 1 Introduction Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification (Kalchbrenner et al., 2014; Liu et al., 2015), question answering and machine translation (Sutskever et al., 2014) and so on. Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching. Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). ∗ Corresponding author. According to the phases of interaction between two sentences, previous models can be classified into three categories. Weak interaction Model"
D16-1176,D15-1280,1,0.746506,"ng interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architectures. 1 Introduction Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification (Kalchbrenner et al., 2014; Liu et al., 2015), question answering and machine translation (Sutskever et al., 2014) and so on. Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching. Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). ∗ Corresponding author. According to the phases of interaction between two sentences, previous models can be classified into three categories. Weak interaction Models Some early works"
D16-1176,P16-1098,1,0.558516,"t attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN (Yin et al., 2015), Attention LSTM(Rockt¨aschel et al., 2015; Hermann et al., 2015). These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level. Strong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions, such as ARC-II (Hu et al., 2014), MV-LSTM (Wan et al., 2016) and DF-LSTMs(Liu et al., 2016). These models can easily capture the difference between semantic capacity of two sentences. In this paper, we propose a new deep neural network architecture to model the strong interactions 1703 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1703–1712, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of two sentences. Different with modelling two sentences with separated LSTMs, we utilize two interdependent LSTMs, called coupled-LSTMs, to fully affect each other at different time steps. The output of coupled-LST"
D16-1176,D14-1162,0,0.0836116,"stochastic gradient descent with the diagonal variant of AdaGrad (Duchi et al., 2011). To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold (Graves, 2013). 6 Experiment In this section, we investigate the empirical performances of our proposed model on two different text matching tasks: classification task (recognizing textual entailment) and ranking task (matching of question and answer). 6.1 Hyperparameters and Training The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pennington et al., 2014)) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.05, 0.0005, 0.0001], l2 regularization [0.0, 5E−5, 1E−5, 1E−6] and the threshold value of gradient norm [5, 10, 100]. The final hyperparameters are set as Table 1. 6.2 Competitor Methods • Neural bag-of-words (NBOW): Each sequence as the sum of the embeddings of"
D16-1176,N15-1091,0,0.0461014,"Missing"
D17-1124,balahur-etal-2010-sentiment,0,0.11879,"Missing"
D17-1124,P16-1139,0,0.0146598,"0.01, 0.001], l2 regularization [0.0, 5E−5, 1E−5] The final hyper-parameters are as follows. The initial learning rate is 0.1. The regularization weight of the parameters is 1E−5. For all the sentences from the five datasets, we parse them with constituency parser (Klein and Manning, 2003) to obtain the trees for our and some competitor models. 5.2 We give some descriptions about the setting of our models and several baseline models. • CharLSTM: Character level LSTM. • TLSTM: Vanilla tree-based LSTM, proposed by Tai et al. (2015). • Cont-TLSTM: Context-dependent tree-based LSTM, introduced by Bowman et al. (2016). • iTLSTM-Lo: Proposed model with Look-Up idiomatic interpreter. • iTLSTM-Mo: Proposed model with Morphology-Sensitive interpreter. #Sentences Table 4: Key statistics for the idioms and sentences in iSent dataset. O(Original) denotes the idioms in dev/test sets are in original forms and have appeared in training set. M(Morphology) and L(Lexical) represent the morphology and lexical idiom variations respectively and they are unseen in training set. 600 500 400 300 200 100 0 5.3 0 5 10 15 20 25 30 35 40 45 50 55 Sentence length Figure 2: The distribution of the number of reviews over different"
D17-1124,W06-3506,0,0.0445032,"sentences such as the “Verb Phrases” and “Adverb Phrases”. For example, the sentence “More often than not, this mixed bag hit its mark” has a positive sentiment. Cont-TLSTM pays much more attention to the word “not” without realizing that it belongs to the collocation “more often than not”, which expresses neutral emotion. In comparison, our model regards this collocation as a whole with neutral sentiment, which is crucial for the final prediction. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the con"
D17-1124,P16-1020,0,0.0323403,"Missing"
D17-1124,P14-1062,0,0.225926,"semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge"
D17-1124,C12-2054,0,0.0320652,"tion. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence representations. More recently, Zhu et al. (2016) propose a DAG-structured LSTM to incorporate external semantics including non-compositional or holistically learned semantics. Its key characteristic is that a DAG needs be built in advance, which merges some detected n-grams as the noncompositional phrases based on external knowledge. Different from this work, we focus on how to integrate detection and understanding of idioms int"
D17-1124,P16-1160,0,0.0248845,"al memory M , which is a table and stores idiomatic information for each idiom as depicted in the top subfigure in Figure 1-(c). Formally, the idiomatic meaning for a phrase can be obtained as: h(i) = M[k] (6) where k denotes the index of the corresponding phrase p. Morphology-Sensitive Model Since most idioms enjoy certain flexibility in morphology, lexicon, syntax, the above model suffers from the problem of idiom variations. To remedy this, inspired by the compositional view of idioms (Nunberg et al., 1994) and recent success of characterbased models (Kim et al., 2016; Lample et al., 2016; Chung et al., 2016), we propose to use CharLSTM to directly encode the meaning of a phrase in an idiomatic space and generate an idiomatic representation, which is not contaminated by its literal meaning and sensitive to different variations. Formally, for each non-leaf node i and its corresponding phrase pi in a constituency tree, we apply charLSTM to phrase pi as depicted in the bottom subfigure in Figure 1-(c) and utilize the emitted hidden states rj to represent the idiomatic meaning of the phrase. rj = Char-LSTM(rj−1 , cj−1 , xj ) (7) where j = 1, 2, · · · , m and m represents the length of the input phrase"
D17-1124,W06-1203,0,0.281998,"eiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. Mo"
D17-1124,J09-1005,0,0.188751,"onvolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom d"
D17-1124,D14-1181,0,0.0266757,"Missing"
D17-1124,P03-1054,0,0.0175655,"he lurch → in the big lurch on the same page → on different pages Table 3: Idiom variations at morphological and lexical level. Add. and Sub. refer to lexical addition and substitution respectively. Train Idiom Sent. 1247 9752 O 124 720 Dev M 21 200 L 40 100 O 124 1403 Test M 21 400 L 40 200 tions of the initial learning rate [0.1, 0.01, 0.001], l2 regularization [0.0, 5E−5, 1E−5] The final hyper-parameters are as follows. The initial learning rate is 0.1. The regularization weight of the parameters is 1E−5. For all the sentences from the five datasets, we parse them with constituency parser (Klein and Manning, 2003) to obtain the trees for our and some competitor models. 5.2 We give some descriptions about the setting of our models and several baseline models. • CharLSTM: Character level LSTM. • TLSTM: Vanilla tree-based LSTM, proposed by Tai et al. (2015). • Cont-TLSTM: Context-dependent tree-based LSTM, introduced by Bowman et al. (2016). • iTLSTM-Lo: Proposed model with Look-Up idiomatic interpreter. • iTLSTM-Mo: Proposed model with Morphology-Sensitive interpreter. #Sentences Table 4: Key statistics for the idioms and sentences in iSent dataset. O(Original) denotes the idioms in dev/test sets are in"
D17-1124,N16-1030,0,0.0328092,"rieved from an external memory M , which is a table and stores idiomatic information for each idiom as depicted in the top subfigure in Figure 1-(c). Formally, the idiomatic meaning for a phrase can be obtained as: h(i) = M[k] (6) where k denotes the index of the corresponding phrase p. Morphology-Sensitive Model Since most idioms enjoy certain flexibility in morphology, lexicon, syntax, the above model suffers from the problem of idiom variations. To remedy this, inspired by the compositional view of idioms (Nunberg et al., 1994) and recent success of characterbased models (Kim et al., 2016; Lample et al., 2016; Chung et al., 2016), we propose to use CharLSTM to directly encode the meaning of a phrase in an idiomatic space and generate an idiomatic representation, which is not contaminated by its literal meaning and sensitive to different variations. Formally, for each non-leaf node i and its corresponding phrase pi in a constituency tree, we apply charLSTM to phrase pi as depicted in the bottom subfigure in Figure 1-(c) and utilize the emitted hidden states rj to represent the idiomatic meaning of the phrase. rj = Char-LSTM(rj−1 , cj−1 , xj ) (7) where j = 1, 2, · · · , m and m represents the lengt"
D17-1124,D09-1033,0,0.0840053,"; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introd"
D17-1124,C10-1113,0,0.0214074,"“Verb Phrases” and “Adverb Phrases”. For example, the sentence “More often than not, this mixed bag hit its mark” has a positive sentiment. Cont-TLSTM pays much more attention to the word “not” without realizing that it belongs to the collocation “more often than not”, which expresses neutral emotion. In comparison, our model regards this collocation as a whole with neutral sentiment, which is crucial for the final prediction. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence repres"
D17-1124,P16-1098,1,0.837358,"ty of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms. The qualitative and quantitative experimental analyses demonstrate the efficacy of our models. The newly-introduced datasets are publicly available at http: //nlp.fudan.edu.cn/data/ 1 Introduction Currently, neural network models have achieved great success for many natural language processing (NLP) tasks , such as text classification (Zhao et al., 2015; Liu et al., 2017), semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recu"
D17-1124,D12-1110,0,0.149592,"Missing"
D17-1124,P17-1001,1,0.859383,"Missing"
D17-1124,D16-1176,1,0.851595,"ty of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms. The qualitative and quantitative experimental analyses demonstrate the efficacy of our models. The newly-introduced datasets are publicly available at http: //nlp.fudan.edu.cn/data/ 1 Introduction Currently, neural network models have achieved great success for many natural language processing (NLP) tasks , such as text classification (Zhao et al., 2015; Liu et al., 2017), semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recu"
D17-1124,Y14-1010,0,0.0550271,"Missing"
D17-1124,P04-1035,0,0.00622438,"lso evaluate our models on these datasets to make a comparison with many recent proposed models. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also derived from the Stanford Sentiment Treebank. • MR The movie reviews with two classes 2 (Pang and Lee, 2005). • SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective. (Pang and Lee, 2004) The detailed statistics about these four datasets are listed in Table 2. 4.2 Reasons for a New Dataset Differing from previous work, which evaluating idiom detection as a standalone task, we want to integrate idiom understanding into sentiment classification task. However, most of existing sentiment datasets do not cover enough idioms or related linguistic phenomenon. To better evaluate our models on idiom understanding task, we proposed an idiom-enriched sentiment classification dataset, in which each sentence contains at least one idiom. Additionally, considering most idioms have certain fl"
D17-1124,P05-1015,0,0.016112,"lassification We list four kinds of datasets which are most commonly used for sentiment classification in NLP community. Additionally, we also evaluate our models on these datasets to make a comparison with many recent proposed models. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also derived from the Stanford Sentiment Treebank. • MR The movie reviews with two classes 2 (Pang and Lee, 2005). • SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective. (Pang and Lee, 2004) The detailed statistics about these four datasets are listed in Table 2. 4.2 Reasons for a New Dataset Differing from previous work, which evaluating idiom detection as a standalone task, we want to integrate idiom understanding into sentiment classification task. However, most of existing sentiment datasets do not cover enough idioms or related linguistic phenomenon. To better evaluate our models on idiom understanding task, we proposed an idiom-enriched"
D17-1124,D14-1216,0,0.0930799,"networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom detector for each ph"
D17-1124,D14-1162,0,0.101534,": The distribution of the number of reviews over different lengths. 5.1 Experimental Settings Loss Function Given a sentence and its label, the output of neural network is the probabilities of the different classes. The parameters of the network are trained to minimise the cross-entropy of the predicted and true label distributions. To minimize the objective, we use stochastic gradient descent with the diagonal variant of AdaGrad (Duchi et al., 2011). Initialization and Hyperparameters In all of our experiments, the word embeddings for all of the models are initialized with the GloVe vectors (Pennington et al., 2014). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. For each task, we take the hyperparameters which achieve the best performance on the development set via a small grid search over combinaCompetitor Models Evaluation over Mainstream Datasets The experimental results are shown in Table 5. We can see Cont-TLSTM outperforms TLSTM on all four tasks, showing the importance of contextsensitive composition. Besides, both iTLSTM-Lo and iTLSTM-Mo achieve better results than TLSTM and Cont-LSTM, which indicates the effectiveness of our introduced module"
D17-1124,D11-1014,0,0.0732044,"Missing"
D17-1124,D13-1170,0,0.390752,"lation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neura"
D17-1124,P15-1150,0,0.377141,"014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic r"
D17-1124,N16-1106,0,0.0186106,"ation (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence representations. More recently, Zhu et al. (2016) propose a DAG-structured LSTM to incorporate external semantics including non-compositional or holistically learned semantics. Its key characteristic is that a DAG needs be built in advance, which merges some detected n-grams as the noncompositional phrases based on external knowledge. Different from this work, we focus on how to integrate detection and understanding of idioms into a unified end-to-end model, in which an idiomatic detector is introduced to adaptively control the semantic compositionality. Particularly, in the whole process no extra information is given to tell which phrases s"
D17-1124,W14-1007,0,0.0191394,"ntation. • We integrate idioms understanding into a real-world NLP task instead of evaluating idiom detection as a standalone task. • We construct a new real-world dataset covering abundant idioms with original and variational forms. The elaborate qualitative and quantitative experimental analyses show the effectiveness of our models. 2 Linguistic Interpretation of Idioms Recently, idioms have raised eyebrows among linguists, psycholinguists, and lexicographers due to their pervasiveness in daily discourse and their fascinating properties in linguistics literature (Villavicencio et al., 2005; Salton et al., 2014). As peculiar linguistic constructions (Villavicencio et al., 2005; Salton et al., 2014), idioms have three following properties: Invisibility Idioms always disguise themselves as normal multi-words in sentences. It makes end-to-end training hard since we should detect idioms first, and then understand them. Idiomaticity Idioms are semantically opaque, whose meanings cannot be derived from their constituent words. Existing compositional distributed approaches fail due to the hypothesis that the meaning of any phrase can be composed of the meanings of its constituents. Flexibility While structu"
D17-1124,P16-1019,0,0.0635211,"et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom detector for each phrase in a sentence to"
D18-1186,W17-5310,0,0.0245657,"Missing"
D18-1186,P14-1062,0,0.0560887,"twork (CIN). CIN can serve as an alternative module of attention mechanism. We first briefly introduce how the convolution works over text sequence, then describe the proposed model and its connection to attention model. 3.1 Convolution over Sequence Convolution is an effective operation in deep neural networks, which convolves the input with a set of filters to extract non-linear compositional features. Although originally designed for computer vision, convolutional models have subsequently shown to be effective for NLP and have achieved excellent performance in sentence modeling Kim (2014); Kalchbrenner et al. (2014), and other traditional NLP tasks Hu et al. (2014); Zeng et al. (2014); Gehring et al. (2017). Given a sentence representation X = d×m [x1 , x2 , · · · , xm ] ∈ R , a convolutional filter W (f ) ∈ Rd×kd , the convolution process is defined as where w, W1 , W2 and b are learnable parameters. 1577   x0i = f W (f ) [xi−[k/2] , · · · , xi+[k/2] ] + b(f ) , (8) where τ is the width of filter, FGN(·) is the filter generation network. A detailed implementation of FGN is presented in Section 3.4. Now we can convolve the two sentences with the generated filters. Convolutional Interaction Network Wx F"
D18-1186,D15-1075,0,0.152807,"Missing"
D18-1186,P17-1152,0,0.54434,"ing hypothesis. Generally, NLI is also related to many other NLP tasks under the paradigm of semantic matching of two texts, such as question answering Hu et al. (2014); Wan et al. (2016) and information retrieval Liu et al. (2015), and so on. An essential challenge is to capture the semantic relevance of two sentences. Due to the semantic gap (or lexical chasm) problem, natural language inference is still a challenging problem. Recently, deep learning is raising a substantial interest in natural language inference and has achieved some great progresses Hu et al. (2014); Parikh et al. (2016); Chen et al. (2017a). To model the complicated semantic relationship between two sentences, previous models heavily utilize various attention mechanism Bahdanau et al. ∗ † (2014); Vaswani et al. (2017) to build the interaction at different granularity (word, phrase and sentence level), such as ABCNN Yin et al. (2016), Attention LSTM Rockt¨aschel et al. (2015), bidirectional attention LSTM Chen et al. (2017a), and so on. While attention is very successful in natural language inference, its mechanism is quite simple and can be regarded as a weighted sum of the target vectors. This paradigm results in a lack of fl"
D18-1186,D14-1181,0,0.0220591,"Missing"
D18-1186,N15-1092,0,0.0149926,"ers and sizes, CIN can capture more complicated interaction patterns. Experiments on three very large datasets demonstrate CIN’s efficacy. 1 Introduction Natural language inference (NLI) is a pivotal and challenging natural language processing (NLP) task. The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothesis. Generally, NLI is also related to many other NLP tasks under the paradigm of semantic matching of two texts, such as question answering Hu et al. (2014); Wan et al. (2016) and information retrieval Liu et al. (2015), and so on. An essential challenge is to capture the semantic relevance of two sentences. Due to the semantic gap (or lexical chasm) problem, natural language inference is still a challenging problem. Recently, deep learning is raising a substantial interest in natural language inference and has achieved some great progresses Hu et al. (2014); Parikh et al. (2016); Chen et al. (2017a). To model the complicated semantic relationship between two sentences, previous models heavily utilize various attention mechanism Bahdanau et al. ∗ † (2014); Vaswani et al. (2017) to build the interaction at di"
D18-1186,W17-5307,0,0.259068,"ing hypothesis. Generally, NLI is also related to many other NLP tasks under the paradigm of semantic matching of two texts, such as question answering Hu et al. (2014); Wan et al. (2016) and information retrieval Liu et al. (2015), and so on. An essential challenge is to capture the semantic relevance of two sentences. Due to the semantic gap (or lexical chasm) problem, natural language inference is still a challenging problem. Recently, deep learning is raising a substantial interest in natural language inference and has achieved some great progresses Hu et al. (2014); Parikh et al. (2016); Chen et al. (2017a). To model the complicated semantic relationship between two sentences, previous models heavily utilize various attention mechanism Bahdanau et al. ∗ † (2014); Vaswani et al. (2017) to build the interaction at different granularity (word, phrase and sentence level), such as ABCNN Yin et al. (2016), Attention LSTM Rockt¨aschel et al. (2015), bidirectional attention LSTM Chen et al. (2017a), and so on. While attention is very successful in natural language inference, its mechanism is quite simple and can be regarded as a weighted sum of the target vectors. This paradigm results in a lack of fl"
D18-1186,E17-1002,0,0.0301943,"Missing"
D18-1186,D16-1244,0,0.0609064,"Missing"
D18-1186,D14-1162,0,0.080918,"layer or another stacked interaction layer. The interaction layers can be stacked for Nx times to capture the complicated matching information. 4.3 SNLI MultiNLI1 MultiNLI2 Quora J (θ) = − 1X log p(t(i) |x(i) , y (i) )+λ||θ||22 , (32) N i where θ represents all the connection weights. We use the Adam optimizer Kingma and Ba (2014) with an initial learning rate of 0.0004. Default L2 regularization λ is set to 10−6 . To avoid overfitting, dropout is applied after each fully connected, recurrent or convolutional layer. Initialization We take advantage of pre-trained word embeddings such as Glove Pennington et al. (2014) to transfer more knowledge from vast unlabeled data. For the words that don’t appear in Glove, we randomly initialize their embeddings from a normal distribution with mean 0.0 and standard deviation 0.1. The network weights are initialized with Xavier normalization Glorot and Bengio (2010) to maintain the variance of activations throughout the forward and backward passes. Biases are uniformly set to zero when the network is constructed. 5.1 Datasets To make quantitative evaluation, our model was evaluated on three well known datasets: Stanford Natural Language Inference dataset (SNLI), MultiN"
D18-1186,C16-1270,0,0.0387673,"Missing"
D18-1186,N16-1170,0,0.0201411,"ion exists between phrases {playing a violin vs. playing an instrument}, instead of the same words. The interaction layer connects playing in Premise to Hypothesis instrument, and connects playing in Hypothesis to Premise violin. Thus, the correlation between instrument in Hypothesis and violin in Premise are boosted, as we know these are important to reasoning. 6 efiting from the development of deep learning and the availability of large-scale annotated datasets, deep neural models have achieved great success. Rockt¨aschel et al. (2015) firstly use LSTM with attention for text matching task. Wang and Jiang (2016) use word-by-word attention to exploit the word-level match. Parikh et al. (2016) propose a new framework to model the relationship between two sentences using interact-compare-aggregate architecture. Chen et al. (2017a) incorporates the chain LSTM and tree LSTM jointly. Zhiguo Wang (2017) use self-attention mechanism to capture contextual information from the whole sentence. Unlike the above models, we use an alternative model to capture the complicate interaction information of two sentences. Another thread of work is the idea of using one network to generate the parameters of another networ"
D18-1186,Q16-1019,0,0.0461876,"Missing"
D18-1186,C14-1220,0,0.106859,"first briefly introduce how the convolution works over text sequence, then describe the proposed model and its connection to attention model. 3.1 Convolution over Sequence Convolution is an effective operation in deep neural networks, which convolves the input with a set of filters to extract non-linear compositional features. Although originally designed for computer vision, convolutional models have subsequently shown to be effective for NLP and have achieved excellent performance in sentence modeling Kim (2014); Kalchbrenner et al. (2014), and other traditional NLP tasks Hu et al. (2014); Zeng et al. (2014); Gehring et al. (2017). Given a sentence representation X = d×m [x1 , x2 , · · · , xm ] ∈ R , a convolutional filter W (f ) ∈ Rd×kd , the convolution process is defined as where w, W1 , W2 and b are learnable parameters. 1577   x0i = f W (f ) [xi−[k/2] , · · · , xi+[k/2] ] + b(f ) , (8) where τ is the width of filter, FGN(·) is the filter generation network. A detailed implementation of FGN is presented in Section 3.4. Now we can convolve the two sentences with the generated filters. Convolutional Interaction Network Wx Filter Generation Network ¯ = f (W (f ) ⊗ X) ∈ Rd×m , X y Y¯ = f (Wx(f"
D19-1355,W16-5307,0,0.245323,"Missing"
D19-1355,D18-1170,0,0.796047,"ord expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task to a sequence labeling task, thus building a unified model for all polysemous words. However, neither of them can totally beat the best word expert supervised methods. More recently, Luo et al. (2018b) propose to leverage the gloss information from WordNet and model the semantic relationship between the context and gloss in an improved memory network. Similarly, Luo et al. (2018a) introduce a (hierarchical) co-attention mechanism to generate co-dependent representations for the context and gloss. Their attempts prove that incorporating gloss knowledge into supervised WSD approach is helpful, but they still have not achieved much improvement, because they may not make full use of gloss knowledge. In this paper, we focus on how to better leverage gloss information in a supervised neural WSD"
D19-1355,P18-1230,0,0.352773,"Missing"
D19-1355,Q14-1019,0,0.507136,"a particular context (Navigli, 2009). Previous WSD approaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all"
D19-1355,C14-1151,0,0.604972,"mental task and long-standing challenge in Natural Language Processing (NLP), which aims to find the exact sense of an ambiguous word in a particular context (Navigli, 2009). Previous WSD approaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing wit"
D19-1355,P16-1085,0,0.402809,"optimal settings for our experiments. We keep the dropout probability at 0.1, set the number of epochs to 4. The initial learning rate is 2e-5, and the batch size is 64. 3.3 GlossBERT(Sent-CLS-WS) We use contextgloss pairs with weak supervision as input. We take the final hidden state of the first token [CLS] and add a classification layer (label ∈ {yes, no}), which weekly highlight the target word by the weak supervision. 3 3.1 Experiments Datasets The statistics of the WSD datasets are shown in Table 2. Training Dataset Following previous work (Luo et al., 2018a,b; Raganato et al., 2017a,b; Iacobacci et al., 2016; Zhong and Ng, 2010), we Settings Results Table 3 shows the performance of our method on the English all-words WSD benchmark datasets. We compare our approach with previous methods. The first block shows the MFS baseline, which selects the most frequent sense in the training corpus for each target word. The second block shows two knowledge-based systems. Leskext+emb (Basile et al., 2014) is a variant of Lesk algorithm (Lesk, 1986) by calculating the gloss-context overlap of the target word. Babelfy (Moro et al., 2014) is a unified graphbased approach which exploits the semantic network struct"
D19-1355,N18-1202,0,0.03381,"l the semantic relationship between the context and gloss in an improved memory network. Similarly, Luo et al. (2018a) introduce a (hierarchical) co-attention mechanism to generate co-dependent representations for the context and gloss. Their attempts prove that incorporating gloss knowledge into supervised WSD approach is helpful, but they still have not achieved much improvement, because they may not make full use of gloss knowledge. In this paper, we focus on how to better leverage gloss information in a supervised neural WSD system. Recently, the pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), have shown their effectiveness to alleviate the effort of feature engineering. Especially, BERT has achieved excellent results in question answering (QA) and natural language inference (NLI). We construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word, thus treating WSD task as a sentence-pair classification problem. We fine3509 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3509–3514, c Hong Kong, Chi"
D19-1355,D17-1120,0,0.545467,"unt in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task to a sequence labeling task, thus building a unified model for all polysemous words. However, neither of them can totally beat the best word expert supervised methods. More recently, Luo et al. (2018b) propose to leverage the gloss information from WordNet and model the semantic relationship between the context and gloss in an impr"
D19-1355,E17-1010,0,0.562265,"unt in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task to a sequence labeling task, thus building a unified model for all polysemous words. However, neither of them can totally beat the best word expert supervised methods. More recently, Luo et al. (2018b) propose to leverage the gloss information from WordNet and model the semantic relationship between the context and gloss in an impr"
D19-1355,S13-1003,0,0.0758858,"d into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task"
D19-1355,N19-1035,1,0.758242,"air classification problem. We describe our construction method with an example (See Table 1). There are four targets in this sentence, and here we take target word research as an example: Context-Gloss Pairs The sentence containing target words is denoted as context sentence. For each target word, we extract glosses of all N possible senses (here N = 4) of the target word (research) in WordNet to obtain the gloss sentence. [CLS] and [SEP] marks are added to the context-gloss pairs to make it suitable for the input of BERT model. A similar idea is also used in aspect-based sentiment analysis (Sun et al., 2019). Context-Gloss Pairs with Weak Supervision Based on the previous construction method, we add weak supervised signals to the context-gloss 3510 Dataset SemCor SE2 SE3 SE07 SE13 SE15 Total 226036 2282 1850 455 1644 1022 Noun 87002 1066 900 159 1644 531 Verb 88334 517 588 296 0 251 Adj 31753 445 350 0 0 160 Adv 18947 254 12 0 0 80 choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD. Table 2: Statistics of the different parts of speech annotations in English all-words WSD datasets. pairs (see the highlighted part in Table 1). The signal i"
D19-1355,P10-4014,0,0.730179,"oaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a"
D19-1355,J14-1003,0,\N,Missing
D19-5410,D18-1443,0,0.026602,"xample, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ stru"
D19-5410,N18-1065,0,0.0832705,"Missing"
D19-5410,P17-1044,0,0.0353276,"ng of the characteristics for the datasets and the increasing development of above three learning methods. Architecture Designs Architecturally speaking, most of existing extractive summarization systems consists of three major modules: sentence encoder, document encoder and decoder. In this paper, our architectural choices vary with two types of document encoders: LSTM2 (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) while we keep the sentence encoder (convolutional neural networks) and decoder (sequence labeling) unchanged3 . The base 2 We use the implementation of He et al. (2017). Since they do not show significant influence on our explored experiments. 3 81 model in all experiments refers to Transformer equipped with sequence labelling. extracted, the representation of the sentence consists of two components: position representation4 , which indicates the position of the sentence in the document; content representation, which contains the semantic information of the sentence. Therefore, we define the position and content information of the sentence as constituent factors, aiming to explore how the selected sentences in the test set relate to the training set in terms"
D19-5410,N18-1150,0,0.271935,"riors residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from abov"
D19-5410,P18-1014,0,0.381999,"and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite"
D19-5410,D19-1327,0,0.230067,"al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al., 2019) makes a similar analysis on datasets biases and presents three factors † which matter for the text summarization task. 80 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 80–89 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Factors of Datasets Measures Model designs Constituent[4.1] Positional coverage rate [4.1.1] Content coverage rate [4.1.2] Architecture designs [6.2] Pre-trained strategies [6.2] Style [4.2] Density [4.2.1] Compression [4.2.2] Training schemas [6.1] Table 1: Organization structure of this paper: four measures pr"
D19-5410,P18-1063,0,0.309888,"typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in"
D19-5410,P16-1046,0,0.11915,"en a dataset D, different learning methods are trying to explain the data in diverse ways, which show different generalization behaviours. Existing learning methods for extractive summarization systems vary in architectures designs, pre-trained strategies and training schemas. Neural Extractive Summarization Recently, neural network-based models have achieved great success in extractive summarization. (Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu, 2019). Existing works on text summarization can roughly fall into one of three classes: exploring networks’ structures with suitable bias (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018); introducing new training schemas (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018) and incorporating large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). Instead of exploring the possibility for a new state-of-the-art along one of above three lines, in this paper, we aim to bridge the gap between the lack of understanding of the characteristics for the datasets and the increasing development of above three learning methods. Architecture Designs Architecturally speaking, most of existing"
D19-5410,N18-2097,0,0.106766,"Missing"
D19-5410,P18-1176,0,0.0310446,"e same dataset. So the results of compression in Table 4 are in line with our expectations, how the model represents long documents to get good performance in text summarization task remains a challenge (Celikyilmaz et al., 2018). Unlike the exploration of density, we attempt to understand how the model extracts sentences when faced with different compression samples. We utilize an attribution technique called Integrated Gradients (IG) (Sundararajan et al., 2017) to separate the position and content information of each sentence. The setting of input x and baseline x’ in this paper is close to Mudrakarta et al. (2018)8 , but it is worth noting that our base model adds positional embedding to each sentence, so input x and baseline x’ both have positional information. Table 5: Experiment aboutP DENSITY , Pct denotes the percentage of ψ(si , S) to si ∈D ψ(si , S). The first three sentences contain more salient information in samples with higher density. In order to comprehend this correlation, we conduct the following experiment. Given an article and summary pair, we assign a score ψ(si , S) to each sentence in article to indicate how much sailent information is contained in the sentence. ψ(si , S) = LCS(si ,"
D19-5410,K16-1028,0,0.0983355,"Missing"
D19-5410,P18-1061,0,0.359477,"nally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of exist"
D19-5410,N18-1158,0,0.292258,"nalysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct f"
D19-5410,N18-1202,0,0.207059,"ing significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al"
D19-5410,P17-1099,0,0.102021,"nnection between priors residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems"
D19-5410,P19-1100,1,0.717017,"l network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al., 2019) makes a similar analysis on datasets biases and presents three factors † which"
I11-1008,W03-1018,0,0.0306981,"l., 2001), semantic role labeling (Toutannova et al., 2005) and syntactic parsing (Finkel et al., 2008). CRFs outperform other models like Maximum Entropy Markov models (McCallum et al., 2000), because they overcome the problem of “label bias”. 65 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 65–74, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2001; Sutton and McCallum, 2006) and introduce the definition of some concepts and parameters. BFGS algorithm (Liu and Nocedal, 1989) and achieve better convergence than the method introduced by Kazama and Tsujii (2003). Stochastic gradient descent (SGD) methods are another kind of L1-regularized training methods. It is a very attractive framework for it often requires much less training time than the batch training algorithm in practice. Tsuruoka et al. (2009) presented a variant of SGD that can efficiently produce compact models with L1 regularization. The main idea is to keep track of the total penalty and the penalty each weight has applied, so that the penalization smooth away the noisy gradient. Although SGD method with cumulative penalty is very efficient, it sometimes fails to converge to the optimum"
I11-1008,P10-1052,0,0.0176354,"WL-QN algorithm and SGD algorithm on the same data sets. For the purpose of run-times comparison, we implemented all the algorithm in a quite similar way, especially in feature extraction and gradient computation. For example, we compute the forward/backward scores in logarithm domain instead of scaling 2 To ensure the objective function decrease a certain value and guarantee the convergence. 69 Table 1: Feature templates for Chinese word segmentation task. (1) ci−1 yi , ci yi , ci+1 yi (2) ci−1 ci yi , ci ci+1 yi , ci−1 ci+1 yi (3) yi−1 yi method, though the latter method was claimed faster (Lavergne et al., 2010). All experiments were performed on a server with Xeon 2.66GHz. 4.1 Chinese Word Segmentation Figure 1: Bakeoff 2005 Chinese word segmentation task: Objective function with fixed α. The first set of experiments used the Chinese word segmentation corpus from the Second International SIGHAN Bakeoff data sets (Emerson, 2005), provided by Peking University. The training data consists of 19,054 sentences, 1,109,947 Chinese words, 1,826,448 Chinese characters and the testing data consists of 1,944 sentences, 104,372 Chinese words, 172,733 Chinese characters. We separated 1,000 sentences from the tra"
I11-1008,I05-3017,0,0.0342785,"function decrease a certain value and guarantee the convergence. 69 Table 1: Feature templates for Chinese word segmentation task. (1) ci−1 yi , ci yi , ci+1 yi (2) ci−1 ci yi , ci ci+1 yi , ci−1 ci+1 yi (3) yi−1 yi method, though the latter method was claimed faster (Lavergne et al., 2010). All experiments were performed on a server with Xeon 2.66GHz. 4.1 Chinese Word Segmentation Figure 1: Bakeoff 2005 Chinese word segmentation task: Objective function with fixed α. The first set of experiments used the Chinese word segmentation corpus from the Second International SIGHAN Bakeoff data sets (Emerson, 2005), provided by Peking University. The training data consists of 19,054 sentences, 1,109,947 Chinese words, 1,826,448 Chinese characters and the testing data consists of 1,944 sentences, 104,372 Chinese words, 172,733 Chinese characters. We separated 1,000 sentences from the training data and use them as the heldout data. The test data was only used for the final accuracy report. The feature templates we used in this experiment were listed in Table 1, where ci denotes the ith Chinese character in an instance, yi denotes the ith label in the instance. Based on the work of Huang and Zhao (2007), i"
I11-1008,N04-1042,0,0.0447919,"very sparse, then the size of the model will be much smaller than that produced by L2 regularization. Compact models are more interpretable, generalizable and manageable, require less resources like memory and storage. It is very meaningful especially for the rapid development of mobile application nowadays, which suffer the scarcity of resources. In many NLP tasks, the feature sets can reach the magnitude of several million. Besides, L1 regularization method can implicitly perform the feature selection, and provide the result for further process such as iterative approach (Vail et al., 2007; Peng and McCallum, 2004). This task requires that we need to train the model as accurate as possible, as to converge to the optimum. The feature selection can be regarded as reliable and unbiased after such a process. Quasi-Newtion method was successfully and efficiently used in L1-regularized model by Andrew and Gao (2007). They presented an algorithm called Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), which is based on LSparse learning framework, which is very popular in the field of nature language processing recently due to the advantages of efficiency and generalizability, can be applied to Conditional Ran"
I11-1008,P08-1109,0,0.0317698,"with less training time for L1 regularization. 1 Xuanjing Huang Fudan University Shanghai, China xjhuang@fudan.edu.cn Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) are one of the most widely-used machine learning approach in the field of nature language processing, for their ability to handle large feature sets and structural dependency between output labels. The applications of CRFs cover a wide range of tasks such as part-of-speech (POS) tagging (Lafferty et al., 2001), semantic role labeling (Toutannova et al., 2005) and syntactic parsing (Finkel et al., 2008). CRFs outperform other models like Maximum Entropy Markov models (McCallum et al., 2000), because they overcome the problem of “label bias”. 65 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 65–74, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2001; Sutton and McCallum, 2006) and introduce the definition of some concepts and parameters. BFGS algorithm (Liu and Nocedal, 1989) and achieve better convergence than the method introduced by Kazama and Tsujii (2003). Stochastic gradient descent (SGD) methods are another kind of L1-regularized"
I11-1008,P07-1104,0,0.0252279,"probabilistic of labeling result for further use as pipeline or reranking. For all types of CRFs, the maximum-likelihood method can be applied for parameter estimation, which means training the model is done by maximizing the log-likelihood on the training data. To avoid overfitting the likelihood is often penalized with the regularization term. There were two common regularization methods named L1 and L2 regularization. L1 regularization, also called Laplace prior, penalizes the weight vector with its L1-norm. L2 regularization, also called Gaussian prior, uses L2-form. Based on the work of Gao et al. (2007), there is no significant difference between these two regularization methods in terms of accuracy. But L1 regularization has a major advantage that L1-regularized training can produce models, of which the feature weights can be very sparse, then the size of the model will be much smaller than that produced by L2 regularization. Compact models are more interpretable, generalizable and manageable, require less resources like memory and storage. It is very meaningful especially for the rapid development of mobile application nowadays, which suffer the scarcity of resources. In many NLP tasks, th"
I11-1008,P05-1073,0,0.0204701,"Missing"
I11-1008,P09-1054,0,0.452642,"Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 65–74, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2001; Sutton and McCallum, 2006) and introduce the definition of some concepts and parameters. BFGS algorithm (Liu and Nocedal, 1989) and achieve better convergence than the method introduced by Kazama and Tsujii (2003). Stochastic gradient descent (SGD) methods are another kind of L1-regularized training methods. It is a very attractive framework for it often requires much less training time than the batch training algorithm in practice. Tsuruoka et al. (2009) presented a variant of SGD that can efficiently produce compact models with L1 regularization. The main idea is to keep track of the total penalty and the penalty each weight has applied, so that the penalization smooth away the noisy gradient. Although SGD method with cumulative penalty is very efficient, it sometimes fails to converge to the optimum, because the training process is usually terminated at a certain number of iterations without explicit stop criteria as in quasi-Newton method. Another problem is that the training result of SGD method is very sensitive to the parameter settings"
I11-1008,I08-4010,0,0.0186301,"esulting model. The fourth column shows the F score of the Chinese word segment results, which is the harmonic mean of precision P (percentage of output Chinese words that exactly match the golden standard Chinese words) and recall R (percentage of golden standard Chinese words that returned by our system). In Table 3, the second column shows the number of passes performed in the training, in our method, this value includes the the number of 4.2 Name Entity Recognition The second set of experiments used the name entity recognition corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008), provided by Microsoft Research Asia. The training data consists of 23,182 sentences, 1,089,050 Chinese characters and the testing data consists of 4,636 sentences, 219,197 Chinese characters. We separated 1,000 sentences from the training data and use them as the heldout data. The training data is annotated with the “IOB” tags representing name entities including person, location and organization. The feature templates we used in this experiment were listed in Table 4. Notice we did not change the label representation made by the origin 71 Table 5: Fourth SIGHAN Bakeoff name entity recogniti"
K18-2026,L16-1680,0,0.119049,"Missing"
K18-2026,E17-1023,0,0.0462706,"Missing"
K18-2026,K17-3002,0,0.015968,"information. We also tried subword embeddings, but it mostly did not help. More precisely, the character-level features are treated as bag-of-characters. Similarly, we use bag-of-morphology for morphological features (one can see number=single as a character). We first assign the embedding vectors for characters and morphological features, and then for each word, we apply a Convolutional Network (CNN) to encode variable length embeddings into one fixed length feature. Biaffine BiLSTM. Similar to Shi et al. (2017); Sato et al. (2017); Vania et al. (2017), we use last year’s first-place model (Dozat et al., 2017), the graph-based biaffine bizLSTM model as our backbone. Given a sentence of N words, the input is first fed to a bi-directional LSTM and obtain the feature of each word wi . A head MLP and a dependent MLP are used to translate the features, which is then fed into a hidden layer to calculate the biaffine attention. Finally, we are able to compute the score of arcs and labels in following way: Table 1: Grouping languages according to typology. 3 we take the features column of the UD data as the morphological features, which includes case, number, tense, mood and so on. See http: //universaldep"
K18-2026,K17-3009,0,0.0989329,"Missing"
K18-2026,P15-2139,0,0.0267164,"also makes use of the information from lemmas, POS taggings and dependency relationships through a group of embeddings precomputed by word2vec. In the later discussion, we take the baseline performance result from the web page of the shared task 2 for comparison. Introduction Dependency Parsing has been a fundamental task in Natural Language Processing (NLP). Recently, universal dependency parsing (Zeman et al., 2018a,b; Nivre et al., 2018) has unified the annotations of different languages and thus made transfer learning among languages possible. Several works using cross-lingual embedding (Duong et al., 2015; Guo et al., 2015) have successfully increased the accuracy of cross-lingual parsing. Beyond embedding-based methods, a natural question is whether we can use a simple way to utilize the universal information. Some previous research either regarded the universal information as extra training signals (e.g., delexicalized embedding (Dehouck and Denis, 2017)), or implicitly trained a network with all features (e.g., adversarial training for parsing in Sato et al. (2017)). In our system, we manually and explicitly share the universal annotations via a shared LSTM component. 3 System Description I"
K18-2026,K17-3010,0,0.0140631,"lexical information, and use morphological features3 and POS tags as the delexicalized information. We also tried subword embeddings, but it mostly did not help. More precisely, the character-level features are treated as bag-of-characters. Similarly, we use bag-of-morphology for morphological features (one can see number=single as a character). We first assign the embedding vectors for characters and morphological features, and then for each word, we apply a Convolutional Network (CNN) to encode variable length embeddings into one fixed length feature. Biaffine BiLSTM. Similar to Shi et al. (2017); Sato et al. (2017); Vania et al. (2017), we use last year’s first-place model (Dozat et al., 2017), the graph-based biaffine bizLSTM model as our backbone. Given a sentence of N words, the input is first fed to a bi-directional LSTM and obtain the feature of each word wi . A head MLP and a dependent MLP are used to translate the features, which is then fed into a hidden layer to calculate the biaffine attention. Finally, we are able to compute the score of arcs and labels in following way: Table 1: Grouping languages according to typology. 3 we take the features column of the UD data as the"
K18-2026,P15-1119,0,0.0333671,"he information from lemmas, POS taggings and dependency relationships through a group of embeddings precomputed by word2vec. In the later discussion, we take the baseline performance result from the web page of the shared task 2 for comparison. Introduction Dependency Parsing has been a fundamental task in Natural Language Processing (NLP). Recently, universal dependency parsing (Zeman et al., 2018a,b; Nivre et al., 2018) has unified the annotations of different languages and thus made transfer learning among languages possible. Several works using cross-lingual embedding (Duong et al., 2015; Guo et al., 2015) have successfully increased the accuracy of cross-lingual parsing. Beyond embedding-based methods, a natural question is whether we can use a simple way to utilize the universal information. Some previous research either regarded the universal information as extra training signals (e.g., delexicalized embedding (Dehouck and Denis, 2017)), or implicitly trained a network with all features (e.g., adversarial training for parsing in Sato et al. (2017)). In our system, we manually and explicitly share the universal annotations via a shared LSTM component. 3 System Description In this submission,"
K18-2026,K18-2001,0,0.0323457,"Missing"
K18-2026,N06-2033,0,0.0518297,"translate the features, which is then fed into a hidden layer to calculate the biaffine attention. Finally, we are able to compute the score of arcs and labels in following way: Table 1: Grouping languages according to typology. 3 we take the features column of the UD data as the morphological features, which includes case, number, tense, mood and so on. See http: //universaldependencies.org/u/feat/ index.html for detailed information. 257 LSTM 1 g Shared LSTM LSTM 2 g Figure 1: An illustration of the joint training framework for two languages. 3.4 We follow the re-parsing method proposed in Sagae and Lavie (2006) to perform model ensemble. Suppose k parsing trees have been obtained, denoted by T1 , T2 , ...Tk , a new graph is constructed by setting the score of each edge to hhi = MLPhead (wi ) hdi = MLPdep (wi ) si = H h U1 hdi + H h u2 where U1 ∈ Rd×d and u2 ∈ Rd are trainable parameters. 3.2 S[u → v] = k X [u → v] ∈ Tk i=1 Joint Training This graph is feed to a MST algorithm to get the ensemble parsing tree Te . Then the relation label of edge [u → v] in Te is voted by all inputs Ti that contains edge [u → v]. For a joint training model of N languages, we have N +1 Biaffne Bi-LSTMs (called LSTMs), s"
K18-2026,K17-3007,0,0.0201368,"information, and use morphological features3 and POS tags as the delexicalized information. We also tried subword embeddings, but it mostly did not help. More precisely, the character-level features are treated as bag-of-characters. Similarly, we use bag-of-morphology for morphological features (one can see number=single as a character). We first assign the embedding vectors for characters and morphological features, and then for each word, we apply a Convolutional Network (CNN) to encode variable length embeddings into one fixed length feature. Biaffine BiLSTM. Similar to Shi et al. (2017); Sato et al. (2017); Vania et al. (2017), we use last year’s first-place model (Dozat et al., 2017), the graph-based biaffine bizLSTM model as our backbone. Given a sentence of N words, the input is first fed to a bi-directional LSTM and obtain the feature of each word wi . A head MLP and a dependent MLP are used to translate the features, which is then fed into a hidden layer to calculate the biaffine attention. Finally, we are able to compute the score of arcs and labels in following way: Table 1: Grouping languages according to typology. 3 we take the features column of the UD data as the morphological featur"
K18-2026,K17-3003,0,0.0143566,"ters as the lexical information, and use morphological features3 and POS tags as the delexicalized information. We also tried subword embeddings, but it mostly did not help. More precisely, the character-level features are treated as bag-of-characters. Similarly, we use bag-of-morphology for morphological features (one can see number=single as a character). We first assign the embedding vectors for characters and morphological features, and then for each word, we apply a Convolutional Network (CNN) to encode variable length embeddings into one fixed length feature. Biaffine BiLSTM. Similar to Shi et al. (2017); Sato et al. (2017); Vania et al. (2017), we use last year’s first-place model (Dozat et al., 2017), the graph-based biaffine bizLSTM model as our backbone. Given a sentence of N words, the input is first fed to a bi-directional LSTM and obtain the feature of each word wi . A head MLP and a dependent MLP are used to translate the features, which is then fed into a hidden layer to calculate the biaffine attention. Finally, we are able to compute the score of arcs and labels in following way: Table 1: Grouping languages according to typology. 3 we take the features column of the UD data as the"
N19-1035,S15-2082,0,0.544125,"Missing"
N19-1035,S14-2149,0,0.0692101,"scenario is similar to QA and NLI, and then the advantage of the pre-trained BERT model can be fully utilized. Our approach is not limited to TABSA, and this construction method can be used for other similar tasks. For ABSA, we can use the same approach to construct the auxiliary sentence with only aspects. In BERT-pair models, BERT-pair-QA-B and BERT-pair-NLI-B achieve better AUC values on sentiment classification, probably because of the modeling of label information. Table 4: Test set results for Semeval-2014 task 4 Subtask 3: Aspect Category Detection. We use the results reported in XRCE (Brun et al., 2014) and NRC-Canada (Kiritchenko et al., 2014). Models 4-way 3-way Binary XRCE NRC-Canada LSTM ATAE-LSTM 78.1 82.9 - 82.0 84.0 88.3 89.9 BERT-single BERT-pair-QA-M BERT-pair-NLI-M BERT-pair-QA-B BERT-pair-NLI-B 83.7 85.2 85.1 85.9 84.6 86.9 89.3 88.7 89.9 88.7 93.3 95.4 94.4 95.6 95.1 5 In this paper, we constructed an auxiliary sentence to transform (T)ABSA from a single sentence classification task to a sentence pair classification task. We fine-tuned the pre-trained BERT model on the sentence pair classification task and obtained the new state-of-the-art results. We compared the experimental re"
N19-1035,S14-2004,0,0.865319,"s received much attention not only in academia but also in industry, providing real-time feedback through online reviews on websites such as Amazon, which can take advantage of customers’ opinions on specific products or services. The underlying assumption of this task is that the entire text has an overall polarity. However, the users’ comments may contain different aspects, such as: “This book is a hardcover version, but the price is a bit high.” The polarity in ‘appearance’ is positive, and the polarity regarding ‘price’ is negative. Aspect-based sentiment analysis (ABSA) (Jo and Oh, 2011; Pontiki et al., 2014, 2015, 2016) aims to identify fine-grained polarity towards a specific aspect. This task allows users to evaluate aggregated sentiments for each aspect of a given product or service and gain a more granular understanding of their quality. ∗ Corresponding author. 380 Proceedings of NAACL-HLT 2019, pages 380–385 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics pre-trained BERT. In this paper, we investigate several methods of constructing an auxiliary sentence and transform (T)ABSA into a sentence-pair classification task. We fine-tune the pre-trai"
N19-1035,C16-1251,0,0.0323549,"r model with the following models: • LR (Saeidi et al., 2016): a logistic regression classifier with n-gram and pos-tag features. • LSTM-Final (Saeidi et al., 2016): a biLSTM model with the final state as a representation. • LSTM-Loc (Saeidi et al., 2016): a biLSTM model with the state associated with the target position as a representation. • LSTM+TA+SA (Ma et al., 2018): a biLSTM model which introduces complex target-level and sentence-level attention mechanisms. • SenticLSTM (Ma et al., 2018): an upgraded version of the LSTM+TA+SA model which introduces external information from SenticNet (Cambria et al., 2016). 3.4 • Dmu-Entnet (Liu et al., 2018): a bidirectional EntNet (Henaff et al., 2016) with external “memory chains” with a delayed memory update mechanism to track entities. Exp-II: ABSA The benchmarks for SemEval-2014 Task 4 are the two best performing systems in Pontiki et al. (2014) and ATAE-LSTM (Wang et al., 2016). When evaluating SemEval-2014 Task 4 subtask 3 and subtask 4, following Pontiki et al. (2014), we use Micro-F1 and accuracy respectively. During the evaluation of SentiHood, following Saeidi et al. (2016), we only consider the four most frequently seen aspects (general, price, tra"
N19-1035,C16-1146,0,0.437322,"ilConstruction of the auxiliary sentence For simplicity, we mainly describe our method with TABSA as an example. We consider the following four methods to convert the TABSA task into a sentence pair classification task: 1 Output S.P. S.P. {yes,no} {yes,no} Table 2: The construction methods. Due to limited space, we use the following abbreviations: S.P. for sentiment polarity, w/o for without, and w/ for with. TABSA In TABSA, a sentence s usually consists of a series of words: {w1 , · · · , wm }, and some of the words {wi1 , · · · , wik } are preidentified targets {t1 , · · · , tk }, following Saeidi et al. (2016), we set the task as a 3class classification problem: given the sentence s, a set of target entities T and a fixed aspect set A = {general, price, transitlocation, saf ety}, predict the sentiment polarity y ∈ {positive, negative, none} over the full set of the target-aspect pairs {(t, a) : t ∈ T, a ∈ A}. As we can see in Table 1, the gold standard polarity of (LOCATION2, price) is negative, while the polarity of (LOCATION1, price) is none. 2.2 Sentiment Positive None None None None Negative None Positive Table 1: An example of SentiHood dataset. Methodology 2.1 Aspect general price safety tran"
N19-1035,C16-1311,0,0.174485,"Missing"
N19-1035,S14-2076,0,0.610962,"refer to more than one object, and sentence-level tasks cannot handle sentences with multiple targets. Therefore, Saeidi et al. (2016) introduce the task of targeted aspect-based sentiment analysis (TABSA), which aims to identify fine-grained opinion polarity towards a specific aspect associated with a given target. The task can be divided into two steps: (1) the first step is to determine the aspects associated with each target; (2) the second step is to resolve the polarity of aspects to a given target. The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), and subsequent neural network-based methods (Nguyen and Shirai, 2015; Wang et al., 2016; Tang et al., 2015, 2016; Wang et al., 2017) achieved higher accuracy. Recently, Ma et al. (2018) incorporate useful commonsense knowledge into a deep neural network to further enhance the result of the model. Liu et al. (2018) optimize the memory network and apply it to their model to better capture linguistic structure. More recently, the pre-trained language models, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have shown their effectiveness to a"
N19-1035,D16-1021,0,0.357654,"Missing"
N19-1035,S14-2036,0,0.0639589,"but one comment may refer to more than one object, and sentence-level tasks cannot handle sentences with multiple targets. Therefore, Saeidi et al. (2016) introduce the task of targeted aspect-based sentiment analysis (TABSA), which aims to identify fine-grained opinion polarity towards a specific aspect associated with a given target. The task can be divided into two steps: (1) the first step is to determine the aspects associated with each target; (2) the second step is to resolve the polarity of aspects to a given target. The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), and subsequent neural network-based methods (Nguyen and Shirai, 2015; Wang et al., 2016; Tang et al., 2015, 2016; Wang et al., 2017) achieved higher accuracy. Recently, Ma et al. (2018) incorporate useful commonsense knowledge into a deep neural network to further enhance the result of the model. Liu et al. (2018) optimize the memory network and apply it to their model to better capture linguistic structure. More recently, the pre-trained language models, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have sho"
N19-1035,N18-2045,0,0.230399,"k can be divided into two steps: (1) the first step is to determine the aspects associated with each target; (2) the second step is to resolve the polarity of aspects to a given target. The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), and subsequent neural network-based methods (Nguyen and Shirai, 2015; Wang et al., 2016; Tang et al., 2015, 2016; Wang et al., 2017) achieved higher accuracy. Recently, Ma et al. (2018) incorporate useful commonsense knowledge into a deep neural network to further enhance the result of the model. Liu et al. (2018) optimize the memory network and apply it to their model to better capture linguistic structure. More recently, the pre-trained language models, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have shown their effectiveness to alleviate the effort of feature engineering. Especially, BERT has achieved excellent results in QA and NLI. However, there is not much improvement in (T)ABSA task with the direct use of the pretrained BERT model (see Table 3). We think this is due to the inappropriate use of the pre-trained BERT model. Since the inpu"
N19-1035,E17-1046,0,0.024385,"uce the task of targeted aspect-based sentiment analysis (TABSA), which aims to identify fine-grained opinion polarity towards a specific aspect associated with a given target. The task can be divided into two steps: (1) the first step is to determine the aspects associated with each target; (2) the second step is to resolve the polarity of aspects to a given target. The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), and subsequent neural network-based methods (Nguyen and Shirai, 2015; Wang et al., 2016; Tang et al., 2015, 2016; Wang et al., 2017) achieved higher accuracy. Recently, Ma et al. (2018) incorporate useful commonsense knowledge into a deep neural network to further enhance the result of the model. Liu et al. (2018) optimize the memory network and apply it to their model to better capture linguistic structure. More recently, the pre-trained language models, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have shown their effectiveness to alleviate the effort of feature engineering. Especially, BERT has achieved excellent results in QA and NLI. However, there is not much"
N19-1035,D15-1298,0,0.108062,"Missing"
N19-1035,D16-1058,0,0.206383,"Missing"
N19-1035,N18-1202,0,0.0638551,"n target. The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), and subsequent neural network-based methods (Nguyen and Shirai, 2015; Wang et al., 2016; Tang et al., 2015, 2016; Wang et al., 2017) achieved higher accuracy. Recently, Ma et al. (2018) incorporate useful commonsense knowledge into a deep neural network to further enhance the result of the model. Liu et al. (2018) optimize the memory network and apply it to their model to better capture linguistic structure. More recently, the pre-trained language models, such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have shown their effectiveness to alleviate the effort of feature engineering. Especially, BERT has achieved excellent results in QA and NLI. However, there is not much improvement in (T)ABSA task with the direct use of the pretrained BERT model (see Table 3). We think this is due to the inappropriate use of the pre-trained BERT model. Since the input representation of BERT can represent both a single text sentence and a pair of text sentences, we can convert (T)ABSA into a sentence-pair classification task and fine-tune the"
N19-1035,S16-1002,0,0.141388,"Missing"
N19-1133,D15-1075,0,0.0809655,"the Transformer paper (Vaswani et al., 2017), the maximum dependency path length of RNN and Transformer are O(n), O(1), respectively. Star-Transformer can pass the message from one node to another node via the relay node so that the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16† consists of 16 datasets, each of them has"
N19-1133,P16-1139,0,0.0131918,"ould be 4.5 times fast than the standard Transformer on average. 5.3 Natural Language Inference Natural Language Inference (NLI) asks the model to identify the semantic relationship between a premise sentence and a corresponding hypothesis sentence. In this paper, we use the Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) for evaluation. Since we want to study how the model encodes the sentence as a vector representation, we set Star-Transformer as a sentence vector-based model and compared it with sentence vector-based models. In this experiment, we follow the previous work (Bowman et al., 2016) to use concat(r1 , r2 , kr1 − r2 k, r1 − r2 ) as the classification feature. The r1 , r2 are representations of premise and hypothesis sentence, it is calculated by sT + max(HT ) which is same with the classification task. See Appendix for the detail of hyper-parameters. As shown in Tab-4, the Star-Transformer outperforms most typical baselines (DiSAN, SPINN) and achieves comparable results compared with the state-of-the-art model. Notably, our model beats standard Transformer by a large margin, which is easy to overfit although we have made a careful hyper-parameters’ searching for Transform"
N19-1133,D16-1053,0,0.0339991,"Missing"
N19-1133,D17-1070,0,0.0417947,"Missing"
N19-1133,P19-1285,0,0.119448,"the virtual relay node. Red edges and blue edges are ring and radical connections, respectively. Recently, the fully-connected attention-based models, like Transformer (Vaswani et al., 2017), become popular in natural language processing (NLP) applications, notably machine translation (Vaswani et al., 2017) and language modeling (Radford et al., 2018). Some recent work also suggest that Transformer can be an alternative to recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in many NLP tasks, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2018), Transformer-XL (Dai et al., 2019) and Universal Transformer (Dehghani et al., 2018). More specifically, there are two limitations of the Transformer. First, the computation and memCorresponding Author. work done at NYU Shanghai, now with AWS Shanghai AI Lab. ‡ h8 h3 h6 Introduction ∗ h1 h2 ory overhead of the Transformer are quadratic to the sequence length. This is especially problematic with long sentences. Transformer-XL (Dai et al., 2019) provides a solution which achieves the acceleration and performance improvement, but it is specifically designed for the language modeling task. Second, studies indicate that Transformer"
N19-1133,D17-1206,0,0.0235026,"tasets. 5 Experiments We evaluate Star-Transformer on one simulation task to probe its behavior when challenged with long-range dependency problem, and three real tasks (Text Classification, Natural Language Inference, and Sequence Labelling). All experiments are ran on a NVIDIA Titan X card. Datasets used in this paper are listed in the Tab-1. We use the Adam (Kingma and Ba, 2014) as our optimizer. On the real task, we set the embedding size to 300 and initialized with GloVe (Pennington et al., 2014). And the symbol “Ours + Char” means an additional character-level pre-trained embedding JMT (Hashimoto et al., 2017) is used. Therefore, the total size of embedding should be 400 which as a result of the concatenation of GloVe and JMT. We also fix the embedding layer of the Star-Transformer in all experiments. Since semi- or unsupervised model is also a feasible solution to improve the model in a parallel direction, such as the ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), we exclude these models in the comparison and focus on the relevant architectures. 5.1 Masked Summation In this section, we introduce a simulation task on the synthetic data to probe the efficiency and nonlocal/long-range dep"
N19-1133,P14-1062,0,0.0176737,"is task, 1 https://github.com/dmlc/dgl and https: //github.com/fastnlp/fastNLP we verify that both Transformer and StarTransformer are good at handling long-range dependencies compared to the LSTM and BiLSTM. 2 Related Work Recently, neural networks have proved very successful in learning text representation and have achieved state-of-the-art results in many different tasks. Modelling Local Compositionality A popular approach is to represent each word as a lowdimensional vector and then learn the local semantic composition functions over the given sentence structures. For example, Kim (2014); Kalchbrenner et al. (2014) used CNNs to capture the semantic representation of sentences, whereas Cho et al. (2014) used RNNs. These methods are biased for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a). Another class of improved"
N19-1133,D14-1181,0,0.00706571,"ncies. In this task, 1 https://github.com/dmlc/dgl and https: //github.com/fastnlp/fastNLP we verify that both Transformer and StarTransformer are good at handling long-range dependencies compared to the LSTM and BiLSTM. 2 Related Work Recently, neural networks have proved very successful in learning text representation and have achieved state-of-the-art results in many different tasks. Modelling Local Compositionality A popular approach is to represent each word as a lowdimensional vector and then learn the local semantic composition functions over the given sentence structures. For example, Kim (2014); Kalchbrenner et al. (2014) used CNNs to capture the semantic representation of sentences, whereas Cho et al. (2014) used RNNs. These methods are biased for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a"
N19-1133,Q16-1026,0,0.0739745,"Missing"
N19-1133,D15-1180,0,0.0386201,"Missing"
N19-1133,D15-1278,0,0.0412974,"Missing"
N19-1133,N18-1202,0,0.0153189,". We use the Adam (Kingma and Ba, 2014) as our optimizer. On the real task, we set the embedding size to 300 and initialized with GloVe (Pennington et al., 2014). And the symbol “Ours + Char” means an additional character-level pre-trained embedding JMT (Hashimoto et al., 2017) is used. Therefore, the total size of embedding should be 400 which as a result of the concatenation of GloVe and JMT. We also fix the embedding layer of the Star-Transformer in all experiments. Since semi- or unsupervised model is also a feasible solution to improve the model in a parallel direction, such as the ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), we exclude these models in the comparison and focus on the relevant architectures. 5.1 Masked Summation In this section, we introduce a simulation task on the synthetic data to probe the efficiency and nonlocal/long-range dependencies of LSTM, Transformer, and the Star-Transformer. As mentioned in (Vaswani et al., 2017), the maximum path length of long-range dependencies of LSTM and Transformer are O(n) and O(1), where n is the sequence length. The maximum dependency path length of Star-Transformer is O(1) with a constant two via the relay node. To validate the"
N19-1133,W12-4501,0,0.0543652,"Missing"
N19-1133,P17-1001,1,0.927262,"t the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16† consists of 16 datasets, each of them has 1400/200/400 samples in train/dev/test. tions are replaced with a “gather and dispatch” mechanism. As a result, we accelerate the Transformer 10 times on the simulation task and 4.5 times on real tasks. The model also preserv"
N19-1133,P16-1101,0,0.0496375,"Missing"
N19-1133,J93-2004,0,0.0685742,"m dependency path length of RNN and Transformer are O(n), O(1), respectively. Star-Transformer can pass the message from one node to another node via the relay node so that the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16† consists of 16 datasets, each of them has 1400/200/400 samples in train/dev/test. tions are replace"
N19-1133,P16-2022,0,0.0596285,"Missing"
N19-1133,D13-1170,0,0.051355,"h analysis of the path length of dependencies in these models. As discussed in the Transformer paper (Vaswani et al., 2017), the maximum dependency path length of RNN and Transformer are O(n), O(1), respectively. Star-Transformer can pass the message from one node to another node via the relay node so that the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension"
N19-1133,P15-1150,0,0.120509,"Missing"
N19-1133,Q16-1016,0,0.0310563,"Missing"
N19-1133,N16-1174,0,0.324121,"the given sentence structures. For example, Kim (2014); Kalchbrenner et al. (2014) used CNNs to capture the semantic representation of sentences, whereas Cho et al. (2014) used RNNs. These methods are biased for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a). Another class of improved methods augments neural networks with a re-reading ability or global state while processing each word (Cheng et al., 2016; Zhang et al., 2018). Modelling Non-Local Compositionality There are two kinds of methods to model the non-local semantic compositions in a text sequence directly. One class of models incorporate syntactic tree into the network structure for learning sentence representations (Tai et al., 2015; Zhu et al., 2015). Another type of models learns the dependencies between words based entirely on self-attention wit"
N19-1133,W17-5308,0,0.0338431,"Missing"
N19-1133,D14-1162,0,0.0876774,"t sequences. Besides the acceleration, the Star-Transformer achieves significant improvement on some modestly sized datasets. 5 Experiments We evaluate Star-Transformer on one simulation task to probe its behavior when challenged with long-range dependency problem, and three real tasks (Text Classification, Natural Language Inference, and Sequence Labelling). All experiments are ran on a NVIDIA Titan X card. Datasets used in this paper are listed in the Tab-1. We use the Adam (Kingma and Ba, 2014) as our optimizer. On the real task, we set the embedding size to 300 and initialized with GloVe (Pennington et al., 2014). And the symbol “Ours + Char” means an additional character-level pre-trained embedding JMT (Hashimoto et al., 2017) is used. Therefore, the total size of embedding should be 400 which as a result of the concatenation of GloVe and JMT. We also fix the embedding layer of the Star-Transformer in all experiments. Since semi- or unsupervised model is also a feasible solution to improve the model in a parallel direction, such as the ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), we exclude these models in the comparison and focus on the relevant architectures. 5.1 Masked Summation In t"
N19-1133,P18-1030,0,0.121072,"d for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a). Another class of improved methods augments neural networks with a re-reading ability or global state while processing each word (Cheng et al., 2016; Zhang et al., 2018). Modelling Non-Local Compositionality There are two kinds of methods to model the non-local semantic compositions in a text sequence directly. One class of models incorporate syntactic tree into the network structure for learning sentence representations (Tai et al., 2015; Zhu et al., 2015). Another type of models learns the dependencies between words based entirely on self-attention without any recurrent or convolutional layers, such as Transformer (Vaswani et al., 2017), which has achieved state-of-the-art results on a machine translation task. The success of Transformer has raised a large"
N19-1277,W16-1603,0,0.11003,"013a) and GloVe (Pennington et al., 2014), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). Compositionality is more critical for Chinese, since Chinese is a logographic writing system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. Thes"
N19-1277,W06-0115,0,0.0562965,"82.98 82.57 82.52 84.87 84.74 84.29 83.23 83.28 85.30 95.87 95.79 95.45 95.45 95.91 VCWE -CNN -LSTM 86.93 86.73 85.98 84.64 84.83 84.53 85.77 85.77 85.25 96.00 95.92 95.96 Model Table 3: Chinese NER and POS tagging results for different pretrained embeddings. The configurations are the same of the ones used in Table 1. Named Entity Recognition Task We evaluate our model on the named entity recognition task. We use an open source Chinese NER model to test our word embeddings on MSRA dataset14 . MSRA is a dataset for simplified Chinese NER. It comes from SIGHAN 2006 shared task for Chinese NER (Levow, 2006). We pretrain word embeddings from different models and feed them into the input layer as features. The key to the task is to extract named entities and their associated types. Better word embeddings could get a higher F1 score of NER. The results in Table 3 show that our model also outperforms baseline models in this task. The performance of CWE and GWE models are similar, both slightly lower than Skip-gram and CBOW models. The F1 score of the JWE model exceeds that of other baseline models and is similar to our model. When removing the CNN and image information, our LSTM with the self-attent"
N19-1277,P17-1188,0,0.394341,"information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is not shown to be better than word2vec model because it has little flexibility and fixed character features. Besides, most of these methods pay less attention to the non-compositionality. For example, the 2 the graphical component of Chinese, referring to https://en.wikipedia.org/wiki/Radical_ (Chinese_characters) 3 the basic pattern of Chinese characters, referring to https://"
N19-1277,W13-3512,0,0.347996,"ssing (NLP) tasks. Most of these NLP tasks also benefit from the pre-trained word embeddings, such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). Compositionality is more critical for Chinese, since Chinese is a logographic writing system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on"
N19-1277,D14-1162,0,0.093764,"ionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks: word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.1 1 Introduction Distributed representations of words, namely word embeddings, encode both semantic and syntactic information into a dense vector. Currently, word embeddings have been playing a pivotal role in many natural language processing (NLP) tasks. Most of these NLP tasks also benefit from the pre-trained word embeddings, such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wie"
N19-1277,C14-1015,0,0.016538,"nal probability estimation (Mikolov et al., 2013a). A different way to learn word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014), which have been shown to be intrinsically linked to Skip-gram and negative sampling (Levy and Goldberg, 2014). The models mentioned above are popular and useful, but they regard individual words as atomic tokens, and the potentially useful internal structured information of words is ignored. To improve the performance of word embedding, sub-word information has been employed (Luong et al., 2013; Qiu et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). These methods focus on alphabetic writing systems, but they are not directly applicable to logographic writing systems. For the alphabetic writing systems, research on Chinese word embedding has gradually emerged. These methods focus on the discovery of making full use of sub-word information. Chen et al. (2015) design a CWE model for jointly learning Chinese characters and word embeddings. Based on the CWE model, Yin et al. (2016) present a multi-granularity embedding (MGE) model, additionally using the em"
N19-1277,P15-2098,0,0.163698,"ng system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. Howev"
N19-1277,D17-1025,0,0.76979,"ethods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is not shown to be better than word2vec model because it has little flexibility and fixed character features. Besides, most of these methods pay less attention to the non-compositionality. For example, the 2 the graphical component of Chinese, referring to https://en.wikipedia.org/wiki/Radical_ (Chinese_characters) 3 the basic pattern of Chinese characters, referring to https://en.wikipedia.org/wiki/Stroke_ (CJKV_character) 2710 Proceedings of NAACL-HLT 2019, pages 2710–2719 c Minneapolis,"
N19-1277,D16-1157,0,0.123752,"14), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). Compositionality is more critical for Chinese, since Chinese is a logographic writing system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kin"
N19-1277,N16-1119,0,0.0580809,"ojanowski et al., 2016). These methods focus on alphabetic writing systems, but they are not directly applicable to logographic writing systems. For the alphabetic writing systems, research on Chinese word embedding has gradually emerged. These methods focus on the discovery of making full use of sub-word information. Chen et al. (2015) design a CWE model for jointly learning Chinese characters and word embeddings. Based on the CWE model, Yin et al. (2016) present a multi-granularity embedding (MGE) model, additionally using the embeddings associated with radicals detected in the target word. Xu et al. (2016) propose a similarity-based characterenhanced word embedding (SCWE) model, exploiting the similarity between a word and its component characters with the semantic knowledge obtained from other languages. Shi et al. (2015) utilize radical information to improve Chinese word embeddings. Yu et al. (2017) introduce a joint learning word embedding (JWE) model and Cao et al. (2018) represent Chinese words as sequences of strokes and learn word embeddings with stroke n-grams information. From another perspective, Liu et al. (2017) provide a new way to automatically extract characterlevel features, cr"
N19-1277,D16-1100,0,0.0856302,"ese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is"
N19-1277,D17-1027,0,0.593563,"ically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is not shown to be b"
P11-2105,P09-2042,1,0.678348,"Missing"
P13-2077,P11-1066,0,0.0275921,"atures, or ﬁnding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used Introduction Community-based (or collaborative) question answering(CQA) such as Yahoo! Answers1 and Baidu Zhidao2 has become a popular online service in recent years. Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users. However, with the increase of the CQA archive, there accumulate massive duplicate questions on CQA websites. One of the primar"
P13-2077,P13-4009,1,0.798255,"m . A:B √ A:B= √ A:A× B :B (8) While given a new question both with title and content parts, MContent is not a zero matrix and could be also employed in the question retrieval process. A simple strategy is to sum up the scores of two parts. 5 Experiments 5.1 Datasets We collected the resolved CQA triples from the “computer” category of Yahoo! Answers and Baidu Zhidao websites. We just selected the resolved questions that already have been given their best answers. The CQA triples are preprocessed with stopwords removal (Chinese sentences are segmented into words in advance by FudanNLP toolkit(Qiu et al., 2013)). In order to evaluate our retrieval system, we divide our dataset into two parts. The ﬁrst part is used as training dataset; the rest is used as test dataset for evaluation. The datasets are shown in Table 2. Figure 2: 3-mode SVD of CQA tensor To deal with such a huge sparse data set, we use singular value decomposition (SVD) implemented in Apache Mahout3 machine learning library, which is implemented on top of Apache Hadoop4 using the map/reduce paradigm and scalable to reasonably large data sets. 3 4 http://mahout.apache.org/ http://hadoop.apache.org 437 DataSet Baidu Zhidao Yahoo! Answers"
P13-2077,P07-1059,0,0.0332427,"mainly focus on generating redundant features, or ﬁnding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used Introduction Community-based (or collaborative) question answering(CQA) such as Yahoo! Answers1 and Baidu Zhidao2 has become a popular online service in recent years. Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users. However, with the increase of the CQA archive, there accumulate massive duplicate questi"
P13-2077,P08-1019,0,\N,Missing
P13-4009,W09-1201,0,0.0699738,"ped mainly for English and not optimized for Chinese. In order to customize an optimized system for Chinese language process, we implement an open source toolkit, FudanNLP5 , which is written in Java. Since most of the state-of-theart methods for NLP are based on statistical learning, the whole framework of our toolkit is established around statistics-based methods, supplemented by some rule-based methods. Therefore, the quality of training data is crucial for our toolkit. However, we ﬁnd that there are some drawbacks in currently most commonly used corpora, such as CTB (Xia, 2000) and CoNLL (Hajič et al., 2009) corpora. For example, in CTB corpus, the set of POS tags is relative small and some categories are derived from the perspective of English grammar. And in CoNLL corpus, the head words are often interrogative particles and punctuations, which are unidiomatic in Chinese. These drawbacks bring more challenges to further analyses, such as information extraction and semantic understanding. Therefore, we ﬁrst construct a corpus with a modiﬁed guideline, which is more in accordance with the common understanding for Chinese grammar. In addition to the basic Chinese NLP tasks The growing need for Chin"
P13-4009,W04-3236,0,0.0704609,": 5 TIME: 7 → → PER LOC → 1 → 1980 8. CS:COO means the coordinate complex sentence. Table 1: Example of the output representation of our toolkit location, organization and other proper name. Conversely, we merge the “VC” and “VE” into “VV” since there is no link verb in Chinese. Finally, we use a tag set with 39 categories in total. Since a POS tag is assigned to each word, not to each character, Chinese POS tagging has two ways: pipeline method or joint method. Currently, the joint method is more popular and eﬀective because it uses more ﬂexible features and can reduce the error propagation (Ng and Low, 2004). In our system, we implement both methods for POS tagging. Besides, we also use some knowledge to improve the performance, such as Chinese surname and the common suﬃxes of the names of locations and organizations. 2.3.3 tively more important, we use language models to capture the internal structures. Second, we merge the continuous NEs with some rulebased strategies. For example, we combine the continuous words “人民/NN 大会堂/NN” into “ 人民大会堂/LOC”. 2.3.4 Dependency parsing Our syntactic parser is currently a dependency parser, which is implemented with the shift-reduce deterministic algorithm bas"
P13-4009,C04-1081,0,0.0463487,"s are trained on our developed corpus. We also develop a visualization module to displaying the output. Table 1 shows the output representation of our toolkit. (4) 2.3.1 Chinese Word Segmentation Diﬀerent from English, Chinese sentences are written in a continuous sequence of characters without explicit delimiters such as the blank space. Since the meanings of most Chinese characters are not complete, words are the basic syntactic and semantic units. Therefore, it is indispensable step to segment the sentence into words in Chinese language processing. We use character-based sequence labeling (Peng et al., 2004) to ﬁnd the boundaries of words. Besides the carefully chosen features, we also use the meaning of character drawn from HowNet(Dong and Dong, 2006), which improves the performance greatly. Since unknown words detection is still one of main challenges of Chinese word segmentation. We implement a constrained Viterbi algorithm to allow users to add their own word dictionary. 2.2.1 Training In the training stage, we use the passiveaggressive algorithm to learn the model parameters. Passive-aggressive (PA) algorithm (Crammer et al., 2006) was proposed for normal multi-class classiﬁcation and can be"
P13-4009,W03-3023,0,0.0336701,"implement both methods for POS tagging. Besides, we also use some knowledge to improve the performance, such as Chinese surname and the common suﬃxes of the names of locations and organizations. 2.3.3 tively more important, we use language models to capture the internal structures. Second, we merge the continuous NEs with some rulebased strategies. For example, we combine the continuous words “人民/NN 大会堂/NN” into “ 人民大会堂/LOC”. 2.3.4 Dependency parsing Our syntactic parser is currently a dependency parser, which is implemented with the shift-reduce deterministic algorithm based on the work in (Yamada and Matsumoto, 2003). The syntactic structure of Chinese is more complex than that of English, and semantic meaning is more dominant than syntax in Chinese sentences. So we select the dependency parser to avoid the minutiae in syntactic constituents and wish to pay more attention to the subsequent semantic analysis. Since the structure of the Chinese language is quite different from that of English, we use more eﬀective features according to the characteristics of Chinese sentences. The common used corpus for Chinese dependency parsing is CoNLL corpus (Hajič et al., 2009). However, there are some illogical cases"
P15-1112,P14-2131,0,0.0326639,"applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use"
P15-1112,P05-1022,0,0.309939,"yi } (9) d∈yˆi where κ is a discount parameter and d represents the nodes in trees. Given a set of training dependency parses D, the final training objective is to minimize the loss function J(Θ), plus a l2 -regulation term: J(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base par"
P15-1112,D14-1082,0,0.718651,"ign of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embedding"
P15-1112,C14-1078,0,0.0254698,"Missing"
P15-1112,J05-1003,0,0.0345127,". X ∆(yi , yˆi ) = κ1{d ∈ / yi } (9) d∈yˆi where κ is a discount parameter and d represents the nodes in trees. Given a set of training dependency parses D, the final training objective is to minimize the loss function J(Θ), plus a l2 -regulation term: J(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination o"
P15-1112,J03-4003,0,0.0595577,"s that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et al. (2003) applied recursive neural networks to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough information to make the correct parsing decisions. Menchetti et al. (2005) used RNNs to re-rank different constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003). 3 ··· (h,ci) d Distance Embedding ··· wh Word Embedding x 1 x2 ··· xK Phrase Representations of Children Figure 3: Architecture of a RCNN unit. Recursive Convolutional Neural Network The dependency grammar is a widely used syntactic structure, which directly reflects relationships among the words in a sentence. In a dependency tree, all nodes are terminal (words) and each node may have more than two children. Therefore, the standard RNN architecture is not suitable for dependency grammar since it is based on the binary tree. In this section, we propose a more general architecture, called rec"
P15-1112,Q13-1012,0,0.238233,"(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser. yˆi = arg max αst (xi , y, Θ) + (1 − α)sb (xi , y) y∈T (xi ) (14) where α ∈ [0, 1] is a hyperparameter; st (xi , y, Θ) and sb (xi , y) are the scores given by RCNN and the base parser respectively. To apply RCNN into"
P15-1112,P10-1110,0,0.21165,"Missing"
P15-1112,P12-1092,0,0.0322838,"could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These emb"
P15-1112,D14-1081,0,0.477752,"ifferences as follows. 1) They first summed up all child nodes into a dense vector vc and then composed subtree representation from vc and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pooling layer. 2) We represent the relative position of each child and its parent with distributed representation (position embeddings), which is very useful for convolutional layer. Figure 7 shows an example of DTRNN to illustrates how RCNN represents phrases as continuous vectors. Specific to the re-ranking model, Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Unlike IORNN, our proposed RCNN is a discriminative model and can optimize the re-ranking strategy for a particular base 1166 parse"
P15-1112,P05-1012,0,0.101813,"ork, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Convolution a bike,NN a,Det Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of"
P15-1112,W04-0308,0,0.0420055,"t with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Convolution a bike,NN a,Det Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Correspond"
P15-1112,P13-1045,0,0.449946,"nd these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree. In this work, we address the problem to rep1159 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resent all level nodes (words or phrases) with dense representations in a dependency tree. We"
P15-1112,D13-1170,0,0.432116,"nd these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree. In this work, we address the problem to rep1159 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resent all level nodes (words or phrases) with dense representations in a dependency tree. We"
P15-1112,P10-1040,0,0.111381,"data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. Howeve"
P15-1112,W03-3023,0,0.0642293,"iginal recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Convolution a bike,NN a,Det Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN F"
P15-1112,D08-1059,0,0.442944,"Missing"
P15-1112,P11-2033,0,0.154309,"Missing"
P15-1112,W09-3839,0,0.0238199,"s D, the final training objective is to minimize the loss function J(Θ), plus a l2 -regulation term: J(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser. yˆi = arg max αst (xi , y, Θ) + (1 − α)sb (xi , y) y∈T (xi ) (14) where α ∈ [0, 1] is a hyperparameter; st (xi , y"
P15-1112,Q14-1017,0,\N,Missing
P15-1168,W14-4012,0,0.0337609,"Missing"
P15-1168,D14-1179,0,0.0110592,"Missing"
P15-1168,D11-1090,0,0.0803899,"with them. This difference indicates that our model has well modeled the combinations of the characters and do not need much help of the feature engineering. The detailed results are shown in Table 4 (4-th and 6-th rows). Table 5 shows the comparisons of our model with the state-of-the-art systems on F-value. The model proposed by Zhang and Clark (2007) is a word-based segmentation method, which exploit features of complete words, while remains of the list are all character-based word segmenters, whose features are mostly extracted from the context characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as the unlabeled data or other knowledge. Although our model only uses simple bigram features, it outperforms the previous state-of-the-art methods which use more complex features. 1750 models P 92.8 93.7 96.0 (Zheng et al., 2013) (Pei et al., 2014) GRNN PKU R 92.0 93.4 95.7 F 92.4 93.5 95.9 MSRA R F 93.6 93.3 94.2 94.4 96.1 96.2 P 92.9 94.6 96.3 P 94.0* 94.4* 95.4 CTB6 R 93.1* 93.4* 95.2 F 93.6* 93.9* 95.3 Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings. models +Pre-train"
P15-1168,I05-3017,0,0.516571,"Missing"
P15-1168,I05-3027,0,0.383422,"Missing"
P15-1168,P14-1028,0,0.732501,"he links denote the information flow, where the solid edges denote the acceptation of the combinations while the dashed edges means rejection of that. As shown in the right figure, we receive a score vector for tagging target character “地” by incorporating all the combination information. Introduction ∗ 雨 Rainy Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many methods (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014) applied the neural network to Chinese word segmentation and achieved a performance that approaches the state-of-the-art methods. However, these neural models just concatenate the embeddings of the context characters, and feed them into neural network. Since the concatenation operation is relatively simple, it is difficult to model the complicated features as the traditional discrete feature based models. Although the complicated interactions of inputs can be modeled by the deep neural network, the previous neural model shows that the deep model cannot outperform the one with"
P15-1168,O03-4002,0,0.922691,"Missing"
P15-1168,P13-4009,1,0.863934,"Missing"
P15-1168,P12-1083,0,0.0153951,"Missing"
P15-1168,P07-1106,0,0.77742,"ly use the simply bigram feature embeddings initialized by the average of two embeddings of consecutive characters element-wisely. Although the model of Pei et al. (2014) greatly benefits from the bigram feature embeddings, our model just obtains a small improvement with them. This difference indicates that our model has well modeled the combinations of the characters and do not need much help of the feature engineering. The detailed results are shown in Table 4 (4-th and 6-th rows). Table 5 shows the comparisons of our model with the state-of-the-art systems on F-value. The model proposed by Zhang and Clark (2007) is a word-based segmentation method, which exploit features of complete words, while remains of the list are all character-based word segmenters, whose features are mostly extracted from the context characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as the unlabeled data or other knowledge. Although our model only uses simple bigram features, it outperforms the previous state-of-the-art methods which use more complex features. 1750 models P 92.8 93.7 96.0 (Zheng et al., 2013) (Pei et al., 2014) GRNN PKU R 92.0 93"
P15-1168,D13-1031,0,0.108722,"rence indicates that our model has well modeled the combinations of the characters and do not need much help of the feature engineering. The detailed results are shown in Table 4 (4-th and 6-th rows). Table 5 shows the comparisons of our model with the state-of-the-art systems on F-value. The model proposed by Zhang and Clark (2007) is a word-based segmentation method, which exploit features of complete words, while remains of the list are all character-based word segmenters, whose features are mostly extracted from the context characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as the unlabeled data or other knowledge. Although our model only uses simple bigram features, it outperforms the previous state-of-the-art methods which use more complex features. 1750 models P 92.8 93.7 96.0 (Zheng et al., 2013) (Pei et al., 2014) GRNN PKU R 92.0 93.4 95.7 F 92.4 93.5 95.9 MSRA R F 93.6 93.3 94.2 94.4 96.1 96.2 P 92.9 94.6 96.3 P 94.0* 94.4* 95.4 CTB6 R 93.1* 93.4* 95.2 F 93.6* 93.9* 95.3 Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings. models +Pre-train (Zheng et al., 2013) (Pe"
P15-1168,D13-1061,0,0.783938,"ons. Specifically, the links denote the information flow, where the solid edges denote the acceptation of the combinations while the dashed edges means rejection of that. As shown in the right figure, we receive a score vector for tagging target character “地” by incorporating all the combination information. Introduction ∗ 雨 Rainy Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many methods (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014) applied the neural network to Chinese word segmentation and achieved a performance that approaches the state-of-the-art methods. However, these neural models just concatenate the embeddings of the context characters, and feed them into neural network. Since the concatenation operation is relatively simple, it is difficult to model the complicated features as the traditional discrete feature based models. Although the complicated interactions of inputs can be modeled by the deep neural network, the previous neural model shows that the deep model cannot outpe"
P15-1168,P13-1045,0,0.181222,"ag j ∈ T (Collobert et al., 2011). 3 …… (1) where W1 ∈ RH2 ×H1 , b1 ∈ RH2 , hi ∈ RH2 . H2 is the number of hidden units in Layer 2. Here, w, H1 and H2 are hyper-parameters chosen on development set. Then, a similar linear transformation is performed without non-linear function followed: f (t|ci−w1 :i+w2 ) = W2 hi + b2 , xi …… Concatenate …… hi = g(W1 ai + b1 ), E MB S Recursive Neural Network A recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a given structure(such as parsing tree) in topological order (Pollack, 1990; Socher et al., 2013a). In the simplest case, children nodes are combined into their parent node using a weight matrix W that is shared across the whole network, followed by a non-linear function g(·). Specifically, if hL and hR are d-dimensional vector representations of left and right children nodes respectively, 3.2 Gated Recursive Neural Network The RNN need a topological structure to model a sequence, such as a syntactic tree. In this paper, we use a directed acyclic graph (DAG), as showing in Figure 3, to model the combinations of the input characters, in which two consecutive nodes in the lower layer are c"
P16-1098,D15-1181,0,0.0282344,"nterest in text semantic matching and has achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). According to their interaction ways, previous models can be classified into three categories: Weak interaction Models Some early works focus on sentence level interactions, such as ARCI(Hu et al., 2014), CNTN(Qiu and Huang, 2015) ∗ Corresponding author Strong Interaction Models Some models build the interaction at different granularity (word, phrase and sentence level), such as ARC-II (Hu et al., 2014), MultiGranCNN (Yin and Sch¨utze, 2015), Multi-Perspective CNN (He et al., 2015), MV-LSTM (Wan et al., 2016), MatchPyramid (Pang et al., 2016). The final matching score depends on these different levels of interactions. In this paper, we adopt a deep fusion strategy to model the strong interactions of two sentences. Given two texts x1:m and y1:n , we define a matching vector hi,j to represent the interaction of the subsequences x1:i and y1:j . hi,j depends on the matching vectors hs,t on previous interactions 1 ≤ s &lt; i and 1 ≤ t &lt; j. Thus, text matching can be regarded as modelling the interaction of two texts in a recursive matching way. Following this idea, we propose d"
P16-1098,D15-1075,0,0.0196488,"-by-word Attention LSTMs: An improved strategy of attention LSTMs, which introduces word-by-word attention mechanism and is proposed by (Rockt¨aschel et al., 2015). k 100 Train 77.9 Test 75.1 100 83.7 80.9 −0.4 −0.2 0 0.2 0.4 −0.2 100 84.8 77.6 running 100 83.2 82.3 pet 100 83.7 83.5 being 100 85.2 84.6 0.6 0.8 A young family enjoys feeling ocean waves lap at their feet by another Experiment-I: Recognizing Textual Entailment Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) (Bowman et al., 2015). This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. Results Table 2 shows the evaluation results on SNLI. The 2nd column of the table gives the number of hidden states. From experimental results, we have several experimental findings. The results of DF-LSTMs outperform all the competitor models with the same number of hidden states whil"
P16-1098,D16-1053,0,0.0572037,"Missing"
P16-1098,D15-1280,1,0.535996,"ed, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. The update of each LSTM unit can be written precisely as (ht , ct ) = LSTM(ht−1 , ct−1 , xt ). (5) Here, the function LSTM(·, ·, ·) is a shorthand for Eq. (2-4). LSTM can map the input sequence of arbitrary length to a fixed-sized vector, and has been successfully applied to a wide range of NLP tasks, such as machine translation (Sutskever et al., 2014), language modelling (Sutskever et al., 2011), text matching (Rockt¨aschel et al., 2015) and text classification (Liu et al., 2015). 4 Deep Fusion LSTMs for Recursively Semantic Matching To deal with two sentences, one straightforward method is to model them with two separate LSTMs. However, this method is difficult to model local interactions of two sentences. Following the recursive matching strategy, we propose a neural model of deep fusion LSTMs (DF-LSTMs), which consists of two interdependent LSTMs to capture the inter- and intrainteractions between two sequences. Figure 2 gives an illustration of DF-LSTMs unit. To facilitate our model, we firstly give some definitions. Given two sequences X = x1 , x2 , · · · , xn an"
P16-1098,D14-1162,0,0.0877353,"Missing"
P16-1098,N15-1091,0,0.0620213,"Missing"
P16-1098,N16-1036,0,\N,Missing
P16-1140,W13-3520,0,0.0595384,"Missing"
P16-1140,P14-2133,0,0.0977274,"s achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? Can we explain the utility of context and form by analyzing neuron activation pattern? 2 Experiment Design To study the proposed questions above, we design four series of experiments to comprehensively compare context-based and character-based word representations on different languages, covering syntactic, morphological and semantic properties. The"
P16-1140,D15-1041,0,0.0268622,"odel? Does a model behave similarly towards phylogenetically-related languages? b) Is word form a more efficient predictor of a certain grammatical function than word context for specific languages? Introduction Word representation is a core issue in natural language processing. Context-based word representation, which is inspired by Harris (1954), has achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? C"
P16-1140,W09-0106,0,0.0107638,"oduction Word representation is a core issue in natural language processing. Context-based word representation, which is inspired by Harris (1954), has achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? Can we explain the utility of context and form by analyzing neuron activation pattern? 2 Experiment Design To study the proposed questions above, we design four series of experiments to compre"
P16-1140,J81-4005,0,0.784035,"Missing"
P16-1140,de-marneffe-etal-2014-universal,0,0.0338399,"Missing"
P16-1140,P15-2076,0,0.03866,"r word-based model captures both semantic information and syntactic information, although the latter is not displayed as explicitly as the former. utility of pure word form and novelly point out the cross-language differences in word representation, which have been overlooked by huge amount of monolingual/bilingual research on well-studied languages. 6 7 Related works There have been a lot of research on interpreting or relating word embedding with linguistic features. Yogatama et al. (2014) projects word embedding into a sparse vector. They found some linguistically interpretable dimensions. Faruqui and Dyer (2015) use linguistic features to build word vector. Their results show that these representation of word meaning can also achieve good performance in the analogy and similarity tasks. These work can be regarded as the foreshadowing of our experiment paradigm that mapping dense vector to a sparse linguistic property space. Besides, a lot of study focus on empirical comparison of different word embedding model. Melamud et al. (2016) investigates the influence of context type and vector dimension on word embedding. Their main finding is that concatenating two different types of embeddings can still im"
P16-1140,N16-1077,0,0.0151527,"nce a word embedding model? Does a model behave similarly towards phylogenetically-related languages? b) Is word form a more efficient predictor of a certain grammatical function than word context for specific languages? Introduction Word representation is a core issue in natural language processing. Context-based word representation, which is inspired by Harris (1954), has achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respon"
P16-1140,D15-1246,0,0.0819388,"Missing"
P16-1140,D15-1176,0,0.0323133,"Missing"
P16-1140,N16-1118,0,0.0491996,"ite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? Can we explain the utility of context and form by analyzing neuron activation pattern? 2 Experiment Design To study the proposed questions above, we design four series of experiments to comprehensively compare context-based and character-based word representations on different languages, covering syntactic, morphological and semantic properties. The basic paradigm is to decode interpretable linguistic f"
P16-1140,D15-1243,0,0.024935,"es, a lot of study focus on empirical comparison of different word embedding model. Melamud et al. (2016) investigates the influence of context type and vector dimension on word embedding. Their main finding is that concatenating two different types of embeddings can still improve performance even if the utility of dimensionality has run out. Andreas and Klein (2014) assess the potential syntactic information encoded in word embeddings by directly apply word embeddings to parser and they concluded that embeddings add redundant information to what the conventional parser has already extracted. Tsvetkov et al. (2015) propose a method to evaluate word embeddings through the alignment of distributional vectors and linguistic word vectors. However, the method still lacks a direct and comprehensive investigation of the utility of form, context and language typological diversity. This is exactly our novelty and contribution. It is worth noticing that K¨ohn (2015) evaluates multilingual word embedding and compares skip-gram, language model and other competitive embedding models. They show that dependencybased skip-gram embedding is effective, even at low dimension. Although K¨ohn (2015) work involves different"
P16-1163,D15-1262,0,0.816312,"on becomes much more challenging when such connectives are missing. In fact, such implicit discourse relations Intuitively, (good, wrong) and (good, ruined), seem to be the most informative word pairs, and it is likely that they will trigger a contrast relation. Therefore, we can see that another main disadvantage of using word pairs is the lack of contextual information, and using n-gram pairs will again suffer from data sparsity problem. Recently, the distributed word representations (Bengio et al., 2006; Mikolov et al., 2013) have shown an advantage when dealing with data sparsity problem (Braud and Denis, 2015), and many deep learning based models are generating substantial interests in text semantic matching and have achieved some significant progresses (Hu 1726 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1726–1735, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Psyllium's not a good crop. You get a rain at the wrong time and the crop is ruined. Bidirectional LSTM Pooling Layer MLP f Gated Relevance Network Bilinear Tensor Single Layer Network Figure 1: The processing framework of the proposed approach. et al.,"
P16-1163,Q15-1024,0,0.374564,"for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion relations. For each classifier, we use an equal number of positive and negative samples as training data, because each of the relations except Expansion is infrequent (Pitler et al., 2009) as what shows in Table 1. The negative samples were chosen randomly from training sections 2-20. 3.2 3.2.1 • Word+GRN: We use the gated relevance network proposed in this paper to capture the semantic interaction scores between every word embedding pair of the two text segments. The"
P16-1163,P13-2013,0,0.594359,"ng@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the following sentence pair with a casual relation as an example: Word pairs, which are one of the most easily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations"
P16-1163,W12-1614,0,0.186442,"an implicit marker. Since explicit relations are easy to identify (Pitler et al., 2008), existing methods achieved good performance on the relations with explicit maker. In recent years, researchers mainly focused on implicit relations. For easily comparing with other methods, in this work, we also use PDTB as the training and testing corpus. As we mentioned above, various approaches have been proposed to do the task. Pitler et al. (2009) proposed to train four binary classifiers using word pairs as well as other rich linguistic features to automatically identify the top-level PDTB relations. Park and Cardie (2012) achieved a higher performance by optimizing the feature set. McKeown and Biran (2013) aims at solving the data sparsity problem, and they extended the work of Pitler et al. (2009) by aggregating word pairs. Rutherford and Xue (2014) used Brown clusters and coreferential patterns as new features and improved the baseline a lot. Braud and Denis (2015) compared different word representations for implicit relation classification. The word pairs feature have been studied by all of the work above, showing its importance on discourse relation. We follow their work, and incorporate word embedding to"
P16-1163,P09-1077,0,0.173917,"e with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only w"
P16-1163,prasad-etal-2008-penn,0,0.92201,"ur model is trained end to end by BackPropagation and Adagrad. The main contribution of this paper can be summarized as follows: • We use word embeddings to replace the original words in the text segments to overcome data sparsity problem. In order to preserve the contextual information, we further encode the text segment to its positional representation through a recurrent neural network. • To deal with the semantic gap problem, we adopt a gated relevance network to capture the semantic interaction between the intermediate representations of the text segments. • Experimental results on PDTB (Prasad et al., 2008) show that the proposed method can achieve better performance in recognizing discourse level relations in all of the relations than the previous methods. 2 The Proposed Method The architecture of our proposed method is shown in figure 1. In the following of this section, we will illustrate the details of the proposed framework. 2.1 Embedding Layer To model the sentences with neural model, we firstly need to transform the one-hot representation of word into the distributed representation. All words of two text segments X and Y will be mapped into low dimensional vector representations, which ar"
P16-1163,E14-1068,0,0.467992,"e Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the fol"
P16-1163,N03-1030,0,0.090673,"on via a Deep Architecture with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held betw"
P16-1163,P10-1040,0,0.0338307,"d+NTN: We use the neural tensor defined in (8) to capture the semantic interaction scores between every word embedding pair, the rest of the method is the same as our proposed method. • LSTM+NTN: We use two single LSTM to generate the positional text segments representation. The rest of the method is the same as Word-NTN. • BLSTM+NTN: We use two single bidirectional LSTM to generate the positional text nw = 50 ρ = 0.01 m = 32 (p, q) = (3, 3) r=2 3.2.2 Parameter Setting For the initialization of the word embeddings used in our model, we use the 50-dimensional pre-trained embeddings provided by Turian et al. (2010), and the embeddings are fixed during training. We only preserve the top 10,000 words according to its frequency of occurrence in the training data, all the text segments are padded to have the same length of 50, the intermediate representations of LSTM are also set to 50. The other parameters are initialized by randomly sampling from uniform distribution in [-0.1,0.1]. For other hyperparameters of our proposed model, we take those hyperparameters that achieved best performance on the development set, and keep the same parameters for other competitors. The final hyper-parameters are show in Ta"
P16-1163,K15-2001,0,0.0985963,"ation could help to determine which part of the two sentence should be focused when identifying their relation. 4 Related Work Discourse relations, which link clauses in text, are used to represent the overall text structure. Many downstream NLP tasks such as text summarization, question answering, and textual entailment can benefit from the task. Along with the increasing requirement, many works have been constructed to automatically identify these relations from different aspects (Pitler et al., 2008; Pitler et al., 2009; Zhou et al., 2010; McKeown and Biran, 2013; Rutherford and Xue, 2014; Xue et al., 2015). For training and comparing the performance of different methods, the Penn Discourse Treebank (PDTB) 2.0, which is large annotated discourse corpuses, were released in 2008 (Prasad et al., 2008). The annotation methodology of it follows the lexically grounded, predicate-argument approach. In PDTB, the discourse relations were predefined by Webber (2004). PDTB-styled discourse relations hold in only a local contextual window, and these relations are organized hierarchically. Also, every relation in PDTB has either an explicit or an implicit marker. Since explicit relations are easy to identify"
P16-1163,C10-2172,0,0.697668,"τ for parameter θτ,i . 3 3.1 Experiment Dataset The dataset we used in this work is Penn Discourse Treebank 2.0 (Prasad et al., 2008), which is one of the largest available annotated corpora of discourse relations. It contains 40,600 relations, which are manually annotated from the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. We follow the recommended section partition of PDTB 2.0, which is to use sections 2-20 for training, sections 21-22 for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion"
P16-1163,miltsakaki-etal-2004-penn,0,\N,Missing
P16-1163,C08-2022,0,\N,Missing
P16-1206,J96-1002,0,0.168189,"ntation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and feature engineering, Chinese word segmentation achieves quite high precision after years of intensive research. To evaluate a word segmenter, the standard metric consists of precision p, recall r, and an evenly-weighted F-score f1 . However, with the successive improvement of performance, state-of-the-art segmenters are hard to be dis"
P16-1206,W02-1001,0,0.0813381,"Missing"
P16-1206,I05-3017,0,0.214398,"e very skewed word distribution at different levels of difficulty1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefi"
P16-1206,N12-1016,0,0.05536,"Missing"
P16-1206,P13-1167,0,0.0503991,"Missing"
P16-1206,P15-1174,0,0.0621592,"Missing"
P16-1206,D15-1013,0,0.0452988,"Missing"
P16-1206,I08-4010,0,0.0249064,"ion at different levels of difficulty1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and"
P16-1206,H94-1018,0,0.356068,"6 shows that the performances of different models with our proposed evaluation metric are significantly correlated in two parallel tests. 0.6 5.4 Visualization of the Weight 0.4 0.3 0.4 0.5 0.6 0.7 fb on parallel test 1 Figure 6: Correlation between the evaluation results fb of two parallel testsets with the proposed metrics on a collection of models. The Pearson correlation is 0.9961, p = 0.000. 5.3 the proposed evaluation metric by comparing the evaluation results with human judgements. The evaluation results with our new metric correlated with human intuition well. Validity and Reliability Jones (1994) concluded some important criteria for the evaluation metrics of NLP system. It is very important to check the validity and reliability of a new metric. Previous section has displayed the validity of As is known, there might be some annotation inconsistency in the dataset. We find that most of the cases with high weight are really valuable difficult test cases, such as the visualized sentences from WB dataset in Figure 7. In the first sentence, the word ‘BMW 族’ (NOUN.People who take bus, metro and then walk to the destination) is an OOV word and contains English characters. The weight of this"
P16-1206,W06-0115,0,0.0338114,"ord distribution at different levels of difficulty1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the"
P16-1206,P02-1040,0,0.102989,"Missing"
P16-1206,C04-1081,0,0.0543738,"able scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and feature engineering, Chinese word segmentation achieves quite high precision after years of intensive research. To evaluate a word segmenter, the"
P16-1206,J02-1002,0,0.0540452,"Missing"
P16-1206,O03-4002,0,0.0971998,"distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and feature engineering, Chinese word segmentation achieves quite high precision after years of intensive research. To evaluate a"
P16-1206,W10-4126,0,\N,Missing
P17-1001,P07-1056,0,0.397818,"Missing"
P17-1001,P14-1062,0,0.00708944,"text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhub"
P17-1001,P16-1098,1,0.203704,"opose an adversarial multi-task framework, in which the shared and private feature spaces are inIntroduction Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information from multiple tasks. However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of 1 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1–10 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1001 forget gate ft , an output gate ot , a memory cell ct and a hidden state ht . d is the number of the LSTM units. The elements of the gating vectors it , ft and ot are in [0, 1]. The LSTM is precisely specified as follows. herently disjoint b"
P17-1001,D15-1280,1,0.52053,"urrent Models for Text Classification Text Classification with LSTM Given a text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. L"
P17-1001,D16-1012,1,0.450341,"Missing"
P17-1001,N15-1092,0,0.0605154,"urrent Models for Text Classification Text Classification with LSTM Given a text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. L"
P17-1001,P11-1015,0,0.0560746,"Missing"
P17-1001,P05-1015,0,0.33778,"Missing"
P17-1001,D14-1162,0,0.098307,"Missing"
P17-1001,D13-1170,0,0.00557314,"lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN)"
P17-1110,D16-1070,0,0.0244839,"for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria. Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features. Inspired by the success of adversarial strategy on domain adaption (Ajakan et al., 2014; Ganin et al., 2"
P17-1110,P09-1059,0,0.0124772,"tence “姚明进 入总决赛 (YaoMing reaches the final)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria"
P17-1110,I08-4010,0,0.329481,"ters following uniform distribution at (−0.05, 0.05). We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al., 2013). Following previous work (Chen et al., 2015b; Pei et al., 2014), all experiments including baseline results are using pre-trained character embedding with bigram feature. Datasets To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008). Table 2 gives the details of the eight datasets. Among these datasets, AS, CITYU and CKIP are traditional Chinese, while the 6.3 Overall Results Table 3 shows the experiment results of the proposed models on test sets of eight CWS datasets, which has three blocks. (1) In the first block, we can see that the performance is boosted by using Bi-LSTM, and the 1197 Sighan05 Datasets MSRA AS PKU Sighan08 CTB CKIP CITYU NCC SXU Train Test Train Test Train Test Train Test Train Test Train Test Train Test Train Test Words 2.4M 0.1M 5.4M 0.1M 1.1M 0.2M 0.6M 0.1M 0.7M 0.1M 1.1M 0.2M 0.5M 0.1M 0.5M 0.1M"
P17-1110,P15-1168,1,0.876477,"ayer Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b). Specifically, given a sequence with n characters X = {x1 , . . . , xn }, the aim of CWS task is to figure out the ground truth of labels Y ∗ = {y1∗ , . . . , yn∗ }: Y ∗ = arg max p(Y |X), Inference Layer (1) Y ∈Ln where L = {B, M, E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural network. In this In ne"
P17-1110,D15-1141,1,0.885499,"Missing"
P17-1110,P15-1172,0,0.0239503,"monly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from"
P17-1110,I05-3017,0,0.16536,"zation, we randomize all parameters following uniform distribution at (−0.05, 0.05). We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al., 2013). Following previous work (Chen et al., 2015b; Pei et al., 2014), all experiments including baseline results are using pre-trained character embedding with bigram feature. Datasets To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008). Table 2 gives the details of the eight datasets. Among these datasets, AS, CITYU and CKIP are traditional Chinese, while the 6.3 Overall Results Table 3 shows the experiment results of the proposed models on test sets of eight CWS datasets, which has three blocks. (1) In the first block, we can see that the performance is boosted by using Bi-LSTM, and the 1197 Sighan05 Datasets MSRA AS PKU Sighan08 CTB CKIP CITYU NCC SXU Train Test Train Test Train Test Train Test Train Test Train Test Train Test Train Test Words 2.4M 0.1M 5.4M 0.1M 1.1M 0.2M 0.6M 0.1M 0.7"
P17-1110,D16-1072,0,0.080826,"Missing"
P17-1110,D16-1012,1,0.0679631,"Missing"
P17-1110,P14-1028,0,0.139815,"S. 2.1 Embedding layer Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b). Specifically, given a sequence with n characters X = {x1 , . . . , xn }, the aim of CWS task is to figure out the ground truth of labels Y ∗ = {y1∗ , . . . , yn∗ }: Y ∗ = arg max p(Y |X), Inference Layer (1) Y ∈Ln where L = {B, M, E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural net"
P17-1110,D13-1062,1,0.685854,"nal)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by integrating share"
P17-1110,P12-1025,0,0.0454498,"Ming reaches the final)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by"
P17-1110,D13-1061,0,0.075779,"l architecture of CWS. 2.1 Embedding layer Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b). Specifically, given a sequence with n characters X = {x1 , . . . , xn }, the aim of CWS task is to figure out the ground truth of labels Y ∗ = {y1∗ , . . . , yn∗ }: Y ∗ = arg max p(Y |X), Inference Layer (1) Y ∈Ln where L = {B, M, E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or re"
P19-1100,P18-3015,0,0.612259,"ering problem? Understanding the above questions can not only help us to choose suitable architectures in different application scenarios, but motivate us to move forward to more powerful frameworks. External Transferable Knowledge and Learning schemas Clearly, the improvement in accuracy and performance is not merely because of the shift from feature engineering to structure engineering, but the flexible ways to incorporate external knowledge (Mikolov et al., 2013; Peters et al., 2018a; Devlin et al., 2018) and learning schemas to introduce extra instructive constraints (Paulus et al., 2017; Arumae and Liu, 2018). For this part, we make some first steps toward answers to the following questions: 1) Which type of pre-trained models (supervised or unsupervised pre-training) is more friendly to the summarization task? 2) When architectures are explored exhaustively, can we push the state-of-the-art results to a new level by introducing external transferable knowledge or changing another learning schema? To make a comprehensive study of above an1049 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1049–1058 c Florence, Italy, July 28 - August 2, 2019. 2019 Ass"
P19-1100,N18-1150,0,0.0446013,"ow neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally"
P19-1100,P18-1063,0,0.35113,"mprove current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com"
P19-1100,P16-1046,0,0.224131,"able knowledge influence extractive summarization and a more popular neural architecture, Transformer. Besides, we come to inconsistent conclusions when analyzing the auto-regressive decoder. More importantly, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optim"
P19-1100,D18-1409,0,0.714632,"Missing"
P19-1100,D18-1443,0,0.128821,"ks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/fastnlp/fastNLP † Archi"
P19-1100,N18-1065,0,0.119485,"Missing"
P19-1100,D17-1223,0,0.0153301,"y, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optimization goals. Although above work improves the performance of summarization system from different perspectives, yet a comprehensive study remains missing. 3 A Testbed for Text Summarization To analyze neu"
P19-1100,P18-1014,0,0.461649,"rization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can gener"
P19-1100,D18-1208,0,0.308389,"g a right direction. 2 Related Work The work is connected to the following threads of work of NLP research. Task-oriented Neural Networks Interpreting Without knowing the internal working mechanism of the neural network, it is easy for us to get into a hobble when the performance of a task has reached the bottleneck. More recently, Peters et al. (2018b) investigate how different learning frameworks influence the properties of learned contextualized representations. Different from this work, in this paper, we focus on dissecting the neural models for text summarization. A similar work to us is Kedzie et al. (2018), which studies how deep learning models perform context selection in terms of several typical summarization architectures, and domains. Compared with this work, we make a more comprehensive study and give more different analytic aspects. For example, we additionally investigate how transferable knowledge influence extractive summarization and a more popular neural architecture, Transformer. Besides, we come to inconsistent conclusions when analyzing the auto-regressive decoder. More importantly, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art"
P19-1100,D14-1181,0,0.0110666,"Missing"
P19-1100,P16-1098,1,0.838798,"te the LSTM-based structure and the Transformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentences from the original document based on contextualized representations: s1"
P19-1100,P17-1001,1,0.855402,"zed representations s1 , · · · , sn . To achieve this goal, we investigate the LSTM-based structure and the Transformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentence"
P19-1100,K16-1028,0,0.137143,"Missing"
P19-1100,N18-1158,0,0.164021,"s how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optimization goals. Although above work improves the performance of summarization system from different perspectives, yet a comprehensive study remains missing. 3 A Testbed for Text Summarization To analyze neural summarization syst"
P19-1100,D14-1162,0,0.0931397,"Enc. represent decoder and encoder respectively. Sup. denotes supervised learning and NEWS. means supervised pre-training knowledge. alytical perspectives, we first build a testbed for summarization system, in which training and testing environment will be constructed. In the training environment, we design different summarization models to analyze how they influence the performance. Specifically, these models differ in the types of architectures (Encoders: CNN, LSTM, Transformer (Vaswani et al., 2017); Decoders: auto-regressive2 , non auto-regressive), external transferable knowledge (GloVe (Pennington et al., 2014), BERT (Devlin et al., 2018), N EWSROOM (Grusky et al., 2018)) and different learning schemas (supervised learning and reinforcement learning). To peer into the internal working mechanism of above testing cases, we provide sufficient evaluation scenarios in the testing environment. Concretely, we present a multi-domain test, sentence shuffling test, and analyze models by different metrics: repetition, sentence length, and position bias, which we additionally developed to provide a better understanding of the characteristics of different datasets. Empirically, our main observations are summariz"
P19-1100,N18-1202,0,0.151942,"for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/fastnlp/fastNLP † Architectures Architecturally, the better performance usually comes at the cost of our understanding of the system. To date, we know little about the functionality of each neural component and the differences between them (Peters et al., 2018b), which raises the following typical questions: 1) How does the choice of different neural architectures (CNN, RNN, Transformer) influence the performance of the summarization system? 2) Which part of components matters for specific dataset? 3) Do current models suffer from the over-engineering problem? Understanding the above questions can not only help us to choose suitable architectures in different application scenarios, but motivate us to move forward to more powerful frameworks. External Transferable Knowledge and Learning schemas Clearly, the improvement in accuracy and performance is"
P19-1100,D15-1044,0,0.131669,"sformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentences from the original document based on contextualized representations: s1 , · · · , sn . Most existing architecture"
P19-1100,P17-1099,0,0.722919,"etter understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization sinc"
P19-1100,P18-1061,0,0.289883,"effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding autho"
P19-1100,W04-1013,0,\N,Missing
P19-1100,D18-1179,0,\N,Missing
P19-1601,P19-1285,0,0.0288274,"tence. Besides, without referring the original text, RNN-based decoder is also hard to preserve the content. The generation quality for long text is also uncontrollable. In this paper, we address the above concerns of disentangled models for style transfer. Different from them, we propose Style Transformer, which takes Transformer (Vaswani et al., 2017) as the basic block. Transformer is a fully-connected selfattention neural architecture, which has achieved many exciting results on natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), language modeling (Dai et al., 2019), text classification (Devlin et al., 2018). Different from RNNs, Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Moreover, Transformer decoder fetches the information from the encoder part via attention mechanism, compared to a fixed size vector used by RNNs. With the strong ability of Transformer, our model can transfer the style of a sentence while better preserving its meaning. The difference between our model and the previous model is shown in Figure 1. Our contributions are summarized as follows: • We introduce a novel trai"
P19-1601,D18-1002,0,0.0215771,". These methods focus on how to disentangle the content and style in the latent space. The latent representation needs better preserve the meaning of the text while reducing its stylistic properties. Due to lacking paired sentence, an adversarial loss (Goodfellow et al., 2014) is used in the latent space to discourage encoding style information in the latent representation. Although the disentangled latent representation brings better interpretability, in this paper, we address the following concerns for these models. 1) It is difficult to judge the quality of disentanglement. As reported in (Elazar and Goldberg, 2018; Lample et al., 2019), the style information can be still recovered from the latent representation even the model has trained adversarially. Therefore, it is not easy to disentangle the stylistic property from the semantics of a sentence. 2) Disentanglement is also unnecessary. Lample et al. (2019) reported that a good decoder can generate the text with the desired style from an entangled latent representation by “overwriting” the original style. 3) Due to the limited capacity of vector representation, the latent representation is hard to capture the rich semantic information, especially for"
P19-1601,W11-2123,0,0.036442,"4 N/A N/A N/A 18 ControlledGen (Hu et al., 2017) CrossAlignment (Shen et al., 2017) MultiDecoder (Fu et al., 2018) CycleRL(Xu et al., 2018) 88.9 76.3 49.9 88.0 14.3 4.3 9.2 2.8 45.7 13.2 37.9 7.2 201 90 127 204 93.9 N/A N/A 97.6 62.1 N/A N/A 4.9 58 N/A N/A 246 Ours (Conditional) Ours (Multi-Class) 93.6 87.6 17.1 20.3 45.3 54.9 78 50 86.8 79.7 66.2 70.5 38 29 Table 2: Automatic evaluation results on Yelp and IMDb datset respectively. Fluency Fluency is measured by the perplexity of the transferred sentence, and we trained a 5-gram language model on the training set of two datasets using KenLM (Heafield, 2011). 4.2.2 Human Evaluation Due to the lack of parallel data in style transfer area, automatic metrics are insufficient to evaluate the quality of the transferred sentence. Therefore we also conduct human evaluation experiments on two datasets. We randomly select 100 source sentences (50 for each sentiment) from each test set for human evaluation. For each review, one source input and three anonymous transferred samples are shown to a reviewer. And the reviewer is asked to choose the best sentence for style control, content preservation, and fluency respectively. • Which sentence has the most opp"
P19-1601,D19-1306,0,0.169694,"Missing"
P19-1601,E17-2068,0,0.0350339,"respectively. 4.2 Evaluation A goal transferred sentence should be a fluent, content-complete one with target style. To evaluate the performance of the different model, following previous works, we compared three different dimensions of generated samples: 1) Style control, 2) Content preservation and 3) Fluency. 4.2.1 Automatic Evaluation Style Control We measure style control automatically by evaluating the target sentiment accuracy of transferred sentences. For an accurate evaluation of style control, we trained two sentiment classifiers on the training set of Yelp and IMDb using fastText (Joulin et al., 2017). Content Preservation To measure content preservation, we calculate the BLEU score (Papineni et al., 2002) between the transferred sentence and its source input using NLTK. A higher BLEU score indicates the transferred sentence can achieve better content preservation by retaining more words from the source sentence. If a human reference is available, we will calculate the BLEU score between the transferred sentence and corresponding reference as well. Two BLEU score metrics are referred to as self -BLEU and ref -BLEU 6002 Yelp Model ACC IMDb ref -BLEU self -BLEU PPL ACC self -BLEU PPL Input C"
P19-1601,D18-1549,0,0.059816,"Missing"
P19-1601,N18-1169,0,0.257719,"ethods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture. Although, learning a latent representation will make the model more interpretable and easy to manipulate, the model which is assumed a fixed size latent representation cannot utilize the information from the source sentence anymore. On the other hand, there are also some approaches without manipulating latent representation are proposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles. However, both lines of the previous models make few attempts to utilize t"
P19-1601,D15-1166,0,0.0606196,"o unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles. However, both lines of the previous models make few attempts to utilize the attention mechanism to refer the long-term history or the source sentence, except Lample et al. (2019). In many NLP tasks, especially for text generation, attention mechanism has been proved to be an essential technique to enable the model to capture the longterm dependency (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017). In this paper, we follow the second line of work and propose a novel method which makes no assumption about the latent representation of source sentence and takes the proven self-attention network, Transformer, as a basic module to train a style transfer system. 5998 x z Encoder Decoder Finally, we will combine the Style Transformer network and discriminator network via an overall learning algorithm in section 3.5 to train our style transfer system. y s (a) Disentangled Style Transfer x Transformer y 3.3 s (b) Style Transformer Figure 1: General illustration of previou"
P19-1601,P11-1015,0,0.175537,"Missing"
P19-1601,P02-1040,0,0.10373,"arget style. To evaluate the performance of the different model, following previous works, we compared three different dimensions of generated samples: 1) Style control, 2) Content preservation and 3) Fluency. 4.2.1 Automatic Evaluation Style Control We measure style control automatically by evaluating the target sentiment accuracy of transferred sentences. For an accurate evaluation of style control, we trained two sentiment classifiers on the training set of Yelp and IMDb using fastText (Joulin et al., 2017). Content Preservation To measure content preservation, we calculate the BLEU score (Papineni et al., 2002) between the transferred sentence and its source input using NLTK. A higher BLEU score indicates the transferred sentence can achieve better content preservation by retaining more words from the source sentence. If a human reference is available, we will calculate the BLEU score between the transferred sentence and corresponding reference as well. Two BLEU score metrics are referred to as self -BLEU and ref -BLEU 6002 Yelp Model ACC IMDb ref -BLEU self -BLEU PPL ACC self -BLEU PPL Input Copy 3.3 23 100 11 5.2 100 5 RetrieveOnly (Li et al., 2018) TemplateBased (Li et al., 2018) DeleteOnly (Li e"
P19-1601,P18-1080,0,0.117337,"uction Text style transfer is the task of changing the stylistic properties (e.g., sentiment) of the text while retaining the style-independent content within the context. Since the definition of the text style is vague, it is difficult to construct paired sentences with the same content and differing styles. Therefore, the studies of text style transfer focus on the unpaired transfer. Recently, neural networks have become the dominant methods in text style transfer. Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework. The encoder maps the text into a style-independent latent ∗ 1 Corresponding author https://github.com/fastnlp/fastNLP representation (vector representation), and the decoder generates a new text with the same content but a different style from the disentangled latent representation plus a style variable. These methods focus on how to disentangle the content and style in the latent space. The latent representation needs better preserve the meaning of the t"
P19-1601,P18-2031,0,0.121373,"perties (e.g., sentiment) of the text while retaining the style-independent content within the context. Since the definition of the text style is vague, it is difficult to construct paired sentences with the same content and differing styles. Therefore, the studies of text style transfer focus on the unpaired transfer. Recently, neural networks have become the dominant methods in text style transfer. Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework. The encoder maps the text into a style-independent latent ∗ 1 Corresponding author https://github.com/fastnlp/fastNLP representation (vector representation), and the decoder generates a new text with the same content but a different style from the disentangled latent representation plus a style variable. These methods focus on how to disentangle the content and style in the latent space. The latent representation needs better preserve the meaning of the text while reducing its stylistic properties. Due to lacking paire"
P19-1601,P16-1009,0,0.0406867,"oposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles. However, both lines of the previous models make few attempts to utilize the attention mechanism to refer the long-term history or the source sentence, except Lample et al. (2019). In many NLP tasks, especially for text generation, attention mechanism has been proved to be an essential technique to enable the model to capture the longterm dependency (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017). In this paper, we follow the second line of work and propose a novel method which makes no assumption about the latent represe"
P19-1601,P18-1090,0,0.177901,"nd holistic attribute discriminators for the effective imposition of semantic structures. Following their work, many methods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture. Although, learning a latent representation will make the model more interpretable and easy to manipulate, the model which is assumed a fixed size latent representation cannot utilize the information from the source sentence anymore. On the other hand, there are also some approaches without manipulating latent representation are proposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a"
P19-1601,N18-1138,0,0.186298,"c attribute discriminators for the effective imposition of semantic structures. Following their work, many methods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture. Although, learning a latent representation will make the model more interpretable and easy to manipulate, the model which is assumed a fixed size latent representation cannot utilize the information from the source sentence anymore. On the other hand, there are also some approaches without manipulating latent representation are proposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a"
P19-1601,N19-1423,0,\N,Missing
W10-3005,W09-1304,0,0.0613034,"osterior probability. They only employ bag-of-words features. On the public biomedical dataset1 , their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009), the same task on Wikipedia is presented. In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue. In the first phase, they use IGTREE algorithm to train a classifier with 3 categories. In the second phase, three different classifiers are trained to find the first token and last token of in-sentence scope and finally combined into a meta classifier. The experiments shown that their system achieves an F1 of nearly 0.85 of identifying hedge cues in the abs"
W10-3005,W08-0606,0,0.0614431,"and used average perceptron as the training algorithm. The rest of the paper is organized as follows. In Section 2, a brief review of related works is presented. Then, we describe our method in Section 3. Experiments and results are presented in the section 4. Finally, the conclusion will be presented in Section 5. Introduction Detecting hedged information in biomedical literatures has received considerable interest in the biomedical natural language processing (NLP) community recently. Hedge information indicates that authors do not or cannot back up their opinions or statements with facts (Szarvas et al., 2008), which exists in many natural language texts, such as webpages or blogs, as well as biomedical literatures. For many NLP applications, such as question answering and information extraction, the information extracted from hedge sentences would be harmful to their final performances. Therefore, the hedge or speculative information should be detected in advance, and dealt with different approaches or discarded directly. In CoNLL-2010 Shared Task (Farkas et al., 2010), there are two different level subtasks: detecting sentences containing uncertainty and identifying the in-sentence scopes of hedg"
W10-3005,W02-1001,0,0.0267214,"ptron (Duda et al., 2001). It adjusts parameters w when a misclassification occurs. Although this framework is very simple, it has been shown that the algorithm converges in a finite number of iterations if the data is linearly separable. Moreover, much less training time is required in practice than the batch learning methods, such as support vector machine (SVM) or conditional maximum entropy (CME). Here we employ a variant perceptron algorithm to train the model, which is commonly named average perceptron since it averages parameters w across iterations. This algorithm is first proposed in Collins (2002). Many experiments of Hedge detection with average perceptron Detecting uncertain sentences The first subtask is to identify sentences containing uncertainty information. In particular, 1 http://www.benmedlock.co.uk/ hedgeclassif.html 2 http://www.inf.u-szeged.hu/rgai/ bioscope 33 NLP problems demonstrate better generalization performance than non averaged parameters. More theoretical proofs can be found in Collins (2002). Different from the standard average perceptron algorithm, we slightly modify the average strategy. The reason to this modification is that the original algorithm is slow sin"
W10-3005,N03-1033,0,0.00986148,"ts and our experimental results when developing the system. Our system architecture is shown in Figure 4, which consists of the following modules. 1. corpus preprocess module, which employs a tokenizer to normalize the corpus; 2. sentence detection module, which uses a binary sentence-level classifier to determine whether a sentence contains uncertainty information; 4.2 3. hedge cues detection module, which identifies which words in a sentence are the hedge cues, we train a binary word-level classifier; Corpus preprocess The sentence are processed with a maximumentropy part-of-speech tagger4 (Toutanova et al., 2003), in which a rule-based tokenzier is used to separate punctuations or other symbols from regular words. Moreover, we train a first-order projective dependency parser with MSTParser5 (McDonald et al., 2005) on the standard WSJ training corpus, which is converted from constituent trees to dependency trees by several heuristic rules6 . 4. cue scope recognition module, which recognizes the corresponding scope for each hedge cue by another word-level classifier. Our experimental results are obtained on the training datasets by 10-fold cross validation. The maximum iteration number for training the"
W10-3005,W10-3001,0,0.036671,"Missing"
W10-3005,P09-2044,0,0.114565,"ock and Briscoe (2007) report their approach based on weakly supervised learning. In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability. They only employ bag-of-words features. On the public biomedical dataset1 , their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009), the same task on Wikipedia is presented. In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue. In the first phase, they use IGTREE algorithm to train a classifier with 3 categories. In th"
W10-3005,P05-1012,0,0.224884,"trained to predict whether each Identifying hedge cues and their scopes Our approach for the second subtask consists of two phases: (1) identifying hedge cues in a sentence, then (2) recognizing their corresponding scopes. 3.2.1 Identifying hedge cues Hedge cues are the most important clues for determining whether a sentence contains uncertain 34 word-cue pair in a sentence is in the scope of the hedge cue. Besides base context features used in the previous phase, we introduce additional syntactic dependency features. These features are generated by a first-order projective dependency parser (McDonald et al., 2005), and listed in Figure 3. The scopes of hedge cues are always covering a consecutive block of words including the hedge cue itself. The ideal method should recognize only one consecutive block for each hedge cue. However, our classifier cannot work so well. Therefore, we apply a simple strategy to process the output of the classifier. The simple strategy is to find a maximum consecutive sequence which covers the hedge cue. If a sentence is considered to contain several hedge cues, we simply combine the consecutive sequences, which have at least one common word, to a large block and assign it t"
W10-3005,P07-1125,0,0.101397,"ference on Computational Natural Language Learning: Shared Task, pages 32–39, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics this subtask is a binary classification problem at sentence-level. We define the score of sentence as the confidence that the sentence contains uncertainty information. The score can be decomposed as the sum of the scores of all words in the sentence, X X wT φ(xi , y) s(xi , y) = S(x, y) = tational linguistic perspective in recent years. In this section, we give a brief review on the related works. For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability. They only employ bag-of-words features. On the public biomedical dataset1 , their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (200"
W10-4132,W06-1615,0,0.132654,"Missing"
W10-4132,C04-1081,0,0.0309809,"Science Fudan University Fudan University Fudan University Shanghai, China Shanghai, China Shanghai, China wjgao616@gmail.com xpqiu@fudan.edu.cn xjhuang@fudan.edu.cn Abstract In this paper, we describe our system1 for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVMstruct (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years of intensive research"
W10-4132,O03-4002,0,0.113539,"of Computer Science Fudan University Fudan University Fudan University Shanghai, China Shanghai, China Shanghai, China wjgao616@gmail.com xpqiu@fudan.edu.cn xjhuang@fudan.edu.cn Abstract In this paper, we describe our system1 for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVMstruct (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years o"
W10-4132,P95-1026,0,0.233427,"he intrinsic geometry of inputs revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context. Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition. They found word accessory variance (Feng et al., 2004) is useful to CWS. 3 Online Passive-Aggressive Algorithm Sequence labeling, the task of assigning labels y = y1 , . . . , yL to an input sequence x = x1 , . . . , xL . Give a sample (x, y), we define the feature is Φ(x, y). Thus, we can label x with a score function,"
W10-4132,I08-4017,0,0.295213,"s. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context. Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition. They found word accessory variance (Feng et al., 2004) is useful to CWS. 3 Online Passive-Aggressive Algorithm Sequence labeling, the task of assigning labels y = y1 , . . . , yL to an input sequence x = x1 , . . . , xL . Give a sample (x, y), we define the feature is Φ(x, y). Thus, we can label x with a score function, ˆ = arg max F (w, Φ(x, z)), y (1) z where w is the parameter of function F (·). The score function of our algorithm is linear function. ˆ is denoted as the Given a"
W10-4132,J04-1004,0,\N,Missing
W10-4165,E06-1018,0,0.0184146,"e clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottleneck is applied in (Niu et al., 2007). In (Dorow and Widdows, 2003) Graph-based clustering algorithm is employed that in a graph a node represents a noun and two nodes have an edge between them if they co-occur in list more than a given number of times. A generative model based on LDA is proposed in (Brody and Lapata, 2009). In our method, we use the triplets (Bordag, 2006) and their intersections from the Internet to construct the feature vectors then sIB is used as the clustering method. 3 Feature Selection Our method select the features of the words similar to (Bordag, 2006) is also using the triplets. In Chinese there are no natural separators between the words as English, so the ﬁrst step in Chinese language processing is often the Chinese word segmentation. In our system we use the FudanNLP toolkit1 to split the words. At the ﬁrst stage, we split the instance of the target word and ﬁlter out the numbers, English words and stop words from it. So we get a se"
W10-4165,E03-1020,0,0.0622769,"s, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottleneck is applied in (Niu et al., 2007). In (Dorow and Widdows, 2003) Graph-based clustering algorithm is employed that in a graph a node represents a noun and two nodes have an edge between them if they co-occur in list more than a given number of times. A generative model based on LDA is proposed in (Brody and Lapata, 2009). In our method, we use the triplets (Bordag, 2006) and their intersections from the Internet to construct the feature vectors then sIB is used as the clustering method. 3 Feature Selection Our method select the features of the words similar to (Bordag, 2006) is also using the triplets. In Chinese there are no natural separators between the"
W10-4165,S07-1037,0,0.157504,"organized as following: ﬁrstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the ﬁrst or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottle"
W10-4165,W97-0322,0,0.0516194,"e sIB clustering algorithm and at last discriminated the word senses. This paper is organized as following: ﬁrstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the ﬁrst or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998"
W10-4165,J98-1004,0,0.175097,"and at last discriminated the word senses. This paper is organized as following: ﬁrstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the ﬁrst or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequenti"
W10-4165,E09-1013,0,\N,Missing
