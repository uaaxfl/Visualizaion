2020.starsem-1.6,W11-2123,0,0.0516606,"odel with the highest accuracy (where perplexity was used in cases of ties). The system was evaluated on the development data every 10K steps. For comparison, we also implement our system in the case where the Transformer is replaced by another NMT system, namely a two-layers LSTM model and the Moses phrase-based machine translation system (Koehn et al., 2007). The neural model, also implemented with OpenNMT-py, is trained and validated in the same way as the Transformer. For Moses, the default model is used in a single setting (LessTrain) with MGIZA word alignment,5 and KenLM language model (Heafield, 2011) using the monolingual data provided in WMT 2014, and MERT tuning on the development set. Here too we compare the combined systems to baseline systems which do not perform decomposition. 4 4.1 S −→ Sc1 |Sc2 |· · · |Scn Experimental Setup Evaluation Using Crowdsourcing In addition to the limitations of BLEU evaluation (Papineni et al., 2002) in the context of MT (Callison-Burch et al., 2006, and much subsequent work), BLEU may correlate negatively with output quality in cases that involve sentence splitting (Sulem et al., 2018a). We therefore evaluate using crowdsourcing, and follow the protoco"
2020.starsem-1.6,P13-1023,1,0.826921,"Scenes can provide additional information about an established entity (Elaborator scenes), commonly participles or relative clauses. For example, “(child) who went home” is an Elaborator scene in “The child who went home is John”. A scene may also be a Participant in another scene. For example, “John went home” in the sentence: “He said John went home”. In other cases, scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split"
2020.starsem-1.6,P17-1104,1,0.804448,"e a Participant in another scene. For example, “John went home” in the sentence: “He said John went home”. In other cases, scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split into different sentences and concatenated according to the order of appearance. More formally, given a decomposition of a sentence S into parallel scenes Sc1 , Sc2 , · · · Scn (indexed by the order of the first token), we obtain the following rule,"
2020.starsem-1.6,P17-4019,1,0.838075,"Missing"
2020.starsem-1.6,W18-2703,0,0.0277102,"er hand, where splitting is used as preprocessing, adequacy scores decrease. In particular, LessTrain Transformer Baseline significantly outperforms the SemSplit counterpart (47.5 vs. 39.8, p &lt; 10−4 ). For sentences longer than 30, SemSplit Transformer in the LessTrain setting significantly outperforms the baseline in terms of fluency (52.1 vs. 39.6, p = 0.02), with only a non-significant (small) degradation in adequacy (41.7 vs. 40.1, p = 0.46). 6 7 Additional Experiments We first explore the performance of the proposed system in low-resource machine translation, by following the approach of Hoang et al. (2018) and randomly select 1M and 100K sentence pairs from the entire English-French training set, defining the 1MTrain and 100KTrain settings respectively. Tuning and testing remain as before. The resulted raw scores for the 1MTrain and 100KTrain settings are presented in Appendix D, Table 4 . We observe that while in 1MTrain, the SemSplit models obtain low results compared to the respective baselines, the SemSplit models obtain higher fluency in 100KTrain, though not significantly. Second, to further explore the sentence splitting component, we replicate our model, separating both parallel and emb"
2020.starsem-1.6,P18-2114,0,0.0634266,"rease in fluency on sentences longer than 30 words on the newstest2014 test corpus for English-to-French translation, with a training corpus of 5M sentence pairs, without degrading adequacy. Considering all sentence lengths, we observe a tradeoff between fluency and adequacy. We explore it using a manual analysis, suggesting that the decrease in adequacy is partly due to the loss of cohesion resulting from the splitting (§6). We then proceed to investigate the case of simulated low-resource settings as well as the effect of other sentence splitting methods, including Splitand-Rephrase models (Aharoni and Goldberg, 2018; Botha et al., 2018) (§7). The latter yield considerably lower scores than the use of simple semantic rules, supporting the case for corpusindependent simplification rules. Building on recent advances in semantic parsing and text simplification, we investigate the use of semantic splitting of the source sentence as preprocessing for machine translation. We experiment with a Transformer model and evaluate using large-scale crowd-sourcing experiments. Results show a significant increase in fluency on long sentences on an English-toFrench setting with a training corpus of 5M sentence pairs, whil"
2020.starsem-1.6,P07-2045,0,0.00711325,"ansformer Baseline, where no splitting is performed. The pipeline architecture is summarized in Figure 1. The Transformer is trained for 200K training steps, both in the FullTrain and the LessTrain settings. The development data was used for selecting the model with the highest accuracy (where perplexity was used in cases of ties). The system was evaluated on the development data every 10K steps. For comparison, we also implement our system in the case where the Transformer is replaced by another NMT system, namely a two-layers LSTM model and the Moses phrase-based machine translation system (Koehn et al., 2007). The neural model, also implemented with OpenNMT-py, is trained and validated in the same way as the Transformer. For Moses, the default model is used in a single setting (LessTrain) with MGIZA word alignment,5 and KenLM language model (Heafield, 2011) using the monolingual data provided in WMT 2014, and MERT tuning on the development set. Here too we compare the combined systems to baseline systems which do not perform decomposition. 4 4.1 S −→ Sc1 |Sc2 |· · · |Scn Experimental Setup Evaluation Using Crowdsourcing In addition to the limitations of BLEU evaluation (Papineni et al., 2002) in t"
2020.starsem-1.6,D18-1080,0,0.113829,"s longer than 30 words on the newstest2014 test corpus for English-to-French translation, with a training corpus of 5M sentence pairs, without degrading adequacy. Considering all sentence lengths, we observe a tradeoff between fluency and adequacy. We explore it using a manual analysis, suggesting that the decrease in adequacy is partly due to the loss of cohesion resulting from the splitting (§6). We then proceed to investigate the case of simulated low-resource settings as well as the effect of other sentence splitting methods, including Splitand-Rephrase models (Aharoni and Goldberg, 2018; Botha et al., 2018) (§7). The latter yield considerably lower scores than the use of simple semantic rules, supporting the case for corpusindependent simplification rules. Building on recent advances in semantic parsing and text simplification, we investigate the use of semantic splitting of the source sentence as preprocessing for machine translation. We experiment with a Transformer model and evaluate using large-scale crowd-sourcing experiments. Results show a significant increase in fluency on long sentences on an English-toFrench setting with a training corpus of 5M sentence pairs, while retaining comparabl"
2020.starsem-1.6,E06-1032,0,0.0500455,", also implemented with OpenNMT-py, is trained and validated in the same way as the Transformer. For Moses, the default model is used in a single setting (LessTrain) with MGIZA word alignment,5 and KenLM language model (Heafield, 2011) using the monolingual data provided in WMT 2014, and MERT tuning on the development set. Here too we compare the combined systems to baseline systems which do not perform decomposition. 4 4.1 S −→ Sc1 |Sc2 |· · · |Scn Experimental Setup Evaluation Using Crowdsourcing In addition to the limitations of BLEU evaluation (Papineni et al., 2002) in the context of MT (Callison-Burch et al., 2006, and much subsequent work), BLEU may correlate negatively with output quality in cases that involve sentence splitting (Sulem et al., 2018a). We therefore evaluate using crowdsourcing, and follow the protocol proposed by Graham et al. (2016). Evaluation was carried out using Amazon Mechanical Turk.6 See Appendix A for a detailed description. Corpora We experiment on the full EnglishFrench training data provided in the WMT setting (Bojar et al., 2014), which corresponds to about 39M sentence pairs after cleaning.2 We refer to this setting as the FullTrain Setting. We also experiment on the Les"
2020.starsem-1.6,W10-1762,0,0.0247513,"ork was done when being affiliated to the Hebrew University of Jerusalem 1 The code and the evaluation data are available at https://github.com/eliorsulem/ Semantic-Structural-Decomposition-for-NMT This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Related Work Sentence segmentation for MT. Segmenting sentences into sub-units, based on punctuation and syntactic structures, and recombining their output has been explored by a number of statistical MT works (Xiong et al., 2009; Goh and Sumita, 2011; Sudoh et al., 2010). In NMT, Pouget-Abadie et al. (2014) segmented the source using ILP, tackling English-to-French neural translation. They con50 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 50–57 Barcelona, Spain (Online), December 12–13, 2020 cluded that segmentation improves overall translation quality but quality may decrease if the segmented fragments are not well-formed. The concatenation may sometimes degrade fluency and result in errors in punctuation and capitalization. Kuang and Xiong (2016) attempted to find split positions such that no reordering wil"
2020.starsem-1.6,D18-1081,1,0.93696,"scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split into different sentences and concatenated according to the order of appearance. More formally, given a decomposition of a sentence S into parallel scenes Sc1 , Sc2 , · · · Scn (indexed by the order of the first token), we obtain the following rule, where “|” is the sentence delimiter: As UCCA allows argument sharing between scenes, the rule may duplicate the sam"
2020.starsem-1.6,P18-1016,1,0.910005,"scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split into different sentences and concatenated according to the order of appearance. More formally, given a decomposition of a sentence S into parallel scenes Sc1 , Sc2 , · · · Scn (indexed by the order of the first token), we obtain the following rule, where “|” is the sentence delimiter: As UCCA allows argument sharing between scenes, the rule may duplicate the sam"
2020.starsem-1.6,P09-2035,0,0.0374661,"semantic units, namely scenes, 2 ∗ This work was done when being affiliated to the Hebrew University of Jerusalem 1 The code and the evaluation data are available at https://github.com/eliorsulem/ Semantic-Structural-Decomposition-for-NMT This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Related Work Sentence segmentation for MT. Segmenting sentences into sub-units, based on punctuation and syntactic structures, and recombining their output has been explored by a number of statistical MT works (Xiong et al., 2009; Goh and Sumita, 2011; Sudoh et al., 2010). In NMT, Pouget-Abadie et al. (2014) segmented the source using ILP, tackling English-to-French neural translation. They con50 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 50–57 Barcelona, Spain (Online), December 12–13, 2020 cluded that segmentation improves overall translation quality but quality may decrease if the segmented fragments are not well-formed. The concatenation may sometimes degrade fluency and result in errors in punctuation and capitalization. Kuang and Xiong (2016) attempted to find"
C08-1002,P98-1013,0,0.153692,"Missing"
C08-1002,P98-1046,0,0.0308457,"where most instances were monosemous. For completeness, we compared our method to theirs2 , achieving similar results. We review related work in Section 2, and discuss the task in Section 3. Section 4 introduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ section"
C08-1002,C96-1055,0,0.370452,"Missing"
C08-1002,W01-0502,0,0.0298959,"1 >, < x2 , C2 , c2 >, ..., < xn , Cn , cn > } ⊆ (X × 2S × S)n , where n is the size of the training set. Let < xn+1 , Cn+1 >∈ (X × 2S ) be a new instance. Our task is to select which of the labels in Cn+1 is its correct label cn+1 (xn+1 does not have to be a previously observed lemma, but its lemma must appear in a VN class). The structure of the task lets us apply a learning algorithm that is especially appropriate for it. What we need is an algorithm that allows us to restrict the possible labels of each instance, both in training and in testing. The sequential model algorithm presented by Even-Zohar and Roth (2001) directly supports this requirement. We use the SNOW learning architecture for multi-class classification (Roth, 1998), which contains an implementation of that algorithm 9 . 5 Experimental Setup We used SemLink VN annotations and parse trees on sections 02-21 of the WSJ Penn Treebank for training, and section 00 as a development set, as is common in the parsing community. We performed two parallel sets of experiments, one using manually created gold standard parse trees and one using parse trees created by a state-of-the-art 9 Experiments on development data revealed that for verbs for which"
C08-1002,J01-3003,0,0.115307,"Missing"
C08-1002,P05-3014,0,0.0532961,"Missing"
C08-1002,J05-1004,0,0.151397,"in classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made available to the community. 2 Using the same sentences and instances, obtained from the authors. 10 number of classes relative to the number of types6 . A classifier may gather valuable information for all members of a certain VN class, without seeing all of its members in the training data. From this perspective the task resembles POS tagging. In both tasks there are many dozens (or more) of possible labels, while eac"
C08-1002,C00-2108,0,0.214619,"Missing"
C08-1002,kipper-etal-2006-extending,0,0.0365947,"ed work in Section 2, and discuss the task in Section 3. Section 4 introduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made"
C08-1002,H05-1111,0,0.166298,"Missing"
C08-1002,P98-1112,0,0.0739319,"Missing"
C08-1002,N07-1069,0,0.045338,"Missing"
C08-1002,W04-2606,0,0.0702308,"ntroduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made available to the community. 2 Using the same sentences and instances, obtai"
C08-1002,J03-4003,0,\N,Missing
C08-1002,C98-1046,0,\N,Missing
C08-1002,C98-1108,0,\N,Missing
C08-1002,C98-1013,0,\N,Missing
C08-1002,P05-1022,0,\N,Missing
C08-1002,J04-1003,0,\N,Missing
C08-1002,C96-2102,0,\N,Missing
C08-1091,P06-1109,0,0.121181,"NLP applications that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented"
C08-1091,W06-2912,0,0.555938,"NLP applications that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented"
C08-1091,P07-1051,0,0.0120828,"pirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (Van Zaanen, 2001), and improving language models (Chen, 1995). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank. Recently, works along this line have for the first time outperformed the right branching heuristic baseline for English. These include the constituent–context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar– based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). All of these use as input POS tag sequences, except of Seginer’s algorithm, which uses plain text. All of these papers induce unlabeled bracketing or dependencies. There are other algorithmic approaches to the problem (e.g., (Adriaans, 1992; Daelemans, 1995; Van Zaanen, 2001)). None of these had evaluated labeled bracketing on annotated corpora. In this paper we focus on the induction of labeled bracketing. Bayesian Model Merging 2 1 The"
C08-1091,A00-1031,0,0.0554477,"Missing"
C08-1091,P95-1031,0,0.0433833,"on 3 we detail our algorithm. The experimental setup and results are presented in Sections 4 and 5. 2 Previous Work Unsupervised parsing has attracted researchers for decades (see (Clark, 2001; Klein, 2005) for recent reviews). Many types of input, syntax formalisms, search procedures, and success criteria were used. Among the theoretical and practical motivations to this problem are the study of human language acquisition (in particular, an empirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (Van Zaanen, 2001), and improving language models (Chen, 1995). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank. Recently, works along this line have for the first time outperformed the right branching heuristic baseline for English. These include the constituent–context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar– based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2"
C08-1091,E03-1009,0,0.335106,"Missing"
C08-1091,P06-1111,0,0.512516,"in the grammar induction models of (Wolf, 1982; Langley and Stromsten, 2000; Petasis et al., 2004; Solan et al., 2005). The BMM model used here (Borensztajn and Zuidema, 2007) combines features of (Petasis et al., 2004) and Stolcke’s algorithm, applying the minimum description length (MDL) principle. We use it here only for initial labeling of existing bracketings. The MDL principle was also used in (Grunwald, 1994; de Marcken, 1995; Clark, 2001). There are only two previous papers we are aware of that induce labeled bracketing and evaluate on corpora annotated with a similar representation (Haghighi and Klein, 2006; Borensztajn and Zuidema, 2007). We utilize and extend the latter’s labeling algorithm. However, the evaluation done by the latter dealt only with labeling, using gold-standard (manually annotated) bracketings. Thus, we can directly compare our results only to (Haghighi and Klein, 2006), where two models (PCFG × NONE and PCFG × CCM) are fully unsupervised. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously, as opposed to our three step method3 . 3 Algorithm Our model consists of three stages: bracketing, initial labeling, and label clusterin"
C08-1091,H05-1004,0,0.0117696,"s other methods in future papers. find a (one-to-one) matching M from X to Y having a maximal weight. In our case, X is the set of model symbols, Y is the set of T or P most frequent target symbols (depending on the desired label set size used), and wij := CXi ,Yj , computed as in greedy mapping (the number of times xi and yj share a constituent). To make the graph complete, we add zero weight edges between induced and target labels that do not share any constituent. The Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) solves this problem, and we used it to perform the LL mapping (see also (Luo, 2005)). We assessed the overall quality of our algorithm, the quality of its labeling stage and the quality of the syntactic clustering (SC) stage. For the overall quality of the induced grammar (both bracketing and labeling) we compare our results with (Haghighi and Klein, 2006), using their setup10 . That setup was used for all numbers reported in this paper. Note that a random baseline would yield very poor results, so there is nothing to be gained from comparing to it. We assessed the quality of the labeling (MDL and SC) stages alone, using only the correct bracketings produced by the first sta"
C08-1091,D07-1043,0,0.0931203,"o map many induced labels to the same target label, and is therefore highly forgiving of large mismatches between the structures of the induced and target grammars. Hence, we also applied a label-to-label (LL) mapping, computed by reducing this problem to optimal assignment in a weighted complete bipartite graph, formally defined as follows. Given a weighted complete bipartite graph G = (X ∪ Y ; X × Y ) where edge (Xi , Yj ) has weight wij , 8 Excluding punctuation and null elements, according to the scheme of (Klein, 2005). 9 There are many possible methods for evaluating clustering quality (Rosenberg and Hirschberg, 2007). For our task, overall f-score is a very natural one. We will address other methods in future papers. find a (one-to-one) matching M from X to Y having a maximal weight. In our case, X is the set of model symbols, Y is the set of T or P most frequent target symbols (depending on the desired label set size used), and wij := CXi ,Yj , computed as in greedy mapping (the number of times xi and yj share a constituent). To make the graph complete, we add zero weight edges between induced and target labels that do not share any constituent. The Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) solv"
C08-1091,P07-1049,0,0.794444,"r output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented here belongs to the first group, annotati"
C08-1091,P06-1072,0,0.179426,"tions that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper. The algorithm presented here belongs to the first"
C08-1091,P02-1017,0,0.156135,"e used. Among the theoretical and practical motivations to this problem are the study of human language acquisition (in particular, an empirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (Van Zaanen, 2001), and improving language models (Chen, 1995). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank. Recently, works along this line have for the first time outperformed the right branching heuristic baseline for English. These include the constituent–context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar– based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). All of these use as input POS tag sequences, except of Seginer’s algorithm, which uses plain text. All of these papers induce unlabeled bracketing or dependencies. There are other algorithmic approaches to the problem (e.g., (Adriaans, 1992; Daelemans, 1995; Van Zaanen, 2001)). None of these had evaluated labeled bra"
C08-1091,P04-1061,0,0.431009,"language, and it can potentially assist NLP applications that utilize parser output. The problem has attracted researchers for decades, and interest has greatly increased recently, in part due to the availability of huge corpora, computation power, and new learning algorithms (see Section 2). A fundamental issue in this research direction is the representation of the resulting induced gramc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. mar. Most recent work (e.g., (Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Smith and Eisner, 2006; Seginer, 2007)) annotates text sentences using a hierarchical bracketing (constituents) or a dependency structure, and thus represents the induced grammar through its behavior in a parsing task. Solan et al. (2005) uses a graph representation, while (Nakamura, 2006) simply uses a grammar formalism such as PCFG. When the bracketing approach is taken, some algorithms label the resulting constituents, while most do not. Each of these approaches can be justified or criticized; a detailed discussion of this issue is beyond the scope of this paper."
C08-1091,C02-1145,0,0.0861723,"highest score: Map(bi ) = argmaxj v bv i · aj |bv ||av | i j (3) The cosine metric grows when the same coordinates (features) in both vectors have higher values. As a result, vectors with high values of the same features (corresponding to similar syntactic behavior) get high scores. 4 Experimental Setup We evaluated our algorithm on English, German and Chinese corpora: the WSJ Penn Treebank, containing economic English newspaper articles, the Brown corpus, containing various English genres, the Negra corpus (Brants, 1997) of German newspaper text, and version 5.0 of the Chinese Penn Treebank (Xue et al., 2002). In each corpus, we used the sentences of length at most 108 , numbering 7422 (WSJ10), 9117 (Brown10), 7542 (NEGRA10) and 4626 (CTB10). For each corpus the following T and P values were used: WSJ10: 26, 8; Brown10: 28, 7; NEGRA10: 22, 6; CTB10: 24, 9. Each number produces a different grammar. For labeled f-score evaluation, the induced labels should be mapped to the target labels9 . We evaluated with two different mapping schemes. For each pair (Xi , Yj ) of induced and target labels, let CXi ,Yj be the number of times they label a constituent having the same span in the same sentence. Follow"
C10-1028,P99-1008,0,0.0281855,"dies (e.g., (Amasyali, 2005)) use semi-automated methods based on language-specific heuristics and dictionaries. At the same time, much work has been done on automated lexical acquisition for a single language, and in particular, on the web-based acquisition of various types of semantic relationships. There is a substantial amount of related studies which deal with the discovery of various relationship types represented in useful resources such as WordNet, including hypernymy (Pantel et al, 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al, 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Girju et al., 2007). While the majority of studies focus on extracting pre-specified semantic relationships, several 242 recent studies were done on the automated discovery of unspecified relationship types. Thus Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement"
C10-1028,W04-3205,0,0.0400847,"the web-based acquisition of various types of semantic relationships. There is a substantial amount of related studies which deal with the discovery of various relationship types represented in useful resources such as WordNet, including hypernymy (Pantel et al, 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al, 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Girju et al., 2007). While the majority of studies focus on extracting pre-specified semantic relationships, several 242 recent studies were done on the automated discovery of unspecified relationship types. Thus Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational similarity between two pairs of words on the same language. Banko et al. (2007) and Rosenfeld and Feldman (2007) find relationship instances where the relationships are not specified in advance. (Davidov and Rappoport,"
C10-1028,P06-1038,1,0.922714,"due to several reasons. First, as opposed to natural language descriptions, pattern clusters are formal. Second, as opposed to the other methods above, pattern clusters provide a ‘generative’ model for the represented relationship – it is possible to obtain from them relationship instances and term pairs, as we indeed utilize in this paper. Third, pattern clusters can be mined in a fully unsupervised manner, or in a focused manner when the relationship desired is known. Finally, pattern methods have proven to be highly efficient and effective for lexical acquisition tasks (Pantel et al, 2004; Davidov and Rappoport, 2006). The proposed framework comprises the following stages. First, given a set of patterns defining a relationship in a source language, we obtain from the web a set of corresponding term pairs. Next, for each of the terms in the obtained term pairs, we retrieve sets of their translations to the target language using available multilingual dictionaries. Now that we have a set of translations for each term in each pair, we retrieve search engine snippets with the translated term pairs. We then select appropriate word senses using web counts, and extract a set of patterns which connect these disamb"
C10-1028,P07-1030,1,0.780742,"or these patterns. Language Chinese Czech French German Greek Hebrew Hindi Italian Russian Turkish Ukrainian Average Capital 0.87 0.93 0.97 0.93 0.87 0.83 0.83 0.93 0.97 0.87 0.93 0.9 Language 0.83 0.83 0.9 0.9 0.83 0.8 0.8 0.87 0.9 0.83 0.87 0.85 Dog breed 0.8 0.77 0.87 0.83 0.77 0.8 0.77 0.83 0.87 0.83 0.8 0.81 Table 3: Precision for three specific relationship types: (country, capital), (country, language) and (dog breed,origin). The precision observed for this task is comparable to precision obtained for Country-Capital and Country-Language in a previous single-language acquisition study (Davidov et al., 2007)10 . The high precision observed for this task indicates that the obtained translated patterns are sufficiently good as a seed for pattern-based mining of specific relationships. 10 It should be noted however that unlike previous work, we only examine the first 30 pairs and we do not use additional disambiguating words as input. 5 Conclusion We proposed a framework which given a set of patterns defining a semantic relationship in a specific source language uses multilingual dictionaries and the web to discover a corresponding pattern cluster for a target language. In the evaluation we confirme"
C10-1028,P08-1079,1,0.620118,"s hypernymy and (to a lesser extent) meronymy, there is no single accepted method (or 1 http://www.internetworldstats.com/stats7.htm 241 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 241–249, Beijing, August 2010 resources) for other less common relationships. Among the methods that have been proposed for specifying lexical relationships are natural language description and rules (Girju et al., 2007), distributional means (Turney, 2005), sample term pairs (Pasca et al, 2006), relationship instances (Banko et al., 2007) and pattern clusters (Davidov and Rappoport, 2008a). In this paper we utilize the last definition. Following (Davidov and Rappoport, 2008a) each semantic relationship can be defined and represented by a set of lexical patterns such that the represented relation holds between entities filling the patterns’ slots. We focus on pattern clusters relationship definition due to several reasons. First, as opposed to natural language descriptions, pattern clusters are formal. Second, as opposed to the other methods above, pattern clusters provide a ‘generative’ model for the represented relationship – it is possible to obtain from them relationship i"
C10-1028,P06-1101,0,0.0868093,"ect such databases for different languages (Pease et al, 2008; Charoenporn et al., 2007). Some studies (e.g., (Amasyali, 2005)) use semi-automated methods based on language-specific heuristics and dictionaries. At the same time, much work has been done on automated lexical acquisition for a single language, and in particular, on the web-based acquisition of various types of semantic relationships. There is a substantial amount of related studies which deal with the discovery of various relationship types represented in useful resources such as WordNet, including hypernymy (Pantel et al, 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al, 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Girju et al., 2007). While the majority of studies focus on extracting pre-specified semantic relationships, several 242 recent studies were done on the automated discovery of unspecified relationship t"
C10-1028,P08-1027,1,0.73628,"s hypernymy and (to a lesser extent) meronymy, there is no single accepted method (or 1 http://www.internetworldstats.com/stats7.htm 241 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 241–249, Beijing, August 2010 resources) for other less common relationships. Among the methods that have been proposed for specifying lexical relationships are natural language description and rules (Girju et al., 2007), distributional means (Turney, 2005), sample term pairs (Pasca et al, 2006), relationship instances (Banko et al., 2007) and pattern clusters (Davidov and Rappoport, 2008a). In this paper we utilize the last definition. Following (Davidov and Rappoport, 2008a) each semantic relationship can be defined and represented by a set of lexical patterns such that the represented relation holds between entities filling the patterns’ slots. We focus on pattern clusters relationship definition due to several reasons. First, as opposed to natural language descriptions, pattern clusters are formal. Second, as opposed to the other methods above, pattern clusters provide a ‘generative’ model for the represented relationship – it is possible to obtain from them relationship i"
C10-1028,E09-1021,1,0.859713,"(Koehn and Knight, 2001). However, MT mainly deals with translation and disambiguation of words at the sentence or document level, while we translate relationship structures as a set of patterns, defined independently of contexts. We also perform pattern-set to pattern-set translation rather than the pattern-to-pattern or pair-topair translation commonly explored in MT studies. This makes it difficult to perform meaningful comparison to existing MT frameworks. However, the MT studies benefit from the proposed framework by enhancement and verification of translated relationship instances. In (Davidov and Rappoport, 2009), we proposed a framework for automated cross-lingual concept mining. We incorporate several principles from this study including concept extension and disambiguation of query language (See Section 3.3). However our goals here are different since we target cross-lingual acquisition of relationship structures rather then concept term lists. 3 Relationship Translation Framework Our framework has the following stages: (1) given a set of patterns in a source language defining some lexical relationship, we use the web to obtain source language term pairs participating in this relationship; (2) we a"
C10-1028,J06-1005,0,0.0296587,") use semi-automated methods based on language-specific heuristics and dictionaries. At the same time, much work has been done on automated lexical acquisition for a single language, and in particular, on the web-based acquisition of various types of semantic relationships. There is a substantial amount of related studies which deal with the discovery of various relationship types represented in useful resources such as WordNet, including hypernymy (Pantel et al, 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al, 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Girju et al., 2007). While the majority of studies focus on extracting pre-specified semantic relationships, several 242 recent studies were done on the automated discovery of unspecified relationship types. Thus Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational simila"
C10-1028,P06-1040,0,0.0253049,"(Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al, 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Girju et al., 2007). While the majority of studies focus on extracting pre-specified semantic relationships, several 242 recent studies were done on the automated discovery of unspecified relationship types. Thus Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational similarity between two pairs of words on the same language. Banko et al. (2007) and Rosenfeld and Feldman (2007) find relationship instances where the relationships are not specified in advance. (Davidov and Rappoport, 2008a) introduced the idea that salient semantic relationships can be defined as pattern clusters, confirming it with SAT analogy test. As explained above, we use this definition in the present study. We also use pattern clusters given by (Davidov and Rappoport, 2008a) as input in our"
C10-1028,C02-1114,0,0.0219702,"08; Charoenporn et al., 2007). Some studies (e.g., (Amasyali, 2005)) use semi-automated methods based on language-specific heuristics and dictionaries. At the same time, much work has been done on automated lexical acquisition for a single language, and in particular, on the web-based acquisition of various types of semantic relationships. There is a substantial amount of related studies which deal with the discovery of various relationship types represented in useful resources such as WordNet, including hypernymy (Pantel et al, 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al, 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Girju et al., 2007). While the majority of studies focus on extracting pre-specified semantic relationships, several 242 recent studies were done on the automated discovery of unspecified relationship types. Thus Turney (2006) provided a pattern distance measure that"
C10-1028,W01-0504,0,0.0337384,"ual queries and documents (Jagarlamudi and Kumaran, 2007). Our goal is to develop and evaluate a languageindependent algorithm for the cross-lingual translation of relationship-defining structures. While our targets are different from those of CLIR, CLIR systems can greatly benefit from our framework, since we can translate the relationships in CLIR queries and subsequently check if the same relationships are present in the retrieved documents. Another field indirectly related to our research is Machine translation (MT). Many MT tasks require automated creation or improvement of dictionaries (Koehn and Knight, 2001). However, MT mainly deals with translation and disambiguation of words at the sentence or document level, while we translate relationship structures as a set of patterns, defined independently of contexts. We also perform pattern-set to pattern-set translation rather than the pattern-to-pattern or pair-topair translation commonly explored in MT studies. This makes it difficult to perform meaningful comparison to existing MT frameworks. However, the MT studies benefit from the proposed framework by enhancement and verification of translated relationship instances. In (Davidov and Rappoport, 20"
C10-1028,W04-2609,0,0.0173151,"guage term pairs. In order to check the correctness of translation of an unspecified semantic relationship we need to compare source and target language relationships. Comparison of relationships is a challenging task, since there are no relationship resources for most relationship types even in a single language, and certainly so for their translations across languages. Thus various studies define and split generic relationships differently even when describing relatively restricted relationship domains (e.g., relationships holding between parts of noun phrases (Nastase and Szpakowicz, 2003; Moldovan et al., 2004)). In order to compare generic relationships we used a manual cross-lingual SAT-like analogy human judgment evaluation7 . This allowed us to assess the quality of the translated pattern clusters, in a similar way as (Davidov and Rappoport, 2008a) did for testing clusters in a single language. For each of the 15 clusters we constructed a cross-lingual analogy question in the following manner. The header of the question was a term pair obtained for the source language pattern cluster. The six multiple choice items included: (1) one of the 10 discovered translated term pairs of the same cluster ("
C10-1028,P06-1102,0,\N,Missing
C10-1028,S07-1003,0,\N,Missing
C10-1028,I08-6012,0,\N,Missing
C10-1028,I08-2144,0,\N,Missing
C10-1028,W06-1659,0,\N,Missing
C10-2028,D09-1020,0,0.104675,"3; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addition to) positive or negative evaluations. Mihalcea and Liu (2006) derive lists of words and phrases with happiness factor from a corpus of blog posts, where each post is annotated by the blogger with a mood label. Balog et al. (2006) use the mood annotation of blog posts coupled with news data in order to discover the ev"
C10-2028,E06-1027,0,0.0101121,"a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addition to) positive or negative evaluations. Mihalcea and Liu (2006) derive lists of words and phrases with happiness factor from a corpus of blog posts, where each post is annotated by the blogger with a"
C10-2028,E06-2031,0,0.0103723,"Missing"
C10-2028,N07-1039,0,0.0985048,"ng the polarity (sometimes called valence) of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addition"
C10-2028,P06-1038,1,0.156193,"Missing"
C10-2028,P08-1079,1,0.118392,"Missing"
C10-2028,W10-2914,1,0.352628,"nd Mihalcea (2008) classify blog posts and news headlines to six sentiment categories. While most of the works on sentiment analysis focus on full text, some works address sentiment analysis in the phrasal and sentence level, see (Yu and Hatzivassiloglou, 2003; Wilson et al., 2005; McDonald et al., 2007; Titov and McDonald, 2008a; Titov and McDonald, 2008b; Wilson et al., 2009; Tsur et al., 2010) among others. Only a few studies analyze the sentiment and polarity of tweets targeted at major brands. Jansen et al. (2009) used a commercial sentiment analyzer as well as a manually labeled corpus. Davidov et al. (2010) analyze the use of the #sarcasm hashtag and its contribution to automatic recognition of sarcastic tweets. To the best of our knowledge, there are no works employing Twitter hashtags to learn a wide range of emotions and the relations between the different emotions. 242 3 Sentiment classification framework Below we propose a set of classification features and present the algorithm for sentiment classification. 3.1 Classification features We utilize four basic feature types for sentiment classification: single word features, n-gram features, pattern features and punctuation features. For the c"
C10-2028,esuli-sebastiani-2006-sentiwordnet,0,0.28883,"entence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addition to) positive or negative evaluations. Mihalcea and Liu (2006) derive lists of words and phrases with happiness factor from a corpus of blog posts, where each post is"
C10-2028,C04-1200,0,0.222508,"sentiment expressions, and (2) determining the polarity (sometimes called valence) of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear,"
C10-2028,P07-1055,0,0.0257433,"vely unexplored sentiment types. Sentiment extraction systems usually require an extensive set of manually supplied sentiment words or a handcrafted sentiment-specific dataset. With the recent popularity of article tagging, some social media types like blogs allow users to add sentiment tags to articles. This allows to use blogs as a large user-labeled dataset for sentiment learning and identification. However, the set of sentiment tags in most blog platforms is somewhat restricted. Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al., 2007). With the recent popularity of the Twitter microblogging service, a huge amount of frequently 1 ∗ * Both authors equally contributed to this paper. WordNet 2.1 definitions. 241 Coling 2010: Poster Volume, pages 241–249, Beijing, August 2010 self-standing short textual sentences (tweets) became openly available for the research community. Many of these tweets contain a wide variety of user-defined hashtags. Some of these tags are sentiment tags which assign one or more sentiment values to a tweet. In this paper we propose a way to utilize such tagged Twitter data for classification of a wide v"
C10-2028,P06-1134,0,0.00825999,") opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addition to) positive or negative evaluations. Mihalcea and Liu (2006) derive lists of words and phrases with happiness factor from a corpus of blog posts, where each post is annotated by the blogger with a mood label. Balog et al. (2"
C10-2028,H05-1044,0,0.504008,"ns, and (2) determining the polarity (sometimes called valence) of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regard"
C10-2028,J09-3003,0,0.321058,"s to classify blog texts according to moods. While (Mishne, 2005) classifies a blog entry (post), (Mihalcea and Liu, 2006) assign a happiness factor to specific words and expressions. Mishne used a much broader range of moods. Strapparava and Mihalcea (2008) classify blog posts and news headlines to six sentiment categories. While most of the works on sentiment analysis focus on full text, some works address sentiment analysis in the phrasal and sentence level, see (Yu and Hatzivassiloglou, 2003; Wilson et al., 2005; McDonald et al., 2007; Titov and McDonald, 2008a; Titov and McDonald, 2008b; Wilson et al., 2009; Tsur et al., 2010) among others. Only a few studies analyze the sentiment and polarity of tweets targeted at major brands. Jansen et al. (2009) used a commercial sentiment analyzer as well as a manually labeled corpus. Davidov et al. (2010) analyze the use of the #sarcasm hashtag and its contribution to automatic recognition of sarcastic tweets. To the best of our knowledge, there are no works employing Twitter hashtags to learn a wide range of emotions and the relations between the different emotions. 242 3 Sentiment classification framework Below we propose a set of classification features"
C10-2028,W03-1017,0,0.152629,"ifferent tasks: (1) Identifying sentiment expressions, and (2) determining the polarity (sometimes called valence) of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadn"
C10-2028,W03-1014,0,0.153509,"Section 4 describes the dataset and labels. Automated and manual evaluation protocols and results are presented in Section 5, followed by a short discussion. 2 Related work Sentiment analysis tasks typically combine two different tasks: (1) Identifying sentiment expressions, and (2) determining the polarity (sometimes called valence) of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (A"
C10-2028,P08-1036,0,0.625511,"of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analysis systems (Akkaya et al., 2009). All works mentioned above identify evaluative sentiment expressions and their polarity. Another line of works aims at identifying a broader range of sentiment classes expressing various emotions such as happiness, sadness, boredom, fear, and gratitude, regardless (or in addition to) positive or negative evaluations. Mihalcea a"
C10-2028,P02-1053,0,0.0190114,"gorithm, while Section 4 describes the dataset and labels. Automated and manual evaluation protocols and results are presented in Section 5, followed by a short discussion. 2 Related work Sentiment analysis tasks typically combine two different tasks: (1) Identifying sentiment expressions, and (2) determining the polarity (sometimes called valence) of the expressed sentiment. These tasks are closely related as the purpose of most works is to determine whether a sentence bears a positive or a negative (implicit or explicit) opinion about the target of the sentiment. Several works (Wiebe, 2000; Turney, 2002; Riloff, 2003; Whitelaw et al., 2005) use lexical resources and decide whether a sentence expresses a sentiment by the presence of lexical items (sentiment words). Others combine additional feature types for this decision (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wilson et al., 2005; Bloom et al., 2007; McDonald et al., 2007; Titov and McDonald, 2008a; Melville et al., 2009). It was suggested that sentiment words may have different senses (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006), thus word sense disambiguation can improve sentiment analy"
C10-2146,A00-1031,0,0.093527,"n word Tagging. Section 3 describes our web-query based algorithm. Section 4 and Section 5 describe experimental setup and results. 2 Previous Work Most supervised POS tagging works address the issue of unknown words. While the general methods of POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interacti"
C10-2146,D07-1019,0,0.0210065,"ormance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance usin"
C10-2146,W04-3205,0,0.023381,"rb-object bigrams from the web by query1 We did follow their experimental procedure as much as we could. Like (Blitzer et al., 2006), we compare our algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data"
C10-2146,W03-0407,0,0.0743266,"to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al., 2003). 3 the words on the right. For each context type we define a web query using two common features supported by the major search engines: wild-card search, expressed using the ‘*’ character, and exact sentence search, expressed by quoted characters. The retrieved sentences contain the parts enclosed in quotes in the exact same place they appear in the query, while an asterisk can be replaced by any single word. For a word u we execute the following three queries for each of its test contexts: 1. Replacement: &quot;u−2 u−1 ∗ u+1 u+2 &quot;. This retrieves words that appear in the same context as u. 2. Lef"
C10-2146,I05-1006,0,0.0296337,"Missing"
C10-2146,J93-2004,0,0.0357365,"Missing"
C10-2146,N06-1020,0,0.0233453,"daptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al., 2003). 3 the words on the right. For each context type we define a web query us"
C10-2146,P06-1089,0,0.0216094,"ally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interactions between POS tags of unknown words with the same lexical form. Unknown word tagging has also been explored in the context of domain adaptation of POS taggers. In this context two directions were explored: a supervised method that requires a manually annotated corpus from the target domain (Daume III, 2007), and a semi-supervised method that uses an unlabeled corpus from the target domain (Blitzer et al., 2006). Both methods require the preparation of a corpus of target domain sentences and re-training the learning algorithm. Blitzer"
C10-2146,C08-1089,0,0.0122093,"serves an application working on web text). Second, preparing a corpus is time consuming, especially when it needs to be manually annotated. Our algorithm requires no corpus from the target data domain, no preprocessing step, and it doesn’t even need to know the identity of the target domain. Consequently, the problem we address here is more difficult (and arguably more useful) than that addressed in previous work1 . The domain adaptation techniques above have not been applied to languages other than English, while our algorithm is shown to perform well in seven scenarios in three languages. Qiu et al. (2008) explored Chinese unknown word POS tagging using internal component and contextual features. Their work is not directly comparable to ours since they did not test a domain adaptation scenario, and used substantially different corpora and evaluation measures in their experiments. Numerous works utilized web resources for NLP tasks. Most of them collected corpora using data mining techniques and used them offline. For example, Keller et al., (2002) and Keller and Lapata (2003) described a method to obtain frequencies for unseen adjective-noun, noun-noun and verb-object bigrams from the web by qu"
C10-2146,W02-1001,0,0.0484009,"he run time overhead is less than 0.5 seconds per an unknown word in the English and German experiments, and less than a second per unknown word in the Chinese experiments. Section 2 reviews previous work on unknown word Tagging. Section 3 describes our web-query based algorithm. Section 4 and Section 5 describe experimental setup and results. 2 Previous Work Most supervised POS tagging works address the issue of unknown words. While the general methods of POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological"
C10-2146,W96-0213,0,0.25897,"this segmentation. To obtain the words filling the ‘*’ slots in our queries, we take all possible segmentations in which the two words appears in our training data. The queries we use in our algorithm are not the only possible ones. For example, a possible query 2 http://developer.yahoo.com/search/boss/ we do not use for the word u is &quot;∗∗u−1 uu+1 u+2 &quot;. The aforementioned set of queries gave the best results in our English, German and Chinese development data and is therefore the one we used. 3.2 Final Tagging The MXPOST Tagger. We integrated our algorithm into the maximum entropy tagger of (Ratnaparkhi, 1996). The tagger uses a set h of contexts (‘history’) for each word wi (the index i is used to allow an easy notation of the previous and next words, whose lexemes and POS tags are used as features). For each such word, the tagger computes the following conditional probability for the tag tr : p(h, tr ) p(tr |h) = P (1) ′ t′r ∈T p(h, tr ) where T is the tag set, and the denominator is simply p(h). The joint probability of a history h and a tag t is defined by: p(h, t) = Z k Y f (h,t) αj j (2) j=1 where α1 , . . . , αk are the model parameters, f1 , . . . , fk are the model’s binary features (indic"
C10-2146,P07-1033,0,0.100422,"Missing"
C10-2146,P07-1078,1,0.847885,"biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al., 2003). 3 the words on the right. For each context type we define a web query using two common features supporte"
C10-2146,P07-1030,1,0.845268,"r algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words tagging. Interestingly, previous works did not succeed in improving POS tagg"
C10-2146,H05-1058,0,0.0393421,"Missing"
C10-2146,I05-3005,0,0.0595225,"cy Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interactions between POS tags of unknown words with the same lexical form. Unknown word tagging has also been explored in the context of domain adaptation of POS taggers. In this context two directions were explored: a supervised method that requires a manually annotated corpus from the target domain (Daume III, 2007), and a semi-supervised method that uses an unlabeled corpus from the target domain (Blitzer et al., 2006). Both methods r"
C10-2146,W04-3206,0,0.0111541,"tal procedure as much as we could. Like (Blitzer et al., 2006), we compare our algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al., 2004), and the discovery of concept-specific relationships (Davidov et al., 2007). Chen et al. (2007) used web queries to suggest spelling corrections. Our work is related to self-training (McClosky et al., 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. Unlike most self-training works, our algorithm is not re-trained using the collected data but utilizes it at test time. Moreover, unlike in these works, in this work the data is collected from the web and is used only during unknown words"
C10-2146,W02-1030,0,0.029218,"chniques above have not been applied to languages other than English, while our algorithm is shown to perform well in seven scenarios in three languages. Qiu et al. (2008) explored Chinese unknown word POS tagging using internal component and contextual features. Their work is not directly comparable to ours since they did not test a domain adaptation scenario, and used substantially different corpora and evaluation measures in their experiments. Numerous works utilized web resources for NLP tasks. Most of them collected corpora using data mining techniques and used them offline. For example, Keller et al., (2002) and Keller and Lapata (2003) described a method to obtain frequencies for unseen adjective-noun, noun-noun and verb-object bigrams from the web by query1 We did follow their experimental procedure as much as we could. Like (Blitzer et al., 2006), we compare our algorithm to the performance of the MXPOST tagger trained on sections 2-21 of WSJ. Like both papers, we experimented in domain adaptation from WSJ to a biological domain. We used the freely available Genia corpus, while they used data from the Penn BioIE project (PennBioIE, 2005). 1275 ing a Web engine. On-line usage of web queries is"
C10-2146,W00-1308,0,0.0184986,"POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orthographical and morphological features with external dictionaries. Nakagawa and Matsumoto (2006) used global and local information by considering interactions between POS tags of unknown words with the same lexical form. Unknown word tagging has also been explored in the context of domain adaptation of POS taggers. In this context two directions were explored: a supervised method that requires a manually annotated"
C10-2146,N03-1033,0,0.126243,"an unknown word in the English and German experiments, and less than a second per unknown word in the Chinese experiments. Section 2 reviews previous work on unknown word Tagging. Section 3 describes our web-query based algorithm. Section 4 and Section 5 describe experimental setup and results. 2 Previous Work Most supervised POS tagging works address the issue of unknown words. While the general methods of POS tagging vary from study to study – Maximum Entropy (Ratnaparkhi, 1996), conditional random fields (Lafferty et al., 2001), perceptron (Collins, 2002), Bidirectional Dependency Network (Toutanova et al., 2003) – the treatment of unknown words is more homogeneous and is generally based on additional features used in the tagging of the unknown word. Brants (2000) used only suffix features. Ratnaparkhi (1996) used orthographical data such as suffixes, prefixes, capital first letters and hyphens, combined with a local context of the word. In this paper we show that we improve upon this method. Toutanova and Manning (2000), Toutanova et al. (2003), Lafferty et al. (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. Huihsin et al. (2005) combined orth"
C10-2146,C02-1145,0,0.0503884,"Missing"
C10-2146,J03-3005,0,\N,Missing
C10-2146,W06-1615,0,\N,Missing
C10-2146,U05-1007,0,\N,Missing
C12-1141,P10-1131,0,0.016314,"algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneous"
C12-1141,D10-1117,0,0.0133099,"Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously 2 . Borensztajn and Z"
C12-1141,W06-2912,0,0.0228754,"blem and use the k-way normalized cut algorithm (Yu and Shi, 2003) to solve it. We evaluate our algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NON"
C12-1141,N09-1009,0,0.0970314,"u and Shi, 2003) to solve it. We evaluate our algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM a"
C12-1141,J02-3001,0,0.0309942,"ree nonterminals (e.g., ‘NP’, ‘VP’, ‘PP’) in an unsupervised manner. We keep the discussion of dependency parsing for future research. Many linguistic theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive"
C12-1141,P10-2036,0,0.0368109,"Missing"
C12-1141,P06-1111,0,0.338063,"s a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then clustering these categories into a ﬁnal set. In this work we focus on improving the critical last stage of narrowing down the large number of induced BMM label"
C12-1141,P07-1107,0,0.0348312,"uctures are different in nature from the phrase categories explored in this paper, our algorithm may be applicable to this case as well. We keep this question for future research. DP has been used for unsupervised syntactic acquisition tasks. Finkel and Manning (2007) Used DP for unsupervised POS induction from dependency structures. Liang et al. (2009) proposed a nonparametric Bayesian generalization of PCFG, based on the hierarchical Dirichlet Process, and applied it to supervised parsing. DP has been used for many other NLP tasks as well (e.g. (Goldwater et al., 2006; Johnson et al., 2007; Haghighi and Klein, 2007; Johnson and Goldwater, 2009)). However, we are not aware of works that explored DP as a model for creating a diverse ensemble of experts for clustering tasks. 3 Building a Diverse Clustering Ensemble As discussed, to induce phrase categories we adopt a two stage approach where we ﬁrst induce a large number of categories and then narrow this into a ﬁnal set. The ﬁrst stage, leading to a collection of BMM categories is similar to RR08. Our novelty is in taking a qualitatively different approach to the critical stage of narrowing down the large number of categories into a small informative set."
C12-1141,N09-1012,0,0.0969143,"7 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe substantially different syntactic functions. In this paper we"
C12-1141,N09-1036,0,0.0308732,"ature from the phrase categories explored in this paper, our algorithm may be applicable to this case as well. We keep this question for future research. DP has been used for unsupervised syntactic acquisition tasks. Finkel and Manning (2007) Used DP for unsupervised POS induction from dependency structures. Liang et al. (2009) proposed a nonparametric Bayesian generalization of PCFG, based on the hierarchical Dirichlet Process, and applied it to supervised parsing. DP has been used for many other NLP tasks as well (e.g. (Goldwater et al., 2006; Johnson et al., 2007; Haghighi and Klein, 2007; Johnson and Goldwater, 2009)). However, we are not aware of works that explored DP as a model for creating a diverse ensemble of experts for clustering tasks. 3 Building a Diverse Clustering Ensemble As discussed, to induce phrase categories we adopt a two stage approach where we ﬁrst induce a large number of categories and then narrow this into a ﬁnal set. The ﬁrst stage, leading to a collection of BMM categories is similar to RR08. Our novelty is in taking a qualitatively different approach to the critical stage of narrowing down the large number of categories into a small informative set. Motivation For the Ensemble A"
C12-1141,P02-1017,0,0.051686,"malize this idea as a global optimization problem and use the k-way normalized cut algorithm (Yu and Shi, 2003) to solve it. We evaluate our algorithm on English, German and Chinese, using various tag set sizes and evaluation measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) p"
C12-1141,P03-1054,0,0.0391133,"Missing"
C12-1141,P04-1061,0,0.0404688,"Papers, pages 2307–2324, COLING 2012, Mumbai, December 2012. 2307 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe"
C12-1141,P08-1006,0,0.0158071,"c theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then clustering these categories into a ﬁnal set. In this"
C12-1141,C02-1145,0,0.114958,"Missing"
C12-1141,J08-2005,0,0.0260252,"’, ‘VP’, ‘PP’) in an unsupervised manner. We keep the discussion of dependency parsing for future research. Many linguistic theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories"
C12-1141,C08-1091,1,0.894179,"annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then clustering these categories into a ﬁnal set. In this work we focus on improving the critical last stage of narrowing down the large number of induced BMM labels into a smaller set of informative categories1 . Naively, one might think that simply replacing the s"
C12-1141,W09-1121,1,0.782673,"out performing a direct mapping to the gold standard. They are based on the observation that a good clustering reduces the uncertainty of the gold standard cluster given the induced cluster and vice-versa. Several such measures exist, we use two widely–accepted ones, the V (Rosenberg and Hirschberg, 2007) measure and the VI (Meila, 2007) measure. The V measure is deﬁned as follows: V= h=1− H(G|T ) H(G) 2hc h+ c ,c = 1− H(T |G) H(T ) For the VI measure, we report its normalized version, NVI. NVI and VI induce the same order over clusterings but NVI values for good clusterings ranges in [0, 1] (Reichart and Rappoport, 2009). The NVI measure is deﬁned to be: NV I = H(G|T ) + H(T |G) H(G) Note that V scores are in [0, 1] and the higher the score, the better the clustering. For NVI, the scores are non-negative and lower scores imply improved clustering quality. We use e as the base of the logarithm. Many other clustering evaluation measures exist. The ones we use here are well accepted in the literature. For a recent review see (Reichart and Rappoport, 2009). 7 Results In this section we demonstrate the effectiveness of our DPMM ensemble for the task of unsupervised induction of syntactic categories (parse tree non"
C12-1141,D07-1043,0,0.139262,"apping the induced labels to the gold labels and then computing the standard labeled parsing F–score 10 . While the labeling accuracy after mapping is not explicitly given, it can computed by dividing the unlabeled F–score with the labeled F–score. The IT based measures provides a way to evaluate the induced clustering without performing a direct mapping to the gold standard. They are based on the observation that a good clustering reduces the uncertainty of the gold standard cluster given the induced cluster and vice-versa. Several such measures exist, we use two widely–accepted ones, the V (Rosenberg and Hirschberg, 2007) measure and the VI (Meila, 2007) measure. The V measure is deﬁned as follows: V= h=1− H(G|T ) H(G) 2hc h+ c ,c = 1− H(T |G) H(T ) For the VI measure, we report its normalized version, NVI. NVI and VI induce the same order over clusterings but NVI values for good clusterings ranges in [0, 1] (Reichart and Rappoport, 2009). The NVI measure is deﬁned to be: NV I = H(G|T ) + H(T |G) H(G) Note that V scores are in [0, 1] and the higher the score, the better the clustering. For NVI, the scores are non-negative and lower scores imply improved clustering quality. We use e as the base of the logarithm"
C12-1141,P07-1049,0,0.369829,"ember 2012. 2307 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe substantially different syntactic func"
C12-1141,P06-1072,0,0.12929,"COLING 2012, Mumbai, December 2012. 2307 1 Introduction Grammar induction is the task of learning grammatical structure from plain text without human supervision. The task is valuable for the understanding of human language acquisition and its output can potentially be used by NLP applications, avoiding the costly and error prone creation of manually annotated corpora. The task has been widely explored (Klein, 2005) and its importance has increased due to the recent availability of huge corpora. The induced grammar can be represented in various ways. Most work (e.g., (Klein and Manning, 2004; Smith and Eisner, 2006; Seginer, 2007; Headden et al., 2009)) annotate text sentences using an unlabeled hierarchical phrase or a dependency structure, and thus represent the induced grammar through its behaviour in a parsing task. An important task in theory and practice that we consider in this work is how to enrich phrase structures with syntactic categories. The two grammars that have been widely explored by the NLP community in the last two decades, phrase structure grammars and dependency grammars, allow the induced structure to be either labeled or not and use the labeling to describe substantially different"
C12-1141,D11-1117,0,0.0346509,"Missing"
C12-1141,N10-1116,0,0.0130274,"n measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously 2 . Borensztajn and Zuidema (2007) presented the Bayesian Model Merging"
C12-1141,W10-2902,0,0.0201848,"n measures. Our results justify our reliance on DPMM and normalized-cut, and demonstrate consistent improvement over previous work. 2 Previous Work Unsupervised parsing attracts researchers for many years (see reviews in (Clark, 2001; Klein, 2005)). In recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the WSJ PennTreebank (Klein and Manning, 2002, 2004; Dennis, 2005; Bod, 2006; Smith and Eisner, 2006; Seginer, 2007; Cohen and Smith, 2009; Headden et al., 2009; Berg-Kirkpatrick and Klein, 2010; Blunsom and Cohn, 2010; Gillenwater et al., 2010; Spitkovsky et al., 2010a,b, 2011b,a). All these works induce unlabeled phrase or dependency structures. In this paper we focus on the induction of syntactic categories for unlabeled phrase structures (parse tree nonterminals) and its evaluation on corpora annotated with a similar representation. There are three previous papers we are aware of that address this problem. Haghighi and Klein (2006) presented two models: PCFG × NONE and PCFG × CCM. These models use the inside-outside and EM algorithms to induce bracketing and labeling simultaneously 2 . Borensztajn and Zuidema (2007) presented the Bayesian Model Merging"
C12-1141,D11-1118,0,0.0429292,"Missing"
C12-1141,C04-1060,0,0.0267815,"dency parsing for future research. Many linguistic theories posit a hierarchical labeled constituent (or constructional) structure, arguing that it has a measurable psychological reality (e.g., (Goldberg, 2006)). Practically, most of the syntactic annotation of corpora used by the NLP community comes in the form of labeled structures. Indeed, modern supervised syntactic parsers aim at learning labeled structures. Moreover, phrase categories are often used in a variety of NLP tasks, such as SRL (Gildea and Jurafsky, 2002; punyakanok et al., 2008), alignment in syntax-based machine translation (Zhang and Gildea, 2004), information extraction (Miyao et al., 2008), etc. Phrase categories can be induced either jointly with the phrase structure (Haghighi and Klein, 2006) or given a previously induced structure (Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Reichart and Rappoport (RR08), which has the leading results, uses a two stage approach where the second stage clusters the phrases of the parse trees induced by an unsupervised parser (Seginer, 2007). This is done by inducing an over-expressive large number of categories using the BMM model of Borensztajn and Zuidema (2007), and then cluster"
C12-1147,P11-2086,0,0.0132508,"s no correlation, and −1 indicates anticorrelation. We also compute a significance p-value, which is the probability for obtaining a given correlation at random (Abdi 2007). Results show that the relative orderings obtained in the different settings are very much in concordance. The obtained Kendall τ correlation coefficients range between (0.46, 0.88). Interestingly, when excluding DMV, results are even more significant (correlation in (0.64, 0.88)). This corresponds to p-values smaller than 10−7 and smaller than 10−13 if we exclude DMV. 9 This is a commonly used measure in NLP (Lapata 2006; Brody and Kantor 2011). 2414 Relation between Learnability Measures. In order to explore the relations between the two learnability measures, we focused on pairs of orderings that use the same parser, but different learnability measures (|P |= 5 pairs). The Kendall τ values in this case range between (0.75, 0.82), which corresponds to p-values < 10−18 . Despite the high correlation between the measures, the biases discovered under the AccuracyLearnability measure are stronger than the ones discovered under the Rate-Learnability measure. This demonstrates the somewhat different perspectives obtained by using differe"
C12-1147,N09-1009,0,0.0254999,"ps Figure 3: The VSS’s with which we experiment. The possible annotations for each structure are marked using solid and dashed lines. alternatives2 . 4 Experimental Setup 4.1 The Parsers In this work we experiment with five parsers of different types. We briefly describe them. Dependency Model with Valence (DMV) (Klein and Manning 2004) is a generative parser that defines a probabilistic grammar for unlabeled dependency structures. This parser is widely used in the field of unsupervised dependency parsing, where the great majority of recent works are in fact elaborations of this model (e.g., (Cohen and Smith 2009; Headden III et al. 2009)). In our experiments we use a supervised version of this parser, by training it using maximum likelihood estimation (MLE). This approach was used in various previous works as an upper bound for the unsupervised model (Blunsom and Cohn 2010; Spitkovsky et al. 2011). Decoding is performed using the Viterbi algorithm3. MST Parser (McDonald et al. 2005)4 formulates dependency parsing as a search for a maximum spanning tree (MST). It uses online training and extends the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003) to learning with structured outputs."
C12-1147,de-marneffe-etal-2006-generating,0,0.0102724,"Missing"
C12-1147,N10-1115,0,0.0778492,"ing with structured outputs. Clear Parser (Choi and Nicolov 2009)5 is a fast transition-based parser that uses the robust risk minimization technique (Zhang et al. 2002). k-best ranking is used to prune the next state in decoding. Su Parser (Nivre 2009)6 is a transition-based parser and an extension of the MALT parser (Nivre et al. 2006). The parser starts by constructing arcs between adjacent words and then swaps the order of input words in order to learn more complex structures. It uses the stackeager algorithm, and is trained using various linear classifiers (including SVM). NonDir Parser (Goldberg and Elhadad 2010)7 is a non-directional, easy-first parser, which is greedy and deterministic. It first attempts to induce a non-directional version of the easiest arcs in 2 Some definitions of verb groups also include auxiliaries. We choose to exclude them from our definition since we use the PTB POS set, which distinguishes modals, but not auxiliaries, from other verbs. 3 http://www.cs.columbia.edu/~scohen/parser.html http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html http://code.google.com/p/clearparser/ 6 http://maltparser.org/ 7 http://www.cs.bgu.ac.il/~yoavg/software/easyfirst/ 4 5 2411 a depen"
C12-1147,P11-1067,1,0.473969,"al. 2012) and is described in detail in Section 3. While these examples are all taken from English, variation is found in any language for which sufficient resources are available (Zeman et al. 2012). Many previous works addressed the difficulties imposed by the lack of established standards for syntactic representation. Jiang and Liu (2009) adapted statistical tools trained with one annotation standard to another. Other works proposed to normalize the different representations into a standard scheme (Ide and Bunt 2010; Zeman et al. 2012). Parsing evaluation is also highly affected by VSS’s. Schwartz et al. (2011) suggested Neutral Edge Direction (NED), an evaluation measure for unsupervised dependency parsing that accepts more than one plausible annotation for dependency VSS’s. Tsarfaty et al. (2011) suggested a new evaluation measure for supervised dependency parsing to address representational variation. The measure is based on tree edit distance. Tsarfaty et al. (2012) extended this measure for comparing between annotations from different formalisms. The emphasis of all the above works was mainly to overcome the problems incurred by the lack of standard, and not to select the most advantageous anno"
C12-1147,W03-3023,0,0.0564775,", pages 2405–2422, COLING 2012, Mumbai, December 2012. 2405 1 Introduction The formal manner in which syntactic relations are represented is at the core of the study of grammar. Numerous representations have been proposed over the years for expressing similar syntactic relations. This diversity of representations is expressed in a variety of syntactic annotation schemes currently in use in NLP. Examples include, for constituency annotation, schemes by (Marcus et al. 1993; Sampson 1995; Nelson et al. 2002, inter alia) and for dependency annotation, schemes by (Collins 1999; Rambow et al. 2002; Yamada and Matsumoto 2003; Johansson and Nugues 2007, inter alia). Variation within the same formalism is expressed in structures that have several alternative annotations (henceforth Varying Syntactic Structure or VSS). In this work we focus on dependency structures, where some of the most basic structures are VSS’s. One example is prepositional phrases, which consist of a preposition followed by a noun phrase (e.g., “about everyone”). While some schemes select the preposition to head the NP (Collins 1999), others select the NP as the head of the preposition (Johansson and Nugues 2007) (see Figure 1). Other prominent"
C12-1147,zeman-etal-2012-hamledt,0,0.00944774,"Missing"
C12-1147,W12-3602,0,\N,Missing
C12-1147,nivre-etal-2006-maltparser,0,\N,Missing
C12-1147,E12-1006,0,\N,Missing
C12-1147,W10-1840,0,\N,Missing
C12-1147,J93-2004,0,\N,Missing
C12-1147,W11-0303,0,\N,Missing
C12-1147,W09-3803,0,\N,Missing
C12-1147,W04-2609,0,\N,Missing
C12-1147,N01-1021,0,\N,Missing
C12-1147,N09-1012,0,\N,Missing
C12-1147,J03-4003,0,\N,Missing
C12-1147,D10-1117,0,\N,Missing
C12-1147,P06-1033,0,\N,Missing
C12-1147,H05-1066,0,\N,Missing
C12-1147,J06-4002,0,\N,Missing
C12-1147,P09-1040,0,\N,Missing
C12-1147,P06-3004,0,\N,Missing
C12-1147,D11-1036,0,\N,Missing
C12-1147,D07-1112,0,\N,Missing
C12-1147,W07-2416,0,\N,Missing
C12-1147,P04-1061,0,\N,Missing
C12-1147,W04-1501,0,\N,Missing
C12-1147,rambow-etal-2002-dependency,0,\N,Missing
C14-1153,P13-1023,1,0.827372,"Missing"
C14-1153,J10-4006,0,0.112148,"Missing"
C14-1153,P99-1008,0,0.273486,"hy (MEG) (Sudre et al., 2012). See (Martin, 2007) for a detailed survey. This parallel evidence to the prominence of our categories provides substance for intriguing future research. 3 Symmetric Patterns Patterns. In this work, patterns are combinations of words and wildcards, which provide a structural phrase representation. Examples of patterns include “X and Y”, “X such as Y”, “X is a country”, etc. Patterns can be used to extract various relations between words. For example, patterns such as “X of a Y” (“basement of a building”) can be useful for detecting the meronymy (part-of) relation (Berland and Charniak, 1999). Symmetric patterns (e.g., “X and Y”, “France and Holland”), which we use in this paper, can be used to detect semantic similarity between words (Widdows and Dorow, 2002). Symmetric Patterns. Symmetric patterns are patterns that contain exactly two wildcards, and where these wildcards are interchangeable. Examples of symmetric patterns include “X and Y”, “X or Y” and “X as well as Y”. Previous works have shown that word pairs that participate in symmetric patterns bare strong semantic resemblance, and consequently, that these patterns can be used to cluster words into semantic categories, whe"
C14-1153,N12-2002,0,0.0199936,"egories expressed by languages, e.g., objects, actions, and properties. We follow human development, acquiring coarse-grained categories and distinctions before detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete “things” (Langacker, 2008, Ch. 4), roughly corresponding to nouns – the main participants in linguistic clauses – that are universally present in the semantics of virtually all languages (Dixon, 2005). Most works on noun classification to semantic categories require large amounts of human annotation to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or rely on language-specific resources such as WordNet (Evans and Orˇasan, 2000; Orˇasan and Evans, 2007). Such heavy supervision is labor intensive and makes these models domain and language dependent. Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can compensate for the lack of input from the senses in text corpora. Our model therefore performs semantic category classification using only a small number of labeled seed words per category. The experiments we conduct show that such weak supervision is sufficient to construct a"
C14-1153,J92-4003,0,0.553962,"milarity measures is described in Section 5.2. SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path lengt"
C14-1153,W00-0717,0,0.0358593,"008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different l"
C14-1153,P06-1038,1,0.918323,"ns Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with known classes to the rest of the concepts. For this aim we apply an iterative variant of the k-Nearest Neighbors algorithm (denoted wit"
C14-1153,P08-1079,1,0.838024,"ossible is the concept of “flexible patterns”, which are composed of high frequency words (HFW) and content words (CW). Every word in the language is defined as either HFW or CW, based on the number of times this word appears in a large corpus. This clustering procedure is applied by traversing a large corpus, and marking words that appear with corpus frequency higher than a predefined threshold t1 as HFWs, and words with corpus frequency lower than t2 as CWs.1 1 We follow (Davidov and Rappoport, 2006) and set t1 = 10−5 , t2 = 10−3 . Note that some words are marked both as HFW and as CW. See (Davidov and Rappoport, 2008) for discussion. 1614 The resulting clusters have a desired property: HFWs are comprised mostly of function words (prepositions, determiners, etc.) while CWs are comprised mostly of content words (nouns, verbs, adjectives and adverbs). This coarse grained clustering is useful for pattern extraction from plain text, since language patterns tend to use fixed function words, while content words change from one instance of the pattern to another (Davidov and Rappoport, 2006). Flexible patterns are extracted by traversing a large corpus and, based on the clustering of words to CWs and HFWs, extract"
C14-1153,P97-1023,0,0.667401,"Missing"
C14-1153,C92-2082,0,0.325937,"Missing"
C14-1153,Y09-1024,0,0.0307906,"Categories. Several works tackled the task of semantic classification, mostly focusing on animacy, concreteness and countability. The vast majority of these works are either supervised (Hatzivassiloglou and McKeown, 1997; Baldwin and Bond, 2003; Peng and Araki, 2005; Øvrelid, 2005; Nagata et al., 2006; Xing et al., 2010; Kwong, 2011; Bowman and Chopra, 2012) or make use of external, language-specific resources such as WordNet (Orˇasan and Evans, 2001; Orˇasan and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a small set of labeled seed words. Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences in instances of hand-crafted patterns such as “X who Y” and “X and his Y”. While their model uses patterns that are tailored to the animacy and gender categories, our model uses automatically induced patterns and is thus applicable to a range of semantic categories. Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais, 1997) based classification features. They used their model to classify nouns into the concrete/abstract category using 40 labeled seed words . Unl"
C14-1153,P08-1068,0,0.264369,"ord embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different label propagation algorithm, while still using the symmetric pattern f"
C14-1153,P08-1119,0,0.805739,"s include “X and Y”, “X as well as Y” and “neither X nor Y”. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with known classes to the res"
C14-1153,Y11-1007,0,0.0612062,"Missing"
C14-1153,N04-1043,0,0.0368862,"2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different label propagation algorithm,"
C14-1153,D13-1006,0,0.0883615,"uages, e.g., objects, actions, and properties. We follow human development, acquiring coarse-grained categories and distinctions before detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete “things” (Langacker, 2008, Ch. 4), roughly corresponding to nouns – the main participants in linguistic clauses – that are universally present in the semantics of virtually all languages (Dixon, 2005). Most works on noun classification to semantic categories require large amounts of human annotation to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or rely on language-specific resources such as WordNet (Evans and Orˇasan, 2000; Orˇasan and Evans, 2007). Such heavy supervision is labor intensive and makes these models domain and language dependent. Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can compensate for the lack of input from the senses in text corpora. Our model therefore performs semantic category classification using only a small number of labeled seed words per category. The experiments we conduct show that such weak supervision is sufficient to construct a high quality classifi"
C14-1153,P06-2077,0,0.0664839,"Missing"
C14-1153,W01-0716,0,0.133972,"Missing"
C14-1153,I05-2018,0,0.0860965,"Missing"
C14-1153,W97-0313,0,0.188417,"on. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label 1620 seeds to achieve good performance; in contrast, our work performs very well with a randomly selected set of labeled seed words. 8 Conclusion We presented a minimally supervised model for noun classification into coarse grained semantic categories. Our model"
C14-1153,D13-1193,1,0.840639,". Previous works have shown that word pairs that participate in symmetric patterns bare strong semantic resemblance, and consequently, that these patterns can be used to cluster words into semantic categories, where a high precision, but low coverage (recall) solution is good enough (Dorow et al., 2005; Davidov and Rappoport, 2006). A key observation of this paper is that symmetric patterns can be also used for semantic classification, where recall is as important as precision. Flexible Patterns. It has been shown in previous work (Davidov and Rappoport, 2006; Turney, 2008; Tsur et al., 2010; Schwartz et al., 2013) that patterns can be extracted from plain text in a fully unsupervised manner. The key idea that makes this procedure possible is the concept of “flexible patterns”, which are composed of high frequency words (HFW) and content words (CW). Every word in the language is defined as either HFW or CW, based on the number of times this word appears in a large corpus. This clustering procedure is applied by traversing a large corpus, and marking words that appear with corpus frequency higher than a predefined threshold t1 as HFWs, and words with corpus frequency lower than t2 as CWs.1 1 We follow (D"
C14-1153,P06-1101,0,0.046761,"abeled words that are used for propagation. Our model, on the other hand, does not require any seed selection procedure, and utilizes a randomly selected set of labeled seed words. Lexical Acquisition. Another line of work focused on the acquisition of semantic categories. In this setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for precision. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the"
C14-1153,D13-1170,0,0.00602652,"th a more sophisticated label propagation algorithm. 5.1.1 Classification Features Baselines In this set of baselines, we use different methods for building our graph. Concretely, instead of adding edges for pairs of words that appear in the same symmetric pattern, we use word similarity measures based on different feature sets as described below. The process of building the graph using the baseline word similarity measures is described in Section 5.2. SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller"
C14-1153,D11-1063,0,0.0331411,"ternal, language-specific resources such as WordNet (Orˇasan and Evans, 2001; Orˇasan and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a small set of labeled seed words. Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences in instances of hand-crafted patterns such as “X who Y” and “X and his Y”. While their model uses patterns that are tailored to the animacy and gender categories, our model uses automatically induced patterns and is thus applicable to a range of semantic categories. Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais, 1997) based classification features. They used their model to classify nouns into the concrete/abstract category using 40 labeled seed words . Unlike our model, which requires only a small set of labeled seeds, their algorithm is actually heavily supervised, requiring thousands of labeled examples for selecting the seed set of labeled words that are used for propagation. Our model, on the other hand, does not require any seed selection procedure, and utilizes a randomly selected set of labeled seed words. Lexical Acquisit"
C14-1153,P09-1050,0,0.0137145,"categories. In this setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for precision. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label 1620 seeds to achieve good performance; in contrast, our work performs very well with a randomly selected set of labeled seed wo"
C14-1153,C02-1114,0,0.757091,"ples of symmetric patterns include “X and Y”, “X as well as Y” and “neither X nor Y”. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with"
C14-1153,W03-1010,0,\N,Missing
D09-1028,P99-1016,0,0.0208669,"raphical relationships can include nearness of two locations and entity-location relationships such as instituteaddress, capital-country, tourist site-city etc. The major differences between relationship acquisition frameworks come from the types and annotation requirements of the supplied input and the basic algorithmic approach used to process this input. A first major algorithmic approach is to represent word contexts as vectors in some space and use distributional measures and automatic clustering in that space. Curran (2002) and Lin (1998) use syntactic features in the vector definition. Caraballo (1999) uses conjunction and appositive annotations in the vector representation.While efforts have been made for improving the computational complexity of these methods (Gorman and Curran, 2006), they remain data and computation intensive. The second major algorithmic approach is to use lexico-syntactic patterns, which have been shown to produce more accurate results than feaOur study is related to geographical information retrieval (GIR) systems. However, our problem is very far from classic GIR problem settings. In GIR, the goal is to classify or retrieve possibly multilingual documents in respons"
D09-1028,W02-0908,0,0.0530811,"Missing"
D09-1028,P06-1038,1,0.751646,"repositories such as Wikipedia. Such sites usually utilize structured information such as machine-readable meta-data, tables, schedule forms or lists, which are relatively convenient for processing. However, automatic utilization of 267 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 267–275, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP ture vectors at a lower computational cost on large corpora (Pantel et al., 2004). Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel and Lin, 2002) and synonymy (Widdows and Dorow, 2002; Davidov and Rappoport, 2006). Some studies deal with the discovery of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and semantic relations between nominals (Davidov and Rappoport, 2008). Extensive frameworks were proposed for iterative discovery of pre-specified (e.g., (Riloff and Jones, 1999)) and unspecified (e.g., (Agichtein and Gravano, 2000)) relation types. The obtained location data can be used as a draft for preparation of travel resources and ondemand travel plans. It can also be used for question answering systems and for automated enrichment and verification of e"
D09-1028,P07-1030,1,0.911119,"and freely accessible form. With the growth of the web, information can be frequently found in ‘ordinary’ web pages such as forums, travelogues or news. In such sites, information is usually noisy, unstructured and present in the form of free text. This type of information can be addressed by lexical patterns. Patterns were shown to be very useful in all sorts of lexical acquisition tasks, giving high precision results at relatively low computational costs (Pantel et al., 2004). Patterndriven search engine queries allow to access such information and gather the required data very efficiently (Davidov et al., 2007). In this paper we present a framework that given a few seed locations as a specification of a region, discovers additional locations (including alternate location names) and map-like travel paths through this region labeled by transport type labels. The type of output produced by our framework here differs from that in previous pattern-based studies. Unlike mainstream pattern-based web mining, it does not target some specific two-slot relationship and attempts to extract word tuples for this relationship. Instead, it discovers geographical networks of transport or access connections. Such net"
D09-1028,C02-1114,0,0.1075,"anies, tourist sites and repositories such as Wikipedia. Such sites usually utilize structured information such as machine-readable meta-data, tables, schedule forms or lists, which are relatively convenient for processing. However, automatic utilization of 267 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 267–275, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP ture vectors at a lower computational cost on large corpora (Pantel et al., 2004). Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel and Lin, 2002) and synonymy (Widdows and Dorow, 2002; Davidov and Rappoport, 2006). Some studies deal with the discovery of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and semantic relations between nominals (Davidov and Rappoport, 2008). Extensive frameworks were proposed for iterative discovery of pre-specified (e.g., (Riloff and Jones, 1999)) and unspecified (e.g., (Agichtein and Gravano, 2000)) relation types. The obtained location data can be used as a draft for preparation of travel resources and ondemand travel plans. It can also be used for question answering systems and for automated en"
D09-1028,P08-1027,1,0.847005,"r, automatic utilization of 267 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 267–275, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP ture vectors at a lower computational cost on large corpora (Pantel et al., 2004). Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel and Lin, 2002) and synonymy (Widdows and Dorow, 2002; Davidov and Rappoport, 2006). Some studies deal with the discovery of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and semantic relations between nominals (Davidov and Rappoport, 2008). Extensive frameworks were proposed for iterative discovery of pre-specified (e.g., (Riloff and Jones, 1999)) and unspecified (e.g., (Agichtein and Gravano, 2000)) relation types. The obtained location data can be used as a draft for preparation of travel resources and ondemand travel plans. It can also be used for question answering systems and for automated enrichment and verification of existing geographical resources. We evaluate our framework on three different regions of different scale and type: Annapurna in Nepal, the south Israel area and the Cardiff area in England. In our evaluatio"
D09-1028,P06-1046,0,0.0160661,"es between relationship acquisition frameworks come from the types and annotation requirements of the supplied input and the basic algorithmic approach used to process this input. A first major algorithmic approach is to represent word contexts as vectors in some space and use distributional measures and automatic clustering in that space. Curran (2002) and Lin (1998) use syntactic features in the vector definition. Caraballo (1999) uses conjunction and appositive annotations in the vector representation.While efforts have been made for improving the computational complexity of these methods (Gorman and Curran, 2006), they remain data and computation intensive. The second major algorithmic approach is to use lexico-syntactic patterns, which have been shown to produce more accurate results than feaOur study is related to geographical information retrieval (GIR) systems. However, our problem is very far from classic GIR problem settings. In GIR, the goal is to classify or retrieve possibly multilingual documents in response to queries in the form ‘theme, spatial relationship, location’, e.g., ‘mountains near New York’ (Purves et al., 2006). Our goal, in contrast, is not document retrieval, but the generatio"
D09-1028,C92-2082,0,0.145216,"mation can be found in web sites of transport companies, tourist sites and repositories such as Wikipedia. Such sites usually utilize structured information such as machine-readable meta-data, tables, schedule forms or lists, which are relatively convenient for processing. However, automatic utilization of 267 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 267–275, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP ture vectors at a lower computational cost on large corpora (Pantel et al., 2004). Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel and Lin, 2002) and synonymy (Widdows and Dorow, 2002; Davidov and Rappoport, 2006). Some studies deal with the discovery of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and semantic relations between nominals (Davidov and Rappoport, 2008). Extensive frameworks were proposed for iterative discovery of pre-specified (e.g., (Riloff and Jones, 1999)) and unspecified (e.g., (Agichtein and Gravano, 2000)) relation types. The obtained location data can be used as a draft for preparation of travel resources and ondemand travel plans. It can als"
D09-1028,P98-2127,0,\N,Missing
D09-1028,C98-2122,0,\N,Missing
D09-1028,W04-3205,0,\N,Missing
D09-1049,P05-1075,0,0.0200978,". MWE Extraction There exists an extensive body of research on MWE extraction (see Wermter and Hahn (2004) for a review), where the only input is a corpus, and the output is a list of MWEs found in it. Most methods collect MWE candidates from the corpus, score them according to some association measure between their components, and accept candidates with scores passing some threshold. The focus of research has been on developing association measures, including statistical, information-theoretic and linguistically motivated measures (e.g., Justeson and Katz (1995), Wermter and Hahn (2006), and Deane (2005)). 3 3.1 Features Surface features include order and distance, partof-speech and inflection of an expression’s words in a sentence. Use of surface features is intuitive and relatively cheap. In addition, many studies have shown the importance of order and distance in MWE extraction in English (two recent examples are (Dias, 2003; Deane, 2005)). Thus, we develop a supervised classifier based on surface features. Many of the surface features make use of an expression’s Canonical Form (CF), thus the learning algorithm assumes that it is given such a form. Formally defining the CF is difficult. In"
D09-1049,E06-1043,0,0.0480735,"by using contextual information in the form of Latent Semantic Analysis (LSA) vectors. LSA vectors of compositional and non-compositional meaning were built from a training set of example sentences and then a nearest neighbor algorithm was applied on the LSA vector of one tested MWE. The technique was tested more thoroughly in Cook et al. (2007). Cook et al. (2007) devised two unsupervised methods to distinguish between compositional (literal) and non-compositional (idiomatic) tokens of verb-object expressions. The first method is based on an expression’s canonical form. In a previous study (Fazly and Stevenson, 2006), the authors came up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and used a corpusbased statistical measure to determine the canonical form(s). The method classifies new tokens as idiomatic if they use a canonical form, and literal otherwise. The second method uses context as well as form. Co-occurrence vectors representing the idiomatic and literal meaning of each expression were computed based on corpus data. Idiomaticmeaning vectors were based on examples matching the expressions’ canonical form. Literal meaning"
D09-1049,W98-0707,0,0.0482631,"Leech et al. (1994) who used this method for automatic part-of-speech tagging for the BNC. Another is a formalism called IDAREX (IDioms And Regular EXpressions) (Breidt et al., 1996). More recent research emphasizes the integration of MWE lexical entries into existing single word lexicons and grammar systems (Villavicencio et al., 2004; Alegria et al., 2004). There is also an attempt to take advantage of regularities in morpho-syntactic properties across MWE groups, which allows encoding the behavior of the group instead of individual expressions (Villavicencio et al., 2004; Gr´egoire, 2007). Fellbaum (1998) discusses some difficulties in representing idioms, which are largely figurative in meaning, in WordNet. More recent work (Fellbaum et al., 2006) focuses on German VP idioms. As already mentioned, one issue with lexical encoding is that it is done manually, making lexicons difficult to create, maintain and extend. The use of regularities among different types of MWEs is one way of reducing the amount of work required. A second issue is that implementations tend to ignore the likelihood and even the possibility of compositional and other interpretations of expressions in text, which can 469 di"
D09-1049,W07-1103,0,0.0431023,"Missing"
D09-1049,P06-2046,0,0.27475,"Missing"
D09-1049,W06-1203,0,0.0618514,"syntactic parser was used to collect sentences containing the MWEs in the active and passive voice using heuristics. Thus, examples such as the following (from the BNC) would not be included in their sample: be common for some expressions. For example, in an MWE identification study, Hashimoto et al. (2006) built an identification system using hand crafted rules for some 100 Japanese idioms. The results showed near perfect performance on expressions without compositional/noncompositional ambiguity but significantly poorer performance on expressions with ambiguity. 2.2 MWE Identification by ML Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analysis (LSA) vectors. LSA vectors of compositional and non-compositional meaning were built from a training set of example sentences and then a nearest neighbor algorithm was applied on the LSA vector of one tested MWE. The technique was tested more thoroughly in Cook et al. (2007). Cook et al. (2007) devised two unsupervised methods to distinguish between compositional (literal) and non-composition"
D09-1049,de-marneffe-etal-2006-generating,0,0.034577,"Missing"
D09-1049,W07-1106,0,0.139023,"Missing"
D09-1049,E09-1086,0,0.266156,"form(s). The method classifies new tokens as idiomatic if they use a canonical form, and literal otherwise. The second method uses context as well as form. Co-occurrence vectors representing the idiomatic and literal meaning of each expression were computed based on corpus data. Idiomaticmeaning vectors were based on examples matching the expressions’ canonical form. Literal meaning vectors were based on examples that did not match the canonical form. New tokens were classified as literal/idiomatic based on their (cooccurrence) vector’s cosine similarity to the idiomatic and literal vectors. (Sporleder and Li, 2009) also attempted to distinguish compositional from non-compositional uses of expressions in text. Their assumption was that if an expression is used literally, but not idiomatically, its component words will be related semantically to several words in the surrounding 1. take a chance: ‘While he still had a chance of being near Maisie, he would take it’. 2. face the consequences: ‘. . . she did not have to face, it appears, the possible serious or even fatal consequences of her decision’. 3. make a distinction: ‘Logically, the distinction between the two aspects of the theory can and should be m"
D09-1049,C04-1141,0,0.0279317,"nce). The vector matching the true assignment is labeled positive. The others are labeled negative. If the sentence is negative, all of the vectors are labeled negative. As mentioned, the output of the method is a distinct binary classifier for each MWE. Although having a single classifier for all expressions would seem advantageous, the wide variation exhibited by MWEs (e.g., for some the passive is common, for other not at all) precludes this option and requires having a separate classifier for each expression. MWE Extraction There exists an extensive body of research on MWE extraction (see Wermter and Hahn (2004) for a review), where the only input is a corpus, and the output is a list of MWEs found in it. Most methods collect MWE candidates from the corpus, score them according to some association measure between their components, and accept candidates with scores passing some threshold. The focus of research has been on developing association measures, including statistical, information-theoretic and linguistically motivated measures (e.g., Justeson and Katz (1995), Wermter and Hahn (2006), and Deane (2005)). 3 3.1 Features Surface features include order and distance, partof-speech and inflection of"
D09-1049,P06-1099,0,0.0233905,"lassifier for each expression. MWE Extraction There exists an extensive body of research on MWE extraction (see Wermter and Hahn (2004) for a review), where the only input is a corpus, and the output is a list of MWEs found in it. Most methods collect MWE candidates from the corpus, score them according to some association measure between their components, and accept candidates with scores passing some threshold. The focus of research has been on developing association measures, including statistical, information-theoretic and linguistically motivated measures (e.g., Justeson and Katz (1995), Wermter and Hahn (2006), and Deane (2005)). 3 3.1 Features Surface features include order and distance, partof-speech and inflection of an expression’s words in a sentence. Use of surface features is intuitive and relatively cheap. In addition, many studies have shown the importance of order and distance in MWE extraction in English (two recent examples are (Dias, 2003; Deane, 2005)). Thus, we develop a supervised classifier based on surface features. Many of the surface features make use of an expression’s Canonical Form (CF), thus the learning algorithm assumes that it is given such a form. Formally defining the C"
D09-1049,W03-1806,0,\N,Missing
D09-1049,W04-0407,0,\N,Missing
D09-1049,C94-1103,0,\N,Missing
D09-1089,P06-1046,0,0.0126855,"w to update. Applications needing data on some very specific domain or on a recent news-related event may find such resources lacking. In addition, manual preparation is error-prone and susceptible to subjective concept membership decisions, frequently resulting in concepts whose terms do not 1 2 See Section 5.1.1. http://www.internetworldstats.com/stats7.htm 852 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 852–861, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP effort has been made for improving the computational complexity of these methods (Gorman and Curran, 2006), they still remain data and computation intensive. scored by human judges. In all cases we have significantly extended the original concept set with high precision. We have also performed a fully automatic evaluation with 150 concepts, showing that the algorithm can re-discover WN concepts with high precision and recall when given only partial lists as input. Section 2 discusses related work, Section 3 details the algorithm, Section 4 describes the evaluation protocol and Section 5 presents our results. 2 The second major algorithmic approach is to use lexico-syntactic patterns. Patterns have"
D09-1089,C92-2082,0,0.0332928,"can re-discover WN concepts with high precision and recall when given only partial lists as input. Section 2 discusses related work, Section 3 details the algorithm, Section 4 describes the evaluation protocol and Section 5 presents our results. 2 The second major algorithmic approach is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns in order to acquire relationships, numerous patternbased methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006). Most of these studies were done for English, while some show the applicability of their methods to other languages, including Greek, Czech, Slovene and French. Related work One of the main goals of this paper is the extension or automated creation of lexical databases such as WN. Due to the importance of WN for NLP tasks, substantial research wa"
D09-1089,P98-2127,0,0.0181854,"iewed as an instance of the concept acquisition problem where the desired concepts contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co-clustering and in (Etzioni et al., 2005) using predefined pattern types. The two main algorithmic approaches to the problem are pattern-based concept discovery and clustering of context feature vectors. The latter approach represents word contexts as vectors in some space and uses similarity measures and automatic clustering in that space (Deerwester et al., 1990). Pereira et al.(1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. While great Most of these papers attempt to discover concepts from data available in some specific language. Recently several studies have proposed to utilize a second language or several specified languages in order to extract or extend concepts (Vintar and Fiˇser, 2008; van der Plas and Tiedemann, 2006) or paraphrases (Bosma and Callison-Burch, 2007). However, these methods usually"
D09-1089,P99-1016,0,0.040591,"icular kind, as done in (Freitag, 2004) using co-clustering and in (Etzioni et al., 2005) using predefined pattern types. The two main algorithmic approaches to the problem are pattern-based concept discovery and clustering of context feature vectors. The latter approach represents word contexts as vectors in some space and uses similarity measures and automatic clustering in that space (Deerwester et al., 1990). Pereira et al.(1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. While great Most of these papers attempt to discover concepts from data available in some specific language. Recently several studies have proposed to utilize a second language or several specified languages in order to extract or extend concepts (Vintar and Fiˇser, 2008; van der Plas and Tiedemann, 2006) or paraphrases (Bosma and Callison-Burch, 2007). However, these methods usually require the availability of parallel corpora, which limits their usefulness. Most of these methods utilize distributional measures, hence"
D09-1089,C08-1021,0,0.0866428,"e show the applicability of their methods to other languages, including Greek, Czech, Slovene and French. Related work One of the main goals of this paper is the extension or automated creation of lexical databases such as WN. Due to the importance of WN for NLP tasks, substantial research was done on direct or indirect automated extension of the English WN (e.g., (Snow et al., 2006)) or WN in other languages (e.g., (Vintar and Fiˇser, 2008)). The majority of this research was done on extending the tree structure (finding new synsets (Snow et al., 2006) or enriching WN with new relationships (Cuadros and Rigau, 2008)) rather than improving the quality of existing concept/synset nodes. Other related studies develop concept acquisition frameworks for on-demand tasks where concepts are defined by user-provided seeds or patterns (Etzioni et al., 2005; Davidov et al., 2007), or for fully unsupervised database creation where concepts are discovered from scratch (Banko et al., 2007; Davidov and Rappoport, 2006). Some papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of the concept acquisition problem where the desired"
D09-1089,P06-1038,1,0.853609,"WN in other languages (e.g., (Vintar and Fiˇser, 2008)). The majority of this research was done on extending the tree structure (finding new synsets (Snow et al., 2006) or enriching WN with new relationships (Cuadros and Rigau, 2008)) rather than improving the quality of existing concept/synset nodes. Other related studies develop concept acquisition frameworks for on-demand tasks where concepts are defined by user-provided seeds or patterns (Etzioni et al., 2005; Davidov et al., 2007), or for fully unsupervised database creation where concepts are discovered from scratch (Banko et al., 2007; Davidov and Rappoport, 2006). Some papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of the concept acquisition problem where the desired concepts contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co-clustering and in (Etzioni et al., 2005) using predefined pattern types. The two main algorithmic approaches to the problem are pattern-based concept discovery and clustering of context feature vectors. The latter approach represents word contexts as vectors in some space and uses simi"
D09-1089,P08-1003,0,0.0253943,"Missing"
D09-1089,P07-1030,1,0.912844,"presents our results. 2 The second major algorithmic approach is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns in order to acquire relationships, numerous patternbased methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006). Most of these studies were done for English, while some show the applicability of their methods to other languages, including Greek, Czech, Slovene and French. Related work One of the main goals of this paper is the extension or automated creation of lexical databases such as WN. Due to the importance of WN for NLP tasks, substantial research was done on direct or indirect automated extension of the English WN (e.g., (Snow et al., 2006)) or WN in other languages (e.g., (Vintar and Fiˇser, 2008)). The majority of this research was done on extending the tree structure (fin"
D09-1089,E09-1021,1,0.20547,"ny studies utilize parsing or POS tagging, which frequently depend on the availability and quality of language-specific tools. Some studies specify seed patterns in advance, and 853 sion algorithm for discovering additional terms; (4) we translate the discovered terms back to the source language, and disambiguate them; (5) we score the back-translated terms using data on their behavior in the intermediate languages, and merge the sets obtained from different languages into a single one, retaining terms whose score passes a certain threshold. Stages 1-3 of the algorithm have been described in (Davidov and Rappoport, 2009), where the goal was to translate a concept given in one language to other languages. The framework presented here includes the new stages 4-5, and its goal and evaluation methods are completely different. it is not clear whether translated patterns can work well on different languages. Also, the absence of clear word segmentation in some languages (e.g., Chinese) can make many methods inapplicable. A few recently proposed concept acquisition methods require only a handful of seed words and no pattern pre-specification (Davidov et al., 2007; Pasca and Van Durme, 2008). While these studies avoi"
D09-1089,P93-1024,0,0.0390182,"Missing"
D09-1089,P06-1101,0,0.137562,"acquire relationships, numerous patternbased methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006). Most of these studies were done for English, while some show the applicability of their methods to other languages, including Greek, Czech, Slovene and French. Related work One of the main goals of this paper is the extension or automated creation of lexical databases such as WN. Due to the importance of WN for NLP tasks, substantial research was done on direct or indirect automated extension of the English WN (e.g., (Snow et al., 2006)) or WN in other languages (e.g., (Vintar and Fiˇser, 2008)). The majority of this research was done on extending the tree structure (finding new synsets (Snow et al., 2006) or enriching WN with new relationships (Cuadros and Rigau, 2008)) rather than improving the quality of existing concept/synset nodes. Other related studies develop concept acquisition frameworks for on-demand tasks where concepts are defined by user-provided seeds or patterns (Etzioni et al., 2005; Davidov et al., 2007), or for fully unsupervised database creation where concepts are discovered from scratch (Banko et al., 2"
D09-1089,P06-2111,0,0.032679,"Missing"
D09-1089,W04-3234,0,0.0141901,"tudies develop concept acquisition frameworks for on-demand tasks where concepts are defined by user-provided seeds or patterns (Etzioni et al., 2005; Davidov et al., 2007), or for fully unsupervised database creation where concepts are discovered from scratch (Banko et al., 2007; Davidov and Rappoport, 2006). Some papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of the concept acquisition problem where the desired concepts contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co-clustering and in (Etzioni et al., 2005) using predefined pattern types. The two main algorithmic approaches to the problem are pattern-based concept discovery and clustering of context feature vectors. The latter approach represents word contexts as vectors in some space and uses similarity measures and automatic clustering in that space (Deerwester et al., 1990). Pereira et al.(1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and a"
D09-1089,C02-1114,0,0.364986,"epts, showing that the algorithm can re-discover WN concepts with high precision and recall when given only partial lists as input. Section 2 discusses related work, Section 3 details the algorithm, Section 4 describes the evaluation protocol and Section 5 presents our results. 2 The second major algorithmic approach is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns in order to acquire relationships, numerous patternbased methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006). Most of these studies were done for English, while some show the applicability of their methods to other languages, including Greek, Czech, Slovene and French. Related work One of the main goals of this paper is the extension or automated creation of lexical databases such as WN. Due to the importance of WN for NLP tasks, s"
D09-1089,W02-0908,0,\N,Missing
D09-1089,P06-1102,0,\N,Missing
D09-1089,C98-2122,0,\N,Missing
D09-1089,I08-6012,0,\N,Missing
D09-1089,I08-2144,0,\N,Missing
D09-1089,vintar-fiser-2008-harvesting,0,\N,Missing
D10-1032,P06-1013,0,0.0232101,"to TSD is WSD (Section 1 and Section 3). Many algorithmic approaches and techniques have been applied to supervised WSD (for reviews see (Agirre and Edmonds, 2006; Mihalcea and Pedersen, 2005; Navigli, 2009)). Among these are various classifiers, ensemble methods combining several supervised classifiers, bootstrapping and semi-supervised learning methods, using the Web as a corpus and knowledge-based methods relying mainly on machine readable dictionaries. Specifically related to this paper are works that exploit syntax (Martinez et al., 2002; Tanaka et al., 2007) and ensemble methods (e.g. (Brody et al., 2006)) to WSD. The references above also describe some unsupervised word sense induction algorithms. Our TSD algorithm uses the SNOW algorithm, which is a sparse network of classifiers (Section 6). Thus, it most resembles the ensemble approach to WSD. That approach has achieved very good results in several WSD shared tasks (Pedersen, 2000; Florian and Yarowsky, 2002). Since temporal reasoning is a direct application of TSD, research on this direction is relevant. Such research goes back to (Passonneau, 1988), which introduced the PUNDIT temporal reasoning system. For each tensed clause, PUNDIT firs"
D10-1032,D08-1073,0,0.0123498,"WA to Boston’) or not (as in ‘Tourists flew TWA to Boston’, or ‘John always flew his own plane to Boston’). The temporal structure of actual time clauses is then further analyzed. PUNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non-actual time. PUNDIT’s algorithmic approach is that of a Prolog rule based system, compared to our statistical learning corpusbased approach. We are not aware of further research that followed their sense disambiguation direction. Current temporal reasoning research focuses on temporal ordering of events (e.g., (Lapata, 2006; Chambers and Jurafsky, 2008)), for which an accepted atomic task is the identification of the temporal relation between two expressions (see e.g., the TempEval task in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and as"
D10-1032,W01-0502,0,0.0608567,"Missing"
D10-1032,W02-1004,0,0.0135657,"s, using the Web as a corpus and knowledge-based methods relying mainly on machine readable dictionaries. Specifically related to this paper are works that exploit syntax (Martinez et al., 2002; Tanaka et al., 2007) and ensemble methods (e.g. (Brody et al., 2006)) to WSD. The references above also describe some unsupervised word sense induction algorithms. Our TSD algorithm uses the SNOW algorithm, which is a sparse network of classifiers (Section 6). Thus, it most resembles the ensemble approach to WSD. That approach has achieved very good results in several WSD shared tasks (Pedersen, 2000; Florian and Yarowsky, 2002). Since temporal reasoning is a direct application of TSD, research on this direction is relevant. Such research goes back to (Passonneau, 1988), which introduced the PUNDIT temporal reasoning system. For each tensed clause, PUNDIT first decides whether it refers to an actual time (as in ‘We flew TWA to Boston’) or not (as in ‘Tourists flew TWA to Boston’, or ‘John always flew his own plane to Boston’). The temporal structure of actual time clauses is then further analyzed. PUNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non-actual time. PUNDIT’s algori"
D10-1032,P06-1095,0,0.0664147,"Missing"
D10-1032,J93-2004,0,0.0354692,"containing 4702 CSFs) to three datasets: training data (2100 sentences, 3183 forms), development data (300 sentences, 498 forms) and test data (600 sentences, 1021 forms). We used the development data to design the features for our learning model and to tune the parameters of the SNOW sequential model. In addition we used this data to design the rules of the ASF type classifier (which is not statistical and does not have a training phase). For the POS features, we induced POS tags using the MXPOST POS tagger (Ratnaparkhi, 1996). The tagger was trained on sections 2-21 of the WSJ PennTreebank (Marcus et al., 1993) annotated with gold standard POS tags. We used a publicly available implementation of the sequential SNOW model8 . We experimented in three conditions. In the first (TypeUnknown), the ASF type is not known at test time. In the last two, it is known at test time. These two conditions differ in whether the type is taken from the gold standard annotation of the test sentences (TypeKnown), or from the output of the simple rule-based classifier (TypeClassifier, see Section 6). For both conditions, the results reported below are when both ASF type features and possible labels sets are provided duri"
D10-1032,W96-0208,0,0.117812,"Missing"
D10-1032,A00-2009,0,0.00983694,"learning methods, using the Web as a corpus and knowledge-based methods relying mainly on machine readable dictionaries. Specifically related to this paper are works that exploit syntax (Martinez et al., 2002; Tanaka et al., 2007) and ensemble methods (e.g. (Brody et al., 2006)) to WSD. The references above also describe some unsupervised word sense induction algorithms. Our TSD algorithm uses the SNOW algorithm, which is a sparse network of classifiers (Section 6). Thus, it most resembles the ensemble approach to WSD. That approach has achieved very good results in several WSD shared tasks (Pedersen, 2000; Florian and Yarowsky, 2002). Since temporal reasoning is a direct application of TSD, research on this direction is relevant. Such research goes back to (Passonneau, 1988), which introduced the PUNDIT temporal reasoning system. For each tensed clause, PUNDIT first decides whether it refers to an actual time (as in ‘We flew TWA to Boston’) or not (as in ‘Tourists flew TWA to Boston’, or ‘John always flew his own plane to Boston’). The temporal structure of actual time clauses is then further analyzed. PUNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non"
D10-1032,C00-2103,0,0.0399099,"temporal relation between two expressions (see e.g., the TempEval task in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and aspect information with lexical semantics for machine translation. Schiehlen (2000) analyzed the effect tense understanding has on MT. Ye and Zhang (2005) explored tense tagging in a cross-lingual context. Ye et al., (2006) extracted features for tense translation between Chinese and English. Murata et al., (2007) compared the performance of several MT systems in translating tense and aspect and found that various ML techniques perform better on the task. Another related field is ‘deep’ parsing, where a sentence is annotated with a structure containing information that might be relevant for semantic interpretation (e.g. (Hajic, 1998; Baldwin et al., 2007)). TSD senses, howev"
D10-1032,S07-1014,0,0.0244413,"UNDIT’s classification is much simpler than in the TSD task, addressing only actual vs. non-actual time. PUNDIT’s algorithmic approach is that of a Prolog rule based system, compared to our statistical learning corpusbased approach. We are not aware of further research that followed their sense disambiguation direction. Current temporal reasoning research focuses on temporal ordering of events (e.g., (Lapata, 2006; Chambers and Jurafsky, 2008)), for which an accepted atomic task is the identification of the temporal relation between two expressions (see e.g., the TempEval task in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and aspect information with lexical semantics for machine translation. Schiehlen (2000) analyzed the effect tense understanding has on MT. Ye and Zhang (2005) explored tense tagging"
D10-1032,I05-1077,0,0.0820161,"ask in SemEval ’07 (Verhagen et al., 2007)). This direction is very different from TSD, which deals with the semantics of individual concrete tense syntactic forms. In this sense, TSD is an even more atomic task for temporal reasoning. A potential application of TSD is machine translation where it can assist in translating tense and aspect. Indeed several papers have explored tense and aspect in the MT context. Dorr (1992) explored the integration of tense and aspect information with lexical semantics for machine translation. Schiehlen (2000) analyzed the effect tense understanding has on MT. Ye and Zhang (2005) explored tense tagging in a cross-lingual context. Ye et al., (2006) extracted features for tense translation between Chinese and English. Murata et al., (2007) compared the performance of several MT systems in translating tense and aspect and found that various ML techniques perform better on the task. Another related field is ‘deep’ parsing, where a sentence is annotated with a structure containing information that might be relevant for semantic interpretation (e.g. (Hajic, 1998; Baldwin et al., 2007)). TSD senses, however, are not explicitly represented in these grammatical structures, and"
D10-1032,W91-0222,0,\N,Missing
D10-1032,W96-0213,0,\N,Missing
D10-1032,D07-1050,0,\N,Missing
D10-1032,C02-1112,0,\N,Missing
D10-1032,W07-1401,0,\N,Missing
D10-1032,J88-2005,0,\N,Missing
D10-1032,W07-2205,0,\N,Missing
D10-1032,W06-0107,0,\N,Missing
D10-1067,P06-1109,0,0.040791,"We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL with them as well. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model wi"
D10-1067,W06-2912,0,0.120731,"We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL with them as well. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model wi"
D10-1067,P07-1051,0,0.0483607,"Missing"
D10-1067,E03-1009,0,0.400157,"ting of the test set sentences contained in L. In the third stage, each of the test subsets is parsed by a model trained only on its corresponding training subset. This stage is motivated by our assumption that the high and low quality subsets manifest dissimilar syntactic patterns, and consequently the statistics of the parser’s parameters suitable for one subset differ from those suitable for another. We compute the confidence score in the second stage using the unsupervised PUPA algorithm (Reichart and Rappoport, 2009b). POS tags for it are induced using the fully unsupervised algorithm of Clark (2003). The parser we experiment with is the incremental parser of Seginer (2007), whose input consists of raw sentences and does not include any kind of supervised POS tags (created either manually or by a supervised algorithm). Consequently, our algorithm is fully unsupervised. The only parameter it has is NH but ZL improves parser performance for most NH values. BZL is related to boosting. In boosting after training one member of the ensemble, examples are reweighted such that examples that are classified correctly are down-weighted. BZL does something sim687 ilar: it uses PUPA to estimate which"
D10-1067,N09-1009,0,0.20552,"Missing"
D10-1067,N09-1012,0,0.314737,"Missing"
D10-1067,A00-2005,0,0.0372042,"r parser since its training method cannot be trivially bootstrapped with parses created in former steps (Seginer, 2007). Related machine learning methods. ZL is related to ensemble methods. Both ZL and such methods produce multiple learners, each of them trained on a different subset of the training data, and decide which learner to use for a particular test instance. Bagging (Breiman, 1996) and boosting (Freund and Schapire, 1996), where the experts utilize the same learning algorithm and differ in the sample of the training data they use for its training, were applied to supervised parsing (Henderson and Brill, 2000; Becker and Osborne, 2005). In Section 3 we discuss the connection of ZL to boosting. Owing to the fact that ZL produces different learners, it is natural to use it in conjunction with an ensemble method, which is what we do in this paper with our EZL model (Section 3). ZL is also related to active learning (AL) (Cohn and Ladner, 1994). AL also uses training subset selection, with the goal of obtaining a faster learning curve for an algorithm. AL is done in supervised settings, usually in order to minimize human annotation costs. AL algorithms providing faster learning than random subset sele"
D10-1067,J04-3001,0,0.0193001,"f ZL to boosting. Owing to the fact that ZL produces different learners, it is natural to use it in conjunction with an ensemble method, which is what we do in this paper with our EZL model (Section 3). ZL is also related to active learning (AL) (Cohn and Ladner, 1994). AL also uses training subset selection, with the goal of obtaining a faster learning curve for an algorithm. AL is done in supervised settings, usually in order to minimize human annotation costs. AL algorithms providing faster learning than random subset selection for parsing have been proposed (Reichart and Rappoport, 2009a; Hwa, 2004). However, we are not aware of AL applications in which the overall performance on the test set has been improved. In addition, our application here is to an unsupervised problem. Algorithms that utilize unsupervised clustering for class decomposition in order to improve classifiers’ performance (e.g. (Vilalta and Rish, 2003)) are related to ZL. In such methods, examples that belong to the same class are clustered, and the induced clusters are considered as separate classes. These methods, however, have been applied only to supervised classification in contrast to our work that addresses unsup"
D10-1067,I08-2097,0,0.0606717,"osed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL w"
D10-1067,P02-1017,0,0.150661,"d nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be made available, we will test ZL with them as well. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these appr"
D10-1067,P04-1061,0,0.588921,"d for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ 10). This raises the question if using more training data (e.g. the entire WSJ) wisely can enhance these models. Recently, Spitkovsky et al., (2010) proposed three approaches for improvement of unsupervised grammar induction by considering the complexity of the training data. The approaches have been applied to the DMV unsupervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model with sentences whose length is up to 15 words. As noted above, such a training protocol fails to improve the performance of the Seginer parser. The other approaches in that paper, bootstrapping via iterated learning of increasingly longer sentences and a combination of the bootstrapping and the short sentences approaches, are not directly applicable to the Seginer parser since its training method cannot be trivially bootstrapped with parses created in former steps (Seginer, 2007). Related machine learning methods. ZL"
D10-1067,J93-2004,0,0.0348999,"uently. Therefore, constituents that are more frequent in the set I receive higher scores after proper regularization is applied to prevent potential biases. The tree score is a combination of the scores of its constituents. Full details of the PUPA algorithm are given in (Reichart and Rappoport, 2009b). The resulting score was shown to be strongly correlated with the extrinsic quality of the parse tree, defined to be its Fscore similarity to the manually created (gold standard) parse tree of the sentence. 4 Experimental Setup We experimented with three English corpora: the WSJ Penn Treebank (Marcus et al., 1993) consisting of economic newspaper texts, the BROWN corpus (Francis and Kucera, 1979) consisting of texts of various English genres (e.g. fiction, humor, romance, mystery and adventure) and the GENIA corpus (Kim et al., 2003) consisting of abstracts of scientific articles from the biological domain. All corpora were stripped of all annotation (bracketing and 688 POS tags). For all corpora we report the parser performance on the entire corpus (WSJ: 49206 sentences, BROWN : 24243 sentences, GENIA : 4661 sentences). For WSJ we also provide an analysis of the performance of the parser when applied"
D10-1067,P06-1043,0,0.0463879,"subset coming from the same source. However, even when the training and test data are from the same source, a ZL algorithm may capture 686 fine differences between subsets. The ZL idea is therefore related to the notions of in-domain and out-of-domain (domain adaptation). In the former, the training and test data are assumed to originate from the same domain. In the latter, the test data comes from a different domain, and therefore has different statistics from the training data. Indeed, the performance of NLP algorithms in domain adaptation scenarios is markedly lower than in in-domain ones (McClosky et al., 2006). ZL takes this observation to the extreme, assuming that a similar situation might exist even in indomain scenarios. After all, a ‘domain’ is only a coarse qualification of the nature of a data set. In NLP, a domain is usually specified as the genre of the text involved (e.g., ‘newspapers’). However, there are additional axes that might influence the statistics obtained from training data, e.g., the syntactic nature of sentences. This section presents our ZL algorithms. We start with the simplest possible ZL algorithm where the subsets are randomly selected. We then describe ZL algorithms bas"
D10-1067,D08-1093,0,0.0340055,"that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency parsers will be mad"
D10-1067,P07-1052,1,0.893911,"lgorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only. When additional constituency"
D10-1067,W09-1103,1,0.366907,"nnotated corpora such as the WSJ PennTreebank. Recent works on unlabeled bracketing or dependencies induction include (Klein and Manning, 2002; Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Bod, 2006b; Bod, 2007; Smith and Eisner, 2006; Seginer, 2007; Cohen et al., 2008; Cohen and Smith, 2009; Headden et al., 2009). Most of the works above use POS tag sequences, created either manually or by a supervised algorithm, as input. The only exception is Seginer’s parser, which induces bracketing from plain text. Our confidence-based ZL algorithms use the PUPA unsupervised parsing quality score (Reichart and Rappoport, 2009b). As far as we know, PUPA is the only unsupervised quality assessment algorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey"
D10-1067,W09-1120,1,0.594352,"nnotated corpora such as the WSJ PennTreebank. Recent works on unlabeled bracketing or dependencies induction include (Klein and Manning, 2002; Klein and Manning, 2004; Dennis, 2005; Bod, 2006a; Bod, 2006b; Bod, 2007; Smith and Eisner, 2006; Seginer, 2007; Cohen et al., 2008; Cohen and Smith, 2009; Headden et al., 2009). Most of the works above use POS tag sequences, created either manually or by a supervised algorithm, as input. The only exception is Seginer’s parser, which induces bracketing from plain text. Our confidence-based ZL algorithms use the PUPA unsupervised parsing quality score (Reichart and Rappoport, 2009b). As far as we know, PUPA is the only unsupervised quality assessment algorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey"
D10-1067,P07-1049,0,0.464828,"upervised dependency parser (Klein and Manning, 2004) and improved its performance. One of these approaches is to train the model with sentences whose length is up to 15 words. As noted above, such a training protocol fails to improve the performance of the Seginer parser. The other approaches in that paper, bootstrapping via iterated learning of increasingly longer sentences and a combination of the bootstrapping and the short sentences approaches, are not directly applicable to the Seginer parser since its training method cannot be trivially bootstrapped with parses created in former steps (Seginer, 2007). Related machine learning methods. ZL is related to ensemble methods. Both ZL and such methods produce multiple learners, each of them trained on a different subset of the training data, and decide which learner to use for a particular test instance. Bagging (Breiman, 1996) and boosting (Freund and Schapire, 1996), where the experts utilize the same learning algorithm and differ in the sample of the training data they use for its training, were applied to supervised parsing (Henderson and Brill, 2000; Becker and Osborne, 2005). In Section 3 we discuss the connection of ZL to boosting. Owing t"
D10-1067,P06-1072,0,0.0503191,"Missing"
D10-1067,N10-1116,0,0.235637,"h direction is how to utilize training data in the best possible way. Klein and Manning (2004) report results for their dependency model with valence (DMV) for unsupervised dependency parsing when it is trained and tested on the same corpus (both when sentence length restriction is imposed, such as for WSJ 10, and when it is not, such as for the entire WSJ). Today’s best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ 10 even when the test set includes longer sentences. Recently, Spitkovsky et al., (2010) demonstrated that training the DMV model on sentences of up to 15 words length yields better results on the entire section 23 of WSJ (with no sentence length restriction) than training with the entire WSJ corpus. In contrast to these dependency models, the Seginer constituency parser achieves its best performance when trained on the entire WSJ corpus either if sentence length restriction is imposed on the test corpus or not. The sentence length restriction training protocol of (Spitkovsky et al., 2010), harms this parser. When the parser is trained with the entire WSJ corpus its F-score perfo"
D10-1067,W06-1604,0,0.069454,"quality assessment algorithm for syntactic parsers that has been proposed. Combining PUPA with Seginer’s parser thus preserves the fully unsupervised nature of the task. Quality assessment of a learning algorithm’s output has been addressed for supervised algorithms 1 For clarity of exposition, we still refer to this corpus as our training corpus. In the algorithms presented in this paper, the test set is included in the training set which is a common practice in unsupervised parsing. 685 (see (Caruana and Niculescu-Mizil, 2006) for a survey) and specifically for supervised syntactic parsing (Yates et al., 2006; Reichart and Rappoport, 2007; Ravi et al., 2008; Kawahara and Uchimoto, 2008). All these algorithms are based on manually annotated data and thus do not preserve the unsupervised nature of the task addressed in this paper. We experiment with the Seginer parser for two reasons. First, this is the best algorithm for the task of fully unsupervised parsing which motivates us to improve its performance. Second, this is the only publicly available unsupervised parser that induces constituency trees. The PUPA score we use in our confidence-based algorithms is applicable for constituency trees only."
D13-1193,P99-1008,0,0.0386137,"several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms"
D13-1193,W04-3205,0,0.0274065,"rovement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2"
D13-1193,P06-1038,1,0.320317,"ori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent aut"
D13-1193,P08-1027,1,0.902015,"ms. As a result, flexible patterns can pick up fine-grained differences between authors’ styles. Unlike other types of pattern features, 1886 Word Frequency. Flexible patterns are composed of high frequency words (HFW) and content words (CW). Every word in the corpus is defined as either HFW or CW. This clustering is performed by counting the number of times each word appears in the corpus of size s. A word that appears more than 10−4 ×s times in a corpus is considered HFW. A word that appears less than 10−3 ×s times in a corpus is considered CW. Some words may serve both as HFWs and CWs (see Davidov and Rappoport (2008b) for discussion). Structure of a Flexible Pattern. Flexible patterns start and end with an HFW. A sequence of zero or more CWs separates consecutive HFWs. At least one CW must appear in every pattern.10 For efficiency, at most six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible patter"
D13-1193,P08-1079,1,0.301097,"ms. As a result, flexible patterns can pick up fine-grained differences between authors’ styles. Unlike other types of pattern features, 1886 Word Frequency. Flexible patterns are composed of high frequency words (HFW) and content words (CW). Every word in the corpus is defined as either HFW or CW. This clustering is performed by counting the number of times each word appears in the corpus of size s. A word that appears more than 10−4 ×s times in a corpus is considered HFW. A word that appears less than 10−3 ×s times in a corpus is considered CW. Some words may serve both as HFWs and CWs (see Davidov and Rappoport (2008b) for discussion). Structure of a Flexible Pattern. Flexible patterns start and end with an HFW. A sequence of zero or more CWs separates consecutive HFWs. At least one CW must appear in every pattern.10 For efficiency, at most six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible patter"
D13-1193,P07-1030,1,0.833997,"were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of au1889 thorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We"
D13-1193,W10-2914,1,0.899099,"six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible pattern sequence. For example, (1) is matched by (2). Partial Flexible Patterns. A flexible pattern may appear in a given tweet with additional words not originally found in the flexible pattern, and/or with only a subset of the HFWs (Davidov et al., 2010a). For example, (3) is a partial match of (1), since the word “great” is not part of the original flexible pattern. Similarly, (4) is another partial match of (1), since (a) the word “good” is not part of the original flexible pattern and (b) the second occurrence of the word “the” does not appear in (4) (missing word is ). marked by 3. “TheHFW greatHFW kingCW ofHFW theHFW ring” 4. “TheHFW goodHFW kingCW ofHFW Spain” We use such cases as features with lower weight, proportional to the number of found HFWs in the 0.5×nf ound ). For example, (1) receives a tweet (w = nexpected weight of 1 (comp"
D13-1193,C10-2028,1,0.724989,"six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible pattern sequence. For example, (1) is matched by (2). Partial Flexible Patterns. A flexible pattern may appear in a given tweet with additional words not originally found in the flexible pattern, and/or with only a subset of the HFWs (Davidov et al., 2010a). For example, (3) is a partial match of (1), since the word “great” is not part of the original flexible pattern. Similarly, (4) is another partial match of (1), since (a) the word “good” is not part of the original flexible pattern and (b) the second occurrence of the word “the” does not appear in (4) (missing word is ). marked by 3. “TheHFW greatHFW kingCW ofHFW theHFW ring” 4. “TheHFW goodHFW kingCW ofHFW Spain” We use such cases as features with lower weight, proportional to the number of found HFWs in the 0.5×nf ound ). For example, (1) receives a tweet (w = nexpected weight of 1 (comp"
D13-1193,C92-2082,0,0.0741925,"aracter n-gram and word n-grams. They experimented with 10 authors of Greek text, and also joined several tweets into a single document. Joining several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used a"
D13-1193,U11-1008,0,0.0528424,"ve mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numbers of tweets per"
D13-1193,P11-1136,1,0.600788,"Missing"
D13-1193,P08-1119,0,0.0645394,"Missing"
D13-1193,W06-1657,0,0.00835382,"res are especially useful for authorship attribution on micro-messages since they are relatively tolerant to typos and non-standard use of punctuation (Stamatatos, 2009). These are common in the nonformal style generally applied in social media services. Consider the example of misspelling “Britney” as “Brittney”. The misspelled name shares the 4-grams “Brit” and “tney” with the correct name. As a result, these features provide information about the author’s style (or at least her topic of interest), which is not available through lexical features. Following standard practice, we use 4-grams (Sanderson and Guenter, 2006; Layton et al., 2010; Koppel et al., 2011b). White spaces are considered characters (i.e., a character n-gram may be composed of letters from two different words). A single white-space is appended to the beginning and the end of each tweet. For efficiency, we consider only character n-gram features that appear at least tcng times in the training set of at least one author (see Section 5). Word n-grams. We hypothesize that word n-gram features would be useful for authorship attribution on micro-messages. We assume that under a strict length restriction, many authors would prefer using short, r"
D13-1193,I11-1018,0,0.248104,"of flavors of the authorship attribution task. 1 Introduction Research in authorship attribution has developed substantially over the last decade (Stamatatos, 2009). The vast majority of such research has been dedicated towards finding the author of long texts, ranging from single passages to book chapters. In recent years, the growing popularity of social media has created special interest, both theoretical and computational, in short texts. This has led to many recent authorship attribution projects that experimented with web data such as emails (Abbasi and Chen, 2008), web forum messages (Solorio et al., 2011) and blogs (Koppel et al., 2011b). This paper addresses the question to what extent the authors of very short texts can be identified. To answer this question, we experiment with Twitter tweets. Twitter messages (tweets) are limited to 140 characters. This restriction imposes major difficulties on Moshe Koppel Department of Computer Science Bar Ilan University koppel@macs.biu.ac.il authorship attribution systems, since authorship attribution methods that work well on long texts are often not as useful when applied to short texts (Burrows, 2002; Sanderson and Guenter, 2006). Nonetheless, tweets"
D13-1193,C08-1114,0,0.0355457,"ks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of au1889 thorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the"
D13-1193,C02-1114,0,0.0215657,"aling since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal"
D18-1081,P18-2114,0,0.100626,"Missing"
D18-1081,E06-1032,0,0.141524,"s. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence’s gramn=1 where BP is the brevity penalty term, pn are the modified precisions, and wn are the corresponding weights, which are usually uniform in practice. The experiments of Papineni et al. (2002) showed that BLEU correlates with human judgments in the ranking of five English-to-Chinese MT systems and that it can distinguish human and machine translations. Although BLEU is widely used in MT, several works have pointed out its shortcomings (e.g., Koehn and Monz, 2006). In particular, Callison-Burch et al. (2006) showed that BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences. tural operations are involved (Nisioi et al"
D18-1081,W14-5603,0,0.122844,"Missing"
D18-1081,P14-1041,0,0.466995,"Missing"
D18-1081,D15-1013,0,0.0213823,"ple structural paraphrases, and performed a correlation analysis with human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their"
D18-1081,W16-6620,0,0.153104,"Missing"
D18-1081,D17-1064,0,0.056638,"erences is used to address cross-reference variation. To address changes in word order, BLEU uses n-gram precision, modified to eliminate repetitions across the references. A brevity term penalizes overly short sentences. Formally: BLEU = BP × exp( N X 3 wn log(pn )) Gold-Standard Splitting Corpus In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of Xu et al. (2016).3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEBSPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by Xu et al. (2016) for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017). We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possib"
D18-1081,P07-2045,0,0.00848781,"Missing"
D18-1081,P17-2014,0,0.578835,"ves high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found in https://github.com/ eliorsulem/HSplit-corpus 2 Nevertheless, they are also used in contexts where struc738 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738–744 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU in TS. While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adˇ equacy. T-BLEU (Stajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning pr"
D18-1081,P02-1040,0,0.113538,"lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despit"
D18-1081,W06-3114,0,0.0406323,"., 2017). We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence’s gramn=1 where BP is the brevity penalty term, pn are the modified precisions, and wn are the corresponding weights, which are usually uniform in practice. The experiments of Papineni et al. (2002) showed that BLEU correlates with human judgments in the ranking of five English-to-Chinese MT systems and that it can distinguish human and machine translations. Although BLEU is widely used in MT, several works have pointed out its shortcomings (e.g., Koehn and Monz, 2006). In particular, Callison-Burch et al. (2006) showed that BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences"
D18-1081,P11-1094,0,0.0127649,"gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple"
D18-1081,D15-1148,0,0.0283395,"Missing"
D18-1081,Q16-1005,0,0.066965,"Missing"
D18-1081,W02-0109,0,0.0130657,"ion. We use the evaluation benchmark provided by Sulem et al. (2018b),11 including system outputs and human evaluation scores corresponding to the first 70 sentences of 4 Examples are taken from Siddharthan (2006). Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon’s signed rank test, p = 1.6 · 10−5 for #Sents and p = 0.002 for SplitSents. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). 5 8 We thus computed the correlation in §4.2 for -FK. LDSC is computed using NLTK. 10 Taking the fourth hypothesis rather than the first has been found to yield considerably less conservative TS systems. 11 https://github.com/eliorsulem/ simplification-acl2018 9 740 BLEU-1ref BLEU-8ref iBLEU-1ref iBLEU-8ref -FK SARI-8ref G 0.43 (0.2) 0.61 (0.07) 0.21 (0.3) 0.61 (0.07) -0.21 (0.3) -0.64 (0.06) -LDSC 0.29 (0.3) Systems/Corpora without Splits M S 1.00 (0) -0.81 (0.01) 0.89 (0.003) -0.59 (0.08) 0.93 (0.001) -0.85 (0.008) 0.89 (0.003) -0.59 (0.08) -0.57 (0.09) 0.67 (0.05) -0.86 (0.007) 0.52 (0.1)"
D18-1081,P15-2135,0,0.122451,"h human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found i"
D18-1081,C10-1152,0,0.541581,"ively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found in https://github.com/ eliorsulem/HSplit-corpus 2 Nevertheless, they are also used in contexts where struc738 Proceedings of the 2018 Conference on Empirical Methods in Natural Langua"
D18-1081,W14-1201,0,0.257452,"s in Natural Language Processing, pages 738–744 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU in TS. While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adˇ equacy. T-BLEU (Stajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality. Correlation with simplicity was not considered in this experiment. Xu et al. (2016) focused on lexical simplification, finding that BLEU obtains reasonable correlation for grammaticality and meaning preservation but fails to capture simplicity, even when multiple references are used. To our knowledge, no"
D18-1081,N18-1063,1,0.869674,"hat BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences. tural operations are involved (Nisioi et al., 2017; Sulem et al., 2018b). 739 readability;8 (3) SARI (Xu et al., 2016), which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LDSC ), which serves as a measure of conservatism.9 We explore two settings. In one (“Standard Reference Setting”, §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by X"
D18-1081,P18-1016,1,0.918364,"hat BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences. tural operations are involved (Nisioi et al., 2017; Sulem et al., 2018b). 739 readability;8 (3) SARI (Xu et al., 2016), which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LDSC ), which serves as a measure of conservatism.9 We explore two settings. In one (“Standard Reference Setting”, §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by X"
D18-1081,P12-2008,0,0.102861,"Missing"
D18-1081,W03-2317,0,0.253965,"Missing"
D18-1081,D11-1038,0,0.501083,"Missing"
D18-1081,P12-1107,0,0.703788,"Missing"
D18-1081,Q16-1029,0,0.276632,"Zhu et al. (2010), respectively (Narayan and Gardent, 2016). Sentence splitting is also the focus of the recently proposed Split-and Rephrase sub-task (Narayan et al., 2017; Aharoni and Goldberg, 2018), in which the automatic metric used is BLEU. For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus – HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by Xu et al. (2016), evaluating a variety of system outputs, as well as HSplit. The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion.2 Second, we experiment with HSplit as BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing"
D18-1081,D17-1062,0,0.230702,"ntences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found in https://github.com/ eliorsulem/HSplit-corpus 2 Nevertheless, they are also used in contexts where struc738 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738–744 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU in TS. While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adˇ equacy. T-BLEU (Stajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive"
E09-1021,W01-0504,0,0.109268,"language we randomly take 100 terms from the appropriate dictionary and set a limit as Limmwe = round(avg(length(w))) where length(w) is the number of words in term w. For languages like Chinese without inherent word segmentation, length(w) is the number of characters in w. While for many languages Limmwe = 1, some languages like Vietnamese usually require two words or more to express terms. 3 Our results in this paper support this conjecture. Another field indirectly related to our research is Machine Translation (MT). Many MT tasks require automated creation or improvement of dictionaries (Koehn and Knight, 2001). However, MT mainly deals with translation and disambiguation of words at the sentence or document level, while we translate whole concepts defined inde177 we select a translation of a term co-appearing most frequently with some translation of a different term of the same concept. We estimate how well translations of different terms are connected to each other. Let C = {Ci } be the given seed words for some concept. Let T r(Ci , n) be the n-th available translation of word Ci and Cnt(s) denote the web count of string s obtained by a search engine. Then we select translation T r(Ci ) according"
E09-1021,P99-1016,0,0.70072,"ciation for Computational Linguistics 175 At the same time, much work has been done on automatic lexical acquisition, and in particular, on the acquisition of concepts. The two main algorithmic approaches are pattern-based discovery, and clustering of context feature vectors. The latter represents word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Deerwester et al., 1990). Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. While a great effort has focused on improving the computational complexity of these methods (Gorman and Curran, 2006), they still remain data and computation intensive. frequently desired to consider available resources from different countries. Such resources are likely to be written in languages different from English. In order to obtain such resources, as before, it would be beneficial, given a concept definition in English, to obtain word lists denoting the same concept in different languages. In both cases a concep"
E09-1021,P98-2127,0,0.566333,"eedings of the 12th Conference of the European Chapter of the ACL, pages 175–183, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 175 At the same time, much work has been done on automatic lexical acquisition, and in particular, on the acquisition of concepts. The two main algorithmic approaches are pattern-based discovery, and clustering of context feature vectors. The latter represents word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Deerwester et al., 1990). Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. While a great effort has focused on improving the computational complexity of these methods (Gorman and Curran, 2006), they still remain data and computation intensive. frequently desired to consider available resources from different countries. Such resources are likely to be written in languages different from English. In order to obtain such resources, as before, it would be bene"
E09-1021,W02-0908,0,0.390988,"Missing"
E09-1021,P06-1038,1,0.944231,"ambiguating translations using web counts; (2) we retrieve from the web snippets where these translations co-appear; (3) we apply a patternbased concept extension algorithm for discovering additional terms from the retrieved data. A few recently proposed concept acquisition methods require only a handful of seed words (Davidov et al., 2007; Pasca and Van Durme, 2008). While these studies avoid some of the obstacles above, it still remains unconfirmed whether such methods are indeed language-independent. In the concept extension part of our algorithm we adapt our concept acquisition framework (Davidov and Rappoport, 2006; Davidov et al., 2007; Davidov and Rappoport, 2008a; Davidov and Rappoport, 2008b) to suit diverse languages, including ones without explicit word segmentation. In our evaluation we confirm the applicability of the adapted methods to 45 languages. 3.1 Concept words and sense selection We start from a set of words denoting a category in a source language. Thus we may use words like (apple, banana, ...) as the definition of fruits or (bear, wolf, fox, ...) as the definition of wild animals2 . Each of these words can be ambiguous. Multilingual dictionaries usually provide many translations, one"
E09-1021,P07-1030,1,0.857425,"rk, Section 3 details the algorithm, Section 4 describes the evaluation protocol and Section 5 presents our results. 2 The current major algorithmic approach for concept acquisition is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006). Most of these studies were done for English, while some show the applicability of their method to some other languages including Russian, Greek, Czech and French. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of the concept acquisition problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co-clustering and in (Etzioni et al., 2005) using predefined pattern types. Many Information Extraction"
E09-1021,P08-1079,1,0.868453,"etrieve from the web snippets where these translations co-appear; (3) we apply a patternbased concept extension algorithm for discovering additional terms from the retrieved data. A few recently proposed concept acquisition methods require only a handful of seed words (Davidov et al., 2007; Pasca and Van Durme, 2008). While these studies avoid some of the obstacles above, it still remains unconfirmed whether such methods are indeed language-independent. In the concept extension part of our algorithm we adapt our concept acquisition framework (Davidov and Rappoport, 2006; Davidov et al., 2007; Davidov and Rappoport, 2008a; Davidov and Rappoport, 2008b) to suit diverse languages, including ones without explicit word segmentation. In our evaluation we confirm the applicability of the adapted methods to 45 languages. 3.1 Concept words and sense selection We start from a set of words denoting a category in a source language. Thus we may use words like (apple, banana, ...) as the definition of fruits or (bear, wolf, fox, ...) as the definition of wild animals2 . Each of these words can be ambiguous. Multilingual dictionaries usually provide many translations, one or more for each sense. We need to select the appro"
E09-1021,P08-1003,0,0.496637,"Missing"
E09-1021,P08-1027,1,0.881578,"Missing"
E09-1021,P93-1024,0,0.686632,"Missing"
E09-1021,C02-1114,0,0.862709,"njunction queries which include 3–5 terms. For languages with Limmwe > 1, we also construct [P ref ix] C1 [Inf ix] C2 [P ostf ix] Ci are slots for concept terms. We allow up to Limmwe space-separated6 words to be in a single slot. Infix may contain punctuation, spaces, and up to Limmwe × 4 words. Prefix and Postfix are limited to contain punctuation characters and/or Limmwe words. Terms of the same concept frequently co-appear in lists. To utilize this, we introduce two additional List pattern types7 : [P ref ix] C1 [Inf ix] (Ci [Inf ix])+ (1) [Inf ix] (Ci [Inf ix])+ Cn [P ostf ix] (2) As in (Widdows and Dorow, 2002; Davidov and Rappoport, 2006), we define a pattern graph. Nodes correspond to terms and patterns to edges. If term pair (w1 , w2 ) appears in pattern P , we add nodes Nw1 , Nw2 to the graph and a directed edge EP (Nw1 , Nw2 ) between them. 4 6 As before, for languages without explicit space-based word separation Limmwe limits the number of characters instead. 7 (X)+ means one or more instances of X. Yahoo! allows restrictions for 42 languages. These are Yahoo! queries where enclosing words in “” means searching for an exact phrase and “*” means a wildcard for exactly one arbitrary word. 5 178"
E09-1021,W04-3234,0,0.0305535,"pattern-based methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006). Most of these studies were done for English, while some show the applicability of their method to some other languages including Russian, Greek, Czech and French. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of the concept acquisition problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co-clustering and in (Etzioni et al., 2005) using predefined pattern types. Many Information Extraction papers discover relationships between words using syntactic patterns (Riloff and Jones, 1999). Related work Unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind, in our task the algorithm should be able to deal well with a wide variety of target languages without any significant manual adaptations. While some of the proposed frameworks could potentially be language-independent, little research has been done to confirm it"
E09-1021,C92-2082,0,0.040249,"ionaries, we show that even with basic 1000 word dictionaries we achieve good performance. Modest time and data requirements allow the incorporation of our method in practical applications. In Section 2 we discuss related work, Section 3 details the algorithm, Section 4 describes the evaluation protocol and Section 5 presents our results. 2 The current major algorithmic approach for concept acquisition is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006). Most of these studies were done for English, while some show the applicability of their method to some other languages including Russian, Greek, Czech and French. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of the concept acquisition probl"
E09-1021,P06-1102,0,\N,Missing
E09-1021,C98-2122,0,\N,Missing
E09-1021,P06-1046,0,\N,Missing
E09-1021,I08-6012,0,\N,Missing
E09-1021,I08-2144,0,\N,Missing
K15-1026,D13-1167,0,0.0182713,"tation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by"
K15-1026,N09-1003,0,0.646382,"f-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models that go beyond the bag-ofwords assumption. Similarity vs. Association Most recent VSM research does not distinguish between association and similarity in a principled way, although notable exceptions exist. Turney (2012) constructed two VSMs with the explicit goal of capturing either similarity or association. A classifier that uses the output of these models was able to predict whether two concepts are associated, similar or both. Agirre et al. (2009) partitioned the wordsim353 dataset into two subsets, one focused on similarity and the other on association. They demonstrated the importance of the association/similarity distinction by showing that some VSMs perform relatively well on one subset while others perform comparatively better on the other. Recently, Hill et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset"
K15-1026,P13-1174,0,0.0338768,"Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Table 1: The six most frequent pattern candidates that co"
K15-1026,P14-1023,0,0.21428,"Missing"
K15-1026,S13-1005,0,0.0664835,"Missing"
K15-1026,N15-1100,0,0.342972,"d similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by defining an SP templa"
K15-1026,D14-1162,0,0.108133,"Abstract has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little infor"
K15-1026,C92-2082,0,0.395839,"l et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used the"
K15-1026,J15-4004,1,0.60606,"Missing"
K15-1026,P08-1119,0,0.0131529,"onymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language"
K15-1026,E14-1051,0,0.0131748,"two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T X log p(wt |wt−c , . . . , wt−1 , wt+1 , . . . , wt+c ) max t=1 where"
K15-1026,P14-2050,0,0.364695,"−c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN"
K15-1026,P14-2086,0,0.150344,"Missing"
K15-1026,D13-1193,1,0.834039,"approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand cra"
K15-1026,C14-1153,1,0.851528,"l tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Table 1: The six most frequent pattern candidates that contain exactly two wildcards and 1-3 words in our corpus. terns inclu"
K15-1026,N03-1033,0,0.0542377,"re 7 and 10. 6 www.cl.cam.ac.uk/˜fh295/simlex.html 7 code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (ρ) between the ranking derived from m’s scores and the one derived from the human scores. Model GloVe BOW CBOW Dep NNSE skip-gram Spearman’s ρ 0.35 0.423 0.43 0.436 0.455 0.462 SP(−) SP(+) 0.434 0.517 Joint (SP"
K15-1026,N13-1090,0,0.67305,"oiri@ie.technion.ac.il Abstract has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Parti"
K15-1026,C08-1114,0,0.0274924,"that cooccur in SPs are semantically similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In thi"
K15-1026,J13-3004,0,0.0208148,"lly similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2"
K15-1026,C12-1118,0,0.122551,"t=1 where T is the number of words in the corpus, and c is a pre-determined window size. Another word2vec architecture, skip-gram, aims to predict the past and future context given a word. Its objective is: T X X log p(wt+j |wt ) max t=1 −c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is"
K15-1026,nivre-etal-2006-maltparser,0,0.00942273,"tml 7 code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (ρ) between the ranking derived from m’s scores and the one derived from the human scores. Model GloVe BOW CBOW Dep NNSE skip-gram Spearman’s ρ 0.35 0.423 0.43 0.436 0.455 0.462 SP(−) SP(+) 0.434 0.517 Joint (SP(+) , skip-gram) Average Human Score 0.563"
K15-1026,C02-1114,0,0.129662,"ty, we propose an alternative, pattern-based, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space word representation. Our representation is constructed in the following way (Section 3). For each word w, we construct a vector v of size V , where V is the size of the lexicon. Each element in v represents the cooccurrence in SPs of w with another word in the lexicon, which results in a sparse word representation. Unlike most previous works that applied SPs to NLP tasks, we do not use a har"
K15-1026,D12-1111,0,0.166728,"that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. Thi"
K15-1026,C10-2028,1,\N,Missing
K15-1026,P06-1038,1,\N,Missing
K15-1026,P99-1008,0,\N,Missing
K17-1013,W13-3520,0,0.0499698,"eled conjll. If both are used, the label is simply conj=conjlr+conjll.7 Consequently, the individual context bags we use in all experiments are: subj, obj, comp, nummod, appos, nmod, acl, amod, prep, adv, compound, conjlr, conjll. 4.2 Training and Evaluation We run the algorithm for context configuration selection only once, with the SGNS training setup described below. Our main evaluation setup is presented below, but the learned configurations are tested in additional setups, detailed in Sect. 5. Training Data Our training corpus is the cleaned and tokenised English Polyglot Wikipedia data (Al-Rfou et al., 2013),8 consisting of approxi7 Given the coordination structure boys and girls, conjlr training pairs are (boys, girls_conj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of t"
K17-1013,P14-2131,0,0.0737811,"v et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still th"
K17-1013,P14-1023,0,0.0288632,"alian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and ve"
K17-1013,C10-1011,0,0.0100926,"nj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of the UD treebank annotated with UPOS tags. The data were then parsed with UD using the graph-based Mate parser v3.61 (Bohnet, 2010)10 with standard settings on TRAIN + DEV of the UD treebank. Evaluation We experiment with the verb pair (222 pairs), adjective pair (111 pairs), and noun pair (666 pairs) portions of SimLex-999. We report Spearman’s ρ correlation between the ranks derived from the scores of the evaluated models and the human scores. Our evaluation setup is borrowed from Levy et al. (2015): we perform 2-fold cross-validation, where the context configurations are optimised on a development set, separate from the unseen test data. Unless stated otherwise, the reported scores are always the averages of the 2 runs"
K17-1013,D14-1082,0,0.0395504,"at the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, rea"
K17-1013,W14-1618,0,0.0925589,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,P06-1038,1,0.647241,"sequential position of each context word. Given the example from Fig. 1, POSIT with the window size 2 extracts the following contexts for discovers: Australian_-2, scientist_-1, stars_+2, with_+1. - DEPS-All: All dependency links without any context selection, extracted from dependency-parsed data with prepositional arc collapsing. - COORD: Coordination-based contexts are used as fast lightweight contexts for improved representations of adjectives and verbs (Schwartz et al., 2016). This is in fact the conjlr context bag, a subset of DEPS-All. - SP: Contexts based on symmetric patterns (SPs, (Davidov and Rappoport, 2006; Schwartz et al., 2015)). For example, if the word X and the word 9 10 Context Group Adj Verb Noun conjlr (A+N+V) obj (N+V) prep (N+V) amod (A+N) compound (N) adv (V) nummod (-) 0.415 -0.028 0.188 0.479 -0.124 0.197 -0.142 0.281 0.309 0.344 0.058 -0.019 0.342 -0.065 0.401 0.390 0.387 0.398 0.416 0.104 0.029 Table 1: 2-fold cross-validation results for an illustrative selection of individual context bags. Results are presented for the noun, verb and adjective subsets of SimLex-999. Values in parentheses denote the class-specific initial pools to which each context is selected based on its ρ sc"
K17-1013,de-marneffe-etal-2014-universal,0,0.0293418,"Missing"
K17-1013,W08-1301,0,0.125848,"Missing"
K17-1013,N15-1184,0,0.0343743,"searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scopre stelle con telescopio nmod amod Australian nsubj scientist dobj discovers case stars with telescope prep:with Figure 1: Ext"
K17-1013,C12-1059,0,0.0242772,"ed in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS configuration is outscored by SP, but the margin is negligible. We als"
K17-1013,D15-1242,0,0.0909931,"bs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case do"
K17-1013,Q15-1016,0,0.665137,"trate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, S"
K17-1013,N15-1142,0,0.0798687,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,D15-1161,0,0.115421,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,P15-1145,0,0.038685,"ore useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Sci"
K17-1013,P13-2109,0,0.0606688,"Missing"
K17-1013,P13-2017,0,0.0177128,"rch algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based configuration space can be developed for multiple languages based on the same design principles. Indeed, in this work we show that the optimal configurations for English translate to improved representations in two additional languages, German and Italian. And so, given a (UD-)parsed training corpus, for each target word w with modifiers m1 , . . . , mk and a head h, the word w is paired with context elements m1 _r1 , . . . , mk"
K17-1013,N15-1050,0,0.0247132,"its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as contexts for V and A si"
K17-1013,N16-1118,0,0.353877,"ill considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when l"
K17-1013,P14-2050,0,0.0802645,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,Q17-1022,1,0.874088,"Missing"
K17-1013,J07-2002,0,0.313892,"Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended"
K17-1013,D14-1162,0,0.0756258,"figurations across languages: these configurations improve the SGNS performance when trained with German or Italian corpora and evaluated on class-specific subsets of the multilingual SimLex-999 (Leviant and Reichart, 2015), without any language-specific tuning. 2 Related Work Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic"
K17-1013,petrov-etal-2012-universal,0,0.227836,"path of the best-scoring lower-level configuration even if its score is lower than that of its origin. As we do not observe any significant improvement with this variant, we opt for the faster and simpler one. 5 https://bitbucket.org/yoavgo/word2vecf 6 SGNS for all models was trained using stochastic gradient descent and standard settings: 15 negative samples, global learning rate: 0.025, subsampling rate: 1e − 4, 15 epochs. Universal Dependencies as Labels The adopted UD scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the universal POS tagset (Petrov et al., 2012). It is straightforward to “translate” previous annotation schemes to UD (de Marneffe et al., 2014). Providing a consistently annotated inventory of categories for similar syntactic constructions across languages, the UD scheme facilitates representation learning in languages other than English, as shown in (Vuli´c and Korhonen, 2016; Vuli´c, 2017). Individual Context Bags Standard post-parsing steps are performed in order to obtain an initial list of individual context bags for our algorithm: (1) Prepositional arcs are collapsed ((Levy and Goldberg, 2014a; Vuli´c and Korhonen, 2016), see Fig."
K17-1013,P93-1034,0,0.74216,"Missing"
K17-1013,K15-1026,1,0.948135,"for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In thi"
K17-1013,N16-1060,1,0.312769,"). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In this work, we propose a simple yet effective framework for selecting context configurations, which yields improved representations for verbs, adjectives, and nouns. We s"
K17-1013,N03-1033,0,0.0161394,"ining Setup We first test whether the context configurations learned in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS co"
K17-1013,P10-1040,0,0.0566914,"ning time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not"
K17-1013,Q14-1020,0,0.0199341,"ified by similar adjectives”. In another example, “two verbs are similar if they are used as predicates of similar nominal subjects” (the nsubj and nsubjpass dependency relations). First, we have to define an expressive context configuration space that contains potential training configurations and is effectively decomposed so that useful configurations may be sought algorithmically. We can then continue by designing a search algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based conf"
K17-1013,E17-2065,1,0.823839,"Missing"
K17-1013,P16-2084,1,0.761929,"Missing"
K17-1013,Q15-1025,0,0.0546541,"dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope n"
K17-1013,D12-1086,0,0.0288956,"epresent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as"
K17-1013,P14-2089,0,0.0430435,"ctures, a particular dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers"
K18-2010,P13-1023,1,0.849468,"corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017) for tokenization, sentence splitting, part-of-speech tagging and morphological tagging. Our system is based on TUPA (Hershcovich et al., 2017, 2018, see §3), a transition-based UCCA parser. UCCA (Universal Conceptual Cognitive Annotation; Abend and Rappoport, 2013) is a cross-linguistic semantic annotation scheme, representing events, participants, attributes and relations in a directed acyclic graph (DAG) structure. UCCA allows reentrancy to support argument sharing, discontinuity (corresponding to non-projectivity in dependency formalisms) and non-terminal nodes (as opposed to dependencies, As their annotation in UD is not yet widespread and standardized, enhanced dependencies are not included in the evaluation metrics for UD parsing, and so TUPA’s ability to parse them is not reflected in the official shared task scores. However, we believe these enh"
K18-2010,Q17-1010,0,0.0156329,"6= root,  y 6;G x i(x) < i(y) Figure 6: The transition set of TUPA. We write the stack with its top to the right and the buffer with its head to the left. (·, ·)X denotes a primary X-labeled edge, and (·, ·)∗X a remote X-labeled edge. i(x) is the swap index (see §3.3). In addition to the specified conditions, the prospective child in an E DGE transition must not already have a primary parent. Parser state S made B G to feel very wel... nsubj We initialized randomly, except for the word embeddings, which are initialized with the 250K most frequent word vectors from fastText for each language (Bojanowski et al., 2017),5 pre-trained over Wikipedia and updated during training. We do not use word embeddings for languages without pre-trained fastText vectors (Ancient Greek, North Sami and Old French). To the feature embeddings, we concatenate numeric features representing the node height, number of (remote) parents and children, and the ratio between the number of terminals to total number of nodes in the graph G. Table 1 lists all feature used for the classifier. Numeric features are taken as they are, whereas categorical features are mapped to real-valued embedding vectors. For each non-terminal node, we sel"
K18-2010,P17-1104,1,0.852228,"ly a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017) for tokenization, sentence splitting, part-of-speech tagging and morphological tagging. Our system is based on TUPA (Hershcovich et al., 2017, 2018, see §3), a transition-based UCCA parser. UCCA (Universal Conceptual Cognitive Annotation; Abend and Rappoport, 2013) is a cross-linguistic semantic annotation scheme, representing events, participants, attributes and relations in a directed acyclic graph (DAG) structure. UCCA allows reentrancy to support argument sharing, discontinuity (corresponding to non-projectivity in dependency formalisms) and non-terminal nodes (as opposed to dependencies, As their annotation in UD is not yet widespread and standardized, enhanced dependencies are not included in the evaluation metrics for UD par"
K18-2010,D17-1009,0,0.0246567,"d nonterminal nodes. By converting UD trees and graphs to a UCCA-like DAG format, we train TUPA almost without modification on the UD parsing task. The generic nature of our approach lends itself naturally to multitask learning. Our code is available at https://github. com/CoNLL-UD-2018/HUJI. 1 Enhanced dependencies. Our method treats enhanced dependencies1 as part of the dependency graph, providing the first approach, to our knowledge, for supervised learning of enhanced UD parsing. Due to the scarcity of enhanced dependencies in UD treebanks, previous approaches (Schuster and Manning, 2016; Reddy et al., 2017) have attempted to recover them using languagespecific rules. Our approach attempts to learn them from data: while only a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017)"
K18-2010,P18-1035,1,0.881214,". Figure 2 demonstrates this format. Note that if the basic dependency is repeated in the enhanced graph (3:nsubj:pass in the example), we do not treat it as an enhanced dependency, so that the converted graph will only contain each edge once. In addition to the UD relations defined in the basic representations, enhanced dependencies may contain the relation ref, used for relative clauses. In addition, they may contain more specific relation subtypes, and optionally also case information. Unified DAG Format To apply TUPA to UD parsing, we convert UD trees and graphs into a unified DAG format (Hershcovich et al., 2018). The format consists of a rooted DAG, where the tokens are the terminal nodes.2 Edges are labeled (but not nodes), and are divided into primary and remote edges, where the primary edges form a tree (all nodes have at most one primary parent, and the root has none). Remote edges (denoted as dashed edges in Figure 1) Language-specific extensions and case information. Dependencies may contain language2 Our conversion code supports full conversion between UCCA and UD, among other representation schemes, and is publicly available at http://github.com/danielhers/semstr/ tree/master/semstr/conversio"
K18-2010,L16-1376,0,0.0585668,"reentrancy, discontinuity and nonterminal nodes. By converting UD trees and graphs to a UCCA-like DAG format, we train TUPA almost without modification on the UD parsing task. The generic nature of our approach lends itself naturally to multitask learning. Our code is available at https://github. com/CoNLL-UD-2018/HUJI. 1 Enhanced dependencies. Our method treats enhanced dependencies1 as part of the dependency graph, providing the first approach, to our knowledge, for supervised learning of enhanced UD parsing. Due to the scarcity of enhanced dependencies in UD treebanks, previous approaches (Schuster and Manning, 2016; Reddy et al., 2017) have attempted to recover them using languagespecific rules. Our approach attempts to learn them from data: while only a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka"
K18-2010,silveira-etal-2014-gold,0,0.0608387,"Missing"
K18-2010,Q16-1023,0,0.0551994,"ich et al., 2017). Table 2: Hyperparameter settings. 4.1 Training details The model is implemented using DyNet v2.0.3 (Neubig et al., 2017).6 Unless otherwise noted, we use the default values provided by the package. We use the same hyperparameters as used in previous experiments on UCCA parsing (Hershcovich et al., 2018), without any hyperparameter 6 Hyperparameters We use dropout (Srivastava et al., 2014) between MLP layers, and recurrent dropout (Gal and Ghahramani, 2016) between BiLSTM layers, both with p = 0.4. We also use word, lemma, coarseand fine-grained POS tag dropout with α = 0.2 (Kiperwasser and Goldberg, 2016): in training, the embedding for a feature value w is replaced α with a zero vector with a probability of #(w)+α , where #(w) is the number of occurrences of w observed. In addition, we use node dropout (Hershcovich et al., 2018): with a probability of 0.1 at each step, all features associated with a single node in the parser state are replaced with zero vectors. For optimization we use a minibatch size of 100, decaying all weights by 10−5 at each update, and train with stochastic gradient descent for 50 epochs with a learning rate of 0.1, followed by AMSGrad (Sashank J. Reddi, 2018) for 250 e"
K18-2010,W16-0906,0,0.335427,"only the root node and terminals exist. We assign the root a swap index of 0, and for each terminal, its position in the text (starting at 1). Whenever a node is created as a result Features. We use vector embeddings representing the words, lemmas, coarse (universal) POS tags and fine-grained POS tags, provided by UDPipe 1.2 during test. For training, we use the gold-annotated lemmas and POS tags. In addition, we use one-character prefix, three-character suffix, shape (capturing orthographic features, e.g., “Xxxx”) and named entity type, provided by spaCy;4 punctuation and gap type features (Maier and Lichte, 2016), and previously predicted edge labels and parser actions. These embeddings are 4 Constraints 5 http://spacy.io 107 http://fasttext.cc Nodes s0 s1 s2 s3 b0 b1 , b 2 , b 3 s0 l, s0 r, s1 l, s1 r, s0 ll, s0 lr, s0 rl, s0 rr, s1 ll, s1 lr, s1 rl, s1 rr s0 L, s0 R, s1 L, s1 R, b0 L, b0 R Edges s0 → s1 , s0 → b0 , s1 → s0 , b0 → s0 s0 → b0 , b0 → s0 Past actions a0 , a1 Global tuning on UD treebanks. wmtuepT#ˆ$xhqyPCIEMN wmtueT#ˆ$xhyN wmtueT#ˆ$xhy wmtueT#ˆ$xhyN wmtuT#ˆ$hPCIEMN wmtuT#ˆ$ wme#ˆ$ Hyperparameter Pre-trained word dim. Lemma dim. Coarse (universal) POS tag dim. Fine-grained POS tag dim. N"
K18-2010,L16-1680,0,0.0933326,"Missing"
K18-2010,K17-3009,0,0.0399393,", 2016; Reddy et al., 2017) have attempted to recover them using languagespecific rules. Our approach attempts to learn them from data: while only a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017) for tokenization, sentence splitting, part-of-speech tagging and morphological tagging. Our system is based on TUPA (Hershcovich et al., 2017, 2018, see §3), a transition-based UCCA parser. UCCA (Universal Conceptual Cognitive Annotation; Abend and Rappoport, 2013) is a cross-linguistic semantic annotation scheme, representing events, participants, attributes and relations in a directed acyclic graph (DAG) structure. UCCA allows reentrancy to support argument sharing, discontinuity (corresponding to non-projectivity in dependency formalisms) and non-terminal nodes (as opposed to dependencies,"
K18-2010,W03-3017,0,0.212918,"s are attached to its argument “peace” in the basic representation, and the argument itself is attached as an orphan. In the enhanced representation, all arguments are attached to the null node as if the elided predicate was present. While UCCA supports empty nodes without surface realization in the form of implicit units, previous work on UCCA parsing has removed these from the graphs. We do the same for UD parsing, dropping null nodes and their associated 3 General Transition-based DAG Parser We now turn to describing TUPA (Hershcovich et al., 2017, 2018), a general transition-based parser (Nivre, 2003). TUPA uses an extended set of transitions and features that supports reentrancies, discontinuities and non-terminal nodes. The parser state is composed of a buffer B of tokens 105 punct conj root punct obj cc nmod orphan nsubj iobj amod case punct I wish all happy holidays , and moreso E9.1 , punct peace on earth . obj advmod cc punct Figure 4: newsgroup-groups.google.com_GuildWars_086f0f64ab633ab3_ENG_20041111_1735000051 (UD_English-EWT), containing a null node (E9.1) and case information (nmod:on). root punct acl d hea acl:relcl d hea aux was made back t nc pu head ca se det robe that od ob"
K18-2010,W17-0411,0,0.0395841,"lemma, fine-grained tag, prefix and suffix features. We train this model for two epochs using stochastic gradient descent with a learning rate of 0.1 (we only trained this many epochs due to time constraints). 4.4 TUPA Results Official evaluation was done on the TIRA online platform (Potthast et al., 2014). Our system (named “HUJI”) ranked 24th in the LAS-F1 ranking (with an average of 53.69 over all test treebanks), 23rd by MLAS (average of 44.6) and 21st by BLEX (average of 48.05). Since our system only performs dependency parsing and not other pipeline tasks, we henceforth focus on LAS-F1 (Nivre and Fang, 2017) for evaluation. After the official evaluation period ended, we discovered several bugs in the conversion between the CoNLL-U format and the unified DAG format, which is used by TUPA for training and is output by it (see §2). We did not re-train TUPA on the training treebanks after fixing these bugs, but we did re-evaluate the already trained models on all test treebanks, and used the fixed code for converting their output to CoNLL-U. This yielded an unofficial average test LAS-F1 of 58.48, an improvement of 4.79 points over the official average score. In particular, for two test sets, ar_padt"
K18-2010,K18-2001,0,0.0386566,"Missing"
N16-1060,D14-1034,1,0.809186,"djectives. A number of evaluation sets consisting of word pairs scored by humans for semantic relations (mostly association and similarity) are in use for VSM evaluation. These include: RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), WordSim353 (Finkelstein et al., 2001), MEN (Bruni 1 Coor ∪ CoorC = DepAll, Coor ∩ CoorC = ∅ et al., 2014) and SimLex999 (Hill et al., 2014).2 Nouns are dominant in almost all of these datasets. For example, RG-65, MC-30 and WordSim353 consist of noun pairs almost exclusively. A few datasets contain pairs of verbs (Yang and Powers, 2006; Baker et al., 2014). The MEN dataset, although dominated by nouns, also contains verbs and adjectives. Nonetheless, the human judgment scores in these datasets reflect relatedness between words. In contrast, the recent SimLex999 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the"
N16-1060,P14-1023,0,0.0791358,"word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extracted from plain text. 4 Experiments Model. We keep the VSM fixed throughout our experiments, changing only the context type. This methodology allows us to evaluate the impact of different contexts on the VSM performance, as context choice is the only modeling decision that changes across experimental conditions. Our VSM is the word2vec skip-gram model (w2vSG, Mikolov et al. (2013a)), which obtains state-ofthe-art results on a variety of NLP tasks (Baroni et al., 2014). We employ the word2vec toolkit.4 For all context types other than BOW we use the word2vec package of (Levy and Goldberg, 2014),5 which augments the standard word2vec toolkit with code that allows arbitrary context definition. Experimental Setup. We experiment with the verb pair (222 pairs) and adjective pair (111 pairs) portions of SimLex999 (Hill et al., 2014). We report the Spearman ρ correlation between the ranks derived from the scores of the evaluated models and the human scores provided in SimLex999.6 We train the w2v-SG model with five different context types: (a) BOW contexts (SG-BOW"
N16-1060,P06-1038,1,0.911956,"Missing"
N16-1060,P07-1030,1,0.829956,"Missing"
N16-1060,C10-2028,1,0.689434,"Missing"
N16-1060,P13-1174,0,0.0126494,"Missing"
N16-1060,C92-2082,0,0.0548764,"Missing"
N16-1060,J15-4004,1,0.897757,"Missing"
N16-1060,P08-1119,0,0.0717551,"Missing"
N16-1060,P14-2050,0,0.60595,"2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Me"
N16-1060,N15-1098,0,0.0104165,"n, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyponymy and entailment. Rubinstein et al. (2015) showed that while state-of-the-art embeddings are successful at capturing taxonomic information (e.g., cow is an animal), they are much less successful in capturing attributive properties (bananas are yellow). In (Schwartz et al., 2015), we showed that word embeddings are unable to distinguish between pairs of words with opposite meanings (antonyms, e.g., good/bad). In this paper we study the difficulties of bag-of-words based word embeddings in representing verb similarity."
N16-1060,N16-1118,0,0.0192828,"4), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyp"
N16-1060,N13-1090,0,0.150625,"pairs of verbs (Yang and Powers, 2006; Baker et al., 2014). The MEN dataset, although dominated by nouns, also contains verbs and adjectives. Nonetheless, the human judgment scores in these datasets reflect relatedness between words. In contrast, the recent SimLex999 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extract"
N16-1060,H05-1105,0,0.0834407,"Missing"
N16-1060,P06-1033,0,0.0393585,"n our corpus. Indeed, due to the significant overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP cont"
N16-1060,W09-3811,0,0.0137806,"based model of Schwartz et al. (2015), with (SRR15) and without (SRR15− ) its antonym detection method. The two rightmost columns present the run time of the w2v-SG models in minutes (Time) and the number of context instances used by the model (#Cont.).10 For each SimLex999 portion, the score of the best w2v-SG model across context types is highlighted in bold font. ated by the word2vec script.7 Models (b)-(d) require the dependency parse trees of the corpus as input. To generate these trees, we employ the Stanford POS Tagger (Toutanova et al., 2003)8 and the stack version of the MALT parser (Nivre et al., 2009).9 The SP contexts are generated using the SPs extracted by the DR 06 algorithm from our training corpus (see Section 3). For BOW contexts, we experiment with three window sizes (2, 5 and 10) and report the best results (window size of 2 across conditions). For dependency based contexts we follow the standard convention in the literature: we consider the immediate heads and modifiers of the represented word. All models are trained with 500 dimensions, the default value of the word2vec script. Other hyperparameters were also set to the default values of the code packages. Results. Table 1 prese"
N16-1060,J07-2002,0,0.121164,"9 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recen"
N16-1060,D14-1162,0,0.116226,"Missing"
N16-1060,P15-2119,1,0.320289,"ect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyponymy and entailment. Rubinstein et al. (2015) showed that while state-of-the-art embeddings are successful at capturing taxonomic information (e.g., cow is an animal), they are much less successful in capturing attributive properties (bananas are yellow). In (Schwartz et al., 2015), we showed that word embeddings are unable to distinguish between pairs of words with opposite meanings (antonyms, e.g., good/bad). In this paper we study the difficulties of bag-of-words based word embeddings in representing verb similarity. 3 Symmetric Patterns (SPs) Lexico-syntactic patterns are templates of text that contain both words and wildcards (Hears"
N16-1060,P11-1067,1,0.763494,"due to the significant overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extract"
N16-1060,C12-1147,1,0.173989,"overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extracted from plain text. 4 Ex"
N16-1060,D13-1193,1,0.55537,"Missing"
N16-1060,C14-1153,1,0.901785,"Missing"
N16-1060,K15-1026,1,0.754717,"ts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2"
N16-1060,N03-1033,0,0.0501082,"s (see text). The bottom lines present the results of the count SP-based model of Schwartz et al. (2015), with (SRR15) and without (SRR15− ) its antonym detection method. The two rightmost columns present the run time of the w2v-SG models in minutes (Time) and the number of context instances used by the model (#Cont.).10 For each SimLex999 portion, the score of the best w2v-SG model across context types is highlighted in bold font. ated by the word2vec script.7 Models (b)-(d) require the dependency parse trees of the corpus as input. To generate these trees, we employ the Stanford POS Tagger (Toutanova et al., 2003)8 and the stack version of the MALT parser (Nivre et al., 2009).9 The SP contexts are generated using the SPs extracted by the DR 06 algorithm from our training corpus (see Section 3). For BOW contexts, we experiment with three window sizes (2, 5 and 10) and report the best results (window size of 2 across conditions). For dependency based contexts we follow the standard convention in the literature: we consider the immediate heads and modifiers of the represented word. All models are trained with 500 dimensions, the default value of the word2vec script. Other hyperparameters were also set to"
N16-1060,J06-3003,0,0.0359954,"Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination"
N16-1060,C08-1114,0,0.0254482,"gs. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which"
N16-1060,C02-1114,0,0.196931,"Missing"
N18-1063,P13-1023,1,0.939995,"y judgments were not considered in this experiment. In this paper we collect human judgments for grammaticality, meaning preservation and structural simplicity. To our knowledge, this is the first work to target structural simplicity evaluation, and it does so both through elicitation of human judgments and through the definition of SAMSA. Xu et al. (2016) were the first to propose two evaluation measures tailored for simplification, focusing on lexical simplification. The first metric is FKBLEU, a combination of iBLEU (Sun In this paper we use the UCCA scheme for defining semantic structure (Abend and Rappoport, 2013). UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for machine translation evaluation (Birch et al., 2016) (Section 2). We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR (Banarescu et al., 2013) or Discourse Representation Structures (Kamp, 1981), as used by Narayan and Gardent (2014). We experiment with SAMSA both where semantic annotation is carried out manually, and where it is carried out by a parser. See Section 4. We conduct hu"
N18-1063,P17-1008,1,0.826314,"ve Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme based on typological (Dixon, 2010b,a, 2012) and cognitive (Langacker, 2008) theories which aims to represent the main semantic phenomena in the text, abstracting away from syntactic detail. UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph (including the words of the text) or to several elements jointly viewed as a single entity according to some semantic or cognitive consideration. Unlike AMR, UCCA semantic units are directly anchored in the text (Abend and Rappoport, 2017; Birch et al., 2016), which allows easy inclusion of a word-toword alignment in the metric model (Section 4). The SAMSA Metric SAMSA’s main premise is that a structurally correct simplification is one where: (1) each sentence contains a single event from the input (UCCA Scene), (2) the main relation of each of the events and their participants are retained in the output. For example, consider “John wrote a book. I read that book.” as a simplification of “I read the book that John wrote.”. Each output sentence contains one Scene, which has the same Scene elements as the source, and would thus"
N18-1063,P11-2117,0,0.106706,"ntence, and the number of input sentences split by each of the systems. Levenshtein distances are taken as negative in order to capture similarity between the output and source/reference. The measure with the highest correlation in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Importantly, the simplicity score does not explicitly refer to the output’s structural simplicity, but rather to its readability. We focus on the overall human score, and compare it to SAMSA. Since diff"
N18-1063,P17-4019,1,0.718902,"ion, compared to the input? Is the output simpler than the input, ignoring the complexity of the words? 6 Table 1: Questions for the human evaluation 6 Human Score Computation Experimental Setup We further compute SAMSA for the 100 sentences of the PWKP test set and the corresponding system outputs. Experiments are conducted in two settings: (1) a semi-automatic setting where Each input-output pair was rated by all five annotators. Other questions appeared without any example. 690 UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software (Abend et al., 2017), and according to the standard annotation guidelines;8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (Hershcovich et al., 2017). Sentence segmentation of the outputs was carried out using the NLTK package (Loper and Bird, 2002). For word alignments, we used the aligner of Sultan et al. (2014).9 7 tural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although"
N18-1063,W11-2107,0,0.0518887,"8 to 20), randomly selected from the test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correlations for METEOR, T-BLEU and TINE with respect to grammaticality. Human simplicity judgments were not considered in this experiment. In this paper we collect hum"
N18-1063,W13-2322,0,0.058209,"ere the first to propose two evaluation measures tailored for simplification, focusing on lexical simplification. The first metric is FKBLEU, a combination of iBLEU (Sun In this paper we use the UCCA scheme for defining semantic structure (Abend and Rappoport, 2013). UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for machine translation evaluation (Birch et al., 2016) (Section 2). We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR (Banarescu et al., 2013) or Discourse Representation Structures (Kamp, 1981), as used by Narayan and Gardent (2014). We experiment with SAMSA both where semantic annotation is carried out manually, and where it is carried out by a parser. See Section 4. We conduct human rating experiments and compare the resulting system rankings with those predicted by SAMSA. We find that SAMSA’s rankings obtain high correlations with human rankings, and compare favorably to existing referencebased measures for TS. Moreover, our results show that existing measures, which mainly target lexical simplification, are ill-suited to predic"
N18-1063,W03-1004,0,0.0773081,"m, and reflect how well the tendency of the systems to introduce changes to the input correlates with the human rankings. The block includes -LDSC , the negative Levenshtein distance from the source sentence, and the number of input sentences split by each of the systems. Levenshtein distances are taken as negative in order to capture similarity between the output and source/reference. The measure with the highest correlation in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Imp"
N18-1063,R13-2011,0,0.549816,"n. In most of the work investigating the structural operations involved in text simplification, both in rulebased systems (Siddharthan and Angrosh, 2014) and in statistical systems (Zhu et al., 2010; Woodsend and Lapata, 2011), the structures that were considered were syntactic. Narayan and Gardent (2014, 2016) proposed to use semantic structures in the simplification model, in particular in order to avoid splits and deletions which are inconsistent with the semantic structures. SAMSA identifies such incoherent splits, e.g., a split of a phrase describing a single event, and penalizes them. ˇ Glavas and Stajner (2013) presented two simplification systems based on event extraction. One of them, named Event-wise Simplification, transforms each factual event motion into a separate sentence. This approach fits with SAMSA’s stipulation, that an optimal structural simplification is one where each (UCCA-) event in the input sentence is assigned a separate output sentence. However, unlike in their model, SAMSA stipulates that not only should multiple events evoked by a verb in the same sentence be avoided in a simplification, but penalizes sentences containing multiple events evoked by a lexical item of any catego"
N18-1063,P11-2087,0,0.12176,"Missing"
N18-1063,P15-2011,0,0.115809,"Missing"
N18-1063,H93-1040,0,0.582465,"Missing"
N18-1063,P17-1104,1,0.799978,"entered the house is John”. They can also be one of the Participants of another Scene, for example, “he will be late” in the sentence: “He said he will be late”. In the other cases, the Scenes are annotated as parallel Scenes (H) which can be linked by a Linker (L): “WhenL [he will arrive at home]H , [he will call them]H ”. 4.1 Matching Scenes to Sentences SAMSA is based on two external linguistic resources. One is a semantic annotation (UCCA in our experiments) of the source side, which can be carried out either manually or automatically, using the TUPA parser3 (Transition-based UCCA parser; Hershcovich et al., 2017) for UCCA. UCCA decomposes each sentence s into a set of Scenes {sc1 , sc2 , .., scn }, where each scene sci contains a main relation mri (sub-span of sci ) and a set of zero or more participants Ai . The second resource is a word-to-word alignment A between the words in the input and one or zero words in the output. The monolingual alignment thus permits SAMSA not to penalize outputs that involve lexical substitutions (e.g., “comUnit Centers. With regard to units which are not Scenes, the category Center denotes the semantic 3 688 https://github.com/danielhers/tupa Given the input sentence Sc"
N18-1063,P14-2075,0,0.0305553,"ion in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Importantly, the simplicity score does not explicitly refer to the output’s structural simplicity, but rather to its readability. We focus on the overall human score, and compare it to SAMSA. Since different systems were used to simplify different portions of the input, correlation is taken at the sentence level. We use the same implementations of SAMSA. Manual UCCA annotation is here performed by one of the authors"
N18-1063,N15-1022,0,0.123516,"res the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. They found that FKBLEU and even more so SARI correlate better with human simplicity judgments than BLEU. On the other hand, BLEU (with multiple references) outperforms the other metrics on the dimensions of grammaticality and meaning preservation. As the Parallel Wikipedia Corpus (PWKP), usually used in simplification research, has been shown to contain a large portion of problematic simplifications (Xu et al., 2015; Hwang et al., 2015), Xu et al. (2016) further proposed to use multiple references (instead of a single reference) in the evaluation measures. SAMSA addresses this issue by directly comparing the input and the output of the simplification system, without requiring manually curated references. Semantic Structures in Text Simplification. In most of the work investigating the structural operations involved in text simplification, both in rulebased systems (Siddharthan and Angrosh, 2014) and in statistical systems (Zhu et al., 2010; Woodsend and Lapata, 2011), the structures that were considered were syntactic. Naray"
N18-1063,N18-2020,1,0.655694,"variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions like nominal argument structures are not addressed by the SRL annotation, Birch et al. (2016) proposed HUME, a human evaluation metric based on UCCA, using the semantic annotation only on the source side when comparing it to the output. We differ from HUME in proposing an automatic metric, tackling monolingual text simplification, rather than MT. The UCCA annotation has also been recently used for the evaluation of Grammatical Error Correction (GEC). The USIM metric (Choshen and Abend, 2018) measures the semantic faithfulness of the output to the source by comparing their respective UCCA graphs. and Zhou, 2012), originally proposed for evaluating paraphrase generation by comparing the output both to the reference and to the input, and of the Flesch-Kincaid Index (FK), a measure of the readability of the text (Kincaid et al., 1975). The second one is SARI (System output Against References and against the Input sentence) which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, delete"
N18-1063,P06-1048,0,0.0541134,"easures for Text-to-text Generation. Other than measuring the number of splits (Narayan and Gardent, 2014, 2016), which only assesses the frequency of this operation and not its quality, no structural measures were previously proposed for the evaluation of structural simplification. The need for such a measure is pressing, given recent interest in structural simplification, e.g., in the Split and Rephrase task (Narayan et al., 2017), which focuses on sentence splitting. In the task of sentence compression, which is similar to simplification in that they both involve deletion and paraphrasing, Clarke and Lapata (2006) showed that a metric that uses syntactic dependencies better correlates with human evaluation than a metric based on surface sub-strings. Toutanova et al. (2016) found that structure-aware metrics obtain higher correlation with human evaluation over bigram-based metrics, in particular with grammaticality judgments, but that they do not significantly outperform bigram-based metrics on any parameter. Both Clarke and Lapata (2006) and Toutanova et al. (2016) use reference-based metrics that use syntactic structure on both the output and the references. SAMSA on the other hand 687 head of the uni"
N18-1063,P14-2124,0,0.020749,"esents SAMSA. Section 5 details the collection of human judgments. Our experimental setup for comparing our human and automatic rankings is given in Section 6, and results are given in Section 7, showing superior results for SAMSA. A discussion on the results is presented in Section 8. 686 uses linguistic annotation only on the source side, with semantic structures instead of syntactic ones. Semantic structures were used in MT evaluation, for example in the MEANT metric (Lo et al., 2012), which compares the output and the reference sentences, both annotated using SRL (Semantic Role Labeling). Lo et al. (2014) proposes the XMEANT variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions like nominal argument structures are not addressed by the SRL annotation, Birch et al. (2016) proposed HUME, a human evaluation metric based on UCCA, using the semantic annotation only on the source side when comparing it to the output. We differ from HUME in proposing an automatic metric, tackling monolingual text simplification, rather than MT. The UCCA annotation has also been recently used for the evaluation of Grammatical Error Correction (GEC"
N18-1063,W11-2112,0,0.0226085,"2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correlations for METEOR, T-BLEU and TINE with respect to grammaticality. Human simplicity judgments were not considered in this experiment. In this paper we collect human judgments for grammaticality, meaning preservation"
N18-1063,W12-3129,0,0.0214234,"d focuses only on structural aspects of simplicity. Section 2 presents previous work. Section 3 discusses UCCA. Section 4 presents SAMSA. Section 5 details the collection of human judgments. Our experimental setup for comparing our human and automatic rankings is given in Section 6, and results are given in Section 7, showing superior results for SAMSA. A discussion on the results is presented in Section 8. 686 uses linguistic annotation only on the source side, with semantic structures instead of syntactic ones. Semantic structures were used in MT evaluation, for example in the MEANT metric (Lo et al., 2012), which compares the output and the reference sentences, both annotated using SRL (Semantic Role Labeling). Lo et al. (2014) proposes the XMEANT variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions like nominal argument structures are not addressed by the SRL annotation, Birch et al. (2016) proposed HUME, a human evaluation metric based on UCCA, using the semantic annotation only on the source side when comparing it to the output. We differ from HUME in proposing an automatic metric, tackling monolingual text simplificat"
N18-1063,E14-1076,0,0.505425,"ation.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations 2 We do not consider other structural operations, such as passive to active transformations (Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/eliorsulem/SAMSA. 685 Proceedings of NAACL-HLT 2018, pages 685–696 c New Orleans,"
N18-1063,W02-0109,0,0.0283945,"the corresponding system outputs. Experiments are conducted in two settings: (1) a semi-automatic setting where Each input-output pair was rated by all five annotators. Other questions appeared without any example. 690 UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software (Abend et al., 2017), and according to the standard annotation guidelines;8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (Hershcovich et al., 2017). Sentence segmentation of the outputs was carried out using the NLTK package (Loper and Bird, 2002). For word alignments, we used the aligner of Sultan et al. (2014).9 7 tural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although when restricted to multiScene sentences, the correlation for SAMSA (semi-automatic) is higher (0.89, p = 0.009 and 0.77, p = 0.04). The highest correlation for meaning preservation is obtained by SAMSAabl which provides further evidence that the retainment of semanti"
N18-1063,W09-0441,0,0.0446774,"he test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correlations for METEOR, T-BLEU and TINE with respect to grammaticality. Human simplicity judgments were not considered in this experiment. In this paper we collect human judgments for grammatical"
N18-1063,W14-5603,0,0.247926,"less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA’s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the"
N18-1063,P14-1041,0,0.455115,"Missing"
N18-1063,Q14-1018,0,0.0288912,"settings: (1) a semi-automatic setting where Each input-output pair was rated by all five annotators. Other questions appeared without any example. 690 UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software (Abend et al., 2017), and according to the standard annotation guidelines;8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (Hershcovich et al., 2017). Sentence segmentation of the outputs was carried out using the NLTK package (Loper and Bird, 2002). For word alignments, we used the aligner of Sultan et al. (2014).9 7 tural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although when restricted to multiScene sentences, the correlation for SAMSA (semi-automatic) is higher (0.89, p = 0.009 and 0.77, p = 0.04). The highest correlation for meaning preservation is obtained by SAMSAabl which provides further evidence that the retainment of semantic structures is a strong predictor of meaning preservation (Sulem"
N18-1063,W16-6620,0,0.357547,"six recent simplification systems for these sentences:5 (1) TSM (Zhu et al., 2010) using Tree-Based SMT, (2) RevILP (Woodsend and Lapata, 2011) using Quasi-Synchronous Grammars, (3) PBMT-R (Wubben et al., 2012) using PhraseBased SMT, (4) Hybrid (Narayan and Gardent, 4 In some cases, the unit u can be a sequence of centers (if there are several minimal centers). In these cases, 1s (u) returns 1 iff the condition holds for all centers. 5 All the data can be found here: http: //homepages.inf.ed.ac.uk/snaraya2/data/ simplification-2016.tgz. 689 5.2 2014), a supervised system using DRS, (5) UNSUP (Narayan and Gardent, 2016), an unsupervised system using DRS, and (6) Split-Deletion (Narayan and Gardent, 2016), the unsupervised system with only structural operations. All these systems explicitly address at least one type of structural simplification operation. The last system, Split-Deletion, performs only structural (i.e., no lexical) operations. It is thus an interesting test case for SAMSA since here the aligner can be replaced by a simple match between identical words. In total we obtain 600 system outputs from the six systems, as well as 100 sentences from the simple Wikipedia side of the corpus, which serve"
N18-1063,P12-2008,0,0.306004,"Missing"
N18-1063,D16-1033,0,0.121306,"on and not its quality, no structural measures were previously proposed for the evaluation of structural simplification. The need for such a measure is pressing, given recent interest in structural simplification, e.g., in the Split and Rephrase task (Narayan et al., 2017), which focuses on sentence splitting. In the task of sentence compression, which is similar to simplification in that they both involve deletion and paraphrasing, Clarke and Lapata (2006) showed that a metric that uses syntactic dependencies better correlates with human evaluation than a metric based on surface sub-strings. Toutanova et al. (2016) found that structure-aware metrics obtain higher correlation with human evaluation over bigram-based metrics, in particular with grammaticality judgments, but that they do not significantly outperform bigram-based metrics on any parameter. Both Clarke and Lapata (2006) and Toutanova et al. (2016) use reference-based metrics that use syntactic structure on both the output and the references. SAMSA on the other hand 687 head of the unit. For example, “dogs” is the center of the expression “big brown dogs” and “box” is the center of “in the box”. There could be more than one Center in a non-Scen"
N18-1063,C16-2036,0,0.119144,"the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA’s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations"
N18-1063,P15-2135,0,0.172626,", the negative Levenshtein distance from the source sentence, and the number of input sentences split by each of the systems. Levenshtein distances are taken as negative in order to capture similarity between the output and source/reference. The measure with the highest correlation in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Importantly, the simplicity score does not explicitly refer to the output’s structural simplicity, but rather to its readability. We focus on th"
N18-1063,W14-1201,0,0.486272,"grammaticality (or fluency), meaning preservation (or adequacy) and simplicity. Human evaluation is usually carried out with a small number of sentences (18 to 20), randomly selected from the test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correla"
N18-1063,D11-1038,0,0.807813,"ased measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations 2 We do not consider other structural operations, such as passive to active transformations (Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/eliorsulem/SAMSA. 685 Proceed"
N18-1063,P12-1107,0,0.426619,"Missing"
N18-1063,Q15-1021,0,0.637778,"Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/eliorsulem/SAMSA. 685 Proceedings of NAACL-HLT 2018, pages 685–696 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Section 9 presents experiments with SAMSA on the QATS evaluation benchmark. the right granularity. Third, defining a semantic measure that does not require references avoids the difficulties incurred by their non-uniqueness, and the difficulty in collecting high quality references, as reported by Xu et al. (2015) and by Narayan and Gardent (2014) with respect to the Parallel Wikipedia Corpus (PWKP; Zhu et al., 2010). SAMSA is further motivated by its use of semantic annotation only on the source side, which allows to evaluate multiple systems using same source-side annotation, and avoids the need to parse system outputs, which can be garbled. 2 Related Work Evaluation Metrics for Text Simplification. As pointed out by Xu et al. (2016), many of the existing measures for TS evaluation do not generalize across systems, because they fail to capture the combined effects of the different simplification oper"
N18-1063,Q16-1029,0,0.517901,"asure that does not require references avoids the difficulties incurred by their non-uniqueness, and the difficulty in collecting high quality references, as reported by Xu et al. (2015) and by Narayan and Gardent (2014) with respect to the Parallel Wikipedia Corpus (PWKP; Zhu et al., 2010). SAMSA is further motivated by its use of semantic annotation only on the source side, which allows to evaluate multiple systems using same source-side annotation, and avoids the need to parse system outputs, which can be garbled. 2 Related Work Evaluation Metrics for Text Simplification. As pointed out by Xu et al. (2016), many of the existing measures for TS evaluation do not generalize across systems, because they fail to capture the combined effects of the different simplification operations. The two main directions pursued are direct human judgments and automatic measures borrowed from machine translation (MT) evaluation. Human judgments generally include grammaticality (or fluency), meaning preservation (or adequacy) and simplicity. Human evaluation is usually carried out with a small number of sentences (18 to 20), randomly selected from the test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016)"
N18-1063,C10-1152,0,0.710055,"isting reference-based measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations 2 We do not consider other structural operations, such as passive to active transformations (Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/el"
N19-1047,P17-1104,1,0.942946,"eventive and relational nouns from concrete nouns may allow improving it even further. In the similar case of compounds, lexical resources for light verbs and idioms may increase performance. Experimental Setup Data. In addition to the UCCA EWT data (§3), we use the reviews section of the UD v2.3 English_EWT treebank (Nivre et al., 2018),12 annotated over the exact same sentences. We additionally use UDPipe v1.2 (Straka et al., 2016; Straka and Straková, 2017), trained on English_EWT,13 for feature extraction. We apply the extended converter to UD as before (§4.2). Parser. We train TUPA v1.3 (Hershcovich et al., 2017, 2018a) on the UCCA EWT data, with the standard train/development/test split. TUPA uses POS tags and syntactic dependencies as features. We experiment both with using gold UD for feature extraction, and with using UDPipe outputs. Evaluation by gold-standard UD. UCCA evaluation is generally carried out by considering a predicted unit as correct if there is a gold unit that matches it in terminal yield and labels. Precision, Recall and F-score (F1) are computed accordingly. For the fine-grained analysis, we split the goldstandard, predicted and matched UCCA units according to the labels of the"
N19-1047,P13-1023,1,0.892441,"formation encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD relations, and which are unmatched. Most content differences are due to (§5): Syntactic analysis plays an important role in semantic parsing, but the nature of this role remains a topic of ongoing debate. The debate has been constrained by the scarcity of empirical comparative studies between syntactic and semantic schemes, which hinders the developme"
N19-1047,P18-1035,1,0.760525,"e phrase and the same element modifying the whole phrase (de Marneffe and Nivre, 2019). An example UD tree is given in Figure 2. UD relations will be written in typewriter font. To facilitate comparison between UCCA and UD, we first assimilate the graphs by abstracting away from formalism differences, obtaining a similar graph format for both schemes. We then match pairs of nodes in the converted UD and UCCA trees if they share all terminals in their yields. UD annotates bi-lexical dependency trees, while UCCA graphs contain non-terminal nodes. In §4.1, we outline the unified DAG converter by Hershcovich et al. (2018a,b),7 which we use to reach a common format. In §4.2, we describe a number of extensions to the converter, which abstract away from further non-content differences. head ca se t nc Basic Conversion Figure 3 presents the same tree from Figure 2 after conversion. The converter adds one pre-terminal per token, and attaches them according to the original dependency tree: traversing it from the root, for each head it creates a non-terminal parent with the edge label head, and adds the dependents as children of the created non-terminal. Relation subtypes are stripped, leaving only universal relatio"
N19-1047,P17-1008,1,0.913294,"Missing"
N19-1047,K18-2010,1,0.86131,"e phrase and the same element modifying the whole phrase (de Marneffe and Nivre, 2019). An example UD tree is given in Figure 2. UD relations will be written in typewriter font. To facilitate comparison between UCCA and UD, we first assimilate the graphs by abstracting away from formalism differences, obtaining a similar graph format for both schemes. We then match pairs of nodes in the converted UD and UCCA trees if they share all terminals in their yields. UD annotates bi-lexical dependency trees, while UCCA graphs contain non-terminal nodes. In §4.1, we outline the unified DAG converter by Hershcovich et al. (2018a,b),7 which we use to reach a common format. In §4.2, we describe a number of extensions to the converter, which abstract away from further non-content differences. head ca se t nc Basic Conversion Figure 3 presents the same tree from Figure 2 after conversion. The converter adds one pre-terminal per token, and attaches them according to the original dependency tree: traversing it from the root, for each head it creates a non-terminal parent with the edge label head, and adds the dependents as children of the created non-terminal. Relation subtypes are stripped, leaving only universal relatio"
N19-1047,W13-2322,0,0.0885814,"Missing"
N19-1047,basile-etal-2012-developing,0,0.0605929,"Missing"
N19-1047,L16-1630,0,0.0465303,"y consistent and coarse-grained treebank annotation. Formally, UD uses bi-lexical trees, with edge labels representing syntactic relations. One aspect of UD similar to UCCA is its preference of lexical (rather than functional) heads. For example, in auxiliary verb constructions (e.g., “is eating”), UD marks the lexical verb (eating) as the head, while other dependency schemes may select the auxiliary is instead. While the approaches are largely inter-translatable (Schwartz et al., 2012), lexical head schemes are more similar in form to semantic schemes, such as UCCA and semantic dependencies (Oepen et al., 2016). Being a dependency representation, UD is structurally underspecified in an important way: it is not possible in UD to mark the distinction between an element modifying the head of the phrase and the same element modifying the whole phrase (de Marneffe and Nivre, 2019). An example UD tree is given in Figure 2. UD relations will be written in typewriter font. To facilitate comparison between UCCA and UD, we first assimilate the graphs by abstracting away from formalism differences, obtaining a similar graph format for both schemes. We then match pairs of nodes in the converted UD and UCCA tree"
N19-1047,Q16-1010,0,0.0164383,"l number of instances of each UD relation; of them, matching UCCA units in gold-standard and in TUPA’s predictions; their intersection, with/without regard to categories. (c) Percentage of correctly categorized edges; for comparison, percentage of most frequent category (see Table 3). (d) Average number of words in corresponding terminal yields. quire a clause to convey the same meaning. The mapping would therefore be more direct using a semantic representation, and we would benefit from breaking the utterance into two Scenes. 8 les the transduction of syntactic structures into semantic ones. Reddy et al. (2016) proposed a rulebased method for converting UD to logical forms. Stanovsky et al. (2016) converted Stanford dependency trees into proposition structures (P ROP S), abstracting away from some syntactic detail. Related Work 9 The use of syntactic parsing as a proxy for semantic structure has a long tradition in NLP. Indeed, semantic parsers have leveraged syntax for output space pruning (Xue and Palmer, 2004), syntactic features (Gildea and Jurafsky, 2002; Hershcovich et al., 2017), joint modeling (Surdeanu et al., 2008; Hajiˇc et al., 2009), and multi-task learning (Swayamdipta et al., 2016, 20"
N19-1047,D16-1134,1,0.839883,"and Rappoport, 2017). A methodology for comparing syntactic and semantic treebanks can also support fine-grained error analysis of semantic parsers, as illustrated by Szubert et al. (2018) for AMR (Banarescu et al., 2013). To demonstrate the utility of our comparison methodology, we perform fine-grained error analysis on UCCA parsing, according to UD relations (§6). Results highlight challenges for current parsing technology, and expose cases where UCCA parsers may benefit from modeling syntactic structure more directly.2 2 A C D E F G H et al., 2018b), and text-to-text generation evaluation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). Formally, UCCA structures are directed acyclic graphs (DAGs) whose nodes (or units) correspond either to words, or to elements viewed as a single entity according to some semantic or cognitive consideration. Edges are labeled, indicating the role of a child in the relation the parent represents. Figure 1 shows a legend of UCCA abbreviations. A Scene is UCCA’s notion of an event or a frame, and is a description of a movement, an action or a state which persists in time. Every Scene contains one primary relation, which can be either a Process or a"
N19-1047,C18-1233,0,0.0405077,"Missing"
N19-1047,N18-2020,1,0.759095,". A methodology for comparing syntactic and semantic treebanks can also support fine-grained error analysis of semantic parsers, as illustrated by Szubert et al. (2018) for AMR (Banarescu et al., 2013). To demonstrate the utility of our comparison methodology, we perform fine-grained error analysis on UCCA parsing, according to UD relations (§6). Results highlight challenges for current parsing technology, and expose cases where UCCA parsers may benefit from modeling syntactic structure more directly.2 2 A C D E F G H et al., 2018b), and text-to-text generation evaluation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). Formally, UCCA structures are directed acyclic graphs (DAGs) whose nodes (or units) correspond either to words, or to elements viewed as a single entity according to some semantic or cognitive consideration. Edges are labeled, indicating the role of a child in the relation the parent represents. Figure 1 shows a legend of UCCA abbreviations. A Scene is UCCA’s notion of an event or a frame, and is a description of a movement, an action or a state which persists in time. Every Scene contains one primary relation, which can be either a Process or a State. Scenes may contai"
N19-1047,W14-2908,0,0.0212426,"Missing"
N19-1047,E17-1051,0,0.0603032,"Missing"
N19-1047,J02-3001,0,0.194352,"Missing"
N19-1047,L16-1376,0,0.0260166,"Parisg Figure 3: Converted UD tree. Non-terminals and head edges are introduced by the unified DAG converter. obl case moved movedg tog he John Johng ad Afterg graduation ,g he , nsubj graduation nsubj ad After punct he case obl obl root obl Comparison Methodology 6 Our data is available at https://github.com/ UniversalConceptualCognitiveAnnotation/ UCCA_English-EWT. 7 https://github.com/huji-nlp/semstr http://bit.ly/ucca_guidelines_v2 480 Reentrancies. Remote edges in UCCA enable reentrancy, forming a DAG together with primary edges. UD allows reentrancy when including enhanced dependencies (Schuster and Manning, 2016),8 which form (bi-lexical) graphs, representing phenomena such as predicate ellipsis (e.g., gapping), and shared arguments due to coordination, control, raising and relative clauses. UCCA is more inclusive in its use of remote edges, and accounts for the entire class of implicit arguments termed Constructional Null Instantiation in FrameNet (Ruppenhofer et al., 2016). For example, in “The Pentagon is bypassing official US intelligence channels [...] in order to create strife” (from EWT), remote edges mark Pentagon as a shared argument of bypassing and create. Another example is “if you call fo"
N19-1047,C12-1147,1,0.868281,"Missing"
N19-1047,P18-1192,0,0.0216691,"are still in many cases the backbone of text understanding systems. Such studies are essential for (1) determining whether and to what extent semantic methods should be adopted for text understanding applications; (2) defining better inductive biases for semantic parsers, and allowing better use of information encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD relations, and which are unmatched. Mos"
N19-1047,L16-1680,0,0.0439899,"Missing"
N19-1047,K17-3009,0,0.0397875,"Missing"
N19-1047,D18-1548,0,0.0289628,"m syntactic ones, which are still in many cases the backbone of text understanding systems. Such studies are essential for (1) determining whether and to what extent semantic methods should be adopted for text understanding applications; (2) defining better inductive biases for semantic parsers, and allowing better use of information encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD relations, and which a"
N19-1047,W15-3502,1,0.828791,"ty between UD and UCCA can be traced back to their shared design principles: both are designed to be applicable across languages and domains, to enable rapid annotation and to support text understanding applications. This section provides a brief introduction to each of the schemes, whereas the next sections discuss their content in further detail.3 UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in text, abstracting away from syntactic forms. Shown to be preserved remarkably well across translations (Sulem et al., 2015), it has been applied to improve text simplification (Sulem 1 This excludes cases of shared argumenthood, which are partially covered by enhanced UD. See §4.1. 2 Our conversion and analysis code is public available at https://github.com/danielhers/synsem. 3 See Supplementary Material for a definition of each category in both schemes, and their abbreviations. 4 Despite the similar terminology, UCCA Adverbials are not necessarily adverbs syntactically. 479 which allow for a unit to participate in several super-ordinate relations. See example in Figure 1. Primary edges form a tree, whereas remote"
N19-1047,P18-1016,1,0.657614,"ring syntactic and semantic treebanks can also support fine-grained error analysis of semantic parsers, as illustrated by Szubert et al. (2018) for AMR (Banarescu et al., 2013). To demonstrate the utility of our comparison methodology, we perform fine-grained error analysis on UCCA parsing, according to UD relations (§6). Results highlight challenges for current parsing technology, and expose cases where UCCA parsers may benefit from modeling syntactic structure more directly.2 2 A C D E F G H et al., 2018b), and text-to-text generation evaluation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). Formally, UCCA structures are directed acyclic graphs (DAGs) whose nodes (or units) correspond either to words, or to elements viewed as a single entity according to some semantic or cognitive consideration. Edges are labeled, indicating the role of a child in the relation the parent represents. Figure 1 shows a legend of UCCA abbreviations. A Scene is UCCA’s notion of an event or a frame, and is a description of a movement, an action or a state which persists in time. Every Scene contains one primary relation, which can be either a Process or a State. Scenes may contain any number of Part"
N19-1047,K16-1019,0,0.0337937,"Missing"
N19-1047,D18-1412,0,0.0332315,"ishes semantic schemes from syntactic ones, which are still in many cases the backbone of text understanding systems. Such studies are essential for (1) determining whether and to what extent semantic methods should be adopted for text understanding applications; (2) defining better inductive biases for semantic parsers, and allowing better use of information encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD"
N19-1047,N18-1106,0,0.0257713,"Missing"
N19-1047,D16-1177,0,0.194381,"Missing"
N19-1047,W04-3212,0,0.0953409,"Missing"
N19-1047,W17-6944,0,0.0226913,"Missing"
N19-1047,W08-2121,0,\N,Missing
N19-1047,N15-1007,0,\N,Missing
P06-1038,P99-1016,0,0.244039,"clustering of context feature vectors. Many of the papers cited below aim at the construction of hyponym (is-a) hierarchies. Note that they can also be viewed as algorithms for category discovery, because a subtree in such a hierarchy defines a lexical category. A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). Pereira (1993) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. 2 We did not compare against methods that use richer syntactic information, both because they are supervised and because they are much more computationally demanding. 3 We are not aware of any multilingual evaluation previously reported on the task. 298 3.2 2005). Again, that paper uses syntactic information. In summary, no previous work has combined the accuracy, scalability and performance advantages of patterns with the fully unsupervised, unannotated nature possible with clustering approaches. This severely limits t"
P06-1038,W04-3205,0,0.0656093,"acquisition. The technique is also quite demanding computationally. The second main algorithmic approach is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). Hearst (1992) uses a manually prepared set of initial lexical patterns in order to discover hierarchical categories, and utilizes those categories in order to automatically discover additional patterns. (Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relationships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs. Both use information obtained by parsing. (Pantel et al, 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using coclustering. Many Information Extraction papers discover relationships between words using syntactic patterns (Ri"
P06-1038,W02-0908,0,0.0584357,"otation and other human input used; (2) the type of lexical relationship targeted; and (3) the basic algorithmic approach. The two main approaches are pattern-based discovery and clustering of context feature vectors. Many of the papers cited below aim at the construction of hyponym (is-a) hierarchies. Note that they can also be viewed as algorithms for category discovery, because a subtree in such a hierarchy defines a lexical category. A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). Pereira (1993) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. 2 We did not compare against methods that use richer syntactic information, both because they are supervised and because they are much more computationally demanding. 3 We are not aware of any multilingual evaluation previously reported on the task. 298 3.2 2005). Again, that paper uses syntactic information. In summary, no previous work has combined t"
P06-1038,W04-3234,0,0.0171572,". (Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relationships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs. Both use information obtained by parsing. (Pantel et al, 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using coclustering. Many Information Extraction papers discover relationships between words using syntactic patterns (Riloff and Jones, 1999). (Widdows and Dorow, 2002; Dorow et al, 2005) discover categories using two hard-coded symmetric patterns, and are thus the closest to us. They also introduce an elegant graph representation that we adopted. They report good results. However, they require POS tagging of the corpus, use only two hard-coded patterns (‘x and y’, ‘x or y’), deal only with nouns, and require non-trivial computations on the graph. A third, less common, approach uses settheore"
P06-1038,C92-2082,0,0.0927781,"le well when the number of words assigned to categories grows. Agglomerative clustering (e.g., (Brown et al, 1992; Li, 1996)) can produce hierarchical word categories from an unannotated corpus. However, we are not aware of work in this direction that has been evaluated with good results on lexical category acquisition. The technique is also quite demanding computationally. The second main algorithmic approach is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). Hearst (1992) uses a manually prepared set of initial lexical patterns in order to discover hierarchical categories, and utilizes those categories in order to automatically discover additional patterns. (Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relationships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs. Both use information obtained by parsing. (Pantel et al, 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as"
P06-1038,C96-1003,0,0.189538,"Missing"
P06-1038,P98-2127,0,0.234887,"type of lexical relationship targeted; and (3) the basic algorithmic approach. The two main approaches are pattern-based discovery and clustering of context feature vectors. Many of the papers cited below aim at the construction of hyponym (is-a) hierarchies. Note that they can also be viewed as algorithms for category discovery, because a subtree in such a hierarchy defines a lexical category. A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). Pereira (1993) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. 2 We did not compare against methods that use richer syntactic information, both because they are supervised and because they are much more computationally demanding. 3 We are not aware of any multilingual evaluation previously reported on the task. 298 3.2 2005). Again, that paper uses syntactic information. In summary, no previous work has combined the accuracy, scalability and pe"
P06-1038,P93-1024,0,0.811891,"Missing"
P06-1038,P99-1008,0,0.814956,"we are not aware of work in this direction that has been evaluated with good results on lexical category acquisition. The technique is also quite demanding computationally. The second main algorithmic approach is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). Hearst (1992) uses a manually prepared set of initial lexical patterns in order to discover hierarchical categories, and utilizes those categories in order to automatically discover additional patterns. (Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relationships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs. Both use information obtained by parsing. (Pantel et al, 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using cocluster"
P06-1038,J98-1004,0,0.259269,"Missing"
P06-1038,J92-4003,0,0.0575061,"in two languages3 . Section 2 surveys previous work. Section 3 describes pattern discovery, and Section 4 describes the formation of categories. Evaluation is presented in Section 5, and a discussion in Section 6. The only previous works addressing our problem and not requiring any syntactic annotation are those that decompose a lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al, 1990). Such matrix decomposition is computationally heavy and has not been proven to scale well when the number of words assigned to categories grows. Agglomerative clustering (e.g., (Brown et al, 1992; Li, 1996)) can produce hierarchical word categories from an unannotated corpus. However, we are not aware of work in this direction that has been evaluated with good results on lexical category acquisition. The technique is also quite demanding computationally. The second main algorithmic approach is to use lexico-syntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). Hearst (1992) uses a manually prepared set of initial lexical patterns in order to discover hierarchical categori"
P06-1038,C02-1114,0,0.919226,"sting relations between verbs. Both use information obtained by parsing. (Pantel et al, 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using coclustering. Many Information Extraction papers discover relationships between words using syntactic patterns (Riloff and Jones, 1999). (Widdows and Dorow, 2002; Dorow et al, 2005) discover categories using two hard-coded symmetric patterns, and are thus the closest to us. They also introduce an elegant graph representation that we adopted. They report good results. However, they require POS tagging of the corpus, use only two hard-coded patterns (‘x and y’, ‘x or y’), deal only with nouns, and require non-trivial computations on the graph. A third, less common, approach uses settheoretic inference, for example (Cimiano et al, 2 Previous Work Much work has been done on lexical acquisition of all sorts. The three main distinguishing axes are (1) the t"
P06-1038,C98-2122,0,\N,Missing
P07-1030,W06-1659,0,0.0719854,"t (at least implicitly) pre-specifying relationship types. Most related work deals with discovery of hypernymy (Hearst, 233 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999). In addition to these basic types, several studies deal with the discovery and labeling of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and nouncompound relationships (Moldovan et al, 2004). Studying relationships between tagged named entities, (Hasegawa et al, 2004; Hassan et al, 2006) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters, where each cluster corresponds to one of a known relationship type. These studies, however, focused on the classification of pairs that were either given or extracted using some supervision, rather than on discovery and definition of which relationships are actually in the corpus. Several papers report on methods for using the web to discover instances of binary relations. However, each of these assumes that the relations themselves are known in advance (implicitly"
P07-1030,W04-2609,0,0.179281,"rk has directly addressed the discovery of generic binary relations in an unrestricted domain without (at least implicitly) pre-specifying relationship types. Most related work deals with discovery of hypernymy (Hearst, 233 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999). In addition to these basic types, several studies deal with the discovery and labeling of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and nouncompound relationships (Moldovan et al, 2004). Studying relationships between tagged named entities, (Hasegawa et al, 2004; Hassan et al, 2006) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters, where each cluster corresponds to one of a known relationship type. These studies, however, focused on the classification of pairs that were either given or extracted using some supervision, rather than on discovery and definition of which relationships are actually in the corpus. Several papers report on methods for using the web to discover instances of binary relati"
P07-1030,P06-1102,0,0.0700964,"ication of pairs that were either given or extracted using some supervision, rather than on discovery and definition of which relationships are actually in the corpus. Several papers report on methods for using the web to discover instances of binary relations. However, each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns (Agichtein and Gravano, 2000; Pantel et al, 2004), pattern-based rules (Etzioni et al, 2004), relation keywords (Sekine, 2006), or word pairs exemplifying relation instances (Pasca et al, 2006; Alfonseca et al, 2006; Rosenfeld and Feldman, 2006). In some recent work (Strube and Ponzetto, 2006), it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought. However, this work does not focus on differentiating among different relations, so that the generated relations might conflate a number of distinct ones. It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of ha"
P07-1030,W06-0507,0,0.0358758,"at were either given or extracted using some supervision, rather than on discovery and definition of which relationships are actually in the corpus. Several papers report on methods for using the web to discover instances of binary relations. However, each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns (Agichtein and Gravano, 2000; Pantel et al, 2004), pattern-based rules (Etzioni et al, 2004), relation keywords (Sekine, 2006), or word pairs exemplifying relation instances (Pasca et al, 2006; Alfonseca et al, 2006; Rosenfeld and Feldman, 2006). In some recent work (Strube and Ponzetto, 2006), it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought. However, this work does not focus on differentiating among different relations, so that the generated relations might conflate a number of distinct ones. It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of handcrafted databases suc"
P07-1030,P99-1008,0,0.11917,"to generate relations directly from a corpus given no additional information of any kind. The key point is that we do not in any manner specify in advance what types of relations we wish to find. 3 Related Work As far as we know, no previous work has directly addressed the discovery of generic binary relations in an unrestricted domain without (at least implicitly) pre-specifying relationship types. Most related work deals with discovery of hypernymy (Hearst, 233 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999). In addition to these basic types, several studies deal with the discovery and labeling of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and nouncompound relationships (Moldovan et al, 2004). Studying relationships between tagged named entities, (Hasegawa et al, 2004; Hassan et al, 2006) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters, where each cluster corresponds to one of a known relationship type. These studies, however, focused on the classification of pairs t"
P07-1030,W04-3205,0,0.0608261,"sh to find. 3 Related Work As far as we know, no previous work has directly addressed the discovery of generic binary relations in an unrestricted domain without (at least implicitly) pre-specifying relationship types. Most related work deals with discovery of hypernymy (Hearst, 233 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999). In addition to these basic types, several studies deal with the discovery and labeling of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and nouncompound relationships (Moldovan et al, 2004). Studying relationships between tagged named entities, (Hasegawa et al, 2004; Hassan et al, 2006) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters, where each cluster corresponds to one of a known relationship type. These studies, however, focused on the classification of pairs that were either given or extracted using some supervision, rather than on discovery and definition of which relationships are actually in the corpus. Several papers report on methods fo"
P07-1030,P06-2021,0,0.0225449,"ome recent work (Strube and Ponzetto, 2006), it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought. However, this work does not focus on differentiating among different relations, so that the generated relations might conflate a number of distinct ones. It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of handcrafted databases such as WordNet (Moldovan et al, 2004; Costello et al, 2006) and Wikipedia (Strube and Ponzetto, 2006). Finally, (Turney, 2006) provided a pattern distance measure which allows a fully unsupervised measurement of relational similarity between two pairs of words; however, relationship types were not discovered explicitly. 4 Outline of the Method We will use two concept words contained in a concept class C to generate a collection of distinct relations in which C participates. In this section we offer a brief overview of our method. Step 1: Use a seed consisting of two (or more) example words to automatically obtain other examples that belong to the same"
P07-1030,P06-1038,1,0.773601,"we will consider related work. In section 4 we will provide an overview of our solution and in section 5 we will consider the details of the method. In section 6 we will illustrate and evaluate the results obtained by our method. Finally, in section 7 we will offer some conclusions and considerations for further work. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics 2 Problem Definition In several studies (e.g., Widdows and Dorow, 2002; Pantel et al, 2004; Davidov and Rappoport, 2006) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense. Although examination of any such set invariably makes it clear why these words have been grouped together into a single concept, it is important to emphasize that the method itself provides no explicit concept definition; in some sense, the implied class is in the eye of the beholder. Nevertheless, both human judgment and comparison with standard lists indicate that the generated sets correspond to concepts with hig"
P07-1030,P98-2182,0,0.111444,"ince such sets can be generated in entirely unsupervised fashion, our challenge is effectively to generate relations directly from a corpus given no additional information of any kind. The key point is that we do not in any manner specify in advance what types of relations we wish to find. 3 Related Work As far as we know, no previous work has directly addressed the discovery of generic binary relations in an unrestricted domain without (at least implicitly) pre-specifying relationship types. Most related work deals with discovery of hypernymy (Hearst, 233 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999). In addition to these basic types, several studies deal with the discovery and labeling of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and nouncompound relationships (Moldovan et al, 2004). Studying relationships between tagged named entities, (Hasegawa et al, 2004; Hassan et al, 2006) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters, where each cluster corresponds to o"
P07-1030,P06-2094,0,0.00694212,"ationship type. These studies, however, focused on the classification of pairs that were either given or extracted using some supervision, rather than on discovery and definition of which relationships are actually in the corpus. Several papers report on methods for using the web to discover instances of binary relations. However, each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns (Agichtein and Gravano, 2000; Pantel et al, 2004), pattern-based rules (Etzioni et al, 2004), relation keywords (Sekine, 2006), or word pairs exemplifying relation instances (Pasca et al, 2006; Alfonseca et al, 2006; Rosenfeld and Feldman, 2006). In some recent work (Strube and Ponzetto, 2006), it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought. However, this work does not focus on differentiating among different relations, so that the generated relations might conflate a number of distinct ones. It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity"
P07-1030,W06-0503,0,0.0302761,"004), relation keywords (Sekine, 2006), or word pairs exemplifying relation instances (Pasca et al, 2006; Alfonseca et al, 2006; Rosenfeld and Feldman, 2006). In some recent work (Strube and Ponzetto, 2006), it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought. However, this work does not focus on differentiating among different relations, so that the generated relations might conflate a number of distinct ones. It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of handcrafted databases such as WordNet (Moldovan et al, 2004; Costello et al, 2006) and Wikipedia (Strube and Ponzetto, 2006). Finally, (Turney, 2006) provided a pattern distance measure which allows a fully unsupervised measurement of relational similarity between two pairs of words; however, relationship types were not discovered explicitly. 4 Outline of the Method We will use two concept words contained in a concept class C to generate a collection of distinct relations in which C participates. In this section w"
P07-1030,P06-1040,0,0.101012,"pairs can be generated without pre-specifying the nature of the relation sought. However, this work does not focus on differentiating among different relations, so that the generated relations might conflate a number of distinct ones. It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of handcrafted databases such as WordNet (Moldovan et al, 2004; Costello et al, 2006) and Wikipedia (Strube and Ponzetto, 2006). Finally, (Turney, 2006) provided a pattern distance measure which allows a fully unsupervised measurement of relational similarity between two pairs of words; however, relationship types were not discovered explicitly. 4 Outline of the Method We will use two concept words contained in a concept class C to generate a collection of distinct relations in which C participates. In this section we offer a brief overview of our method. Step 1: Use a seed consisting of two (or more) example words to automatically obtain other examples that belong to the same class. Call these concept words. (For instance, if our example wor"
P07-1030,C02-1114,0,0.856564,"the problem we intend to solve. In section 3, we will consider related work. In section 4 we will provide an overview of our solution and in section 5 we will consider the details of the method. In section 6 we will illustrate and evaluate the results obtained by our method. Finally, in section 7 we will offer some conclusions and considerations for further work. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics 2 Problem Definition In several studies (e.g., Widdows and Dorow, 2002; Pantel et al, 2004; Davidov and Rappoport, 2006) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense. Although examination of any such set invariably makes it clear why these words have been grouped together into a single concept, it is important to emphasize that the method itself provides no explicit concept definition; in some sense, the implied class is in the eye of the beholder. Nevertheless, both human judgment and comparison with standard lists indicate that"
P07-1030,C92-2082,0,\N,Missing
P07-1030,P04-1053,0,\N,Missing
P07-1030,P06-2086,0,\N,Missing
P07-1052,P05-1022,0,0.721384,"ion Many algorithms for major NLP applications such as information extraction (IE) and question answering (QA) utilize the output of statistical parsers (see (Yates et al., 2006)). While the average performance of statistical parsers gradually improves, the quality of many of the parses they produce is too low for applications. When the training and test 408 data are taken from different domains (the parser adaptation scenario) the ratio of such low quality parses becomes even higher. Figure 1 demonstrates these phenomena for two leading models, Collins (1999) model 2, a generative model, and Charniak and Johnson (2005), a reranking model. The parser adaptation scenario is the rule rather than the exception for QA and IE systems, because these usually operate over the highly variable Web, making it very difficult to create a representative corpus for manual annotation. Medium quality parses may seriously harm the performance of such systems. In this paper we address the problem of assessing parse quality, using a Sample Ensemble Parse Assessment (SEPA) algorithm. We use the level of agreement among several copies of a parser, each of which trained on a different sample from the training data, to predict the"
P07-1052,W01-0521,0,0.0786942,"Missing"
P07-1052,A00-2005,0,0.244236,"Missing"
P07-1052,H05-1064,0,0.038262,"Missing"
P07-1052,N03-1022,0,0.0850279,"ted by sampling, with replacement, L examples from the training pool, where L is the size of the training pool. Conversely, each of our samples is smaller than the training set, and is created by sampling without replacement. See Section 3 (‘regarding S’) for a discussion of this issue. predictors in classifiers’ output to posterior probabilities is given in (Caruana and Niculescu-Mizil, 2006). As far as we know, the application of a sample based parser ensemble for assessing parse quality is novel. Many IE and QA systems rely on the output of parsers (Kwok et al., 2001; Attardi et al., 2001; Moldovan et al., 2003). The latter tries to address incorrect parses using complex relaxation methods. Knowing the quality of a parse could greatly improve the performance of such systems. 3 The Sample Ensemble Parse Assessment (SEPA) Algorithm In this section we detail our parse assessment algorithm. Its input consists of a parsing algorithm A, an annotated training set T R, and an unannotated test set T E. The output provides, for each test sentence, the parse generated for it by A when trained on the full training set, and a grade assessing the parse’s quality, on a continuous scale between 0 to 100. Application"
P07-1052,P00-1016,0,0.496173,"Missing"
P07-1052,W06-1604,0,0.262901,"e to web-based applications such as QA and IE. Generative statistical parsers compute a probability p(a, s) for each sentence annotation, so the immediate technique that comes to mind for assessing parse quality is to simply use p(a, s). Another seemingly trivial method is to assume that shorter sentences would be parsed better than longer ones. However, these techniques produce results that are far from optimal. In Section 5 we show the superiority of our method over these and other baselines. Surprisingly, as far as we know there is only one previous work explicitly addressing this problem (Yates et al., 2006). Their WOODWARD algorithm filters out high quality parses by performing semanProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 408–415, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Fraction of parses 1 Collins, ID Collins, Adap. Charniak, ID Charniak,Adap. 0.8 0.6 0.4 0.2 80 85 90 95 100 F score Figure 1: F-score vs. the fraction of parses whose f-score is at least that f-score. For the in-domain scenario, the parsers are tested on sec 23 of the WSJ Penn Treebank. For the parser adaptation scenario, they are te"
P07-1052,J03-4003,0,\N,Missing
P07-1078,A00-2018,0,0.278039,"Missing"
P07-1078,P05-1022,0,0.0729994,"the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They train the PCFG parser and the reranker with the manually annotated WSJ data, and parse the NANC data with the 50-best PCFG parser. Then they proceed in two directi"
P07-1078,W03-0407,0,0.204778,"eds in adapting a generative parser between domains using a small manually annotated dataset. • The first formulation (related to the number of unknown words in a sentence) of when selftraining is valuable. Section 2 discusses previous work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second la"
P07-1078,J04-3001,0,0.0522258,"Missing"
P07-1078,H05-1064,0,0.0186989,"f selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They train the PCFG parser and the reranker with the manually annotated WSJ data, and parse the NANC data with the 50-best PCFG parser. Then they proceed in two directions. In the first, they reorder the 50-best parse list"
P07-1078,N06-1020,0,0.839769,"work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They"
P07-1078,P06-1043,0,0.610862,"work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They"
P07-1078,E03-1008,0,0.819559,"l seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only. However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only one of our scenarios (OI, see below). A work closely related to ours is (Steedman et al., 2003a), which applied co-training (Blum and Mitchell, 1998) and self-training to Collins’ parsing model using a small seed dataset (500 sentences for both methods and 1,000 sentences for co-training only). The seed, self-training and test datasets they used are similar to those we use in our II experiment (see below), but the self-training protocols are different. They first train the parser with the seed sentences sampled from WSJ sections 2-21. Then, iteratively, 30 sentences are sampled from these sections, parsed by the parser, and the 20 best sentences (in terms of parser confidence defined a"
P07-1078,N03-1031,0,0.507694,"l seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only. However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only one of our scenarios (OI, see below). A work closely related to ours is (Steedman et al., 2003a), which applied co-training (Blum and Mitchell, 1998) and self-training to Collins’ parsing model using a small seed dataset (500 sentences for both methods and 1,000 sentences for co-training only). The seed, self-training and test datasets they used are similar to those we use in our II experiment (see below), but the self-training protocols are different. They first train the parser with the seed sentences sampled from WSJ sections 2-21. Then, iteratively, 30 sentences are sampled from these sections, parsed by the parser, and the 20 best sentences (in terms of parser confidence defined a"
P07-1078,J03-4003,0,\N,Missing
P08-1027,S07-1103,0,0.0764141,"Missing"
P08-1027,S07-1085,0,0.369594,"e term ‘nominal’ follows (Girju et al., 2007), and includes simple nouns, noun compounds and multiword expressions serving as nouns. ing’ denotes a morning that happens in the summer. These two relationships are completely different semantically but are similar syntactically, and distinguishing between them could be essential for NLP applications such as question answering and machine translation. Relation classification usually relies on a training set in the form of tagged data. To improve results, some systems utilize additional manually constructed semantic resources such as WordNet (WN) (Beamer et al., 2007). However, in many domains and languages such resources are not available. Furthermore, usage of such resources frequently requires disambiguation and connection of the data to the resource (word sense disambiguation in the case of WordNet). Manual disambiguation is unfeasible in many practical tasks, and an automatic one may introduce errors and greatly degrade performance. It thus makes sense to try to minimize the usage of such resources, and utilize only corpus contexts in which the relevant words appear. A leading method for utilizing context information for classification and extraction"
P08-1027,S07-1084,0,0.22594,"sed, classification algorithm and the nature of training/test data. 2.1 Available Resources Many relation classification algorithms utilize WordNet. Among the 15 systems presented by the 14 SemEval teams, some utilized the manually provided WordNet tags for the dataset pairs (e.g., (Beamer et al., 2007)). In all cases, usage of WN tags improves the results significantly. Some other systems that avoided using the labels used WN as a supporting resource for their algorithms (Costello, 2007; Nakov and Hearst, 2007; Kim and Baldwin, 2007). Only three avoided WN altogether (Hendrickx et al., 2007; Bedmar et al., 2007; Aramaki et al., 2006). Other resources used for relationship discovery include Wikipedia (Strube and Ponzetto, 2006), thesauri or synonym sets (Turney, 2005) and domainspecific semantic hierarchies like MeSH (Rosario and Hearst, 2001). While usage of these resources is beneficial in many cases, high quality word sense annotation is not easily available. Besides, lexical resources are not available for many languages, and their coverage is limited even for English when applied to some restricted domains. In this paper we do not use any manually annotated resources apart from the classificatio"
P08-1027,S07-1081,0,0.0410648,"ominent category. Major differences between these methods include available resources, degree of preprocessing, features used, classification algorithm and the nature of training/test data. 2.1 Available Resources Many relation classification algorithms utilize WordNet. Among the 15 systems presented by the 14 SemEval teams, some utilized the manually provided WordNet tags for the dataset pairs (e.g., (Beamer et al., 2007)). In all cases, usage of WN tags improves the results significantly. Some other systems that avoided using the labels used WN as a supporting resource for their algorithms (Costello, 2007; Nakov and Hearst, 2007; Kim and Baldwin, 2007). Only three avoided WN altogether (Hendrickx et al., 2007; Bedmar et al., 2007; Aramaki et al., 2006). Other resources used for relationship discovery include Wikipedia (Strube and Ponzetto, 2006), thesauri or synonym sets (Turney, 2005) and domainspecific semantic hierarchies like MeSH (Rosario and Hearst, 2001). While usage of these resources is beneficial in many cases, high quality word sense annotation is not easily available. Besides, lexical resources are not available for many languages, and their coverage is limited even for English whe"
P08-1027,P06-1038,1,0.360218,"As a first step, we randomly sample a set of hook words, which will be used in order to discover relationships that generally occur in the corpus. To avoid selection of ambiguous words or typos, we do not select words with frequency higher than a parameter FC and lower than a threshold FB . We also limit the total number N of hook words. For each hook word, we now create a hook corpus, the set of the contexts in which the word appears. Each context is a window containing W words or punctuation characters before and after the hook word. 3.2 Pattern Specification To specify patterns, following (Davidov and Rappoport, 2006) we classify words into highfrequency words (HFWs) and content words (CWs). A word whose frequency is more (less) than FH (FC ) is considered to be a HFW (CW). Our patterns have the general form [Prefix] CW1 [Infix] CW2 [Postfix] where Prefix, Infix and Postfix contain only HFWs. We require Prefix and Postfix to be a single HFW, while Infix can contain any number of HFWs (limiting pattern length by window size). This form may include patterns like ‘such X as Y and’. At this stage, the pattern slots can contain only single words; however, when using the final pattern clusters for nominal relati"
P08-1027,P08-1079,1,0.775088,"aluated our algorithm on SemEval-07 Task 4 data, showing superior results over participating algorithms that did not utilize WordNet disambiguation tags. We also show how pattern clusters can be used for a completely unsupervised classification of the test set. Since in this case no training data is used, this allows the automated discovery of a potentially unbiased classification scheme. Section 2 discusses related work, Section 3 outlines the pattern clustering algorithm, Section 4 details three classification methods, and Sections 5 and 6 describe the evaluation protocol and results. 2 In (Davidov and Rappoport, 2008) we focus on the pattern cluster resource type itself, presenting an evaluation of its intrinsic quality based on SAT tests. In the present paper we focus on showing how the resource can be used to improve a known NLP task. 2.2 Degree of Preprocessing 228 2 Related Work Numerous methods have been devised for classification of semantic relationships, among which those holding between nominals constitute a prominent category. Major differences between these methods include available resources, degree of preprocessing, features used, classification algorithm and the nature of training/test data."
P08-1027,W04-2610,0,0.0489332,"ent an approach to extract pattern clusters from an untagged corpus. Each such cluster represents some unspecified lexical relationship. In this paper, we use these pattern clusters as the (only) source of machine learning features for a nominal relationship classification problem. Unlike the majority of current studies, we avoid using any other features that require some language-specific information or are devised for specific relationship types. 229 2.4 Classification Algorithm Various learning algorithms have been used for relation classification. Common choices include variations of SVM (Girju et al., 2004; Nastase et al., 2006), decision trees and memory-based learners. Freely available tools like Weka (Witten and Frank, 1999) allow easy experimentation with common learning algorithms (Hendrickx et al., 2007). In this paper we did not focus on a single ML algorithm, letting algorithm selection be automatically based on cross-validation results on the training set, as in (Hendrickx et al., 2007) but using more algorithms and allowing a more flexible parameter choice. 2.5 Training Data As stated above, several categorization schemes for nominals have been proposed. Nastase and Szpakowicz (2003)"
P08-1027,J06-1005,0,0.16585,"from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). Several studies utilize syntactic features. Many other works manually develop a set of heuristic features devised with some specific relationship in mind, like a WordNet-based meronymy feature (Bedmar et al., 2007) or size-of feature (Aramaki et al., 2006). However, the most prominent feature type is based on lexico-syntactic patterns in which the related words co-appear. Since (Hearst, 1992), numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g., (Girju et al., 2006; Snow et al., 2006; Banko et al, 2007)). Rosenfeld and Feldman (2007) discover relationship instances by clustering entities appearing in similar contexts. Strategies were developed for discovery of multiple patterns for some specified lexical relationship (Pantel and Pennacchiotti, 2006) and for unsupervised pattern ranking (Turney, 2006). Davidov et al. (2007) use pattern clusters to define general relationships, but these are specific to a given concept. No study so far has proposed a method to define, discover and represent general relationships present in an arbitrary corpus. In (Davidov"
P08-1027,C92-2082,0,0.306094,"es such resources are not available. Furthermore, usage of such resources frequently requires disambiguation and connection of the data to the resource (word sense disambiguation in the case of WordNet). Manual disambiguation is unfeasible in many practical tasks, and an automatic one may introduce errors and greatly degrade performance. It thus makes sense to try to minimize the usage of such resources, and utilize only corpus contexts in which the relevant words appear. A leading method for utilizing context information for classification and extraction of relationships is that of patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). The standard classification process is to find in an auxiliary corpus a set of patterns in which a given training word pair co-appears, and use pattern-word pair co-appearance statistics as features for machine learning algorithms. In this paper we introduce a novel approach, based on utilizing pattern clusters that are prepared separately and independently of the training set. We do not utilize any manually constructed resource or any manual tagging of training data beyond the cor227 Proceedings of ACL-08: HLT, pages 227–235, c Columbus, Ohio, USA, June 2008"
P08-1027,S07-1049,0,0.0207074,"een these methods include available resources, degree of preprocessing, features used, classification algorithm and the nature of training/test data. 2.1 Available Resources Many relation classification algorithms utilize WordNet. Among the 15 systems presented by the 14 SemEval teams, some utilized the manually provided WordNet tags for the dataset pairs (e.g., (Beamer et al., 2007)). In all cases, usage of WN tags improves the results significantly. Some other systems that avoided using the labels used WN as a supporting resource for their algorithms (Costello, 2007; Nakov and Hearst, 2007; Kim and Baldwin, 2007). Only three avoided WN altogether (Hendrickx et al., 2007; Bedmar et al., 2007; Aramaki et al., 2006). Other resources used for relationship discovery include Wikipedia (Strube and Ponzetto, 2006), thesauri or synonym sets (Turney, 2005) and domainspecific semantic hierarchies like MeSH (Rosario and Hearst, 2001). While usage of these resources is beneficial in many cases, high quality word sense annotation is not easily available. Besides, lexical resources are not available for many languages, and their coverage is limited even for English when applied to some restricted domains. In this pa"
P08-1027,W04-2609,0,0.462054,"en classification problem, will be represented by some of the discovered clusters. We then use the training set to label some of the clusters, and the labeled clusters to assign classes to tested items. One of the advantages of our method is that it can be used not only for classification, but also for further analysis and retrieval of the observed relationships2 . The semantic relationships between the components of noun compounds and between nominals in general are not easy to categorize rigorously. Several different relationship hierarchies have been proposed (Nastase and Szpakowicz, 2003; Moldovan et al., 2004). Some classes, like Container-Contained, Time-Event and Product-Producer, appear in several classification schemes, while classes like ToolObject are more vaguely defined and are subdivided differently. Recently, SemEval-07 Task 4 (Girju et al., 2007) proposed a benchmark dataset that includes a subset of 7 widely accepted nominal relationship (NR) classes, allowing consistent evaluation of different NR classification algorithms. In the SemEval event, 14 research teams evaluated their algorithms using this benchmark. Some of the teams have used the manually annotated WN labels provided with t"
P08-1027,P06-1015,0,0.3241,"ces are not available. Furthermore, usage of such resources frequently requires disambiguation and connection of the data to the resource (word sense disambiguation in the case of WordNet). Manual disambiguation is unfeasible in many practical tasks, and an automatic one may introduce errors and greatly degrade performance. It thus makes sense to try to minimize the usage of such resources, and utilize only corpus contexts in which the relevant words appear. A leading method for utilizing context information for classification and extraction of relationships is that of patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006). The standard classification process is to find in an auxiliary corpus a set of patterns in which a given training word pair co-appears, and use pattern-word pair co-appearance statistics as features for machine learning algorithms. In this paper we introduce a novel approach, based on utilizing pattern clusters that are prepared separately and independently of the training set. We do not utilize any manually constructed resource or any manual tagging of training data beyond the cor227 Proceedings of ACL-08: HLT, pages 227–235, c Columbus, Ohio, USA, June 2008. 2008 Association for Computatio"
P08-1027,P06-1101,0,0.0799972,"words frequencies to WordNet-based features (Moldovan et al., 2004). Several studies utilize syntactic features. Many other works manually develop a set of heuristic features devised with some specific relationship in mind, like a WordNet-based meronymy feature (Bedmar et al., 2007) or size-of feature (Aramaki et al., 2006). However, the most prominent feature type is based on lexico-syntactic patterns in which the related words co-appear. Since (Hearst, 1992), numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g., (Girju et al., 2006; Snow et al., 2006; Banko et al, 2007)). Rosenfeld and Feldman (2007) discover relationship instances by clustering entities appearing in similar contexts. Strategies were developed for discovery of multiple patterns for some specified lexical relationship (Pantel and Pennacchiotti, 2006) and for unsupervised pattern ranking (Turney, 2006). Davidov et al. (2007) use pattern clusters to define general relationships, but these are specific to a given concept. No study so far has proposed a method to define, discover and represent general relationships present in an arbitrary corpus. In (Davidov and Rappoport, 200"
P08-1027,H05-1047,0,0.00749218,"different sets vary greatly in difficulty. However, we also obtain a nice insight as to why this happens – relations like Theme-Tool seem very ambiguous and are mapped to several clusters, while relations like Product-Producer seem to be well-defined by the obtained pattern clusters. The SemEval dataset does not explicitly mark items whose correct classification requires analysis of the context of the whole sentence in which they appear. Since our algorithm does not utilize test sen234 7 Conclusion Relationship classification is known to improve many practical tasks, e.g., textual entailment (Tatu and Moldovan, 2005). We have presented a novel framework for relationship classification, based on pattern clusters prepared as a standalone resource independently of the training set. Our method outperforms current state-of-the-art algorithms that do not utilize WordNet tags on Task 4 of SemEval-07. In practical situations, it would not be feasible to provide a large amount of such sense disambiguation tags manually. Our method also shows competitive performance compared to the majority of task participants that do utilize WN tags. Our method can produce labeled pattern clusters, which can be potentially useful"
P08-1027,P06-1040,0,0.466558,"However, the most prominent feature type is based on lexico-syntactic patterns in which the related words co-appear. Since (Hearst, 1992), numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g., (Girju et al., 2006; Snow et al., 2006; Banko et al, 2007)). Rosenfeld and Feldman (2007) discover relationship instances by clustering entities appearing in similar contexts. Strategies were developed for discovery of multiple patterns for some specified lexical relationship (Pantel and Pennacchiotti, 2006) and for unsupervised pattern ranking (Turney, 2006). Davidov et al. (2007) use pattern clusters to define general relationships, but these are specific to a given concept. No study so far has proposed a method to define, discover and represent general relationships present in an arbitrary corpus. In (Davidov and Rappoport, 2008) we present an approach to extract pattern clusters from an untagged corpus. Each such cluster represents some unspecified lexical relationship. In this paper, we use these pattern clusters as the (only) source of machine learning features for a nominal relationship classification problem. Unlike the majority of current"
P08-1027,S07-1039,0,\N,Missing
P08-1027,S07-1080,0,\N,Missing
P08-1027,P07-1030,1,\N,Missing
P08-1027,S07-1003,0,\N,Missing
P08-1027,W01-0511,0,\N,Missing
P08-1079,W06-0507,0,0.102087,"o discover and define generic relationships that exist in the entire domain. Studying relationships between tagged named entities, (Hasegawa et al., 2004; Hassan et al., 2006) proposed unsupervised clustering methods that assign given sets of pairs into several clusters, where each cluster corresponds to one of a known set of relationship types. Their classification setting is thus very different from our unsupervised discovery one. Several recent papers discovered relations on the web using seed patterns (Pantel et al., 2004), rules (Etzioni et al., 2004), and word pairs (Pasca et al., 2006; Alfonseca et al., 2006). The latter used the notion of hook which we also use in this paper. Several studies utilize some preprocessing, including parsing (Hasegawa et al., 2004; Hassan et al., 2006) and usage of syntactic (Suchanek et al., 2006) and morphological (Pantel et al., 2004) information in patterns. Several algorithms use manuallyprepared resources, including WordNet (Moldovan et al., 2004; Costello et al., 2006) and Wikipedia (Strube and Ponzetto, 2006). In this paper, we do not utilize any language-specific preprocessing or any other resources, which makes our algorithm relatively easily portable betwee"
P08-1079,P99-1008,0,0.668835,"Related Work Extraction of relation information from text is a large sub-field in NLP. Major differences between pattern approaches include the relationship types sought (including domain restrictions), the degrees of supervision and required preprocessing, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of which main types of generic relationships"
P08-1079,W04-3205,0,0.748587,"g, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of which main types of generic relationships actually exist in an unrestricted domain. Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational similarity between two pairs of words; such a measure could in principle be used by a clustering algorith"
P08-1079,P06-2021,0,0.0201726,"our unsupervised discovery one. Several recent papers discovered relations on the web using seed patterns (Pantel et al., 2004), rules (Etzioni et al., 2004), and word pairs (Pasca et al., 2006; Alfonseca et al., 2006). The latter used the notion of hook which we also use in this paper. Several studies utilize some preprocessing, including parsing (Hasegawa et al., 2004; Hassan et al., 2006) and usage of syntactic (Suchanek et al., 2006) and morphological (Pantel et al., 2004) information in patterns. Several algorithms use manuallyprepared resources, including WordNet (Moldovan et al., 2004; Costello et al., 2006) and Wikipedia (Strube and Ponzetto, 2006). In this paper, we do not utilize any language-specific preprocessing or any other resources, which makes our algorithm relatively easily portable between languages, as we demonstrate in our bilingual evaluation. 2.3 Evaluation Method Evaluation for hypernymy and synonymy usually uses WordNet (Lin and Pantel, 2002; Widdows and Dorow, 2002; Davidov and Rappoport, 2006). For more specific lexical relationships like relationships between verbs (Chklovski and Pantel, 2004), nominals (Girju et al., 2004; Girju et al., 2007) or meronymy subtypes (Berland an"
P08-1079,P06-1038,1,0.765088,"erating them. 693 6 present SAT and comparison evaluation results. 2 Related Work Extraction of relation information from text is a large sub-field in NLP. Major differences between pattern approaches include the relationship types sought (including domain restrictions), the degrees of supervision and required preprocessing, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly a"
P08-1079,P07-1030,1,0.457809,"not discussed. Unlike (Turney, 2006), we do not perform any pattern ranking. Instead we produce (possibly overlapping) hard clusters, where each pattern cluster represents a relationship discovered in the domain. Banko et al. (2007) and Rosenfeld and Feldman (2007) find relationship instances where the relationships are not specified in advance. They aim to find relationship instances rather than identify generic semantic relationships. Thus, their representation is very different from ours. In addition, (Banko et al., 2007) utilize supervised tools such as a POS tagger and a shallow parser. Davidov et al. (2007) proposed a method for unsupervised discovery of concept-specific relations. That work, like ours, relies on pattern clusters. However, it requires initial word seeds and targets the discovery of relationships specific for some given concept, while we attempt to discover and define generic relationships that exist in the entire domain. Studying relationships between tagged named entities, (Hasegawa et al., 2004; Hassan et al., 2006) proposed unsupervised clustering methods that assign given sets of pairs into several clusters, where each cluster corresponds to one of a known set of relationshi"
P08-1079,P08-1027,1,0.541626,"Missing"
P08-1079,W04-2610,0,0.0510896,"ources, including WordNet (Moldovan et al., 2004; Costello et al., 2006) and Wikipedia (Strube and Ponzetto, 2006). In this paper, we do not utilize any language-specific preprocessing or any other resources, which makes our algorithm relatively easily portable between languages, as we demonstrate in our bilingual evaluation. 2.3 Evaluation Method Evaluation for hypernymy and synonymy usually uses WordNet (Lin and Pantel, 2002; Widdows and Dorow, 2002; Davidov and Rappoport, 2006). For more specific lexical relationships like relationships between verbs (Chklovski and Pantel, 2004), nominals (Girju et al., 2004; Girju et al., 2007) or meronymy subtypes (Berland and Charniak, 1999) there is still little agreement which important relationships should be defined. Thus, there are more than a dozen different type hierarchies and tasks proposed for noun compounds (and nominals in general), including (Nastase and Szpakowicz, 2003; Girju et al., 2005; Girju et al., 2007). There are thus two possible ways for a fair eval694 uation. A study can develop its own relationship definitions and dataset, like (Nastase and Szpakowicz, 2003), thus introducing a possible bias; or it can accept the definition and datase"
P08-1079,J06-1005,0,0.36002,"relation information from text is a large sub-field in NLP. Major differences between pattern approaches include the relationship types sought (including domain restrictions), the degrees of supervision and required preprocessing, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of which main types of generic relationships actually exist in an"
P08-1079,P04-1053,0,0.215945,"sought (including domain restrictions), the degrees of supervision and required preprocessing, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of which main types of generic relationships actually exist in an unrestricted domain. Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational similar"
P08-1079,W06-1659,0,0.229637,"n restrictions), the degrees of supervision and required preprocessing, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of which main types of generic relationships actually exist in an unrestricted domain. Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational similarity between two pairs"
P08-1079,C92-2082,0,0.76865,"resources is labor intensive and susceptible to arbitrary human decisions. In addition, manually constructed semantic databases are not easily portable across text domains or languages. Hence, there is a need for developing semantic acquisition algorithms that are as unsupervised and language independent as possible. A fundamental type of semantic resource is that of concepts (represented by sets of lexical items) and their inter-relationships. While there is relatively good agreement as to what concepts are One of the leading methods in semantics acquisition is based on patterns (see e.g., (Hearst, 1992; Pantel and Pennacchiotti, 2006)). The standard process for pattern-based relation extraction is to start with hand-selected patterns or word pairs expressing a particular relationship, and iteratively scan the corpus for co-appearances of word pairs in patterns and for patterns that contain known word pairs. This methodology is semi-supervised, requiring prespecification of the desired relationship or handcoding initial seed words or patterns. The method is quite successful, and examining its results in detail shows that concept relationships are often being manifested by several different p"
P08-1079,W04-2609,0,0.0481501,"that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of which main types of generic relationships actually exist in an unrestricted domain. Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational similarity between two pairs of words; such a measure could in principle be used by a clustering algorithm in order to deduce relationship types, but this was not discussed. Unlike (Tur"
P08-1079,P06-1015,0,0.0707764,"labor intensive and susceptible to arbitrary human decisions. In addition, manually constructed semantic databases are not easily portable across text domains or languages. Hence, there is a need for developing semantic acquisition algorithms that are as unsupervised and language independent as possible. A fundamental type of semantic resource is that of concepts (represented by sets of lexical items) and their inter-relationships. While there is relatively good agreement as to what concepts are One of the leading methods in semantics acquisition is based on patterns (see e.g., (Hearst, 1992; Pantel and Pennacchiotti, 2006)). The standard process for pattern-based relation extraction is to start with hand-selected patterns or word pairs expressing a particular relationship, and iteratively scan the corpus for co-appearances of word pairs in patterns and for patterns that contain known word pairs. This methodology is semi-supervised, requiring prespecification of the desired relationship or handcoding initial seed words or patterns. The method is quite successful, and examining its results in detail shows that concept relationships are often being manifested by several different patterns. In this paper, unlike th"
P08-1079,P06-1102,0,0.0831268,", while we attempt to discover and define generic relationships that exist in the entire domain. Studying relationships between tagged named entities, (Hasegawa et al., 2004; Hassan et al., 2006) proposed unsupervised clustering methods that assign given sets of pairs into several clusters, where each cluster corresponds to one of a known set of relationship types. Their classification setting is thus very different from our unsupervised discovery one. Several recent papers discovered relations on the web using seed patterns (Pantel et al., 2004), rules (Etzioni et al., 2004), and word pairs (Pasca et al., 2006; Alfonseca et al., 2006). The latter used the notion of hook which we also use in this paper. Several studies utilize some preprocessing, including parsing (Hasegawa et al., 2004; Hassan et al., 2006) and usage of syntactic (Suchanek et al., 2006) and morphological (Pantel et al., 2004) information in patterns. Several algorithms use manuallyprepared resources, including WordNet (Moldovan et al., 2004; Costello et al., 2006) and Wikipedia (Strube and Ponzetto, 2006). In this paper, we do not utilize any language-specific preprocessing or any other resources, which makes our algorithm relative"
P08-1079,P06-1101,0,0.0921488,"sts, while our focus is on generating them. 693 6 present SAT and comparison evaluation results. 2 Related Work Extraction of relation information from text is a large sub-field in NLP. Major differences between pattern approaches include the relationship types sought (including domain restrictions), the degrees of supervision and required preprocessing, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, v"
P08-1079,W06-0503,0,0.0137569,"sign given sets of pairs into several clusters, where each cluster corresponds to one of a known set of relationship types. Their classification setting is thus very different from our unsupervised discovery one. Several recent papers discovered relations on the web using seed patterns (Pantel et al., 2004), rules (Etzioni et al., 2004), and word pairs (Pasca et al., 2006; Alfonseca et al., 2006). The latter used the notion of hook which we also use in this paper. Several studies utilize some preprocessing, including parsing (Hasegawa et al., 2004; Hassan et al., 2006) and usage of syntactic (Suchanek et al., 2006) and morphological (Pantel et al., 2004) information in patterns. Several algorithms use manuallyprepared resources, including WordNet (Moldovan et al., 2004; Costello et al., 2006) and Wikipedia (Strube and Ponzetto, 2006). In this paper, we do not utilize any language-specific preprocessing or any other resources, which makes our algorithm relatively easily portable between languages, as we demonstrate in our bilingual evaluation. 2.3 Evaluation Method Evaluation for hypernymy and synonymy usually uses WordNet (Lin and Pantel, 2002; Widdows and Dorow, 2002; Davidov and Rappoport, 2006). For"
P08-1079,H05-1047,0,0.00882861,"ice.nc.huji.ac.il arir@cs.huji.ac.il Abstract and which concepts should exist in a lexical resource, identifying types of important lexical relationships is a rather difficult task. Most established resources (e.g., WordNet) represent only the main and widely accepted relationships such as hypernymy and meronymy. However, there are many other useful relationships between concepts, such as noun-modifier and inter-verb relationships. Identifying and representing these explicitly can greatly assist various tasks and applications. There are already applications that utilize such knowledge (e.g., (Tatu and Moldovan, 2005) for textual entailment). We present a novel framework for the discovery and representation of general semantic relationships that hold between lexical items. We propose that each such relationship can be identified with a cluster of patterns that captures this relationship. We give a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges highfrequency words-based patterns around randomly selected hook words. Pattern clusters can be used to extract instances of the corresponding relationships. To assess the quality of discovered relationships, we use th"
P08-1079,P06-1040,0,0.317083,"are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of which main types of generic relationships actually exist in an unrestricted domain. Turney (2006) provided a pattern distance measure that allows a fully unsupervised measurement of relational similarity between two pairs of words; such a measure could in principle be used by a clustering algorithm in order to deduce relationship types, but this was not discussed. Unlike (Turney, 2006), we do not perform any pattern ranking. Instead we produce (possibly overlapping) hard clusters, where each pattern cluster represents a relationship discovered in the domain. Banko et al. (2007) and Rosenfeld and Feldman (2007) find relationship instances where the relationships are not specified in advanc"
P08-1079,C02-1114,0,0.783286,"AT and comparison evaluation results. 2 Related Work Extraction of relation information from text is a large sub-field in NLP. Major differences between pattern approaches include the relationship types sought (including domain restrictions), the degrees of supervision and required preprocessing, and evaluation method. 2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy (Hearst, 1992; Pantel et al., 2004; Snow et al., 2006), synonymy (Davidov and Rappoport, 2006; Widdows and Dorow, 2002) and meronymy (Berland and Charniak, 1999; Girju et al., 2006). Since named entities are very important in NLP, many studies define and discover relations between named entities (Hasegawa et al., 2004; Hassan et al., 2006). Work was also done on relations between verbs (Chklovski and Pantel, 2004). There is growing research on relations between nominals (Moldovan et al., 2004; Girju et al., 2007). 2.2 Degree of Supervision and Preprocessing While numerous studies attempt to discover one or more pre-specified relationship types, very little previous work has directly attempted the discovery of"
P08-1079,C02-1144,0,\N,Missing
P08-1098,W04-3202,0,0.02149,"ure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in 867 or"
P08-1098,P96-1042,0,0.173428,"verage of a wide variety of domains in human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a"
P08-1098,W05-0619,0,0.00963254,") and Brown (right) 40000 5000 10000 15000 constituents 20000 25000 30000 35000 constituents Figure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994)"
P08-1098,J04-3001,0,0.70764,"an expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse t"
P08-1098,J93-2004,0,0.0340148,"ding classifier. As a consequence, training such a classifier which takes into account several annotation tasks will best be performed on a rich corpus annotated with respect to all inputrelevant tasks. Both kinds of annotation tasks, similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as"
P08-1098,P00-1016,0,0.0259827,"n human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might us"
P08-1098,J05-1004,0,0.0421395,"similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as possible for a set of classifiers instead of a single classifier only. This enables the creation of a single combined corpus annotated with respect to various annotation tasks, while preserving the advantages of AL with 862 respect to"
P08-1098,P07-1052,1,0.855102,"etric. It is calculated on the token-level as V Etok (t) = − c V (li , t) V (li , t) 1 X log (1) log k i=0 k k where V (lki ,t) is the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each sentence in the unlabeled pool, the agreement between the committee members was calculated using the function reported by Reichart and Rappoport (2007): AF (s) = 1 N X f score(mi , ml ) (2) i,l∈[1...N ],i6=l Where mi and ml are the committee members and N = k2 ·(k22 −1) is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF (s). 4.2 Experimental settings For the NE task we employed the classifier described by Tomanek et al. (2007): The NE tagger is based on Conditional Random Fields (Lafferty et al., 2001) 5 We randomly sampled L = e"
P08-1098,W04-1221,0,0.0193363,"Missing"
P08-1098,P04-1075,0,0.0205662,"E task on WSJ (left) and Brown (right) 40000 5000 10000 15000 constituents 20000 25000 30000 35000 constituents Figure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combinati"
P08-1098,tomanek-hahn-2008-approximating,1,0.775115,"selection can be a better choice than one-sided selection in multiple annotation scenarios. Thus, considering all annotation tasks in the selection process (even if the selection protocol is as simple as the alternating selection protocol) is better than selecting only with respect to one task. Further, it should be noted that overall the more sophisticated rank combination protocol does not perform much better than the simpler alternating selection protocol in all scenarios. Finally, Figure 4 shows the disagreement curves for the two tasks on the WSJ corpus. As has already been discussed by Tomanek and Hahn (2008), disagreement curves can be used as a stopping criterion and to monitor the progress of AL-driven annotation. This is especially valuable when no annotated validation set is available (which is needed for plotting learning curves). We can see that the disagreement curves significantly flatten approximately at the same time as the learning curves do. In the context of MTAL, disagreement curves might not only be interesting as a stopping criterion but rather as a switching criterion, i.e., to identify when MTAL could be turned into one-sided selection. This would be the case if in an MTAL scena"
P08-1098,D07-1051,1,0.935507,"tions to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse trees and named entity annotations. For such an applic"
P08-1098,D07-1082,0,0.0153277,"protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in 867 order to improve the classification error rate for one specific classification task. In contrast, the focus of multi-task AL is on strategies to select training material for multi classifier systems where all classifiers cover different classification tasks. 7 Discussion Our treatment of MTAL within the context of the or"
P08-1098,J03-4003,0,\N,Missing
P08-1098,W03-0419,0,\N,Missing
P08-1098,P02-1016,0,\N,Missing
P08-1117,P05-1022,0,0.0890764,"Missing"
P08-1117,P04-1054,0,0.0535599,"n-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems"
P08-1117,P08-1079,1,0.824531,"to identify specialized phrase types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations be"
P08-1117,P07-1030,1,0.762465,"e types needed by their FSAs; once our system has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities;"
P08-1117,P07-2040,0,0.0615941,"unberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to"
P08-1117,J93-2004,0,0.0322102,"represented by commas, there are two main strands of research with similar goals: 1) systems that directly analyze commas, whether labeling them with syntactic information or correcting inappropriate use in text; and 2) systems that extract relations from text, typically by trying to identify paraphrases. The significance of interpreting the role of commas in sentences has already been identified by (van Delden and Gomez, 2002; Bayraktar et al., 1998) and others. A review of the first line of research is given in (Say and Akman, 1997). In (Bayraktar et al., 1998) the WSJ PennTreebank corpus (Marcus et al., 1993) is analyzed and a very detailed list of syntactic patterns that correspond to different roles of commas is created. However, they do not study the extraction of entailed relations as a function of the comma’s interpretation. Furthermore, the syntactic patterns they identify are unlexicalized and would not support the level of semantic relations that we show in this paper. Finally, theirs is a manual process completely dependent on syntactic patterns. While our comma resolution system uses syntactic parse information as its main source of features, the approach we have developed focuses on the"
P08-1117,P06-1015,0,0.0143877,"o relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of senten"
P08-1117,P06-1102,0,0.0129114,"tem has been trained, it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general ap"
P08-1117,W04-2401,1,0.775126,"eloping comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited ac"
P08-1117,P06-2094,0,0.0190797,", it can be applied directly to raw text. Fourth, they exclude from their analysis and evaluation any comma they deem to have been incorrectly used in the source text. We include all commas that are present in the 1032 text in our annotation and evaluation. There is a large body of NLP literature on punctuation. Most of it, however, is concerned with aiding syntactic analysis of sentences and with developing comma checkers, much based on (Nunberg, 1990). Pattern-based relation extraction methods (e.g., (Davidov and Rappoport, 2008; Davidov et al., 2007; Banko et al., 2007; Pasca et al., 2006; Sekine, 2006)) could in theory be used to extract relations represented by commas. However, the types of patterns used in web-scale lexical approaches currently constrain discovered patterns to relatively short spans of text, so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and li"
P08-1117,W04-3206,0,0.0131183,", so will most likely fail on structures whose arguments cover large spans (for example, appositional clauses containing relative clauses). Relation extraction approaches such as (Roth and Yih, 2004; Roth and Yih, 2007; Hirano et al., 2007; Culotta and Sorenson, 2004; Zelenko et al., 2003) focus on relations between Named Entities; such approaches miss the more general apposition and list relations we recognize in this work, as the arguments in these relations are not confined to Named Entities. Paraphrase Acquisition work such as that by (Lin and Pantel, 2001; Pantel and Pennacchiotti, 2006; Szpektor et al., 2004) is not constrained to named entities, and by using dependency trees, avoids the locality problems of lexical methods. However, these approaches have so far achieved limited accuracy, and are therefore hard to use to augment existing NLP systems. 4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manually annotated them with comma information3 . This annotated corpus served as both training and test datasets (using cross-validation). By studying a number of sentences from WSJ (not among"
P09-1004,P98-1013,0,0.349106,"Missing"
P09-1004,A97-1052,0,0.109374,"n carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using shallow vs. full constituency syntactic information in an English SRL system. Their results clearly demonstrate the advantage of using full annotation. The identification of arguments has also been carried out in the context of automatic subcategorization frame acquisition. Notable examples include (Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002) who all used statistical hypothesis testing to filter a parser’s output for arguments, with the goal of compiling verb subcategorization lexicons. However, these works differ from ours as they attempt to characterize the behavior of a verb type, by collecting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, no"
P09-1004,burchardt-etal-2006-salsa,0,0.0836252,"Missing"
P09-1004,W04-2412,0,0.0805222,"Missing"
P09-1004,W05-0620,0,0.274152,"Missing"
P09-1004,E03-1009,0,0.0425319,"Missing"
P09-1004,P07-1071,0,0.0258833,"Missing"
P09-1004,palmer-etal-2008-pilot,0,0.00647986,"tation of the corpus to be annotated. The three works above are relevant but incomparable to our work, due to the extensive amount of supervision (namely, VerbNet and a rule-based or supervised syntactic system) they used, both in detecting the syntactic structure and in detecting the arguments. Work has been carried out in a few other languages besides English. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of"
P09-1004,N06-2026,0,0.085771,"Missing"
P09-1004,J05-1004,0,0.746745,"Missing"
P09-1004,J02-3001,0,0.977533,"classifier to determine for each word whether it is inside, outside or in the beginning of an argument (Hacioglu and Ward, 2003). Other works have integrated argument classification and identification into one step (Collobert and Weston, 2007), while others went further and combined the former two along with parsing into a single model (Musillo Related Work The advance of machine learning based approaches in this field owes to the usage of large scale annotated corpora. English is the most stud29 proach used similarity measures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using"
P09-1004,P06-2038,0,0.0231847,"gorization lexicons. However, these works differ from ours as they attempt to characterize the behavior of a verb type, by collecting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, notably in the CoNLL 2001 shared task (Tjong Kim Sang and D`ejean, 2001). Clause information has been applied to accelerating a syntactic parser (Glaysher and Moldovan, 2006). and Merlo, 2006). Work on less supervised methods has been scarce. Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. However, they utilize the extensive verb lexicon VerbNet, which lists the possible argument structures allowable for each verb, and supervised syntactic tools. Using VerbNet along with the output of a rule-based chunker (in 2004) and a supervised syntactic parser (in 2005), they spot instances in the corpus that are very similar to the syntactic patterns listed in VerbNet. They then use these as seed"
P09-1004,J08-2006,0,0.717808,"Missing"
P09-1004,P07-1025,0,0.143726,"equential tagging of words, training an SVM classifier to determine for each word whether it is inside, outside or in the beginning of an argument (Hacioglu and Ward, 2003). Other works have integrated argument classification and identification into one step (Collobert and Weston, 2007), while others went further and combined the former two along with parsing into a single model (Musillo Related Work The advance of machine learning based approaches in this field owes to the usage of large scale annotated corpora. English is the most stud29 proach used similarity measures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a de"
P09-1004,W06-1601,0,0.77507,"hm requires thousands to dozens of thousands sentences annotated with POS tags, syntactic annotation and SRL annotation. Current algorithms show impressive results but only for languages and domains where plenty of annotated data is available, e.g., English newspaper texts (see Section 2). Results are markedly lower when testing is on a domain wider than the training one, even in English (see the WSJ-Brown results in (Pradhan et al., 2008)). Only a small number of works that do not require manually labeled SRL training data have been done (Swier and Stevenson, 2004; Swier and Stevenson, 2005; Grenager and Manning, 2006). These papers have replaced this data with the VerbNet (Kipper et al., 2000) lexical resource or a set of manually written rules and supervised parsers. A potential answer to the SRL training data bottleneck are unsupervised SRL models that require little to no manual effort for their training. Their output can be used either by itself, or as training material for modern supervised SRL algorithms. In this paper we present an algorithm for unsupervised argument identification. The only type of annotation required by our algorithm is POS tagThe task of Semantic Role Labeling (SRL) is often divi"
P09-1004,P07-1049,0,0.170148,"model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of argument candidates, which a-priori contains all consecutive sequences of words that do not contain the predicate in question. 3.1 Algorithm overview As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sen30 L tence (Seginer, 2007). This parser is unique in that it is able to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) achieving state-of-the-art results. Since our algorithm uses millions to tens of millions sentences, we must use very fast tools. The parser’s high speed (thousands of words per second) enables us to process these large amounts of data. L L DT NNS The materials L IN L L L VBP in DT NN reach each The only type of supervised annotation we use is POS tagging. We use the taggers MXPOST (Ratnaparkhi, 1996) for English and TreeTagger (Schmid, 1994) for Spanish, to obtain"
P09-1004,C04-1186,0,0.0108749,"asures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using shallow vs. full constituency syntactic information in an English SRL system. Their results clearly demonstrate the advantage of using full annotation. The identification of arguments has also been carried out in the context of automatic subcategorization frame acquisition. Notable examples include (Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002) who all used statistical hypothesis testing to filter a parser’s output for arguments, with the goal of compiling verb subcategorization lexicons. However,"
P09-1004,P06-1072,0,0.0195592,"Missing"
P09-1004,N03-2009,0,0.0560056,"Missing"
P09-1004,N07-2021,0,0.0249603,"sh. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of argument candidates, which a-priori contains all consecutive sequences of words that do not contain the predicate in question. 3.1 Algorithm overview As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sen30 L tence (Seginer, 2007). This parser is unique in that it is able to induce a bracketing (unlabeled parsing) fro"
P09-1004,H05-1111,0,0.159997,"cting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, notably in the CoNLL 2001 shared task (Tjong Kim Sang and D`ejean, 2001). Clause information has been applied to accelerating a syntactic parser (Glaysher and Moldovan, 2006). and Merlo, 2006). Work on less supervised methods has been scarce. Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. However, they utilize the extensive verb lexicon VerbNet, which lists the possible argument structures allowable for each verb, and supervised syntactic tools. Using VerbNet along with the output of a rule-based chunker (in 2004) and a supervised syntactic parser (in 2005), they spot instances in the corpus that are very similar to the syntactic patterns listed in VerbNet. They then use these as seed for a bootstrapping algorithm, which consequently identifies the verb arguments in the corpus and assigns their semantic roles"
P09-1004,W01-0708,0,0.0857682,"Missing"
P09-1004,W04-3212,0,0.0783785,"e form. WP. The constituent is preceded by a Wh-pronoun. That. The constituent is preceded by a “that” marked by an “IN” POS tag indicating that it is a subordinating conjunction. Argument identification. For each predicate in the corpus, its argument candidates are now defined to be the constituents contained in the minimal clause containing the predicate. However, these constituents may be (and are) nested within each other, violating a major restriction on SRL arguments. Hence we now prune our set, by keeping only the siblings of all of the verb’s ancestors, as is common in supervised SRL (Xue and Palmer, 2004). Spanish CQUE. The constituent is preceded by a word with the POS “CQUE” which denotes the word “que” as a conjunction. INT. The constituent is preceded by a word with the POS “INT” which denotes an interrogative pronoun. CSUB. The constituent is preceded by a word with one of the POSs “CSUBF”, “CSUBI” or “CSUBX”, which denote a subordinating conjunction. Figure 2: The set of lexico-syntactic patterns that mark clauses which were used by our model. 3.3 Using collocations We use the following observation to filter out some superfluous argument candidates: since the arguments of a predicate man"
P09-1004,J08-2004,0,0.0210271,"g a test corpus. However, ARGID was not the task of that work, which dealt solely with argument classification. ARGID was performed by manually-created rules, requiring a supervised or manual syntactic annotation of the corpus to be annotated. The three works above are relevant but incomparable to our work, due to the extensive amount of supervision (namely, VerbNet and a rule-based or supervised syntactic system) they used, both in detecting the syntactic structure and in detecting the arguments. Work has been carried out in a few other languages besides English. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannot"
P09-1004,W04-3213,0,\N,Missing
P09-1004,W96-0213,0,\N,Missing
P09-1004,J08-2001,0,\N,Missing
P09-1004,W05-0628,0,\N,Missing
P09-1004,J04-4004,0,\N,Missing
P09-1004,S07-1008,0,\N,Missing
P09-1004,N07-1070,0,\N,Missing
P09-1004,C98-1013,0,\N,Missing
P09-1004,P93-1032,0,\N,Missing
P09-1004,W06-2303,0,\N,Missing
P10-1011,J90-1003,0,0.0570847,", w2 ) = log Language 4 Lexicon Generation Experiments We tested our algorithm by generating bilingual lexicons for Hebrew and Spanish, using English as a pivot language. We chose a language pair for which basically no parallel corpora exist2 , and that do not share ancestry or writing system in a way that can provide cues for alignment. We conducted the test twice: once creating a Hebrew-Spanish lexicon, and once creating a Spanish-Hebrew one. P r(w1 , w2 ) P r(w1 )P r(w2 ) 4.1 where P r(w1 , w2 ) is the co-occurrence count, and P r(wi ) is the total number of appearance of wi in the corpus (Church and Hanks, 1990). We define the signature G(w)N,k of w to be the set of N words with the highest PMI with w. Note that a word’s signature includes words in the same language. Therefore, two signatures of words in different languages cannot be directly compared; we compare them using a lexicon L as explained below. Signature is a function of w parameterized by N and k. We discuss the selection of these parameters in section 4.1.5. Experimental Setup N ASL (s, t) = |{w∈G(s)|L(w)∩G(t)6=∅}| N 4.1.1 Corpora The Hebrew and Spanish corpora were extracted from Israeli and Spanish newspaper websites respectively (see"
P10-1011,W09-0805,1,0.878958,"Missing"
P10-1011,W09-1117,0,0.0943313,"tion Rapp (1999) and Fung (1998) discussed semantic similarity estimation using cross-lingual context vector alignment. Both works rely on a pre-existing large (16-20K entries), correct, oneto-one lexicon between the source and target languages, which is used to align context vectors between languages. The context vector data was extracted from comparable (monolingual but domain-related) corpora. Koehn and Knight (2002) were able to do without the initial large lexicon by limiting themselves to related languages that share a writing system, and using identicallyspelled words as context words. Garera et al. (2009) and Pekar et al. (2006) suggested different methods for improving the context vectors data in each language before aligning them. Garera et al. (2009) replaced the traditional window-based cooccurrence counting with dependency-tree based counting, while Pekar et al. (2006) predicted missing co-occurrence values based on similar words in the same language. In the latter work, the oneto-one lexicon assumption was not made: when a context word had multiple equivalents, it was mapped into all of them, with the original probability equally distributed between them. 3 Algorithm Our algorithm transf"
P10-1011,C96-1006,0,0.122729,"Bilingual lexicons can be generated using nonparallel corpora or pivot language lexicons (see 1 In this paper we focus on single word head entries. Multi-word expressions form a major topic in NLP and their handling is deferred to future work. 98 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98–107, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics intersection for a correct translation pair printemps and primavera may include two synonym words, spring and springtime. Variations of this method were proposed by (Kaji and Aizono, 1996; Bond et al., 2001; Paik et al., 2004; Ahn and Frampton, 2006). One weakness of IC is that it relies on pivot language synonyms to identify correct translations. In the above example, if the relatively rare springtime had not existed or was missing from the input lexicons, IC would not have been able to discern that primavera is a correct translation. This may result in low recall. tion and signature ranking. The signature of word w is the set of words that co-occur with w most strongly. While co-occurrence scores are used to compute signatures, signatures, unlike context vectors, do not cont"
P10-1011,kaji-etal-2008-automatic,0,0.719272,"ual cooccurrences to improve a lexicon generated using a pivot language was suggested by Tanaka and Iwasaki (1996). Schafer and Yarowsky (2002) created lexicons between English and a target local language (e.g. Gujarati) using a related language (e.g. Hindi) as pivot. An English pivot lexicon was used in conjunction with pivot-target cognates. Cross-lingual co-occurrences were used to remove errors, together with other cues such as edit distance and Inverse Document Frequencies (IDF) scores. It appears that this work assumed a single alignment was possible from English to the target language. Kaji et al. (2008) used a pivot English lexicon to generate initial Japanese-Chinese and ChineseJapanese lexicons, then used co-occurrences information, aligned using the initial lexicon, to identify correct translations. Unlike other works, which require alignments of pairs (i.e., two cooccurring words in one language translatable into two co-occurring words in the other), this method 3.1 Input Resources The resources required by our algorithm as evaluated in this paper are: (a) two bilingual lexicons, one from the source to the pivot language and the other from the pivot to the target language. In principle,"
P10-1011,W02-0902,0,0.664612,"ir of which frequently cooccurring. This is a relatively rare occurrence, which may explain the low recall rates of their results. oumalova, 2001). 2.3 Cross-lingual Co-occurrences in Lexicon Construction Rapp (1999) and Fung (1998) discussed semantic similarity estimation using cross-lingual context vector alignment. Both works rely on a pre-existing large (16-20K entries), correct, oneto-one lexicon between the source and target languages, which is used to align context vectors between languages. The context vector data was extracted from comparable (monolingual but domain-related) corpora. Koehn and Knight (2002) were able to do without the initial large lexicon by limiting themselves to related languages that share a writing system, and using identicallyspelled words as context words. Garera et al. (2009) and Pekar et al. (2006) suggested different methods for improving the context vectors data in each language before aligning them. Garera et al. (2009) replaced the traditional window-based cooccurrence counting with dependency-tree based counting, while Pekar et al. (2006) predicted missing co-occurrence values based on similar words in the same language. In the latter work, the oneto-one lexicon as"
P10-1011,P09-1030,0,0.0583566,"ations. We present the non-aligned signatures (NAS) similarity score for signature and use it to rank these translations. NAS is based on the number of headword signature words that may be translated using the input noisy lexicon into words in the signature of a candidate translation. We evaluate our algorithm by generating a bilingual lexicon for Hebrew and Spanish using pivot Hebrew-English and English-Spanish lexicons compiled by a professional publishing house. We show that the algorithm outperforms existing algorithms for handling divergence induced by lexical intransitivity. 2 2.1 2.2.2 Mausam et al. (2009) used many input bilingual lexicons to create bilingual lexicons for new language pairs. They represent the multiple input lexicons in a single undirected graph, with words from all the lexicons as nodes. The input lexicons translation pairs define the edges in the graph. New translation pairs are inferred based on cycles in the graph, that is, the existence of multiple paths between two words in different languages. In a sense, this is a generalization of the pivot language idea, where multiple pivots are used. In the example above, if both English and German are used as pivots, printemps and"
P10-1011,W04-2204,0,0.521036,"nonparallel corpora or pivot language lexicons (see 1 In this paper we focus on single word head entries. Multi-word expressions form a major topic in NLP and their handling is deferred to future work. 98 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98–107, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics intersection for a correct translation pair printemps and primavera may include two synonym words, spring and springtime. Variations of this method were proposed by (Kaji and Aizono, 1996; Bond et al., 2001; Paik et al., 2004; Ahn and Frampton, 2006). One weakness of IC is that it relies on pivot language synonyms to identify correct translations. In the above example, if the relatively rare springtime had not existed or was missing from the input lexicons, IC would not have been able to discern that primavera is a correct translation. This may result in low recall. tion and signature ranking. The signature of word w is the set of words that co-occur with w most strongly. While co-occurrence scores are used to compute signatures, signatures, unlike context vectors, do not contain the score values. For each given s"
P10-1011,W06-2006,0,0.0670259,"a or pivot language lexicons (see 1 In this paper we focus on single word head entries. Multi-word expressions form a major topic in NLP and their handling is deferred to future work. 98 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98–107, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics intersection for a correct translation pair printemps and primavera may include two synonym words, spring and springtime. Variations of this method were proposed by (Kaji and Aizono, 1996; Bond et al., 2001; Paik et al., 2004; Ahn and Frampton, 2006). One weakness of IC is that it relies on pivot language synonyms to identify correct translations. In the above example, if the relatively rare springtime had not existed or was missing from the input lexicons, IC would not have been able to discern that primavera is a correct translation. This may result in low recall. tion and signature ranking. The signature of word w is the set of words that co-occur with w most strongly. While co-occurrence scores are used to compute signatures, signatures, unlike context vectors, do not contain the score values. For each given source headword we compute"
P10-1011,P99-1067,0,0.981209,"d to the same sense of the headword, and would therefore constitute translations of each other. The applicability of this method is limited by the availability of machine-readable dictionaries produced by the same publishing house. Not surprisingly, this method has been proposed by lexicographers working in such companies (Sk99 relies on alignments of 3-word cliques in each language, every pair of which frequently cooccurring. This is a relatively rare occurrence, which may explain the low recall rates of their results. oumalova, 2001). 2.3 Cross-lingual Co-occurrences in Lexicon Construction Rapp (1999) and Fung (1998) discussed semantic similarity estimation using cross-lingual context vector alignment. Both works rely on a pre-existing large (16-20K entries), correct, oneto-one lexicon between the source and target languages, which is used to align context vectors between languages. The context vector data was extracted from comparable (monolingual but domain-related) corpora. Koehn and Knight (2002) were able to do without the initial large lexicon by limiting themselves to related languages that share a writing system, and using identicallyspelled words as context words. Garera et al. (2"
P10-1011,2001.mtsummit-papers.10,0,0.0957921,"be generated using nonparallel corpora or pivot language lexicons (see 1 In this paper we focus on single word head entries. Multi-word expressions form a major topic in NLP and their handling is deferred to future work. 98 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98–107, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics intersection for a correct translation pair printemps and primavera may include two synonym words, spring and springtime. Variations of this method were proposed by (Kaji and Aizono, 1996; Bond et al., 2001; Paik et al., 2004; Ahn and Frampton, 2006). One weakness of IC is that it relies on pivot language synonyms to identify correct translations. In the above example, if the relatively rare springtime had not existed or was missing from the input lexicons, IC would not have been able to discern that primavera is a correct translation. This may result in low recall. tion and signature ranking. The signature of word w is the set of words that co-occur with w most strongly. While co-occurrence scores are used to compute signatures, signatures, unlike context vectors, do not contain the score value"
P10-1011,C96-2098,0,0.0582524,"mpute its signature and the signatures of each of its translation candidates. Signature computation utilizes a monolingual corpus to discover the words that are most strongly related to the word. We now rank the candidates according to the non-aligned signatures (NAS) similarity score, which assesses the similarity between each candidate’s signature and that of the headword. For each headword, we select the t translations with the highest NAS scores as correct translations. Pivot Language. Using cross-lingual cooccurrences to improve a lexicon generated using a pivot language was suggested by Tanaka and Iwasaki (1996). Schafer and Yarowsky (2002) created lexicons between English and a target local language (e.g. Gujarati) using a related language (e.g. Hindi) as pivot. An English pivot lexicon was used in conjunction with pivot-target cognates. Cross-lingual co-occurrences were used to remove errors, together with other cues such as edit distance and Inverse Document Frequencies (IDF) scores. It appears that this work assumed a single alignment was possible from English to the target language. Kaji et al. (2008) used a pivot English lexicon to generate initial Japanese-Chinese and ChineseJapanese lexicons,"
P10-1011,C94-1048,0,0.537001,"-readable bilingual lexicons. The texts are aligned to each other, at chunk- and/or word-level. Alignment is generally evaluated by consistency (source words should be translated to a small number of target words over the entire corpus) and minimal shifting (in each occurrence, the source should be aligned to a translation nearby). For a review of such methods see (Lopez, 2008). The limited availability of parallel corpora of sufficient size for most language pairs restricts the usefulness of these methods. 2.2 Multiple Pivot Languages Pivot Language Without Corpora 2.2.1 Inverse Consultation Tanaka and Umemura (1994) generated a bilingual lexicon using a pivot language. They approached lexical intransitivity divergence using Inverse Consultation (IC). IC examines the intersection of two pivot language sets: the set of pivot translations of a source-language word w, and the set of pivot translations of each target-language word that is a candidate for being a translation to w. IC generally requires that the intersection set contains at least two words, which are synonyms. For example, the intersection of the English translations of French printemps and Spanish resorte contains only a single word, spring. T"
P10-1011,W02-2026,0,\N,Missing
P10-1024,P09-1004,1,0.94955,"to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised parses. Finally, Abend et al. (2009) tackled the argument identification task alone and did not perform argument classification of any sort. Subcategorization Acquisition. This task specifies for each predicate the number, type and order of obligatory arguments. Determining the allowable subcategorization frames for a given predicate necessarily involves separating its cores from its allowable adjuncts (which are not framed). Notable works in the field include (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). All these works used a parsed corpus in order to collect, for each predicate, a set of hypothesized su"
P10-1024,P98-1013,0,0.117176,"Missing"
P10-1024,J98-2002,0,0.0393524,"ich are the head words of the argument (see below). The head words themselves, once chosen, are represented by the lemma. We now compute the following measures. Selectional Preference (SP). Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores. Selectional preference induction is a wellestablished task in NLP. It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate. Several methods have been suggested (Resnik, 1996; Li and Abe, 1998; Schulte im Walde et al., 2008). We use the paradigm of (Erk, 2007). For a given predicate slot pair (p, s), we define its preference to the argument head h to be: X SP (p, s, h) = P r(h′ |p, s) · sim(h, h′ ) Predicate-Slot Collocation. Since cores are obligatory, when a predicate persistently appears with an argument in a certain slot, the arguments in this slot tends to be cores. This notion can be captured by the (predicate, slot) joint distribution. We use the Pointwise Mutual Information measure (PMI) to capture the slot and the predicate’s collocation tendency. Let p be a predicate and"
P10-1024,D09-1049,1,0.800923,"ttach to a given frame. For instance, while the ‘Getting’ 1 PropBank annotates modals and negation words as modifiers. Since these are not arguments in the common usage of the term, we exclude them from the discussion in this paper. 227 formulation models the core-adjunct distinction explicitly. Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003). supervised detection of multiword expressions (MWEs). An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al., 2003; Sporleder and Li, 2009) (see also (Boukobza and Rappoport, 2009)). Both tasks aim to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised"
P10-1024,P98-2127,0,0.208825,"wing measures: 1. Simple SP – a selectional preference measure defined to be P r(head|slot, predicate). 2. Vast Corpus SP – similar to ‘Simple SP’ but with a much larger corpus. It uses roughly 100M arguments which were extracted from the web-crawling based corpus of (Gabrilovich and Markovitch, 2005) and the British National Corpus (Burnard, 2000). 3. Thesaurus SP – a selectional preference measure which follows the paradigm of (Erk, 2007) (Section 3.3) and defines the similarity between two heads to be the Jaccard affinity between their two entries in Lin’s automatically compiled thesaurus (Lin, 1998)10 . 4. Pr(slot|predicate) – an alternative to the used predicate-slot collocation measure. 5. PMI(slot, head) – an alternative to the used argument-slot collocation measure. 6. Head Dependence – the entropy of the predicate distribution given the slot and the head (following (Merlo and Esteve Ferrer, 2006)): curate predictions in this context than Clark’s. The 39832 sentences of PropBank’s sections 2– 21 were used as a test set without bounding their lengths8 . Cores were defined to be any argument bearing the labels ‘A0’ – ‘A5’, ‘C-A0’ – ‘C-A5’ or ‘R-A0’ – ‘R-A5’. Adjuncts were defined to be"
P10-1024,W05-0620,0,0.0326707,"Missing"
P10-1024,E03-1009,0,0.241793,"field (Baldwin et al., 2009). 228 3 Algorithm (PSH) joint distribution. This section details the process of extracting samples from this joint distribution given a raw text corpus. We start by parsing the corpus using the Seginer parser (Seginer, 2007). This parser is unique in its ability to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) with strong results. Its high speed (thousands of words per second) allows us to use millions of sentences, a prohibitive number for other parsers. We continue by tagging the corpus using Clark’s unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abend et al., 2010)2 . The classes corresponding to prepositions and to verbs are manually selected from the induced clusters3 . A preposition is defined to be any word which is the first word of an argument and belongs to a prepositions cluster. A verb is any word belonging to a verb cluster. This manual selection requires only a minute, since the number of classes is very small (34 in our experiments). In addition, knowing what is considered a preposition is part of the task definition itself. Argument identification is hard even for supervised models"
P10-1024,J05-1004,0,0.440004,"Missing"
P10-1024,P07-1028,0,0.168001,"ves, once chosen, are represented by the lemma. We now compute the following measures. Selectional Preference (SP). Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores. Selectional preference induction is a wellestablished task in NLP. It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate. Several methods have been suggested (Resnik, 1996; Li and Abe, 1998; Schulte im Walde et al., 2008). We use the paradigm of (Erk, 2007). For a given predicate slot pair (p, s), we define its preference to the argument head h to be: X SP (p, s, h) = P r(h′ |p, s) · sim(h, h′ ) Predicate-Slot Collocation. Since cores are obligatory, when a predicate persistently appears with an argument in a certain slot, the arguments in this slot tends to be cores. This notion can be captured by the (predicate, slot) joint distribution. We use the Pointwise Mutual Information measure (PMI) to capture the slot and the predicate’s collocation tendency. Let p be a predicate and s a slot, then: P S(p, s) = P M I(p, s) = log = log N (p, s)Σp′ ,s′"
P10-1024,W10-2911,1,0.832945,"to tag each argument separately. Finally, most works address the task at the verb type level, trying to detect the allowable frames for each type. Consequently, the common evaluation focuses on the quality of the allowable frames acquired for each verb type, and not on the classification of specific arguments in a given corpus. Such a token level evaluation was conducted in a few works (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000), but often with a small number of verbs or a small number of frames. A discussion of the differences between type and token level evaluation can be found in (Reichart et al., 2010). PP attachment. PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb. This task’s relation to the core-adjunct distinction was addressed in several works. For instance, the results of (Hindle and Rooth, 1993) indicate that their PP attachment system works better for cores than for adjuncts. Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks. Unlike in this work, their classifier requires extensive supervision incl"
P10-1024,W06-1601,0,0.209909,"ude them from the discussion in this paper. 227 formulation models the core-adjunct distinction explicitly. Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003). supervised detection of multiword expressions (MWEs). An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al., 2003; Sporleder and Li, 2009) (see also (Boukobza and Rappoport, 2009)). Both tasks aim to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised parses. Finally, Abend et al. (2009) tackled the argument identification task alone and did not perform argument classification of any sort. Subcategorization Acquisition. This task spec"
P10-1024,saint-dizier-2006-prepnet,0,0.0270763,"Missing"
P10-1024,J93-1005,0,0.205251,"a given corpus. Such a token level evaluation was conducted in a few works (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000), but often with a small number of verbs or a small number of frames. A discussion of the differences between type and token level evaluation can be found in (Reichart et al., 2010). PP attachment. PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb. This task’s relation to the core-adjunct distinction was addressed in several works. For instance, the results of (Hindle and Rooth, 1993) indicate that their PP attachment system works better for cores than for adjuncts. Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks. Unlike in this work, their classifier requires extensive supervision including WordNet, language-specific features and a supervised parser. Their features are generally motivated by common linguistic considerations. Features found adaptable to a completely unsupervised scenario are used in this work as well. The core-adjunct distinction task was tackled in the context of child language"
P10-1024,C00-2100,0,0.0345506,"from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised parses. Finally, Abend et al. (2009) tackled the argument identification task alone and did not perform argument classification of any sort. Subcategorization Acquisition. This task specifies for each predicate the number, type and order of obligatory arguments. Determining the allowable subcategorization frames for a given predicate necessarily involves separating its cores from its allowable adjuncts (which are not framed). Notable works in the field include (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). All these works used a parsed corpus in order to collect, for each predicate, a set of hypothesized subcategorization frames, to be filtered by hypothesis testing methods. This line of work differs from ours in a few aspects. First, all works use manual or supervised syntactic annotations, usually including a POS tagger. Second, the common approach to the task focuses on syntax and tries to identify the entire frame, rather than to tag each argument separately. Finally, most works address the task at the verb type level, trying to detect the allowable frames for each type. C"
P10-1024,P08-1057,0,0.019391,"argument (see below). The head words themselves, once chosen, are represented by the lemma. We now compute the following measures. Selectional Preference (SP). Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores. Selectional preference induction is a wellestablished task in NLP. It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate. Several methods have been suggested (Resnik, 1996; Li and Abe, 1998; Schulte im Walde et al., 2008). We use the paradigm of (Erk, 2007). For a given predicate slot pair (p, s), we define its preference to the argument head h to be: X SP (p, s, h) = P r(h′ |p, s) · sim(h, h′ ) Predicate-Slot Collocation. Since cores are obligatory, when a predicate persistently appears with an argument in a certain slot, the arguments in this slot tends to be cores. This notion can be captured by the (predicate, slot) joint distribution. We use the Pointwise Mutual Information measure (PMI) to capture the slot and the predicate’s collocation tendency. Let p be a predicate and s a slot, then: P S(p, s) = P M"
P10-1024,P07-1049,0,0.107502,"(Collins, 1999). In his Model 2, Collins modifies his parser to provide a coreadjunct prediction, thereby improving its performance. The Combinatory Categorial Grammar (CCG) The study of prepositions is a vibrant research area in NLP. A special issue of Computational Linguistics, which includes an extensive survey of related work, was recently devoted to the field (Baldwin et al., 2009). 228 3 Algorithm (PSH) joint distribution. This section details the process of extracting samples from this joint distribution given a raw text corpus. We start by parsing the corpus using the Seginer parser (Seginer, 2007). This parser is unique in its ability to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) with strong results. Its high speed (thousands of words per second) allows us to use millions of sentences, a prohibitive number for other parsers. We continue by tagging the corpus using Clark’s unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abend et al., 2010)2 . The classes corresponding to prepositions and to verbs are manually selected from the induced clusters3 . A preposition is defined to be any word which is the first word of an ar"
P10-1024,E09-1086,0,0.0241467,"any type of non-core argument to attach to a given frame. For instance, while the ‘Getting’ 1 PropBank annotates modals and negation words as modifiers. Since these are not arguments in the common usage of the term, we exclude them from the discussion in this paper. 227 formulation models the core-adjunct distinction explicitly. Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003). supervised detection of multiword expressions (MWEs). An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al., 2003; Sporleder and Li, 2009) (see also (Boukobza and Rappoport, 2009)). Both tasks aim to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000"
P10-1024,J08-2002,0,0.0400788,"Missing"
P10-1024,W02-2033,0,0.0238468,"t their PP attachment system works better for cores than for adjuncts. Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks. Unlike in this work, their classifier requires extensive supervision including WordNet, language-specific features and a supervised parser. Their features are generally motivated by common linguistic considerations. Features found adaptable to a completely unsupervised scenario are used in this work as well. The core-adjunct distinction task was tackled in the context of child language acquisition. Villavicencio (2002) developed a classifier based on preposition selection and frequency information for modeling the distinction for locative prepositional phrases. Her approach is not entirely corpus based, as it assumes the input sentences are given in a basic logical form. Syntactic Parsing. The core-adjunct distinction is included in many syntactic annotation schemes. Although the Penn Treebank does not explicitly annotate adjuncts and cores, a few works suggested mapping its annotation (including function tags) to core-adjunct labels. Such a mapping was presented in (Collins, 1999). In his Model 2, Collins"
P10-1024,W04-3212,0,0.0229873,"iments). In addition, knowing what is considered a preposition is part of the task definition itself. Argument identification is hard even for supervised models and is considerably more so for unsupervised ones (Abend et al., 2009). We therefore confine ourselves to sentences of length not greater than 10 (excluding punctuation) which contain a single verb. A sequence of words will be marked as an argument of the verb if it is a constituent that does not contain the verb (according to the unsupervised parse tree), whose parent is an ancestor of the verb. This follows the pruning heuristic of (Xue and Palmer, 2004) often used by SRL algorithms. The corpus is now tagged using an unsupervised POS tagger. Since the sentences in question are short, we consider every word which does not belong to a closed class cluster as a head word (an argument can have several head words). A closed class is a class of function words with relatively few word types, each of which is very frequent. Typical examples include determiners, prepositions and conjunctions. A class which is not closed is open. In this paper, we define closed classes to be clusters in which the ratio between the number of word tokens and the number o"
P10-1024,W04-3213,0,\N,Missing
P10-1024,W96-0213,0,\N,Missing
P10-1024,H05-1111,0,\N,Missing
P10-1024,J08-2005,0,\N,Missing
P10-1024,N07-1070,0,\N,Missing
P10-1024,J09-2001,0,\N,Missing
P10-1024,A97-1052,0,\N,Missing
P10-1024,J03-4003,0,\N,Missing
P10-1024,C98-1013,0,\N,Missing
P10-1024,P09-2019,0,\N,Missing
P10-1024,J06-3002,0,\N,Missing
P10-1024,P10-1132,1,\N,Missing
P10-1024,P03-1065,0,\N,Missing
P10-1024,C98-2122,0,\N,Missing
P10-1132,C04-1080,0,0.200831,"esults (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small numb"
P10-1132,P06-3002,0,0.135348,"omputational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tag"
P10-1132,J92-4003,0,0.310333,"Missing"
P10-1132,E03-1009,0,0.444192,"on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005"
P10-1132,D07-1023,0,0.0562564,"Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation eff"
P10-1132,C04-1052,0,0.0150501,"0 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary prov"
P10-1132,D08-1036,0,0.806358,"formance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model"
P10-1132,J01-2001,0,0.345512,"gment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to their association with the prototypes. Second, we use a distributional representation which has been shown to be effective for unsupervised parsing (Seginer, 2007). Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001). 3 Distributional Algorithm Our algorithm is given a plain text corpus and optionally a desired number of clusters k. Its output is a partitioning of words into clusters. The algorithm utilizes two representations, distributional and morphological. Although eventually the latter is used before the former, for clarity of presentation we begin by detailing the base distributional algorithm. In the next section we describe the morphological representation and its integration into the base algorithm. Overview. The algorithm consists of two main stages: landmark clusters discovery, and word mappin"
P10-1132,D07-1043,0,0.0843834,"accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high"
P10-1132,P07-1094,0,0.788838,"lgorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use t"
P10-1132,E95-1020,0,0.340359,"Missing"
P10-1132,P09-1059,0,0.0187048,"words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task. 1 Introduction Part-of-speech (POS) tagging is a fundamental NLP task, used by a wide variety of applications. However, there is no single standard POS tagging scheme, even for English. Schemes vary significantly across corpora and even more so across languages, creating difficulties in using POS tags across domains and for multi-lingual systems (Jiang et al., 2009). Automatic induction of POS tags from plain text can greatly alleviate this problem, as well as eliminate the efforts incurred by manual annotations. It is also a problem of great theoretical interest. Consequently, POS induction is a vibrant research area (see Section 2). In this paper we present an algorithm based on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the ∗ Omri Abend is grateful to the Azr"
P10-1132,D07-1031,0,0.388703,"lark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstra"
P10-1132,P07-1049,0,0.547456,"vised POS Induction through Prototype Discovery Omri Abend1∗ Roi Reichart2 1 Ari Rappoport1 Institute of Computer Science, 2 ICNC Hebrew University of Jerusalem {omria01|roiri|arir}@cs.huji.ac.il Abstract central members. Our algorithm first clusters words based on a fine morphological representation. It then clusters the most frequent words, defining landmark clusters which constitute the cores of the categories. Finally, it maps the rest of the words to these categories. The last two stages utilize a distributional representation that has been shown to be effective for unsupervised parsing (Seginer, 2007). We evaluated the algorithm in both English and German, using four different mapping-based and information theoretic clustering evaluation measures. The results obtained are generally better than all existing POS induction algorithms. Section 2 reviews related work. Sections 3 and 4 detail the algorithm. Sections 5, 6 and 7 describe the evaluation, experimental setup and results. We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of t"
P10-1132,P05-1044,0,0.156947,"given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding t"
P10-1132,D09-1072,0,0.35004,"to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology"
P10-1132,J94-2001,0,0.0948809,"best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model"
P10-1132,P09-1057,0,0.290719,"and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the ent"
P10-1132,C08-1091,1,0.86954,"a derived accuracy. The Many-to-1 measure finds the mapping between the gold standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster. The 1-to-1 measure finds the mapping between the induced and gold standard clusters which maximizes accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three d"
P10-1132,W09-1121,1,0.696459,"rs are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high frequency threshold), n = 50 (the para"
P10-1132,W10-2911,1,0.823065,"nts is based on their association with the ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computat"
P10-1132,W10-2909,1,0.563823,"Missing"
P10-1132,D09-1071,0,\N,Missing
P10-1132,N06-1041,0,\N,Missing
P10-1132,P08-1085,0,\N,Missing
P10-1133,P99-1008,0,0.0380724,", Section 3 details the algorithmic framework, Section 4 describes the experimental setup, and Section 5 presents our results. 2 Related work Numerous methods have been developed for extraction of diverse semantic relationships from text. While several studies propose relationship identification methods using distributional analysis of feature vectors (Turney, 2005), the majority of the proposed open-domain relations extraction frameworks utilize lexical patterns connecting a pair of related terms. (Hearst, 1992) manually designed lexico-syntactic patterns for extracting hypernymy relations. (Berland and Charniak, 1999; Girju et al, 2006) proposed a set of patterns for meronymy relations. Davidov and Rappoport (2008a) used pattern clusters to disambiguate nominal compound relations. Extensive frameworks were proposed for iterative discovery of any pre-specified (e.g., (Riloff and Jones, 1999; Chklovski and Pantel, 2004)) and unspecified (e.g., (Banko et al., 2007; Rosenfeld and Feldman, 2007; Davidov and Rappoport, 2008b)) relation types. The majority of the above methods utilize the following basic strategy. Given (or discovering automatically) a set of patterns or relationshiprepresenting term pairs, thes"
P10-1133,W04-3205,0,0.0320834,"sing distributional analysis of feature vectors (Turney, 2005), the majority of the proposed open-domain relations extraction frameworks utilize lexical patterns connecting a pair of related terms. (Hearst, 1992) manually designed lexico-syntactic patterns for extracting hypernymy relations. (Berland and Charniak, 1999; Girju et al, 2006) proposed a set of patterns for meronymy relations. Davidov and Rappoport (2008a) used pattern clusters to disambiguate nominal compound relations. Extensive frameworks were proposed for iterative discovery of any pre-specified (e.g., (Riloff and Jones, 1999; Chklovski and Pantel, 2004)) and unspecified (e.g., (Banko et al., 2007; Rosenfeld and Feldman, 2007; Davidov and Rappoport, 2008b)) relation types. The majority of the above methods utilize the following basic strategy. Given (or discovering automatically) a set of patterns or relationshiprepresenting term pairs, these methods mine the web for these patterns and pairs, iteratively obtaining more instances. The proposed strategies generally include some weighting/frequency/contextbased algorithms (e.g. (Pantel and Pennacchiotti, 2006)) to reduce noise. Some of the methods are suitable for retrieval of numerical attribut"
P10-1133,P06-1038,1,0.822802,"d comparison data Extraction of comparison information. The third group, Pcompare , consists of comparison patterns. They allow to compare objects directly even when no attribute values are mentioned. This group includes attribute equality patterns such as ‘[Object1] has the same width as [Object2]’, and attribute inequality ones such as ‘[Object1] is wider than [Object2]’. We execute search queries for each of these patterns, and extract a set of ordered term pairs, keeping track of the relationships encoded by the pairs. We use these pairs to build a directed graph (Widdows and Dorow, 2002; Davidov and Rappoport, 2006) in which nodes are objects (not necessarily with assigned values) and edges correspond to extracted co-appearances of objects inside the comparison patterns. The directions of edges are determined by the comparison sign. If two objects co-appear inside an equality pattern we put a bidirectional edge between them. Now we would like to extract the attribute values for the given object and its similar objects. We will also extract bounds and comparison information in order to verify the extracted values and to approximate the missing ones. To allow us to extract attribute-specific information, w"
P10-1133,P07-1030,1,0.829648,"mining for attribute values and comparison statements; (3) processing the results. 3.1 Similar objects and class label To verify and estimate attribute values for the given object we utilize similar objects (cohyponyms) and the object’s class label (hypernym). In the WN enrichment scenario we can easily obtain these, since we get the object’s synset as input. However, in Question Answering (QA) scenarios we do not have such information. To obtain it we employ a strategy which uses WordNet along with pattern-based web mining. Our web mining part follows common patternbased retrieval practice (Davidov et al., 2007). We utilize Yahoo! Boss API to perform search engine queries. For an object name Obj we query the Web using a small set of pre-defined co-hyponymy patterns like “as * and/or [Obj]”2 . In the WN enrichment scenario, we can add the WN class label to each query in order to restrict results to the desired word sense. In the QA scenario, if we are given the full question and not just the (object, attribute) pair we can add terms appearing in the question and having a strong PMI with the object (this can be estimated using any fixed corpus). However, this is not essential. We then extract new terms"
P10-1133,P08-1027,1,0.784101,"tion 5 presents our results. 2 Related work Numerous methods have been developed for extraction of diverse semantic relationships from text. While several studies propose relationship identification methods using distributional analysis of feature vectors (Turney, 2005), the majority of the proposed open-domain relations extraction frameworks utilize lexical patterns connecting a pair of related terms. (Hearst, 1992) manually designed lexico-syntactic patterns for extracting hypernymy relations. (Berland and Charniak, 1999; Girju et al, 2006) proposed a set of patterns for meronymy relations. Davidov and Rappoport (2008a) used pattern clusters to disambiguate nominal compound relations. Extensive frameworks were proposed for iterative discovery of any pre-specified (e.g., (Riloff and Jones, 1999; Chklovski and Pantel, 2004)) and unspecified (e.g., (Banko et al., 2007; Rosenfeld and Feldman, 2007; Davidov and Rappoport, 2008b)) relation types. The majority of the above methods utilize the following basic strategy. Given (or discovering automatically) a set of patterns or relationshiprepresenting term pairs, these methods mine the web for these patterns and pairs, iteratively obtaining more instances. The prop"
P10-1133,P08-1079,1,0.731066,"tion 5 presents our results. 2 Related work Numerous methods have been developed for extraction of diverse semantic relationships from text. While several studies propose relationship identification methods using distributional analysis of feature vectors (Turney, 2005), the majority of the proposed open-domain relations extraction frameworks utilize lexical patterns connecting a pair of related terms. (Hearst, 1992) manually designed lexico-syntactic patterns for extracting hypernymy relations. (Berland and Charniak, 1999; Girju et al, 2006) proposed a set of patterns for meronymy relations. Davidov and Rappoport (2008a) used pattern clusters to disambiguate nominal compound relations. Extensive frameworks were proposed for iterative discovery of any pre-specified (e.g., (Riloff and Jones, 1999; Chklovski and Pantel, 2004)) and unspecified (e.g., (Banko et al., 2007; Rosenfeld and Feldman, 2007; Davidov and Rappoport, 2008b)) relation types. The majority of the above methods utilize the following basic strategy. Given (or discovering automatically) a set of patterns or relationshiprepresenting term pairs, these methods mine the web for these patterns and pairs, iteratively obtaining more instances. The prop"
P10-1133,J06-1005,0,0.0161823,"rithmic framework, Section 4 describes the experimental setup, and Section 5 presents our results. 2 Related work Numerous methods have been developed for extraction of diverse semantic relationships from text. While several studies propose relationship identification methods using distributional analysis of feature vectors (Turney, 2005), the majority of the proposed open-domain relations extraction frameworks utilize lexical patterns connecting a pair of related terms. (Hearst, 1992) manually designed lexico-syntactic patterns for extracting hypernymy relations. (Berland and Charniak, 1999; Girju et al, 2006) proposed a set of patterns for meronymy relations. Davidov and Rappoport (2008a) used pattern clusters to disambiguate nominal compound relations. Extensive frameworks were proposed for iterative discovery of any pre-specified (e.g., (Riloff and Jones, 1999; Chklovski and Pantel, 2004)) and unspecified (e.g., (Banko et al., 2007; Rosenfeld and Feldman, 2007; Davidov and Rappoport, 2008b)) relation types. The majority of the above methods utilize the following basic strategy. Given (or discovering automatically) a set of patterns or relationshiprepresenting term pairs, these methods mine the w"
P10-1133,C92-2082,0,0.331068,"llowed a meaningful approximation of missing attribute values. Section 2 discusses related work, Section 3 details the algorithmic framework, Section 4 describes the experimental setup, and Section 5 presents our results. 2 Related work Numerous methods have been developed for extraction of diverse semantic relationships from text. While several studies propose relationship identification methods using distributional analysis of feature vectors (Turney, 2005), the majority of the proposed open-domain relations extraction frameworks utilize lexical patterns connecting a pair of related terms. (Hearst, 1992) manually designed lexico-syntactic patterns for extracting hypernymy relations. (Berland and Charniak, 1999; Girju et al, 2006) proposed a set of patterns for meronymy relations. Davidov and Rappoport (2008a) used pattern clusters to disambiguate nominal compound relations. Extensive frameworks were proposed for iterative discovery of any pre-specified (e.g., (Riloff and Jones, 1999; Chklovski and Pantel, 2004)) and unspecified (e.g., (Banko et al., 2007; Rosenfeld and Feldman, 2007; Davidov and Rappoport, 2008b)) relation types. The majority of the above methods utilize the following basic s"
P10-1133,W06-1808,0,0.0277479,"ze a small set of patterns to extract physical object sizes and use the averages of the obtained values for a noun compound classification task. (Banerjee et al, 2009) developed a method for dealing with quantity consensus queries (QCQs) where there is uncertainty about the answer quantity (e.g. “driving time from Paris to Nice”). They utilize a textual snippet feature and snippet quantity in order to select and rank intervals of the requested values. This approach is particularly useful when it is possible to obtain a substantial amount of a desired attribute values for the requested query. (Moriceau, 2006) proposed a rulebased system which analyzes the variation of the extracted numerical attribute values using information in the textual context of these values. A significant body of recent research deals with extraction of various data from web tables and lists (e.g., (Cafarella et al., 2008; Crestan and Pantel, 2010)). While in the current research we do not utilize this type of information, incorporation of the numerical data extracted from semistructured web pages can be extremely beneficial for our framework. All of the above numerical attribute extraction systems utilize only direct infor"
P10-1133,P06-1015,0,0.0218947,"were proposed for iterative discovery of any pre-specified (e.g., (Riloff and Jones, 1999; Chklovski and Pantel, 2004)) and unspecified (e.g., (Banko et al., 2007; Rosenfeld and Feldman, 2007; Davidov and Rappoport, 2008b)) relation types. The majority of the above methods utilize the following basic strategy. Given (or discovering automatically) a set of patterns or relationshiprepresenting term pairs, these methods mine the web for these patterns and pairs, iteratively obtaining more instances. The proposed strategies generally include some weighting/frequency/contextbased algorithms (e.g. (Pantel and Pennacchiotti, 2006)) to reduce noise. Some of the methods are suitable for retrieval of numerical attributes. However, most of them do not exploit the numerical nature of the attribute data. Our research is related to a sub-domain of question answering (Prager, 2006), since one of the applications of our framework is answering questions on numerical values. The majority of the proposed QA frameworks rely on pattern-based relationship acquisition (Ravichandran and Hovy, 2009). However, most QA studies focus on different types of problems than our paper, including question classification, paraphrasing, etc. 1309 S"
P10-1133,P02-1006,0,0.132232,"Missing"
P10-1133,C02-1114,0,0.0218951,"ing for values, bounds and comparison data Extraction of comparison information. The third group, Pcompare , consists of comparison patterns. They allow to compare objects directly even when no attribute values are mentioned. This group includes attribute equality patterns such as ‘[Object1] has the same width as [Object2]’, and attribute inequality ones such as ‘[Object1] is wider than [Object2]’. We execute search queries for each of these patterns, and extract a set of ordered term pairs, keeping track of the relationships encoded by the pairs. We use these pairs to build a directed graph (Widdows and Dorow, 2002; Davidov and Rappoport, 2006) in which nodes are objects (not necessarily with assigned values) and edges correspond to extracted co-appearances of objects inside the comparison patterns. The directions of edges are determined by the comparison sign. If two objects co-appear inside an equality pattern we put a bidirectional edge between them. Now we would like to extract the attribute values for the given object and its similar objects. We will also extract bounds and comparison information in order to verify the extracted values and to approximate the missing ones. To allow us to extract att"
P10-1133,S07-1103,0,\N,Missing
P11-1067,N10-1083,0,0.0100955,"Missing"
P11-1067,P10-1131,0,0.00560222,"ing logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-super"
P11-1067,W04-1501,0,0.0673155,"in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex"
P11-1067,D10-1117,0,0.186649,"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 2004; Cohen et al., 2008; Headden et al., 2009; Spitkovsky et al., 2010a; Gillenwater et al., 2010; Berg-Kirkpatrick et al., 2010; Blunsom and Cohn, 2010, inter alia). Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated. However, there are cases in which there is no linguistic consensus as to what the correct annotation is (K¨ubler et al., 2009). Examples include which verb is the head in a verb group structure (e.g., “can” or “eat” in “can eat”), and which ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguis"
P11-1067,N09-1009,0,0.0289598,"blematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of researc"
P11-1067,P09-1041,0,0.00977741,"re. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent sta"
P11-1067,P10-2036,0,0.0356408,"Missing"
P11-1067,C08-1042,0,0.0740417,"Missing"
P11-1067,N09-1012,0,0.26345,"Missing"
P11-1067,W07-2416,0,0.0665149,"note that these controversies are reflected in the evaluation of this task, resulting in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s sc"
P11-1067,P04-1061,0,0.712703,"lizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substa"
P11-1067,J93-2004,0,0.0435294,"Missing"
P11-1067,D10-1120,0,0.0191741,"Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and ve"
P11-1067,nivre-etal-2006-maltparser,0,0.00840752,"applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (2005) compared two different conversion schemes in German supervised constituency parsing and found one to have positive influence on parsing quality. Dependen"
P11-1067,P06-1033,0,0.363717,"iner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (200"
P11-1067,rambow-etal-2002-dependency,0,0.0689242,", if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possib"
P11-1067,P06-1072,0,0.0773069,"Missing"
P11-1067,N10-1116,0,0.195398,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,P10-1130,0,0.232129,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W10-2902,0,0.312419,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W05-1516,0,0.038097,"Missing"
P11-1067,W06-2904,0,0.0476608,"Missing"
P11-1067,W03-3023,0,0.776919,"”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possible, since different papers use different gold standard annotations even when they are all deriv"
P11-1067,J03-4003,0,\N,Missing
P11-1067,D07-1112,0,\N,Missing
P11-1067,D07-1096,0,\N,Missing
P13-1023,W12-3602,0,0.0249551,"d language teaching) (Siddharthan, 2006), paraphrase detection (Dolan et al., 2004), summarization (Knight and Marcu, 2000), and question answering (Wang et al., 2007). 5 2009)), recent work has also successfully tackled the task of predicting semantic structures in the form of DAGs (Jones et al., 2012). The most prominent annotation scheme in NLP for English syntax is the Penn Treebank. Many syntactic schemes are built or derived from it. An increasingly popular alternative to the PTB are dependency structures, which are usually represented as trees whose nodes are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to naturally represent constructions that have more than one head, or in which the identity of the head is not clear. They also face difficulties in representing units that participate in multiple relations. UCCA proposes a different formalism that addresses these problems by introducing a new node for every relation (cf. (Sangati and Mazza, 2009)). Related Work Several annotated corpora offer a joint syntactic and semantic representation. Examples include the Groningen Meaning bank (Basile et al., 2012), Treebank Semantics (Butler and"
P13-1023,W13-0101,1,0.811533,"d and determine its semantic type. There can be one or more Cs in a non-Scene unit4 . Other sub-units of non-Scene units are categorized into three types. First, units that apply to a single C are annotated as Elaborators (E). For instance, “big” in “big dogs” is an E, while “dogs” is a C. We also mark determiners as Es in this coarsegrained layer5 . Second, relations that relate two or 2 As UCCA annotates categories on its edges, Scene nodes bear no special indication. They can be identified by examining the labels on their outgoing edges (see below). 3 Repeated here with minor changes from (Abend and Rappoport, 2013), which focuses on the categories themselves. 4 By allowing several Cs we avoid the difficulties incurred by the common single head assumption. In some cases the Cs are inferred from context and can be implicit. 5 Several Es that apply to a single C are often placed in 230 more Cs, highlighting a common feature or role (usually coordination), are called Connectors (N). See an example in Figure 2(b). Relators (R) cover all other types of relations between two or more Cs. Rs appear in two main varieties. In one, Rs relate a single entity to a super-ordinate relation. For instance, in “I heard no"
P13-1023,C12-1083,0,0.0412168,"Missing"
P13-1023,J08-4004,0,0.0169918,"Missing"
P13-1023,P98-1013,0,0.064756,"by the structural pattern it appears in. Table 1: The complete set of categories in UCCA’s foundational layer. specifications. The definition of a Scene is motivated cross-linguistically and is similar to the semantic aspect of the definition of a “clause” in Basic Linguistic Theory2 . Table 1 provides a concise description of the categories used by the foundational layer3 . We turn to a brief description of them. Simple Scenes. Every Scene contains one main relation, which is the anchor of the Scene, the most important relation it describes (similar to frameevoking lexical units in FrameNet (Baker et al., 1998)). We distinguish between static Scenes, that describe a temporally persistent state, and processual Scenes that describe a temporally evolving event, usually a movement or an action. The main relation receives the category State (S) in static and Process (P) in processual Scenes. We note that the S-P distinction is introduced here mostly for practical purposes, and that both categories can be viewed as sub-categories of the more abstract category Main Relation. A Scene contains one or more Participants (A). This category subsumes concrete and abstract participants as well as embedded Scenes ("
P13-1023,basile-etal-2012-developing,0,0.0780182,"are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to naturally represent constructions that have more than one head, or in which the identity of the head is not clear. They also face difficulties in representing units that participate in multiple relations. UCCA proposes a different formalism that addresses these problems by introducing a new node for every relation (cf. (Sangati and Mazza, 2009)). Related Work Several annotated corpora offer a joint syntactic and semantic representation. Examples include the Groningen Meaning bank (Basile et al., 2012), Treebank Semantics (Butler and Yoshimoto, 2012) and the Lingo Redwoods treebank (Oepen et al., 2004). UCCA diverges from these projects in aiming to abstract away from syntactic variation, and is therefore less coupled with a specific syntactic theory. In this section we compare UCCA to some of the major approaches to grammatical representation in NLP. We focus on English, which is the most studied language and the focus of this paper. Syntactic annotation schemes come in many forms, from lexical categories such as POS tags to intricate hierarchical structures. Some formalisms focus particul"
P13-1023,D10-1119,0,0.0565017,"ly represent semantic distinctions. Put differently, UCCA advocates an approach that treats syntax as a hidden layer when learning the mapping between form and meaning, while existing syntactic approaches aim to model it manually and explicitly. UCCA does not build on any other annotation layers and therefore implicitly assumes that semantic annotation can be learned directly. Recent work suggests that indeed structured prediction methods have reached sufficient maturity to allow direct learning of semantic distinctions. Examples include Naradowsky et al. (2012) for semantic role labeling and Kwiatkowski et al. (2010) for semantic parsing to logical forms. While structured prediction for the task of predicting tree structures is already well established (e.g., (Suzuki et al., A different strand of work addresses the construction of an interlingual representation, often with a motivation of applying it to machine translation. Examples include the UNL project (Uchida and Zhu, 2001), the IAMTC project (Dorr et al., 2010) and the AMR project (Banarescu et al., 2012). These projects share with UCCA their emphasis on cross-linguistically valid annotations, but diverge from UCCA in three important respects. First"
P13-1023,J93-2004,0,0.0680554,"ound, and describe the compilation of a UCCAannotated corpus. 1 Introduction Syntactic structures are mainly committed to representing the formal patterns of a language, and only indirectly reflect semantic distinctions. For instance, while virtually all syntactic annotation schemes are sensitive to the structural difference between (a) “John took a shower” and (b) “John showered”, they seldom distinguish between (a) and the markedly different (c) “John took my book”. In fact, the annotations of (a) and (c) are identical under the most widely-used schemes for English, the Penn Treebank (PTB) (Marcus et al., 1993) and CoNLL-style dependencies (Surdeanu et al., 2008) (see Figure 1). ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 228 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228–238, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics and represent the descendant unit’s role in forming the semantics of the parent unit. Therefore, the internal structure of a unit is represented by its outbound edges and their categories, while the roles a unit plays in the relations it participat"
P13-1023,meyers-etal-2004-annotating,0,0.0538404,"Missing"
P13-1023,D12-1074,0,0.0193629,"to abstract away from specific syntactic forms and to only represent semantic distinctions. Put differently, UCCA advocates an approach that treats syntax as a hidden layer when learning the mapping between form and meaning, while existing syntactic approaches aim to model it manually and explicitly. UCCA does not build on any other annotation layers and therefore implicitly assumes that semantic annotation can be learned directly. Recent work suggests that indeed structured prediction methods have reached sufficient maturity to allow direct learning of semantic distinctions. Examples include Naradowsky et al. (2012) for semantic role labeling and Kwiatkowski et al. (2010) for semantic parsing to logical forms. While structured prediction for the task of predicting tree structures is already well established (e.g., (Suzuki et al., A different strand of work addresses the construction of an interlingual representation, often with a motivation of applying it to machine translation. Examples include the UNL project (Uchida and Zhu, 2001), the IAMTC project (Dorr et al., 2010) and the AMR project (Banarescu et al., 2012). These projects share with UCCA their emphasis on cross-linguistically valid annotations,"
P13-1023,C04-1051,0,0.010373,"g in the park”). The second contains a simple clause headed by “playing”. While the parse trees of these sentences are very different, their UCCA annotation in the foundational layer differ only in terms of Function units: “ChildrenA [areF playingC ]P [inR theE parkC ]A ” and “ThereF areF childrenA [playing]P [inR theE parkC ]A ”10 . Aside from machine translation, a great variety of semantic tasks can benefit from a scheme that is relatively insensitive to syntactic variation. Examples include text simplification (e.g., for second language teaching) (Siddharthan, 2006), paraphrase detection (Dolan et al., 2004), summarization (Knight and Marcu, 2000), and question answering (Wang et al., 2007). 5 2009)), recent work has also successfully tackled the task of predicting semantic structures in the form of DAGs (Jones et al., 2012). The most prominent annotation scheme in NLP for English syntax is the Penn Treebank. Many syntactic schemes are built or derived from it. An increasingly popular alternative to the PTB are dependency structures, which are usually represented as trees whose nodes are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to n"
P13-1023,J05-1004,0,0.0450142,"Missing"
P13-1023,P11-1067,1,0.370397,"Missing"
P13-1023,W08-2121,0,0.028666,"Missing"
P13-1023,D09-1058,0,0.0606921,"Missing"
P13-1023,J11-4006,0,0.0206347,"Missing"
P13-1023,D07-1003,0,0.0118337,"arse trees of these sentences are very different, their UCCA annotation in the foundational layer differ only in terms of Function units: “ChildrenA [areF playingC ]P [inR theE parkC ]A ” and “ThereF areF childrenA [playing]P [inR theE parkC ]A ”10 . Aside from machine translation, a great variety of semantic tasks can benefit from a scheme that is relatively insensitive to syntactic variation. Examples include text simplification (e.g., for second language teaching) (Siddharthan, 2006), paraphrase detection (Dolan et al., 2004), summarization (Knight and Marcu, 2000), and question answering (Wang et al., 2007). 5 2009)), recent work has also successfully tackled the task of predicting semantic structures in the form of DAGs (Jones et al., 2012). The most prominent annotation scheme in NLP for English syntax is the Penn Treebank. Many syntactic schemes are built or derived from it. An increasingly popular alternative to the PTB are dependency structures, which are usually represented as trees whose nodes are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to naturally represent constructions that have more than one head, or in which the ident"
P13-1023,C98-1013,0,\N,Missing
P15-2119,P14-2050,0,0.0210063,"nt is considered large) . We model this task as a learning problem, in which concepts have a feature representation based on a state-of-the-art DM. A property-predictor is then trained to predict, for any given concept, whether the property applies to it or not (in a binary classification setup), or the strength of affiliation between the property and the concept (in a regression setup). By evaluating the performance of these predictors, we assess the degree to which the property is captured by the DM. We experiment with four state-of-the-art DMs (Baroni and Lenci, 2010; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). Our results show that all DMs, quite successful in many semantic tasks, fail when it comes to predicting attributive properties of concepts. For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties. This result, which may be attributed to an essential difference between taxonomic and attributive properties, demonstrates possible limitations of the distributional hypothesis, at least in terms of the information captured by c"
P15-2119,J10-4006,0,0.77946,"r not (e.g., whether or not the concept elephant is considered large) . We model this task as a learning problem, in which concepts have a feature representation based on a state-of-the-art DM. A property-predictor is then trained to predict, for any given concept, whether the property applies to it or not (in a binary classification setup), or the strength of affiliation between the property and the concept (in a regression setup). By evaluating the performance of these predictors, we assess the degree to which the property is captured by the DM. We experiment with four state-of-the-art DMs (Baroni and Lenci, 2010; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). Our results show that all DMs, quite successful in many semantic tasks, fail when it comes to predicting attributive properties of concepts. For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties. This result, which may be attributed to an essential difference between taxonomic and attributive properties, demonstrates possible limitations of the distributional hypothesis, at"
P15-2119,D14-1162,0,0.0774181,"We model this task as a learning problem, in which concepts have a feature representation based on a state-of-the-art DM. A property-predictor is then trained to predict, for any given concept, whether the property applies to it or not (in a binary classification setup), or the strength of affiliation between the property and the concept (in a regression setup). By evaluating the performance of these predictors, we assess the degree to which the property is captured by the DM. We experiment with four state-of-the-art DMs (Baroni and Lenci, 2010; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). Our results show that all DMs, quite successful in many semantic tasks, fail when it comes to predicting attributive properties of concepts. For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties. This result, which may be attributed to an essential difference between taxonomic and attributive properties, demonstrates possible limitations of the distributional hypothesis, at least in terms of the information captured by current stateof-the-art DMs"
P15-2119,P14-1023,0,0.294874,"nal hypothesis may not be equally applicable to all types of semantic information. 1 Introduction The Distributional Hypothesis states that the meaning of words can be inferred from their linguistic environment (Harris, 1954). This hypothesis lies at the heart of distributional models (DMs), which approximate the meaning of words by considering the statistics of their co-occurrence with other words in the lexicon. DMs have shown impressive results in many semantic tasks, such as predicting the similarity of two words, grouping words into semantic categories, and solving analogy questions (see Baroni et al. (2014) for a recent survey). They are also used as a source of semantic information by many downstream applications, including syntactic parsing (Socher et al., 2013), image annotation (Klein et al., 2014), and semantic frame identification (Hermann et al., 2014). 726 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 726–730, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Binary Classification. For each property p, we take concepts"
P15-2119,C14-1153,1,0.884029,"Missing"
P15-2119,P13-1045,0,0.0198942,"can be inferred from their linguistic environment (Harris, 1954). This hypothesis lies at the heart of distributional models (DMs), which approximate the meaning of words by considering the statistics of their co-occurrence with other words in the lexicon. DMs have shown impressive results in many semantic tasks, such as predicting the similarity of two words, grouping words into semantic categories, and solving analogy questions (see Baroni et al. (2014) for a recent survey). They are also used as a source of semantic information by many downstream applications, including syntactic parsing (Socher et al., 2013), image annotation (Klein et al., 2014), and semantic frame identification (Hermann et al., 2014). 726 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 726–730, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Binary Classification. For each property p, we take concepts for which p applies to be positive instances, and concepts for which it does not as negative instances. For example, the property is loud is positive for a trum"
P15-2119,J15-4004,0,\N,Missing
P15-2119,P14-1136,0,\N,Missing
P16-1198,W06-2920,0,0.182577,"Missing"
P16-1198,D07-1101,0,0.383801,"Missing"
P16-1198,W05-1504,0,0.109175,"Missing"
P16-1198,E12-1008,0,0.0406703,"Missing"
P16-1198,Q15-1035,0,0.542351,"Missing"
P16-1198,P06-1063,0,0.0317044,"Missing"
P16-1198,P10-1001,0,0.223319,"Missing"
P16-1198,D10-1125,0,0.0466361,"Missing"
P16-1198,J93-2004,0,0.0536623,"Missing"
P16-1198,P09-1039,0,0.0560953,"Missing"
P16-1198,D11-1022,0,0.0387869,"Missing"
P16-1198,P13-2109,0,0.157969,"earlier transition based work. 2 Undirected MST with the Boruvka Algorithm In this section we define the MST problem in undirected graphs. We then discuss the Burovka algorithm (Boruvka, 1926; Nesetril et al., 2001) which forms the basis for the randomized algorithm of (Karger et al., 1995) we employ in this paper. In the next section we will describe the Karger et al. (1995) algorithm in more details. Problem Definition. For a connected undirected graph G(V, E), where V is the set of n vertices fer here to the classical implementation employed by modern parsers (e.g. (McDonald et al., 2005b; Martins et al., 2013)). 2105 and E the set of m weighted edges, the MST problem is defined as finding the sub-graph of G which is the tree (a connected acyclic graph) with the lowest sum of edge weights. The opposite problem – finding the maximum spanning tree – can be solved by the same algorithms used for the minimum variant by simply negating the graph’s edge weights. Graph Contraction. In order to understand the Boruvka algorithm, let us first define the Graph Contraction operation. For a given undirected ˜ ⊆ E, this opergraph G(V, E) and a subset E ation creates a new graph, GC (VC , EC ). In this new graph,"
P16-1198,E06-1011,0,0.387379,"Missing"
P16-1198,D07-1096,0,0.38472,"Missing"
P16-1198,D12-1067,0,0.738231,"Missing"
P16-1198,N12-1054,0,0.0357112,"Missing"
P16-1198,D10-1001,0,0.0530841,"Missing"
P16-1198,D08-1016,0,0.231051,"Missing"
P16-1198,D12-1030,0,0.0356175,"Missing"
P16-1198,W07-2216,0,0.062195,"Missing"
P16-1198,P05-1012,0,0.16488,"is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n2 ) directed MST inference. 1 Introduction Dependency parsers are major components of a large number of NLP applications. As application models are applied to constantly growing amounts of data, efficiency becomes a major consideration. 1 We refer to parsing approaches that produce only projective dependency trees as projective parsing and to approaches that produce all types of dependency trees as non-projective parsing. 2 Some pruning algorithms require initial construction of the full graph, which r"
P16-1198,H05-1066,0,0.142187,"is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n2 ) directed MST inference. 1 Introduction Dependency parsers are major components of a large number of NLP applications. As application models are applied to constantly growing amounts of data, efficiency becomes a major consideration. 1 We refer to parsing approaches that produce only projective dependency trees as projective parsing and to approaches that produce all types of dependency trees as non-projective parsing. 2 Some pruning algorithms require initial construction of the full graph, which r"
P16-1198,C96-1058,0,\N,Missing
P17-1008,P16-1231,0,0.0157234,"G and G IVING, but D EPARTING may also be evoked by “depart” and “exit”, and G IVING by “donate” and “gift”. A different approach to SRT is taken by Vector Space Models (VSM), which eschew the use of symbolic structures, instead modeling all linguistic elements as vectors, from the level of words to phrases and sentences. Proponents of this approach generally invoke neural network methods, obtaining impressive results on a variety of tasks including lexical tasks such as cross-linguistic word similarity (Ammar et al., 2016), machine translation (Bahdanau et al., 2015), and dependency parsing (Andor et al., 2016). VSMs are also attractive in being flexible enough to model non-local and gradient phenomena (e.g., Socher et al., 2013). However, more research is needed to clarify the scope of semantic phenomena that such models are able to reliably capture. We therefore only lightly touch on VSMs in this survey. Finally, a major consideration in semantic analysis, and one of its great potential advantages, is its cross-linguistic universality. While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as2 We use the term “Text Semantics”, rather tha"
P17-1008,D14-1059,0,0.00455111,"retical linguistics (Levin and Hovav, 2005) and in NLP, through approaches such as Semantic Role Labeling (SRL; Gildea and Jurafsky, 2002), formal semantic analysis (e.g., Bos, 2008), and Abstract Meaning Representation (AMR; Banarescu et al., 2013). Many other useful meaning components have been proposed, and are discussed at a greater depth in Section 3. 3 Another approach to defining an SRT is through external (extra-textual) criteria or applications. For instance, a semantic representation can be defined to support inference, as in textual entailment (Dagan et al., 2006) or natural logic (Angeli and Manning, 2014). Other examples include defining a semantic representation in terms of supporting knowledge base querying (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), or defining semantics through a different modality, for instance interpreting text in terms of images that correspond to it (Kiros et al., 2014), or in terms of embodied motor and perceptual schemas (Feldman et al., 2010). Semantic Content We turn to discussing the main content types encoded by semantic representation schemes. Due to space limitations, we focus only on text semantics, which studies the meaning relationships between"
P17-1008,Q13-1005,0,0.0440543,"ections and motions, and their relative configuration. Logical Structure. Logical structure, including quantification, negation, coordination and their associated scope distinctions, is the cornerstone of semantic analysis in much of theoretical linguistics, and has attracted much attention in NLP as well. Common representations are often based on variants of predicate calculus, and are useful for applications that require mapping text into an external, often executable, formal language, such as a querying language (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or robot instructions (Artzi and Zettlemoyer, 2013). Logical structures are also useful for recognizing entailment relations between sentences, as some entailments can be computed from the text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and informati"
P17-1008,W13-2322,0,0.580068,"(e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field. 1 Introduction Schemes for Semantic Representation of Text (SRT) aim to reflect the meaning of sentences and texts in a transparent way. There has recently been an influx of proposals for semantic representations and corpora, e.g. GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013b) and Universal Decompositional Semantics (UDS; White et al., 2016). Nevertheless, no detailed assessment of the relative merits of the different schemes has been carried out, nor their comparison to previous sentential analysis schemes, notably syntactic ones. An understanding of the achievements and gaps of semantic analysis in NLP is crucial to its future prospects. In this paper we begin to chart the various proposals for semantic schemes according to the content they support. As not many semantic queries on texts can at present be answered with near human"
P17-1008,P08-1090,0,0.0150874,"6, pp. 23-24). Temporal Relations. Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time. Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013). A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Duni"
P17-1008,P09-1068,0,0.0359219,"Missing"
P17-1008,basile-etal-2012-developing,0,0.0126281,"posals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field. 1 Introduction Schemes for Semantic Representation of Text (SRT) aim to reflect the meaning of sentences and texts in a transparent way. There has recently been an influx of proposals for semantic representations and corpora, e.g. GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013b) and Universal Decompositional Semantics (UDS; White et al., 2016). Nevertheless, no detailed assessment of the relative merits of the different schemes has been carried out, nor their comparison to previous sentential analysis schemes, notably syntactic ones. An understanding of the achievements and gaps of semantic analysis in NLP is crucial to its future prospects. In this paper we begin to chart the various proposals for semantic schemes according to the content they support. As not many semantic queries on texts can at prese"
P17-1008,I05-2035,0,0.217307,"ions (Xue et al., 2014). As an example, UDS. Universal Decompositional Semantics (White et al., 2016) is a multi-layered scheme, which currently includes semantic role anno81 Structure Grammar (HPSG; Pollard and Sag, 1994), where syntactic and semantic features are represented as feature bundles, which are iteratively composed through unification rules to form composite units. HPSG-based SRT schemes commonly use the Minimal Recursion Semantics (Copestake et al., 2005) formalism. Annotated corpora and manually crafted grammars exist for multiple languages (Flickinger, 2002; Oepen et al., 2004; Bender and Flickinger, 2005, inter alia), and generally focus on argument structural and logical semantic phenomena. The Broad-coverage Semantic Dependency Parsing shared task and corpora (Oepen et al., 2014, 2015) include corpora annotated with the PDT-TL, and dependencies extracted from the HPSG grammars Enju (Miyao, 2006) and the LinGO English Reference Grammar (ERG; Flickinger, 2002). tation, word senses and aspectual classes (e.g., realis/irrealis). UDS emphasizes accessible distinctions, which can be collected through crowd-sourcing. However, the skeletal structure of UDS representations is derived from syntactic"
P17-1008,W15-0128,0,0.0248707,"s immediate constituents and their syntactic relationships, which are generally assumed to form a closed set (Montague, 1970, and much subsequent work). Thus, the interpretation of a sentence can be computed bottom-up, by establishing the meaning of individual words, and recursively composing them, to obtain the full sentential semantics. The order and type of these compositions are determined by the syntactic structure. Compositionality is employed by linguistically expressive grammars, such as those based on CCG and HPSG, and has proven to be a powerful method for various applications. See (Bender et al., 2015) for a recent discussion of the advantages of compositional SRTs. Nevertheless, a compositional account meets difficulties when faced with multi-word expressions and in accounting for cases like “he sneezed the napkin off the table”, where it is difficult to determine whether “sneezed” or “off” account for the constructional meaning. Construction Grammar (Fillmore et al., 1988; Goldberg, 1995) answers these issues by using an open set of construction-specific compositional operators, and supporting lexical en84 tries of varying lengths. Several ongoing projects address the implementation of th"
P17-1008,P16-1166,0,0.0211545,"notation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow structures of discourse units"
P17-1008,P10-1160,0,0.0252622,"ries across schemes. Most SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered. For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives. FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as “president”. Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015). Core and Non-core Arguments. Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003). While it is possible to define the distinction distributionally as one between obligatory and optional arguments, here we focus on the semantic dimension, which distinguishes arguments whose meaning is predicate-specific and are necessary components of the described event (core), and those which are predicate-general (non-core). For example, FrameNet defines core arguments as conceptually necessary components of a frame, that mak"
P17-1008,J02-3001,0,0.0351407,"rface. We note that this definition of semantics is somewhat different from the one intended here, which defines semantic schemes as theories of meaning. We stipulate that a fundamental component of the content conveyed by SRTs is argument structure – who did what to whom, where, when and why, i.e., events, their participants and the relations between them. Indeed, the fundamental status of argument structure has been recognized by essentially all approaches to semantics both in theoretical linguistics (Levin and Hovav, 2005) and in NLP, through approaches such as Semantic Role Labeling (SRL; Gildea and Jurafsky, 2002), formal semantic analysis (e.g., Bos, 2008), and Abstract Meaning Representation (AMR; Banarescu et al., 2013). Many other useful meaning components have been proposed, and are discussed at a greater depth in Section 3. 3 Another approach to defining an SRT is through external (extra-textual) criteria or applications. For instance, a semantic representation can be defined to support inference, as in textual entailment (Dagan et al., 2006) or natural logic (Angeli and Manning, 2014). Other examples include defining a semantic representation in terms of supporting knowledge base querying (Zelle"
P17-1008,C12-1089,0,0.0055984,", these schemes require considerable adaptation when ported across languages (Kozhevnikov and Titov, 2013). Ongoing research tackles the generalization of VerbNet’s unlexicalized roles to a universally applicable set (e.g., Schneider et al., 2015). Few SRT schemes place cross-linguistically applicability as one of their main criteria, examples include UCCA, and the LinGO Grammar Matrix (Bender and Flickinger, 2005), both of which draw on typological theory. Vector space models, which embed words and sentences in a vector space, have also been applied to induce a shared cross-linguistic space (Klementiev et al., 2012; Rajendran et al., 2015; Wu et al., 2016). However, further evaluation is required in order to determine what aspects of meaning these representations reflect reliably. 6 6.1 Syntax and Semantics Syntactic and Semantic Generalization 6.2 Syntactic distinctions are generally guided by a combination of semantic and distributional considerations, where emphasis varies across schemes. Consider phrase-based syntactic structures, common examples of which, such as the Penn Treebank for English (Marcus et al., 1993) and the Penn Chinese Treebank (Xue et al., 2005), are adaptations of X-bar theory. Co"
P17-1008,S12-1048,0,0.0213673,"relations beyond the scope of a single sentence are often represented by specialized semantic resources and not by general ones, despite the absence of a clear boundary line between them. This, however, is beginning to change with some schemes, e.g., GMB and UCCA, already supporting cross-sentence semantic relations.3 Spatial Relations. The representation of spatial relations is pivotal in cognitive theories of meaning (e.g., Langacker, 2008), and in application domains such as geographical information systems or robotic navigation. Important tasks in this field include Spatial Role Labeling (Kordjamshidi et al., 2012) and the more recent SpaceEval (Pustejovsky et al., 2015). The tasks include the identification and classification of spatial elements and relations, such as places, paths, directions and motions, and their relative configuration. Logical Structure. Logical structure, including quantification, negation, coordination and their associated scope distinctions, is the cornerstone of semantic analysis in much of theoretical linguistics, and has attracted much attention in NLP as well. Common representations are often based on variants of predicate calculus, and are useful for applications that requi"
P17-1008,P13-1117,0,0.024359,"purely semantic schemes such as AMR and UCCA consider (1) “founding of the school”, (2) “president of the United States” and (3) “United States president”. UD is faithful to the syntactic structure and represents (1) and (2) similarly, while assigning a different structure to (3). In contrast, AMR and UCCA perform a semantic generalization and represents examples (2) and (3) similarly and differently from (1). ever, as both PropBank and FrameNet are lexicalized schemes, and as lexicons diverge wildly across languages, these schemes require considerable adaptation when ported across languages (Kozhevnikov and Titov, 2013). Ongoing research tackles the generalization of VerbNet’s unlexicalized roles to a universally applicable set (e.g., Schneider et al., 2015). Few SRT schemes place cross-linguistically applicability as one of their main criteria, examples include UCCA, and the LinGO Grammar Matrix (Bender and Flickinger, 2005), both of which draw on typological theory. Vector space models, which embed words and sentences in a vector space, have also been applied to induce a shared cross-linguistic space (Klementiev et al., 2012; Rajendran et al., 2015; Wu et al., 2016). However, further evaluation is required"
P17-1008,E12-1059,0,0.00970673,"terms of semantic features computed from them, whose validity is established by human annotators (e.g., Agirre et al., 2013, 2014). Finally, where semantic schemes are induced through manual annotation (and not through auUniversality. One of the great promises of semantic analysis (over more surface forms of analysis) is its cross-linguistic potential. However, while the theoretical and applicative importance of universality in semantics has long been recognized (Goddard, 2011), the nature of universal semantics remains unknown. Recently, projects such as BabelNet (Ehrmann et al., 2014), UBY (Gurevych et al., 2012) and Open Multilingual Wordnet4 , constructed huge multi-lingual semantic nets, by linking resources such as Wikipedia and WordNet and processing them using modern NLP. However, such projects currently focus on lexical semantic and encyclopedic information rather than on text semantics. Symbolic SRT schemes such as SRL schemes and AMR have also been studied for their crosslinguistic applicability (Pad´o and Lapata, 2009; Sun et al., 2010; Xue et al., 2014), indicating partial portability across languages. Translated versions of PropBank and FrameNet have been constructed for multiple languages"
P17-1008,D10-1119,0,0.0189537,"el for different senses of the English possessive construction, regardless of whether they correspond to ownership (e.g., “John’s dog”) or to a different meaning, such as marking an argument of a nominal predicate (e.g., “John’s kick”). See Section 6. CCG-based Schemes. CCG (Steedman, 2000) is a lexicalized grammar (i.e., nearly all semantic content is encoded in the lexicon), which defines a theory of how lexical information is composed to form the meaning of phrases and sentences (see Section 6.2), and has proven effective in a variety of semantic tasks (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Artzi and Zettlemoyer, 2013, inter alia). Several projects have constructed logical representations by associating CCG with semantic forms (by assigning logical forms to the leaves). For example, Boxer (Bos, 2008) and GMB, which builds on Boxer, use Discourse Representation Structures (Kamp and Reyle, 1993), while Lewis and Steedman (2013) used Davidsonian-style λ-expressions, accompanied by lexical categorization of the predicates. These schemes encode events with their argument structures, and include an elaborate logical structure, as well as lexical and discourse information. OntoNotes i"
P17-1008,P13-1134,0,0.0333479,"Missing"
P17-1008,D13-1149,0,0.027642,"man evaluation is the ultimate criterion for validating an SRT scheme given our definition of semantics as meaning as it is understood by a language speaker. Determining how well an SRT scheme corresponds to human interpretation of a text is ideally carried out by asking annotators to make some semantic prediction or annotation according to pre-specified guidelines, and to compare this to the information extracted from the SRT. Question Answering SRL (QASRL; He et al., 2015) is an SRL scheme which solicits nonexperts to answer mostly wh-questions, converting their output to an SRL annotation. Hartshorne et al. (2013) and Reisinger et al. (2015) use crowdsourcing to elicit semantic role features, such as whether the argument was volitional in the described event, in order to evaluate proposals for semantic role sets. Another evaluation approach is task-based evaluation. Many semantic representations in NLP are defined with an application in mind, making this type of evaluation natural. For instance, a major motivation for AMR is its applicability to machine translation, making MT a natural (albeit hitherto unexplored) testbed for AMR evaluation. Another example is using question answering to evaluate seman"
P17-1008,Q13-1015,0,0.0481678,"theoretical linguistics, and has attracted much attention in NLP as well. Common representations are often based on variants of predicate calculus, and are useful for applications that require mapping text into an external, often executable, formal language, such as a querying language (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or robot instructions (Artzi and Zettlemoyer, 2013). Logical structures are also useful for recognizing entailment relations between sentences, as some entailments can be computed from the text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012). The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between the"
P17-1008,D15-1076,0,0.00372503,"tegies for collecting some aspects of the annotation. Schemes also differ in other aspects discussed in Sections 5 and 6. 5 Evaluation Human evaluation is the ultimate criterion for validating an SRT scheme given our definition of semantics as meaning as it is understood by a language speaker. Determining how well an SRT scheme corresponds to human interpretation of a text is ideally carried out by asking annotators to make some semantic prediction or annotation according to pre-specified guidelines, and to compare this to the information extracted from the SRT. Question Answering SRL (QASRL; He et al., 2015) is an SRL scheme which solicits nonexperts to answer mostly wh-questions, converting their output to an SRL annotation. Hartshorne et al. (2013) and Reisinger et al. (2015) use crowdsourcing to elicit semantic role features, such as whether the argument was volitional in the described event, in order to evaluate proposals for semantic role sets. Another evaluation approach is task-based evaluation. Many semantic representations in NLP are defined with an application in mind, making this type of evaluation natural. For instance, a major motivation for AMR is its applicability to machine transl"
P17-1008,liakata-etal-2010-corpora,0,0.028923,"lenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow structures of discourse units categorized either according to their topic or according to their function within the text. An example is the segmentation of scientific papers into functional segments and their labeling with categories such as BACKGROUND and D ISCUSSION (Liakata et al., 2010). See (Webber et al., 2011) for a survey of discourse structure in NLP. Discourse relations beyond the scope of a single sentence are often represented by specialized semantic resources and not by general ones, despite the absence of a clear boundary line between them. This, however, is beginning to change with some schemes, e.g., GMB and UCCA, already supporting cross-sentence semantic relations.3 Spatial Relations. The representation of spatial relations is pivotal in cognitive theories of meaning (e.g., Langacker, 2008), and in application domains such as geographical information systems or"
P17-1008,W97-1311,0,0.296131,"78 evoke the same frame type, such as “leave” and “depart”), and PropBank (where roles are verbspecific). PropBank’s role sets were extended by subsequent projects such as AMR. Another prominent semantic role inventory is VerbNet (Kipper et al., 2008) and subsequent projects (Bonial et al., 2011; Schneider et al., 2015), which define a closed set of abstract semantic roles (such as AGENT, PATIENT and I NSTRUMENT) that apply to all predicate arguments. The events discussed here should not be confused with events as defined in Information Extraction and related tasks such as event coreference (Humphreys et al., 1997), which correspond more closely to the everyday notion of an event, such as a political or financial event, and generally consist of multiple events in the sense discussed here. The representation of such events is recently receiving considerable interest within NLP, e.g. the Richer Event Descriptions framework (RED; Ikuta et al., 2014). Co-reference and Anaphora. Co-reference allows to abstract away from the different ways to refer to the same entity, and is commonly included in semantic resources. Coreference interacts with argument structure annotation, as in its absence each argument is ar"
P17-1008,P07-1113,0,0.0157312,"recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow struc"
P17-1008,miltsakaki-etal-2004-penn,0,0.0823945,"text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012). The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between them into a hierarchical, closed category set, including high-level relation types like T EMPORAL, C OMPARISON and C ONTINGENCY and finer-grained ones such as J USTIFICATION and E XCEPTION. Another commonly used resource is the RST Discourse TreeInference and Entailment. A primary motivation for many semantic schemes is their ability to support inference and entailment. Indeed, means for predicting logical entailment are built into many forms of semantic representations. A different approach was taken in the tasks of Recognizing"
P17-1008,J05-1004,0,0.341427,")). This distinction may be important for text understanding, as the inferred cases tend to be more ambiguous (“she” in (1) might not refer to “Ann”). Other schemes, such as AMR, eschew this distinction and use the same terms to represent all cases of coreference. Predicates and Arguments. While predicateargument relations are universally recognized as fundamental to semantic representation, the interpretation of the terms varies across schemes. Most SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered. For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives. FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as “president”. Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015). Core and Non-core Arguments. Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003). Whi"
P17-1008,W14-0702,0,0.0194485,"(Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual clas"
P17-1008,J88-2003,0,0.861713,"applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open chal"
P17-1008,W16-1007,0,0.0048061,"le, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson a"
P17-1008,Q16-1010,0,0.0535261,"Missing"
P17-1008,P10-1100,0,0.0164222,"Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time. Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013). A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recentl"
P17-1008,S15-2153,0,0.129344,"Missing"
P17-1008,Q15-1034,0,0.142523,"Missing"
P17-1008,S14-2008,0,0.0868123,"Missing"
P17-1008,J15-4003,0,0.0161424,"st SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered. For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives. FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as “president”. Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015). Core and Non-core Arguments. Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003). While it is possible to define the distinction distributionally as one between obligatory and optional arguments, here we focus on the semantic dimension, which distinguishes arguments whose meaning is predicate-specific and are necessary components of the described event (core), and those which are predicate-general (non-core). For example, FrameNet defines core arguments as conceptually necessary components of a frame, that make the frame unique and"
P17-1008,W15-1612,0,0.0752109,"ges, is its cross-linguistic universality. While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as2 We use the term “Text Semantics”, rather than the commonly used “Sentence Semantics” to include inter-sentence semantic relations as well. 78 evoke the same frame type, such as “leave” and “depart”), and PropBank (where roles are verbspecific). PropBank’s role sets were extended by subsequent projects such as AMR. Another prominent semantic role inventory is VerbNet (Kipper et al., 2008) and subsequent projects (Bonial et al., 2011; Schneider et al., 2015), which define a closed set of abstract semantic roles (such as AGENT, PATIENT and I NSTRUMENT) that apply to all predicate arguments. The events discussed here should not be confused with events as defined in Information Extraction and related tasks such as event coreference (Humphreys et al., 1997), which correspond more closely to the everyday notion of an event, such as a political or financial event, and generally consist of multiple events in the sense discussed here. The representation of such events is recently receiving considerable interest within NLP, e.g. the Richer Event Descripti"
P17-1008,W12-3205,0,0.0188224,"een sentences, as some entailments can be computed from the text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012). The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between them into a hierarchical, closed category set, including high-level relation types like T EMPORAL, C OMPARISON and C ONTINGENCY and finer-grained ones such as J USTIFICATION and E XCEPTION. Another commonly used resource is the RST Discourse TreeInference and Entailment. A primary motivation for many semantic schemes is their ability to support inference and entailment. Indeed, means for predicting logical entailment are built into many forms of semantic representations"
P17-1008,D16-1177,0,0.189031,"Missing"
P17-1008,J00-4004,0,0.123845,"(Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partition"
P17-1008,P13-1045,0,0.0269841,"approach to SRT is taken by Vector Space Models (VSM), which eschew the use of symbolic structures, instead modeling all linguistic elements as vectors, from the level of words to phrases and sentences. Proponents of this approach generally invoke neural network methods, obtaining impressive results on a variety of tasks including lexical tasks such as cross-linguistic word similarity (Ammar et al., 2016), machine translation (Bahdanau et al., 2015), and dependency parsing (Andor et al., 2016). VSMs are also attractive in being flexible enough to model non-local and gradient phenomena (e.g., Socher et al., 2013). However, more research is needed to clarify the scope of semantic phenomena that such models are able to reliably capture. We therefore only lightly touch on VSMs in this survey. Finally, a major consideration in semantic analysis, and one of its great potential advantages, is its cross-linguistic universality. While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as2 We use the term “Text Semantics”, rather than the commonly used “Sentence Semantics” to include inter-sentence semantic relations as well. 78 evoke the same frame ty"
P17-1008,W06-3510,0,0.0923512,"Missing"
P17-1008,W15-3502,1,0.702518,"Missing"
P17-1008,C10-1119,0,0.0610223,"ong been recognized (Goddard, 2011), the nature of universal semantics remains unknown. Recently, projects such as BabelNet (Ehrmann et al., 2014), UBY (Gurevych et al., 2012) and Open Multilingual Wordnet4 , constructed huge multi-lingual semantic nets, by linking resources such as Wikipedia and WordNet and processing them using modern NLP. However, such projects currently focus on lexical semantic and encyclopedic information rather than on text semantics. Symbolic SRT schemes such as SRL schemes and AMR have also been studied for their crosslinguistic applicability (Pad´o and Lapata, 2009; Sun et al., 2010; Xue et al., 2014), indicating partial portability across languages. Translated versions of PropBank and FrameNet have been constructed for multiple languages (e.g., Akbik et al., 2016; Hartmann and Gurevych, 2013). How4 83 http://compling.hss.ntu.edu.sg/omw/ ing a cross-linguistically consistent dependencybased annotation, and whose categories are motivated by a combination of distributional and semantic considerations. For example, UD would distinguish between the dependency type between “John” and “brother” in “John, my brother, arrived” and “John, who is my brother, arrived”, despite thei"
P17-1008,xue-etal-2014-interlingua,0,0.303418,"AMR covers predicate-argument relations, including semantic roles (adapted from PropBank) that apply to a wide variety of predicates (including verbal, nominal and adjectival predicates), modifiers, co-reference, named entities and some time expressions. AMR does not currently support relations above the sentence level, and is admittedly Englishcentric, which results in an occasional conflation of semantic phenomena that happen to be similarly realized in English, into a single semantic category. AMR thus faces difficulties when assessing the invariance of its structures across translations (Xue et al., 2014). As an example, UDS. Universal Decompositional Semantics (White et al., 2016) is a multi-layered scheme, which currently includes semantic role anno81 Structure Grammar (HPSG; Pollard and Sag, 1994), where syntactic and semantic features are represented as feature bundles, which are iteratively composed through unification rules to form composite units. HPSG-based SRT schemes commonly use the Minimal Recursion Semantics (Copestake et al., 2005) formalism. Annotated corpora and manually crafted grammars exist for multiple languages (Flickinger, 2002; Oepen et al., 2004; Bender and Flickinger,"
P17-1008,S13-2001,0,0.00881579,"at introduce additional, independent or distinct relations from that of the frame such as time, place, manner, means and degree (Ruppenhofer et al., 2016, pp. 23-24). Temporal Relations. Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time. Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013). A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the ye"
P17-1008,D07-1071,0,0.0476805,"Missing"
P17-1008,W10-4205,0,\N,Missing
P17-1008,J93-2004,0,\N,Missing
P17-1008,D10-1089,0,\N,Missing
P17-1008,W08-2222,0,\N,Missing
P17-1008,H05-1079,0,\N,Missing
P17-1008,S14-2010,0,\N,Missing
P17-1008,S13-1004,0,\N,Missing
P17-1008,Q14-1022,0,\N,Missing
P17-1008,P13-2131,0,\N,Missing
P17-1008,P13-1023,1,\N,Missing
P17-1008,W15-1622,0,\N,Missing
P17-1008,D16-1102,0,\N,Missing
P17-1008,L16-1262,0,\N,Missing
P17-1008,ehrmann-etal-2014-representing,0,\N,Missing
P17-1008,W14-2903,0,\N,Missing
P17-1104,W15-0126,0,0.0787572,"Missing"
P17-1104,S15-2162,0,0.169588,"ation approach used for dependency graph parsing (Agi´c et al., 2015; Fern´andez-Gonz´alez and Martins, 2015), where dependency graphs were converted into dependency trees and then parsed by dependency tree parsers. In our setting, the conversion to trees consists simply of removing remote edges from the graph, and then to bilexical trees by applying the same procedure as for bilexical graphs. Baseline parsers. We evaluate two bilexical graph semantic dependency parsers: DAGParser (Ribeyre et al., 2014), the leading transition-based parser in SemEval 2014 (Oepen et al., 2014) and TurboParser (Almeida and Martins, 2015), a graph-based parser from SemEval 2015 (Oepen et al., 2015); UPARSE (Maier and Lichte, 2016), a transition-based constituency parser supporting discontinuous constituents; and two bilexical tree parsers: MaltParser (Nivre et al., 2007), and the stack LSTM-based parser of Dyer et al. (2015, henceforce “LSTM Parser”). Default settings are used in all cases.9 DAGParser and UPARSE use beam search by default, with a beam size of 5 and 4 respectively. The other parsers are greedy. 5 Results Comparison to tree parsers. For completeness, and as parsing technology is considerably more Table 2 present"
P17-1104,N15-1006,0,0.022221,"n-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 1127 Proceedings of the 55th Annual Meeting of the Association for Computational Ling"
P17-1104,N16-1052,0,0.028808,"Missing"
P17-1104,P16-1231,0,0.0374852,"ility has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, in"
P17-1104,D15-1198,0,0.0200772,"Missing"
P17-1104,W13-2322,0,0.278436,"ene, despite not being overtly marked. Third, consider the possessive construction in Figure 1c. While in UCCA “trip” evokes a scene in which “John and Mary” is a Participant, a syntactic scheme would analyze this phrase similarly to “John and Mary’s shoes”. These examples demonstrate that a UCCA parser, and more generally semantic parsers, face an additional level of ambiguity compared to their syntactic counterparts (e.g., “after graduation” is formally very similar to “after 2pm”, which does not evoke a scene). Section 6 discusses UCCA in the context of other semantic schemes, such as AMR (Banarescu et al., 2013). Alongside recent progress in dependency parsing into projective trees, there is increasing interest in parsing into representations with more general structural properties (see Section 6). One such property is reentrancy, namely the sharing of semantic units between predicates. For instance, in Figure 1a, “John” is an argument of both “gradu1128 ation” and “moved”, yielding a DAG rather than a tree. A second property is discontinuity, as in Figure 1b, where “gave up” forms a discontinuous semantic unit. Discontinuities are pervasive, e.g., with multi-word expressions (Schneider et al., 2014)"
P17-1104,D16-1134,1,0.37369,"tion (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015). It has also proven useful for machine translation evaluation (Birch et al., 2016). UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual simi"
P17-1104,P17-1008,1,0.0617527,"Missing"
P17-1104,P17-4019,1,0.688987,"mantic DAG structures, and in languages that frequently use discontinuous structures. 1 Introduction Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015). It has also proven useful for machine translation evaluation (Birch et al., 2016). UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing nov"
P17-1104,D14-1082,0,0.0488151,"ransition-based dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015). R EDUCE pops the stack, to allow removing a node once all its edges have been created. To handle discontinuous nodes, S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, F INISH pops the root node and marks the state as terminal. Classifier. The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models. 1. Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron algorithm (Collins and Roark, 2004) and M IN U PDATE (Goldberg and Elhadad, 2011): each feature requires a minimum number of updates in training to be included in the model.2 2. Changing the model to a feedforward neural network with dense"
P17-1104,P04-1015,0,0.0157842,"5). Finally, F INISH pops the root node and marks the state as terminal. Classifier. The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models. 1. Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron algorithm (Collins and Roark, 2004) and M IN U PDATE (Goldberg and Elhadad, 2011): each feature requires a minimum number of updates in training to be included in the model.2 2. Changing the model to a feedforward neural network with dense embedding features, TUPAMLP (“multi-layer perceptron”), uses an architecture similar to that of Chen and Manning (2014), but with two rectified linear layers 2 We also experimented with a linear model using dense embedding features, trained with the averaged structured perceptron algorithm. It performed worse than the sparse perceptron model and was hence discarded. 1129 Before Transition Tra"
P17-1104,E17-1051,0,0.0497003,"essed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 1127 Proceedings of the 5"
P17-1104,S15-2154,0,0.0768183,"Missing"
P17-1104,P15-1033,0,0.0724261,"er, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the Engl"
P17-1104,P15-1147,0,0.0513024,"Missing"
P17-1104,P14-1134,0,0.0453764,"Missing"
P17-1104,C12-1059,0,0.035982,"ord dropout (Kiperwasser and Goldberg, 2016): with a certain probability, the embedding for a word is replaced with a zero vector. We do not apply word dropout to the external word embeddings. Finally, for all classifiers we add a novel realvalued feature to the input vector, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too. Training. For training the transition classifiers, we use a dynamic oracle (Goldberg and Nivre, 2012), i.e., an oracle that outputs a set of optimal transitions: when applied to the current parser state, the gold standard graph is reachable from the resulting state. For example, the oracle would predict a N ODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a R IGHT-E DGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created. The transition predicted by the classifier is deemed correct and is applied to the parser state to reach the subse"
P17-1104,P16-1001,0,0.00460807,"argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/"
P17-1104,J13-4006,0,0.014711,"Missing"
P17-1104,D15-1162,0,0.00802403,"STM ... LSTM LSTM After graduation ... to Paris Figure 3: Illustration of the TUPA model. Top: parser state (stack, buffer and intermediate graph). Bottom: TUPABiLTSM architecture. Vector representation for the input tokens is computed by two layers of bidirectional LSTMs. The vectors for specific tokens are concatenated with embedding and numeric features from the parser state (for existing edge labels, number of children, etc.), and fed into the MLP for selecting the next transition. according to the perceptron update rule. POS tags and syntactic dependency labels are extracted using spaCy (Honnibal and Johnson, 2015).5 We use the categorical cross-entropy objective function and optimize the NN classifiers with the Adam optimizer (Kingma and Ba, 2014). 4 Experimental Setup Data. We conduct our experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as outof-domain data.6 Table 1 presents some statistics for the two corpora. We use passages of indices up to 676 of the Wiki corpus as our training set, passages 688–808 as development set, and passages 942–1028 as in-domain t"
P17-1104,W12-3602,0,0.0376842,"Missing"
P17-1104,N15-1080,0,0.0474402,"Missing"
P17-1104,N15-1114,0,0.00764079,"xplore different conversion procedures (Kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation. A parser for UCCA will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (Birch et al., 2016). We believe UCCA’s merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (Narayan and Gardent, 2014) and summarization (Liu et al., 2015). Acknowledgments This work was supported by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI). The first author was supported by a fellowship from the Edmond and Lily Safra Center for Brain Sciences. We thank Wolfgang Maier, Nathan Schneider, Elior Sulem and the anonymous reviewers for their helpful comments. References Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotation (UCCA). In Proc. of ACL. pages 22"
P17-1104,P15-1116,0,0.289762,"t for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as foll"
P17-1104,W16-0906,0,0.40494,"rsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and co"
P17-1104,P14-1041,0,0.0073383,"mits of its generality. In addition, we will explore different conversion procedures (Kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation. A parser for UCCA will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (Birch et al., 2016). We believe UCCA’s merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (Narayan and Gardent, 2014) and summarization (Liu et al., 2015). Acknowledgments This work was supported by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI). The first author was supported by a fellowship from the Edmond and Lily Safra Center for Brain Sciences. We thank Wolfgang Maier, Nathan Schneider, Elior Sulem and the anonymous reviewers for their helpful comments. References Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotat"
P17-1104,W03-3017,0,0.099497,"ures (e.g., “John and Mary” in Figure 1c), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases (either the preposition or the head noun can serve as the constituent’s head). To our knowledge, no existing parser supports all structural properties required for UCCA parsing. 3 Transition-based UCCA Parsing We now turn to presenting TUPA. Building on previous work on parsing reentrancies, discontinuities and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. Transition-based parsers (Nivre, 2003) scan the text from start to end, and create the parse incrementally by applying a transition at each step to the parser’s state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V, E, `) of constructed nodes and edges, where V is the set of nodes, E is the set of edges, and ` : E → L is the label function, L being the set of possible labels. Some states are marked as terminal, meaning that G is the final output. A classifier is used at each step to select the next transition based on features en"
P17-1104,P09-1040,0,0.0295173,"transitions to allow the parser to create remote edges without the possibility of producing invalid graphs. To support the prediction of multiple parents, node and edge transitions leave the stack unchanged, as in other work on transition-based dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015). R EDUCE pops the stack, to allow removing a node once all its edges have been created. To handle discontinuous nodes, S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, F INISH pops the root node and marks the state as terminal. Classifier. The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models. 1. Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron"
P17-1104,S15-2153,0,0.548009,"Missing"
P17-1104,S14-2008,0,0.193615,"Missing"
P17-1104,D14-1048,0,0.103799,"Missing"
P17-1104,D15-1136,0,0.0427713,"Missing"
P17-1104,W05-1513,0,0.0531703,"or UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented"
P17-1104,C08-1095,0,0.421265,"nstituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and tree"
P17-1104,S14-2034,0,0.0808372,"Missing"
P17-1104,Q14-1016,0,0.0262482,"Banarescu et al., 2013). Alongside recent progress in dependency parsing into projective trees, there is increasing interest in parsing into representations with more general structural properties (see Section 6). One such property is reentrancy, namely the sharing of semantic units between predicates. For instance, in Figure 1a, “John” is an argument of both “gradu1128 ation” and “moved”, yielding a DAG rather than a tree. A second property is discontinuity, as in Figure 1b, where “gave up” forms a discontinuous semantic unit. Discontinuities are pervasive, e.g., with multi-word expressions (Schneider et al., 2014). Finally, unlike most dependency schemes, UCCA uses non-terminal nodes to represent units comprising more than one word. The use of non-terminal nodes is motivated by constructions with no clear head, including coordination structures (e.g., “John and Mary” in Figure 1c), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases (either the preposition or the head noun can serve as the constituent’s head). To our knowledge, no existing parser supports all structural properties required for UCCA parsing. 3 Transition-based UCCA Parsing We now turn to presenti"
P17-1104,W15-3502,1,0.653204,"use discontinuous structures. 1 Introduction Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015). It has also proven useful for machine translation evaluation (Birch et al., 2016). UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based"
P17-1104,K16-1019,0,0.0758887,"Missing"
P17-1104,S14-2027,0,0.0497783,"Missing"
P17-1104,P15-3004,0,0.115722,"Missing"
P17-1104,N15-3006,0,0.0603445,"Missing"
P17-1104,S16-1181,0,0.0214062,"Missing"
P17-1104,P15-2141,0,0.0149878,"of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as traine"
P17-1104,N15-1040,0,0.0220755,"of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as traine"
P17-1104,P15-1095,0,0.0608038,"Missing"
P17-1104,W09-3825,0,0.232693,"ed techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of"
P17-1104,P11-1069,0,0.013152,"hievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 1127 Proceedings of the 55th Annual Meeting of the Association f"
P17-1104,D16-1065,0,0.0304795,"o distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 11"
P17-1104,P13-1043,0,0.0342008,"ural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is struc"
P17-1104,S14-2081,0,\N,Missing
P17-1104,Q16-1023,0,\N,Missing
P17-1104,J16-4009,0,\N,Missing
P17-1104,D16-1183,0,\N,Missing
P17-4019,P13-1023,1,0.878492,"o non-expert annotators and stability under translation (Sulem et al., 2015). The scheme has recently proven useful for machine translation evaluation (Birch et al., 2016). UCCA emphasizes accessibility and intuitive distinctions in the definition of its categories. UCCAApp complements this effort by offering an intuitive user interface (see Figure 1 and Section 4), which does not require background in formal representation and linguistics, as attested by the steep learning curve of annotators with no background in these fields that used UCCAApp in the annotation of the UCCA Wikipedia corpus (Abend and Rappoport, 2013).2 Aside from the annotation interface, the application includes modules for defining annotation schemes (layers), and for project management. Importantly, the system supports a multi-layered architecture, where the same text passage may be annotated by multiple layers. See Section 3. In order to facilitate the adoption of UCCAApp by other research groups, we built UCCAApp using recent, standard web technology. The server is based on Django, accompanied with a PostWe present UCCAApp, an open-source, flexible web-application for syntactic and semantic phrase-based annotation in general, and for"
P17-4019,W13-2322,0,0.0855499,"AT6 is an open-source web-application with support for distributed collaborative annotation in a variety of annotation formats, including phrase-structure annotation. However, as it is mostly geared towards flat annotations, phrase structure annotation with FLAT is somewhat difficult. SynTree7 also supports phrase-structure annotation, focusing on Chinese. Its applicability, however, has so far been limited due to its documentation being formulated only in Chinese. To the best of our knowledge, none of these applications support DAG annotation, or full keyboard functionality. The AMR editor8 (Banarescu et al., 2013) supports annotation through an interface based on the linearization of the AMR DAGs using the PENMAN notation. The application has a number of dedicated functionalities both for facilitating the annotation process (e.g., nodes and edges can be added either by directly inputting their textual representations, or through more elaborate modals), and for validating that the resulting annotation is indeed well-formed. The editor is also well integrated with the AMR category set, facilitating the navigation through its rich ontology. UCCAApp supports many of the properties required for AMR annotati"
P17-4019,basile-etal-2012-developing,0,0.0151492,"ure 3 presents a screenshot of the admin screens used for managing the different resources. The admin interface is designed to scale to tens of thousands of tasks and passages, and supports advanced pagination, search and filtering capabilities. 4 Annotation Interface The system’s core is its annotation interface. Annotation is carried out over passages, rather than individual sentences. The application thus supports the annotation of inter-sentence relations, which feature in many annotation schemes, including the Penn Discourse Treebank (Miltsakaki et al., 2004), the Groningen Meaning Bank (Basile et al., 2012), and UCCA. UCCAApp fully supports both mouse functionality, for novice annotators, and keyboard functionality, which we have found to be faster, and more suitable for experienced annotators. allows annotators to do. • Tokenization tasks, whose input is a passage of text and whose output is a sequence of tokens. Tokenization tasks are carried out by hand-correcting the output of an automatic tokenizer. • Annotation task, whose input is a set of tokens, and whose output is a DAG structure, whose leaves are the tokens. The task proceeds by iteratively grouping tokens into units. • Review task, w"
P17-4019,D16-1134,1,0.752785,"5), a gap we address in this work. UCCA (Universal Conceptual Cognitive Annotation) is a cross-linguistically applicable semantic representation scheme, building on the Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, accessibility to non-expert annotators and stability under translation (Sulem et al., 2015). The scheme has recently proven useful for machine translation evaluation (Birch et al., 2016). UCCA emphasizes accessibility and intuitive distinctions in the definition of its categories. UCCAApp complements this effort by offering an intuitive user interface (see Figure 1 and Section 4), which does not require background in formal representation and linguistics, as attested by the steep learning curve of annotators with no background in these fields that used UCCAApp in the annotation of the UCCA Wikipedia corpus (Abend and Rappoport, 2013).2 Aside from the annotation interface, the application includes modules for defining annotation schemes (layers), and for project management. Im"
P17-4019,P10-1160,0,0.0730564,"n formal notation. While not all functionalities required for AMR annotation are currently supported in UCCAApp (importantly, the application does not support representations that are not anchored in the words and phrases of the text), future work will address the adaptation of UCCAApp to AMR annotation. A number of recent annotation web applications support dependency annotation, and provide efImplicit Sub-Units. Many annotation schemes encode units that do not correspond to any span of text. Examples include traces, such as in the Penn Treebank (Marcus et al., 1993), and implicit arguments (Gerber and Chai, 2010). UCCAApp supports this functionality by allowing units to have remote sub-units that do not correspond to any span of text. Implicit sub-units may receive categories just like any other unit. Restrictions Module. UCCAApp supports the introduction of restrictions over the joint occurrence of different categories within a layer. Restrictions may either forbid a category from having any children, require that two categories appear together as siblings or children of one another, or forbid them from doing so. Restrictions are validated when a unit is declared finished, or when the passage is subm"
P17-4019,W13-3711,0,0.0627427,"Missing"
P17-4019,J93-2004,0,0.0598354,"hat does not require annotators to be versed in formal notation. While not all functionalities required for AMR annotation are currently supported in UCCAApp (importantly, the application does not support representations that are not anchored in the words and phrases of the text), future work will address the adaptation of UCCAApp to AMR annotation. A number of recent annotation web applications support dependency annotation, and provide efImplicit Sub-Units. Many annotation schemes encode units that do not correspond to any span of text. Examples include traces, such as in the Penn Treebank (Marcus et al., 1993), and implicit arguments (Gerber and Chai, 2010). UCCAApp supports this functionality by allowing units to have remote sub-units that do not correspond to any span of text. Implicit sub-units may receive categories just like any other unit. Restrictions Module. UCCAApp supports the introduction of restrictions over the joint occurrence of different categories within a layer. Restrictions may either forbid a category from having any children, require that two categories appear together as siblings or children of one another, or forbid them from doing so. Restrictions are validated when a unit i"
P17-4019,miltsakaki-etal-2004-penn,0,0.0403127,"f the API is included in the project’s repository. Figure 3 presents a screenshot of the admin screens used for managing the different resources. The admin interface is designed to scale to tens of thousands of tasks and passages, and supports advanced pagination, search and filtering capabilities. 4 Annotation Interface The system’s core is its annotation interface. Annotation is carried out over passages, rather than individual sentences. The application thus supports the annotation of inter-sentence relations, which feature in many annotation schemes, including the Penn Discourse Treebank (Miltsakaki et al., 2004), the Groningen Meaning Bank (Basile et al., 2012), and UCCA. UCCAApp fully supports both mouse functionality, for novice annotators, and keyboard functionality, which we have found to be faster, and more suitable for experienced annotators. allows annotators to do. • Tokenization tasks, whose input is a passage of text and whose output is a sequence of tokens. Tokenization tasks are carried out by hand-correcting the output of an automatic tokenizer. • Annotation task, whose input is a set of tokens, and whose output is a DAG structure, whose leaves are the tokens. The task proceeds by iterat"
P17-4019,W15-3502,1,0.699361,"plications, very few open source applications support phrase-structure annotation (see Section 5), a gap we address in this work. UCCA (Universal Conceptual Cognitive Annotation) is a cross-linguistically applicable semantic representation scheme, building on the Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, accessibility to non-expert annotators and stability under translation (Sulem et al., 2015). The scheme has recently proven useful for machine translation evaluation (Birch et al., 2016). UCCA emphasizes accessibility and intuitive distinctions in the definition of its categories. UCCAApp complements this effort by offering an intuitive user interface (see Figure 1 and Section 4), which does not require background in formal representation and linguistics, as attested by the steep learning curve of annotators with no background in these fields that used UCCAApp in the annotation of the UCCA Wikipedia corpus (Abend and Rappoport, 2013).2 Aside from the annotation interface, the applic"
P17-4019,W16-4011,0,\N,Missing
P18-1016,P13-1023,1,0.898652,"by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we propose an intermediate way for performing sentence splitting, presenting Direct Semantic Splitting (DSS), a simple and efficient algorithm based on a semantic parser which supports the direct decomposition of the sentence into its main semantic constituents. After splitting, NMT-based simplification is performed, using the NTS system. We show that the resulting system outperforms H YBRID in both automatic and human evaluation. We use the UCCA scheme for semantic representation (Abend and Rappoport, 2013), where the semantic units are anchored in the text, which simplifies the splitting operation. We further leverage the explicit distinction in UCCA between types of Scenes (events), applying a specific rule for each of the cases. Nevertheless, the DSS approach can be adapted to other semantic schemes, like AMR (Banarescu et al., 2013). We collect human judgments for multiple variants of our system, its sub-components, H YBRID and similar systems that use phrase-based MT. This results in a sizable human evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pa"
P18-1016,P17-4019,1,0.74298,"the deletion module, and trained on WEB-SPLIT (Narayan et al., 2017). DSS gets a higher BLEU score (46.45 vs. 39.97) and performs more splittings (number of output sentences per input sentence of 1.73 vs. 1.26). 7 Additional Experiments Replacing the parser by manual annotation. In order to isolate the influence of the parser on the results, we implement a semi-automatic version of the semantic component, which uses manual UCCA annotation instead of the parser, focusing of the first 70 sentences of the test corpus. We employ a single expert UCCA annotator and use the UCCAApp annotation tool (Abend et al., 2017). Results are presented in Table 6, for both SENTS and SEMoses. In the case of SEMoses, meaning preservation is improved when manual UCCA annotation is used. On the other hand, simplicity degrades, possibly due to the larger number of Scenes marked by the human annotator (TUPA tends to under-predict Scenes). This effect doesn’t 14 We use the evaluation tools provided in https:// github.com/danielhers/ucca, ignoring 9 sentences for which different tokenizations of proper nouns are used in the automatic and manual parsing. 169 Identity G M S StS In return, Rollo swore fealty to Charles, converte"
P18-1016,W10-1607,0,0.0610648,"Missing"
P18-1016,N13-1092,0,0.0245249,"Missing"
P18-1016,P17-1017,0,0.0256153,"The eventwise simplification one, which separates events to separate output sentences, is similar to our semantic component. Differences are in that we use a single semantic representation for defining the rules (rather than a combination of semantic and syntactic criteria), and avoid the need for complex rules for retaining grammaticality by using a subsequent neural component. Split and Rephrase. Narayan et al. (2017) recently proposed the Split and Rephrase task, focusing on sentence splitting. For this purpose they presented a specialized parallel corpus, derived from the WebNLG dataset (Gardent et al., 2017). The latter is obtained from the DBPedia knowledge base (Mendes et al., 2012) using content selection and crowdsourcing, and is annotated with semantic triplets of subject-relation-object, obtained semi-automatically. They experimented with five systems, including one similar to H YBRID , as well as sequence-to-sequence methods for generating sentences from the source text and its semantic forms. The present paper tackles both structural and lexical simplification, and examines the effect of sentence splitting on the subsequent application of a neural system, in terms of its tendency to perfo"
P18-1016,I17-1030,0,0.453564,"ere Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinforcement learning, using SARI (Xu et al., 2016), BLEU, and cosine similarity to the source as the reward. None of these models explicitly addresses sentence splitting. Alva-Manchego et al. (2017) proposed to reduce conservatism, observed in PBMT and NMT systems, by first identifying simplification operations in a parallel corpus and then using sequencelabeling to perform the simplification. However, they did not address common structural operations, such as sentence splitting, and claimed that their method is not applicable to them. Xu et al. (2016) used Syntax-based Machine Translation (SBMT) for sentence simplification, using a large scale paraphrase dataset (Ganitketitch et al., 2013) for training. While it does not target structural simplification, we include it in our evaluation"
P18-1016,R13-2011,0,0.0567807,"Missing"
P18-1016,W13-2322,0,0.105637,"into its main semantic constituents. After splitting, NMT-based simplification is performed, using the NTS system. We show that the resulting system outperforms H YBRID in both automatic and human evaluation. We use the UCCA scheme for semantic representation (Abend and Rappoport, 2013), where the semantic units are anchored in the text, which simplifies the splitting operation. We further leverage the explicit distinction in UCCA between types of Scenes (events), applying a specific rule for each of the cases. Nevertheless, the DSS approach can be adapted to other semantic schemes, like AMR (Banarescu et al., 2013). We collect human judgments for multiple variants of our system, its sub-components, H YBRID and similar systems that use phrase-based MT. This results in a sizable human evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while"
P18-1016,P17-1104,1,0.57328,"64 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UCCA categories used are: Parallel Scene (H), Linker (L), Participant (A), Process/State (P/S), Center (C), Elaborator (E), Relator (R). minimal center of a UCCA unit u to be the UCCA graph’s leaf reached by starting from u and iteratively selecting the child tagged as Center. For generating UCCA’s structures we use TUPA, a transition-based parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). TUPA uses an expressive set of transitions, able to support all structural properties required by the UCCA scheme. Its transition classifier is based on an MLP that receives a BiLSTM encoding of elements in the parser state (buffer, stack and intermediate graph), given word embeddings and other features. A Scene is UCCA’s notion of an event or a frame, and is a unit that corresponds to a movement, an action or a state which persists in time. Every Scene contains one main relation, which can be either a Process or a State. Scenes contain one or more Parti"
P18-1016,P11-2087,0,0.0245507,"tient). Semantic annotation is used on the source side in both training and test. Lexical simplification is performed using the Moses system. H YBRID is the most similar system to ours architecturally, in that it uses a combination of a semantic structural component and an MT component. Narayan and Gardent (2016) proposed instead an unsupervised pipeline, where sentences are split based on a probabilistic model trained on the semantic structures of Simple Wikipedia as well as a language model trained on the same corpus. Lexical simplification is there performed using the unsupervised model of Biran et al. (2011). As their BLEU and adequacy scores are lower than H YBRID’s, we use the latter for comparison. ˇ Stajner and Glavaˇs (2017) combined rule-based simplification conditioned on event extraction, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkabl"
P18-1016,N15-1022,0,0.0124421,"Sc1 |· · · |Scn i=1 where S −A is S without the unit A. For example, this rule converts the sentence “He observed the planet which has 14 known satellites” to “He observed the planet |Planet has 14 known satellites.”. Article regeneration is not covered by the rule, as its output is directly fed into the NMT component. After the extraction of Parallel Scenes and Elaborator Scenes, the resulting simplified Parallel Scenes are placed before the Elaborator Scenes. See Figure 1. 4 Experimental Setup Neural component. We use the NTS-w2v model6 provided by N17, obtained by training on the corpus of Hwang et al. (2015) and tuning on the corpus of Xu et al. (2016). The training set is based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia, including both good matches and partial matches whose similarity score is above the 0.45 scale threshold (Hwang et al., 2015). The total size of the training set is about 280K aligned sentences, of which 150K sentences are full matches and 130K are partial matches.7 Neural Component The split sentences are run through the NTS stateof-the-art neural TS system (Nisioi et al., 2017), built using the OpenNMT neural machine tran"
P18-1016,D16-1134,1,0.637953,"mbined rule-based simplification conditioned on event extraction, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for the evaluation of machine translation (Birch et al., 2016) and, recently, for the evaluation of TS (Sulem et al., 2018) and grammatical error correction (Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph or to several elements viewed as a single entity according to some semantic or cognitive consideration. 164 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UC"
P18-1016,P13-1151,0,0.0245926,"ools and the SBMT-SARI system.) 4 https://github.com/danielhers/tupa 5 http://www.cs.huji.ac.il/˜oabend/ ucca.html 6 https://github.com/senisioi/ NeuralTextSimplification 7 We also considered the default initialization for the neural component, using the NTS model without word embeddings. Experimenting on the tuning set, the w2v approach got higher BLEU and SARI scores (for h1 and h4 respectively) than the default approach. 166 mented by Zhang and Lapata (2017).8 We use the released output of H YBRID, trained on a corpus extracted from Wikipedia, which includes the aligned sentence pairs from Kauchak (2013), the aligned revision sentence pairs in Woodsend and Lapata (2011), and the PWKP corpus, totaling about 296K sentence pairs. The tuning set is the same as for the above systems. In order to isolate the effect of NMT, we also implement SEMoses, where the neural-based component is replaced by the phrase-based MT system Moses,9 which is also used in H YBRID. The training, tuning and test sets are the same as in the case of SENTS. MGIZA10 is used for word alignment. The KenLM language model is trained using the target side of the training corpus. Human evaluation. Human evaluation is carried out"
P18-1016,N18-2020,1,0.633278,"entation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for the evaluation of machine translation (Birch et al., 2016) and, recently, for the evaluation of TS (Sulem et al., 2018) and grammatical error correction (Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph or to several elements viewed as a single entity according to some semantic or cognitive consideration. 164 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UCCA categories used are: Parallel Scene (H), Linker (L), Participant (A), Process/State (P/S), Center (C), Elaborator (E)"
P18-1016,P17-4012,0,0.101354,"corpus of Xu et al. (2016). The training set is based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia, including both good matches and partial matches whose similarity score is above the 0.45 scale threshold (Hwang et al., 2015). The total size of the training set is about 280K aligned sentences, of which 150K sentences are full matches and 130K are partial matches.7 Neural Component The split sentences are run through the NTS stateof-the-art neural TS system (Nisioi et al., 2017), built using the OpenNMT neural machine translation framework (Klein et al., 2017). The architecture includes two LSTM layers, with hidden states of 500 units in each, as well as global attention combined with input feeding (Luong et al., 2015). Training is done with a 0.3 dropout probability (Srivastava et al., 2014). This model uses alignment probabilities between the predictions and the original sentences, rather than characterbased models, to retrieve the original words. We here consider the w2v initialization for NTS (N17), where word2vec embeddings of size 300 are trained on Google News (Mikolov et al., 2013a) and local embeddings of size 200 are trained on the traini"
P18-1016,P07-2045,0,0.00578241,"tems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while §7 presents a more detailed analysis of the system’s sub-components and related settings. 2 highly similar to the target. Other PBMT for TS systems include the work of Coster and Kauchak (2011b), which uses Moses (Koehn et al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinforcement learning, using SARI (Xu et al.,"
P18-1016,W11-1601,0,0.186464,"an evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while §7 presents a more detailed analysis of the system’s sub-components and related settings. 2 highly similar to the target. Other PBMT for TS systems include the work of Coster and Kauchak (2011b), which uses Moses (Koehn et al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinf"
P18-1016,P11-2117,0,0.0596756,"an evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while §7 presents a more detailed analysis of the system’s sub-components and related settings. 2 highly similar to the target. Other PBMT for TS systems include the work of Coster and Kauchak (2011b), which uses Moses (Koehn et al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinf"
P18-1016,N03-1017,0,0.0466807,"syntactic ones reduces the number of rules. For example, relative clauses and appositives can correspond to the same semantic category. In syntax-based splitting, a generation module is sometimes added after the split (Siddharthan, 2004), addressing issues such as reordering and determiner selection. In our model, no explicit regeneration is applied to the split sentences, which are fed directly to an NMT system. ˇ Glavaˇs and Stajner (2013) used a rule-based system conditioned on event extraction and syntax Related Work MT-based sentence simplification. Phrasebased Machine Translation (PBMT; Koehn et al., 2003) was first used for TS by Specia (2010), who showed good performance on lexical simplification and simple rewriting, but under-prediction ˇ of other operations. Stajner et al. (2015) took a similar approach, finding that it is beneficial to use training data where the source side is 1 The benchmark can be found in https://github. com/eliorsulem/simplification-acl2018. 163 gether with an unsupervised lexical simplifier. They tackle a different setting, and aim to simplify texts (rather than sentences), by allowing the deletion of entire input sentences. for defining two simplification models. T"
P18-1016,W02-0109,0,0.0690134,"Missing"
P18-1016,D15-1166,0,0.139512,"Missing"
P18-1016,seretan-2012-acquisition,0,0.0329057,"Missing"
P18-1016,mendes-etal-2012-dbpedia,0,0.012139,"ntences, is similar to our semantic component. Differences are in that we use a single semantic representation for defining the rules (rather than a combination of semantic and syntactic criteria), and avoid the need for complex rules for retaining grammaticality by using a subsequent neural component. Split and Rephrase. Narayan et al. (2017) recently proposed the Split and Rephrase task, focusing on sentence splitting. For this purpose they presented a specialized parallel corpus, derived from the WebNLG dataset (Gardent et al., 2017). The latter is obtained from the DBPedia knowledge base (Mendes et al., 2012) using content selection and crowdsourcing, and is annotated with semantic triplets of subject-relation-object, obtained semi-automatically. They experimented with five systems, including one similar to H YBRID , as well as sequence-to-sequence methods for generating sentences from the source text and its semantic forms. The present paper tackles both structural and lexical simplification, and examines the effect of sentence splitting on the subsequent application of a neural system, in terms of its tendency to perform other simplification operations. For this purpose, we adopt a semantic corp"
P18-1016,E14-1076,0,0.17511,"), confirm that the references are indeed simpler than the source, indicating that the observed conservatism is excessive. Our methods for performing sentence splitting as pre-processing allows the TS system to perform other structural (e.g. deletions) and lexical (e.g. word substitutions) operations, thus increasing both structural and lexical simplicity. For combining linguistically informed sentence splitting with data-driven TS, two main methods have been proposed. The first involves handcrafted syntactic rules, whose compilation and validation are laborious (Shardlow, 2014). For example, Siddharthan and Angrosh (2014) used 111 rules for relative clauses, appositions, subordination and coordination. Moreover, syntactic splitting rules, which form a substantial part of the rules, are usually language specific, requiring the development of new rules when ported to other languages (Alu´ısio and Gasperin, 2010; Seretan, 2012; Hung et al., 2012; Barlacchi and Tonelli, 2013, for Portuguese, French, Vietnamese, and Italian respectively). The second method uses linguistic information for detecting potential splitting points, while splitting probabilities are learned usSentence splitting is a major simplification op"
P18-1016,W14-5603,0,0.0667224,"ation suffers from a considerable disadvantage in that they are overconservative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the stateof-the-art in combined lexical and structural simplification. 1 Introduction Text Simplification (TS) is generally defined as the conversion of a sentence into one or more simpler sentences. It has been shown useful both as a preprocessing step for tasks such as Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162"
P18-1016,W11-2802,0,0.0331345,"to perform the simplification. However, they did not address common structural operations, such as sentence splitting, and claimed that their method is not applicable to them. Xu et al. (2016) used Syntax-based Machine Translation (SBMT) for sentence simplification, using a large scale paraphrase dataset (Ganitketitch et al., 2013) for training. While it does not target structural simplification, we include it in our evaluation for completeness. Structural sentence simplification. Syntactic hand-crafted sentence splitting rules were proposed by Chandrasekar et al. (1996), Siddharthan (2002), Siddhathan (2011) in the context of rulebased TS. The rules separate relative clauses and coordinated clauses and un-embed appositives. In our method, the use of semantic distinctions instead of syntactic ones reduces the number of rules. For example, relative clauses and appositives can correspond to the same semantic category. In syntax-based splitting, a generation module is sometimes added after the split (Siddharthan, 2004), addressing issues such as reordering and determiner selection. In our model, no explicit regeneration is applied to the split sentences, which are fed directly to an NMT system. ˇ Gla"
P18-1016,P14-1041,0,0.562608,"s. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 162–173 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing a parallel corpus. For example, in the system of Narayan and Gardent (2014) (henceforth, H YBRID), the state-of-the-art for joint structural and lexical TS, potential splitting points are determined by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we propose an intermediate way for performing sentence splitting, presenting Direct Semantic Splitting (DSS), a simple and efficient algorithm based on a semantic parser which supports the direct decomposition of the sentence into its main semantic constituents. After splitting, NMT-based simplification is performed, using the NTS system. We show that the resul"
P18-1016,W06-3104,0,0.0216327,"that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, and 85 syntactic rules for subordination and coordination. Narayan and Gardent (2014) argued that syntactic structures do not always capture the semantic arguments of a frame, which may result in wrong splitting boundaries. Consequently, they proposed"
P18-1016,W16-6620,0,0.0603997,"litting boundaries. Consequently, they proposed a supervised system (H YBRID) that uses semantic structures (Discourse Semantic Representations, (Kamp, 1981)) for sentence splitting and deletion. Splitting candidates are pairs of event variables associated with at least one core thematic role (e.g., agent or patient). Semantic annotation is used on the source side in both training and test. Lexical simplification is performed using the Moses system. H YBRID is the most similar system to ours architecturally, in that it uses a combination of a semantic structural component and an MT component. Narayan and Gardent (2016) proposed instead an unsupervised pipeline, where sentences are split based on a probabilistic model trained on the semantic structures of Simple Wikipedia as well as a language model trained on the same corpus. Lexical simplification is there performed using the unsupervised model of Biran et al. (2011). As their BLEU and adequacy scores are lower than H YBRID’s, we use the latter for comparison. ˇ Stajner and Glavaˇs (2017) combined rule-based simplification conditioned on event extraction, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotat"
P18-1016,D17-1064,0,0.0398096,"d lexical simplifier. They tackle a different setting, and aim to simplify texts (rather than sentences), by allowing the deletion of entire input sentences. for defining two simplification models. The eventwise simplification one, which separates events to separate output sentences, is similar to our semantic component. Differences are in that we use a single semantic representation for defining the rules (rather than a combination of semantic and syntactic criteria), and avoid the need for complex rules for retaining grammaticality by using a subsequent neural component. Split and Rephrase. Narayan et al. (2017) recently proposed the Split and Rephrase task, focusing on sentence splitting. For this purpose they presented a specialized parallel corpus, derived from the WebNLG dataset (Gardent et al., 2017). The latter is obtained from the DBPedia knowledge base (Mendes et al., 2012) using content selection and crowdsourcing, and is annotated with semantic triplets of subject-relation-object, obtained semi-automatically. They experimented with five systems, including one similar to H YBRID , as well as sequence-to-sequence methods for generating sentences from the source text and its semantic forms. Th"
P18-1016,C16-2036,0,0.0172535,"vative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the stateof-the-art in combined lexical and structural simplification. 1 Introduction Text Simplification (TS) is generally defined as the conversion of a sentence into one or more simpler sentences. It has been shown useful both as a preprocessing step for tasks such as Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational"
P18-1016,P17-2014,0,0.644536,"s a preprocessing step for tasks such as Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 162–173 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing a parallel corpus. For example, in the system of Narayan and Gardent (2014) (henceforth, H YBRID), the state-of-the-art for joint structural and lexical TS, potential splitting points are determined by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we prop"
P18-1016,P15-2135,0,0.0658153,"le is sometimes added after the split (Siddharthan, 2004), addressing issues such as reordering and determiner selection. In our model, no explicit regeneration is applied to the split sentences, which are fed directly to an NMT system. ˇ Glavaˇs and Stajner (2013) used a rule-based system conditioned on event extraction and syntax Related Work MT-based sentence simplification. Phrasebased Machine Translation (PBMT; Koehn et al., 2003) was first used for TS by Specia (2010), who showed good performance on lexical simplification and simple rewriting, but under-prediction ˇ of other operations. Stajner et al. (2015) took a similar approach, finding that it is beneficial to use training data where the source side is 1 The benchmark can be found in https://github. com/eliorsulem/simplification-acl2018. 163 gether with an unsupervised lexical simplifier. They tackle a different setting, and aim to simplify texts (rather than sentences), by allowing the deletion of entire input sentences. for defining two simplification models. The eventwise simplification one, which separates events to separate output sentences, is similar to our semantic component. Differences are in that we use a single semantic represent"
P18-1016,P02-1040,0,0.103091,"est corpus.13 The resulting corpus, totaling 1960 sentence pairs, each annotated by 3 annotators, also include Additional baselines. We report human and automatic evaluation scores for Identity (where the output is identical to the input), for Simple Wikipedia where the output is the corresponding aligned sentence in the PWKP corpus, and for the SBMT-SARI system, tuned against SARI (Xu et al., 2016), which maximized the SARI score on this test set in previous works (Nisioi et al., 2017; Zhang and Lapata, 2017). Automatic evaluation. The automatic metrics used for the evaluation are: (1) BLEU (Papineni et al., 2002) (2) SARI (System output Against References and against the Input sentence; Xu et al., 2016), which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. (3) Fadd : the addition component of the SARI score (F-score); (4) Fkeep : the keeping component of the SARI score (F-score); (5) Pdel : the deletion component of the SARI score (precision).11 Each metric is computed against the 8 available references. We also assess system conservatism, reporting the percentage of"
P18-1016,W16-3411,0,0.0971014,"Missing"
P18-1016,N18-1063,1,0.890064,"ion, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for the evaluation of machine translation (Birch et al., 2016) and, recently, for the evaluation of TS (Sulem et al., 2018) and grammatical error correction (Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph or to several elements viewed as a single entity according to some semantic or cognitive consideration. 164 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UCCA categories used are: Parallel Scene (H), Linker (L), Parti"
P18-1016,D11-1038,0,0.63656,"opt a semantic corpus-independent approach for sentence splitting that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, and 85 syntactic rules for subordination and coordination. Narayan and Gardent (2014) argued that syntactic structures do not always capture the semantic arguments of a frame, which may res"
P18-1016,P12-1107,0,0.425457,"Missing"
P18-1016,Q15-1021,0,0.319562,"Missing"
P18-1016,Q16-1029,0,0.531781,"al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinforcement learning, using SARI (Xu et al., 2016), BLEU, and cosine similarity to the source as the reward. None of these models explicitly addresses sentence splitting. Alva-Manchego et al. (2017) proposed to reduce conservatism, observed in PBMT and NMT systems, by first identifying simplification operations in a parallel corpus and then using sequencelabeling to perform the simplification. However, they did not address common structural operations, such as sentence splitting, and claimed that their method is not applicable to them. Xu et al. (2016) used Syntax-based Machine Translation (SBMT) for sentence simplification, using a large sca"
P18-1016,P01-1067,0,0.386144,"rms of its tendency to perform other simplification operations. For this purpose, we adopt a semantic corpus-independent approach for sentence splitting that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, and 85 syntactic rules for subordination and coordination. Narayan and Gardent (2014) argued that sy"
P18-1016,D17-1062,0,0.483685,"Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 162–173 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing a parallel corpus. For example, in the system of Narayan and Gardent (2014) (henceforth, H YBRID), the state-of-the-art for joint structural and lexical TS, potential splitting points are determined by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we propose an intermediate way for performing senten"
P18-1016,C10-1152,0,0.774435,"nes the effect of sentence splitting on the subsequent application of a neural system, in terms of its tendency to perform other simplification operations. For this purpose, we adopt a semantic corpus-independent approach for sentence splitting that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, a"
P18-1035,C16-1179,0,0.0293305,"dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 14676 15313 12339 648950 765025 458277 4268 454 503 506 10047 1558 1324 413 67 67 79894 10059"
P18-1035,P13-1023,1,0.689843,"17). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging"
P18-1035,P17-1008,1,0.849635,"ion to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-"
P18-1035,S17-2157,0,0.0294137,"Missing"
P18-1035,P17-1112,0,0.40641,"ks, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained t"
P18-1035,D15-1198,0,0.0487784,"a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing usin"
P18-1035,P13-2131,0,0.301159,"Missing"
P18-1035,D17-1130,0,0.0636435,"d scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use UD as an auxiliary task, inspired by previous work on joint syntactic and semantic parsing (see §2). In order to reach comparable analyses c"
P18-1035,W13-2322,0,0.537693,"et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 A"
P18-1035,P16-1016,0,0.0209388,"ons, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 201"
P18-1035,S16-1176,0,0.0250755,"period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many language"
P18-1035,copestake-flickinger-2000-open,0,0.268289,"AG. ing predicate-argument relations between content words in a sentence. All are based on semantic formalisms converted into bilexical dependencies— directed graphs whose nodes are text tokens. Edges are labeled, encoding semantic relations between the tokens. Non-content tokens, such as punctuation, are left out of the analysis (see Figure 1c). Graphs containing cycles have been removed from the SDP datasets. We use one of the representations from the SemEval shared tasks: DM (DELPH-IN MRS), converted from DeepBank (Flickinger et al., 2012), a corpus of hand-corrected parses from LinGO ERG (Copestake and Flickinger, 2000), an HPSG (Pollard and Sag, 1994) using Minimal Recursion Semantics (Copestake et al., 2005). Abstract Meaning Representation. AMR (Banarescu et al., 2013) is a semantic representation that encodes information about named entities, argument structure, semantic roles, word sense and co-reference. AMRs are rooted directed graphs, in which both nodes and edges are labeled. Most AMRs are DAGs, although cycles are permitted. AMR differs from the other schemes we consider in that it does not anchor its graphs in the words of the sentence (Figure 1b). Instead, AMR graphs connect variables, concepts ("
P18-1035,R13-1008,0,0.0636649,"Missing"
P18-1035,D12-1091,0,0.0687685,"Missing"
P18-1035,E17-1051,0,0.0811458,"dentifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use UD as an auxiliary task, inspired by previous work on joint syntactic and semantic parsing (see §2). In o"
P18-1035,E17-2026,0,0.0280809,"-of-domain settings. Converting AMR. In the conversion from AMR, node labels are dropped. Since alignments are not part of the AMR graph (see Figure 3b), we use automatic alignments (see §7), and attach each node with an edge to each of its aligned terminals. Named entities in AMR are represented as a subgraph, whose name-labeled root has a child for each token in the name (see the two name nodes in Figure 1b). We collapse this subgraph into a single node whose children are the name tokens. 6 Unlabeled parsing for auxiliary tasks. To simplify the auxiliary tasks and facilitate generalization (Bingel and Søgaard, 2017), we perform unlabeled parsing for AMR, DM and UD, while still predicting edge labels in UCCA parsing. To support unlabeled parsing, we simply remove all labels from the E DGE, R EMOTE and N ODE transitions output by the oracle. This results in a much smaller number of transitions the classifier has to select from (no more than 10, as opposed to 45 in labeled UCCA parsing), allowing us to use no BiLSTMs and fewer dimensions and layers for task-specific MLPs of auxiliary tasks (see §7). This limited capacity forces the network to use the shared parameters for all tasks, increasing generalizatio"
P18-1035,P07-1033,0,0.0687063,"Missing"
P18-1035,K17-3002,0,0.061272,"Missing"
P18-1035,W06-1615,0,0.113866,"ble conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably ye"
P18-1035,S15-2154,0,0.118318,"roperties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After graduation ... to Paris Figure 2: Illustration of the TUPA model, adapted from Hershco"
P18-1035,D12-1133,0,0.0289112,"While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outpe"
P18-1035,Q17-1010,0,0.101123,"Missing"
P18-1035,K17-1038,0,0.0366366,"recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SD"
P18-1035,C16-1013,0,0.0141455,"AMR, we use LDC2017T10, identical to the dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 14676 15313 12339 648950 765025 458277 4268 454 503 506 1004"
P18-1035,W17-2607,0,0.0258614,"ss models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abe"
P18-1035,N09-1037,0,0.0416094,"as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain s"
P18-1035,P17-2098,0,0.0295376,"of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive An"
P18-1035,P14-1134,0,0.0947973,"ignable to text tokens, a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in address"
P18-1035,Q16-1023,0,0.0770141,"training and testing on 20K; (4) German indomain setting on 20K, with somewhat noisy annotation. For MTL experiments, we use unlabeled AMR, DM and UD++ parsing as auxiliary tasks in English, and unlabeled UD parsing in French and German.12 We also report baseline results training only the UCCA training sets. Hyperparameters. We initialize embeddings randomly. We use dropout (Srivastava et al., 2014) between MLP layers, and recurrent dropout (Gal and Ghahramani, 2016) between BiLSTM layers, both with p = 0.4. We also use word (α = 0.2), tag (α = 0.2) and dependency relation (α = 0.5) dropout (Kiperwasser and Goldberg, 2016).14 In addition, we use a novel form of dropout, node dropout: with a probability of 0.1 at each step, all features associated with a single node in the parser state are replaced with zero vectors. For optimization we use a minibatch size of 100, decaying all weights by 10−5 at each update, and train with stochastic gradient descent for N epochs with a learning rate of 0.1, followed by AMSGrad (Sashank J. Reddi, 2018) for N epochs with α = 0.001, β1 = 0.9 and β2 = 0.999. We use N = 50 for English and German, and N = 400 for French. We found this training strategy better than using only one of"
P18-1035,P17-1043,0,0.0162428,"align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other"
P18-1035,N16-1179,0,0.0248575,"performed on the respective development sets. For AMR, we use LDC2017T10, identical to the dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 1467"
P18-1035,P17-1014,0,0.0333107,"ted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consist"
P18-1035,J02-3001,0,0.403436,"epresented by their concepts. Figure 1c presents a DM semantic dependency graph, containing multiple roots: “After”, “moved” and “to”, of which “moved” is marked as top. Punctuation tokens are excluded from SDP graphs. Figure 1d presents a UD tree. Edge labels express syntactic relations. In this section, we outline the parsing tasks we address. We focus on representations that produce full-sentence analyses, i.e., produce a graph covering all (content) words in the text, or the lexical concepts they evoke. This contrasts with “shallow” semantic parsing, primarily semantic role labeling (SRL; Gildea and Jurafsky, 2002; Palmer et al., 2005), which targets argument structure phenomena using flat structures. We consider four formalisms: UCCA, AMR, SDP and Universal Dependencies. Figure 1 presents one sentence annotated in each scheme. terminal nodes to semantic units that participate in some super-ordinate relation. Edges are labeled, indicating the role of a child in the relation the parent represents. Nodes and edges belong to one of several layers, each corresponding to a “module” of semantic distinctions. UCCA’s foundational layer (the only layer for which annotated data exists) mostly covers predicate-ar"
P18-1035,D15-1169,0,0.0278292,"ore, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduatio"
P18-1035,P16-1001,0,0.0136125,"sers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use UD as an auxiliary task, inspir"
P18-1035,W08-2124,0,0.347516,"Missing"
P18-1035,W16-0906,0,0.0721203,"ginally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After graduation ... to Paris Figu"
P18-1035,E17-1005,0,0.143027,"unlabeled parsing for AMR, DM and UD, while still predicting edge labels in UCCA parsing. To support unlabeled parsing, we simply remove all labels from the E DGE, R EMOTE and N ODE transitions output by the oracle. This results in a much smaller number of transitions the classifier has to select from (no more than 10, as opposed to 45 in labeled UCCA parsing), allowing us to use no BiLSTMs and fewer dimensions and layers for task-specific MLPs of auxiliary tasks (see §7). This limited capacity forces the network to use the shared parameters for all tasks, increasing generalization (Mart´ınez Alonso and Plank, 2017). Data. For UCCA, we use v1.2 of the English Wikipedia corpus (Wiki; Abend and Rappoport, 2013), with the standard train/dev/test split (see Table 1), and the Twenty Thousand Leagues Under the Sea corpora (20K; Sulem et al., 2015), annotated in English, French and German.8 For English and French we use 20K v1.0, a small parallel corpus comprising the first five chapters of the book. As in previous work (Hershcovich et al., 2017), we use the English part only as an out-of-domain test set. We train and test on the French part using the standard split, as well as the German corpus (v0.9), which i"
P18-1035,S16-1166,0,0.236391,"MR differs from the other schemes we consider in that it does not anchor its graphs in the words of the sentence (Figure 1b). Instead, AMR graphs connect variables, concepts (from a predefined set) and constants (which may be strings or numbers). Still, most AMR nodes are alignable to text tokens, a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR par"
P18-1035,D17-1206,0,0.0339219,"of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structur"
P18-1035,S17-2090,0,0.399132,"from the other schemes we consider in that it does not anchor its graphs in the words of the sentence (Figure 1b). Instead, AMR graphs connect variables, concepts (from a predefined set) and constants (which may be strings or numbers). Still, most AMR nodes are alignable to text tokens, a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency t"
P18-1035,J13-4006,0,0.0493299,"stant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U ,"
P18-1035,N10-1004,0,0.0377672,"ode is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data."
P18-1035,P17-1104,1,0.788059,"nstruction. 4 Semantic Dependency Parsing. SDP uses a set of related representations, targeted in two recent SemEval shared tasks (Oepen et al., 2014, 2015), and extended by Oepen et al. (2016). They correspond to four semantic representation schemes, referred to as DM, PAS, PSD and CCD, representGeneral Transition-based DAG Parser All schemes considered in this work exhibit reentrancy and discontinuity (or non-projectivity), to varying degrees. In addition, UCCA and AMR 2 375 http://github.com/stanfordnlp/CoreNLP Parser state contain non-terminal nodes. To parse these graphs, we extend TUPA (Hershcovich et al., 2017), a transition-based parser originally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees"
P18-1035,W03-3017,0,0.184239,"2 375 http://github.com/stanfordnlp/CoreNLP Parser state contain non-terminal nodes. To parse these graphs, we extend TUPA (Hershcovich et al., 2017), a transition-based parser originally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition,"
P18-1035,L16-1630,0,0.315096,"nd cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefi"
P18-1035,S15-2153,0,0.68935,"Missing"
P18-1035,S14-2008,0,0.45662,"Missing"
P18-1035,J05-1004,0,0.173338,"s. Figure 1c presents a DM semantic dependency graph, containing multiple roots: “After”, “moved” and “to”, of which “moved” is marked as top. Punctuation tokens are excluded from SDP graphs. Figure 1d presents a UD tree. Edge labels express syntactic relations. In this section, we outline the parsing tasks we address. We focus on representations that produce full-sentence analyses, i.e., produce a graph covering all (content) words in the text, or the lexical concepts they evoke. This contrasts with “shallow” semantic parsing, primarily semantic role labeling (SRL; Gildea and Jurafsky, 2002; Palmer et al., 2005), which targets argument structure phenomena using flat structures. We consider four formalisms: UCCA, AMR, SDP and Universal Dependencies. Figure 1 presents one sentence annotated in each scheme. terminal nodes to semantic units that participate in some super-ordinate relation. Edges are labeled, indicating the role of a child in the relation the parent represents. Nodes and edges belong to one of several layers, each corresponding to a “module” of semantic distinctions. UCCA’s foundational layer (the only layer for which annotated data exists) mostly covers predicate-argument structure, sema"
P18-1035,P17-1186,0,0.412812,"Missing"
P18-1035,N18-1135,0,0.14592,"et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduation John moved C R to Paris (a) UCCA move-01 AR G ARG0 e tim 2 city person after 0 graduate-01 name ARG name op1 name name op1 op1 3 LA ”John” ”Paris” (b) AMR top ARG1 ARG2 After graduation ARG1 , John ARG2 ARG1 moved ARG2 to Paris (c) DM root obl case After graduation punct , obl nsubj John case moved to Paris (d) UD Tackled Parsing Tasks Figure 1: Example graph for each task. Figure 1a presents a UCCA g"
P18-1035,E17-1035,0,0.26432,"results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduation John moved C R to Paris (a) UCCA move-01 AR G ARG0"
P18-1035,P05-1073,0,0.0770883,"sing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et a"
P18-1035,C16-1059,0,0.0181885,"ntical to the dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 14676 15313 12339 648950 765025 458277 4268 454 503 506 10047 1558 1324 4"
P18-1035,P11-1157,0,0.0691334,"Missing"
P18-1035,P15-2141,0,0.0251523,"ted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use"
P18-1035,N15-1040,0,0.0470044,"ted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use"
P18-1035,D15-1136,0,0.026836,"AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general"
P18-1035,P16-1147,0,0.0808505,"Missing"
P18-1035,W09-3825,0,0.0260949,"sition-based parser originally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After gra"
P18-1035,C08-1095,0,0.13319,"s all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After graduation ... to Paris Figure 2: Illustration of the TUPA model, ad"
P18-1035,D16-1065,0,0.0211895,"subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies."
P18-1035,L16-1376,0,0.11312,"as an auxiliary task, inspired by previous work on joint syntactic and semantic parsing (see §2). In order to reach comparable analyses cross-linguistically, UD often ends up in annotation that is similar to the common practice in semantic treebanks, such as linking content words to content words wherever possible. Using UD further allows conducting experiments on languages other than English, for which AMR and SDP annotated data is not available (§7). In addition to basic UD trees, we use the enhanced++ UD graphs available for English, which are generated by the Stanford CoreNLP converters (Schuster and Manning, 2016).2 These include additional and augmented relations between content words, partially overlapping with the notion of remote edges in UCCA: in the case of control verbs, for example, a direct relation is added in enhanced++ UD between the subordinated verb and its controller, which is similar to the semantic schemes’ treatment of this construction. 4 Semantic Dependency Parsing. SDP uses a set of related representations, targeted in two recent SemEval shared tasks (Oepen et al., 2014, 2015), and extended by Oepen et al. (2016). They correspond to four semantic representation schemes, referred to"
P18-1035,K17-1040,0,0.0240475,"rences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of it"
P18-1035,P16-2038,0,0.25454,"mples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in seman"
P18-1035,W15-3502,1,0.792046,"much smaller number of transitions the classifier has to select from (no more than 10, as opposed to 45 in labeled UCCA parsing), allowing us to use no BiLSTMs and fewer dimensions and layers for task-specific MLPs of auxiliary tasks (see §7). This limited capacity forces the network to use the shared parameters for all tasks, increasing generalization (Mart´ınez Alonso and Plank, 2017). Data. For UCCA, we use v1.2 of the English Wikipedia corpus (Wiki; Abend and Rappoport, 2013), with the standard train/dev/test split (see Table 1), and the Twenty Thousand Leagues Under the Sea corpora (20K; Sulem et al., 2015), annotated in English, French and German.8 For English and French we use 20K v1.0, a small parallel corpus comprising the first five chapters of the book. As in previous work (Hershcovich et al., 2017), we use the English part only as an out-of-domain test set. We train and test on the French part using the standard split, as well as the German corpus (v0.9), which is a pre-release and still contains a considerable amount of noisy annotation. Tuning is performed on the respective development sets. For AMR, we use LDC2017T10, identical to the dataset targeted in SemEval 2017 (May and Priyadars"
P18-1035,K16-1019,0,0.0296507,"ork has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduation John moved C R to Paris"
P18-1035,S16-1181,0,\N,Missing
S19-2001,P13-1023,1,0.793151,"anguages and settings. Full results can be found in the task’s website https://competitions. codalab.org/competitions/19160. 1 Overview Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes have recently been put forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this shared task, we focus on UCCA parsing in multiple languages. U After , A P graduation John A P A moved R to C Paris Figure 1: An example UCCA graph. One of our goals is to benefit semantic parsing i"
S19-2001,P17-1104,1,0.828203,"in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2 Task Definition UCCA represents the semantics of linguistic utterances as directed acyclic graphs (DAGs), where terminal (childless) nodes correspond to the text tokens, and non-terminal nodes to semantic units that participate in some super-ordinate relation. Edges are labeled, indicating the role of a child in the relation the parent represents. Nodes and edges belong to one of several layers, each corresponding to a “module” of semantic distinctions. UCCA’s foundational layer covers the predicate-argument structure evoked by predicates of all grammatical categ"
S19-2001,P17-1008,1,0.873462,"e model TUPA (which has been the only available parser for UCCA), the task opens a variety of paths for future improvement. Cross-lingual transfer, which capitalizes on UCCA’s tendency to be preserved in translation, was employed by a number of systems and has proven remarkably effective. Indeed, the high scores obtained for French parsing in a low-resource setting suggest that high quality UCCA parsing can be straightforwardly extended to additional languages, with only a minimal amount of manual labor. Moreover, given the conceptual similarity between the different semantic representations (Abend and Rappoport, 2017), it is likely the parsers developed for the shared task will directly contribute to the development of other semantic parsing technology. Such a contribution is facilitated by the available conversion scripts available between UCCA and other formats. Acknowledgments We are deeply grateful to Dotan Dvir and the UCCA annotation team for their diligent work on the corpora used in this shared task. This work was supported by the Israel Science Foundation (grant No. 929/17), and by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s"
S19-2001,P18-1035,1,0.686571,"Missing"
S19-2001,P17-4019,1,0.839017,"across predicates), and the modeling of the interface with lexical semantics. UCCA is a cross-linguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010b,a, 2012). It has demonstrated applicability to multiple languages, including English, French and German, and pilot annotation projects were conducted on a few languages more. UCCA structures have been shown to be well-preserved in translation (Sulem et al., 2015), and to support rapid annotation by nonexperts, assisted by an accessible annotation interface (Abend et al., 2017).1 UCCA has already shown applicative value for text simplifica1 https://github.com/omriabnd/UCCA-App P S A D Process State Participant Adverbial C E N R Center Elaborator Connector Relator H L G Parallel Scene Linker Ground F Function Scene Elements The main relation of a Scene that evolves in time (usually an action or movement). The main relation of a Scene that does not evolve in time. Scene participant (including locations, abstract entities and Scenes serving as arguments). A secondary relation in a Scene. Elements of Non-Scene Units Necessary for the conceptualization of the parent unit"
S19-2001,N18-1041,0,0.0193142,"escu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this shared task, we focus on UCCA parsing in multiple languages. U After , A P graduation John A P A moved R to C Paris Figure 1: An example UCCA graph. One of our goals is to benefit semantic parsing in languages with less annotated resources by making use of data from more resource-rich languages. We refer to this approach as cross-lingual parsing, while other works (Zhang et al., 2017, 2018) define cross-lingual parsing as the task of parsing text in one language to meaning representation in another language. In addition to its potentia"
S19-2001,E17-2039,0,0.166987,"Missing"
S19-2001,S19-2002,0,0.452346,"Missing"
S19-2001,W13-2322,0,0.360278,"ing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings. Full results can be found in the task’s website https://competitions. codalab.org/competitions/19160. 1 Overview Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes have recently been put forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018)"
S19-2001,D16-1134,1,0.805636,"nt aspects of the parent unit. Inter-Scene Relations A Scene linked to other Scenes by regular linkage (e.g., temporal, logical, purposive). A relation between two or more Hs (e.g., “when”, “if”, “in order to”). A relation between the speech event and the uttered Scene (e.g., “surprisingly”). Other Does not introduce a relation or participant. Required by some structural pattern. Table 1: The complete set of categories in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2 Task Definition UCCA represents the semantics of linguistic utterances as directed acyclic graphs (DAGs), where terminal (childless) nodes cor"
S19-2001,Q17-1010,0,0.00930481,"urces, we held both an open and a closed track for submissions in the English and German settings. Closed track submissions were only allowed to use the gold-standard UCCA annotation distributed for the task in the target language, and were limited in their use of additional resources. Concretely, the only additional data they were allowed to use is that used by TUPA, which consists of automatic annotations provided by spaCy:10 POS tags, syntactic dependency relations, and named entity types and spans. In addition, the closed track only allowed the use of word embeddings provided by fastText (Bojanowski et al., 2017)11 for all languages. Systems in the open track, on the other hand, were allowed to use any additional resource, such as UCCA annotation in other languages, dictionaries or datasets for other tasks, provided that they make sure not to use any additional gold standard annotation over the same text used in the UCCA 10 11 http://spacy.io http://fasttext.cc corpora.12 In both tracks, we required that submitted systems are not trained on the development data. We only held an open track for French, due to the paucity of training data. The four settings and two tracks result in a total of 7 competiti"
S19-2001,N18-2020,1,0.825476,"nkage (e.g., temporal, logical, purposive). A relation between two or more Hs (e.g., “when”, “if”, “in order to”). A relation between the speech event and the uttered Scene (e.g., “surprisingly”). Other Does not introduce a relation or participant. Required by some structural pattern. Table 1: The complete set of categories in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2 Task Definition UCCA represents the semantics of linguistic utterances as directed acyclic graphs (DAGs), where terminal (childless) nodes correspond to the text tokens, and non-terminal nodes to semantic units that participate in some super-or"
S19-2001,N19-1423,0,0.0188459,"Three of the teams adapted the baseline TUPA parser, or parts of it to form their parser, namely TüPa, CUNY-PekingU and XLangMo; HLT@SUDA used a constituency parser (Stern et al., 2017) as a component in their model; DANGNT@UIT.VNU-HCM is a rule-based system over the Stanford Parser, and the rest are newly constructed parsers. All teams found it useful to use external resources beyond those provided by the Shared Task. Four submissions used external embeddings, MUSE (Conneau et al., 2017) in the case of MaskParse@Deskiñ and XLangMo, ELMo (Peters et al., 2018) in the case of TüPa,15 and BERT (Devlin et al., 2019) in the case of HLT@SUDA. 15 GCN-Sem used ELMo in the closed tracks, training on the available data. # Team English-Wiki (closed) 1 HLT@SUDA 2 baseline 3 Davis 4 CUNY-PekingU 5 DANGNT@UIT. VNU-HCM 6 GCN-Sem English-Wiki (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo 5 DANGNT@UIT. VNU-HCM English-20K (closed) 1 HLT@SUDA 2 baseline 3 CUNY-PekingU 4 GCN-Sem English-20K (open) 1 HLT@SUDA 2 TüPa 3 XLangMo 4 baseline German-20K (closed) 1 HLT@SUDA 2 CUNY-PekingU 3 baseline 4 GCN-Sem German-20K (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo French-20K (open) 1 HLT@SUDA 2 XLangMo 3 MaskParse@Deskiñ 4 ba"
S19-2001,N15-1114,0,0.02374,"forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this shared task, we focus on UCCA parsing in multiple languages. U After , A P graduation John A P A moved R to C Paris Figure 1: An example UCCA graph. One of our goals is to benefit semantic parsing in languages with less annotated resources by making use of data from more resource-rich languages. We refer to this approach as cross-lingual parsing, while other works (Zhang et al., 2017, 2018) define cross-lingual parsing as the task of parsing text in one language to me"
S19-2001,S19-2012,0,0.0438323,"Missing"
S19-2001,S19-2015,0,0.0128217,"= `2 . Labeled precision and recall are defined by dividing the number of matching edges in G1 and G2 by |E1 |and |E2 |, respectively. F1 is their harmonic mean: 2· Precision · Recall Precision + Recall Unlabeled precision, recall and F1 are the same, but without requiring that `1 = `2 for the edges to match. We evaluate these measures for primary and remote edges separately. For a more finegrained evaluation, we additionally report precision, recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all"
S19-2001,S19-2013,0,0.0121127,"recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all the tracks.14 12 We are not aware of any such annotation, but include this restriction for completeness. 13 The official evaluation script providing both coarse-grained and fine-grained scores can be found in https://github.com/huji-nlp/ucca/blob/ master/scripts/evaluate_standard.py. 14 It was later discovered that CUNY-PekingU used some of the evaluation data for training in the open tracks, and they were thus disqualified from these tracks."
S19-2001,P05-1013,0,0.222956,"Missing"
S19-2001,L16-1630,0,0.0839468,"structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings. Full results can be found in the task’s website https://competitions. codalab.org/competitions/19160. 1 Overview Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes have recently been put forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this share"
S19-2001,S15-2153,0,0.699415,"Missing"
S19-2001,N18-1202,0,0.0167804,"g the number of non-terminal nodes in the UCCA graphs. Three of the teams adapted the baseline TUPA parser, or parts of it to form their parser, namely TüPa, CUNY-PekingU and XLangMo; HLT@SUDA used a constituency parser (Stern et al., 2017) as a component in their model; DANGNT@UIT.VNU-HCM is a rule-based system over the Stanford Parser, and the rest are newly constructed parsers. All teams found it useful to use external resources beyond those provided by the Shared Task. Four submissions used external embeddings, MUSE (Conneau et al., 2017) in the case of MaskParse@Deskiñ and XLangMo, ELMo (Peters et al., 2018) in the case of TüPa,15 and BERT (Devlin et al., 2019) in the case of HLT@SUDA. 15 GCN-Sem used ELMo in the closed tracks, training on the available data. # Team English-Wiki (closed) 1 HLT@SUDA 2 baseline 3 Davis 4 CUNY-PekingU 5 DANGNT@UIT. VNU-HCM 6 GCN-Sem English-Wiki (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo 5 DANGNT@UIT. VNU-HCM English-20K (closed) 1 HLT@SUDA 2 baseline 3 CUNY-PekingU 4 GCN-Sem English-20K (open) 1 HLT@SUDA 2 TüPa 3 XLangMo 4 baseline German-20K (closed) 1 HLT@SUDA 2 CUNY-PekingU 3 baseline 4 GCN-Sem German-20K (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo French-2"
S19-2001,S19-2016,0,0.028329,"Missing"
S19-2001,P17-1076,0,0.0200505,"etection threshold on the output probabilities. In terms of using the data, all teams but one used the UCCA XML format, two used the CoNLLU format, which is derived by a lossy conversion process, and only one team found the other data formats helpful. One of the teams (MaskParse@Deskiñ) built a new training data adapted to their model by repeating each sentence N times, N being the number of non-terminal nodes in the UCCA graphs. Three of the teams adapted the baseline TUPA parser, or parts of it to form their parser, namely TüPa, CUNY-PekingU and XLangMo; HLT@SUDA used a constituency parser (Stern et al., 2017) as a component in their model; DANGNT@UIT.VNU-HCM is a rule-based system over the Stanford Parser, and the rest are newly constructed parsers. All teams found it useful to use external resources beyond those provided by the Shared Task. Four submissions used external embeddings, MUSE (Conneau et al., 2017) in the case of MaskParse@Deskiñ and XLangMo, ELMo (Peters et al., 2018) in the case of TüPa,15 and BERT (Devlin et al., 2019) in the case of HLT@SUDA. 15 GCN-Sem used ELMo in the closed tracks, training on the available data. # Team English-Wiki (closed) 1 HLT@SUDA 2 baseline 3 Davis 4 CUNY"
S19-2001,W15-3502,1,0.833068,"hich are often different from those tackled in syntactic parsing, including reentrancy (e.g., for sharing arguments across predicates), and the modeling of the interface with lexical semantics. UCCA is a cross-linguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010b,a, 2012). It has demonstrated applicability to multiple languages, including English, French and German, and pilot annotation projects were conducted on a few languages more. UCCA structures have been shown to be well-preserved in translation (Sulem et al., 2015), and to support rapid annotation by nonexperts, assisted by an accessible annotation interface (Abend et al., 2017).1 UCCA has already shown applicative value for text simplifica1 https://github.com/omriabnd/UCCA-App P S A D Process State Participant Adverbial C E N R Center Elaborator Connector Relator H L G Parallel Scene Linker Ground F Function Scene Elements The main relation of a Scene that evolves in time (usually an action or movement). The main relation of a Scene that does not evolve in time. Scene participant (including locations, abstract entities and Scenes serving as arguments)."
S19-2001,P18-1016,1,0.825356,"her types of non-Scene relations: (1) Rs that relate a C to some super-ordinate relation, and (2) Rs that relate two Cs pertaining to different aspects of the parent unit. Inter-Scene Relations A Scene linked to other Scenes by regular linkage (e.g., temporal, logical, purposive). A relation between two or more Hs (e.g., “when”, “if”, “in order to”). A relation between the speech event and the uttered Scene (e.g., “surprisingly”). Other Does not introduce a relation or participant. Required by some structural pattern. Table 1: The complete set of categories in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2"
S19-2001,S19-2014,0,0.0201593,"ng that `1 = `2 for the edges to match. We evaluate these measures for primary and remote edges separately. For a more finegrained evaluation, we additionally report precision, recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all the tracks.14 12 We are not aware of any such annotation, but include this restriction for completeness. 13 The official evaluation script providing both coarse-grained and fine-grained scores can be found in https://github.com/huji-nlp/ucca/blob/ master/scripts/evaluate_s"
S19-2001,D16-1177,0,0.0906443,"Missing"
S19-2001,S19-2017,0,0.0169292,"all Unlabeled precision, recall and F1 are the same, but without requiring that `1 = `2 for the edges to match. We evaluate these measures for primary and remote edges separately. For a more finegrained evaluation, we additionally report precision, recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all the tracks.14 12 We are not aware of any such annotation, but include this restriction for completeness. 13 The official evaluation script providing both coarse-grained and fine-grained scores ca"
S19-2001,I17-1084,0,0.0291596,"Missing"
S19-2001,D18-1194,0,0.0739063,"Missing"
W05-0506,C00-1019,0,0.0787038,"Missing"
W05-0506,2001.mtsummit-ebmt.0,0,0.169,"Missing"
W05-0506,W05-0510,0,\N,Missing
W05-0506,W97-1007,0,\N,Missing
W06-2003,P04-1021,0,0.086155,"Missing"
W06-2003,W02-2017,0,0.0783467,"Missing"
W06-2003,W05-0805,0,0.0369585,"Missing"
W06-2003,C00-2162,0,0.0652858,"Missing"
W06-2003,W04-0109,0,0.056313,"Missing"
W06-2003,W97-0311,0,\N,Missing
W06-2003,W04-0106,0,\N,Missing
W06-2003,N03-2016,0,\N,Missing
W06-2003,P00-1037,0,\N,Missing
W06-2003,J01-2001,0,\N,Missing
W06-2003,J96-4002,0,\N,Missing
W06-2003,C04-1086,0,\N,Missing
W06-2003,J98-4003,0,\N,Missing
W06-2003,W05-0606,0,\N,Missing
W09-0805,P06-1084,0,0.202027,"Missing"
W09-0805,W05-0706,0,0.0315255,"Missing"
W09-0805,P03-1036,0,0.147187,"for languages where context is important for correct segmentation. Our main target language is Hebrew, and we experimented with Arabic as well. As far as we know, there is no work on unsupervised segmentation of words in Hebrew which does not utilize language-specific tools such as morphological analyzers. Lee et al. (2003) addressed supervised word segmentation in Arabic and have some aspects similar to our approach. As in their study, we Extensive research has been done on word segmentation, where, unlike in our study, the segmentation is evaluated for every word, regardless of its context. Creutz (2003) presents an algorithm for unsupervised segmentation under these assumptions. He proposes a probabilistic model which 1 Transcription is according to (Ornan, 2005), except for Shin which is denoted by “$”. 37 the second assumption may fail. This is due to the fact that some languages like Hebrew and Arabic may express relationships not by isolated function words but by morphemes attached in writing to other words. As an example, consider the English word “and’, which was shown to be very useful in concept acquisition (Dorow et al., 2005). In Hebrew this word is usually expressed as the morphem"
W09-0805,P93-1024,0,0.308292,"ematic to apply the proposed methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic algorithmic approach used. The two main algorithmic approaches are clustering of context feature vectors and pattern-based discovery. The first approach is to map each word to a feature vector and cluster these vectors. Example of such algorithms are (Pereira et al., 1993) and (Lin, 1998) that use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. 3 Unsupervised Discovery of Word Categories Recently, there is a growing interest in the second main algorithmic approach, usage of lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Thus (Dorow et al., 2005) discover categories using two basic pre-specified patterns (“x and y”, “x or y”). In this study we use word segmentation t"
W09-0805,P08-1084,0,0.0603708,"Missing"
W09-0805,N07-1020,0,0.0508307,"lts for English and Russian. However, no previous work applies a fully unsupervised concept acquisition for Hebrew. In our study we combine their concept acquisition framework with a simple unsupervised word segmentation technique. Our evaluation confirms the weakness of word-based frameworks for morphology-rich languages such as Hebrew, and shows that utilizing the proposed word segmentation can overcome this weakness while keeping the concept acquisition approach fully unsupervised. utilizes the distributions of morpheme length and frequency to estimate the quality of the induced morphemes. Dasgupta and Ng (2007) improves over (Creutz, 2003) by suggesting a simpler approach. They segment a prefix using the word frequency with and without a prefix. Other recent studies that follow the context-independent setup include (Creutz and Lagus, 2005; Keshava and Pitler, 2005; Demberg, 2007). They test their methods on English, Finnish and Turkish. All of these studies, however, assume contextindependency of segmentation, disregarding the ambiguity that may come from context. This makes it problematic to apply the proposed methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal i"
W09-0805,C02-1114,0,0.0292522,"languages 2 They do not include any specific words, only a relative order of high/low frequency words, and hence can be used on 38 words in total, from which two are (non-adjacent) content words. Four meta-patterns are used: CHC, CHCH, CHHC, HCHC. Second, those patterns which give rise to symmetric lexical relationships are identified. The meaning of phrases constructed from those patterns is (almost) invariant to the order of the content words contained in them. An example for such a pattern is ‘x and y’. In order to identify such useful patterns, for each pattern we build a graph following (Widdows and Dorow, 2002). The graph is constructed from a node for each content word, and a directed arc from the node ‘x’ to ‘y’ if the corresponding content words appear in the pattern such that ‘x’ precedes ‘y’. Then we calculate several symmetry measures on the graph structure and select the patterns with best values for these measures. The third stage is the generation of categories. We extract tightly connected sets of words from the unified graph which combines all graphs of selected patterns. Such sets of words define the desired categories. The patterns which include the ‘x and y’ substring are among the mos"
W09-0805,P06-1038,1,0.829447,"f the language grows, as our lexical knowledge comes solely from a corpus (words appear with and without the function morphemes); (2) information derived from the presence of these morphemes in the sentence is usually lost. In this paper we address the important task of a fully unsupervised acquisition of Hebrew lexical categories (or concepts – words sharing a significant aspect of their meaning). We are not aware of any previous work on this task for Hebrew. Due to the problem above, the performance of many acquisition algorithms deteriorates unacceptably. This happens, for example, in the (Davidov and Rappoport, 2006) algorithm that utilizes automatically detected function words as the main building block for pattern construction. In order to overcome this problem, one should separate such prefixes from the compound words (words consisting of function morphemes attached to content words) in the input corpus. When we consider some particular word, there are frequently many options to split it to smaller strings. Fortunately, the set of function words is small and closed, and the set of grammatical sequences of function prefixes is also small. Hence we assume it does not cost us much to know in advance what"
W09-0805,P07-1030,1,0.90552,"Missing"
W09-0805,P08-1027,1,0.897996,"Missing"
W09-0805,P08-1079,1,0.894923,"Missing"
W09-0805,P07-1116,0,0.0306853,"frameworks for morphology-rich languages such as Hebrew, and shows that utilizing the proposed word segmentation can overcome this weakness while keeping the concept acquisition approach fully unsupervised. utilizes the distributions of morpheme length and frequency to estimate the quality of the induced morphemes. Dasgupta and Ng (2007) improves over (Creutz, 2003) by suggesting a simpler approach. They segment a prefix using the word frequency with and without a prefix. Other recent studies that follow the context-independent setup include (Creutz and Lagus, 2005; Keshava and Pitler, 2005; Demberg, 2007). They test their methods on English, Finnish and Turkish. All of these studies, however, assume contextindependency of segmentation, disregarding the ambiguity that may come from context. This makes it problematic to apply the proposed methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic algorithmic approach used. The two main algo"
W09-0805,P98-2127,0,0.0959341,"d methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic algorithmic approach used. The two main algorithmic approaches are clustering of context feature vectors and pattern-based discovery. The first approach is to map each word to a feature vector and cluster these vectors. Example of such algorithms are (Pereira et al., 1993) and (Lin, 1998) that use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. 3 Unsupervised Discovery of Word Categories Recently, there is a growing interest in the second main algorithmic approach, usage of lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Thus (Dorow et al., 2005) discover categories using two basic pre-specified patterns (“x and y”, “x or y”). In this study we use word segmentation to improve the (D"
W09-0805,J95-3004,0,\N,Missing
W09-0805,D08-1109,0,\N,Missing
W09-0805,P03-1051,0,\N,Missing
W09-0805,C98-2122,0,\N,Missing
W09-1103,W04-3202,0,0.205117,"Missing"
W09-1103,J04-3001,0,0.699938,"tance. In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated sentences S, select a subset S1 ⊂ S for human annotation such that the human efforts in annotating S1 are minimized while the parser performance when trained with this sample is maximized. Previous works addressing training sample size vs. parser performance for constituency parsers (Section 2) evaluated training sample size using the total number of constituents (TC). Sentences differ in length and therefore in annotation efforts, and it has been argued (see, e.g, (Hwa, 2004)) that TC reflects the number of decisions the human annotator makes when syntactically annotating the sample, assuming uniform cost for each decision. In this paper we posit that important aspects of the efforts involved in annotating a sample are not reflected by the TC measure. Since annotators analyze sentences rather than a bag of constituents, sentence structure has a major impact on their cognitive efforts. Sizeable psycholinguistic literature points to the connection between nested structures in the syntactic structure of a sentence and its annotation efforts. This has motivated us to"
W09-1103,I05-1006,0,0.0136421,"uction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated sentences S, select a subset S1 ⊂ S for human annotation such that the human efforts in annotating S1 are minimized while the parser performance when trained with this sample is maximized. Previous works addressing training sample size vs. parser p"
W09-1103,W06-1606,0,0.0141812,"a novel optimisation problem, the constrained multiset multicover problem, and for cluster-based sampling according to syntactic parameters. Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance. 1 Introduction State of the art statistical parsers require large amounts of manually annotated data to achieve good performance. Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone. Statistical parsers are major components in NLP applications such as QA (Kwok et al., 2001), MT (Marcu et al., 2006) and SRL (Toutanova et al., 2005). These often operate over the highly variable Web, which consists of texts written in many languages and genres. Since the performance of parsers markedly degrades when training and test data come from different domains (Lease and Charniak, 2005), large amounts of training data from each domain are required for using them effectively. Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance. In this paper we address this problem through sample selection: given a parsing algorithm"
W09-1103,P00-1016,0,0.04922,"Missing"
W09-1103,P08-1098,1,0.78239,"diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Cognitively Driven Evaluation Measures While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance. We focus on structures that are widely agreed to impose a high cognitive load on the human annotator and on theories considering the cognitive resources required in parsing a complete sentence. Based on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentences. Nested structures. A nested structure is a par"
W09-1103,W07-1001,0,0.0600623,"e present work, and TE gave the best results. They used an uncertainty sampling protocol, where in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters. The most uncertain sentences are selected from the clusters, the training taking into account the densities of the clusters. They reduced the number of training sentences required for their parser to achieve its best performance from 1300 to 400. The importance of cognitively driven measures of sentences’ syntactic complexity has been recognized by Roark et al. (2007) who demonstrated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity"
W09-1103,P02-1016,0,0.264408,"Missing"
W09-1103,D07-1051,0,0.0146346,"ated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Cognitively Driven Evaluation Measures While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance. We focus on structures that are widely agreed to impose a high cognitive load on the human annotator and on theories considering the cognitive resources required in parsing a complete sentence. Based on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentenc"
W09-1103,P05-1073,0,0.0255509,"Missing"
W09-1103,C08-1143,0,0.0193374,"ere in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters. The most uncertain sentences are selected from the clusters, the training taking into account the densities of the clusters. They reduced the number of training sentences required for their parser to achieve its best performance from 1300 to 400. The importance of cognitively driven measures of sentences’ syntactic complexity has been recognized by Roark et al. (2007) who demonstrated their utility for mild cognitive impairment diagnosis. Zhu et al. (2008) used a clustering algorithm for sampling the initial labeled set in an AL algorithm for word sense disambiguation and text classification. In contrast to our CBS method, they proceeded with iterative uncertainty AL selection. Melville et al. (2005) used parameter-based sample selection for a classifier in a classic active learning setting, for a task very different from ours. Sample selection has been applied to many NLP applications. Examples include base noun phrase chunking (Ngai, 2000), named entity recognition (Tomanek et al., 2007) and multi–task annotation (Reichart et al., 2008). 3 Co"
W09-1103,J93-2004,0,\N,Missing
W09-1103,J04-4004,0,\N,Missing
W09-1103,J03-4003,0,\N,Missing
W09-1108,P06-1109,0,0.0183595,"ndidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al.,"
W09-1108,W06-2912,0,0.0126787,"ndidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al.,"
W09-1108,P07-1051,0,0.0125582,"lldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durm"
W09-1108,P99-1016,0,0.0286053,"orpus annotation and other human input used, and in their basic algorithmic approach. Some methods directly aim at concept acquisition, while the direct goal in some is the construction of hyponym (‘is-a’) hierarchies. A subtree in such a hierarchy can be viewed as defining a concept. A major algorithmic approach is to represent word contexts as vectors in some space and use distributional measures and clustering in that space. Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Several studies avoid requiring any syntactic annotation. Some methods are based on decompo49 sition of a lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comparison to pattern approaches (see below), and most of them do not scale well for very large datasets. The second main approach is to use lex"
W09-1108,P05-1022,0,0.0409138,"ws results of this test. P P+ Reg. Window 4 4 ×4 Window 18 5 No windowing 33 21 Table 4: Percentage of noisy concepts as a function of windowing. We can see that while windowing is still essential even with available parser data, using this data we can significantly reduce windowing requirements, allowing us to discover more concepts from the same data. Timing requirements are modest, considering we parsed such large amounts of data. BNC parsing took 45 minutes, and the total single-machine processing time for the 68Gb DMOZ corpus was 4 days6 . In comparison, a state-of-art supervised parser (Charniak and Johnson, 2005) would process the same amount of data in 1.3 years7 . 7 Discussion We have presented a framework which utilizes an efficient fully unsupervised parser for unsupervised pattern-based discovery of concepts. We showed that utilization of unsupervised parser in pattern acquisition not only allows successful extraction of MWEs but also improves the quality of obtained concepts, avoiding noise and adding new terms missed by the parse-less approach. At the same time, the framework remains fully unsupervised, allowing its straightforward application to different languages as supported by our bilingua"
W09-1108,P06-1038,1,0.276144,"named entity recognition tools (Dorow et al., 2005). Pantel et al. (2004) reduce the depth of linguistic data used, but their method requires POS tagging. TextRunner (Banko et al., 2007) utilizes a set of pattern-based seed-less strategies in order to extract relational tuples from text. However, this system contains many language-specific modules, including the utilization of a parser in one of the processing stages. Thus the majority of the existing pattern-based concept acquisition systems rely on pattern/word seeds or supervised language-specific tools, some of which are very inefficient. Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. This framework allows a fully unsupervised seed-less discovery of concepts without relying on language-specific tools. However, it completely ignores potentially useful syntactic or morphological information. For example, the pattern ‘X and his Y’ is useful for acquiring the concept of family member types, as in “his siblings and his parents’. Without syntactic information, it can capture noise, as in “... in ireland) and his wife)” (parentheses denote syntactic constituen"
W09-1108,P07-1030,1,0.856066,"dels (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing with the most recent and comprehensive data, web-based evaluation has some methodological drawbacks such as limited repeatability (Kilgarriff, 2007). In this study we apply our framework on offline corpora in settings similar to that of previous work, in order to be able to make proper comparisons. 3 Efficient Unsupervised Parsing Our method utilizes the information induced by unsupervised parsers. Specifically, we make use of the bracketings induced by Seginer’s parser"
W09-1108,E09-1021,1,0.73873,"plar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing with the most recent and comprehensive data, web-based evaluation has some methodological drawbacks such as limited repeatability (Kilgarriff, 2007). In this study we apply our framework on offline corpora in settings similar to that of previous work, in order to be able to make proper comparisons. 3 Efficient Unsupervised Parsing Our method utilizes the information induced by unsupervised parsers. Specifically, we make use of the bracketings induced by Seginer’s parser1 (Seginer, 2007). This parser has advantages in three ma"
W09-1108,P06-1046,0,0.0196722,"measures and clustering in that space. Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Several studies avoid requiring any syntactic annotation. Some methods are based on decompo49 sition of a lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comparison to pattern approaches (see below), and most of them do not scale well for very large datasets. The second main approach is to use lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns, numerous pattern-based methods have been proposed for the discovery of concepts from seeds. Other studies develop concept acquisition for on-de"
W09-1108,C08-1042,0,0.0310889,"Missing"
W09-1108,C92-2082,0,0.0225641,"lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comparison to pattern approaches (see below), and most of them do not scale well for very large datasets. The second main approach is to use lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns, numerous pattern-based methods have been proposed for the discovery of concepts from seeds. Other studies develop concept acquisition for on-demand tasks where concepts are defined by user-provided seeds. Many of these studies utilize information obtained by language-specific parsing and named entity recognition tools (Dorow et al., 2005). Pantel et al. (2004) reduce the depth of linguistic data used, but their method requires POS tagging. TextRunner (Banko et al., 2007) utilizes a set of pattern-based seed-less strategies in orde"
W09-1108,P02-1017,0,0.0189153,"nsidered as a candidate for the ‘dog type’ concept), and misses the discovery of a valid multiword candidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed framework"
W09-1108,P04-1061,0,0.0710161,"nd misses the discovery of a valid multiword candidate (“Australlian Bulldog”). While symmetry-based filtering greatly reduces such noise, the basic problem remains. As a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzio"
W09-1108,P98-2127,0,0.227447,"on lexical acquisition of all sorts and the acquisition of concepts in particular. Concept acquisition methods differ in the type of corpus annotation and other human input used, and in their basic algorithmic approach. Some methods directly aim at concept acquisition, while the direct goal in some is the construction of hyponym (‘is-a’) hierarchies. A subtree in such a hierarchy can be viewed as defining a concept. A major algorithmic approach is to represent word contexts as vectors in some space and use distributional measures and clustering in that space. Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Several studies avoid requiring any syntactic annotation. Some methods are based on decompo49 sition of a lexically-defined matrix (by SVD, PCA etc), e.g. (Sch¨utze, 1998; Deerwester et al., 1990). While great effort has been made for improving the computational complexity of distributional methods (Gorman and Curran, 2006), they still remain highly computationally intensive in comp"
W09-1108,J93-2004,0,0.0336989,"Missing"
W09-1108,P93-1024,0,0.709275,"Missing"
W09-1108,P07-1049,0,0.224633,"tion in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing with the most recent and comprehensive data, web-"
W09-1108,P06-1072,0,0.0191763,"s a result, incorporating at least some parsing information in a language-independent and efficient manner could be beneficial. Unsupervised parsing has been explored for several decades (see (Clark, 2001; Klein, 2005) for recent reviews). Recently, unsupervised parsers have for the first time outperformed the right branching heuristic baseline for English. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. These works learn an unlabeled syntactic structure, dependency or constituency. In this work we use constituency trees as our syntactic representation. Another important factor in concept acquisition is the source of textual data used. To take advantage of the rapidly expanding web, many of the proposed frameworks utilize web queries rather than local corpora (Etzioni et al., 2005; Davidov et al., 2007; Pasca and Van Durme, 2008; Davidov and Rappoport, 2009). While these methods have a definite practical advantage of dealing wit"
W09-1108,C02-1114,0,0.423695,"mples of such patterns include “((C1 )in C2 ))”, “(C1 )(such(as(((C2 )”, and “(C1 )and(C2 )”3 . We dismiss rare patterns that appear less than TP times per million words. Symmetric patterns. Many of the pattern candidates discovered in the previous stage are not usable. In order to find a usable subset, we focus on the symmetric patterns. We define a symmetric pattern as a pattern in which the same pair of terms (C words) is likely to appear in both left-to-right and right-toleft orders. In order to identify symmetric patterns, for each pattern we define a pattern graph G(P ), as proposed by (Widdows and Dorow, 2002). If term pair (C1 , C2 ) appears in pattern P in some context, 3 This paper does not use any punctuation since the parser is provided with sentences having all non-alphabetic characters removed. We assume word separation. C1,2 can be a word or a multiword expression. we add nodes c1 , c2 to the graph and a directed edge EP (c1 , c2 ) between them. In order to select symmetric patterns, we create such a pattern graph for every discovered pattern, and create a symmetric subgraph SymG(P) in which we take only bidirectional edges from G(P ). Then we compute three measures for each pattern candida"
W09-1108,C96-1003,0,\N,Missing
W09-1108,W02-0908,0,\N,Missing
W09-1108,J98-1004,0,\N,Missing
W09-1108,P08-1003,0,\N,Missing
W09-1108,C98-2122,0,\N,Missing
W09-1120,P08-1087,0,0.0373849,"Missing"
W09-1120,P06-1109,0,0.161926,"quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is ver"
W09-1120,W06-2912,0,0.276366,"quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is ver"
W09-1120,E03-1009,0,0.635014,"ides a quality score for every sentence in a parsed sentences set. An NLP application can then decide if to use a parse or not, according to its own definition of a high quality parse. For example, it can select every sentence whose score is above some threshold, or the k top scored sentences. The selection strategy is application dependent and is beyond the scope of this paper. The unsupervised parser we use is the Seginer (2007) incremental parser2 , which achieves state-ofthe-art results without using manually created POS tags. The POS tags we use are induced by the unsupervised tagger of (Clark, 2003)3 . Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward. Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality (Reichart and Rappoport, 2007). While the algorithms of Yates et al. (2006), Kawahara and Uchimoto (2008) and Ravi et al. (2008) are supervised (Se"
W09-1120,N04-4028,0,0.0137071,"highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ 20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA 20, 15610 sentences, 108540 constiteunts), b"
W09-1120,P06-1111,0,0.0345575,"Missing"
W09-1120,I08-2097,0,0.171963,"s of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighbor"
W09-1120,P02-1017,0,0.653127,"tagger and an unsupervised parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many"
W09-1120,P04-1061,0,0.485316,"d parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used"
W09-1120,P08-2055,0,0.0273299,"arsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ 20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA 20, 15610 sentences, 108540 constiteunts), both containing newspaper texts. The unsupervised parsers of the kind add"
W09-1120,N03-1022,0,0.0302606,"Missing"
W09-1120,D08-1093,0,0.432067,"e on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighboring constituents. Co"
W09-1120,P07-1052,1,0.766884,"ality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the POS sequences of their yield"
W09-1120,C08-1091,1,0.869411,"Missing"
W09-1120,P07-1076,0,0.0153557,"e). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ 20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA 20, 15610 s"
W09-1120,P07-1049,0,0.460773,"German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for man"
W09-1120,P08-1117,1,0.858039,"Missing"
W09-1120,P06-1072,0,0.219551,"from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement. 1 Introduction In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data. The last decade has seen significant progress in this field of research (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b; Smith and Eisner, 2006; Seginer, 2007). Many NLP systems use the output of supervised parsers (e.g., (Kwok et al., 2001) for QA, (Moldovan et al., 2003) for IE, (Punyakanok et al., 2008) for SRL, (Srikumar et al., 2008) for Textual Inference and (Avramidis and Koehn, 2008) for MT). To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse (Lease and Charniak, 2005; McClosky and Charniak, 2008). In the highly variable Web, where many of these systems are used, it is very difficult to create a representativ"
W09-1120,J07-1003,0,0.0139084,"rain the parser. In the first work, high quality parses were selected using an ensemble method, while in the second a binary classifier was used (see above). The first system achieved the highest score in the CoNLL 2007 shared task on domain adaptation of dependency parsers, and the second system improved over the basic self-training protocol. Chen et al. (2008) parsed target domain sentences and used short dependencies information, which is often accurate, to adapt a dependency parser to the Chinese language. Automatic quality assessment has been extensively explored for machine translation (Ueffing and Ney, 2007) and speech recognition (Koo et al., 2001). Other NLP tasks where it has been explored include semi-supervised relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003), and dialog systems (Lin and Weng, 2008). The idea of representing a constituent by its yield and (a different definition of) context is used by the CCM unsupervised parsing model (Klein and Manning, 2002). As far as we know the current work is the first to use unsupervised POS tags for the selection of high quality parses. 4 Evaluation Setup We experiment with sentences o"
W09-1120,P01-1067,0,0.153538,"Missing"
W09-1120,W06-1604,0,0.219236,"ssessment of high quality parses cre156 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ated by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers. Assessing the quality of a learning algorithm’s output and selecting high quality instances has been addressed for supervised algorithms (Caruana and Niculescu-Mizil, 2006) and specifically for supervised parsers (Yates et al., 2006; Reichart and Rappoport, 2007; Kawahara and Uchimoto, 2008; Ravi et al., 2008). Moreover, it has been shown to be valuable for supervised parser adaptation between domains (Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; Chen et al., 2008). However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1 . The constituents in the batch are represented using the"
W09-1120,C08-1015,0,\N,Missing
W09-1120,N03-1004,0,\N,Missing
W09-1120,J08-2005,0,\N,Missing
W09-1120,J03-4003,0,\N,Missing
W09-1120,D07-1111,0,\N,Missing
W09-1120,P08-2026,0,\N,Missing
W09-1120,C96-2102,0,\N,Missing
W09-1121,P98-1012,0,0.113655,"e range is (0, 1] and gives higher scores to clusterings that are preferable. As noted by (Rosenberg and Hirschberg, 2007), the Q measure does not explicitly address the completeness of the suggested clustering. Due to the cost term, if two clusterings have the same H(C|K) value, the model prefers the one with the lower number of clusters, but the trade-off between homogeneity and completeness is not explicitly addressed. In the next section we describe the V and VI measures, which are IT measures that explicitly assess both the homogeneity and completeness of the clustering solution. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. It does not explicitly use IT concepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Rin"
W09-1121,P08-2012,0,0.0258213,"completeness is not explicitly addressed. In the next section we describe the V and VI measures, which are IT measures that explicitly assess both the homogeneity and completeness of the clustering solution. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. It does not explicitly use IT concepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Ringger, 2008), using clustering F-score, the adjusted Rand index, V, VI and Q2 for document clustering; and (Reichart and Rappoport, 2008), using greedy 1-to1 and many-to-1 mappings for evaluating labeled parse tree induction. Schulte im Walde (2003) used clustering to induce semantic verb classes and extensively discussed non-IT based clustering evaluation measures. Pfitzner et al."
W09-1121,P07-1094,0,0.0810787,"emonstrate the superiority of NVI in a large experiment involving an important NLP application, grammar induction, using real corpus data in English, German and Chinese. 1 Introduction Clustering is a major technique in machine learning and its application areas. It lies at the heart of unsupervised learning, which has great potential advantages over supervised learning. This is especially true for NLP, due to the high efforts and costs incurred by the human annotations required for training supervised algorithms. Recent NLP problems addressed by clustering include POS induction (Clark, 2003; Goldwater and Griffiths, 2007), word sense disambiguation (Shin and Choi, 2004), semantic role labeling (Baldewein et al., 2004), pitch accent type disambiguation (Levow, 2006) and grammar induction (Klein, 2005). Evaluation of clustering results is a challenging task. In this paper we address the external measures setting, where a correct assignment of elements to classes is available and is used for evaluating the quality of another assignment of the elements into clusters. Many NLP works have used external clustering evaluation measures (see Section 2). Recently, two measures have been proposed that avoid many of the we"
W09-1121,C08-1042,0,0.0722077,"measures that explicitly assess both the homogeneity and completeness of the clustering solution. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. It does not explicitly use IT concepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Ringger, 2008), using clustering F-score, the adjusted Rand index, V, VI and Q2 for document clustering; and (Reichart and Rappoport, 2008), using greedy 1-to1 and many-to-1 mappings for evaluating labeled parse tree induction. Schulte im Walde (2003) used clustering to induce semantic verb classes and extensively discussed non-IT based clustering evaluation measures. Pfitzner et al. (2008) presented a comparison of clustering evaluation measures (IT based and others). While their analysis"
W09-1121,N06-1029,0,0.0220773,"nese. 1 Introduction Clustering is a major technique in machine learning and its application areas. It lies at the heart of unsupervised learning, which has great potential advantages over supervised learning. This is especially true for NLP, due to the high efforts and costs incurred by the human annotations required for training supervised algorithms. Recent NLP problems addressed by clustering include POS induction (Clark, 2003; Goldwater and Griffiths, 2007), word sense disambiguation (Shin and Choi, 2004), semantic role labeling (Baldewein et al., 2004), pitch accent type disambiguation (Levow, 2006) and grammar induction (Klein, 2005). Evaluation of clustering results is a challenging task. In this paper we address the external measures setting, where a correct assignment of elements to classes is available and is used for evaluating the quality of another assignment of the elements into clusters. Many NLP works have used external clustering evaluation measures (see Section 2). Recently, two measures have been proposed that avoid many of the weaknesses of previous measures and exhibit several attractive properties (see Sections 2 and 3): the VI measure (Meila, 2007) and the V measure (Ro"
W09-1121,C08-1091,1,0.889755,"oncepts and avoids mapping. In this paper we focus on V and VI; a detailed comparison with BCubed is out of our scope here and will be done in future work. Several recent NLP papers used clustering techniques and evaluation measures. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution; (Headden et al., 2008), using VI, V, greedy 1-to-1 and many-to-1 mapping for evaluating unsupervised POS induction; (Walker and Ringger, 2008), using clustering F-score, the adjusted Rand index, V, VI and Q2 for document clustering; and (Reichart and Rappoport, 2008), using greedy 1-to1 and many-to-1 mappings for evaluating labeled parse tree induction. Schulte im Walde (2003) used clustering to induce semantic verb classes and extensively discussed non-IT based clustering evaluation measures. Pfitzner et al. (2008) presented a comparison of clustering evaluation measures (IT based and others). While their analysis is extensive, their experiments were confined to artificial data. In this work, we experiment with a complex NLP application using large real datasets. 3 dom variables. When assuming the uniform distribution, the probability of an event (a clas"
W09-1121,D07-1043,0,0.234476,"6) and grammar induction (Klein, 2005). Evaluation of clustering results is a challenging task. In this paper we address the external measures setting, where a correct assignment of elements to classes is available and is used for evaluating the quality of another assignment of the elements into clusters. Many NLP works have used external clustering evaluation measures (see Section 2). Recently, two measures have been proposed that avoid many of the weaknesses of previous measures and exhibit several attractive properties (see Sections 2 and 3): the VI measure (Meila, 2007) and the V measure (Rosenberg and Hirschberg, 2007). However, each of these has a serious drawback. The possible values of VI lie in [0, 2log N ], where N is the size of the clustered dataset. Hence it has limited use when comparing performance on different datasets. V measure values lie in [0, 1] regardless of the dataset, but the measure strongly favors a clustering having many small clusters. In addition, V does not have many of the attractive properties of VI. This paper has two contributions. First, we propose the NVI measure, a normalization of VI which guarantees that the score of clusterings that VI considers good lies in [0,1], regard"
W09-1121,P07-1049,0,0.0845474,"Missing"
W09-1121,C02-1145,0,0.038109,"Missing"
W09-1121,W04-0817,0,\N,Missing
W09-1121,E03-1009,0,\N,Missing
W09-1121,J06-2001,0,\N,Missing
W09-1121,C98-1012,0,\N,Missing
W10-2909,P06-3002,0,0.0656106,"ddressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrast"
W10-2909,W95-0101,0,0.0949316,", 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3 The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. 61 In addition to comparing the different algorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm a"
W10-2909,J92-4003,0,0.440785,"Missing"
W10-2909,E03-1009,0,0.761571,"o local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results. 1 arir@cs.huji.ac.il Introduction Unsupervised part-of-speech (POS) induction is of major theoretical and practical importance. It counters the arbitrary nature of manually designed tag sets,"
W10-2909,D07-1023,0,0.155129,"Missing"
W10-2909,P08-1085,0,0.142883,"d algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words i"
W10-2909,P07-1094,0,0.646832,"drastically, exhibit better correlation between perplexity and external clustering quality3 . Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types cluster"
W10-2909,P05-1044,0,0.494732,"2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) with a detailed demonstration in (Johnson, 2007). All these authors (except Smith and Eisner (2005), see below), however, reported average results over several runs and did not try to identify the runs that produce high quality tagging. Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. In this p"
W10-2909,D07-1031,0,0.78092,"for which the entropy-based filter improves perplexity more drastically, exhibit better correlation between perplexity and external clustering quality3 . Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used dis"
W10-2909,N09-1010,0,0.0478669,"Missing"
W10-2909,D08-1109,0,0.0436534,"2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3 The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. 61 In addition to comparing the different algorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm and our ZCC algorithm. Clustering Quality Evaluation. The induced POS tags have arbitrary names. To evaluate them against a manually annotated corpus, a proper correspondence"
W10-2909,J06-4002,0,0.0639965,"Missing"
W10-2909,D09-1072,0,0.496511,"of 0.9) remains relatively constant. Figure 2 (right) shows that models for which the entropy-based filter improves perplexity more drastically, exhibit better correlation between perplexity and external clustering quality3 . Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and"
W10-2909,J94-2001,0,0.490967,"ood or data probability for this tagger) is evaluated on the test set. Moreover, we show that our algorithm outperforms existing POS taggers for most evaluation measures (Table 3). Identifying good solutions among many runs of a randomly-initialized algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et a"
W10-2909,P09-1057,0,0.287856,"supervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyde"
W10-2909,W09-1121,1,0.93754,"gorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm and our ZCC algorithm. Clustering Quality Evaluation. The induced POS tags have arbitrary names. To evaluate them against a manually annotated corpus, a proper correspondence with the gold standard POS tags should be established. Many evaluation measures for unsupervised clustering against gold standard exist. Here we use measures from two well accepted families: mapping based and information theoretic (IT) based. For a recent discussion on this subject see (Reichart and Rappoport, 2009). The mapping based measures are accuracy with greedy many-to-1 (M-1) and with greedy 1-to-1 (1-1) mappings of the induced to the gold labels. In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster. After each induced label is mapped to a gold label, tagging accuracy is computed. Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus. The IT based measures we use are V (Rosenberg and Hirschber"
W10-2909,D07-1043,0,0.077745,"rt and Rappoport, 2009). The mapping based measures are accuracy with greedy many-to-1 (M-1) and with greedy 1-to-1 (1-1) mappings of the induced to the gold labels. In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster. After each induced label is mapped to a gold label, tagging accuracy is computed. Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus. The IT based measures we use are V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009). The latter is a normalization of the VI measure (Meila, 2007). VI and NVI induce the same order over clusterings but NVI values for good clusterings lie in [0, 1]. For V, the higher the score, the better the clustering. For NVI lower scores imply improved clustering quality. We use e as the base of the logarithm. Evaluation of the Quality Test. To measure the correlation between the score produced by the tagging quality test and the external quality of a tagging, we use two well accepted measures: Spearman’s rank correlation coefficient and Kendall Tau"
W10-2909,E95-1020,0,0.466767,"is 1.1. 3 partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and"
W10-2909,P04-1062,0,0.0284705,"ed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. Moreover, we show that our algorithm outperforms existing POS taggers for most evaluation measures (Table 3). Identifying good solutions among many runs of a randomly-initialized algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was apPrevious Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag,"
W10-2909,C04-1052,0,\N,Missing
W10-2909,D09-1071,0,\N,Missing
W10-2909,N06-1041,0,\N,Missing
W10-2909,C04-1080,0,\N,Missing
W10-2909,D08-1036,0,\N,Missing
W10-2911,E03-1009,0,0.841377,"great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level e"
W10-2911,D07-1023,0,0.0965229,"e two average variants, micro and macro. Macro average computes the total number of matches over all words and normalizes in the end. Recall (R), Precision (P) and their harmonic average (Fscore) are accordingly defined: R= IM Pl i=1 |Ai | P = M acroI = In the example in Section 3 showing an unreasonable behavior of IT-based measures, the score depends on r for both MacroI and MicroI. With our new measures, recall is always 1, but precision is nr . This is true both for 1-1 and M-1 mappings. Hence, the new measures show reasonable behavior in this example for all r values. MicroI was used in (Dasgupta and Ng, 2007) with a manually compiled mapping. Their mapping was not based on a well-defined scheme but on a heuristic. Moreover, providing a manual mapping might be impractical when the number of clusters is large, and can be inaccurate, especially when the clustering is not of very high quality. In the following we discuss how to compute the 1-1 and M-1 greedy mappings for each measure. IM Pl i=1 |h(Bi )| 2RP = R+P l 1-1 Mapping. We compute h by finding the maximal weighted matching in a bipartite graph. In this graph one side represents the induced clusters, the other represents the gold classes and th"
W10-2911,P06-1038,1,0.769359,"tudy, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algorithm that will be invoked on a diverse set of cor"
W10-2911,P08-1079,1,0.802772,"iment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algorithm that will be invoked on a diverse set of corpora having different token st"
W10-2911,N09-1019,0,0.0203112,"against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute merits of different a"
W10-2911,D08-1036,0,0.454036,"tion, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute"
W10-2911,P06-1144,0,0.0349584,"Missing"
W10-2911,P07-1094,0,0.337074,"s on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on t"
W10-2911,W06-1633,0,0.0304537,"ance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuabl"
W10-2911,C08-1042,0,0.109725,"Missing"
W10-2911,C08-1091,1,0.872091,"polysemous, the common case in NLP. We demonstrate the benefits of our measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different"
W10-2911,W09-1121,1,0.936781,"n Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among H(C) = − P|C| H(C|K) = − c=1 P|K| I k=1 ck N P|K |P|C| c=1 k=1 log Ick N P|K| I k=1 ck N I log P|C|ck c=1 Ick H(K) and H(K|C) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: h= ( c= ( 1 1− H(C|K) H(C) H(C) = 0 H(C) 6= 0 1 1− H(K|C) H(K) H(K) = 0 H(K) 6= 0 V = 79 2hc h+c N V I(C, K) = ( H(C|K)+H(K|C) H(C) H(K) show below, these measures do not suffer from the problems discussed for IT measures in Section 3. All measures are mapping-based: first, a mapping between the induced and gold clusters is performed, and then a measure E is computed. As is common in the"
W10-2911,D07-1043,0,0.0829272,"sion and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among H(C) = − P|C| H(C|K) = − c=1 P|K| I k=1 ck N P|K |P|C| c=1 k=1 log Ick N P|K| I k=1 ck N I log P|C|ck c=1 Ick H(K) and H(K|C) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: h= ( c= ( 1 1− H(C|K) H(C) H(C) = 0 H(C) 6= 0 1 1− H(K|C) H(K) H(K) = 0 H(K) 6= 0 V = 79 2hc h+c N V I(C, K) = ( H(C|K)+H(K|C) H(C) H(K) show below, these measures do not suffer from the problems discussed for IT measures in Section 3. All measures are mapping-based: first, a mapping between the induced and gold clusters is performed, and then a meas"
W10-2911,J06-2001,0,0.335372,"ur measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algo"
W10-2911,P04-1062,0,0.0714122,"Missing"
W10-2911,J93-2004,0,\N,Missing
W10-2911,D09-1071,0,\N,Missing
W10-2911,P10-1132,1,\N,Missing
W10-2914,P09-2041,0,0.0657087,"94; Gibbs and Colston, 2007), automatic recognition of sarcasm is a novel task, addressed only by few works. In the context of opinion mining, sarcasm is mentioned briefly as a hard nut that is yet to be cracked, see comprehensive overview by (Pang and Lee, 2008). Tepperman et al. (2006) identify sarcasm in spoken dialogue systems, their work is restricted to sarcastic utterances that contain the expression ‘yeah-right’ and it depends heavily on cues in the spoken dialogue such as laughter, pauses within the speech stream, the gender (recognized by voice) of the speaker and prosodic features. Burfoot and Baldwin (2009) use SVM to determine whether newswire articles are true or satirical. They introduce the notion of validity which models absurdity via a measure somewhat close to 114 sarcasm recognition for review ranking and summarization systems and for brand monitoring systems. PMI. Validity is relatively lower when a sentence includes a made-up entity or when a sentence contains unusual combinations of named entities such as, for example, those in the satirical article beginning “Missing Brazilian balloonist Padre spotted straddling Pink Floyd flying pig”. We note that while sarcasm can be based on exagg"
W10-2914,P06-1038,1,0.852042,"a one line summary. Reviews refer to a specific product and rarely address each other. Each review sentence is, therefore, part of a context – the specific product, the star rating, the summary and other sentences in that review. In that sense, sentences in the Amazon dataset differ radically from the contextless tweets. It is worth mentioning that the majority of reviews are on the very positive side (star rating average of 4.2 stars). 3 Pattern extraction Our main feature type is based on surface patterns. In order to extract such patterns automatically, we followed the algorithm given in (Davidov and Rappoport, 2006). We classified words into high-frequency words (HFWs) and content words (CWs). A word whose corpus frequency is more (less) than FH (FC ) is considered to be a HFW (CW). Unlike in (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. We also consider [product], [company], [title], [author] tags as HFWs for pattern extraction. We define a pattern as an ordered sequence of high frequency words and slots for content words. The FH and FC thresholds were set to 1000 words per million (upper bound for FC ) and 100 words per million (lower bound for FH )3 . Classification Al"
W10-2914,D07-1035,0,0.0159626,"at acquired it. The seed sentences together with newly acquired sentences constitute the (enriched) training set. Data enrichment was performed only for the Amazon dataset where we have a manually tagged seed and the sentence structure is closer to standard English grammar. We refer the reader to (Tsur et al., 2010) for more details about the enrichment process and for a short discussion about the usefulness of web-based data enrichment in the scope of sarcasm recognition. to be expected, since non-sarcastic sentences outnumber sarcastic ones, definitely when most online reviews are positive (Liu et al., 2007). This generally positive tendency is also reflected in our data – the average number of stars is 4.12. Seed training set with #sarcasm (Twitter). We used a sample of 1500 tweets marked with the #sarcasm hashtag as a positive set that represents sarcasm styles special to Twitter. However, this set is very noisy (see discussion in Section 5). Seed training set (cross domain). Results obtained by training on the 1500 #sarcasm hashtagged tweets were not promising. Examination of the #sarcasm tagged tweets shows that the annotation is biased and noisy as we discuss in length in Section 5. A better"
W10-2914,H05-1067,0,0.0879148,"tion of sarcasm. The proposed algorithm utilizes some features specific to (Amazon) product reviews. This paper continues this line, proposing SASI a robust algorithm that successfully captures sarcastic sentences in other, radically different, domains such as twitter. Utsumi (1996; 2000) introduces the implicit display theory, a cognitive computational framework that models the ironic environment. The complex axiomatic system depends heavily on complex formalism representing world knowledge. While comprehensive, it is currently impractical to implement on a large scale or for an open domain. Mihalcea and Strapparava (2005) and Mihalcea and Pulman (2007) present a system that identifies humorous one-liners. They classify sentences using naive Bayes and SVM. They conclude that the most frequently observed semantic features are negative polarity and human-centeredness. These features are also observed in some sarcastic utterances. Some philosophical, psychological and linguistic theories of irony and sarcasm are worth referencing as a theoretical framework: the constraints satisfaction theory (Utsumi, 1996; Katz, 2005), the role playing theory (Clark and Gerrig, 1984), the echoic mention framework (Wilson and Sper"
W10-2914,P04-1035,0,0.00466619,"dge a book by its cover’, choosing it as the title of the review reveals its sarcastic nature. Although the negative sentiment is very explicit in the iPod review (5), the sarcastic effect emerges from the pun that assumes the knowledge that the design is one of the most celebrated features of Apple’s products. (None of the above reasoning was directly introduced to our algorithm.) Modeling the underlying patterns of sarcastic utterances is interesting from the psychological and cognitive perspectives and can benefit various NLP systems such as review summarization (Popescu and Etzioni, 2005; Pang and Lee, 2004; Wiebe et al., 2004; Hu and Liu, 2004) and dialogue systems. Following the ‘brilliant-butcruel’ hypothesis (Danescu-Niculescu-Mizil et al., 2009), it can help improve ranking and recommendation systems (Tsur and Rappoport, 2009). All systems currently fail to correctly classify the sentiment of sarcastic sentences. In this paper we utilize the semi-supervised sarcasm identification algorithm (SASI) of (Tsur et al., 2010). The algorithm employs two modules: semi supervised pattern acquisition for identifying sarcastic patterns that serve as features for a classifier, and a classification stage"
W10-2914,H05-1043,0,0.0269981,"ng the expression ‘don’t judge a book by its cover’, choosing it as the title of the review reveals its sarcastic nature. Although the negative sentiment is very explicit in the iPod review (5), the sarcastic effect emerges from the pun that assumes the knowledge that the design is one of the most celebrated features of Apple’s products. (None of the above reasoning was directly introduced to our algorithm.) Modeling the underlying patterns of sarcastic utterances is interesting from the psychological and cognitive perspectives and can benefit various NLP systems such as review summarization (Popescu and Etzioni, 2005; Pang and Lee, 2004; Wiebe et al., 2004; Hu and Liu, 2004) and dialogue systems. Following the ‘brilliant-butcruel’ hypothesis (Danescu-Niculescu-Mizil et al., 2009), it can help improve ranking and recommendation systems (Tsur and Rappoport, 2009). All systems currently fail to correctly classify the sentiment of sarcastic sentences. In this paper we utilize the semi-supervised sarcasm identification algorithm (SASI) of (Tsur et al., 2010). The algorithm employs two modules: semi supervised pattern acquisition for identifying sarcastic patterns that serve as features for a classifier, and a"
W10-2914,C96-2162,0,0.221374,"in the satirical article beginning “Missing Brazilian balloonist Padre spotted straddling Pink Floyd flying pig”. We note that while sarcasm can be based on exaggeration or unusual collocations, this model covers only a limited subset of the sarcastic utterances. Tsur et al. (2010) propose a semi supervised framework for recognition of sarcasm. The proposed algorithm utilizes some features specific to (Amazon) product reviews. This paper continues this line, proposing SASI a robust algorithm that successfully captures sarcastic sentences in other, radically different, domains such as twitter. Utsumi (1996; 2000) introduces the implicit display theory, a cognitive computational framework that models the ironic environment. The complex axiomatic system depends heavily on complex formalism representing world knowledge. While comprehensive, it is currently impractical to implement on a large scale or for an open domain. Mihalcea and Strapparava (2005) and Mihalcea and Pulman (2007) present a system that identifies humorous one-liners. They classify sentences using naive Bayes and SVM. They conclude that the most frequently observed semantic features are negative polarity and human-centeredness. Th"
W10-2914,J04-3002,0,0.0625604,"ver’, choosing it as the title of the review reveals its sarcastic nature. Although the negative sentiment is very explicit in the iPod review (5), the sarcastic effect emerges from the pun that assumes the knowledge that the design is one of the most celebrated features of Apple’s products. (None of the above reasoning was directly introduced to our algorithm.) Modeling the underlying patterns of sarcastic utterances is interesting from the psychological and cognitive perspectives and can benefit various NLP systems such as review summarization (Popescu and Etzioni, 2005; Pang and Lee, 2004; Wiebe et al., 2004; Hu and Liu, 2004) and dialogue systems. Following the ‘brilliant-butcruel’ hypothesis (Danescu-Niculescu-Mizil et al., 2009), it can help improve ranking and recommendation systems (Tsur and Rappoport, 2009). All systems currently fail to correctly classify the sentiment of sarcastic sentences. In this paper we utilize the semi-supervised sarcasm identification algorithm (SASI) of (Tsur et al., 2010). The algorithm employs two modules: semi supervised pattern acquisition for identifying sarcastic patterns that serve as features for a classifier, and a classification stage that classifies eac"
W10-2914,H05-2017,0,\N,Missing
W13-0101,P10-1024,1,0.832932,"ut the cake”. “quickly” meets both the syntactic and the semantic criteria for an adjunct: it is optional and it serves to restrict the meaning of “walked”. It also has a similar semantic content when appearing with different verbs (“walk quickly”, “eat quickly”, “talk quickly” etc.). “the cake” meets both the syntactic and the semantic criteria for a core: it is obligatory, and completes the meaning of “cut”. However, many other cases are not as obvious. For instance, in “he walked into his office”, the boldfaced argument is a core according to Framenet, but an adjunct according to PropBank (Abend and Rappoport, 2010). The core-adjunct distinction in UCCA is translated into the distinction between D’s (Adverbials) and A’s (Participants). UCCA is a semantic scheme and therefore the syntactic criterion of “obligatoriness” is not applicable, and is instead left to be detected by statistical means. Instead, UCCA defines A’s as units that introduce a new participant to the Scene and D’s as units that add more information to the Scene without introducing a participant. Revisiting our earlier examples, in “Woody cut the cake”, “the cake” introduces a new participant and is therefore an A, while in “Woody walked q"
W13-0101,P09-1004,1,0.904978,"Missing"
W13-0101,P98-1013,0,0.553651,"ers of annotation, special categories will be devoted to annotating part-whole relations and the semantic relations described by determiners. Figure 2(c) presents the annotation of this example. 4 3.2 Beyond Simple Scenes Nominal Predicates. The foundational layer of UCCA annotates the argument structure of nominal predicates much in the same fashion as that of verbal predicates. This accords with the standard practice in several NLP resources, which tend to use the same formal devices for annotating nominal and verbal argument structure (see, e.g., NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)). For example, consider “his speech against the motion”. “speech” evokes a Scene that evolves in time and is therefore a P. The Scene has two Participants, namely “his” and “against the motion”. Multiple Parents. In general, a unit may participate in more than one relation. To this end, UCCA allows a unit to have multiple parents. Recall that in UCCA, a non-terminal node represents a relation, and its descendants are the sub-units comprising it. A unit’s category is a label over the edge connecting love E A A P A Woody bones D P generally E big dogs P C bike his (a) John home rides C A A A (b"
W13-0101,W12-3602,0,0.185448,"irst, UCCA rejects the assumption that every structure has a unique head. Formally, instead of selecting a single head whose descendants are (the heads of) the argument units, UCCA introduces a new node for each relation, whose descendants are all the sub-units comprising that relation, including the predicate and its arguments. The symmetry between the descendants is broken through the features placed on the edges. Consider coordination structures as an example. The difficulty of dependency grammar to capture such structures is exemplified by the 8 possible annotations in current use in NLP (Ivanova et al., 2012). In UCCA, all elements of the coordination (i.e., the conjunction along with its conjuncts) are descendants of a mutual parent, where only their categories distinguish between their roles. For instance, in “John and Mary”, “John”, “Mary” and “and” are all listed under a joint parent. Discontiguous conjunctions (such as “either John or Mary”) are also handled straightforwardly by placing “either” and “or” under a single parent, which in turn serves as a Connector (Figure 2(h)). Note that the edges between “either” and “or” and their mutual parent have no category labels, since the unit “either"
W13-0101,J93-2004,0,0.0515533,"tic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the schemes requires highly proficient workforce for its annotation. For example, the Penn Treebank project (PTB) (Marcus et al., 1993) used linguistics graduates as annotators. In this paper we propose a radically different approach to grammatical annotation. Under this approach, only semantic distinctions are manually annotated, while distributional regularities are induced using statistical algorithms and without any direct supervision. This approach has four main advantages. First, it facilitates manual annotation that would no longer require close acquaintance with syntactic theory. Second, a data-driven approach for detecting distributional regularities is less prone to errors and to the incorporation of implicit biases"
W13-0101,J08-2001,0,0.0283594,"Missing"
W13-0101,meyers-etal-2004-annotating,0,0.34244,"otated as an E. In more refined layers of annotation, special categories will be devoted to annotating part-whole relations and the semantic relations described by determiners. Figure 2(c) presents the annotation of this example. 4 3.2 Beyond Simple Scenes Nominal Predicates. The foundational layer of UCCA annotates the argument structure of nominal predicates much in the same fashion as that of verbal predicates. This accords with the standard practice in several NLP resources, which tend to use the same formal devices for annotating nominal and verbal argument structure (see, e.g., NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)). For example, consider “his speech against the motion”. “speech” evokes a Scene that evolves in time and is therefore a P. The Scene has two Participants, namely “his” and “against the motion”. Multiple Parents. In general, a unit may participate in more than one relation. To this end, UCCA allows a unit to have multiple parents. Recall that in UCCA, a non-terminal node represents a relation, and its descendants are the sub-units comprising it. A unit’s category is a label over the edge connecting love E A A P A Woody bones D P generally E big dogs P C bike"
W13-0101,J05-1004,0,0.17863,"their mother who abandoned ... children]A ” • “motherA ... [gave up acting]P ” • “motherA ... [focus on raising]P [her children]A ” 4 Previous Work Many grammatical annotation schemes have been proposed over the years in an attempt to capture the richness of grammatical phenomena. In this section, we focus on approaches that provide a sizable corpus of annotated text. We put specific emphasis on English corpora, which is the most studied language and the focus language of this paper. Semantic Role Labeling Schemes. The most prominent schemes to SRL are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) for verbal predicates and NomBank for nominal predicates (Meyers et al., 2004). They share with UCCA their focus on semanticallymotivated rather than distributionally-motivated distinctions. However, unlike UCCA, they annotate each predicate separately, yielding shallow representations which are hard to learn directly without using syntactic parsing as preprocessing (Punyakanok et al., 2008). In addition, UCCA has a wider coverage than these projects, as it addresses both verbal, nominal and adjectival predicates. Recently, the Framenet Constructicon project (Fillm"
W13-0101,J08-2005,0,0.0468631,"A for Universal Conceptual Cognitive Annotation. The scheme covers many of the most important elements and relations present in linguistic utterances, including verb-argument structure, optional adjuncts such as adverbials, clause embeddings, and the linkage between them. The scheme is supported by extensive typological crosslinguistic evidence and accords with the leading Cognitive Linguistics theories. 1 Introduction Syntactic annotation is used as scaffolding in a wide variety of NLP applications. Examples include Machine Translation (Yamada and Knight, 2001), Semantic Role Labeling (SRL) (Punyakanok et al., 2008) and Textual Entailment (Yuret et al., 2010). Syntactic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the schemes requires highly proficient workforce for its annota"
W13-0101,P11-1067,1,0.910132,"Missing"
W13-0101,P01-1067,0,0.0330633,"end, we propose a simple semantic annotation scheme, UCCA for Universal Conceptual Cognitive Annotation. The scheme covers many of the most important elements and relations present in linguistic utterances, including verb-argument structure, optional adjuncts such as adverbials, clause embeddings, and the linkage between them. The scheme is supported by extensive typological crosslinguistic evidence and accords with the leading Cognitive Linguistics theories. 1 Introduction Syntactic annotation is used as scaffolding in a wide variety of NLP applications. Examples include Machine Translation (Yamada and Knight, 2001), Semantic Role Labeling (SRL) (Punyakanok et al., 2008) and Textual Entailment (Yuret et al., 2010). Syntactic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the sch"
W13-0101,S10-1009,0,0.0251657,"The scheme covers many of the most important elements and relations present in linguistic utterances, including verb-argument structure, optional adjuncts such as adverbials, clause embeddings, and the linkage between them. The scheme is supported by extensive typological crosslinguistic evidence and accords with the leading Cognitive Linguistics theories. 1 Introduction Syntactic annotation is used as scaffolding in a wide variety of NLP applications. Examples include Machine Translation (Yamada and Knight, 2001), Semantic Role Labeling (SRL) (Punyakanok et al., 2008) and Textual Entailment (Yuret et al., 2010). Syntactic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the schemes requires highly proficient workforce for its annotation. For example, the Penn Treebank project"
W13-0101,nissim-etal-2004-annotation,0,\N,Missing
W13-0101,W04-2705,0,\N,Missing
W13-0101,C08-2024,0,\N,Missing
W13-0101,W11-0908,0,\N,Missing
W13-0101,de-marneffe-etal-2006-generating,0,\N,Missing
W13-0101,N07-1071,0,\N,Missing
W13-0101,W10-4205,0,\N,Missing
W13-0101,alvez-etal-2008-complete,0,\N,Missing
W13-0101,N10-1011,0,\N,Missing
W13-0101,E12-1024,0,\N,Missing
W13-0101,fillmore-etal-2002-seeing,0,\N,Missing
W13-0101,N07-1021,0,\N,Missing
W13-0101,D10-1115,0,\N,Missing
W13-0101,D12-1016,0,\N,Missing
W13-0101,W01-1315,0,\N,Missing
W13-0101,S10-1076,0,\N,Missing
W13-0101,E06-1040,0,\N,Missing
W13-0101,W97-0301,0,\N,Missing
W13-0101,N10-1138,0,\N,Missing
W13-0101,J97-1003,0,\N,Missing
W13-0101,W11-2507,0,\N,Missing
W13-0101,D12-1110,0,\N,Missing
W13-0101,D10-1089,0,\N,Missing
W13-0101,C10-1018,0,\N,Missing
W13-0101,W11-2805,0,\N,Missing
W13-0101,W01-1311,0,\N,Missing
W13-0101,W97-1301,0,\N,Missing
W13-0101,D08-1094,0,\N,Missing
W13-0101,N09-1041,0,\N,Missing
W13-0101,W02-1006,0,\N,Missing
W13-0101,W08-0606,0,\N,Missing
W13-0101,P86-1004,0,\N,Missing
W13-0101,H86-1011,0,\N,Missing
W13-0101,C08-1107,0,\N,Missing
W13-0101,J05-3002,0,\N,Missing
W13-0101,W08-2205,0,\N,Missing
W13-0101,W08-2222,0,\N,Missing
W13-0101,W11-0114,0,\N,Missing
W13-0101,J12-1002,0,\N,Missing
W13-0101,D09-1143,0,\N,Missing
W13-0101,P05-1053,0,\N,Missing
W13-0101,N01-1021,0,\N,Missing
W13-0101,S10-1008,0,\N,Missing
W13-0101,W03-1016,0,\N,Missing
W13-0101,C02-1151,0,\N,Missing
W13-0101,C92-2082,0,\N,Missing
W13-0101,C02-1114,0,\N,Missing
W13-0101,W08-1105,0,\N,Missing
W13-0101,P92-1017,0,\N,Missing
W13-0101,H92-1024,0,\N,Missing
W13-0101,J03-4003,0,\N,Missing
W13-0101,W08-1301,0,\N,Missing
W13-0101,D07-1109,0,\N,Missing
W13-0101,C08-1108,0,\N,Missing
W13-0101,P03-1054,0,\N,Missing
W13-0101,S12-1047,0,\N,Missing
W13-0101,W09-2415,0,\N,Missing
W13-0101,P04-1072,0,\N,Missing
W13-0101,P02-1040,0,\N,Missing
W13-0101,P11-1062,0,\N,Missing
W13-0101,W09-3941,0,\N,Missing
W13-0101,P04-1015,0,\N,Missing
W13-0101,P98-1046,0,\N,Missing
W13-0101,C98-1046,0,\N,Missing
W13-0101,J95-1001,0,\N,Missing
W13-0101,P98-1116,0,\N,Missing
W13-0101,C98-1112,0,\N,Missing
W13-0101,P10-1160,0,\N,Missing
W13-0101,P09-1077,0,\N,Missing
W13-0101,J06-3003,0,\N,Missing
W13-0101,D07-1071,0,\N,Missing
W13-0101,P11-1154,0,\N,Missing
W13-0101,D11-1123,0,\N,Missing
W13-0101,P03-1017,0,\N,Missing
W13-0101,C98-1013,0,\N,Missing
W13-0101,J05-3004,0,\N,Missing
W13-0101,P06-1038,1,\N,Missing
W13-0101,P98-2241,0,\N,Missing
W13-0101,C98-2236,0,\N,Missing
W13-0101,W13-0112,0,\N,Missing
W13-0101,S10-1065,0,\N,Missing
W13-0101,C04-1185,0,\N,Missing
W13-0101,P04-1054,0,\N,Missing
W13-0101,P05-3027,0,\N,Missing
W13-0101,J95-2003,0,\N,Missing
W13-0101,P06-1095,0,\N,Missing
W13-0101,J10-4006,0,\N,Missing
W13-0101,J98-1004,0,\N,Missing
W13-0101,D11-1024,0,\N,Missing
W13-0101,P08-1118,0,\N,Missing
W13-0101,P08-1028,0,\N,Missing
W13-0101,P12-1015,0,\N,Missing
W13-0101,P05-3021,0,\N,Missing
W13-0101,P04-1011,0,\N,Missing
W13-0101,P87-1022,0,\N,Missing
W13-0101,J04-1003,0,\N,Missing
W13-0101,S10-1010,0,\N,Missing
W13-0101,J08-1001,0,\N,Missing
W13-0101,S10-1059,0,\N,Missing
W13-0101,P10-1005,0,\N,Missing
W13-0101,J12-4003,0,\N,Missing
W13-0101,P10-1045,0,\N,Missing
W13-0101,P09-1046,0,\N,Missing
W13-0101,J02-3001,0,\N,Missing
W13-0101,P08-1090,0,\N,Missing
W13-0101,R11-1037,0,\N,Missing
W13-0101,P09-1068,0,\N,Missing
W13-0101,D08-1083,0,\N,Missing
W13-0101,P10-1143,0,\N,Missing
W13-0101,P98-2182,0,\N,Missing
W13-0101,C98-2177,0,\N,Missing
W13-0101,P08-2045,0,\N,Missing
W13-0101,P10-1100,0,\N,Missing
W13-0101,N10-1013,0,\N,Missing
W13-0101,P98-2127,0,\N,Missing
W13-0101,C98-2122,0,\N,Missing
W13-0101,prasad-etal-2008-penn,0,\N,Missing
W13-0101,P87-1021,0,\N,Missing
W13-0101,J86-3001,0,\N,Missing
W13-0101,P10-1044,0,\N,Missing
W13-0101,D11-1050,0,\N,Missing
W13-0101,P04-1019,0,\N,Missing
W13-0101,P98-2143,0,\N,Missing
W13-0101,C98-2138,0,\N,Missing
W13-0101,E06-1015,0,\N,Missing
W13-0101,roberts-etal-2012-annotating,0,\N,Missing
W13-0101,S12-1048,0,\N,Missing
W13-0101,S12-1030,0,\N,Missing
W13-0101,costa-branco-2012-timebankpt,0,\N,Missing
W13-0101,W10-2805,0,\N,Missing
W13-0101,R11-1046,0,\N,Missing
W13-0101,I11-1127,0,\N,Missing
W13-0101,mendes-etal-2012-dbpedia,0,\N,Missing
W13-0101,R11-1004,0,\N,Missing
W13-0101,W11-2819,0,\N,Missing
W13-0101,W11-0144,0,\N,Missing
W13-0101,islam-inkpen-2006-second,0,\N,Missing
W13-0101,W03-2605,0,\N,Missing
W13-0101,W93-0102,0,\N,Missing
W13-0101,W95-0103,0,\N,Missing
W13-0101,D11-1016,0,\N,Missing
W13-0101,W09-4303,0,\N,Missing
W13-0101,mani-etal-2008-spatialml,0,\N,Missing
W13-0101,D10-1113,0,\N,Missing
W13-0101,S12-1001,0,\N,Missing
W13-0101,D11-1014,0,\N,Missing
W13-0101,N12-1065,0,\N,Missing
W13-0101,D10-1048,0,\N,Missing
W13-0101,W07-2316,0,\N,Missing
W13-0101,W06-1623,0,\N,Missing
W13-0101,D07-1076,0,\N,Missing
W13-0101,P11-1053,0,\N,Missing
W13-0101,P11-1056,0,\N,Missing
W13-0101,H05-1091,0,\N,Missing
W13-0101,S12-1056,0,\N,Missing
W13-0101,S12-1055,0,\N,Missing
W15-3502,W13-0101,1,0.89651,"n-corresponding units in the two languages according to various parameters, and show that many of them are due to ambiguity or semantic changes. These results offer a better understanding of UCCA’s stability and suggest paths for further improvements. translations is seldom addressed and has yet to be adequately supported (see Section 2), a gap we address in this paper using a detailed analysis of a semantically annotated parallel corpus. Universal Cognitive Conceptual Annotation (UCCA) is a coarse-grained semantic annotation scheme which builds on typological and cognitive linguistic theory (Abend and Rappoport, 2013a; Abend and Rappoport, 2013b). The scheme aims to be applicable cross-linguistically, to abstract away from specific syntactic forms and to directly represent semantic distinctions. These properties make UCCA an appealing source of structural annotation which is cross-linguistically stable. We give an overview of UCCA in Section 3. This paper focuses on the case study of EnglishFrench, a well studied language pair in MT. We demonstrate through this language pair both UCCA’s portability, namely its ability to be applied to different languages, and its stability, namely its ability to preserve"
W15-3502,dorr-etal-2002-duster,0,0.0497329,"fore disregarded in this work. We find that even for French-specific phenomena, current UCCA categories permit their annotation in the foundational layer without requiring changes in the definitions or additional categories. Due to space limitations, we only present here one case of interest. The full analysis according to the grammar book can be found in Sulem (2014) (Ap4.2 Stability Overcoming cross-linguistic divergences (or translation divergences) is one of the main challenges in machine translation. We briefly review the main examples of translation divergences presented in (Dorr, 1994; Dorr et al., 2002; Dorr et al., 2004), adapting the original English-Spanish examples to English-French analogues. Then, for each example, we present its annotation according 3 www.cs.huji.ac.il/˜eliors/papers/ elior_sulem_thesis.pdf 14 glish example contains a Process (“to run”) and a Participant (“in”). The annotation in French is somewhat different, where “entrer” (“enter”) is a Process, while “en courant” (“running”) is an Adverbial. to UCCA. The resulting annotations show that UCCA abstracts away from almost all of these divergences and exposes the semantic similarity, demonstrating the stability of the s"
W15-3502,P13-1023,1,0.89616,"n-corresponding units in the two languages according to various parameters, and show that many of them are due to ambiguity or semantic changes. These results offer a better understanding of UCCA’s stability and suggest paths for further improvements. translations is seldom addressed and has yet to be adequately supported (see Section 2), a gap we address in this paper using a detailed analysis of a semantically annotated parallel corpus. Universal Cognitive Conceptual Annotation (UCCA) is a coarse-grained semantic annotation scheme which builds on typological and cognitive linguistic theory (Abend and Rappoport, 2013a; Abend and Rappoport, 2013b). The scheme aims to be applicable cross-linguistically, to abstract away from specific syntactic forms and to directly represent semantic distinctions. These properties make UCCA an appealing source of structural annotation which is cross-linguistically stable. We give an overview of UCCA in Section 3. This paper focuses on the case study of EnglishFrench, a well studied language pair in MT. We demonstrate through this language pair both UCCA’s portability, namely its ability to be applied to different languages, and its stability, namely its ability to preserve"
W15-3502,P98-1013,0,0.108902,"Missing"
W15-3502,W13-2322,0,0.791641,"s. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT) systems. Here, we examine the potential of semantic structures. While semantic annotation is appealing as a source of cross-linguistically stable structures, little has been accomplished in demonstrating this stabili"
W15-3502,P13-2074,0,0.0256835,"Missing"
W15-3502,P90-1017,0,0.626931,"ans la maison” (“enter in the house”). In UCCA there is a Participant in both languages. Thematic divergence: Realization of verb arguments in syntactic configurations that reflect different thematic to syntactic mapping orders. For example, “I like this house” – “Cette maison me plaˆıt” (“this house pleases to me”). In UCCA there are two Participants in English as well as two Participants in French (“cette maison” / “this house” and “me” / “me”). Promotional/Demotional divergence: Promotion is the case where a modifier in the source language is promoted to a main verb in the target language (Dorr, 1990; Gola, 2012). Demotion is its mirror image, where a main verb in the source language becomes a modifier in the target language. An example where an English adverb is promoted to a main verb is the French: “John usually goes home” – “John a l’habitude de rentrer a` la maison” (“John has the habit to go home”). In UCCA, both “usually” and “a l’habitude” (“has the habit”) are annotated as Adverbials. An example where an English verb is demoted to an adverb is the French “to run in” – “entrer en courant” (“enter running”). In UCCA, the EnTo summarize, aside from the case of demotional divergence,"
W15-3502,J94-4004,0,0.756645,"ss Translations: A French-English Case Study Elior Sulem Omri Abend Ari Rappoport Institute of Computer Science School of Informatics Institute of Computer Science Hebrew University of Jerusalem University of Edinburgh Hebrew University of Jerusalem eliors@cs.huji.ac.il oabend@inf.ed.ac.uk arir@cs.huji.ac.il Abstract models are effective at improving reordering at the phrase level, they are limited in their ability to map between arbitrarily divergent structures. Crosslinguistic divergences therefore pose a difficult problem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based system"
W15-3502,Y09-2025,0,0.0492529,"Missing"
W15-3502,W13-2203,0,0.141644,"Missing"
W15-3502,C12-1053,0,0.0452236,"Missing"
W15-3502,P14-1134,0,0.0517551,"Missing"
W15-3502,2007.tmi-papers.10,0,0.0706023,"Missing"
W15-3502,P05-1033,0,0.011631,"ysis of the major French grammatical phenomena. Second, we annotate a parallel English-French corpus with UCCA, and quantify the similarity of the structures on both sides. Results show a high degree of stability across translations, supporting the usage of semantic annotations over syntactic ones in structure-aware MT systems. 1 Introduction Structural information, be it syntactic or semantic, has the potential to address long-standing problems in Statistical Machine Translation (SMT), such as phrase-level (rather than word-level) reordering and discontiguous phrases. Structureaware models1 (Chiang, 2005; Liu et al., 2006; Mi et al., 2008) aim to address these and other problems by taking into account the hierarchical structure of language. However, while structure-aware 1 We use the term “structure-aware” rather than “syntaxbased” so to include any type of hierarchical structure. 11 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 11–22, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Scenes (a similar notion to a “frame”; see Section 3) in both languages have a correspondent in the other. We analyze the non-correspond"
W15-3502,W04-1513,0,0.0414053,"ons: A French-English Case Study Elior Sulem Omri Abend Ari Rappoport Institute of Computer Science School of Informatics Institute of Computer Science Hebrew University of Jerusalem University of Edinburgh Hebrew University of Jerusalem eliors@cs.huji.ac.il oabend@inf.ed.ac.uk arir@cs.huji.ac.il Abstract models are effective at improving reordering at the phrase level, they are limited in their ability to map between arbitrarily divergent structures. Crosslinguistic divergences therefore pose a difficult problem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-lingu"
W15-3502,D11-1067,0,0.019995,"Missing"
W15-3502,C12-1083,0,0.0279686,"nguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT) systems. Here, we examine the potential of semantic structures. While semantic annotation is appealing as a source of cross-linguistically stable structures, little has been accomplished in demonstrating this stability through a detailed corpus study. In this paper, we experiment with the UCCA conceptual-cognitive annotation scheme in an English-French case study. First, we show that UCCA can be"
W15-3502,P03-1054,0,0.00769169,"anova et al., 2003). We compute the number of verbs in the parallel corpus and compare them to the number of Scenes. We exclude auxiliaries since such verbs tend to differ considerably between languages. We manually correct the tagging (by a single annotator, highly proficient in both languages), and therefore expect these numbers to be comparable in quality to a gold standard7 . The syntactic constituents we study are noun phrases (NP), prepositional phrases (PP) and adverb phrases (ADVP in English and AdP in French). We used the Stanford parser’s pretrained models for English (englishPCFG, (Klein and Manning, 2003)) and French (the frenchFactored (Green et al., 2011)), with the same manual tokenization taken from the UCCA annotation. Six passages which contain very long sentences in French and for which the parser was unable to produce a parse were omitted from this evaluation. We note that we include in our analysis Scenes marked as unanalyzable (For example: “Hello!”), but exclude Scenes appearing as remote Participants, so to avoid double counting. In order to correct for possible biases of the parsers towards overprediction or underprediction of certain syntactic constituents, we conduct the followi"
W15-3502,P13-1117,0,0.0599532,"nslation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitute"
W15-3502,N03-1033,0,0.005738,"= i ni . The F-score F is the harmonic mean of P and R. This measure provides an upper bound of the number of aligned units in the two languages, looking at the category of the units and their appearance in aligned passages. We note that the measures described are more applicable in this context than statistical correlation measures (e.g., the Pearson correlation coefficient). This is because a stable scheme is determined by the similarity of the count vectors in absolute terms, rather than their statistical correlation. Experimental setup. For tagging, we use the Stanford POS tagger package (Toutanova et al., 2003). We compute the number of verbs in the parallel corpus and compare them to the number of Scenes. We exclude auxiliaries since such verbs tend to differ considerably between languages. We manually correct the tagging (by a single annotator, highly proficient in both languages), and therefore expect these numbers to be comparable in quality to a gold standard7 . The syntactic constituents we study are noun phrases (NP), prepositional phrases (PP) and adverb phrases (ADVP in English and AdP in French). We used the Stanford parser’s pretrained models for English (englishPCFG, (Klein and Manning,"
W15-3502,J13-4007,0,0.0570847,"cture modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT) systems. Here, we examine the potential of semantic"
W15-3502,1987.mtsummit-1.10,0,0.876098,"Missing"
W15-3502,D13-1064,0,0.0153984,"likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT)"
W15-3502,W15-1613,0,0.0380493,"Missing"
W15-3502,N13-1060,0,0.0254615,"Missing"
W15-3502,W10-1814,0,0.137732,"Missing"
W15-3502,C10-1081,0,0.0208106,"Missing"
W15-3502,P06-1077,0,0.0232994,"jor French grammatical phenomena. Second, we annotate a parallel English-French corpus with UCCA, and quantify the similarity of the structures on both sides. Results show a high degree of stability across translations, supporting the usage of semantic annotations over syntactic ones in structure-aware MT systems. 1 Introduction Structural information, be it syntactic or semantic, has the potential to address long-standing problems in Statistical Machine Translation (SMT), such as phrase-level (rather than word-level) reordering and discontiguous phrases. Structureaware models1 (Chiang, 2005; Liu et al., 2006; Mi et al., 2008) aim to address these and other problems by taking into account the hierarchical structure of language. However, while structure-aware 1 We use the term “structure-aware” rather than “syntaxbased” so to include any type of hierarchical structure. 11 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 11–22, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Scenes (a similar notion to a “frame”; see Section 3) in both languages have a correspondent in the other. We analyze the non-corresponding units in the t"
W15-3502,W06-1606,0,0.0249226,"roblem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent exampl"
W15-3502,P08-1023,0,0.0224531,"ical phenomena. Second, we annotate a parallel English-French corpus with UCCA, and quantify the similarity of the structures on both sides. Results show a high degree of stability across translations, supporting the usage of semantic annotations over syntactic ones in structure-aware MT systems. 1 Introduction Structural information, be it syntactic or semantic, has the potential to address long-standing problems in Statistical Machine Translation (SMT), such as phrase-level (rather than word-level) reordering and discontiguous phrases. Structureaware models1 (Chiang, 2005; Liu et al., 2006; Mi et al., 2008) aim to address these and other problems by taking into account the hierarchical structure of language. However, while structure-aware 1 We use the term “structure-aware” rather than “syntaxbased” so to include any type of hierarchical structure. 11 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 11–22, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Scenes (a similar notion to a “frame”; see Section 3) in both languages have a correspondent in the other. We analyze the non-corresponding units in the two languages accor"
W15-3502,N09-2004,0,0.0396594,"Missing"
W15-3502,J05-1004,0,0.125686,"essed the portability of semantic annotation schemes, namely whether the same scheme, often originally developed for English, can be applied to other languages. Burchardt et al. (2009) addressed the application of the English FrameNet (Baker et al., 1998) to German. They found that about a third of the verb senses identified in the German corpus were not covered by FrameNet. Their analysis further revealed that the English category set is not always sufficient, resulting in the introduction of a new category for German. Van der Plas et al. (2010) addressed the application of English PropBank (Palmer et al., 2005) to French, and found that while the scheme can be applied to French, the annotation requires proficiency in both languages. Samardzic et al. (2010a; 2010b) also studied the portability of the English PropBank to French, and found that the overwhelming majority of the French verbal predicates in the corpus correspond to a verb sense in the PropBank lexicon. The portability of PropBank was also examined in the case of English-Chinese through the construction of annotated parallel corpora used in the OntoNotes project (Weischedel et al., 2012). Portability has also been studied in the context of"
W15-3502,P12-1095,0,0.036912,"Missing"
W15-3502,xue-etal-2014-interlingua,0,0.0606707,"Missing"
W15-3502,C12-1185,0,0.0269247,"Missing"
W15-3502,P08-1064,0,0.0265816,"ase Study Elior Sulem Omri Abend Ari Rappoport Institute of Computer Science School of Informatics Institute of Computer Science Hebrew University of Jerusalem University of Edinburgh Hebrew University of Jerusalem eliors@cs.huji.ac.il oabend@inf.ed.ac.uk arir@cs.huji.ac.il Abstract models are effective at improving reordering at the phrase level, they are limited in their ability to map between arbitrarily divergent structures. Crosslinguistic divergences therefore pose a difficult problem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Se"
W15-3502,C98-1013,0,\N,Missing
W19-3316,P13-1023,1,0.855673,"e subject cannot be ranked lower than the direct object (e.g., a subject construed as a T HEME cannot have a direct object construed as an AGENT). Indirect objects in the English double object construction4 are treated as R ECIPIENT construals. (18) I sent [John]R ECIPIENT↝R ECIPIENT a cake. (19) I sent a cake [to John]R ECIPIENT↝G OAL . (20) I baked [John]R ECIPIENT↝R ECIPIENT a cake. (21) I paid [John]R ECIPIENT↝R ECIPIENT [$10]C OST↝C OST . Interannotator Agreement Study Data. We piloted our guidelines using a sample of 100 scenes from the English UCCA-annotated Wiki corpus5 as detailed by Abend and Rappoport (2013). UCCA is a scheme for annotating coarsegrained predicate-argument structure such that syntactically varied paraphrases and translations should receive similar analyses. It captures both static and dynamic scenes and their participants, but does not mark semantic roles. Annotators. Four annotators (A, B, C, D), all authors of this paper, took part in this study. All are computational linguistics researchers. Datasets. Prior to development of guidelines for subjects and objects, one of the annotators (Annotator A) sampled 106 Wiki documents (44k tokens) and tagged all 10k instances of UCCA Part"
W19-3316,S17-1022,1,0.738698,"Missing"
W19-3316,J05-1004,0,0.198024,"positions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabulary predicates are encountered. But is a lexicon really necessary for role annotation? A general-purpose set of role labels with detailed criteria for each can potentially bypass coverage limitations of lexicon-based approaches, while still supporting some degree of generalization across grammatical paraphrases. Second, the nonreliance on a lexicon potentially simplifies the annotation process in some respects. For example, no explicit predicate disamb"
W19-3316,Q15-1034,0,0.0554848,"Missing"
W19-3316,P18-1018,1,0.601633,"orical semantic roles (Fillmore, 1968, 1982; Levin, 1993) or bundles of proto-properties (Dowty, 1991; Reisinger et al., 2015) that generalize across verbs. A parallel line of work (§2) has looked at the meanings coded by grammatical phrase-markers such as prepositions and possessives and how to disambiguate them. These inquiries necessarily overlap because many prepositions mark verb arguments or modifiers. Consequently, insights from the study of prepositions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabular"
