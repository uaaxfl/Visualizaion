K19-1084,Global Autoregressive Models for Data-Efficient Sequence Learning,2019,0,3,3,0,26371,tetiana parshakova,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Standard autoregressive seq2seq models are easily trained by max-likelihood, but tend to show poor results under small-data conditions. We introduce a class of seq2seq models, GAMs (Global Autoregressive Models), which combine an autoregressive component with a log-linear component, allowing the use of global \textit{a priori} features to compensate for lack of data. We train these models in two steps. In the first step, we obtain an \textit{unnormalized} GAM that maximizes the likelihood of the data, but is improper for fast inference or evaluation. In the second step, we use this GAM to train (by distillation) a second autoregressive model that approximates the \textit{normalized} distribution associated with the GAM, and can be used for fast inference and evaluation. Our experiments focus on language modelling under synthetic conditions and show a strong perplexity reduction of using the second autoregressive model over the standard one."
D19-5617,Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness,2019,27,0,3,0,9749,alexandre berard,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"We share a French-English parallel corpus of Foursquare restaurant reviews, and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where better-quality MT would be greatly beneficial. We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness. We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems. Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words."
W18-6555,Char2char Generation with Reranking for the {E}2{E} {NLG} Challenge,2018,24,3,2,0.833333,5964,shubham agarwal,Proceedings of the 11th International Conference on Natural Language Generation,0,"This paper describes our submission to the E2E NLG Challenge. Recently, neural seq2seq approaches have become mainstream in NLG, often resorting to pre- (respectively post-) processing \textit{delexicalization} (relexicalization) steps at the word-level to handle rare words. By contrast, we train a simple character level seq2seq model, which requires no pre/post-processing (delexicalization, tokenization or even lowercasing), with surprisingly good results. For further improvement, we explore two re-ranking approaches for scoring candidates. We also introduce a synthetic dataset creation procedure, which opens up a new way of creating artificial datasets for Natural Language Generation."
W17-5519,A surprisingly effective out-of-the-box char2char model on the {E}2{E} {NLG} Challenge dataset,2017,0,10,2,0.833333,5964,shubham agarwal,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"We train a char2char model on the E2E NLG Challenge data, by exploiting {``}out-of-the-box{''} the recently released tfseq2seq framework, using some of the standard options offered by this tool. With minimal effort, and in particular without delexicalization, tokenization or lowercasing, the obtained raw predictions, according to a small scale human evaluation, are excellent on the linguistic side and quite reasonable on the adequacy side, the primary downside being the possible omissions of semantic material. However, in a significant number of cases (more than 70{\%}), a perfect solution can be found in the top-20 predictions, indicating promising directions for solving the remaining issues."
W16-1611,{LSTM}-Based Mixture-of-Experts for Knowledge-Aware Dialogues,2016,11,8,2,0,21202,phong le,Proceedings of the 1st Workshop on Representation Learning for {NLP},0,"We introduce an LSTM-based method for dynamically integrating several word-prediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP."
S16-2019,Orthogonality regularizer for question answering,2016,13,0,3,0,24326,chunyang xiao,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,"Learning embeddings of words and knowledge base elements is a promising approach for open domain question answering. Based on the remark that relations and entities are distinct object types lying in the same embedding space, we analyze the benefit of adding a regularizer favoring the embeddings of entities to be orthogonal to those of relations. The main motivation comes from the observation that modifying the embeddings using prior knowledge often helps performance. The experiments show that incorporating the regularizer yields better results on a challenging question answering benchmark."
P16-1127,Sequence-based Structured Prediction for Semantic Parsing,2016,36,61,2,0,24326,chunyang xiao,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query. Building on recent work by (Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNN-based sequential predictor. Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints."
C16-1103,Natural Language Generation through Character-based {RNN}s with Finite-state Prior Knowledge,2016,10,10,2,0,35733,raghav goyal,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recently Wen et al. (2015) have proposed a Recurrent Neural Network (RNN) approach to the generation of utterances from dialog acts, and shown that although their model requires less effort to develop than a rule-based system, it is able to improve certain aspects of the utterances, in particular their naturalness. However their system employs generation at the word-level, which requires one to pre-process the data by substituting named entities with placeholders. This pre-processing prevents the model from handling some contextual effects and from managing multiple occurrences of the same attribute. Our approach uses a character-level model, which unlike the word-level model makes it possible to learn to {``}copy{''} information from the dialog act to the target without having to pre-process the input. In order to avoid generating non-words and inventing information not present in the input, we propose a method for incorporating prior knowledge into the RNN in the form of a weighted finite-state automaton over character sequences. Automatic and human evaluations show improved performance over baselines on several evaluation criteria."
D15-1233,Reversibility reconsidered: finite-state factors for efficient probabilistic sampling in parsing and generation,2015,22,0,1,1,26373,marc dymetman,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We restate the classical logical notion of generation/parsing reversibility in terms of feasible probabilistic sampling, and argue for an implementation based on finite-state factors. We propose a modular decomposition that reconciles generation accuracy with parsing robustness and allows the introduction of dynamic contextual factors. (Opinion Piece)"
2015.jeptalnrecital-court.11,Adaptation par enrichissement terminologique en traduction automatique statistique fond{\\'e}e sur la g{\\'e}n{\\'e}ration et le filtrage de bi-segments virtuels,2015,-1,-1,2,0,5281,christophe servan,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons des travaux pr{\'e}liminaires sur une approche permettant d{'}ajouter des termes bilingues {\`a} un syst{\`e}me de Traduction Automatique Statistique (TAS) {\`a} base de segments. Les termes sont non seulement inclus individuellement, mais aussi avec des contextes les englobant. Tout d{'}abord nous g{\'e}n{\'e}rons ces contextes en g{\'e}n{\'e}ralisant des motifs (ou patrons) observ{\'e}s pour des mots de m{\^e}me nature syntaxique dans un corpus bilingue. Enfin, nous filtrons les contextes qui n{'}atteignent pas un certain seuil de confiance, {\`a} l{'}aide d{'}une m{\'e}thode de s{\'e}lection de bi-segments inspir{\'e}e d{'}une approche de s{\'e}lection de donn{\'e}es, pr{\'e}c{\'e}demment appliqu{\'e}e {\`a} des textes bilingues align{\'e}s."
E14-2013,A Lightweight Terminology Verification Service for External Machine Translation Engines,2014,5,3,3,0,39852,alessio bosca,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose a demonstration of a domainspecific terminology checking service which works on top of any generic blackbox MT, and only requires access to a bilingual terminology resource in the domain. In cases where an incorrect translation of a source term was proposed by the generic MT service, our service locates the wrong translation of the term in the target and suggests a terminologically correct translation for this term."
D14-1131,Exact Decoding for Phrase-Based Statistical Machine Translation,2014,43,3,2,0.850169,5041,wilker aziz,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model. We replace this intractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model. Exact optimisation is achieved through a coarseto-fine strategy with connections to adaptive rejection sampling. We perform exact optimisation with unpruned language models of order 3 to 5 and show searcherror curves for beam search and cube pruning on standard test sets. This is the first work to tractably tackle exact optimisation with language models of orders higher than 3.
2014.amta-researchers.15,Comparison of data selection techniques for the translation of video lectures,2014,30,1,7,0,7150,joern wuebker,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"For the task of online translation of scientific video lectures, using huge models is not possible. In order to get smaller and efficient models, we perform data selection. In this paper, we perform a qualitative and quantitative comparison of several data selection techniques, based on cross-entropy and infrequent n-gram criteria. In terms of BLEU, a combination of translation and language model cross-entropy achieves the most stable results. As another important criterion for measuring translation quality in our application, we identify the number of out-of-vocabulary words. Here, infrequent n-gram recovery shows superior performance. Finally, we combine the two selection techniques in order to benefit from both their strengths."
W13-2260,Investigations in Exact Inference for Hierarchical Translation,2013,28,6,2,0.850169,5041,wilker aziz,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling. We also present a first implementation of that method along with experimental results shedding light on some fundamental issues. In hierarchical translation, inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model. We replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain. Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions. While the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework."
W13-1804,A Convexity-based Generalization of {V}iterbi for Non-Deterministic Weighted Automata,2013,12,0,1,1,26373,marc dymetman,Proceedings of the 11th International Conference on Finite State Methods and Natural Language Processing,0,"We propose a novel approach for the maxstring problem in acyclic nondeterministic weighted FSAxe2x80x99s, which is based on a convexity-related notion of domination among intermediary results, and which can be seen as a generalization of the usual dynamic programming technique for finding the max-path (a.k.a. Viterbi approximation) in such automata."
P13-4015,{SORT}: An Interactive Source-Rewriting Tool for Improved Translation,2013,17,2,3,0.588235,27123,shachar mirkin,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"The quality of automatic translation is affected by many factors. One is the divergence between the specific source and target languages. Another lies in the source text itself, as some texts are more complex than others. One way to handle such texts is to modify them prior to translation. Yet, an important factor that is often overlooked is the source translatability with respect to the specific translation system and the specific model that are being used. In this paper we present an interactive system where source modifications are induced by confidence estimates that are derived from the translation model in use. Modifications are automatically generated and proposed for the userxe2x80x99s approval. Such a system can reduce postediting effort, replacing it by cost-effective pre-editing that can be done by monolinguals."
2013.mtsummit-posters.8,Confidence-driven Rewriting for Improved Translation,2013,-1,-1,3,0.588235,27123,shachar mirkin,Proceedings of Machine Translation Summit XIV: Posters,0,None
W12-6106,Optimization and Sampling for {NLP} from a Unified Viewpoint,2012,24,4,1,1,26373,marc dymetman,Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology,0,"The OS* algorithm is a unified approach to exact optimization and sampling, based on incremental refinements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search. We first give a detailed description of OS*. We then explain how it can be applied to several NLP tasks, giving more details on two such applications: (i) decoding and sampling with a high-order HMM, and (ii) decoding and sampling with the intersection of a PCFG and a high-order LM."
W12-5701,Hybrid Adaptation of Named Entity Recognition for Statistical Machine Translation,2012,8,3,3,1,8115,vassilina nikoulina,Proceedings of the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid {MT},0,None
P12-1003,Prediction of Learning Curves in Machine Translation,2012,16,13,3,0,21982,prasanth kolachina,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Parallel data in the domain of interest is the key resource when training a statistical machine translation (SMT) system for a specific purpose. Since ad-hoc manual translation can represent a significant investment in time and money, a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful. In this work, we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data.n n We consider two scenarios, 1) Monolingual samples in the source and target languages are available and 2) An additional small amount of parallel corpus is also available. We propose methods for predicting learning curves in both these scenarios."
D12-1103,Exact Sampling and Decoding in High-Order Hidden {M}arkov Models,2012,17,10,2,0,42040,simon carter,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We present a method for exact optimization and sampling from high order Hidden Markov Models (HMMs), which are generally handled by approximation techniques. Motivated by adaptive rejection sampling and heuristic search, we propose a strategy based on sequentially refining a lower-order language model that is an upper bound on the true model we wish to decode and sample from. This allows us to build tractable variable-order HMMs. The ARPA format for language models is extended to enable an efficient use of the max-backoff quantities required to compute the upper bound. We evaluate our approach on two problems: a SMS-retrieval task and a POS tagging experiment using 5-gram models. Results show that the same approach can be used for exact optimization and sampling, while explicitly constructing only a fraction of the total implicit state-space."
W10-3801,Intersecting Hierarchical and Phrase-Based Models of Translation: Formal Aspects and Algorithms,2010,10,2,1,1,26373,marc dymetman,Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation,0,"We address the problem of constructing hybrid translation systems by intersecting a Hiero-style hierarchical system with a phrase-based system and present formal techniques for doing so. We model the phrase-based component by introducing a variant of weighted finite-state automata, called -automata, provide a self-contained description of a general algorithm for intersecting weighted synchronous context-free grammars with finite-state automata, and extend these constructs to -automata. We end by briefly discussing complexity properties of the presented algorithms."
specia-etal-2010-dataset,A Dataset for Assessing Machine Translation Evaluation Metrics,2010,20,21,3,0.714286,2509,lucia specia,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe a dataset containing 16,000 translations produced by four machine translation systems and manually annotated for quality by professional translators. This dataset can be used in a range of tasks assessing machine translation evaluation metrics, from basic correlation analysis to training and test of machine learning-based metrics. By providing a standard dataset for such tasks, we hope to encourage the development of better MT evaluation metrics."
2010.eamt-1.31,Learning an Expert from Human Annotations in Statistical Machine Translation: the Case of Out-of-Vocabulary Words,2010,-1,-1,2,0.324398,5041,wilker aziz,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,None
2010.amta-papers.31,Machine Translation Using Overlapping Alignments and {S}ample{R}ank,2010,33,42,3,0,2484,benjamin roth,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We present a conditional-random-field approach to discriminatively-trained phrase-based machine translation in which training and decoding are both cast in a sampling framework and are implemented uniformly in a new probabilistic programming language for factor graphs. In traditional phrase-based translation, decoding infers both a ``Viterbi'' alignment and the target sentence. In contrast, in our approach, a rich overlapping-phrase alignment is produced by a fast deterministic method, while probabilistic decoding infers only the target sentence, which is then able to leverage arbitrary features of the entire source sentence, target sentence and alignment. By using SampleRank for learning we could in principle efficiently estimate hundreds of thousands of parameters. Test-time decoding is done by MCMC sampling with annealing. To demonstrate the potential of our approach we show preliminary experiments leveraging alignments that may contain overlapping bi-phrases."
P09-1038,Phrase-Based Statistical Machine Translation as a Traveling Salesman Problem,2009,17,22,2,0,47214,mikhail zaslavskiy,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"An efficient decoding algorithm is a crucial element of any statistical machine translation system. Some researchers have noted certain similarities between SMT decoding and the famous Traveling Salesman Problem; in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task. In this paper, we focus on the reverse mapping, showing that any phrase-based SMT decoding problem can be directly reformulated as a TSP. The transformation is very natural, deepens our understanding of the decoding problem, and allows direct use of any of the powerful existing TSP solvers for SMT decoding. We test our approach on three datasets, and compare a TSP-based decoder to the popular beam-search algorithm. In all cases, our method provides competitive or better performance."
P09-1089,Source-Language Entailment Modeling for Translating Unknown Terms,2009,34,47,5,0.588235,27123,shachar mirkin,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,This paper addresses the task of handling unknown terms in SMT. We propose using source-language monolingual models and resources to paraphrase the source text prior to translation. We further present a conceptual extension to prior work by allowing translations of entailed texts rather than paraphrases only. A method for performing this process efficiently is presented and applied to some 2500 sentences with unknown terms. Our experiments show that the proposed approach substantially increases the number of properly translated texts.
2009.mtsummit-papers.17,Complexity-Based Phrase-Table Filtering for Statistical Machine Translation,2009,-1,-1,3,0,21324,nadi tomeh,Proceedings of Machine Translation Summit XII: Papers,0,None
2009.eamt-smart.10,Sentence-level confidence estimation for {MT},2009,-1,-1,3,0.714286,2509,lucia specia,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,None
2009.eamt-1.5,Estimating the Sentence-Level Quality of Machine Translation Systems,2009,18,145,5,0.714286,2509,lucia specia,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"We investigate the problem of predicting the quality of sentences produced by machine translation systems when reference translations are not available. The problem is addressed as a regression task and a method that takes into account the contribution of different features is proposed. We experiment with this method for translations produced by various MT systems and different language pairs, annotated with quality scores both automatically and manually. Results show that our method allows obtaining good estimates and that identifying a reduced set of relevant features plays an important role. The experiments also highlight a number of outstanding features that were consistently selected as the most relevant and could be used in different ways to improve MT performance or to enhance MT evaluation."
W08-0407,Experiments in Discriminating Phrase-Based Translations on the Basis of Syntactic Coupling Features,2008,8,4,2,1,8115,vassilina nikoulina,Proceedings of the {ACL}-08: {HLT} Second Workshop on Syntax and Structure in Statistical Translation ({SSST}-2),0,"We describe experiments on discriminating English to French phrase-based translations through the use of syntactic coupling features. Using a robust rule-based dependency parser, we parse both the English source and the French translation candidates from the nbest list returned by our phrase-based system; we compute for each candidate a number of coupling features, that is, values that depend on the amount of alignment between edges in the source and target structures, and discriminatively train the weights of these coupling features. We compare different feature combinations. Although the improvements in terms of automatic measures such as Bleu and Nist are inconclusive, an initial human assessment of the results appears to show certain qualitative improvements."
W08-0323,Using Syntactic Coupling Features for Discriminating Phrase-Based Translations ({WMT}-08 Shared Translation Task),2008,3,3,2,1,8115,vassilina nikoulina,Proceedings of the Third Workshop on Statistical Machine Translation,0,"Our participation in the shared translation task at WMT-08 focusses on news translation from English to French. Our main goal is to contrast a baseline version of the phrase-based MATRAX system, with a version that incorporates syntactic coupling features in order to discriminate translations produced by the baseline system. We report results comparing different feature combinations."
H05-1095,Translating with Non-contiguous Phrases,2005,19,80,4,0,5047,michel simard,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a phrase-based statistical machine translation method, based on non-contiguous phrases, i.e. phrases with gaps. A method for producing such phrases from a word-aligned corpora is proposed. A statistical translation model is also presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data."
2005.jeptalnrecital-long.24,Une approche {\\`a} la traduction automatique statistique par segments discontinus,2005,-1,-1,4,0,5047,michel simard,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente une m{\'e}thode de traduction automatique statistique bas{\'e}e sur des segments non-continus, c{'}est-{\`a}-dire des segments form{\'e}s de mots qui ne se pr{\'e}sentent pas n{\'e}c{\'e}ssairement de fa{\c{c}}on contigu{\""e} dans le texte. On propose une m{\'e}thode pour produire de tels segments {\`a} partir de corpus align{\'e}s au niveau des mots. On pr{\'e}sente {\'e}galement un mod{\`e}le de traduction statistique capable de tenir compte de tels segments, de m{\^e}me qu{'}une m{\'e}thode d{'}apprentissage des param{\`e}tres du mod{\`e}le visant {\`a} maximiser l{'}exactitude des traductions produites, telle que mesur{\'e}e avec la m{\'e}trique NIST. Les traductions optimales sont produites par le biais d{'}une recherche en faisceau. On pr{\'e}sente finalement des r{\'e}sultats exp{\'e}rimentaux, qui d{\'e}montrent comment la m{\'e}thode propos{\'e}e permet une meilleure g{\'e}n{\'e}ralisation {\`a} partir des donn{\'e}es d{'}entra{\^\i}nement."
P03-2017,Towards Interactive Text Understanding,2003,16,4,1,1,26373,marc dymetman,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,This position paper argues for an interactive approach to text understanding. The proposed model extends an existing semantics-based text authoring system by using the input text as a source of information to assist the user in re-authoring its content. The approach permits a reliable deep semantic analysis by combining automatic information extraction with a minimal amount of human intervention.
E03-2003,Controlled Authoring of Biological Experiment Reports,2003,7,2,2,0.925926,10313,caroline brun,Demonstrations,0,"We give a demonstration of an application of XRCE's controlled text authoring system MDA to biological experiment reports. This work is the result of a collaboration between XRCE's Document Content Models team, CNRS's Institut de Biologie Structurale, and Protein'eXpert, a company specialized in biotechnology based in Grenoble. We start with a brief presentation of the partners involved and their respective goals. We then give some technical background on the MDA system. Some novel features of the application are discussed, in particular how MDA can be used for integrating the formalization of an experimental protocol with its associated textual documentation."
2003.jeptalnrecital-poster.14,{MDA}-{XML} : une exp{\\'e}rience de r{\\'e}daction contr{\\^o}l{\\'e}e multilingue bas{\\'e}e sur {XML},2003,-1,-1,3,0,14084,guy lapalme,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Nous d{\'e}crivons dans cet article l{'}implantation d{'}un syst{\`e}me de r{\'e}daction contr{\^o}l{\'e}e multilingue dans un environnement XML. Avec ce syst{\`e}me, un auteur r{\'e}dige interactivement un texte se conformant {\`a} des r{\`e}gles de bonne formation aux niveaux du contenu s{\'e}mantique et de la r{\'e}alisation linguistique d{\'e}crites par un sch{\'e}ma XML. Nous discutons les avantages de cette approche ainsi que les difficult{\'e}s rencontr{\'e}es lors du d{\'e}veloppement de ce syst{\`e}me. Nous concluons avec un exemple d{'}application {\`a} une classe de documents pharmaceutiques."
C02-1128,"Text Authoring, Knowledge Acquisition and Description Logics",2002,10,7,1,1,26373,marc dymetman,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We present a principled approach to the problem of connecting a controlled document authoring system with a knowledge base. We start by describing closed-world authoring situations, in which the knowledge base is used for constraining the possible documents and orienting the user's selections. Then we move to open-world authoring situations in which, additionally, choices made during authoring are echoed back to the knowledge base. In this way the information implicitly encoded in a document becomes explicit in the knowledge base and can be re-exploited for simplifying the authoring of new documents. We show how a Datalog KB is sufficient for is the closed-world situation, while a Description Logic KB is better-adapted to the more complex open-world situation, All along, we pay special attention to logically sound solutions and to decidability issues in the different processes."
W00-1404,Document structure and multilingual authoring,2000,12,27,2,0.925926,10313,caroline brun,{INLG}{'}2000 Proceedings of the First International Conference on Natural Language Generation,0,"The use of XML-based authoring tools is swiftly becoming a standard in the world of technical documentation. An XML document is a mixture of structure (the tags) and surface (text between the tags). The structure reflects the choices made by the author during the top-down stepwise refinement of the document under control of a DTD grammar. These choices are typically choices of meaning which are independent of the language in which the document is rendered, and can be seen as a kind of interlingua for the class of documents which is modeled by the DTD. Based on this remark, we advocate a radicalization of XML authoring, where the semantic content of the document is accounted for exclusively in terms of choice structures, and where appropriate rendering/realization mechanisms are responsible for producing the surface, possibly in several languages simultaneously. In this view, XML authoring has strong connections to natural language generation and text authoring. We describe the IG (Interaction Grammar) formalism, an extension of DTD's which permits powerful linguistic manipulations, and show its application to the production of multilingual versions of a certain class of pharmaceutical documents."
C00-2149,Context-Free Grammar Rewriting and the Transfer of Packed Linguistic Representations,2000,10,3,1,1,26373,marc dymetman,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"We propose an algorithm for the trausfer of packed linguistic structures, that is, finite collections of labelled graphs which share certain subparts. A labelled graph is seen as a word over a vocabulary of description elements (nodes, arcs, labels), and a collection of graphs as a set of such words, that is, as a language over description elements. A packed representation for the collection of graphs is then viewed as a context-free grammar which generates such a language. We present an algorithm that uses a conventional set of transfer rules but is capable of rewriting the CFG representing the source packed structure into a CFG representing the target packed structure that preserves the compaction properties of the source CFG."
C00-1036,{XML} and Multilingual Document Authoring: Convergent Trends,2000,11,60,1,1,26373,marc dymetman,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Typical approaches to XML authoring view a XML document as a mixture of structure (the tags) and surface (text between the tags). We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms. This move is based on the view that the author's choices when authoring XML documents are best seen as language-neutral semantic decisions, that the structure can then be viewed as interlingual content, and that the textual output should be derived from this content by language-specific realization mechanisms, thus assimilating XML authoring to Multilingual Document Authoring. However, standard XML tools have important limitations when used for such a purpose: (1) they are weak at propagating semantic dependencies between different parts of the structure, and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units. We present two related proposals for overcoming these limitations: one (GF) originating in the tradition of mathematical proof editors and constructive type theory, the other (IG), a specialization of Definite Clause Grammars strongly inspired by GF."
P98-1057,Group Theory and Linguistic Processing,1998,5,1,1,1,26373,marc dymetman,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"There is currently much interest in bringing together the tradition of categorial grammar, and especially the Lambek calculus (Lambek, 1958), with the more recent paradigm of linear logic (Girard, 1987) to which it has strong ties. One active research area concerns the design of non-commutative versions of linear logic (Abrusci, 1991; Rdtor6, 1993) which can be sensitive to word order while retaining the hypothetical reasoning capabilities of standard (commutative) linear logic that make it so well-adapted to handling such phenomena as quantifier scoping (Dalrymple et al., 1995). Some connections between the Lambek calculus and group structure have long been known (van Benthem, 1986), and linear logic itself has some aspects strongly reminiscent of groups (the producer/consumer duality of a formula A with its linear negation Aa-), but no serious attempt has been made so far to base a theory of linguistic description solely on group structure. This paper presents such a model, G-grammars (for group grammars), and argues that:"
C98-1055,Group Theory and Linguistic Processing,1998,5,1,1,1,26373,marc dymetman,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"There is currently much interest in bringing together the tradition of categorial grammar, and especially the Lambek calculus (Lambek, 1958), with the more recent paradigm of linear logic (Girard, 1987) to which it has strong ties. One active research area concerns the design of non-commutative versions of linear logic (Abrusci, 1991; Rdtor6, 1993) which can be sensitive to word order while retaining the hypothetical reasoning capabilities of standard (commutative) linear logic that make it so well-adapted to handling such phenomena as quantifier scoping (Dalrymple et al., 1995). Some connections between the Lambek calculus and group structure have long been known (van Benthem, 1986), and linear logic itself has some aspects strongly reminiscent of groups (the producer/consumer duality of a formula A with its linear negation Aa-), but no serious attempt has been made so far to base a theory of linguistic description solely on group structure. This paper presents such a model, G-grammars (for group grammars), and argues that:"
C96-1044,Extended Dependency Structures and their Formal Interpretation,1996,10,5,1,1,26373,marc dymetman,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We describe two semantically-oriented dependency-structure formalisms, U-forms and S-forms. U-forms have been previously used in machine translation as interlingual representations, but without being provided with a formal interpretation. S-forms, which we introduce in this paper, are a scoped version of U-forms, and we define a compositional semantics mechanism for them. Two types of semantic composition are basic: complement incorporation and modifier incorporation. Binding of variables is done at the time of incorporation, permitting much flexibility in composition order and a simple account of the semantic effects of permuting several incorporations."
C94-2199,A Simple Transformation for Offline-Parsable Grammars and its Termination Properties,1994,10,2,1,1,26373,marc dymetman,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We present, in easily reproducible terms, a simple transformation for offline-parsable grammars which results in a provably terminating parsing program directly top-down interpretable in Prolog. The transformation consists in two steps: (1) removal of empty-productions, followed by: (2) left-recursion elimination. It is related both to left-corner parsing (where the grammar is compiled, rather than interpreted through a parsing program, and with the advantage of guaranteed termination in the presence of empty productions) and to the Generalized Greibach Normal Form for DCGs (with the advantage of implementation simplicity)."
1993.tmi-1.17,Translation Analysis and Translation Automation,1993,10,51,2,0.509363,33186,pierre isabelle,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"We argue that the concept of translation analysis provides a suitable foundation for a new generation of translation support tools. We show that pre-existing translations can be analyzed into a structured translation memory and describe our TransSearch bilingual concordancing system, which allows translators to harness such a memory. We claim that translation analyzers can help detect translation errors in draft translations and we present the results of an experiment on the detection of deceptive cognates conducted as part of our TransCheck project. Finally, we claim that translation analysis can facilitate the speech-to-text transcription of dictated translations and introduce our new TransTalk project."
C92-1057,A Generalized {G}reibach Normal Form for Definite Clause Grammars,1992,7,9,1,1,26373,marc dymetman,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"An arbitrary definite clause grammar can be transformed into a so-called Generalized Greibach Normal Form (GGNF), a generalization of the classical Greibach Normal Form (GNF) for context-free grammars.The normalized definite clause grammar is declaratively equivalent to the original definite clause grammar, that is, it assigns the same analyses to the same strings. Offline-parsability of the original grammar is reflected in an elementary textual property of the transformed grammar. When this property holds, a direct (top-down) Prolog implementation of the normalized grammar solves the parsing problem: all solutions are enumerated on backtracking and execution terminates.When specialized to the simpler case of context-free grammars, the GGNF provides a variant to the GNF, where the transformed context-free grammar not only generates the same strings as the original grammar, but also preserves their degrees of ambiguity (this last property does not hold for the GNF).The GGNF seems to be the first normal form result for DCGs. It provides an explicit factorization of the potential sources of undecidability for the parsing problem, and offers valuable insights on the computational structure of unification grammars in general."
W91-0104,"Inherently Reversible Grammars, Logic Programming and Computability",1991,-1,-1,1,1,26373,marc dymetman,Reversible Grammar in Natural Language Processing,0,None
C90-3017,A Symmetrical Approach to Parsing and Generation,1990,10,22,1,1,26373,marc dymetman,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"Lexical Grammars are a class of unification grammars which share a fixed rule component, for which there exists a simple left-recursion elimination transformation. The parsing and generation programs are seen as two dual non-left-recursive versions of the original grammar, and are implemented through a standard top-down Prolog interpreter. Formal criteria for termination are given as conditions on lexical entries: during parsing as well as during generation the processing of a lexical entry consumes some amount of a guide; the guide used for parsing is a list of words remaining to be analyzed, while the guide for generation is a list of the semantics of constituents waiting to be generated."
C88-1053,{CRITTER}: a translation system for agricultural market reports,1988,8,20,2,0.509363,33186,pierre isabelle,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The CRITTER system is being developed to translate agricultural market reports between English and French. It is based on a transfer model, and designed to be reversible. The source and target language texts are described by means of: a) a surface syntactic representation consisting of a tree annotated with feature structures, built by an extraposition grammar, and b) a semantic representation exhibiting predicate argument structures and constrained by type checking, built in parallel with the syntactic structure in compositional fashion. CRITTERS's implementation is still incomplete, but results obtained so far are promising."
1988.tmi-1.12,Reversible logic grammars for machine translation,1988,-1,-1,1,1,26373,marc dymetman,Proceedings of the Second Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
C86-1121,Two Approaches to Commonsense Inferencing for Discourse Analysis,1986,4,3,1,1,26373,marc dymetman,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"The dominant philosophy regarding the formalization of Commonsense Inferencing in the physical domain consists in the exploitation of the tarskian scheme axiomatization interpretation borrowed from mathematical logic. The commonsense postulates constitute the axiomatization, and the real world provides the model for this axiomatization.The observation of the effective activity of linguistic communication and of the commonsense inferencing processes which are involved in it show the unacceptability of this scheme.An alternative is proposed, where the notion of conceptual category plays a principal role, and where the principle of logical adequation of an axiomatization to a model is replaced by a notion of projection of a conceptual structure onto the observed reality."
