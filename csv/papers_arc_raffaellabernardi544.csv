2021.splurobonlp-1.3,Visually Grounded Follow-up Questions: a Dataset of Spatial Questions Which Require Dialogue History,2021,-1,-1,4,0,1050,tianai dong,Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics,0,"In this paper, we define and evaluate a methodology for extracting history-dependent spatial questions from visual dialogues. We say that a question is history-dependent if it requires (parts of) its dialogue history to be interpreted. We argue that some kinds of visual questions define a context upon which a follow-up spatial question relies. We call the question that restricts the context: trigger, and we call the spatial question that requires the trigger question to be answered: zoomer. We automatically extract different trigger and zoomer pairs based on the visual property that the questions rely on (e.g. color, number). We manually annotate the automatically extracted trigger and zoomer pairs to verify which zoomers require their trigger. We implement a simple baseline architecture based on a SOTA multimodal encoder. Our results reveal that there is much room for improvement for answering history-dependent questions."
2021.emnlp-main.736,Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy,2021,-1,-1,2,1,1051,alberto testoni,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Generating goal-oriented questions in Visual Dialogue tasks is a challenging and longstanding problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model{'}s conjecture about the referent. We take the GuessWhat?! game as a case-study. We show that dialogues generated by Confirm-it are more natural and effective than beam search decoding without re-ranking."
2021.eacl-main.178,The Interplay of Task Success and Dialogue Quality: An in-depth Evaluation in Task-Oriented Visual Dialogues,2021,-1,-1,2,1,1051,alberto testoni,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"When training a model on referential dialogue guessing games, the best model is usually chosen based on its task success. We show that in the popular end-to-end approach, this choice prevents the model from learning to generate linguistically richer dialogues, since the acquisition of language proficiency takes longer than learning the guessing task. By comparing models playing different games (GuessWhat, GuessWhich, and Mutual Friends), we show that this discrepancy is model- and task-agnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set."
2021.acl-srw.11,{``}{I}{'}ve Seen Things You People Wouldn{'}t Believe{''}: Hallucinating Entities in {G}uess{W}hat?!,2021,-1,-1,2,1,1051,alberto testoni,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Natural language generation systems have witnessed important progress in the last years, but they are shown to generate tokens that are unrelated to the source input. This problem affects computational models in many NLP tasks, and it is particularly unpleasant in multimodal systems. In this work, we assess the rate of object hallucination in multimodal conversational agents playing the GuessWhat?! referential game. Better visual processing has been shown to mitigate this issue in image captioning; hence, we adapt to the GuessWhat?! task the best visual processing models at disposal, and propose two new models to play the Questioner agent. We show that the new models generate few hallucinations compared to other renowned models available in the literature. Moreover, their hallucinations are less severe (affect task-accuracy less) and are more human-like. We also analyse where hallucinations tend to occur more often through the dialogue: hallucinations are less frequent in earlier turns, cause a cascade hallucination effect, and are often preceded by negative answers, which have been shown to be harder to ground."
2020.winlp-1.9,Effective questions in referential visual dialogue,2020,-1,-1,3,0,2544,mauricio mazuecos,Proceedings of the The Fourth Widening Natural Language Processing Workshop,0,"An interesting challenge for situated dialogue systems is referential visual dialog: by asking questions, the system has to identify the referent to which the user refers to. Task success is the standard metric used to evaluate these systems. However, it does not consider how effective each question is, that is how much each question contributes to the goal. We propose a new metric, that measures question effectiveness. As a preliminary study, we report the new metric for state of the art publicly available models on GuessWhat?!. Surprisingly, successful dialogues do not have a higher percentage of effective questions than failed dialogues. This suggests that a system with high task success is not necessarily one that generates good questions."
2020.splu-1.4,They Are Not All Alike: Answering Different Spatial Questions Requires Different Grounding Strategies,2020,-1,-1,7,1,1051,alberto testoni,Proceedings of the Third International Workshop on Spatial Language Understanding,0,"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70{\%} on spatial questions and 6.27{\%} overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions."
2020.findings-emnlp.248,{B}e {D}ifferent to {B}e {B}etter! {A} {B}enchmark to {L}everage the {C}omplementarity of {L}anguage and {V}ision,2020,-1,-1,5,1,2491,sandro pezzelle,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitable for virtually any language and vision tasks. However, current approaches often require them to combine redundant information provided by language and vision. Inspired by real-life communicative contexts, we propose a novel task where either modality is necessary but not sufficient to make a correct prediction. To do so, we first build a dataset of images and corresponding sentences provided by human participants. Second, we evaluate state-of-the-art models and compare their performance against human speakers. We show that, while the task is relatively easy for humans, best-performing models struggle to achieve similar results."
2020.alvr-1.4,On the role of effective and referring questions in {G}uess{W}hat?!,2020,-1,-1,3,0,2544,mauricio mazuecos,Proceedings of the First Workshop on Advances in Language and Vision Research,0,"Task success is the standard metric used to evaluate referential visual dialogue systems. In this paper we propose two new metrics that evaluate how each question contributes to the goal. First, we measure how effective each question is by evaluating whether the question discards objects that are not the referent. Second, we define referring questions as those that univocally identify one object in the image. We report the new metrics for human dialogues and for state of the art publicly available models on GuessWhat?!. Regarding our first metric, we find that successful dialogues do not have a higher percentage of effective questions for most models. With respect to the second metric, humans make questions at the end of the dialogue that are referring, confirming their guess before guessing. Human dialogues that use this strategy have a higher task success but models do not seem to learn it."
W19-2912,Quantifiers in a Multimodal World: Hallucinating Vision with Language and Sound,2019,-1,-1,3,1,1051,alberto testoni,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"Inspired by the literature on multisensory integration, we develop a computational model to ground quantifiers in perception. The model learns to pick, out of nine quantifiers ({`}few{'}, {`}many{'}, {`}all{'}, etc.), the one that is more likely to describe the percent of animals in a visual-auditory input containing both animals and artifacts. We show that relying on concurrent sensory inputs increases model performance on the quantification task. Moreover, we evaluate the model in a situation in which only the auditory modality is given, while the visual one is {`}hallucinanted{'} either from the auditory input itself or from a linguistic caption describing the quantity of entities in the auditory input. This way, the model exploits prior associations between modalities. We show that the model profits from the prior knowledge and outperforms the auditory-only setting."
W19-0418,Evaluating the Representational Hub of Language and Vision Models,2019,0,0,4,1,6079,ravi shekhar,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"The multimodal models used in the emerging field at the intersection of computational linguistics and computer vision implement the bottom-up processing of the {``}Hub and Spoke{''} architecture proposed in cognitive science to represent how the brain processes and combines multi-sensory inputs. In particular, the Hub is implemented as a neural network encoder. We investigate the effect on this encoder of various vision-and-language tasks proposed in the literature: visual question answering, visual reference resolution, and visually grounded dialogue. To measure the quality of the representations learned by the encoder, we use two kinds of analyses. First, we evaluate the encoder pre-trained on the different vision-and-language tasks on an existing {``}diagnostic task{''} designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities."
P19-1350,Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering,2019,18,0,4,1,14563,claudio greco,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree."
N19-1265,"Beyond task success: A closer look at jointly learning to see, ask, and {G}uess{W}hat",2019,0,3,6,1,6079,ravi shekhar,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We propose a grounded dialogue state encoder which addresses a foundational issue on how to integrate visual grounding with dialogue system components. As a test-bed, we focus on the GuessWhat?! game, a two-player game where the goal is to identify an object in a complex visual scene by asking a sequence of yes/no questions. Our visually-grounded encoder leverages synergies between guessing and asking questions, as it is trained jointly using multi-task learning. We further enrich our model via a cooperative learning regime. We show that the introduction of both the joint architecture and cooperative learning lead to accuracy improvements over the baseline system. We compare our approach to an alternative system which extends the baseline with reinforcement learning. Our in-depth analysis shows that the linguistic skills of the two models differ dramatically, despite approaching comparable performance levels. This points at the importance of analyzing the linguistic output of competing systems beyond numeric comparison solely based on task success."
P18-2019,Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in Predicting Quantifiers,2018,9,0,3,1,2491,sandro pezzelle,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We study the role of linguistic context in predicting quantifiers ({`}few{'}, {`}all{'}). We collect crowdsourced data from human participants and test various models in a local (single-sentence) and a global context (multi-sentence) condition. Models significantly out-perform humans in the former setting and are only slightly better in the latter. While human performance improves with more linguistic context (especially on proportional quantifiers), model performance suffers. Models are very effective in exploiting lexical and morpho-syntactic patterns; humans are better at genuinely understanding the meaning of the (global) context."
N18-1039,"Comparatives, Quantifiers, Proportions: a Multi-Task Model for the Learning of Quantities from Vision",2018,0,3,3,1,2491,sandro pezzelle,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"The present work investigates whether different quantification mechanisms (set comparison, vague quantification, and proportional estimation) can be jointly learned from visual scenes by a multi-task computational model. The motivation is that, in humans, these processes underlie the same cognitive, non-symbolic ability, which allows an automatic estimation and comparison of set magnitudes. We show that when information about lower-complexity tasks is available, the higher-level proportional task becomes more accurate than when performed in isolation. Moreover, the multi-task model is able to generalize to unseen combinations of target/non-target objects. Consistently with behavioral evidence showing the interference of absolute number in the proportional task, the multi-task model no longer works when asked to provide the number of target objects in the scene."
C18-1104,Ask No More: Deciding when to guess in referential visual dialogue,2018,29,1,5,1,6079,ravi shekhar,Proceedings of the 27th International Conference on Computational Linguistics,0,"Our goal is to explore how the abilities brought in by a dialogue manager can be included in end-to-end visually grounded conversational agents. We make initial steps towards this general goal by augmenting a task-oriented visual dialogue model with a decision-making component that decides whether to ask a follow-up question to identify a target referent in an image, or to stop the conversation to make a guess. Our analyses show that adding a decision making component produces dialogues that are less repetitive and that include fewer unnecessary questions, thus potentially leading to more efficient and less unnatural interactions."
C18-1199,Grounded Textual Entailment,2018,39,5,8,0,30853,hoa vu,Proceedings of the 27th International Conference on Computational Linguistics,0,"Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant {``}world{''} or {``}situation{''}). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare {``}blind{''} and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing {``}grounding{''} in an optimal fashion."
W17-6938,Vision and Language Integration: Moving beyond Objects,2017,10,2,6,1,6079,ravi shekhar,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-6939,Can You See the (Linguistic) Difference? Exploring Mass/Count Distinction in Vision,2017,17,0,5,0,946,david smith,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
P17-1024,{FOIL} it! Find One mismatch between Image and Language caption,2017,26,19,7,1,6079,ravi shekhar,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and {`}foil{'} captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake ({`}foil word{'}). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image."
E17-2054,Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision,2017,9,1,3,1,2491,sandro pezzelle,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a {`}fuzzy{'} measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided."
W16-3208,Building a Bagpipe with a Bag and a Pipe: Exploring Conceptual Combination in Vision,2016,13,4,3,1,2491,sandro pezzelle,Proceedings of the 5th Workshop on Vision and Language,0,None
W16-3211,"{``}Look, some Green Circles!{''}: Learning to Quantify from Images",2016,13,7,6,0,33012,ionut sorodoc,Proceedings of the 5th Workshop on Vision and Language,0,"In this paper, we investigate whether a neural network model can learn the meaning of natural language quantifiers (no, some and all) from their use in visual contexts. We show that memory networks perform well in this task, and that explicit counting is not necessary to the systemxe2x80x99s performance, supporting psycholinguistic evidence on the acquisition of quantifiers."
P16-1144,The {LAMBADA} dataset: Word prediction requiring a broad discourse context,2016,15,8,5,0,15539,denis paperno,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text."
J16-4003,"There Is No Logical Negation Here, But There Are Alternatives: Modeling Conversational Negation with Distributional Semantics",2016,55,5,3,0,24892,german kruszewski,Computational Linguistics,0,"Logical negation is a challenge for distributional semantics, because predicates and their negations tend to occur in very similar contexts, and consequently their distributional vectors are very similar. Indeed, it is not even clear what properties a negated distributional vector should possess. However, when linguistic negation is considered in its actual discourse usage, it often performs a role that is quite different from straightforward logical negation. If someone states, in the middle of a conversation, that This is not a dog, the negation strongly suggests a restricted set of alternative predicates that might hold true of the object being talked about. In particular, other canids and middle-sized mammals are plausible alternatives, birds are less likely, skyscrapers and other large buildings virtually impossible. Conversational negation acts like a graded similarity function, of the sort that distributional semantics might be good at capturing. In this article, we introduce a large data set of alternative plausibility ratings for conversationally negated nominal predicates, and we show that simple similarity in distributional semantic space provides an excellent fit to subject data. On the one hand, this fills a gap in the literature on conversational negation, proposing distributional semantics as the right tool to make explicit predictions about potential alternatives of negated predicates. On the other hand, the results suggest that negation, when addressed from a broader pragmatic perspective, far from being a nuisance, is an ideal application domain for distributional semantic methods."
W15-2712,Distributional Semantics in Use,2015,43,3,1,1,1053,raffaella bernardi,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"In this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. Distributional semantic models are very attractive models of meaning mainly because they capture conceptual aspects and are automatically induced from natural language data. However, they need to be extended in order to account for language use in a discourse or dialogue context. We discuss phenomena that the new generation of distributional semantic models should capture, and propose concrete tasks on which they could be tested."
W14-5403,{TUHOI}: {T}rento Universal Human Object Interaction Dataset,2014,13,18,3,1,10457,dieuthu le,Proceedings of the Third Workshop on Vision and Language,0,"This paper describes the Trento Universal Human Object Interaction dataset, TUHOI, which is dedicated to human object interactions in images.1 Recognizing human actions is an important yet challenging task. Most available datasets in this field are limited in numbers of actions and objects. A large dataset with various actions and human object interactions is needed for training and evaluating complicated and robust human action recognition systems, especially systems that combine knowledge learned from language and vision. We introduce an image collection with more than two thousand actions which have been annotated through crowdsourcing. We review publicly available datasets, describe the annotation process of our image collection and some statistics of this dataset. Finally, experimental results on the dataset including human action recognition based on objects and an analysis of the relation between human-object positions in images and prepositions in language are presented."
W14-5418,Coloring Objects: Adjective-Noun Visual Semantic Compositionality,2014,8,5,3,0,3796,dat nguyen,Proceedings of the Third Workshop on Vision and Language,0,"This paper reports preliminary experiments aiming at verifying the conjecture that semantic compositionality is a general process irrespective of the underlying modality. In particular, we model compositionality of an attribute with an object in the visual modality as done in the case of an adjective with a noun in the linguistic modality. Our experiments show that the concept topologies in the two modalities share similarities, results that strengthen our conjecture. 1 Language and Vision Recently, fields like computational linguistics and computer vision have converged to a common way of capturing and representing the linguistic and visual information of atomic concepts, through vector space models. At the same time, advances in computational semantics have lead to effective and linguistically inspired approaches of extending such methods from single concepts to arbitrary linguistic units (e.g. phrases), through means of vector-based semantic composition (Mitchell and Lapata, 2010). Compositionality is not to be considered only an important component from a linguistic perspective, but also from a cognitive perspective and there has been efforts to validate it as a general cognitive process. However, in computer vision so far compositionality has received limited attention. Thus, in this work, we study the phenomenon of visual compositionality and we complement limited previous literature that has focused on event compositionality (Stxc2xa8"
S14-2001,{S}em{E}val-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment,2014,23,179,4,1,10198,marco marelli,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs)."
marelli-etal-2014-sick,A {SICK} cure for the evaluation of compositional distributional semantic models,2014,14,202,5,1,10198,marco marelli,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes."
2014.lilt-9.5,Frege in Space: A Program for Composition Distributional Semantics,2014,152,125,2,0,12129,marco baroni,"Linguistic Issues in Language Technology, Volume 9, 2014 - Perspectives on Semantic Representations for Textual Inference",0,"The lexicon of any natural language encodes a huge number of distinct word meanings. Just to understand this article, you will need to know what thousands of words mean. The space of possible sentential meanings is infinite: In this article alone, you will encounter many sentences that express ideas you have never heard before, we hope. Statistical semantics has addressed the issue of the vastness of word meaning by proposing methods to harvest meaning automatically from large collections of text (corpora). Formal semantics in the Fregean tradition has developed methods to account for the infinity of sentential meaning based on the crucial insight of compositionality, the idea that meaning of sentences is built incrementally by combining the meanings of their constituents. This article sketches a new approach to semantics that brings together ideas from statistical and formal semantics to account, in parallel, for the richness of lexical meaning and the combinatorial power of sentential semantics. We adopt, in particular, the idea that word meaning can be approximated by the patterns of co-occurrence of words in corpora from statistical semantics, and the idea that compositionality can be captured in terms of a syntax-driven calculus of function application from formal semantics."
W13-0603,Sentence paraphrase detection: When determiners and word order make the difference,2013,23,11,2,0,22647,nghia pham,Proceedings of the {IWCS} 2013 Workshop Towards a Formal Distributional Semantics,0,"Researchers working on distributional semantics have recently taken up the challenge of going beyond lexical meaning and tackle the issue of compositionality. Several Compositional Distributional Semantics Models (CDSMs) have been developed and promising results have been obtained in evaluations carried out against data sets of small phrases and as well as data sets of sentences. However, we believe there is the need to further develop good evaluation tasks that show whether CDSM truly capture compositionality. To this end, we present an evaluation task that highlights some differences among the CDSMs currently available by challenging them in detecting semantic differences caused by word order switch and by determiner replacements. We take as starting point simple intransitive and transitive sentences describing similar events, that we consider to be paraphrases of each other but not of the foil paraphrases we generate from them. Only the models sensitive to word order and determiner phrase meaning and their role in the sentence composition will not be captured into the foilsxe2x80x99 trap."
R13-1061,{CCG} Categories for Distributional Semantic Models,2013,18,1,2,0,9487,paramita mirza,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"For the last decade, distributional semantics has been an active area of research to address the problem of understanding the semantics of words in natural language. The core principal of the distributional semantic approach is that the linguistic context surrounding a given word, which is represented as a vector, provides important information about its meaning. In this paper we investigate the possibility to exploit Combinatory Categorial Grammar (CCG) categories as syntactic features to be relevant for characterizing the context vector and hence the meaning of words. We find that the CCG categories can enhance the representation of verb meaning."
P13-2010,A relatedness benchmark to test the role of determiners in compositional distributional semantics,2013,15,13,1,1,1053,raffaella bernardi,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set."
D13-1072,Exploiting Language Models for Visual Recognition,2013,25,8,3,1,10457,dieuthu le,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset."
W12-3304,Query classification using topic models and support vector machine,2012,13,1,2,1,10457,dieuthu le,Proceedings of {ACL} 2012 Student Research Workshop,0,"This paper describes a query classification system for a specialized domain. We take as a case study queries asked to a search engine of an art, cultural and history library and classify them against the library cataloguing categories. We show how click-through links, i.e., the links that a user clicks after submitting a query, can be exploited for extracting information useful to enrich the query as well as for creating the training set for a machine learning based classifier. Moreover, we show how Topic Model can be exploited to further enrich the query with hidden topics induced from the library meta-data. The experimental evaluations show that this system considerably outperforms a matching and ranking classification approach, where queries (and categories) were also enriched with similar information."
E12-1004,Entailment above the word level in distributional semantics,2012,33,136,2,0,12129,marco baroni,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We introduce two ways to detect entailment using distributional semantic representations of phrases. Our first experiment shows that the entailment relation between adjective-noun constructions and their head nouns (big cat|= cat), once represented as semantic vector pairs, generalizes to lexical entailment among nouns (dog|= animal). Our second experiment shows that a classifier fed semantic vector pairs can similarly generalize the entailment relation among quantifier phrases (many dogs|= some dogs) to entailment involving unseen quantifiers (all cats|= several cats). Moreover, nominal and quantifier phrase entailment appears to be cued by different distributional correlates, as predicted by the type-based view of entailment in formal semantics."
W11-4103,Query classification via Topic Models for an art image archive,2011,11,2,2,1,10457,dieuthu le,Proceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage,0,"In recent years, there has been an increasing amount of literature on query classification. Click-through information has been shown to be a useful source for improving this task. However, far too little attention has been paid to queries in very specific domains such as art, culture and history. We propose an approach that exploits topic models built from a domain specific corpus as a mean to enrich both the query and the categories against which the query need to be classified. We take an Art Library as the case study and show that topic model enrichment improves over the enrichment via click-through considerably."
W10-4359,Towards an Empirically Motivated Typology of Follow-Up Questions: The Role of Dialogue Context,2010,13,7,2,1,45118,manuel kirschner,Proceedings of the {SIGDIAL} 2010 Conference,0,"A central problem in Interactive Question Answering (IQA) is how to answer Follow-Up Questions (FU Qs), possibly by taking advantage of information from the dialogue context. We assume that FU Qs can be classified into specific types which determine if and how the correct answer relates to the preceding dialogue. The main goal of this paper is to propose an empirically motivated typology of FU Qs, which we then apply in a practical IQA setting. We adopt a supervised machine learning framework that ranks answer candidates to FU Qs. Both the answer ranking and the classification of FU Qs is done in this framework, based on a host of measures that include shallow and deep inter-utterance relations, automatically collected dialogue management meta information, and human annotation. We use Principal Component Analysis (PCA) to integrate these measures. As a result, we confirm earlier findings about the benefit of distinguishing between topic shift and topic continuation FU Qs. We then present a typology of FU Qs that is more fine-grained, extracted from the PCA and based on real dialogue data. Since all our measures are automatically computable, our results are relevant for IQA systems dealing with naturally occurring FU Qs."
bernardi-etal-2010-context,Context Fusion: The Role of Discourse Structure and Centering Theory,2010,16,5,1,1,1053,raffaella bernardi,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Questions are not asked in isolation. Their context, viz. the preceding interactions, might be of help to understand them and retrieve the correct answer. Previous research in Interactive Question Answering showed that context fusion has a big potential to improve the performance of answer retrieval. In this paper, we study how much context, and what elements of it, should be considered to answer Follow-Up Questions (FU Qs). Following previous research, we exploit Logistic Regression Models to learn aspects of dialogue structure relevant to answering FU Qs. We enrich existing models based on shallow features with deep features, relying on the theory of discourse structure of (Chai and Jin, 2004), and on Centering Theory, respectively. Using models trained on realistic IQA data, we show which of the various theoretically motivated features hold up against empirical evidence. We also show that, while these deep features do not outperform the shallow ones on their own, an IQA system's answer correctness increases if the shallow and deep features are combined."
N09-3003,Exploring Topic Continuation Follow-up Questions using Machine Learning,2009,7,4,2,1,45118,manuel kirschner,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium",0,"Some of the Follow-Up Questions (FU Q) that an Interactive Question Answering (IQA) system receives are not topic shifts, but rather continuations of the previous topic. In this paper, we propose an empirical framework to explore such questions, with two related goals in mind: (1) modeling the different relations that hold between the FU Q's answer and either the FU Q or the preceding dialogue, and (2) showing how this model can be used to identify the correct answer among several answer candidates. For both cases, we use Logistic Regression Models that we learn from real IQA data collected through a live system. We show that by adding dialogue context features and features based on sequences of domain-specific actions that represent the questions and answers, we obtain important additional predictors for the model, and improve the accuracy with which our system finds correct answers."
W08-1604,Context Modelling for {IQA}: the Role of Tasks and Entities,2008,12,4,1,1,1053,raffaella bernardi,Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions,0,"In a realistic Interactive Question Answering (IQA) setting, users frequently ask follow-up questions. By modeling how the questions' focus evolves in IQA dialogues, we want to describe what makes a particular follow-up question salient. We introduce a new focus model, and describe an implementation of an IQA system that we use for exploring our theory. To learn properties of salient focus transitions from data, we use logistic regression models that we validate on the basis of predicted answer correctness."
2007.sigdial-1.8,An Empirical View on {IQA} Follow-up Questions,2007,4,5,2,1,45118,manuel kirschner,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"In a realistic Interactive Question Answering (IQA) situation, one third of the users pose follow-up questions, i.e., go beyond a single question per dialogue. We identify two different perspectives according to which these follow-ups can be described: informational transitions and context dependency. By understanding exactly how informational transitions occur in IQA dialogues, we propose a method to guarantee that focus tree based IQA systems provide wide coverage of follow-up questions that trigger the respective set of informational transitions."
bernardi-etal-2006-pos,{POS} tagset design for {I}talian,2006,16,3,1,1,1053,raffaella bernardi,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We aim to automatically induce a PoS tagset for Italian by analysing the distributional behaviour of Italian words. To this end, we propose an algorithm that (a) extracts information from loosely labelled dependency structures that encode only basic and broadly accepted syntactic relations, namely Head/Dependent and the distinction of dependents into Argument vs. Adjunct, and (b) derives a possible set of word classes. The paper reports on some preliminary experiments carried out using the induced tagset in conjunction with state-of-the-art PoS taggers. The method proposed to design a proper tagset exploits little, if any, language-specific knowledge: hence it is in principle applicable to any language."
U05-1025,Automatic Induction of a {POS} Tagset for {I}talian,2005,11,3,1,1,1053,raffaella bernardi,Proceedings of the Australasian Language Technology Workshop 2005,0,"In this paper we present work in progress on the PoS annotation of an Italian Corpus (CORIS) developed at CILTA (University of Bologna). We aim to automatically induce the PoS tagset by analysing the distributional behaviour of Italian words by relying only on theory-neutral linguistic knowledge. To this end, we propose an algorithm that derives a possible tagset to be further interpreted and defined by the linguist. The algorithm extracts information from loosely labelled dependency structures that encode only basic and broadly accepted syntactic relations, namely Head/Dependent, and the distinction of dependents into Argument vs. Adjunct."
W00-2032,Deriving polarity effects,2000,-1,-1,1,1,1053,raffaella bernardi,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,None
