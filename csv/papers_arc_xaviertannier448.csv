2021.jeptalnrecital-deft.3,Classification multilabel de concepts m{\\'e}dicaux pour l{'}identification du profil clinique du patient (Multilabel classification of medical concepts for patient{'}s clinical profile identification ),2021,-1,-1,7,0,5680,christel gerardin,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Atelier D{\\'E}fi Fouille de Textes (DEFT),0,"La premi{\`e}re t{\^a}che du D{\'e}fi fouille de textes 2021 a consist{\'e} {\`a} extraire automatiquement, {\`a} partir de cas cliniques, les ph{\'e}notypes pathologiques des patients regroup{\'e}s par t{\^e}te de chapitre du MeSH-maladie. La solution pr{\'e}sent{\'e}e est celle d{'}un classifieur multilabel bas{\'e} sur un transformer. Deux transformers ont {\'e}t{\'e} utilis{\'e}s : le camembert-large classique (run 1) et le camembert-large fine-tun{\'e} (run 2) sur des articles biom{\'e}dicaux fran{\c{c}}ais en acc{\`e}s libre. Nous avons {\'e}galement propos{\'e} un mod{\`e}le Â« bout-enbout Â», avec une premi{\`e}re phase d{'}extraction d{'}entit{\'e}s nomm{\'e}es {\'e}galement bas{\'e}e sur un transformer de type camembert-large et un classifieur de genre sur un mod{\`e}le Adaboost. Nous obtenons un tr{\`e}s bon rappel et une pr{\'e}cision correcte, pour une F1-mesure autour de 0,77 pour les trois runs. La performance du mod{\`e}le Â« bout-en-bout Â» est similaire aux autres m{\'e}thodes."
2020.jeptalnrecital-taln.35,Mod{\\`e}le neuronal pour la r{\\'e}solution de la cor{\\'e}f{\\'e}rence dans les dossiers m{\\'e}dicaux {\\'e}lectroniques (Neural approach for coreference resolution in electronic health records ),2020,-1,-1,4,1,18601,julien tourille,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"La r{\'e}solution de la cor{\'e}f{\'e}rence est un {\'e}l{\'e}ment essentiel pour la constitution automatique de chronologies m{\'e}dicales {\`a} partir des dossiers m{\'e}dicaux {\'e}lectroniques. Dans ce travail, nous pr{\'e}sentons une approche neuronale pour la r{\'e}solution de la cor{\'e}f{\'e}rence dans des textes m{\'e}dicaux {\'e}crits en anglais pour les entit{\'e}s g{\'e}n{\'e}rales et cliniques en nous {\'e}valuant dans le cadre de r{\'e}f{\'e}rence pour cette t{\^a}che que constitue la t{\^a}che 1C de la campagne i2b2 2011."
2020.jeptalnrecital-deft.11,Participation de l{'}{\\'e}quipe du {LIMICS} {\\`a} {DEFT} 2020 (Participation of team {LIMICS} in the {DEFT} 2020 challenge ),2020,-1,-1,4,0,5682,perceval wajsburt,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Atelier D{\\'E}fi Fouille de Textes",0,"Nous pr{\'e}sentons dans cet article les m{\'e}thodes con{\c{c}}ues et les r{\'e}sultats obtenus lors de notre participation {\`a} la t{\^a}che 3 de la campagne d{'}{\'e}valuation DEFT 2020, consistant en la reconnaissance d{'}entit{\'e}s nomm{\'e}es du domaine m{\'e}dical. Nous proposons deux mod{\`e}les diff{\'e}rents permettant de prendre en compte les entit{\'e}s imbriqu{\'e}es, qui repr{\'e}sentent une des difficult{\'e}s du jeu de donn{\'e}es propos{\'e}es, et pr{\'e}sentons les r{\'e}sultats obtenus. Notre meilleur run obtient la meilleure performance parmi les participants, sur l{'}une des deux sous-t{\^a}ches du d{\'e}fi."
2019.jeptalnrecital-deft.3,Participation de l{'}{\\'e}quipe {LAI} {\\`a} {DEFT} 2019 (Participation of team {LAI} in the {DEFT} 2019 challenge ),2019,-1,-1,3,0,27336,jacques hilbey,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. D{\\'e}fi Fouille de Textes (atelier TALN-RECITAL),0,"Nous pr{\'e}sentons dans cet article les m{\'e}thodes con{\c{c}}ues et les r{\'e}sultats obtenus lors de notre participation {\`a} la t{\^a}che 3 de la campagne d{'}{\'e}valuation DEFT 2019. Nous avons utilis{\'e} des approches simples {\`a} base de r{\`e}gles ou d{'}apprentissage automatique, et si nos r{\'e}sultats sont tr{\`e}s bons sur les informations simples {\`a} extraire comme l{'}{\^a}ge et le sexe du patient, ils restent mitig{\'e}s sur les t{\^a}ches plus difficiles."
W18-5622,Evaluation of a Sequence Tagging Tool for Biomedical Texts,2018,0,2,6,1,18601,julien tourille,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts."
W17-4211,Unsupervised Event Clustering and Aggregation from Newswire and Web Articles,2017,11,1,3,0,31706,swen ribeiro,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,"In this paper, we present an unsupervised pipeline approach for clustering news articles based on identified event instances in their content. We leverage press agency newswire and monolingual word alignment techniques to build meaningful and linguistically varied clusters of articles from the web in the perspective of a broader event type detection task. We validate our approach on a manually annotated corpus of Web articles."
S17-2098,{LIMSI}-{COT} at {S}em{E}val-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives,2017,0,5,3,1,18601,julien tourille,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks."
P17-2035,Neural Architecture for Temporal Relation Extraction: A {B}i-{LSTM} Approach for Detecting Narrative Containers,2017,20,23,4,1,18601,julien tourille,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present a neural architecture for containment relation identification between medical events and/or temporal expressions. We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus. Our model achieves an F-measure of 0.613 and outperforms the best result reported on this corpus to date."
E17-2117,Temporal information extraction from clinical text,2017,15,5,3,1,18601,julien tourille,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus and the THYME corpus, and show that a common approach can be used for both languages."
2017.jeptalnrecital-long.13,"Apprendre des repr{\\'e}sentations jointes de mots et d{'}entit{\\'e}s pour la d{\\'e}sambigu{\\\\\i}sation d{'}entit{\\'e}s (Combining Word and Entity Embeddings for Entity Linking)""",2017,-1,-1,7,0,2103,jose moreno,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"La d{\'e}sambigu{\""\i}sation d{'}entit{\'e}s (ou liaison d{'}entit{\'e}s), qui consiste {\`a} relier des mentions d{'}entit{\'e}s d{'}un texte {\`a} des entit{\'e}s d{'}une base de connaissance, est un probl{\`e}me qui se pose, entre autre, pour le peuplement automatique de bases de connaissances {\`a} partir de textes. Une difficult{\'e} de cette t{\^a}che est la r{\'e}solution d{'}ambigu{\""\i}t{\'e}s car les syst{\`e}mes ont {\`a} choisir parmi un nombre important de candidats. Cet article propose une nouvelle approche fond{\'e}e sur l{'}apprentissage joint de repr{\'e}sentations distribu{\'e}es des mots et des entit{\'e}s dans le m{\^e}me espace, ce qui permet d{'}{\'e}tablir un mod{\`e}le robuste pour la comparaison entre le contexte local de la mention d{'}entit{\'e} et les entit{\'e}s candidates."
S16-1002,{S}em{E}val-2016 Task 5: Aspect Based Sentiment Analysis,2016,13,194,13,0,18398,maria pontiki,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams."
S16-1175,{LIMSI}-{COT} at {S}em{E}val-2016 Task 12: Temporal relation identification using a pipeline of classifiers,2016,9,1,4,1,18601,julien tourille,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"SemEval 2016 Task 12 addresses temporal reasoning in the clinical domain. In this paper, we present our participation for relation extraction based on gold standard entities (subtasks DR and CR). We used a supervised approach comparing plain lexical features to word embeddings for temporal relation identification, and obtained above-median scores."
L16-1179,Datasets for Aspect-Based Sentiment Analysis in {F}rench,2016,6,3,2,0,2673,marianna apidianaki,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Aspect Based Sentiment Analysis (ABSA) is the task of mining and summarizing opinions from text about specific entities and their aspects. This article describes two datasets for the development and testing of ABSA systems for French which comprise user reviews annotated with relevant entities, aspects and polarity values. The first dataset contains 457 restaurant reviews (2365 sentences) for training and testing ABSA systems, while the second contains 162 museum reviews (655 sentences) dedicated to out-of-domain evaluation. Both datasets were built as part of SemEval-2016 Task 5 {``}Aspect-Based Sentiment Analysis{''} where seven different languages were represented, and are publicly available for research purposes."
L16-1307,A Dataset for Open Event Extraction in {E}nglish,2016,13,4,2,1,6565,kiemhieu nguyen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This article presents a corpus for development and testing of event schema induction systems in English. Schema induction is the task of learning templates with no supervision from unlabeled texts, and to group together entities corresponding to the same role in a template. Most of the previous work on this subject relies on the MUC-4 corpus. We describe the limits of using this corpus (size, non-representativeness, similarity of roles across templates) and propose a new, partially-annotated corpus in English which remedies some of these shortcomings. We make use of Wikinews to select the data inside the category Laws {\&} Justice, and query Google search engine to retrieve different documents on the same events. Only Wikinews documents are manually annotated and can be used for evaluation, while the others can be used for unsupervised learning. We detail the methodology used for building the corpus and evaluate some existing systems on this new data."
2016.jeptalnrecital-poster.19,Extraction de relations temporelles dans des dossiers {\\'e}lectroniques patient (Extracting Temporal Relations from Electronic Health Records),2016,-1,-1,4,1,18601,julien tourille,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"L{'}analyse temporelle des documents cliniques permet d{'}obtenir des repr{\'e}sentations riches des informations contenues dans les dossiers {\'e}lectroniques patient. Cette analyse repose sur l{'}extraction d{'}{\'e}v{\'e}nements, d{'}expressions temporelles et des relations entre eux. Dans ce travail, nous consid{\'e}rons que nous disposons des {\'e}v{\'e}nements et des expressions temporelles pertinents et nous nous int{\'e}ressons aux relations temporelles entre deux {\'e}v{\'e}nements ou entre un {\'e}v{\'e}nement et une expression temporelle. Nous pr{\'e}sentons des mod{\`e}les de classification supervis{\'e}e pour l{'}extraction de des relations en fran{\c{c}}ais et en anglais. Les performances obtenues sont comparables dans les deux langues, sugg{\'e}rant ainsi que diff{\'e}rents domaines cliniques et diff{\'e}rentes langues pourraient {\^e}tre abord{\'e}s de mani{\`e}re similaire."
W15-2603,Redundancy in {F}rench Electronic Health Records: A preliminary study,2015,16,1,2,0,32928,eva dhondt,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"The use of Electronic Health Records (EHRs) is becoming more prevalent in healthcare institutions world-wide. These digital records contain a wealth of information on patientsxe2x80x99 health in the form of Natural Language text. The electronic format of the clinical notes has evident advantages in terms of storage and shareability, but also makes it easy to duplicate information from one document to another through copy-pasting. Previous studies have shown that (copy-paste-induced) redundancy can reach high levels in American EHRs, and that these high levels of redundancy have a negative effect on the performance of Natural Language Processing (NLP) tools that are used to process EHRs automatically. In this paper, we present a preliminary study on the level of redundancy in French EHRs. We study the evolution of redundancy over time, and its occurrence in respect to different document types and sections in a small corpus comprising of three patient records (361 documents). We find that average redundancy levels in our subset are lower than those observed in U.S. corpora (respectively 33% vs. up to 78%), which may indicate different cultural practices between these two countries. Moreover, we find no evidence of the incremental increase (over time) of redundant text in clinical notes which has been found in American EHRs. These results suggest that redundancy mitigating strategies may not be needed when processing French EHRs."
P15-1019,Generative Event Schema Induction with Entity Disambiguation,2015,31,26,2,1,6565,kiemhieu nguyen,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper presents a generative model to event schema induction. Previous methods in the literature only use head words to represent entities. However, elements other than head words contain useful information. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement."
D15-1055,Automatic Extraction of Time Expressions Accross Domains in {F}rench Narratives,2015,20,2,2,0,18809,mike nzali,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"The prevalence of temporal references across all types of natural language utterances makes temporal analysis a key issue in Natural Language Processing. This work adresses three research questions: 1/is temporal expression recognition specific to a particular domain? 2/if so, can we characterize domain specificity? and 3/how can subdomain specificity be integrated in a single tool for unified temporal expression extraction? Herein, we assess temporal expression recognition from documents written in French covering three domains. We present a new corpus of clinical narratives annotated for temporal expressions , and also use existing corpora in the newswire and historical domains. We show that temporal expressions can be extracted with high performance across domains (best F-measure 0.96 obtained with a CRF model on clinical narratives). We argue that domain adaptation for the extraction of temporal expressions can be done with limited efforts and should cover pre-processing as well as temporal specific tasks."
2015.jeptalnrecital-long.5,Analyse d{'}expressions temporelles dans les dossiers {\\'e}lectroniques patients,2015,-1,-1,3,0,18809,mike nzali,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les r{\'e}f{\'e}rences {\`a} des ph{\'e}nom{\`e}nes du monde r{\'e}el et {\`a} leur caract{\'e}risation temporelle se retrouvent dans beaucoup de types de discours en langue naturelle. Ainsi, l{'}analyse temporelle appara{\^\i}t comme un {\'e}l{\'e}ment important en traitement automatique de la langue. Cet article pr{\'e}sente une analyse de textes en domaine de sp{\'e}cialit{\'e} du point de vue temporel. En s{'}appuyant sur un corpus de documents issus de plusieurs dossiers {\'e}lectroniques patient d{\'e}sidentifi{\'e}s, nous d{\'e}crivons la construction d{'}une ressource annot{\'e}e en expressions temporelles selon la norme TimeML. Par suite, nous utilisons cette ressource pour {\'e}valuer plusieurs m{\'e}thodes d{'}extraction automatique d{'}expressions temporelles adapt{\'e}es au domaine m{\'e}dical. Notre meilleur syst{\`e}me statistique offre une performance de 0,91 de F-mesure, surpassant pour l{'}identification le syst{\`e}me {\'e}tat de l{'}art HeidelTime. La comparaison de notre corpus de travail avec le corpus journalistique FR-Timebank permet {\'e}galement de caract{\'e}riser les diff{\'e}rences d{'}utilisation des expressions temporelles dans deux domaines de sp{\'e}cialit{\'e}."
2015.jeptalnrecital-long.7,"D{\\'e}sambigu{\\\\\i}sation d{'}entit{\\'e}s pour l{'}induction non supervis{\\'e}e de sch{\\'e}mas {\\'e}v{\\'e}nementiels""",2015,-1,-1,2,1,6565,kiemhieu nguyen,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente un mod{\`e}le g{\'e}n{\'e}ratif pour l{'}induction non supervis{\'e}e d{'}{\'e}v{\'e}nements. Les pr{\'e}c{\'e}dentes m{\'e}thodes de la litt{\'e}rature utilisent uniquement les t{\^e}tes des syntagmes pour repr{\'e}senter les entit{\'e}s. Pourtant, le groupe complet (par exemple, {''}un homme arm{\'e}{''}) apporte une information plus discriminante (que {''}homme{''}). Notre mod{\`e}le tient compte de cette information et la repr{\'e}sente dans la distribution des sch{\'e}mas d{'}{\'e}v{\'e}nements. Nous montrons que ces relations jouent un r{\^o}le important dans l{'}estimation des param{\`e}tres, et qu{'}elles conduisent {\`a} des distributions plus coh{\'e}rentes et plus discriminantes. Les r{\'e}sultats exp{\'e}rimentaux sur le corpus de MUC-4 confirment ces progr{\`e}s."
tannier-2014-extracting,Extracting News Web Page Creation Time with {DCTF}inder,2014,22,4,1,1,5686,xavier tannier,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Web pages do not offer reliable metadata concerning their creation date and time. However, getting the document creation time is a necessary step for allowing to apply temporal normalization systems to web pages. In this paper, we present DCTFinder, a system that parses a web page and extracts from its content the title and the creation date of this web page. DCTFinder combines heuristic title detection, supervised learning with Conditional Random Fields (CRFs) for document date extraction, and rule-based creation time recognition. Using such a system allows further deep and efficient temporal analysis of web pages. Evaluation on three corpora of English and French web pages indicates that the tool can extract document creation times with reasonably high accuracy (between 87 and 92{\textbackslash}{\%}). DCTFinder is made freely available on http://sourceforge.net/projects/dctfinder/, as well as all resources (vocabulary and annotated documents) built for training and evaluating the system in English and French, and the English trained model itself."
moriceau-tannier-2014-french,{F}rench Resources for Extraction and Normalization of Temporal Expressions with {H}eidel{T}ime,2014,13,7,2,0.688422,7014,veronique moriceau,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe the development of French resources for the extraction and normalization of temporal expressions with HeidelTime, a open-source multilingual, cross-domain temporal tagger. HeidelTime extracts temporal expressions from documents and normalizes them according to the TIMEX3 annotation standard. Several types of temporal expressions are extracted: dates, times, durations and temporal sets. French resources have been evaluated in two different ways: on the French TimeBank corpus, a corpus of newspaper articles in French annotated according to the ISO-TimeML standard, and on a user application for automatic building of event timelines. Results on the French TimeBank are quite satisfaying as they are comparable to those obtained by HeidelTime in English and Spanish on newswire articles. Concerning the user application, we used two temporal taggers for the preprocessing of the corpus in order to compare their performance and results show that the performances of our application on French documents are better with HeidelTime. The French resources and evaluation scripts are publicly available with HeidelTime."
de-groc-tannier-2014-evaluating,Evaluating Web-as-corpus Topical Document Retrieval with an Index of the {O}pen{D}irectory,2014,9,0,2,1,39962,clement groc,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article introduces a novel protocol and resource to evaluate Web-as-corpus topical document retrieval. To the contrary of previous work, our goal is to provide an automatic, reproducible and robust evaluation for this task. We rely on the OpenDirectory (DMOZ) as a source of topically annotated webpages and index them in a search engine. With this OpenDirectory search engine, we can then easily evaluate the impact of various parameters such as the number of seed terms, queries or documents, or the usefulness of various term selection algorithms. A first fully automatic evaluation is described and provides baseline performances for this task. The article concludes with practical information regarding the availability of the index and resource files."
de-groc-etal-2014-thematic,Thematic Cohesion: measuring terms discriminatory power toward themes,2014,12,0,2,1,39962,clement groc,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a new measure of thematic cohesion. This measure associates each term with a weight representing its discriminatory power toward a theme, this theme being itself expressed by a list of terms (a thematic lexicon). This thematic cohesion criterion can be used in many applications, such as query expansion, computer-assisted translation, or iterative construction of domain-specific lexicons and corpora. The measure is computed in two steps. First, a set of documents related to the terms is gathered from the Web by querying a Web search engine. Then, we produce an oriented co-occurrence graph, where vertices are the terms and edges represent the fact that two terms co-occur in a document. This graph can be interpreted as a recommendation graph, where two terms occurring in a same document means that they recommend each other. This leads to using a random walk algorithm that assigns a global importance value to each vertex of the graph. After observing the impact of various parameters on those importance values, we evaluate their correlation with retrieval effectiveness."
C14-1114,Ranking Multidocument Event Descriptions for Building Thematic Timelines,2014,15,5,2,1,6565,kiemhieu nguyen,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper tackles the problem of timeline generation from traditional news sources. Our system builds thematic timelines for a general-domain topic defined by a user query. The system selects and ranks events relevant to the input query. Each event is represented by a one-sentence description in the output timeline. We present an inter-cluster ranking algorithm that takes events from multiple clusters as input and that selects the most salient and relevant events. A cluster, in our work, contains all the events happening in a specific date. Our algorithm utilizes the temporal information derived from a large collection of extensively temporal analyzed texts. Such temporal information is combined with textual contents into an event scoring model in order to rank events based on their salience and query-relevance."
F13-3006,An Interface for Validating and Evaluating Thematic Timelines (Une interface pour la validation et l{'}{\\'e}valuation de chronologies th{\\'e}matiques) [in {F}rench],2013,0,0,1,1,5686,xavier tannier,Proceedings of TALN 2013 (Volume 3: System Demonstrations),0,None
F13-2018,Extraction of temporal relations between clinical events in clinical documents (Extraction des relations temporelles entre {\\'e}v{\\'e}nements m{\\'e}dicaux dans des comptes rendus hospitaliers) [in {F}rench],2013,-1,-1,2,0,8591,pierre zweigenbaum,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
D13-1098,Building Event Threads out of Multiple News Articles,2013,17,3,1,1,5686,xavier tannier,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set 1 ."
P12-1077,Finding Salient Dates for Building Thematic Timelines,2012,27,22,2,0,40008,remy kessler,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related."
tannier-2012-webannotator,"{W}eb{A}nnotator, an Annotation Tool for Web Pages",2012,4,5,1,1,5686,xavier tannier,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This article presents WebAnnotator, a new tool for annotating Web pages. WebAnnotator is implemented as a Firefox extension, allowing annotation of both offline and inline pages. The HTML rendering fully preserved and all annotations consist in new HTML spans with specific styles. WebAnnotator provides an easy and general-purpose framework and is made available under CeCILL free license (close to GNU GPL), so that use and further contributions are made simple. All parts of an HTML document can be annotated: text, images, videos, tables, menus, etc. The annotations are created by simply selecting a part of the document and clicking on the relevant type and subtypes. The annotated elements are then highlighted in a specific color. Annotation schemas can be defined by the user by creating a simple DTD representing the types and subtypes that must be highlighted. Finally, annotations can be saved (HTML with highlighted parts of documents) or exported (in a machine-readable format)."
arnulphy-etal-2012-event,Event Nominals: Annotation Guidelines and a Manually Annotated Corpus in {F}rench,2012,9,2,2,1,42890,beatrice arnulphy,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Within the general purpose of information extraction, detection of event descriptions is an important clue. A word refering to an event is more powerful than a single word, because it implies a location, a time, protagonists (persons, organizations{\textbackslash}dots). However, if verbal designations of events are well studied and easier to detect than nominal ones, nominal designations do not claim as much definition effort and resources. In this work, we focus on nominals desribing events. As our application domain is information extraction, we follow a named entity approach to describe and annotate events. In this paper, we present a typology and annotation guidelines for event nominals annotation. We applied them to French newswire articles and produced an annotated corpus. We present observations about the designations used in our manually annotated corpus and the behavior of their triggers. We provide statistics concerning word ambiguity and context of use of event nominals, as well as machine learning experiments showing the difficulty of using lexicons for extracting events."
bittar-etal-2012-temporal,Temporal Annotation: A Proposal for Guidelines and an Experiment with Inter-annotator Agreement,2012,10,3,4,0,27920,andre bittar,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This article presents work carried out within the framework of the ongoing ANR (French National Research Agency) project Chronolines, which focuses on the temporal processing of large news-wire corpora in English and French. The aim of the project is to create new and innovative interfaces for visualizing textual content according to temporal criteria. Extracting and normalizing the temporal information in texts through linguistic annotation is an essential step towards attaining this objective. With this goal in mind, we developed a set of guidelines for the annotation of temporal and event expressions that is intended to be compatible with the TimeML markup language, while addressing some of its pitfalls. We provide results of an initial application of these guidelines to real news-wire texts in French over several iterations of the annotation process. These results include inter-annotator agreement figures and an error analysis. Our final inter-annotator agreement figures compare favorably with those reported for the TimeBank 1.2 annotation project."
tannier-etal-2012-evolution,Evolution of Event Designation in Media: Preliminary Study,2012,4,2,1,1,5686,xavier tannier,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Within the general purpose of information extraction, detection of event descriptions is often an important clue. An important characteristic of event designation in texts, and especially in media, is that it changes over time. Understanding how these designations evolve is important in information retrieval and information extraction. Our first hypothesis is that, when an event first occurs, media relate it in a very descriptive way (using verbal designations) whereas after some time, they use shorter nominal designations instead. Our second hypothesis is that the number of different nominal designations for an event tends to stabilize itself over time. In this article, we present our methodology concerning the study of the evolution of event designations in French documents from the news agency AFP. For this preliminary study, we focused on 7 topics which have been relatively important in France. Verbal and nominal designations of events have been manually annotated in manually selected topic-related passages. This French corpus contains a total of 2064 annotations. We then provide preliminary interesting statistical results and observations concerning these evolutions."
paroubek-tannier-2012-rough,A Rough Set Formalization of Quantitative Evaluation with Ambiguity,2012,22,0,2,0,5615,patrick paroubek,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we present the founding elements of a formal model of the evaluation paradigm in natural language processing. We propose an abstract model of objective quantitative evaluation based on rough sets, as well as the notion of potential performance space for describing the performance variations corresponding to the ambiguity present in hypothesis data produced by a computer program, when comparing it to the reference data created by humans. A formal model of the evaluation paradigm will be useful for comparing evaluations protocols, investigating evaluation constraint relaxation and getting a better understanding of the evaluation paradigm, provided it is general enough to be able to represent any natural language processing task."
F12-2014,Un crit{\\`e}re de coh{\\'e}sion th{\\'e}matique fond{\\'e} sur un graphe de cooccurrences (Topical Cohesion using Graph Random Walks) [in {F}rench],2012,-1,-1,2,1,39962,clement groc,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
W11-1106,"{G}rawl{TCQ}: Terminology and Corpora Building by Ranking Simultaneously Terms, Queries and Documents using Graph Random Walks",2011,15,4,1,1,5686,xavier tannier,Proceedings of {T}ext{G}raphs-6: Graph-based Methods for Natural Language Processing,0,"In this paper, we present GrawlTCQ, a new bootstrapping algorithm for building specialized terminology, corpora and queries, based on a graph model. We model links between documents, terms and queries, and use a random walk with restart algorithm to compute relevance propagation. We have evaluated GrawlTCQ on an AFP English corpus of 57,441 news over 10 categories. For corpora building, GrawlTCQ outperforms the BootCaT tool, which is vastly used in the domain. For 1,000 documents retrieved, we improve mean precision by 25%. GrawlTCQ has also shown to be faster and more robust than BootCaT over iterations."
2011.jeptalnrecital-court.6,G{\\'e}n{\\'e}ration automatique de questions {\\`a} partir de textes en fran{\\c{c}}ais (Automatic generation of questions from texts in {F}rench),2011,-1,-1,4,0,44944,louis viron,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons dans cet article un g{\'e}n{\'e}rateur automatique de questions pour le fran{\c{c}}ais. Le syst{\`e}me de g{\'e}n{\'e}ration proc{\`e}de par transformation de phrases d{\'e}claratives en interrogatives et se base sur une analyse syntaxique pr{\'e}alable de la phrase de base. Nous d{\'e}taillons les diff{\'e}rents types de questions g{\'e}n{\'e}r{\'e}es. Nous pr{\'e}sentons {\'e}galement une {\'e}valuation de l{'}outil, qui d{\'e}montre que 41 {\%} des questions g{\'e}n{\'e}r{\'e}es par le syst{\`e}me sont parfaitement bien form{\'e}es."
2011.jeptalnrecital-court.9,Un lexique pond{\\'e}r{\\'e} des noms d{'}{\\'e}v{\\'e}nements en fran{\\c{c}}ais (A weighted lexicon of event names in {F}rench),2011,-1,-1,2,1,42890,beatrice arnulphy,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article d{\'e}crit une {\'e}tude sur l{'}annotation automatique des noms d{'}{\'e}v{\'e}nements dans les textes en fran{\c{c}}ais. Plusieurs lexiques existants sont utilis{\'e}s, ainsi que des r{\`e}gles syntaxiques d{'}extraction, et un lexique compos{\'e} de fa{\c{c}}on automatique, permettant de fournir une valeur sur le niveau d{'}ambigu{\""\i}t{\'e} du mot en tant qu{'}{\'e}v{\'e}nement. Cette nouvelle information permettrait d{'}aider {\`a} la d{\'e}sambigu{\""\i}sation des noms d{'}{\'e}v{\'e}nements en contexte."
tannier-moriceau-2010-fidji,{FIDJI}: Web Question-Answering at Quaero 2009,2010,6,3,1,1,5686,xavier tannier,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents the participation of FIDJI system to the Web Question-Answering evaluation campaign organized by Quaero in 2009. FIDJI is an open-domain question-answering system which combines syntactic information with traditional QA techniques such as named entity recognition and term weighting in order to validate answers through multiple documents. It was originally designed to process ``clean'' document collections. Overall results are significantly lower than in traditional campaigns but results (for French evaluation) are quite good compared to other state-of-the-art systems. They show that a syntax-based strategy, applied on uncleaned Web data, can still obtain good results. Moreover, we obtain much higher scores on ``complex'' questions, i.e. `how' and `why' questions, which are more representative of real user needs. These results show that questioning the Web with advanced linguistic techniques can be done without heavy pre-processing and with results that come near to best systems that use strong resources and large structured indexes."
galibert-etal-2010-hybrid,Hybrid Citation Extraction from Patents,2010,7,3,3,0,13778,olivier galibert,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The Quaero project organized a set of evaluations of Named Entity recognition systems in 2009. One of the sub-tasks consists in extracting citations from patents, i.e. references to other documents, either other patents or general literature from English-language patents. We present in this paper the participation of LIMSI in this evaluation, with a complete system description and the evaluation results. The corpus shown that patent and non-patent citations have a very different nature. We then separated references to other patents and to general literature papers and we created a hybrid system. For patent citations, the system used rule-based expert knowledge on the form of regular expressions. The system for detecting non-patent citations, on the other hand, is purely stochastic (machine learning with CRF++). Then we mixed both approaches to provide a single output. 4 teams participated to this task and our system obtained the best results of this evaluation campaign, even if the difference between the first two systems is poorly significant."
quintard-etal-2010-question,Question Answering on Web Data: The {QA} Evaluation in Qu{\\ae}ro,2010,10,24,8,0,42953,ludovic quintard,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In the QA and information retrieval domains progress has been assessed via evaluation campaigns(Clef, Ntcir, Equer, Trec).In these evaluations, the systems handle independent questions and should provide one answer to each question, extracted from textual data, for both open domain and restricted domain. Qu{\ae}ro is a program promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Among the many research areas concerned by Qu{\ae}ro. The Quaero project organized a series of evaluations of Question Answering on Web Data systems in 2008 and 2009. For each language, English and French the full corpus has a size of around 20Gb for 2.5M documents. We describe the task and corpora, and especially the methodologies used in 2008 to construct the test of question and a new one in the 2009 campaign. Six types of questions were addressed, factual, Non-factual(How, Why, What), List, Boolean. A description of the participating systems and the obtained results is provided. We show the difficulty for a question-answering system to work with complex data and questions."
galibert-etal-2010-named,Named and Specific Entity Detection in Varied Data: The Qu{\\ae}ro Named Entity Baseline Evaluation,2010,9,14,10,0,13778,olivier galibert,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The Qu{\ae}ro program that promotes research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within its context a set of evaluations of Named Entity recognition systems was held in 2009. Four tasks were defined. The first two concerned traditional named entities in French broadcast news for one (a rerun of ESTER 2) and of OCR-ed old newspapers for the other. The third was a gene and protein name extraction in medical abstracts. The last one was the detection of references in patents. Four different partners participated, giving a total of 16 systems. We provide a synthetic descriptions of all of them classifying them by the main approaches chosen (resource-based, rules-based or statistical), without forgetting the fact that any modern system is at some point hybrid. The metric (the relatively standard Slot Error Rate) and the results are also presented and discussed. Finally, a process is ongoing with preliminary acceptance of the partners to ensure the availability for the community of all the corpora used with the exception of the non-Qu{\ae}ro produced ESTER 2 one."
grappy-etal-2010-corpus,A Corpus for Studying Full Answer Justification,2010,7,6,7,0,42529,arnaud grappy,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Question answering (QA) systems aim at retrieving precise information from a large collection of documents. To be considered as reliable by users, a QA system must provide elements to evaluate the answer. This notion of answer justification can also be useful when developping a QA system in order to give criteria for selecting correct answers. An answer justification can be found in a sentence, a passage made of several consecutive sentences or several passages of a document or several documents. Thus, we are interesting in pinpointing the set of information that allows to verify the correctness of the answer in a candidate passage and the question elements that are missing in this passage. Moreover, the relevant information is often given in texts in a different form from the question form: anaphora, paraphrases, synonyms. In order to have a better idea of the importance of all the phenomena we underlined, and to provide enough examples at the QA developer's disposal to study them, we decided to build an annotated corpus."
2010.jeptalnrecital-court.6,Une {\\'e}tude des questions {``}complexes{''} en question-r{\\'e}ponse,2010,-1,-1,2,1,7014,veronique moriceau,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La plupart des syst{\`e}mes de question-r{\'e}ponse ont {\'e}t{\'e} con{\c{c}}us pour r{\'e}pondre {\`a} des questions dites {``}factuelles{''} (r{\'e}ponses pr{\'e}cises comme des dates, des lieux), et peu se sont int{\'e}ress{\'e}s au traitement des questions complexes. Cet article pr{\'e}sente une typologie des questions en y incluant les questions complexes, ainsi qu{'}une typologie des formes de r{\'e}ponses attendues pour chaque type de questions. Nous pr{\'e}sentons {\'e}galement des exp{\'e}riences pr{\'e}liminaires utilisant ces typologies pour les questions complexes, avec de bons r{\'e}sultats."
2010.jeptalnrecital-court.25,Les entit{\\'e}s nomm{\\'e}es {\\'e}v{\\'e}nement et les verbes de cause-cons{\\'e}quence,2010,-1,-1,2,1,42890,beatrice arnulphy,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"L{'}extraction des {\'e}v{\'e}nements d{\'e}sign{\'e}s par des noms est peu {\'e}tudi{\'e}e dans des corpus g{\'e}n{\'e}ralistes. Si des lexiques de noms d{\'e}clencheurs d{'}{\'e}v{\'e}nements existent, les probl{\`e}mes de polys{\'e}mie sont nombreux et beaucoup d{'}{\'e}v{\'e}nements ne sont pas introduits par des d{\'e}clencheurs. Nous nous int{\'e}ressons dans cet article {\`a} une hypoth{\`e}se selon laquelle les verbes induisant la cause ou la cons{\'e}quence sont de bons indices quant {\`a} la pr{\'e}sence d{'}{\'e}v{\'e}nements nominaux dans leur cotexte."
2009.jeptalnrecital-court.6,Apport de la syntaxe dans un syst{\\`e}me de question-r{\\'e}ponse : {\\'e}tude du syst{\\`e}me {FIDJI}.,2009,-1,-1,2,1,7014,veronique moriceau,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article pr{\'e}sente une s{\'e}rie d{'}{\'e}valuations visant {\`a} {\'e}tudier l{'}apport d{'}une analyse syntaxique robuste des questions et des documents dans un syst{\`e}me de questions-r{\'e}ponses. Ces {\'e}valuations ont {\'e}t{\'e} effectu{\'e}es sur le syst{\`e}me FIDJI, qui utilise {\`a} la fois des informations syntaxiques et des techniques plus {``}traditionnelles{''}. La s{\'e}lection des documents, l{'}extraction de la r{\'e}ponse ainsi que le comportement selon les diff{\'e}rents types de questions ont {\'e}t{\'e} {\'e}tudi{\'e}s."
tannier-muller-2008-evaluation,Evaluation Metrics for Automatic Temporal Annotation of Texts,2008,9,10,1,1,5686,xavier tannier,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Recent years have seen increasing attention in temporal processing of texts as well as a lot of standardization effort of temporal information in natural language. A central part of this information lies in the temporal relations between events described in a text, when their precise times or dates are not known. Reliable human annotation of such information is difficult, and automatic comparisons must follow procedures beyond mere precision-recall of local pieces of information, since a coherent picture can only be considered at a global level. We address the problem of evaluation metrics of such information, aiming at fair comparisons between systems, by proposing some measures taking into account the globality of a text."
S07-1110,{XRCE}-{T}: {XIP} Temporal Module for {T}emp{E}val campaign.,2007,8,17,2,0.453441,41138,caroline hagege,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We present the system we used for the TempEval competition. This system relies on a deep syntactic analyzer that has been extended for the treatment of temporal expressions, thus making temporal processing a complement to a better general purpose text understanding system."
U06-1026,Natural Language Processing and {XML} Retrieval,2006,4,3,2,0,49884,alan woodley,Proceedings of the Australasian Language Technology Workshop 2006,0,"XML information retrieval (XML-IR) systems respond to user queries with results more specific than documents. XML-IR queries contain both content and structural requirements traditionally expressed in a formal language. However, an intuitive alternative is natural language queries (NLQs). Here, we discuss three approaches for handling NLQs in an XMLIR system that are comparable to, and even outperform formal language queries."
C04-1008,Annotating and measuring temporal relations in texts,2004,13,24,2,0,5596,philippe muller,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper focuses on the automated processing of temporal information in written texts, more specifically on relations between events introduced by verbs in finite clauses. While this problem has been largely studied from a theoretical point of view, it has very rarely been applied to real texts, if ever, with quantified results. The methodology required is still to be defined, even though there have been proposals in the strictly human annotation case. We propose here both a procedure to achieve this task and a way of measuring the results. We have been testing the feasibility of this on newswire articles, with promising results."
2004.jeptalnrecital-long.29,Une m{\\'e}thode pour l{'}annotation de relations temporelles dans des textes et son {\\'e}valuation,2004,-1,-1,2,0,5596,philippe muller,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article traite de l{'}annotation automatique d{'}informations temporelles dans des textes et vise plus particuli{\`e}rement les relations entre {\'e}v{\'e}nements introduits par les verbes dans chaque clause. Si ce probl{\`e}me a mobilis{\'e} beaucoup de chercheurs sur le plan th{\'e}orique, il reste en friche pour ce qui est de l{'}annotation automatique syst{\'e}matique (et son {\'e}valuation), m{\^e}me s{'}il existe des d{\'e}buts de m{\'e}thodologie pour faire r{\'e}aliser la t{\^a}che par des humains. Nous proposons ici {\`a} la fois une m{\'e}thode pour r{\'e}aliser la t{\^a}che automatiquement et une mani{\`e}re de mesurer {\`a} quel degr{\'e} l{'}objectif est atteint. Nous avons test{\'e} la faisabilit{\'e} de ceci sur des d{\'e}p{\^e}ches d{'}agence avec des premiers r{\'e}sultats encourageants."
