2020.cl-3.3,abeille-barrier-2004-enriching,0,0.0372292,"Missing"
2020.cl-3.3,C16-1327,0,0.0608024,"Missing"
2020.cl-3.3,L18-1719,0,0.0272427,"Missing"
2020.cl-3.3,apresjan-etal-2006-syntactically,0,0.0289852,"of lexical strings in the deep-syntactic representation of the sentence. They are substituted for lexemes at the next, lower level (surface-syntactic representation). Some features that are captured within the deep-syntactic representation in other approaches (e.g., topic-focus articulation in Functional Generative Description, see Section 3.3) are described at a separate, more abstract level, so-called semantic representation in MTT. The multileveled scheme proposed by the MTT is applied in a corpus for Russian (SynTagRus) and in a treebank for Spanish (AnCora-UPF Treebank): • In SynTagRus (Apresjan et al. 2006), Russian sentences were assigned a morphological annotation and a surface-syntactic dependency tree (these annotations are available at http://www.ruscorpora.ru/). In addition, a lexical semantic annotation and lexical-functional annotation were announced by Boguslavsky (2014). While lexical semantic annotation consisted in disambiguating ambiguous words that have different lemmas and/or different part-of-speech tags, the aim of the lexical functional annotation was to identify LFs and their arguments and values in the texts. Semantic annotation, as described by Apresjan et al. (2006), does n"
2020.cl-3.3,W04-2704,0,0.0734599,"tic layer is not available. 3.4 Proposition Bank and Closely Related Resources The Proposition Bank (PropBank) project aimed at “adding a layer of predicate– argument information, or semantic role labels, to the syntactic structures of the Penn Treebank” (Palmer, Gildea, and Kingsbury 2005, page 71). The project started by marking clause nuclei composed of verbal predicates and their arguments (predicate– argument structure); PropBank annotation pointed to constituents in the original Penn Treebank annotation (Kingsbury and Palmer 2002). Later, “modifiers of event variables” were added (e.g., Babko-Malaya et al. 2004), broadening the predicate–argument 621 Computational Linguistics Volume 46, Number 3 structures with adjuncts. The main features of PropBank annotation as presented by Palmer, Gildea, and Kingsbury (2005) are: • Structure: directed acyclic graph, typically consisting of multiple unconnected components. The nodes can be ordered following the surface word order. • Nodes are constituents of the Penn Treebank surface tree (but in PropBanks of other languages, the surface structure may be a dependency tree). Predicates are represented by terminal nodes, arguments are represented by their highest-s"
2020.cl-3.3,P98-1013,0,0.728722,"Missing"
2020.cl-3.3,C14-1133,0,0.048993,"Missing"
2020.cl-3.3,W13-2322,0,0.766328,"ˇcuk 1965) SynTagRus, AnCora-UPF Functional Generative Description (FGD; Sgall 1967) Associated lexical resource Used in NLP apps Languages MT hi, ur, bn, te ECD MT ru, en, es, fr PDT, PCEDT PDT-VALLEX MT cs, en PropBank (Kingsbury and Palmer 2002) PropBank + NomBank + PDTB PropBank lex. many en, ar, zh, fi, hi, ur, fa, pt, tr, de, fr FrameNet-based approaches such as SALSA (Erk and Pado 2004) e.g. TIGER Treebank (for SALSA) FrameNet Enju (Yakushiji et al. 2005) Enju Treebank DELPH-IN (Oepen and Lønning 2006) DeepBank Sequoia (Candito et al. 2014) Sequoia Abstract Meaning Representation (AMR; Banarescu et al. 2013) AMR Bank Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013) English Wiki, parallel fiction, etc. Enhanced Universal Dependencies (Schuster and Manning 2016) Universal Dependencies ERG en, de, fr, ko IE en, zh many en, de, es, ja fr PropBank lex. many en, zh, pt, ko, vi, es, fr, de en, de, fr relation extraction ar, bg, cs, en, et, fi, it, lt, lv, nl, pl ru, sk, sv, ta, uk We ended up with a selection of frameworks, listed in Table 1 (and then described in detail in Section 3) in roughly chronological order of their introduction. Associated corpora and major lexicograph"
2020.cl-3.3,I08-2099,0,0.0593094,"an treebanks. Figure 1 shows an example sentence from the Hindi treebank with four karakas. ´ 8 As Przepiorkowski (2016) says: “Probably all modern linguistic theories assume some form of the Argument-Adjunct dichotomy, which may be traced back to Lucien Tesni`ere’s 1959 distinction between actants and circumstants.” 614 ˇ c´ıkov´a ˇ Zabokrtsk y, ´ Zeman, and Sevˇ Sentence Meaning Representations Across Languages Table 2 The six karaka relations of the Paninian syntax. Note that there is no karaka labeled k6, at least not in modern annotation schemes referring to the Paninian grammar, such as Begum et al. (2008). Relation number 6 denotes possession but it does not have the karaka status and is labeled r6. karta karma karana sampradaana apaadaana adhikarana k1 k2 k3 k4 k5 k7 doer / agent / subject patient / object instrument recipient / beneficiary source location in space or time Figure 1 A Hindi sentence with the first four karaka relations. The relations prefixed lwg are chunk-internal, i.e., they are not part of the main structure. • Structure: rooted tree.9 The nodes can be ordered following the surface word order; the order is partial if the tree contains an empty node. • Nodes generally corres"
2020.cl-3.3,P19-2012,0,0.0183343,"n and Nivre 2010). More recently, extensions from parsing into trees to parsing into more general graphs (which is supposed to be beneficial for downstream semantic processing) have been studied, too (Kuhlmann and Jonsson 2015). When it comes to the order of nodes in deep-syntactic representations, the discussion in the literature seems to be much less structured. The dominating approach is that 25 There are exceptions, such as systems in which tokens are artificially permuted in a specific way in order to facilitate parsing, for instance, by reducing the length of long-distance dependencies (Bommasani 2019). 26 A dependency tree is projective if and only if an edge from node x to node y implies that x is an ancestor of all nodes located linearly between x and y. 645 Computational Linguistics Volume 46, Number 3 the node order in deep-syntactic structures is not paid much attention to, but the structures are presented as ordered and it is assumed that the linear order can be induced from the linear order of corresponding surface strings. In fact, most deep-syntactic nodes are somehow anchored in the totally ordered sequence of sentence tokens, implicitly or explicitly. However, depending on a cho"
2020.cl-3.3,burchardt-etal-2006-salsa,0,0.0613297,"Missing"
2020.cl-3.3,F12-2024,0,0.0706851,"reference, the same lexical unit (node) serves as argument of multiple predicates. • Semantically ambiguous predicates and their valency frames are disambiguated. • Coordination: Conjunction is treated as the head, that is, like a predicate, in EDS; empty nodes are used where an overt conjunction is not available. In DM, coordination is transformed to a left-to-right chain (see the “Mel’ˇcuk/Moscow style” in Section 4.4). 3.8 Sequoia French Treebank In the Sequoia corpus, the deep-syntactic representation (Candito et al. 2014) was built on top of the existing surface-syntactic representation (Candito and Seddah 2012), which followed the annotation scheme used in the French Treebank (Abeill´e and Barrier 2004). The surface-syntactic annotation in the Sequoia corpus, originally based on constituent trees, was converted into dependencies and used for specification of the dependency-oriented deep-syntactic representation. The main features of the deep-syntactic representation can be summarized as follows (Candito and Perrier 2016): • Structure: directed graph; may contain cycles (Figure 10). The nodes can be ordered following the surface word order. • Nodes of the graph correspond to content words. Function w"
2020.cl-3.3,J14-1002,0,0.0606323,"Missing"
2020.cl-3.3,W08-1301,0,0.265466,"Missing"
2020.cl-3.3,W18-4912,0,0.0399968,"Missing"
2020.cl-3.3,A97-1021,0,0.126295,", 2006) have a narrower (and more theoretical) focus, dealing with dependency and valency issues. They also include review chapters on individual theories dealing with these concepts. We can also list many published attempts at comparing various features of deepsyntactic frameworks; however, to our knowledge each of them handles only a very limited number of existing frameworks and/or narrow scope of features compared. Hajiˇcov´a and Kuˇcerov´a (2002) compare three frameworks, namely, PropBank (Kingsbury and Palmer 2002), the LCS Database containing Lexical Conceptual Structures introduced by Dorr (1997), and the (pilot) annotation of the Prague Dependency Treebank (Hajiˇc 1998); a possible mapping among these three representations is sketched, with a focus on mapping semantic roles. A mapping from PropBank argument labels to 20 thematic roles used in VerbNet (Kipper, Dang, and Palmer 2000) is designed by Rambow et al. (2003). Ellsworth et al. (2004) compare PropBank, SALSA (Erk and Pado 2004), and FrameNet (Johnson et al. 2002), with a focus on several selected phenomena such as metaphor, support constructions, words with multiple meaning aspects, phrases realizing more than one semantic rol"
2020.cl-3.3,W17-0406,1,0.838651,"Missing"
2020.cl-3.3,W19-7717,1,0.92148,"Recursion Semantics from the DEPLH-IN project,2 and show a few similarities across the frameworks. Oepen et al. (2015) compare three approaches (DELPH-IN semantic annotation, Enju Predicate– Argument Structures, and the deep-syntactic annotation of the Prague Czech-English Dependency Treebank) in relation to the task of broad-coverage semantic dependency parsing in SemEval 2015. Kuhlmann and Oepen (2016) follow up on the SemEval paper and describe graph properties of the three frameworks from the SemEval task, plus CCG Dependencies and Abstract Meaning Representation. Zhu, Li, and Chiticariu (2019) take two approaches, namely, the semantic role labeling approach of the PropBank project and Abstract Meaning Representation, as a point of departure to propose and discuss which issues are to be covered by the universal semantic representation (with the focus on temporal features and modality). To the best of our knowledge, so far the most comprehensive comparison of existing semantic representation accounts (Abend and Rappoport 2017) puts together a list of central semantic phenomena (predicates, argument structure, semantic roles, coreference, anaphora, temporal and spatial relations, disc"
2020.cl-3.3,duran-aluisio-2012-propbank,0,0.0654259,"Missing"
2020.cl-3.3,erk-pado-2004-powerful,0,0.207601,"Missing"
2020.cl-3.3,W08-1105,0,0.0473235,"by Menezes and Richardson (2003) makes use of Logical Forms similar to those introduced by Jensen (1993); the expected advantage of using Logical Forms for such a purpose is that “additional generality obtained by normalizing both the lexical and syntactic form of examples, they may then be matched and applied more broadly when new sentences are translated.” A Logical Form is an unordered graph representing the relations among the most meaningful elements of a sentence. Nodes are identified by the lemma of a content word and directed, labeled arcs indicate the underlying semantic relations. • Filippova and Strube (2008) present an unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees. A tree of an original sentence is pruned, that is, edges are removed in an optimized way so that retained edges form a valid tree and their total edge weight is maximized. Finally, a shortened sentence is synthetized from the pruned compression tree. The trees to be pruned result from a transformation of surface dependency trees. During this transformation, function words like determiners, auxiliary verbs, and negative particles are removed from"
2020.cl-3.3,J11-3004,0,0.0788417,"Missing"
2020.cl-3.3,L18-1013,0,0.0594348,"Missing"
2020.cl-3.3,W07-2413,0,0.0172022,"Missing"
2020.cl-3.3,D14-1163,0,0.0273627,"1 are not well characterized. The herpesvirus encodes a functional Entity1 that activates Entity2. Entity1 can functionally cooperate to synergetically activate Entity2. The Entity1 play key roles by activating Entity2. Figure 7 Enju Predicate–Argument Structures of the sentence A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. (Adapted from Oepen et al. 2015). Although the direction and the labels of the edges are deep-syntactic, all surface words including function words are included as graph nodes. resulting graph is connected and acyclic (Hashimoto et al. 2014). Due to the direct correspondence between nodes and words, the nodes are totally ordered. • Words are converted to their base forms and augmented with their POS tags. Every word in a sentence is treated as a predicate, an argument, or both. • A predicate has a certain category and governs zero or more arguments. • Predicate categories are relatively fine-grained and rather syntactically oriented, such as verb arg123 relation for a verb that takes two NP objects or a verb that takes one NP object and one sentential complement, conj arg12 relation for subordinating conjunctions that take two ar"
2020.cl-3.3,P07-1077,0,0.0308269,"majority25 of approaches the linear precedence of tokens in the original sentence is simply preserved (be the writing system oriented left-to-right as in Latin-based scripts, or right-to-left or top-down as in Arabic and Japanese, respectively). There is a rich body of literature dealing with word order in surface-syntactic formalisms, for example, from the following perspectives: 1. What empirical evidence on word-order phenomena can be found in syntactically annotated data, such as in the studies on non-projectivity26 structures occurring in dependency treebanks (Kuhlmann and Nivre 2006), (Havelka 2007), or a typological view of Alzetta et al. (2018). Word-order phenomena that are non-trivial to handle and require, for example, using traces in constituency formalisms or allowing non-projectivities in dependency formalisms, sometimes also lead to introducing finer-grained categories such as mildly context-sensitive grammar (Joshi, Shanker, and Weir 1990) or mildly non-projective ´ dependency grammar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some graph-based mode"
2020.cl-3.3,W12-3602,0,0.409593,"than one semantic role, and non-local semantic roles. ˇ Zabokrtsk y´ (2005) points out several parallels between Meaning-Text theory ˇ (Zolkovskij and Mel‘ˇcuk 1965) and Functional Generative Description (Sgall 1967) in general, and more specifically between the deep-syntactic level of the former one and tectogrammatical level of the latter one. 1 There are also entirely different approaches to representing sentence meaning, such as vector space models; they are outside of the focus of our study. 606 ˇ c´ıkov´a ˇ Zabokrtsk y, ´ Zeman, and Sevˇ Sentence Meaning Representations Across Languages Ivanova et al. (2012) contrast seven annotation schemes for syntactic-semantic dependencies: CoNLL Syntactic Dependencies (Nivre et al. 2007), CoNLL PropBank Semantics (Surdeanu et al. 2008), Stanford Basic and Collapsed Dependencies (De Marneffe and Manning 2008), Enju Predicate–Argument Structures (Yakushiji et al. 2005), as well as Syntactic Derivation Trees and Minimal Recursion Semantics from the DEPLH-IN project,2 and show a few similarities across the frameworks. Oepen et al. (2015) compare three approaches (DELPH-IN semantic annotation, Enju Predicate– Argument Structures, and the deep-syntactic annotation"
2020.cl-3.3,kingsbury-palmer-2002-treebank,0,0.339136,"le future developments of the particular resources are outlined, usually by the authors of the resources themselves. ´ The volumes by Agel et al. (2003, 2006) have a narrower (and more theoretical) focus, dealing with dependency and valency issues. They also include review chapters on individual theories dealing with these concepts. We can also list many published attempts at comparing various features of deepsyntactic frameworks; however, to our knowledge each of them handles only a very limited number of existing frameworks and/or narrow scope of features compared. Hajiˇcov´a and Kuˇcerov´a (2002) compare three frameworks, namely, PropBank (Kingsbury and Palmer 2002), the LCS Database containing Lexical Conceptual Structures introduced by Dorr (1997), and the (pilot) annotation of the Prague Dependency Treebank (Hajiˇc 1998); a possible mapping among these three representations is sketched, with a focus on mapping semantic roles. A mapping from PropBank argument labels to 20 thematic roles used in VerbNet (Kipper, Dang, and Palmer 2000) is designed by Rambow et al. (2003). Ellsworth et al. (2004) compare PropBank, SALSA (Erk and Pado 2004), and FrameNet (Johnson et al. 2002), with a fo"
2020.cl-3.3,Q15-1040,0,0.0202955,"ar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some graph-based models are able to produce non-projective dependencies natively (McDonald and Satta 2007), while special techniques had to be developed to adapt transition-based models for non-projective parsing (Kuhlmann and Nivre 2010). More recently, extensions from parsing into trees to parsing into more general graphs (which is supposed to be beneficial for downstream semantic processing) have been studied, too (Kuhlmann and Jonsson 2015). When it comes to the order of nodes in deep-syntactic representations, the discussion in the literature seems to be much less structured. The dominating approach is that 25 There are exceptions, such as systems in which tokens are artificially permuted in a specific way in order to facilitate parsing, for instance, by reducing the length of long-distance dependencies (Bommasani 2019). 26 A dependency tree is projective if and only if an edge from node x to node y implies that x is an ancestor of all nodes located linearly between x and y. 645 Computational Linguistics Volume 46, Number 3 the"
2020.cl-3.3,P06-2066,0,0.0346833,"cy-oriented, then in a vast majority25 of approaches the linear precedence of tokens in the original sentence is simply preserved (be the writing system oriented left-to-right as in Latin-based scripts, or right-to-left or top-down as in Arabic and Japanese, respectively). There is a rich body of literature dealing with word order in surface-syntactic formalisms, for example, from the following perspectives: 1. What empirical evidence on word-order phenomena can be found in syntactically annotated data, such as in the studies on non-projectivity26 structures occurring in dependency treebanks (Kuhlmann and Nivre 2006), (Havelka 2007), or a typological view of Alzetta et al. (2018). Word-order phenomena that are non-trivial to handle and require, for example, using traces in constituency formalisms or allowing non-projectivities in dependency formalisms, sometimes also lead to introducing finer-grained categories such as mildly context-sensitive grammar (Joshi, Shanker, and Weir 1990) or mildly non-projective ´ dependency grammar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some"
2020.cl-3.3,J16-4009,0,0.196255,"Semantics (Surdeanu et al. 2008), Stanford Basic and Collapsed Dependencies (De Marneffe and Manning 2008), Enju Predicate–Argument Structures (Yakushiji et al. 2005), as well as Syntactic Derivation Trees and Minimal Recursion Semantics from the DEPLH-IN project,2 and show a few similarities across the frameworks. Oepen et al. (2015) compare three approaches (DELPH-IN semantic annotation, Enju Predicate– Argument Structures, and the deep-syntactic annotation of the Prague Czech-English Dependency Treebank) in relation to the task of broad-coverage semantic dependency parsing in SemEval 2015. Kuhlmann and Oepen (2016) follow up on the SemEval paper and describe graph properties of the three frameworks from the SemEval task, plus CCG Dependencies and Abstract Meaning Representation. Zhu, Li, and Chiticariu (2019) take two approaches, namely, the semantic role labeling approach of the PropBank project and Abstract Meaning Representation, as a point of departure to propose and discuss which issues are to be covered by the universal semantic representation (with the focus on temporal features and modality). To the best of our knowledge, so far the most comprehensive comparison of existing semantic representati"
2020.cl-3.3,W19-3317,0,0.0319945,"nology, implicit terminals are empty leaf nodes. • A unit may participate in more than one relation; that is why the graph is not necessarily tree (Figure 14). • Relations are labeled with coarse-grained categories; the inventory contains 12 values such as P – Process, A – Participant, D – Adverbial, E – Elaborator, and N – Connector. 16 See https://nert-nlp.github.io/AMR-Bibliography/. In some languages this required language-specific modifications of the annotation scheme because of phenomena that have no analogy in English, viz. coreference of noun classifiers in Vietnamese as discussed by Linh and Nguyen (2019), or third-person clitic pronouns in Spanish as discussed by Migueles-Abraira, Agerri, and de Ilarraza (2018). 17 Unlike some other frameworks, in UCCA the term relation does not mean an edge. It is one of two types of concepts of which an utterance is constructed, the other being an entity. 633 Computational Linguistics Volume 46, Number 3 Figure 13 UCCA graph of the sentence John kicked his ball (Adapted from Abend and Rappoport 2013). The non-scene unit his ball is represented as a subgraph with one non-terminal and two terminal nodes; the C edge marks ball as the “center,” while E means “e"
2020.cl-3.3,L16-1147,0,0.0463032,"Missing"
2020.cl-3.3,W07-2216,0,0.0195375,"phenomena that are non-trivial to handle and require, for example, using traces in constituency formalisms or allowing non-projectivities in dependency formalisms, sometimes also lead to introducing finer-grained categories such as mildly context-sensitive grammar (Joshi, Shanker, and Weir 1990) or mildly non-projective ´ dependency grammar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some graph-based models are able to produce non-projective dependencies natively (McDonald and Satta 2007), while special techniques had to be developed to adapt transition-based models for non-projective parsing (Kuhlmann and Nivre 2010). More recently, extensions from parsing into trees to parsing into more general graphs (which is supposed to be beneficial for downstream semantic processing) have been studied, too (Kuhlmann and Jonsson 2015). When it comes to the order of nodes in deep-syntactic representations, the discussion in the literature seems to be much less structured. The dominating approach is that 25 There are exceptions, such as systems in which tokens are artificially permuted in"
2020.cl-3.3,W04-2705,0,0.406758,"complete the transaction by year-end. The ARGM-TMP edge between expects and by year-end seems disputable but it appears in the annotated data, so we include it, too. Traces and their antecedents are connected to chains identifying grammatical coreference within sentence boundaries. Note, however, that the textual coreference between it and the thrift holding company is not annotated. Pustejovsky et al. (2005) announced a project of merging the English PropBank with four other resources that focused on other parts considered as belonging to sentence meaning in English, namely with: • NomBank (Meyers et al. 2004), in which argument structure was assigned with eventive nouns occurring in PropBank (data of the Wall Street Journal Corpus of the Penn Treebank). First, “markable” noun instances were identified among common nouns, that is, eventive nouns that are accompanied by a PropBank-defined argument or adjunct. In each noun phrase with such a noun, the head was identified and its arguments and adjuncts were marked and assigned a semantic role label from the PropBank label set (ARG0 to ARG5 and different ARGM labels; see https://nlp.cs.nyu.edu/meyers/NomBank.html for detailed annotation instructions)."
2020.cl-3.3,L18-1486,0,0.0378796,"Missing"
2020.cl-3.3,W13-3724,0,0.0651479,"Missing"
2020.cl-3.3,W04-2703,0,0.241655,"Missing"
2020.cl-3.3,miltsakaki-etal-2004-penn,0,0.424125,"ach noun phrase with such a noun, the head was identified and its arguments and adjuncts were marked and assigned a semantic role label from the PropBank label set (ARG0 to ARG5 and different ARGM labels; see https://nlp.cs.nyu.edu/meyers/NomBank.html for detailed annotation instructions). • Penn Discourse Treebank (PDTB), in which the relations between propositions (i.e., meanings of individual clauses made up of a predicate–argument structure and related adjuncts) are annotated. Propositions are marked as arguments with regard to discourse connectives, which are either explicit or implicit (Miltsakaki et al. 2004a, 2004b). • TimeBank (Pustejovsky et al. 2003), in which temporal features of propositions (expressed by temporal adjuncts, temporal prepositions and connectives, tensed verbs, etc.) and temporal relations between propositions are annotated. 623 Computational Linguistics • Volume 46, Number 3 Coreference Annotation created at the University of Essex, which contained texts from a subset of the Penn Treebank (Poesio and Vieira 1998) and the Gnome Corpus (Poesio 2004) annotated with coreference relations. Merging these resources meant that the clause nuclei composed of verbal predicates and thei"
2020.cl-3.3,L16-1606,0,0.0297999,"Missing"
2020.cl-3.3,S14-2056,1,0.857807,"Missing"
2020.cl-3.3,2020.lrec-1.497,1,0.868476,"Missing"
2020.cl-3.3,L16-1377,0,0.0331719,"Missing"
2020.cl-3.3,K19-2001,0,0.212083,"Missing"
2020.cl-3.3,S15-2153,1,0.913576,"Missing"
2020.cl-3.3,W04-2327,0,0.138285,"ated. Propositions are marked as arguments with regard to discourse connectives, which are either explicit or implicit (Miltsakaki et al. 2004a, 2004b). • TimeBank (Pustejovsky et al. 2003), in which temporal features of propositions (expressed by temporal adjuncts, temporal prepositions and connectives, tensed verbs, etc.) and temporal relations between propositions are annotated. 623 Computational Linguistics • Volume 46, Number 3 Coreference Annotation created at the University of Essex, which contained texts from a subset of the Penn Treebank (Poesio and Vieira 1998) and the Gnome Corpus (Poesio 2004) annotated with coreference relations. Merging these resources meant that the clause nuclei composed of verbal predicates and their arguments, as captured in PropBank, were broadened with the argument structures for instances of common nouns (NomBank) and, finally, the isolated islands were connected with discourse relations (PDTB). By also having an explicit temporal and coreference annotation, the initially limited focus of PropBank was substantially extended, providing a more complex semantic annotation than available in the particular resources. A more general goal of the merging project w"
2020.cl-3.3,J98-2001,0,0.0194006,"ment structure and related adjuncts) are annotated. Propositions are marked as arguments with regard to discourse connectives, which are either explicit or implicit (Miltsakaki et al. 2004a, 2004b). • TimeBank (Pustejovsky et al. 2003), in which temporal features of propositions (expressed by temporal adjuncts, temporal prepositions and connectives, tensed verbs, etc.) and temporal relations between propositions are annotated. 623 Computational Linguistics • Volume 46, Number 3 Coreference Annotation created at the University of Essex, which contained texts from a subset of the Penn Treebank (Poesio and Vieira 1998) and the Gnome Corpus (Poesio 2004) annotated with coreference relations. Merging these resources meant that the clause nuclei composed of verbal predicates and their arguments, as captured in PropBank, were broadened with the argument structures for instances of common nouns (NomBank) and, finally, the isolated islands were connected with discourse relations (PDTB). By also having an explicit temporal and coreference annotation, the initially limited focus of PropBank was substantially extended, providing a more complex semantic annotation than available in the particular resources. A more ge"
2020.cl-3.3,P13-1051,1,0.926379,"athesis. For instance, a noun with the preposition by with a passive verb is assigned the final grammatical function of an object in the surface structure. When displayed in a linear sentence, all words of the sentence are parts of the surface-syntactic representation and are connected with final grammatical functions. Those nodes that correspond to content words in the sentence enter the deep-syntactic representation and are assigned also a canonical grammatical function; see Figure 10. 3.9 Abstract Meaning Representation Abstract Meaning Representations (AMRs) introduced by Banarescu et al. (2013) represent sentences as graphs in which non-leaf nodes stand for variables and only leaf nodes capture lexical content (i.e., only leaves are labeled with concepts). An example of such structure is depicted in Figure 12. Compared with most other approaches under our survey, the correspondence between AMR structures and surface-syntactic structures such as surface dependency trees is relatively limited, as the origins of AMR go back rather to a knowledge representation tradition. • Structure: A directed graph, typically acyclic, although cycles are not completely excluded (Kuhlmann and Oepen 20"
2020.cl-3.3,W19-3319,0,0.207272,"Missing"
2020.cl-3.3,prasad-etal-2008-penn,0,0.0567831,"dicate (see Section 4.2.2), coordinating conjunctions are often part of the sentence meaning representation (similarly to content words) and different accounts of coordination are documented in Section 4.4. 652 ˇ c´ıkov´a ˇ Zabokrtsk y, ´ Zeman, and Sevˇ Sentence Meaning Representations Across Languages Unlike subordination and coordination as intrasentential relations, relations between propositions that are separated into different sentences (inter-sentential relations) are omitted in most approaches or, if considered, they are annotated at a separate layer. • PDTB (Miltsakaki et al. 2004b; Prasad et al. 2008) is a project related to Penn Treebank and PropBank (cf. Section 3.4). It annotates the Wall Street Journal Section of the Penn Treebank with discourse relations. If an explicit discourse connective is found in the sentence or between two sentences, it is assigned a sense tag. If no discourse connective is present, a connective expression is added into the structure (being encoded as a lexical item or with a special label). Discourse relations are assigned between clauses in a sentence and between each successive pair of sentences within paragraphs. • In Prague Dependency Treebank (Section 3.3"
2020.cl-3.3,W04-1908,0,0.172299,"Missing"
2020.cl-3.3,W05-0302,0,0.0368945,"tence Meaning Representations Across Languages Figure 5 PropBank annotation over the constituents of the Penn Treebank for the sentence The thrift holding company said it expects to obtain regulatory approval and complete the transaction by year-end. The ARGM-TMP edge between expects and by year-end seems disputable but it appears in the annotated data, so we include it, too. Traces and their antecedents are connected to chains identifying grammatical coreference within sentence boundaries. Note, however, that the textual coreference between it and the thrift holding company is not annotated. Pustejovsky et al. (2005) announced a project of merging the English PropBank with four other resources that focused on other parts considered as belonging to sentence meaning in English, namely with: • NomBank (Meyers et al. 2004), in which argument structure was assigned with eventive nouns occurring in PropBank (data of the Wall Street Journal Corpus of the Penn Treebank). First, “markable” noun instances were identified among common nouns, that is, eventive nouns that are accompanied by a PropBank-defined argument or adjunct. In each noun phrase with such a noun, the head was identified and its arguments and adjun"
2020.cl-3.3,L16-1376,0,0.418969,"ructured sentence meaning representations (deep-syntactic representations),1 in order to demonstrate that there are basic principles shared by most (if not all) of them, on the one hand, and specific decisions, on the other. The shared principles, being considered the core elements of deep-syntactic representations, will be reformulated into a handful of humble suggestions for a discussion on a unifying approach to sentence meaning. This perspective justifies the inclusion of the Universal Dependencies project, though currently not containing a proper sentence meaning annotation (Schuster and Manning 2016), since the project sets trends in carrying out a unified annotation at the surface-syntactic level. 1.2 Existing Surveys These days, one can find comprehensive handbooks collecting a number of descriptions of various linguistic issues and language data resources. The recent Handbook of Linguistic Annotation (Ide and Pustejovsky 2017) provides an overview of annotation approaches applied in several tens of data resources capturing a wide range of language phenomena. Design decisions on the annotation schemes, evolution, and possible future developments of the particular resources are outlined,"
2020.cl-3.3,L16-1680,0,0.0564827,"Missing"
2020.cl-3.3,K17-3009,0,0.0599691,"Missing"
2020.cl-3.3,W08-2121,0,0.182274,"Missing"
2020.cl-3.3,taule-etal-2008-ancora,0,0.0776985,"Missing"
2020.cl-3.3,W11-0403,0,0.0431887,"Missing"
2020.cl-3.3,W08-0325,1,0.782768,"Missing"
2020.cl-3.3,W12-2511,0,0.0307876,"Missing"
2020.cl-3.3,K18-2001,1,0.890212,"Missing"
2020.cl-3.3,W17-6944,0,0.064815,"Missing"
2020.cl-3.3,W19-3320,0,0.0264804,"Missing"
2020.conll-shared.1,W13-2322,0,0.190653,"owards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that mod (domain) mod (domain) other (ARG1)-of exemplify-01 ARG0 and op1 cotton soybean op2 op3 rice op4 et-cetera Figure 4: Abstract Meaning Representation (AMR) for the running example A simila"
2020.conll-shared.1,W15-0128,1,0.796384,"des). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). Elementary Dependency Structures The EDS graphs (Oepen and Lønning, 2006) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015).3 Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) encode English Resource Semantics in a variablefree semantic dependency graph—not limited to bi-lexical dependencies—where graph nodes correspond to logical predications and edges to labeled argument positions. The EDS conversion from underspecified logical forms to directed graphs discards partial information on semantic scope from the full ERS, which makes these graphs abstractly— if not linguistically—similar to Abstract Meaning Representation (see below). Nodes in EDS are in principle independent of surface lexical units, b"
2020.conll-shared.1,D16-1134,1,0.827452,"(UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency fr"
2020.conll-shared.1,W13-0101,1,0.816811,"imilar 〈2:9〉 sempos adj.denot ACT #Benef sempos x EXT almost 〈23:29〉 sempos adv.denot.grad.neg coref.gram #Gen sempos x RSTR other 〈53:58〉 sempos adj.denot Figure 2: Semantic dependency graphs for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Prague Tectogrammatical Graphs (PTG). In addition to node properties, visualized similarly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al.,"
2020.conll-shared.1,E17-2039,1,0.896998,"Missing"
2020.conll-shared.1,2020.emnlp-main.195,0,0.0866269,"with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite"
2020.conll-shared.1,2020.conll-shared.2,1,0.940067,"Structure (DRS), the meaning representations at the core of Discourse Representation Theory (DRT; Kamp and Reyle, 1993; Van der Sandt, 1992; Asher, 1993). DRSs can model many challenging semantic phenomena including quantifiers, negation, scope, pronoun resolution, presupposition accommodation, and discourse structure. Moreover, they are directly translatable into first-order logic formulas to account for logical inference. DRG used in the shared task represents a type of graph encoding of DRS that makes the graphs structurally as close as possible to the structures found in other frameworks; Abzianidze et al. (2020) provide more details on the design choices in the DRG encoding. The source DRS annotations are taken from data release 3.0.0 of the Parallel Meaning Bank (PMB; Abzianidze et al., 2017; Bos et al., 2017).6 Although the annotations in the PMB are compositionally derived from lexical semantics, anchoring information is not explicit in its DRSs; thus, (like AMR) the DRG framework formally instantiates Flavor (2) of meaning representations. The DRG of the running example is given in Figure 5. The concepts (vissualized as oval shapes) are represented by WordNet 3.0 senses and semantic roles (in dia"
2020.conll-shared.1,W19-1201,1,0.906261,"Missing"
2020.conll-shared.1,2020.lrec-1.234,1,0.763502,"s (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP task series, to go well beyond the impressionistic observations from §3 and ideally lead to contrastive refinement across linguistic sc"
2020.conll-shared.1,2020.conll-shared.7,1,0.75652,"Missing"
2020.conll-shared.1,D19-1393,0,0.0309981,"nt alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR"
2020.conll-shared.1,2020.findings-emnlp.89,0,0.016475,"ebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (20"
2020.conll-shared.1,2020.acl-main.119,0,0.179058,"s similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that do not enforce strict correspondences between tokens in the input sentence and the concepts in meaning representation graphs. The iterative inference framework is also based on an encoder–decoder architecture. The encoder takes the sentence as input and computes contextualized token embeddings that are used as text memory by a decoder that iteratively predicts the next node given the text memory and a predicted parent node in the partially constructed graph memory at the previous time step, and then identifies the parent node for the newly pre"
2020.conll-shared.1,2020.conll-shared.6,0,0.0921787,"guage per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu 1 Background and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the correspon"
2020.conll-shared.1,P13-2131,0,0.187673,"Missing"
2020.conll-shared.1,W11-2927,1,0.757531,"was not formally enforced. 5 Evaluation Following the previous edition of the shared task, the official MRP metric for the task is the microaverage F1 score across frameworks over all tuple types that encode ‘atoms’ of information in MRP graphs. The cross-framework metric uniformly evaluates graphs of different flavors, regardless of a specific framework exhibiting (a) labeled or unlabeled nodes or edges, (b) nodes with or without anchors, and (c) nodes and edges with optional properties and attributes, respectively (see Table 4). The MRP metric generalizes earlier frameworkspecific metrics (Dridan and Oepen, 2011; Cai and Knight, 2013; Hershcovich et al., 2019a) in terms of decomposing each graph into sets of typed tuples, as indicated in Figure 6. To quantify graph similarity in terms of tuple overlap, a correspondence relation between the nodes of the goldstandard and system graphs must be determined. Adapting a search procedure for the NP-hard maximum common edge subgraph (MCES) isomorphism problem, the MRP scorer will search for the node-to-node correspondence that maximizes the intersection of tuples between two graphs, where node identifiers (m and n in Figure 6) act like variables that can be e"
2020.conll-shared.1,K19-2007,0,0.0722366,"ground and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the corresponding (but richer) frameworks in 2020, EDS and PTG, respectively, and the original bi-lexical semantic dependency graphs remain independently available (Oepen et al., 2015). 1 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 1–22 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics (a) a unifying formal model over different semantic graph banks (§2)"
2020.conll-shared.1,1997.iwpt-1.10,0,0.772333,"Missing"
2020.conll-shared.1,W19-1202,0,0.0200921,"ntation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP"
2020.conll-shared.1,K19-2016,0,0.0872997,"UCCA, and AMR. This allows a comparison on nearly equal grounds: as Table 9 shows, in terms of LPPS F1 , the state-of-the-art has substantially improved for EDS and AMR parsing, but stayed the same for UCCA. However, as mentioned in §6, remote edge detection for UCCA improved substantially, though it carries only a small weight in terms of overall scores due to the scarcity of remote edges. For EDS, the strongest results were obtained in the MRP 2019 official competition by SUDA– Alibaba (Zhang et al., 2019c). However, in the post-evaluation stage, they were outperformed by the Peking system (Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard"
2020.conll-shared.1,D19-1278,0,0.0153051,"Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets"
2020.conll-shared.1,N18-2020,1,0.822488,"ed on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency frame and, thus, correspond more closely to the n"
2020.conll-shared.1,P19-4007,0,0.0514516,"Missing"
2020.conll-shared.1,2020.acl-main.629,0,0.125349,"Missing"
2020.conll-shared.1,N18-1104,0,0.0512423,"tudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite limited training data in the case of UCCA and DRG. Furthermore, the evaluation sets for most of the frameworks comprise different text types and subject matters—offering some hope of robustness to"
2020.conll-shared.1,N18-1000,0,0.197513,"Missing"
2020.conll-shared.1,K19-2006,0,0.10434,"been included in the CoNLL 2009 Shared Task on Semantic Role Labeling (Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019),"
2020.conll-shared.1,S19-2002,0,0.0495413,"Missing"
2020.conll-shared.1,hajic-etal-2012-announcing,0,0.357504,"Missing"
2020.conll-shared.1,kingsbury-palmer-2002-treebank,0,0.452155,"o distinguish different types of the underlying DRS elements. logy broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 4 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 1. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 4 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
2020.conll-shared.1,J16-4009,1,0.877647,"d (d) increased crossfertilization of parsing approaches (§7). 2 tute ordered graphs. A natural way to visualize a bi-lexical dependency graph is to draw its edges as semicircles in the halfplane above the sentence. An ordered graph is called noncrossing if in such a drawing, the semicircles intersect only at their endpoints (this property is a natural generalization of projectivity as it is known from dependency trees). A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. Increased terminological uniformity and guidance in how to navigate this rich and diverse landscape are among the desirable side-effects of the MRP task series. The following paragraphs provide semi-formal definiti"
2020.conll-shared.1,P17-1104,1,0.878575,".78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRG"
2020.conll-shared.1,P19-1232,0,0.0175512,"Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Compara"
2020.conll-shared.1,P18-1035,1,0.856893,"e invited to develop parsing systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a"
2020.conll-shared.1,W19-6202,0,0.0118155,"sed factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by"
2020.conll-shared.1,S19-2001,1,0.83448,"Missing"
2020.conll-shared.1,2020.findings-emnlp.288,0,0.0119332,"with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general"
2020.conll-shared.1,P19-1450,0,0.182109,"istinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four"
2020.conll-shared.1,P18-1040,0,0.0323045,", summarization, or text generation. Maybe equally importantly, the MRP task design capitalizes on uniformity of representations and evaluation, enabling resource creators and parser developers to more closely (inter)relate representations and parsing approaches across a diverse range of semantic graph frameworks. This facilitates DRG is a novel graph representation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also m"
2020.conll-shared.1,W19-1203,0,0.0982514,"he UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets are language-neutral. The mismatch between German tokens and English lemmas of senses must be expected to add additional complexity to"
2020.conll-shared.1,2020.acl-main.607,0,0.0482936,"Missing"
2020.conll-shared.1,K19-2001,1,0.563644,"Missing"
2020.conll-shared.1,P18-1037,0,0.0412388,"tokens and English lemmas of senses must be expected to add additional complexity to German DRG parsing. Direct comparison to non-MRP results is impossible: we are using a new version of AMRbank. Gold-standard tokenization is not provided for any of the frameworks. We use the MRP scorer. However, general trends appear consistent with recent developments. Pretrained embeddings and crosslingual transfer help; but multi-task learning less so. There is yet progress to be made in sharing information between parsers for different frameworks and making better use of their overlap. Prior to MRP 2019, Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. Lyu et al. (2020) recently improved the latent alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on al"
2020.conll-shared.1,K19-2003,1,0.845577,"on marks in the left or right periphery of a normalized anchor. Assuming the string Oh no! as a hypothetical parser input, the following anchorings will all be considered equivalent: {h0 : 6i}, {h0 : 2i, h3 : 6i}, {h0 : 1i, h1 : 6i}, and {h0 : 5i}. 6 Six teams submitted parser outputs to the shared task within the official evaluation period. In addition, we received two submissions after the submission deadline, which we mark as ‘unofficial’. We further include results from an additional ‘reference’ system by one of the task co-organizers, namely EDS outputs from the grammar-based ERG parser (Oepen and Flickinger, 2019). Table 5 presents an overview of the participating systems and the tracks and frameworks they submitted results for. All official systems submitted results for the cross-framework track (across all frameworks), and additionally five of them submitted results to the cross-lingual track as well (where TJU-BLCU did not submit UCCA parser outputs in the cross-lingual track). We note that the shared task explicitly allowed partial submissions, in order to lower the bar for participation (which is no doubt substantial). Two of the teams—ISCAS and TJUBLCU—declined the invitation to submit a system d"
2020.conll-shared.1,P14-5010,0,0.0027367,"ces of ‘raw’ sentence strings and (b) in pre-tokenized, partof-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premium-quality morpho-syntactic dependency analyses were provided to participants, called the MRP 2020 companion parses. These parses were obtained using a prerelease of the ‘future’ UDPipe architecture (Straka, 2018; Straka and Strakov´a, 2020), trained on available gold-standard UD 2.x treebanks, for English augmented with conversions from PTB-style annotations in the WSJ and OntoNotes corpora (Hovy et al., 2006), using the UD-style CoreNLP 4.0 tokenizer (Manning et al., 2014) and jack-knifing where appropriate (to avoid overlap with the texts underlying the MRP semantic graphs). Table 4: Different tuple types per framework. on-line CodaLab infrastructure. Teams were allowed to make repeated submissions, but only the most recent successful upload to CodaLab within the evaluation period was considered for the official, primary ranking of submissions. Task participants were encouraged to process all inputs using the same general parsing system, but—owing to inevitable fuzziness about what constitutes ‘one’ parser—this constraint was not formally enforced. 5 Evaluatio"
2020.conll-shared.1,S15-2153,1,0.842966,"Missing"
2020.conll-shared.1,J93-2004,0,0.0699436,", as fully lexically anchored and wholly unanchored, respectively, leading to the categorization of mixed forms of anchoring as Flavor (1), and allow for the presence of ordered graphs, in principle at least, at all levels of the hierarchy.2 Meaning Representation Frameworks The shared task combines five distinct frameworks for graph-based meaning representation, each with its specific formal and linguistic assumptions. This section reviews the frameworks and presents English example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 4 are prewhere unanchored nodes for unexpressed material beyond the surface string can be postulated (Schuster and Manning, 2016). Whether or not these nodes occupy a well-defined position in the otherwise total order of basic UD nodes remains an open questi"
2020.conll-shared.1,S16-1166,0,0.0329712,"context of the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the su"
2020.conll-shared.1,S14-2008,1,0.896688,"Missing"
2020.conll-shared.1,S17-2090,0,0.0310415,"the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although mo"
2020.conll-shared.1,2020.conll-shared.4,0,0.136232,"(Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word"
2020.conll-shared.1,2020.conll-shared.8,0,0.415434,"Missing"
2020.conll-shared.1,P17-1186,0,0.154444,"Missing"
2020.conll-shared.1,2020.lrec-1.497,1,0.867008,"Missing"
2020.conll-shared.1,W19-1204,0,0.0450391,"Missing"
2020.conll-shared.1,W16-6401,0,0.0666192,"Missing"
2020.conll-shared.1,Q18-1043,1,0.868093,"Missing"
2020.conll-shared.1,2020.conll-shared.5,0,0.310999,"impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1"
2020.conll-shared.1,N18-2040,1,0.887249,"Missing"
2020.conll-shared.1,L16-1376,0,0.0817331,"Missing"
2020.conll-shared.1,D17-1129,1,0.843784,"contextualized token embeddings with XLM-R (Conneau et al., 2019) on the encoder side, and then on the decoder side, uses separate attention heads to predict the node labels, identify anchors for nodes, and predict edges between nodes, as well as edge labels. Because the label set for nodes is typically very large, rather than predicting the node labels directly, the PERIN system reduces the search space by predicting ‘relative rules’ that can be used to map surface token strings to node labels in meaning representation graphs, an idea that is similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that d"
2020.conll-shared.1,P19-1454,0,0.0201296,"extualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by the fact that two of the 2020 frameworks (PTG and DR"
2020.conll-shared.1,D18-1263,0,0.0141923,"ng systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction"
2020.conll-shared.1,2020.emnlp-main.196,0,0.450632,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.wmt-1.104,0,0.540185,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.lt4hala-1.20,0,0.0828616,"Missing"
2020.conll-shared.1,2020.conll-shared.3,1,0.64593,"Missing"
2020.conll-shared.1,W15-3502,1,0.865834,"arly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges fr"
2020.conll-shared.1,P19-1009,0,0.173641,"Missing"
2020.conll-shared.1,D19-1392,0,0.119567,"Missing"
2020.conll-shared.1,N18-1063,1,0.899522,"Missing"
2020.conll-shared.1,K19-2014,0,0.104365,"Missing"
2020.conll-shared.1,P18-1016,1,0.879706,"true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient),"
2020.conll-shared.3,hajic-etal-2012-announcing,0,0.286415,"Missing"
2020.conll-shared.3,S14-2056,1,0.897368,"the CoNLL 2020 shared task (Oepen et al., 2020) is based mostly on the tectogrammatical layer; however, references have to be followed all the way down to the word layer in order to provide anchoring of graph nodes in the underlying text. The shared task featured PTG data in two languages: English and Czech. The English data was taken from the same sources as in the previous shared task (CoNLL MRP 2019, Oepen et al. 2019); however, a different conversion procedure had been used in the previous task, leading to different (and simpler) target graphs, known as Prague Semantic Dependencies (PSD, Miyao et al. 2014). The source text originates in the Wall Street Journal portion of the Penn TreeBank and the source • paratactic structures such as coordination are encoded using technical dependencies, special edge labels and attributes; • coreference links are encoded as node attributes instead of being treated as edges. As the representations in the shared task are not restricted to trees, additional edges were added to more directly encode paratactic structures and coreference. The resulting structures are called Prague Tectogrammatical Graphs (PTG). 2 Graph Properties and Anchoring The typical node in a"
2020.conll-shared.3,2020.conll-shared.1,1,0.64593,"Missing"
2020.conll-shared.3,K19-2001,1,0.775501,"Missing"
2020.iwpt-1.16,2020.iwpt-1.21,0,0.391189,". This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and st"
2020.iwpt-1.16,2020.iwpt-1.24,0,0.181901,"dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an"
2020.iwpt-1.16,W17-6507,1,0.792227,"Missing"
2020.iwpt-1.16,2020.iwpt-1.20,0,0.157709,"Missing"
2020.iwpt-1.16,2020.iwpt-1.23,0,0.193264,"on that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependen"
2020.iwpt-1.16,2020.iwpt-1.26,0,0.254795,"Missing"
2020.iwpt-1.16,2020.iwpt-1.19,0,0.133706,"is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an enhancement originates in the de-lexicalized edge label. A wide range of parsers (graph-based biaffine, transitionbased), and pre-trained embeddings (XLM-R or mBERT or language specific BERTs) is used. Finally, several teams (Emory NLP (He and Choi, 2020), ShanghaiTech (Wang et al., 2020), ADAPT, Køpsala (Hershcovich et al., 2020), RobertNLP (Gr¨unewald and Friedrich, 2020)) do not use conversion (or only to restore de-lexicalized labels), but instead use a graph-based parser that can directly produce enhanced dependency graphs. The output of the graph-based parser is often combined with information from a standard UD parser to ensure well-formedness and connectedness of the resulting graph. 6 8 Results We include two baseline results:10 baseline1 was obtained by taking gold basic UD trees and copying these into the enhanced layer without any"
2020.iwpt-1.16,2020.iwpt-1.18,0,0.167819,"its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-"
2020.iwpt-1.16,P80-1024,0,0.722615,"Missing"
2020.iwpt-1.16,2020.iwpt-1.17,0,0.156172,"Missing"
2020.iwpt-1.16,D19-1279,0,0.19421,"pendent with dependency label L2 has an empty node i2.1 as parent which itself is an L1 dependent of i1, its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into"
2020.iwpt-1.16,D19-1277,0,0.0922493,"Missing"
2020.iwpt-1.16,2020.lrec-1.497,1,0.848313,"Missing"
2020.iwpt-1.16,W18-6012,0,0.132583,"hether the same is true for enhanced dependency parsing. The challenge is both formal and practical. First, the enhanced representation is a connected graph, possibly containing cycles, while previous work on dependency parsing mostly dealt with rooted trees. Second, as some dependency labels incorporate the lemma of certain dependents and other additional information, the set of labels to be predicted is much larger and language-dependent. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Schuster et al., 2017; Nivre et al., 2018). Moreover, most state of the art work in dependency parsing uses a graph-based approach, where the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen et al., 2014, 2015, 2019) and could even lead to state of the art performance for downstream applications as shown by the results of the Extrinsic Evaluation Parsing shared-task (Oepen et al., 2017). 151 Proceedings of the 16th Internation"
2020.iwpt-1.16,K19-2001,0,0.0535138,"Missing"
2020.iwpt-1.16,S15-2153,1,0.852196,"Missing"
2020.iwpt-1.16,S14-2008,1,0.812645,"e set of labels to be predicted is much larger and language-dependent. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Schuster et al., 2017; Nivre et al., 2018). Moreover, most state of the art work in dependency parsing uses a graph-based approach, where the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen et al., 2014, 2015, 2019) and could even lead to state of the art performance for downstream applications as shown by the results of the Extrinsic Evaluation Parsing shared-task (Oepen et al., 2017). 151 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 151–161 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics 3 Enhanced Universal Dependencies conj conj UD version 22 states that apart from the morphological and basic dependency annotation layers, strings may be annotated with an additional, enhanced, dependency layer"
2020.iwpt-1.16,2020.acl-demos.14,0,0.0565369,"node i2.1 as parent which itself is an L1 dependent of i1, its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by"
2020.iwpt-1.16,L16-1680,0,0.0883819,"Missing"
2020.iwpt-1.16,2020.iwpt-1.22,0,0.144364,"combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an enhancement originates in the de-lexicalized edge label. A wide range of parsers (graph-based biaffine, transitionbased), and pre-trained embeddings (XLM-R or mBERT or language specific BERTs) is used. Finally, several teams (Emory NLP (He and Choi, 2020), ShanghaiTech (Wang et al., 2020), ADAPT, Køpsala (Hershcovich et al., 2020), RobertNLP (Gr¨unewald and Friedrich, 2020)) do not use conversion (or only to restore de-lexicalized labels), but instead use a graph-based parser that can directly produce enhanced dependency graphs. The output of the graph-based parser is often combined with information from a standard UD parser to ensure well-formedness and connectedness of the resulting graph. 6 8 Results We include two baseline results:10 baseline1 was obtained by taking gold basic UD trees and copying these into the enhanced layer without any modifications. Baseline2 uses UDPi"
2020.iwpt-1.16,K18-2001,1,0.884896,"Missing"
2020.lrec-1.497,de-marneffe-etal-2006-generating,1,\N,Missing
2020.lrec-1.497,zeman-2008-reusable,1,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2014-universal,1,\N,Missing
2020.lrec-1.497,W08-1301,1,\N,Missing
2020.lrec-1.497,petrov-etal-2012-universal,0,\N,Missing
2020.lrec-1.497,P13-1051,1,\N,Missing
2020.lrec-1.497,P15-2111,0,\N,Missing
2020.lrec-1.497,L16-1376,1,\N,Missing
2020.lrec-1.497,L16-1262,1,\N,Missing
2020.lrec-1.497,W18-6012,1,\N,Missing
2020.lrec-1.637,de-marneffe-etal-2014-universal,0,0.0450277,"Missing"
2020.lrec-1.637,2020.lrec-1.497,1,0.886652,"Missing"
2020.lrec-1.637,petrov-etal-2012-universal,0,0.129093,"Missing"
2020.lrec-1.637,K17-3009,0,0.0543396,"Missing"
2020.lrec-1.637,L16-1680,0,0.0726695,"e The Yorùbá Bible is our primary source of data. This choice is opportunistic: the Bible does not pose copyright issues, and it is a massively parallel text, allowing for cross-lingual transfer techniques. (Oluokun, 2018) provided a preliminary automatic annotation of the data through projection from Bible text in other languages, annotated according to the UD guidelines. A subset of 100 sentences was manually checked and released. We re-checked and corrected this initial dataset, and doubled the size of the corpus by adding new annotated sentences. These were first processed using a parser (Straka et al., 2016) trained on the first 100 sentences, then a fair amount of post-editing was performed using the tree editor TrEd (Pajas and Fabian, 2000). Well-formedness of the data is checked with the Python validation script maintained by the Universal Dependencies Consortium.3 To facilitate cross-lingual analysis, we focus on those Bible chapters that are also available in other languages in UD, viz. Ancient Greek, Latin, Gothic and Old Church Slavonic. 4. Yorùbá-specific Decisions A critical part of any corpus annotation project, regardless of its type and scale, is the annotation scheme (Lu, 2014). No Y"
2020.lrec-1.637,K18-2001,1,0.847273,"TrSent), training words (TrWord), test sentences (TsSent) and test words (TsWord) for each run. There are 5583 words in total, and 27.9 words per sentence on average. model was trained only on 180 manually annotated sentences. 200 sentences 90:10 UPOS 92.63 UAS 71.77 LAS 64.88 Table 5: Average results of universal part-of-speech (UPOS) tagging, unlabeled (UAS) and labeled attachment score (LAS) with 200 sentences. It is difficult and potentially misleading to compare parsing scores in different languages; nevertheless, we would like to provide some context by looking at a few results from the CoNLL 2018 shared task (Zeman et al., 2018). Table 6 shows shared task results for four treebanks from the same domain, i.e., the Bible. The parser used in our experiments, UDPipe 1.2, served as the baseline parser in the shared task; in addition, we also show the score of the best parser for each treebank. The shared task setting was different because the systems had to process raw text while in our experiments the parser has access to gold-standard tokenization. On the other hand, the other four treebanks contain significantly larger training sets. Interestingly, the scores obtained by UDPipe 1.2 on t"
2020.lrec-1.637,zeman-2008-reusable,1,0.648517,"r rules. The Universal Dependencies (UD) project (Nivre et al., 2020) was started to provide a universal inventory of categories/tagsets (allowing language-specific extensions where necessary) and guidelines for consistent annotation across languages of the world by providing a transparent and accessible framework for experts and non-specialists alike. The annotation scheme for representing dependency structure is based on Stanford Dependencies (SD) (De Marneffe et al., 2014), Google universal part-of-speech tags (Petrov et al., 2011) and the Interset interlingua for morphosyntactic features (Zeman, 2008). The UD initiative harmonised these projects into a single coherent framework. UD is based on dependency relations that exist between lexical units in a construction. Words are connected by directed relations known as dependencies, the word at the start of the relation is called head (parent), and the word at the end of the relation is called dependent (child). The head-dependent approach can be traced back to (Tesnière, 1959). Unlike some other dependency grammars, in UD the heads are normally content words, while function words and punctuation symbols are normally leaves (dependents that do"
2020.sigtyp-1.4,2020.sigtyp-1.1,0,0.0711656,"Missing"
2020.sigtyp-1.4,P07-1009,0,0.0613293,"Missing"
2020.sigtyp-1.4,C16-1123,0,0.0570439,"Missing"
2020.tlt-1.9,L18-1719,0,0.055995,"Missing"
2020.tlt-1.9,K18-2005,0,0.0279404,"n schemes even for the treebanks for the same language. As an example, two well known POS tagging schemes for English language include the POS tagging scheme of the Penn Treebank1 (Marcus et al., 1994) and the Universal POS tagset (Petrov et al., 2012). The Universal Dependencies (UD) Project (Nivre et al., 2016b; Nivre et al., 2020) was introduced in 2014 as a means of unifying all the novel features of different annotation formats as a universal annotation scheme consistent across different languages. It has since become a standard reference to compare scores relating to parser performance (Che et al., 2018; Martínez Alonso et al., 2017), study of language-specific features (Alzetta et al., 2018), and for dependency parsing shared tasks on UD (Zeman et al., 2018). UDv2.5 (Zeman et al., 2019) contains 157 treebanks in 90 languages, with multiple treebanks for some languages. Regardless of the differences in genre or the teams involved in building the treebanks, all treebanks of one language should be consistent with respect to the annotation guidelines, both intra and inter treebanks. However, this is often not the case, primarily because of the different sources of origin of the annotated data."
2020.tlt-1.9,L18-1347,0,0.0254495,"e variability in score in such cases depends on the architecture of the trained model, and is not comparable across different languages, or even when a different architecture is employed on the same data. Dickinson and Meurers (2003a; 2003b) focus on finding an n-gram of tokens in the corpus that occurs in the same context (referred to as a variation nucleus) such that its different occurrences are annotated differently. Originally coined for continuous annotation,2 the method was eventually adapted to look for inconsistencies in discontinuous annotation as well (Dickinson and Meurers, 2005). Chun et al. (2018) compare the POS annotation consistency for several Korean treebanks by using the relative frequency of the individual POS tags, while also briefly mentioning the cause of the variation in their distribution. While such analysis is slightly helpful in terms of drawing a comparison, it does not consider the interaction of different POS tags with each other. To illustrate such interactions, an n-gram-based approach might be utilised. 3 KLcpos3 and Measure Definition In a delexicalised cross-language parser transfer scenario, Rosa and Žabokrtský (2015) show that the KL-Divergence score of POS tri"
2020.tlt-1.9,E03-1068,0,0.116596,"ilar enough. The same technique was employed to evaluate the different Russian treebanks in UDv2.2 (Nivre et al., 2018) against each other by Droganova et al. (2018). It is worth stating here that the performance of the used tagger or parser may be a bottleneck, with the additional variables of the size and genre composition of the evaluated treebanks, among others. Furthermore, the acceptable variability in score in such cases depends on the architecture of the trained model, and is not comparable across different languages, or even when a different architecture is employed on the same data. Dickinson and Meurers (2003a; 2003b) focus on finding an n-gram of tokens in the corpus that occurs in the same context (referred to as a variation nucleus) such that its different occurrences are annotated differently. Originally coined for continuous annotation,2 the method was eventually adapted to look for inconsistencies in discontinuous annotation as well (Dickinson and Meurers, 2005). Chun et al. (2018) compare the POS annotation consistency for several Korean treebanks by using the relative frequency of the individual POS tags, while also briefly mentioning the cause of the variation in their distribution. While"
2020.tlt-1.9,P05-1040,0,0.0890214,"rs. Furthermore, the acceptable variability in score in such cases depends on the architecture of the trained model, and is not comparable across different languages, or even when a different architecture is employed on the same data. Dickinson and Meurers (2003a; 2003b) focus on finding an n-gram of tokens in the corpus that occurs in the same context (referred to as a variation nucleus) such that its different occurrences are annotated differently. Originally coined for continuous annotation,2 the method was eventually adapted to look for inconsistencies in discontinuous annotation as well (Dickinson and Meurers, 2005). Chun et al. (2018) compare the POS annotation consistency for several Korean treebanks by using the relative frequency of the individual POS tags, while also briefly mentioning the cause of the variation in their distribution. While such analysis is slightly helpful in terms of drawing a comparison, it does not consider the interaction of different POS tags with each other. To illustrate such interactions, an n-gram-based approach might be utilised. 3 KLcpos3 and Measure Definition In a delexicalised cross-language parser transfer scenario, Rosa and Žabokrtský (2015) show that the KL-Diverge"
2020.tlt-1.9,W05-1714,0,0.0350677,"data from treebanks of distinct language families, making the threshold less dependent on the properties of individual languages. We demonstrate the utility of the proposed measure by listing the treebanks in Universal Dependencies version 2.5 (UDv2.5) (Zeman et al., 2019) data that are annotated consistently with other treebanks of the same language. However, the measure could be used to assess inter-treebank annotation consistency under other (non-UD) annotation guidelines as well. 1 Introduction There exist a multitude of treebanks for different languages (Zeman et al., 2014). As noted by Kakkonen (2006), there exist a variety of formats and annotation schemes even for the treebanks for the same language. As an example, two well known POS tagging schemes for English language include the POS tagging scheme of the Penn Treebank1 (Marcus et al., 1994) and the Universal POS tagset (Petrov et al., 2012). The Universal Dependencies (UD) Project (Nivre et al., 2016b; Nivre et al., 2020) was introduced in 2014 as a means of unifying all the novel features of different annotation formats as a universal annotation scheme consistent across different languages. It has since become a standard reference to"
2020.tlt-1.9,H94-1020,0,0.13475,"(UDv2.5) (Zeman et al., 2019) data that are annotated consistently with other treebanks of the same language. However, the measure could be used to assess inter-treebank annotation consistency under other (non-UD) annotation guidelines as well. 1 Introduction There exist a multitude of treebanks for different languages (Zeman et al., 2014). As noted by Kakkonen (2006), there exist a variety of formats and annotation schemes even for the treebanks for the same language. As an example, two well known POS tagging schemes for English language include the POS tagging scheme of the Penn Treebank1 (Marcus et al., 1994) and the Universal POS tagset (Petrov et al., 2012). The Universal Dependencies (UD) Project (Nivre et al., 2016b; Nivre et al., 2020) was introduced in 2014 as a means of unifying all the novel features of different annotation formats as a universal annotation scheme consistent across different languages. It has since become a standard reference to compare scores relating to parser performance (Che et al., 2018; Martínez Alonso et al., 2017), study of language-specific features (Alzetta et al., 2018), and for dependency parsing shared tasks on UD (Zeman et al., 2018). UDv2.5 (Zeman et al., 20"
2020.udw-1.20,L16-1682,0,0.0310118,"Missing"
2020.udw-1.20,L18-1412,0,0.0278535,"Missing"
2020.udw-1.20,W19-7713,1,0.839127,"ebank with full syntactic annotation, which means that it is hard to develop language technology applications that require both tagging and parsing. It is in this context that we have developed the first Universal Dependencies (UD) treebank for Albanian, called UD Albanian-TSA.2 Although still very limited in size, it constitutes a first step towards developing a large-scale treebank within the UD scheme, enabling NLP research as well as comparative studies involving other languages, and there is also research showing that even a few annotated sentences can contribute to good parsing results (Meechan-Maddon and Nivre, 2019). In the following sections we introduce some of the key features of the Albanian language (Section 2), provide a brief summary of related work with regard to NLP for Albanian (Section 3), and describe the steps taken to develop the treebank (Section 4). We then discuss in some detail a selection of linguistic constructions in Albanian that pose challenges for the UD annotation framework and that are interesting from a cross-linguistic perspective (Section 5). 2 The Albanian Language Albanian belongs to the Indo-European family of languages, but it constitutes its own branch within the family."
2020.udw-1.20,L16-1262,1,0.874517,"Missing"
2020.udw-1.20,2020.lrec-1.497,1,0.864352,"Missing"
2020.udw-1.20,W17-0413,0,0.0191849,"m with the analysis of English ought, which combines with a to-infinitive, and is analyzed with the same syntactic structure that we propose for the Albanian modal verbs. xcomp mark (23) duhet të shkoj must.3SG to go.1SG VERB PART VERB ‘I must go’ This analysis is parallel to constructions of modal-like verbs with verbal complements, e.g., shpresoj të kthehem (I hope to return) and therefore ensures a uniform analysis for all subjunctive constructions introduced by të. However, we note that similar constructions are not annotated consistently in all UD treebanks. For example, in Modern Greek (Prokopidis and Papageorgiou, 2017), the analysis of a construction with πρέπει prépei (must), which also takes the form of a subjunctive with a particle (να na), treats the second verb as the head and assigns the relation aux to both the modal verb and the particle. On the other hand, a drawback of the analysis that we propose for Albanian is that it calls for a different treatment of duhet in impersonal constructions, where duhet takes a past participle instead of a verb in subjunctive as a complement.17 An example of this is illustrated in (24). aux (24) duhet folur must.3SG spoken.PAST.PTCPL AUX VERB ‘it must be spoken’ The"
2020.udw-1.20,trommer-kallulli-2004-morphological,0,0.212581,"Missing"
2020.udw-1.20,W17-7604,0,0.0161685,"s, we have observed that the corpus of Kote et al. (2019) shows some variation in the tagging of ambiguous word forms. For example, the word të (to/of) appears both with PART and DET when occurring in verb groups, while our treebank only uses PART in this position.10 Similarly, the word që (that) appears with CCONJ, SCONJ and PRON when introducing relative clauses, while our treebank only uses PRON in this position. Most of these differences should be relatively easy to harmonize. 4.5 Syntactic Annotation The syntactic annotation was performed manually using the annotation tool UD Annotatrix (Tyers et al., 2017), a browser-based tool customized for manual annotation of dependency trees in UD. Applying the UD guidelines to Albanian turned out to be relatively straightforward for the majority of syntactic constructions. In the next section, we discuss some phenomena that gave rise to questions that may be of more general interest to the community. 5 Challenging Constructions 5.1 Core Arguments One of the fundamental questions when annotating a new language in UD is to determine criteria for distinguishing core arguments from oblique modifiers, including deciding whether there are more than two core arg"
2020.wildre-1.7,de-marneffe-etal-2014-universal,0,0.035617,"Missing"
2020.wildre-1.7,petrov-etal-2012-universal,0,0.128278,"Missing"
2020.wildre-1.7,W17-7623,0,0.0297043,"was to resurrect annotation work towards monolingual and parallel treebanks for languages such as Hindi, Marathi, Bengali, Kannada, and Malayalam. To accomplish this treebank model, the Pāṇinian Kāraka Dependency annotation scheme was followed (Bharati et al., 2006). The annotation scheme was previously also utilized to annotate data in Telugu, Urdu, and Kashmiri (Begum et al., 2008; Husain et al., 2010; Bhat, 2017). Within the Universal Dependencies framework, as of UD release 2.5, treebanks and parsers are available for Sanskrit, Hindi, Urdu, Marathi, Tamil, and Telugu (Zeman et al., 2019; Ravishankar, 2017; Straka and Straková, 2019). 1 http://www.censusindia.gov.in/2011Census/C-16_ 25062018_NEW.pdf 2 https://universaldependencies.org/ 3 http://meity.gov.in/content/ language-computing-group-vi 33 NLP research in Bhojpuri has led to the development of a statistical POS tagger (Ojha et al., 2015; Singh and Jha, 2015), a machine-readable dictionary (Ojha, 2016), a language identification tool (Kumar et al., 2018), a Sanskrit-Bhojpuri machine translation system (Sinha and Jha, 2018), and more recently an English-Bhojpuri machine translation system (Ojha, 2019). Nevertheless, there is no prior work"
2020.wildre-1.7,W08-1301,0,0.05436,"Missing"
2020.wildre-1.7,W08-1300,0,0.279519,"Missing"
2020.wildre-1.7,K17-3009,0,0.0697303,"Missing"
2020.wildre-1.7,zeman-2008-reusable,1,0.780019,"Missing"
2021.findings-emnlp.303,P15-1136,0,0.0504828,"Missing"
2021.findings-emnlp.303,N09-1037,0,0.136783,"Missing"
2021.findings-emnlp.303,guillou-etal-2014-parcor,0,0.0314253,"henomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but it was created independently of coreference et al., 2021), the biggest collection of coreference annotation. This is the case of GUM for Engli"
2021.findings-emnlp.303,2020.lrec-1.641,0,0.0768802,"Missing"
2021.findings-emnlp.303,hendrickx-etal-2008-coreference,0,0.126363,"Missing"
2021.findings-emnlp.303,N19-1419,0,0.0170907,"ctic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, whi"
2021.findings-emnlp.303,W05-0303,0,0.12226,"cuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but i"
2021.findings-emnlp.303,D19-1588,0,0.0125746,"ount, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phe"
2021.findings-emnlp.303,J94-4002,0,0.242889,"the coreference annotareflexive and relative constructions), tion. 3570 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limis"
2021.findings-emnlp.303,L18-1065,0,0.0239068,"syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but it was created independently of coreference et al., 2021), the biggest collection of coreference annotation. This is the case of GUM for English datasets converted to a harmonize"
2021.findings-emnlp.303,N18-2108,0,0.0459646,"Missing"
2021.findings-emnlp.303,2020.findings-emnlp.245,0,0.031795,"Missing"
2021.findings-emnlp.303,L16-1026,1,0.753884,"ructures correspond by design. In the latter case, To the best of our knowledge, there are only two coreference annotations made use either of conlarge-scale coreference-annotated datasets where syntax is closely linked to coreference relations. stituency trees – an English dataset from OntoNotes In AnCora-CO (Recasens and Martí, 2010), co- (Weischedel et al., 2011), and Spanish and Catalan referring mentions are nodes in constituency trees, datasets from the AnCora project (Recasens and and in the Prague Dependency corpora (Hajiˇc et al., Martí, 2010)), or of dependency trees – a Czech 2020; Nedoluzhko et al., 2016; Mikulová et al., dataset from the Prague Dependency Treebank (Hajiˇc et al., 2020), and English and Czech datasets 2017), coreference relations are annotated directly from the Prague Czech-English Dependency Treebetween syntactic heads in dependency trees and bank (Nedoluzhko et al., 2016). mention spans are implicitly defined as subtrees of the heads. The selection resulted in 9 datasets, for which Finkel and Manning (2009) deal with issues simi- we use their CorefUD labels: (1) English-GUM: lar to our work and have developed a model that per- Georgetown Multilayer Corpus (Zeldes, 2017) (th"
2021.findings-emnlp.303,P02-1014,0,0.105273,"utational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite p"
2021.findings-emnlp.303,2020.lrec-1.497,1,0.883163,"Missing"
2021.findings-emnlp.303,P13-1051,1,0.859969,"Missing"
2021.findings-emnlp.303,L18-1061,0,0.0383519,"Missing"
2021.findings-emnlp.303,2020.acl-main.622,0,0.0142617,"s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generati"
2021.findings-emnlp.303,K17-3009,0,0.0297901,"is a collection of coreference datasets unidependency trees, would be beneficial in the long fied under a common scheme. Mention spans in term from various linguistic and computational per- all 9 datasets result from manual annotation. Despectives, especially if we hypothesize that: pendency trees available in the collection follow the Universal Dependencies (UD) scheme (Nivre 1. mentions are not just unconstrained subseet al., 2020) and result from manual annotation in quences of tokens, but mostly correspond to one case and from automatic parsing with UDPipe syntactically meaningful units, (Straka and Straková, 2017) in the 8 remaining 2. certain types of coreference relations are man- cases. In all cases, the dependency trees came into ifested primarily by syntactic means (such as existence independently of the coreference annotareflexive and relative constructions), tion. 3570 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to prono"
2021.iwpt-1.15,W17-6507,1,0.881486,"notation is richer than in UD 2.7. Besides improvements in the officially released versions of the individual treebanks, a few other things have changed in comparison to the IWPT 2020 task. The English data now includes the GUM treebank (its enhanced annotation was not present in UD 2.7 but it was being prepared for UD 2.8 and it was ready in time for the shared task). As in 2020, we include two French treebanks whose enhanced annotation is still not included in the official UD releases, but the annotation is more conservative this year, omitting the extra labels for diathesis neutralization (Candito et al., 2017) and surface vs deep syntax markers. Still, some enhancements in French go slightly beyond the official UD guidelines (see below for details). In Polish, we now harmonize the relation subtypes in the three treebanks so that merging them into one dataset is no longer an issue. Finally, we omit the Chukchi treebank, which is new in UD 2.7 and has enhanced graphs, but the graphs are there only 147 conj conj obj nsubj nummod punct orphan cc orphan Sue has 5 euros , Pat 6 and Kim 3 Figure 1: A basic tree of a gapping structure. conj conj obj nsubj nummod punct nsubj obj cc nsubj obj Sue has 5 euros"
2021.iwpt-1.15,2021.iwpt-1.24,0,0.0565891,"Missing"
2021.iwpt-1.15,2020.acl-main.747,0,0.0880101,"ng task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021) and UNIPI (Attardi et al., 2021) use a method that learns weights for the scores obtained from various layers of the BERT model to be used as input for the biaffine parser. Most teams reduce the number of edge labels during training by de-lexic"
2021.iwpt-1.15,2021.iwpt-1.18,0,0.0205018,"This should be seen as a diagnostic only, and is intended to gain further insights into the capability of various systems to deal with challenging phenomena, such as the proper analysis of phenomena occurring in the context of coordination and ellipsis. 7 Approaches The predominant approach to obtaining the enhanced dependency graph is to use a biaffine function, i.e., predicting for each pair of nodes how likely it is that they are in a parent-child relation. There is wide variety in the way the final annotation graph is obtained, and ensuring that the result is valid (i.e. connected). GREW (Guillaume and Perrier, 2021) uses manually constructed rewrite rules to map basic UD into EUD, while FASTPARSE (Anderson and Gómez-Rodríguez, 2021) and NUIG (Choudhary and O’riordan, 2021) reformulate the task as a sequence-labeling task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of th"
2021.iwpt-1.15,2021.iwpt-1.17,0,0.05987,"Missing"
2021.iwpt-1.15,2021.iwpt-1.16,0,0.0440323,"Missing"
2021.iwpt-1.15,2020.lrec-1.497,1,0.896496,"Missing"
2021.iwpt-1.15,2021.iwpt-1.22,0,0.0523448,"Missing"
2021.iwpt-1.15,W18-6012,0,0.397597,"Missing"
2021.iwpt-1.15,W13-3728,0,0.40752,"nd subsequent work have shown that considerable progress has been made in multilingual dependency parsing. For enhanced dependency parsing, there are additional challenges. The enhanced representation is a connected directed graph, possibly containing cycles, while the bulk of dependency parsing work still focuses on rooted trees. The set of labels to be predicted is also much larger, as some enhanced dependency labels incorporate the lemma of certain dependents. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Nyblom et al., 2013; Schuster et al., 2017; Nivre et al., 2018). Moreover, most state-of-the-art work in dependency parsing uses a graph-based approach, where 146 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 146–157 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen"
2021.iwpt-1.15,2020.conll-shared.0,0,0.0794056,"Missing"
2021.iwpt-1.15,K19-2001,0,0.064997,"Missing"
2021.iwpt-1.15,S15-2153,1,0.846515,"Missing"
2021.iwpt-1.15,S14-2008,1,0.870583,"Missing"
2021.iwpt-1.15,2020.acl-demos.14,0,0.236593,"mpare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task. 1 2 Introduction Universal Dependencies (UD) (Nivre et al., 2020) is a framework for cross-linguistically consistent treebank annotation that has so far been applied to 114 languages. UD defines two levels of annotation, the basic trees and the enhanced graphs (EUD) (Schuster and Manning, 2016). There are several good parsers that can predict the basic trees (including tokenization and morphology) for previously unseen text (Straka et al., 2016; Qi et al., 2020). Two large shared tasks on basic UD parsing were organized at CoNLL (Zeman et al., 2017, 2018). Enhanced UD parsing attracted comparatively less attention until the shared task organized at IWPT 2020 (Bouma et al., 2020). The present paper describes a second instance of that task, organized as a part of the 17th International Conference on Parsing Technologies1 (IWPT), collocated with ACL-IJCNLP 2021. Like in the previous year, the evaluation was done on datasets covering 17 languages from four language familiies. This paper is a follow-up of the overview paper of the previous instance of the"
2021.iwpt-1.15,L16-1376,0,0.284241,"Missing"
2021.iwpt-1.15,2021.iwpt-1.23,0,0.012184,"cted). GREW (Guillaume and Perrier, 2021) uses manually constructed rewrite rules to map basic UD into EUD, while FASTPARSE (Anderson and Gómez-Rodríguez, 2021) and NUIG (Choudhary and O’riordan, 2021) reformulate the task as a sequence-labeling task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021"
2021.iwpt-1.15,L16-1680,0,0.0641786,"Missing"
2021.iwpt-1.15,2021.eacl-demos.10,0,0.427708,"Missing"
2021.iwpt-1.15,2021.iwpt-1.20,0,0.0398853,"kenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021) and UNIPI (Attardi et al., 2021) use a method that learns weights for the scores obtained from various layers of the BERT model to be used as input for the biaffine parser. Most teams reduce the number of edge labels during training by de-lexicalizing edge labels. Dependency paths involving an empty node are us"
2021.iwpt-1.15,K18-2001,1,0.752485,"Missing"
berka-etal-2012-automatic,niessen-etal-2000-evaluation,0,\N,Missing
berka-etal-2012-automatic,C08-1141,0,\N,Missing
berka-etal-2012-automatic,J03-1002,0,\N,Missing
berka-etal-2012-automatic,fishel-etal-2012-terra,1,\N,Missing
berka-etal-2012-automatic,vilar-etal-2006-error,0,\N,Missing
berka-etal-2012-automatic,W11-2107,0,\N,Missing
bojar-etal-2010-data,P02-1040,0,\N,Missing
bojar-etal-2010-data,P07-2045,1,\N,Missing
bojar-etal-2010-data,N03-1017,0,\N,Missing
bojar-etal-2010-data,J03-1002,0,\N,Missing
bojar-etal-2010-data,J07-2003,0,\N,Missing
bojar-etal-2010-data,P03-1021,0,\N,Missing
bojar-etal-2014-hindencorp,bojar-etal-2010-data,1,\N,Missing
bojar-etal-2014-hindencorp,W12-3152,0,\N,Missing
bojar-etal-2014-hindencorp,W11-4624,0,\N,Missing
bojar-etal-2014-hindencorp,W07-1709,0,\N,Missing
bojar-etal-2014-hindencorp,C10-2010,0,\N,Missing
bojar-etal-2014-hindencorp,majlis-zabokrtsky-2012-language,0,\N,Missing
bojar-etal-2014-hindencorp,bojar-etal-2012-joy,1,\N,Missing
C00-2100,P91-1027,0,0.26691,"nd Minnen, 1998; Carroll and Rooth, 1998) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. The data (PDT)"
C00-2100,J93-2002,0,0.634068,"98; Carroll and Rooth, 1998) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. The data (PDT) is thanks to"
C00-2100,A97-1052,0,0.73027,"erved frame types) was 450. 5 Comparison with related work Preliminary work on SF extraction from corpora was done by (Brent, 1991; Brent, 1993; Brent, 1994) and (Webster and Marcus, 1989; Ushioda et al., 1993). Brent (Brent, 1993; Brent, 1994) uses the standard method of testing miscue probabilities for filtering frames observed with a verb. (Brent, 1994) presents a method for estimating p!f . Brent applied his method to a small number of verbs and associated SF types. (Manning, 1993) applies Brent’s method to parsed data and obtains a subcategorization dictionary for a larger set of verbs. (Briscoe and Carroll, 1997; Carroll and Minnen, 1998) differs from earlier work in that a substantially larger set of SF types are considered; (Carroll and Rooth, 1998) use an EM algorithm to learn subcategorization as a result of learning rule probabilities, and, in turn, to improve parsing accuracy by applying the verb SFs obtained. (Basili and Vindigni, 1998) use a conceptual clustering algorithm for acquiring subcategorization frames for Italian. They establish a partial order on partially overlapping OFs (similar to our OF subsets) which is then used to suggest a potential SF. A complete comparison of all the prev"
C00-2100,W98-1114,0,0.0563471,"5 Comparison with related work Preliminary work on SF extraction from corpora was done by (Brent, 1991; Brent, 1993; Brent, 1994) and (Webster and Marcus, 1989; Ushioda et al., 1993). Brent (Brent, 1993; Brent, 1994) uses the standard method of testing miscue probabilities for filtering frames observed with a verb. (Brent, 1994) presents a method for estimating p!f . Brent applied his method to a small number of verbs and associated SF types. (Manning, 1993) applies Brent’s method to parsed data and obtains a subcategorization dictionary for a larger set of verbs. (Briscoe and Carroll, 1997; Carroll and Minnen, 1998) differs from earlier work in that a substantially larger set of SF types are considered; (Carroll and Rooth, 1998) use an EM algorithm to learn subcategorization as a result of learning rule probabilities, and, in turn, to improve parsing accuracy by applying the verb SFs obtained. (Basili and Vindigni, 1998) use a conceptual clustering algorithm for acquiring subcategorization frames for Italian. They establish a partial order on partially overlapping OFs (similar to our OF subsets) which is then used to suggest a potential SF. A complete comparison of all the previous approaches with the cu"
C00-2100,W98-1505,0,0.111682,"; Brent, 1994) and (Webster and Marcus, 1989; Ushioda et al., 1993). Brent (Brent, 1993; Brent, 1994) uses the standard method of testing miscue probabilities for filtering frames observed with a verb. (Brent, 1994) presents a method for estimating p!f . Brent applied his method to a small number of verbs and associated SF types. (Manning, 1993) applies Brent’s method to parsed data and obtains a subcategorization dictionary for a larger set of verbs. (Briscoe and Carroll, 1997; Carroll and Minnen, 1998) differs from earlier work in that a substantially larger set of SF types are considered; (Carroll and Rooth, 1998) use an EM algorithm to learn subcategorization as a result of learning rule probabilities, and, in turn, to improve parsing accuracy by applying the verb SFs obtained. (Basili and Vindigni, 1998) use a conceptual clustering algorithm for acquiring subcategorization frames for Italian. They establish a partial order on partially overlapping OFs (similar to our OF subsets) which is then used to suggest a potential SF. A complete comparison of all the previous approaches with the current work is given in Table 2. While these approaches differ in size and quality of training data, number of SF ty"
C00-2100,J93-1003,0,0.0847181,"Missing"
C00-2100,P98-1080,0,0.0317281,"Missing"
C00-2100,W99-0632,0,0.0619095,"Missing"
C00-2100,P99-1051,0,0.107523,"Missing"
C00-2100,E99-1007,0,0.0818818,"Missing"
C00-2100,W99-0503,0,0.0676496,"Missing"
C00-2100,W93-0109,0,0.841214,"8) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. The data (PDT) is thanks to grant No. 405/96/K214 of the Grant"
C00-2100,P89-1022,0,0.102663,"ns by a parser. (Carroll and Minnen, 1998; Carroll and Rooth, 1998) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. Th"
C00-2100,H91-1067,0,\N,Missing
C00-2100,C98-1077,0,\N,Missing
C00-2100,P93-1032,0,\N,Missing
C02-1118,W98-1505,0,\N,Missing
C02-1118,W98-1114,0,\N,Missing
C02-1118,H91-1067,0,\N,Missing
C02-1118,C92-2066,0,\N,Missing
C02-1118,P97-1003,0,\N,Missing
C02-1118,C00-2100,1,\N,Missing
C02-1118,W00-1325,0,\N,Missing
C02-1118,W00-1327,0,\N,Missing
C02-1118,A97-1052,0,\N,Missing
C02-1118,J93-2002,0,\N,Missing
C02-1118,P00-1058,0,\N,Missing
C02-1118,P96-1025,0,\N,Missing
C02-1118,P98-1080,0,\N,Missing
C02-1118,C98-1077,0,\N,Missing
C02-1118,P93-1032,0,\N,Missing
C02-1118,P91-1027,0,\N,Missing
C02-1118,C92-2065,0,\N,Missing
C02-1118,P98-1071,0,\N,Missing
C02-1118,C98-1068,0,\N,Missing
C02-1118,stranakova-lopatkova-zabokrtsky-2002-valency,0,\N,Missing
I08-3008,W06-2912,0,0.0352328,"Missing"
I08-3008,P06-1109,0,0.0319591,"Missing"
I08-3008,nivre-etal-2006-talbanken05,0,0.0221452,"Missing"
I08-3008,P05-1022,0,0.021496,"Missing"
I08-3008,P00-1056,0,0.0310736,"t the paragraphs into sentences and pruned sentences with suspicious length, contents (sequence of dashes, for instance) or both. We ended up with 430,808 Swedish sentences and 6,154,663 tokens. Since the Acquis texts are available in 21 languages, we can also exploit the Danish Acquis and its alignment with the Swedish one. We use it to study the similarity of the two languages, and for the “gloss” experiment in Section 5.1. Paragraphlevel alignment is provided as part of Acquis and contains 283,509 aligned segments. Word-level alignment, needed for our experiment, was obtained using GIZA++ (Och and Ney 2000). The treebanks are manually tagged with parts of speech and morphological information. For some of our experiments, we needed to automatically retag the target (Swedish) treebank, and to tag the Swedish Acquis. For that purpose we used the Swedish tagger of Jan Hajič, a variant of Hajič’s Czech tagger (Hajič 2004) retrained on Swedish data. 3 Treebank Normalization The two treebanks were developed by different teams, using different annotation styles and guidelines. They would be systematically different even if their texts were in the same language, but it is 5 Legislative texts are a specia"
I08-3008,P97-1003,0,0.022247,"Missing"
I08-3008,P99-1065,0,0.0099563,"reebanks, while the Charniak-Johnson reranking parser works with phrase structures. For our experiments, we con3 There are other approaches to domain adaptation as well. For instance, Steedman et al. (2003) address domain adaptation using a weakly supervised method called co-training. Two parsers, each applying a different strategy, mutually prepare new training examples for each other. We have not tested co-training for crosslanguage adaptation. 4 We used the CoNLL 2006 versions of these treebanks. 36 verted the treebanks from dependencies to phrases, using the “flattest-possible” algorithm (Collins et al. 1999; algorithm 2 of Xia and Palmer 2001). The morphological annotation of the treebanks helped us to label the non-terminals. Although the Charniak’s parser can be taught a new inventory of labels, we found it easier to map head morpho-tags directly to Penn-Treebank-style non-terminals. Hence the parser can think it’s processing Penn Treebank data. The morphological annotation of the treebanks is further discussed in Section 4. We also experimented with a large body of unannotated Swedish texts. Such data could theoretically be acquired by crawling the Web; here, however, we used the freely avail"
I08-3008,P04-1061,0,0.0224556,"Missing"
I08-3008,N03-1017,0,0.0996805,"276 not exceeding 40 words. The Swedish treebank Talbanken05 (Nivre et al. 2006) contains 11,042 sentences (191,467 tokens). It was converted at Växjö from the much older Talbanken76 treebank, created at the Lund University. Again, the texts belong to mixed domains. We split the data to 10,700 training and 342 test sentences, out of which 317 do not exceed 40 words. Both treebanks are dependency treebanks, while the Charniak-Johnson reranking parser works with phrase structures. For our experiments, we con3 There are other approaches to domain adaptation as well. For instance, Steedman et al. (2003) address domain adaptation using a weakly supervised method called co-training. Two parsers, each applying a different strategy, mutually prepare new training examples for each other. We have not tested co-training for crosslanguage adaptation. 4 We used the CoNLL 2006 versions of these treebanks. 36 verted the treebanks from dependencies to phrases, using the “flattest-possible” algorithm (Collins et al. 1999; algorithm 2 of Xia and Palmer 2001). The morphological annotation of the treebanks helped us to label the non-terminals. Although the Charniak’s parser can be taught a new inventory of"
I08-3008,J93-2004,0,0.0429532,"Missing"
I08-3008,P06-1043,0,0.0234507,"t data. Both treebanks have also been parsed after delexicalization into various tag sets: Danish gold standard converted to the hybrid sv/da tag set, Swedish Mamba gold standard, and Swedish automatically tagged with hybrid tags. The reranker did not prove useful for lexicalized Swedish, although it helped with Danish. (We cur7 F = 2×P×R / (P+R) 39 P 44.59 42.94 61.85 60.22 63.47 64.74 R 42.04 40.80 65.03 62.85 67.67 68.15 F 43.28 41.84 63.40 61.50 65.50 66.40 Table 2. Cross-language parsing accuracy. 7 Self-Training Finally, we explored the self-training based domain-adaptation technique of McClosky et al. (2006) in this setting. McClosky et al. trained the Brown parser on one domain of English (WSJ), parsed a large corpus of a second domain (NANTC), trained a new Charniak (non-reranking) parser on WSJ plus the parsed NANTC, and tested the new parser on data from a third domain (Brown Corpus). They observed improvement over baseline in spite of the fact that the large corpus was not in the third domain. Our setting is similar. We train the Brown parser on Danish treebank and apply it to Swedish Acquis. Then we train new Charniak parser on Danish treebank and the parsed Swedish Acquis, and test the par"
I08-3008,J03-3002,1,0.354664,"s. “object” form (the distinction between English he and him). DDT calls the same paradigm “nominative” vs. “unmarked” case. We explore two techniques of making unknown words known. We call them glosses and delexicalization, respectively. • Most noun phrases in both languages distinguish just the common and neuter genders. However, some pronouns could be classified as masculine or feminine. Swedish tags use the masculine gender, Danish do not. • DDT does not use special part of speech for numbers — they are tagged as adjectives. This approach needs a Danish-Swedish (da-sv) bitext. As shown by Resnik and Smith (2003), parallel texts can be acquired from the Web, which makes this type of resource more easily available than a treebank. We benefited from the Acquis dasv alignments. Similarly to phrase-based translation systems, we used GIZA++ (Och and Ney 2000) to obtain one-to-many word alignments in both directions, then combined them into a single set of refined alignments using the “final-and” method of Koehn et al. (2003). The refined alignments provided us with two-way tables of a source word and all its possible translations, with weights. Using these tables, we glossed each Swedish word by its Danish"
I08-3008,E03-1008,0,0.0130025,"Missing"
I08-3008,H01-1014,0,0.0139302,"reranking parser works with phrase structures. For our experiments, we con3 There are other approaches to domain adaptation as well. For instance, Steedman et al. (2003) address domain adaptation using a weakly supervised method called co-training. Two parsers, each applying a different strategy, mutually prepare new training examples for each other. We have not tested co-training for crosslanguage adaptation. 4 We used the CoNLL 2006 versions of these treebanks. 36 verted the treebanks from dependencies to phrases, using the “flattest-possible” algorithm (Collins et al. 1999; algorithm 2 of Xia and Palmer 2001). The morphological annotation of the treebanks helped us to label the non-terminals. Although the Charniak’s parser can be taught a new inventory of labels, we found it easier to map head morpho-tags directly to Penn-Treebank-style non-terminals. Hence the parser can think it’s processing Penn Treebank data. The morphological annotation of the treebanks is further discussed in Section 4. We also experimented with a large body of unannotated Swedish texts. Such data could theoretically be acquired by crawling the Web; here, however, we used the freely available JRC-Acquis corpus of EU legislat"
I08-3008,steinberger-etal-2006-jrc,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K18-2001,K18-2015,0,0.053009,"Missing"
K18-2001,Q17-1010,0,0.211935,"Missing"
K18-2001,K18-2010,0,0.0386566,"Missing"
K18-2001,K18-2017,0,0.075361,"Missing"
K18-2001,W06-2920,0,0.453112,"Missing"
K18-2001,K18-2025,0,0.0365994,"Missing"
K18-2001,K18-2005,0,0.120251,"Missing"
K18-2001,K18-2013,1,0.806044,"Missing"
K18-2001,K18-2026,0,0.0321915,"Missing"
K18-2001,K18-2012,0,0.0235436,"above are all intrinsic measures: they evaluate the grammatical analysis task per se, with the hope that better scores correspond to output that is more useful for downstream NLP applications. Nevertheless, such correlations are not automatically granted. We thus seek to complement our task with an extrinsic evaluation, where the output of parsing systems is exploited by applications like biological event extraction, opinion analysis and negation scope resolution. This optional track involves English only. It is organized in collaboration with the EPE initiative;7 for details see Fares et al. (2018). Syntactic Word Alignment The higher segmentation level is based on the notion of syntactic word. Some languages contain multi-word tokens (MWT) that are regarded as contractions of multiple syntactic words. For example, the German token zum is a contraction of the preposition zu “to” and the article dem “the”. Syntactic words constitute independent nodes in dependency trees. As shown by the example, it is not required that the MWT is a pure concatenation of the participating words; the simple token alignment thus does not work when MWTs 4 TIRA: The System Submission Platform Similarly to our"
K18-2001,K18-2003,0,0.040574,"Missing"
K18-2001,K18-2006,0,0.0774162,"Missing"
K18-2001,K18-2014,0,0.0664725,"Missing"
K18-2001,K18-2008,0,0.0697052,"Missing"
K18-2001,L16-1262,1,0.910778,"Missing"
K18-2001,W17-0411,1,0.849881,"and in the system output before comparing them. In the end-to-end evaluation of our task, LAS is re-defined as the harmonic mean (F1 ) of precision P and recall R, where P = #correctRelations #systemNodes (1) R= #correctRelations #goldNodes (2) LAS = 2P R P +R (3) Note that attachment of all nodes including punctuation is evaluated. LAS is computed separately for each of the 82 test files and a macro-average of all these scores is used to rank the systems. 3.2 MLAS: Morphology-Aware Labeled Attachment Score MLAS aims at cross-linguistic comparability of the scores. It is an extension of CLAS (Nivre and Fang, 2017), which was tested experimentally in the 2017 task. CLAS focuses on dependencies between content words and disregards attachment of function words; in MLAS, function words are not ignored, but they are treated as features of content words. In addition, part-of-speech tags and morphological features are evaluated, too. 3.3 BLEX: Bilexical Dependency Score BLEX is similar to MLAS in that it focuses on relations between content words. Instead of morphological features, it incorporates lemmatization in the evaluation. It is thus closer to semantic content and evaluates two aspects of UD annota5 ar"
K18-2001,K18-2022,0,0.0296323,"Missing"
K18-2001,K18-2011,1,0.844373,"Missing"
K18-2001,W17-0412,1,0.901947,"Missing"
K18-2001,L16-1680,1,0.90044,"Missing"
K18-2001,K17-3009,1,0.858784,"Missing"
K18-2001,tiedemann-2012-parallel,0,0.0674866,"at follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers provided with large amounts of freely available data. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. We provided dependency-annotated training and test data, and also large quantities of crawled raw texts. Other language resources are available from third-party servers and we only referred to the respective download sites. 2.1 Training Data: UD 2.2 Training and development data came from the Universal Dependencies (UD) 2.2 collection (Nivre et al., 2018). This year, the official UD release immediately followed the test phase of the shared task. The training and development data were available to the"
K18-2001,K18-2016,0,0.0988933,"Missing"
K18-2001,K18-2019,0,0.110064,"Missing"
K18-2001,K18-2007,0,0.0602044,"Missing"
K18-2001,K18-2004,0,0.103154,"Missing"
K19-2015,E17-1051,0,0.0210719,". Unfortunately, we were not able to replace it with an analogous white-listed resource, therefore we did not use it. Flanigan et al. (2014) presented the first approach to AMR parsing, which is based around the idea of identifying concepts and relations in source sentences utilizing a novel training algorithm and additional linguistic knowledge. The parser was further improved for the SemEval 2016 Shared Task 8 (Flanigan et al., 2016). JAMR parser utilizes a rule-based aligner to match word spans in a sentence to concepts they evoke, which is applied in a pipeline before training the parser. Damonte et al. (2017) proposed a transitionbased parser for AMR not dissimilar to the A RC E AGER transition system for dependency tree parsing, which parses sentences left-to-right in real time. Lyu and Titov (2018) presented an AMR parser that jointly learns to align and parse treating alignments as latent variables in a joint probabilistic model. The authors argue that simultaneous learning of alignment and parses benefits the parsing in the sense that alignment is directly informed by the parsing objective thus producing overall better alignments. Zhang et al. (2019a) and (Zhang et al., 2019b) recently reporte"
K19-2015,S16-1186,0,0.147579,"oken alignments. The parser was evaluated on DMRS, EDS and AMR graphs. Lexicon extraction partially relies on Propbank (Palmer et al., 2005), which is not in the shared task whitelist. Unfortunately, we were not able to replace it with an analogous white-listed resource, therefore we did not use it. Flanigan et al. (2014) presented the first approach to AMR parsing, which is based around the idea of identifying concepts and relations in source sentences utilizing a novel training algorithm and additional linguistic knowledge. The parser was further improved for the SemEval 2016 Shared Task 8 (Flanigan et al., 2016). JAMR parser utilizes a rule-based aligner to match word spans in a sentence to concepts they evoke, which is applied in a pipeline before training the parser. Damonte et al. (2017) proposed a transitionbased parser for AMR not dissimilar to the A RC E AGER transition system for dependency tree parsing, which parses sentences left-to-right in real time. Lyu and Titov (2018) presented an AMR parser that jointly learns to align and parse treating alignments as latent variables in a joint probabilistic model. The authors argue that simultaneous learning of alignment and parses benefits the parsi"
K19-2015,P14-1134,0,0.0899179,"ficantly improved UCCA parsing. Buys and Blunsom (2017) proposed a neural encoder-decoder transition-based parser for full MRS-based semantic graphs. The decoder is extended with stack-based embedding features which allows the graphs to be predicted jointly with unlexicalized predicates and their token alignments. The parser was evaluated on DMRS, EDS and AMR graphs. Lexicon extraction partially relies on Propbank (Palmer et al., 2005), which is not in the shared task whitelist. Unfortunately, we were not able to replace it with an analogous white-listed resource, therefore we did not use it. Flanigan et al. (2014) presented the first approach to AMR parsing, which is based around the idea of identifying concepts and relations in source sentences utilizing a novel training algorithm and additional linguistic knowledge. The parser was further improved for the SemEval 2016 Shared Task 8 (Flanigan et al., 2016). JAMR parser utilizes a rule-based aligner to match word spans in a sentence to concepts they evoke, which is applied in a pipeline before training the parser. Damonte et al. (2017) proposed a transitionbased parser for AMR not dissimilar to the A RC E AGER transition system for dependency tree pars"
K19-2015,P17-1104,0,0.0834802,"ing criteria: • reporting reasonably good results; • accompanied by open-source code available to use; • with instructions sufficient to run the code; • using only the resources from the shared task whitelist. Peng et al. (2017) presented a neural parser that was designed to work with three semantic dependency graph frameworks, namely, DM, PAS and PSD. The authors proposed a single-task and two multitask learning approaches and extended their work with a new approach (Peng et al., 2018) to learning semantic parsers from multiple datasets. The first specialized parser for UCCA was presented by Hershcovich et al. (2017). It utilized 158 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 158–165 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2015 2.0 and AMR 1.0. The proposed attention-based model is aligner-free and deals with AMR parsing as sequence-to-graph task. Additionally, the authors proposed an alternative view on reentrancy converting an AMR graph into a tree by duplicating nodes that have reentrant relations and then adding an extra layer of annotation by assigning an index to each"
K19-2015,W13-2322,0,0.0750243,"Missing"
K19-2015,P18-1035,0,0.0722741,"ssigning an index to each node so that the duplicates of the same node would have the same id and could be merged to recover the original AMR graph. This series of papers looks very promising, but unfortunately we were not able to test the parser due to them being published after the end of the shared task. novel transition set and features based on bidirectional LSTMs and was developed to deal with specific features of UCCA graphs, such as DAG structure of the graph, discontinuous structures, and non-terminal nodes corresponding to complex semantic units. The work saw further development in (Hershcovich et al., 2018), where authors presented a generalized solution for transition-based parsing of DAGs and explored multitask learning across several representations, showing that using other formalisms in joint learning significantly improved UCCA parsing. Buys and Blunsom (2017) proposed a neural encoder-decoder transition-based parser for full MRS-based semantic graphs. The decoder is extended with stack-based embedding features which allows the graphs to be predicted jointly with unlexicalized predicates and their token alignments. The parser was evaluated on DMRS, EDS and AMR graphs. Lexicon extraction pa"
K19-2015,P17-1112,0,0.146085,"ublished after the end of the shared task. novel transition set and features based on bidirectional LSTMs and was developed to deal with specific features of UCCA graphs, such as DAG structure of the graph, discontinuous structures, and non-terminal nodes corresponding to complex semantic units. The work saw further development in (Hershcovich et al., 2018), where authors presented a generalized solution for transition-based parsing of DAGs and explored multitask learning across several representations, showing that using other formalisms in joint learning significantly improved UCCA parsing. Buys and Blunsom (2017) proposed a neural encoder-decoder transition-based parser for full MRS-based semantic graphs. The decoder is extended with stack-based embedding features which allows the graphs to be predicted jointly with unlexicalized predicates and their token alignments. The parser was evaluated on DMRS, EDS and AMR graphs. Lexicon extraction partially relies on Propbank (Palmer et al., 2005), which is not in the shared task whitelist. Unfortunately, we were not able to replace it with an analogous white-listed resource, therefore we did not use it. Flanigan et al. (2014) presented the first approach to"
K19-2015,W12-3602,0,0.178883,"Pennington et al. 2014). We use the same version that is described in the 1 All conversion scripts that we created for this shared task are available on GitHub at https: //github.com/ufal/mrptask/tree/master/ conll-2019-system. 2 See detailed format description at http: //alt.qcri.org/semeval2015/task18/index. php?id=data-and-tools 159 3.2 paper – 100-dimensional vectors trained on Wikipedia and Gigaword. EDS We do not have any parser specifically for EDS. However, EDS is closely related to DM (DM is a lossy conversion of EDS, where nodes that do not represent surface words have been removed (Ivanova et al., 2012)). We thus work with the hypothesis that a DM graph is a subset of the corresponding EDS graph, and we submit our DM graph to be also evaluated as EDS. This is obviously just an approximation, as EDS parsing is a task inherently more complex than DM parsing. The hope is that the DM parser will be able to identify some EDS edges while others will be missing, and the overall results will still be better than if we did not predict anything at all. To illustrate this, consider Figures 2 and 3. Four DM edges are also present in the EDS graph (in one case, the corresponding nodes have different labe"
K19-2015,P18-1037,0,0.0753331,"ased around the idea of identifying concepts and relations in source sentences utilizing a novel training algorithm and additional linguistic knowledge. The parser was further improved for the SemEval 2016 Shared Task 8 (Flanigan et al., 2016). JAMR parser utilizes a rule-based aligner to match word spans in a sentence to concepts they evoke, which is applied in a pipeline before training the parser. Damonte et al. (2017) proposed a transitionbased parser for AMR not dissimilar to the A RC E AGER transition system for dependency tree parsing, which parses sentences left-to-right in real time. Lyu and Titov (2018) presented an AMR parser that jointly learns to align and parse treating alignments as latent variables in a joint probabilistic model. The authors argue that simultaneous learning of alignment and parses benefits the parsing in the sense that alignment is directly informed by the parsing objective thus producing overall better alignments. Zhang et al. (2019a) and (Zhang et al., 2019b) recently reported results that outperform all previously reported SMATCH scores, on both AMR 3 System Description 3.1 DM and PSD To deal with the DM and PSD frameworks we chose a parser that was described in (Pe"
K19-2015,D14-1162,0,0.0835974,"s that utilize the multitask learning approach. Unfortunately, the project seems to be stalled and multitask parsing part is not available. We proceeded with the single-task model (NeurboParser), in which models for each formalism are trained completely separately. To reproduce the experiment from the paper we needed to perform the following steps: • Convert the training data from the MRP format to the input format required by the parser.1 The input format is the same as the one used in the 2015 SemEval Shared Task2 (see Figure 1 for an example). • Download pre-trained word embeddings (GloVe, Pennington et al. 2014). We use the same version that is described in the 1 All conversion scripts that we created for this shared task are available on GitHub at https: //github.com/ufal/mrptask/tree/master/ conll-2019-system. 2 See detailed format description at http: //alt.qcri.org/semeval2015/task18/index. php?id=data-and-tools 159 3.2 paper – 100-dimensional vectors trained on Wikipedia and Gigaword. EDS We do not have any parser specifically for EDS. However, EDS is closely related to DM (DM is a lossy conversion of EDS, where nodes that do not represent surface words have been removed (Ivanova et al., 2012))."
K19-2015,S16-1166,0,0.227814,"Missing"
K19-2015,K17-3009,0,0.0608601,"Missing"
K19-2015,L16-1262,1,0.852408,"Missing"
K19-2015,P19-1009,0,0.119223,"Missing"
K19-2015,D19-1392,0,0.198179,"Missing"
K19-2015,K19-2001,0,0.0377402,"Missing"
K19-2015,S15-2153,1,0.936422,"Missing"
K19-2015,J05-1004,0,0.157863,"generalized solution for transition-based parsing of DAGs and explored multitask learning across several representations, showing that using other formalisms in joint learning significantly improved UCCA parsing. Buys and Blunsom (2017) proposed a neural encoder-decoder transition-based parser for full MRS-based semantic graphs. The decoder is extended with stack-based embedding features which allows the graphs to be predicted jointly with unlexicalized predicates and their token alignments. The parser was evaluated on DMRS, EDS and AMR graphs. Lexicon extraction partially relies on Propbank (Palmer et al., 2005), which is not in the shared task whitelist. Unfortunately, we were not able to replace it with an analogous white-listed resource, therefore we did not use it. Flanigan et al. (2014) presented the first approach to AMR parsing, which is based around the idea of identifying concepts and relations in source sentences utilizing a novel training algorithm and additional linguistic knowledge. The parser was further improved for the SemEval 2016 Shared Task 8 (Flanigan et al., 2016). JAMR parser utilizes a rule-based aligner to match word spans in a sentence to concepts they evoke, which is applied"
K19-2015,P17-1186,0,0.0464305,"Missing"
K19-2015,N18-1135,0,0.0610706,"represented in a common JSON-based 2 Related Work For the purposes of this work we considered previous work matching the following criteria: • reporting reasonably good results; • accompanied by open-source code available to use; • with instructions sufficient to run the code; • using only the resources from the shared task whitelist. Peng et al. (2017) presented a neural parser that was designed to work with three semantic dependency graph frameworks, namely, DM, PAS and PSD. The authors proposed a single-task and two multitask learning approaches and extended their work with a new approach (Peng et al., 2018) to learning semantic parsers from multiple datasets. The first specialized parser for UCCA was presented by Hershcovich et al. (2017). It utilized 158 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 158–165 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2015 2.0 and AMR 1.0. The proposed attention-based model is aligner-free and deals with AMR parsing as sequence-to-graph task. Additionally, the authors proposed an alternative view on reentrancy converting an AMR graph into"
L16-1015,J92-4003,0,0.304047,"we say that something is an adverb in language X, we should be able to support such a claim by some measurable evidence rather than just by saying that it becomes an adverb if translated to English. 2. Related Work There is a body of literature about POS tagging of under-resourced languages. Most approaches rely on the existence of some form of parallel (or comparable) data. We will discuss only those approaches that attempt at using the same tagset across languages, and not those aiming at unsupervised induction, such as the well-known Brown clusters induced in a fully unsupervised fashion (Brown et al., 1992). An overview of such truly unsupervised approaches can be found in (Christodouloupoulos et al., 2010).2 • For some multilingual NLP tasks, such as unsupervised dependency parsing (or parser transfer), it might be more important to preprocess all languages under study as similarly as possible (including POS tagging), rather than to maximize accuracy with respect to highly different gold-standard data in individual languages. 1 However, we do not say that our method is completely language-independent. For instance, we rely on the existence of a meaningful tokenization in the target language. 2"
L16-1015,D10-1056,0,0.0699114,"Missing"
L16-1015,W02-2006,0,0.0990035,"Missing"
L16-1015,P11-1061,0,0.321767,"because they rely on dictionaries or parallel corpora such as the Bible. In this paper, we propose a different method named delexicalized tagging, for which we only need a raw corpus of the target language. We transfer tagging models trained on annotated corpora of one or more resource-rich languages. We employ language-independent features such as word length, frequency, neighborhood entropy, character classes (alphabetic vs. numeric vs. punctuation) etc. We demonstrate that such features can, to certain extent, serve as predictors of the part of speech, represented by the universal POS tag (Das and Petrov, 2011). Keywords: delexicalized tagging, HamdleDT 2.0, features expansion, classifier 1. Introduction languages); the model is independent of individual word forms. In delexicalized parsing, word form sequences are substituted by sequences of POS tags, which—of course— is not extendable to tagging. Instead, we substitute word forms by vectors of numerical features that can be computed using only unannotated monolingual texts. The background intuition is that the individual POS categories will tend to manifest similar statistical properties across languages (e.g., prepositions tend to be short, relat"
L16-1015,P13-2112,0,0.0359974,"Missing"
L16-1015,I05-1075,0,0.080472,"Missing"
L16-1015,majlis-zabokrtsky-2012-language,1,0.885772,"Missing"
L16-1015,L16-1262,1,0.852652,"Missing"
L16-1015,petrov-etal-2012-universal,0,0.0316314,"sulting classifier is used to assign POS tags to all words’ feature vectors in the target languages, X NN = f (y) y∈N ext(w) X − y∈N ext(w) 4. we evaluate our approach on the target languages for which there are labeled data available, and assume that reasonably similar accuracies are reached also for the other target languages. f (y) f (y) log NN NN 5. substituting word entropy X SN = f (y) y∈Subst(w) 3.2. Tagset X A prerequisite to our approach is a common tagset for both the source and the target languages. We use the same tagset as (Das and Petrov, 2011), the Google Universal POS tag set (Petrov et al., 2012). With just 12 tags it is fairly y∈Subst(w) 3 97 − f (y) f (y) log SN SN http://universaldependencies.org/ 3.4. 6. is number – binary value is number(w), In our approach, we need two types of data resources: 7. is punctuation – binary value is punctuation(w), • raw monolingual texts for both source and target languages; this data is used for extracting feature vectors for words in individual languages; we use W2C, a web-based corpus of 120 languages (Majliˇs and ˇ Zabokrtsk´ y, 2012), 8. relative frequency after number log |i : ci = w ∧ is number(ci−1 )| f (w) • POS-tagged data for source lang"
L16-1015,N01-1026,0,0.208375,"Missing"
L16-1015,I08-3008,1,0.770159,"n the existence of a meaningful tokenization in the target language. 2 There is a certain terminological confusion in this area: sometimes the word “unsupervised” is used also for situations in which there are no hand-tagged data available for the target language, but some manual annotation of the source language exists and is projected across parallel data like in (Das and Petrov, 2011). We prefer to avoid the term “unsupervised” when manual annotation is used in any language. We propose “delexicalized tagging”, a new method for under-resourced languages. In analogy to delexicalized parsing (Zeman and Resnik, 2008), we transfer a tagging model from a resource-rich language (or a set of 96 (Yarowsky and Ngai, 2001) project POS tags from English to French and Chinese via both automatic and gold alignment, and report substantial growth of accuracy after using de-noising postprocessing. (Fossum and Abney, 2005) extend this approach by projecting multiple source languages onto a target language. (Das and Petrov, 2011) use graph-based label propagation for cross-lingual knowledge transfer, and estimate emission distributions in the target language using a loglinear model. (Duong et al., 2013) choose only auto"
L16-1015,P15-2044,0,\N,Missing
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
L16-1630,D11-1031,0,0.0192561,"linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where subst"
L16-1630,W13-2322,0,0.201241,"ns as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer t"
L16-1630,W08-2222,0,0.0201912,"and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collec"
L16-1630,Q15-1040,1,0.86691,"banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was perf"
L16-1630,J93-2004,0,0.0550061,"ies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “includ"
L16-1630,S14-2082,0,0.0694129,"information. We envision that general availability of a standardized and comprehensive set of semantic dependency graphs and associated tools will stimulate more research in this sub-area of semantic parsing. To date, reported ‘parsing success’ 7 To seek to relate these different approaches to the encoding of lexical valency, one can multiply out the DM frame identifiers with verb lemmata, which yields a count of some 4,600 distinct combinations, i.e. slightly less than the set of observed sense distinctions in PSD. measures in terms of dependency F1 range between the high seventies for PSD (Martins & Almeida, 2014) and high eighties to low nineties for CCD, DM, and PAS (Du et al., 2015; Miyao et al., 2014). Such variation may in principle be owed to differences in the number and complexity of linguistic distinctions made, to homogeneity and consistency of training and test data, and of course to the cumulative effort that has gone into pushing the state of the art on individual target representations. A deeper understanding of these parameters, as well as of contentful vs. superficial linguistic differences across frameworks, will be a prerequisite to judging the relative suitability of different resour"
L16-1630,J07-4004,0,0.0382917,"bank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannot"
L16-1630,copestake-flickinger-2000-open,1,0.723309,"al trees), and with corresponding ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic propert"
L16-1630,S14-2056,1,0.955729,"s). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Jo"
L16-1630,P15-1149,0,0.063476,"s of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional"
L16-1630,flickinger-etal-2014-towards,1,0.859802,"ing ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks a"
L16-1630,P10-1035,0,0.0195635,"n of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBan"
L16-1630,hajic-etal-2012-announcing,1,0.917423,"Missing"
L16-1630,S15-2153,1,0.925074,"Missing"
L16-1630,S14-2008,1,0.928327,"Missing"
L16-1630,oepen-lonning-2006-discriminant,1,0.921801,"rivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). To"
L16-1630,J07-3004,0,0.0404021,"hon tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Tree"
L16-1630,W12-3602,1,0.888177,"of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). Top nodes in this representat"
L18-1290,K17-3002,0,0.0273794,"aux advmod det cop nsubj advmod nmod nsubj But not always do those three agree and not always are their decisions equal CCONJ PART ADV AUX DET NUM VERB CCONJ PART ADV AUX PRON NOUN ADJ Figure 3: An example of a matched sentence (before conversion). 5.1. Input Data Our methodology can in principle be applied to any UD treebank. The dataset and experiments presented in this paper are based on Czech, English and Finnish treebanks from UD 2.1 (Nivre et al., 2017). In addition, large web corpora of the three languages (Zeman et al., 2017) (Ginter et al., 2017) were parsed by two parsers (Stanford (Dozat et al., 2017) and Baseline UDPipe (Zeman et al., 2017) entries in the CoNLL17 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies) and used as an additional source of trees to get more candidate material. After double parsing, only trees with identical analysis were kept to ensure the quality of the automatic parses. An informal manual inspection of these trees confirmed that the quality is sufficient. All input data are in the CoNLL-U format.3 For English, there are four UD treebanks: “Original”, LinES, ParTUT, PUD. For Czech, we used the training parts of “Original”, CAC and FicTr"
L18-1290,W17-0406,1,0.349262,"et al., 2016). The annotation style of UD does not mark ellipsis explicitly when it does not have to: most types are solved by simply promoting one orphaned dependent to the position of its missing parent. Admittedly, there are treebanks that overtly annotate a wider range of elliptical structures. Our main reason for working with UD is practical: substantial data is available in this annotation style for several dozens of languages, and state-of-the art parsers have been trained and tested on UD. The one exception where UD explicitly marks ellipsis are certain types of gapping and stripping (Droganova and Zeman, 2017), where multiple orphaned dependents of a missing predicate have to be connected using a special relation called orphan (Figure 1). In the present work we investigate how frequent are the orphan relations in data, how well can existing parsers learn to recognize them, and how can we extend the data to provide more training material and improve parsing accuracy. 2. Data For the purpose of the experiments we use the system outputs from the CoNLL 2017 Shared Task (Zeman et al., 2017), that are now available as a corpus. We chose 12 teams whose systems surpassed baseline results (Zeman et al., 201"
L18-1290,Q16-1025,0,0.0212181,"English English web Finnish web Related Work The idea of artificial generation or modification of corpora is not new and it has been occasionally applied to various areas of language learning, whenever the studied phenomenon is underrepresented in existing resources. To name just a few: In (van der Plas et al., 2009), creation of an artificial treebank from an existing text treebank helps to overcome domain differences. (Khoshnavataher et al., 2015) artificialy modify text to look like obfuscated plagiarism; the resulting corpus is used to train a plagiarismdetecting system for Persian. And (Gulordava and Merlo, 2016) generate word-order permutations to study the imInitial 1.7M / 102K 23M / 2M 408K / 24K 883K / 89K 31M / 4.3M Processed 13K / 498 37K / 2369 6.8K / 284 6.4K / 422 31K / 2442 Manual NA NA 3.7K / 183 3.6K / 238 13K / 1000 Table 3: The size of the data. Initial: the size of the input data, tokens/sentences; Processed: the size of the data after the application of the conversion pipeline, tokens/sentences; Manual: the size of the data after manual correction, tokens/sentences 1851 pact of word order on parsing accuracy in twelve different languages. 8. Conclusion We have presented experiments tha"
L18-1290,L16-1262,1,0.886738,"Missing"
L18-1290,W17-0412,0,0.0295645,"Missing"
L18-1290,W17-0416,0,0.0210676,"s contain a “nsubj” (subject) and an “advmod” (adverbial modifier). After transformation the sentence would lose an adjective and its dependent. The new structure is shown in Figure 4. It should be mentioned that patterns do not require a particular word order, only particular dependents. Thus the sentence in Figure 5 is a match as well. The methodology requires manual efforts. After application of the script, the data have to be checked and corrected: • After artificial omission sentences must remain grammatically correct (Figure 10, Figure 11); Creating Artificial Treebanks Recent research (Schuster et al., 2017; Droganova and Zeman, 2017) provides a detailed overview of elliptical constructions within the UD framework and presents typical patterns that can be used for detection of elliptic constructions. This information allows us to develop a script that transforms non-elliptic UD style trees to elliptic trees. Figure 2 shows a subtree pattern that matches sentences where gapping (Johnson, 2009) could potentially occur (but 1846 • The patterns are designed to match as many instances as possible, so the erroneous instances have to be filtered out or manually corrected. All sentences at Figures 3, 5"
L18-1290,W17-7604,0,0.0123989,"pronoun, we create an elliptical sentence where two actors perform presumably the same action but with different patients (Figure 6, Figure 7). Another example would be a rule for copular constructions. If the main clause contains a copula, the sentence must be converted into Type 2 structure (Figure 8, Results We provide artificial ellipsis treebanks for three languages, Czech, English and Finnish, using our processing pipeline explained in previous sections. Furthermore, the data for English and Finnish is manually checked and fixed to be grammatical and naturally-sound using UD Annotatrix (Tyers et al., 2017) annotation tool. This further ensures the 1850 conj cc obj xcomp advmod obj det obl amod Tunsin itseni onnelliseksi mutta samaan aikaan tunsin my¨os syv¨aa¨ kaipausta I felt myself happy but at the same time I felt also deep yearning VERB PRON ADJ CCONJ PRON NOUN VERB ADV ADJ NOUN Figure 16: An example of a Finnish sentence automatically identified in the parsed Finnish web corpus. conj cc orphan xcomp advmod obj det amod Tunsin itseni onnelliseksi mutta samaan aikaan my¨os syv¨aa¨ kaipausta I felt myself happy but at the same time also deep yearning VERB PRON ADJ CCONJ PRON NOUN ADV ADJ NOUN"
L18-1290,N09-2032,0,0.0229589,"Missing"
L18-1290,K17-3001,1,0.889728,"Missing"
P13-1051,C00-2143,0,0.133964,"rinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling"
P13-1051,W06-2920,0,0.0228052,"section, we identify the CS styles defined in the previous section as used in the primary treebank data sources; statistical observations (such as the amount of annotated shared modifiers) presented here, as well as experiments on CS-style convertibility presented in Section 5.2, are based on the normalized shapes of the treebanks as contained in the HamleDT 1.0 treebank collection (Zeman et al., 2012).15 Some of the treebanks were downloaded individually from the web, but most of them came from previously published collections for dependency parsing campaigns: six languages from CoNLL-2006 (Buchholz and Marsi, 2006), seven languages from CoNLL-2007 (Nivre et al., 2007), two languages from CoNLL-2009 (Hajiˇc and others, 2009), three languages from ICON-2010 (Husain et al., 2010). Obviously, there is a certain risk that the CS-related information contained in the source treebanks was slightly biased by the properties of the CoNLL format upon conversion. In addition, many of the treebanks were natively dependency-based (cf. the 2nd column of Table 1), but some were originally based on constituents and thus specific converters to the CoNLL format had to be created (for instance, the Spanish phrase-structure"
P13-1051,dzeroski-etal-2006-towards,0,0.138307,"CS head, or attaching shared modifiers below the nearest conjunct). Even if it does not make sense to create the full Cartesian product of all dimensions because some values cannot be combined, it allows to explore the space of possible CS styles systematically.9 One can find various arguments supporting the particular choices. MTT possesses a complex set of linguistic criteria for identifying the governor of a relation (see Mazziotta (2011) for an overview), which lead to MS. MS is preferred in a rule-based dependency parsing system of Lombardo and Lesmo (1998). PS is advocated by ˇ ep´anek (2006) who claims that it can represent Stˇ shared modifiers using a single additional binary attribute, while MS would require a more complex co-indexing attribute. An argumentation of Tratz and Hovy (2011) follows a similar direction: We would like to change our [MS] handling of coordinating conjunctions to treat the coordinating conjunction as the head [PS] because this has fewer ambiguities than [MS]. . . We conclude that the influence of the choice of coordination style is a well-known problem in dependency syntax. Nevertheless, published works usually focus only on a narrow ad-hoc selection of"
P13-1051,W12-0503,1,0.894933,"Missing"
P13-1051,afonso-etal-2002-floresta,0,0.0183801,"hers, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinatio"
P13-1051,W03-2405,0,0.0337781,"Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling variants defined in Section 3.2. Variations in representing coordination structures Our analysis of variations in representing coordination structures is based on observations from a set of dependency treebanks for 26 languages.7 5 We use the already establishe"
P13-1051,W09-1201,1,0.0605496,"Missing"
P13-1051,W12-3603,1,0.894647,"Missing"
P13-1051,ramasamy-zabokrtsky-2012-prague,1,0.894745,"Missing"
P13-1051,E09-1047,0,0.144276,"Missing"
P13-1051,W98-0502,0,0.490473,"tactic surface means of expressing coordination relations is fuzzy. Some languages can use enclitics instead of conjunctions/prepositions, e.g. Latin “Senatus Populusque Romanus”. Purely hypotactic surface means such as the preposition in “John with Mary” occur too.4 Related work Let us first recall the basic well-known characteristics of CSs. In the simplest case of a CS, a coordinating conjunction joins two (usually syntactically and semantically compatible) words or phrases called conjuncts. Even this simplest case is difficult to represent within a dependency tree because, in the words of Lombardo and Lesmo (1998): Dependency paradigms exhibit obvious difficulties with coordination because, differently from most linguistic structures, it is not possible to characterize the coordination construct with a general schema involving a head and some modifiers of it. Proper formal representation of CSs is further complicated by the following facts: • Careful semantic analysis of CSs discloses additional complications: if a node is modified by a CS, it might happen that it is the node itself (and not its modifiers) what should be semantically considered as a conjunct. Note the difference between “red and white"
P13-1051,J93-2004,0,0.0420156,"rent granularity of syntactic labels. 3 3.1 Topological variations We distinguish the following dimensions of topological variations of CS styles (see Figure 1): Family – configuration of conjuncts. We divide the topological variations into three main groups, labeled as Prague (fP), Moscow (fM), and vided by IXA Group) (Aduriz and others, 2003), Bulgarian: BulTreeBank (Simov and Osenova, 2005), Czech: Prague Dependency Treebank 2.0 (Hajiˇc et al., 2006), Danish: Danish Dependency Treebank (Kromann et al., 2004), Dutch: Alpino Treebank (van der Beek and others, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treeban"
P13-1051,D07-1013,0,0.158065,"Missing"
P13-1051,taule-etal-2008-ancora,0,0.0175746,"Missing"
P13-1051,zeman-etal-2012-hamledt,1,0.0729458,"Missing"
P13-1051,D11-1116,0,\N,Missing
P13-1051,J03-4003,0,\N,Missing
P13-1051,D07-1096,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2006-generating,0,\N,Missing
rosa-etal-2014-hamledt,zeman-2008-reusable,1,\N,Missing
rosa-etal-2014-hamledt,J93-2004,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2014-universal,0,\N,Missing
rosa-etal-2014-hamledt,C00-2143,0,\N,Missing
rosa-etal-2014-hamledt,W08-1301,0,\N,Missing
rosa-etal-2014-hamledt,W13-3721,0,\N,Missing
rosa-etal-2014-hamledt,D11-1006,0,\N,Missing
rosa-etal-2014-hamledt,P13-1051,1,\N,Missing
rosa-etal-2014-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
rosa-etal-2014-hamledt,berovic-etal-2012-croatian,0,\N,Missing
rosa-etal-2014-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
rosa-etal-2014-hamledt,W03-2405,0,\N,Missing
rosa-etal-2014-hamledt,P13-2017,0,\N,Missing
rosa-etal-2014-hamledt,taule-etal-2008-ancora,0,\N,Missing
rosa-etal-2014-hamledt,W10-1819,0,\N,Missing
rosa-etal-2014-hamledt,afonso-etal-2002-floresta,0,\N,Missing
S14-2008,D12-1133,0,0.0149346,"ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t"
S14-2008,W06-2920,0,0.101128,"em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz & Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier simil"
S14-2008,de-marneffe-etal-2006-generating,0,0.0702807,"Missing"
S14-2008,oepen-lonning-2006-discriminant,1,0.758648,"antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czec"
S14-2008,W09-1201,1,0.855031,"Missing"
S14-2008,J05-1004,0,0.129381,"ependency graphs for Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o"
S14-2008,P06-1055,0,0.0124328,"e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i"
S14-2008,hajic-etal-2012-announcing,1,0.772691,"Missing"
S14-2008,W12-3602,1,0.885947,"yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical ("
S14-2008,C08-1095,0,0.478753,".27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae & Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par"
S14-2008,P10-5006,0,0.0693315,"ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure"
S14-2008,J93-2004,0,0.0590496,"t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: Task 8 at SemEval 2014 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure"
S14-2008,P07-1031,0,0.0115637,"ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass"
S14-2008,meyers-etal-2004-annotating,0,0.0465249,"Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another"
S14-2008,S14-2056,1,0.893424,"pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCE"
S14-2008,C10-1011,0,\N,Missing
S14-2008,S14-2080,0,\N,Missing
S14-2008,S14-2082,0,\N,Missing
S14-2008,J02-3001,0,\N,Missing
S14-2008,W15-0128,1,\N,Missing
S14-2008,D07-1096,0,\N,Missing
S14-2008,cinkova-2006-propbank,0,\N,Missing
S14-2056,D11-1037,1,0.860866,"ing data is positively biased towards our ensemble members.13 But even with this caveat, it seems fair to observe that the ERG and Enju parsers both are very competitive for the DM and PAS target representations, respectively, specifically so when judged in exact match scores. A possible explanation for these results lies in the depth of grammatical information available to these parsers, where DM or PAS semantic dependency graphs are merely a simpliefied view on the complete underlying HPSG analyses. These parsers have performed well in earlier contrastive evaluation too (Miyao et al., 2007; Bender et al., 2011; Ivanova et al., 2013; inter alios). Results for the Treex English parsing scenario, on the other hand, show that this ensemble member is not fine-tuned for the PCEDT target representation; due to the reasons mentioned above, its performance even falls behind the shared task baseline. As is evident from the comparison of labeled vs. unlabeled F1 scores, (a) the PCEDT parser is comparatively stronger at recovering semantic dependency structure than at assigning labels, and (b) about the same appears to be the case for the best-performing Priberam system (on this target representation). by the"
S14-2056,hajic-etal-2012-announcing,0,0.189879,"Missing"
S14-2056,A00-2022,1,0.412002,"er most commonly used with the ERG, called PET (Callmeier, 2002),1 constructs a complete, This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ 1 The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. conversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Par"
S14-2056,J93-2004,0,0.0466436,"plied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) f"
S14-2056,S14-2082,0,0.0922872,"rees can contain generated nodes, which represent elided words and do not correspond to any surface to5 Results and Reflections Seeing as our ‘in-house’ parsers are not directly trained on the semantic dependency graphs provided for the Task, but rather are built from additional linguistic resources, we submitted results from the parsing pipelines sketched in Sections 2 to 4 above to the open SDP track. Table 1 summarizes parser performance in terms of labeled and unlabeled F1 (LF and UF)12 and fullsentence exact match (LM and UM), comparing to the best-performing submission (dubbed Priberam; Martins and Almeida, 2014) to this track. Judging by the official SDP evaluation metric, average labeled F1 over the three representations, our ensemble ranked last among six participating 10 The system was able to output the following functors (ordered in the descending order of their frequency in the system output): RSTR, PAT, ACT, CONJ.member, APP, MANN, LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP, MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND, TSIN, REG, DIR2, CNCS, and TTILL. 11 In the SDP context, the target representation derived from the PCEDT is called by the same name as the original treebank; but note th"
S14-2056,S14-2008,1,0.87444,"Missing"
S14-2056,H05-1066,0,0.0955956,"Missing"
S14-2056,J05-1004,0,0.0249374,"ere used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for Engli"
S14-2056,W07-2207,1,0.819624,"ge numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ 1 The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. conversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Parsing System Enju Predicate–Argument Structures (PAS) are derived from the automatic HPSG-style annotation of the PTB, which was primarily used for the development of the Enj"
S14-2056,J08-1002,1,0.798501,"used for the development of the Enju parsing system4 (Miyao, 2006). A notable feature of this parser is that the grammar is not developed by hand; instead, the Enju HPSG-style treebank is first developed, and the grammar (or, more precisely, the vast majority of lexical entries) is automatically extracted from the treebank (Miyao et al., 2004). In this ‘projection’ step, PTB annotations such as empty categories and coindexation are used for deriving the semantic representations that correspond to HPSG derivations. Its probabilistic model for disambiguation is also trained using this treebank (Miyao and Tsujii, 2008).5 The PAS data set is an extraction of predicate– argument structures from the Enju HPSG treebank. The Enju parser outputs results in ‘readyto-use’ formats like phrase structure trees and predicate–argument structures, as full HPSG analyses are not friendly to users who are not familiar with the HPSG theory. The gold-standard PAS target data in the Task was developed using this function; the conversion program from full HPSG analyses to predicate–argument structures was applied to the Enju Treebank. Predicate–argument structures (PAS) represent word-to-word semantic dependencies, such as sema"
S14-2056,W12-3602,1,\N,Missing
S14-2056,D07-1096,0,\N,Missing
S14-2056,oepen-lonning-2006-discriminant,1,\N,Missing
S14-2056,W13-5707,1,\N,Missing
S15-2153,D12-1133,0,0.036307,"semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2015 test data.11 To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtaine"
S15-2153,cinkova-2006-propbank,1,0.772792,".6993 .5743 .6719 − .5630 .5675 .5490 − Table 2: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table 1 reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discardin"
S15-2153,de-marneffe-etal-2006-generating,0,0.0580061,"Missing"
S15-2153,S14-2080,0,0.42538,"ably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) tra"
S15-2153,hajic-etal-2012-announcing,1,0.91158,"Missing"
S15-2153,W12-3602,1,0.93906,"ctionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as ‘loca"
S15-2153,P10-5006,0,0.0554855,"R-arg ACT-arg PAT-arg RSTR EXT RSTR CONJ.m APPS.m ADDR-arg APPS.m CONJ.m A similar technique is almost impossible to apply to other crops , such as cotton , soybeans _ _ _ ev-w218f2 _ _ _ ev-w119f2 _ _ _ _ _ _ _ _ _ CONJ.m and _ rice . _ _ (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD). Figure 1: Sample semantic dependency graphs for Example (1). sentence’ semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be task- and domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (o"
S15-2153,J93-2004,0,0.0679778,"stitute of Informatics, Tokyo Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics • Stanford University, Center for the Study of Language and Information ♠ ◦ sdp-organizers@emmtee.net Abstract more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic de"
S15-2153,S14-2082,0,0.0634842,"ormation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) transition-based dependency graph parsing in the spirit of Titov et al. (2009) (Du et al., 2014) extended with weighted tree approximation, parser ensemble (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble sequence labeling for argument detection for each predicate, SVM classifiers for top node recognition and sense prediction ERG & Enju companion — — — companion Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structure"
S15-2153,meyers-etal-2004-annotating,0,0.0555434,"the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:"
S15-2153,S14-2056,1,0.858363,"ndencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. 916 tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived f"
S15-2153,S14-2008,1,0.363094,"Missing"
S15-2153,J05-1004,0,0.359612,"s preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-"
S15-2153,P07-1031,0,0.0187644,"ersely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.10 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns"
W01-1832,P98-1080,0,\N,Missing
W01-1832,C98-1077,0,\N,Missing
W01-1832,P99-1065,0,\N,Missing
W05-1518,W98-1118,0,0.0274576,"more difficult once some threshold has been touched, exploring the potential of approach combination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). The authors have investigated two combination techniques (constituent voting and"
W05-1518,P98-1029,0,0.0277036,"hich often leads to the development of several different approaches to the same problem. If these approaches are independent enough in terms of not producing the same kinds of errors, there is a hope that their combination can bring further improvement to the field. While improving any single approach gets more and more difficult once some threshold has been touched, exploring the potential of approach combination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parser"
W05-1518,A00-2018,0,0.103377,"rough N) to each word. In that sense, a dependency parser is similar to classifiers like POS taggers. Unless it deliberately fails to assign a parent to a word (or assigns 171 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 171–178, c Vancouver, October 2005. 2005 Association for Computational Linguistics Par- Author ser ec Eugene Charniak mc Michael Collins zž dz thr thl thp Zden k Žabokrtský Daniel Zeman Tomáš Holan Brief description Accuracy Tune Test A maximum-entropy inspired parser, home in constituency-based structures. English version described in Charniak (2000), Czech adaptation 2002 – 2003, unpublished. Uses a probabilistic context-free grammar, home in constituencybased structures. Described in (Haji et al., 1998; Collins et al., 1999). Purely rule-based parser, rules are designed manually, just a few lexical lists are collected from the training data. 2002, unpublished. A statistical parser directly modeling syntactic dependencies as word bigrams. Described in (Zeman, 2004). 83.6 85.0 81.7 83.3 74.3 76.2 73.8 75.5 71.0 Three parsers. Two of them use a sort of push-down automata and differ from each other only in the way they process the sentence"
W05-1518,N03-1004,0,0.0246351,"r more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). The authors have investigated two combination techniques (constituent voting and naïve Bayes), and two ways of their application to the (full) parsing: parser switching, and similarity switching. They were able to gain 1.6 con"
W05-1518,P99-1065,0,0.0923575,"Missing"
W05-1518,W02-1004,0,0.0198748,"ombination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). The authors have investigated two combination techniques (constituent voting and naïve Bayes), and two ways of their application to the (full) parsing: parser switching, and si"
W05-1518,A94-1016,0,0.0162445,"can bring further improvement to the field. While improving any single approach gets more and more difficult once some threshold has been touched, exploring the potential of approach combination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 19"
W05-1518,P98-1081,0,0.0278628,"Missing"
W05-1518,J01-2002,0,0.0363613,"Missing"
W05-1518,C00-1051,0,0.0362285,"Missing"
W05-1518,W99-0623,0,\N,Missing
W05-1518,C98-1078,0,\N,Missing
W05-1518,C98-1029,0,\N,Missing
W09-1219,burchardt-etal-2006-salsa,0,0.140217,"Missing"
W09-1219,kawahara-etal-2002-construction,0,0.0504944,"rmation: • parent (syntactic dependency) for each token 1 For more details on the two tasks and challenges, see Haji et al. (2009). • label for each syntactic dependency (token) • label for every predicate • for every token (predicate or nonpredicate) A and every predicate P in the sentence, say whether there is a semantic relation between P and A (A is an argument of P) and if so, provide a label for the relation (role of the argument) The organizers of the shared task provided training and evaluation data (Haji et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Taulé et al., 2008; Kawahara et al., 2002; Xue and Palmer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is a sequence of three components: a surface syntactic parser, a syntactic tagger that assigns labels to the syntactic dependencies and a semantic classifier (labels both the predicates and the roles of their arguments). We did not attempt to gain advantage from training a joint classifier for all the subtasks. We did not have time to do much beyond putting together the basic infrastructure. The components 2 and 3 are thus fairly primitive. 2.1 Surface Dependency Parser We use the parser de"
W09-1219,zeman-2008-reusable,1,\N,Missing
W09-1219,W08-2121,0,\N,Missing
W09-1219,W09-1201,0,\N,Missing
W09-1219,taule-etal-2008-ancora,0,\N,Missing
W10-1732,J07-2003,0,0.0514656,"em on moderately-sized corpora for the other three languages and seven translation directions. We describe our experiments with hierarchical phrase-based machine translation for WMT 2010 Shared Task. We provide a detailed description of our configuration and data so the results are replicable. For English-to-Czech translation, we experiment with several datasets of various sizes and with various preprocessing sequences. For the other 7 translation directions, we just present the baseline results. 1 2 The Translation System Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using"
W10-1732,N03-1017,0,0.00823088,"ls (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: Introduction Czech is a language with rich morphology (both inflectional and derivational) and relatively free word order. In fact, the predicate-argument structure, often encoded by fixed word order in English, is usually captured by inflection (especially the system of 7 grammatical cases) in Czech. While the"
W10-1732,P07-2045,0,0.0125766,"ces. For the other 7 translation directions, we just present the baseline results. 1 2 The Translation System Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: Introduction Czech is a language with rich morphology (both inflectional and deriva"
W10-1732,J03-1002,0,0.00367099,"s to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: Introduction Czech is a language with rich morphology (both inflectional and derivational) and relatively free word order. In fact, the predicate-argument structure, often encoded by fixed word order in English, is usuall"
W10-1732,E99-1010,0,0.0598773,"Translation System Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: Introduction Czech is a language with rich morphology (both inflectional and derivational) and relatively free word order. In fact, the predicate-argument structure"
W11-2163,J07-2003,0,0.0605725,"s not include really language-specific techniques: we neither split German compounds, nor do we address the peculiarities of Czech mentioned above. Still, comparability of the results is limited, as the quality and quantity of English-Czech data differs from that of the other pairs. 496 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 496–500, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2 The Translation System • N-best decoding: use_unique_nbest=true Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.3.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using"
W11-2163,N03-1017,0,0.0144065,"ls (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.3.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT (for the sake of reproducibility, we keep the original names of the options; for their detailed explanation please refer to the documentation available on-line at the Joshua project site). -ipi is the number of intermediate initial points per Z-MERT iteration. • Grammar extraction: maxPhraseSpan=10 maxPhraseLeng"
W11-2163,P07-2045,0,0.011878,"1 Association for Computational Linguistics 2 The Translation System • N-best decoding: use_unique_nbest=true Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.3.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT (for the sake of reproducibility, we keep the original names of the options; for the"
W11-2163,J03-1002,0,0.00260658,"s to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.3.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT (for the sake of reproducibility, we keep the original names of the options; for their detailed explanation please refer to the documentation available on-line at the Joshua project site). -ipi is the number of intermediat"
W11-2163,E99-1010,0,0.0658687,"e_unique_nbest=true Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.3.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT (for the sake of reproducibility, we keep the original names of the options; for their detailed explanation please refer to the documentation available on-line at th"
W11-2163,W08-0325,0,0.051366,"Missing"
W12-3151,N03-1017,0,0.00460604,"guage pair. The current version of the system does not include really language-speciﬁc techniques: we neither split German compounds, nor do we address the peculiarities of Czech mentioned above. 395 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 395–400, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 The Translation System Our translation system is built around Moses1 (Koehn et al., 2007). Two-way word alignment was computed using GIZA++2 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Weights of the system were optimized using MERT (Och, 2003). No lexical reordering model was trained. For language modeling we use the SRILM toolkit3 (Stolcke, 2002) with modiﬁed KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 3 Corpus cs-en de-en es-en fr-en de-cs es-cs fr-cs SentPairs 782,756 2,079,049 2,123,036 2,144,820 652,193 692,118 686,300 Tokens lng1 17,997,673 55,143,719 61,784,972 69,568,241 17,422,620 20,189,811 22,220,780 Tokens lng2 20,964,639 57,741,141 59,217,471 59,939,548 15,383,601 16,324,910 16,190,365 Table 1: Number of sentence pairs and tokens for e"
W12-3151,P07-2045,0,0.0384306,"slation accuracies and see why some directions are easier than others. Future work will beneﬁt from knowing what are the special processing needs for a given language pair. The current version of the system does not include really language-speciﬁc techniques: we neither split German compounds, nor do we address the peculiarities of Czech mentioned above. 395 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 395–400, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 The Translation System Our translation system is built around Moses1 (Koehn et al., 2007). Two-way word alignment was computed using GIZA++2 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Weights of the system were optimized using MERT (Och, 2003). No lexical reordering model was trained. For language modeling we use the SRILM toolkit3 (Stolcke, 2002) with modiﬁed KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 3 Corpus cs-en de-en es-en fr-en de-cs es-cs fr-cs SentPairs 782,756 2,079,049 2,123,036 2,144,820 652,193 692,118 686,300 Tokens lng1 17,997,673 55,143,719 61,784,972 69,568,241 17,422,620"
W12-3151,J03-1002,0,0.00389253,"uture work will beneﬁt from knowing what are the special processing needs for a given language pair. The current version of the system does not include really language-speciﬁc techniques: we neither split German compounds, nor do we address the peculiarities of Czech mentioned above. 395 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 395–400, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 The Translation System Our translation system is built around Moses1 (Koehn et al., 2007). Two-way word alignment was computed using GIZA++2 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Weights of the system were optimized using MERT (Och, 2003). No lexical reordering model was trained. For language modeling we use the SRILM toolkit3 (Stolcke, 2002) with modiﬁed KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 3 Corpus cs-en de-en es-en fr-en de-cs es-cs fr-cs SentPairs 782,756 2,079,049 2,123,036 2,144,820 652,193 692,118 686,300 Tokens lng1 17,997,673 55,143,719 61,784,972 69,568,241 17,422,620 20,189,811 22,220,780 Tokens lng2 20,964,639 57,741,141 59,217,471 59,9"
W12-3151,P03-1021,0,0.00934928,"language-speciﬁc techniques: we neither split German compounds, nor do we address the peculiarities of Czech mentioned above. 395 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 395–400, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 The Translation System Our translation system is built around Moses1 (Koehn et al., 2007). Two-way word alignment was computed using GIZA++2 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Weights of the system were optimized using MERT (Och, 2003). No lexical reordering model was trained. For language modeling we use the SRILM toolkit3 (Stolcke, 2002) with modiﬁed KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 3 Corpus cs-en de-en es-en fr-en de-cs es-cs fr-cs SentPairs 782,756 2,079,049 2,123,036 2,144,820 652,193 692,118 686,300 Tokens lng1 17,997,673 55,143,719 61,784,972 69,568,241 17,422,620 20,189,811 22,220,780 Tokens lng2 20,964,639 57,741,141 59,217,471 59,939,548 15,383,601 16,324,910 16,190,365 Table 1: Number of sentence pairs and tokens for every language pair in the parallel training corpus. Languages"
W12-3151,W08-0325,0,0.0370889,"d in an attempt to restore and normalize the directed (opening/closing) quotation marks (i.e. ""quoted"" → “quoted”). The motivation is twofold here: First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses; second, if Moses learns to output directed quotation marks, subsequent detokenization will be easier. The data are then tagged and lemmatized. We used the Morče tagger for Czech and English lemmatization and TreeTagger for German, Spanish and French lemmatization. All these tools are embedded in the Treex analysis framework (Žabokrtský et al., 2008). The lemmas are used later to compute word alignment. Besides, they are needed to apply “supervised truecasing” to the data: we cast the case of the lemma to the form, relying on our morphological analyzers and taggers to identify proper names, all other words are lowercased. Note that guessing of the true case is only needed for the sentence-initial token. Other words can typically be left in their original form, unless they are uppercased as a form of HIGHLIGHTING. 3.1 Quotation Marks A broad range of characters is used to represent quotation marks in the training data: straight ASCII quota"
W12-5614,W06-2920,0,0.0599716,"test data and we could not measure the accuracy on the test set. Later on, after the official results were announced, the full test set was released. From now on, we will refer to the original training data as dtrain, to the development data as dtest, to the combination of dtrain+dtest as etrain and to the final test data as etest. Tokens 268096 26416 294512 39775 dtrain dtest etrain etest Sentences 12041 1233 13274 1828 Table 1: Size of the various datasets. The data were provided in two file formats and two encodings of the Devanāgarī script. We work exclusively with the CoNLL data format (Buchholz and Marsi, 2006) and the UTF-8 encoding. Two versions of morphological annotation were provided, corresponding to two tracks of the shared task: Gold and Auto. As suggested by its name, the Gold version contains more information and more accurate information. In the Auto version, values of some token attributes were assigned using automated tools, and values of the (many!) remaining attributes were empty because no automatic tool was available to assign them. An example of one token from the Gold training data follows (including transliteration): Index 32 32 Token िकया kiyā Lemma कर kara CPOS VM VM POS v v Fe"
W13-2207,N03-1017,0,0.0174967,"llel corpora: the UN corpus (English, French and Spanish), the Giga French-English corpus and CzEng (Czech-English). We did not use any large corpus for Russian-English. Table 1 shows the sizes of the training data. Corpus cs-en de-en es-en fr-en ru-en de-cs es-cs fr-cs ru-cs Czeng cs-en UN es-en fr-en Giga fr-en The Translation System Both sets of experiments use the same basic framework. The translation system is built around Moses1 (Koehn et al., 2007). Two-way word alignment was computed using GIZA++2 (Och and Ney, 2003), and alignment symmetrization using the growdiag-ﬁnal-and heuristic (Koehn et al., 2003). Weights of the system were optimized using MERT (Och, 2003). No lexical reordering model was trained. For language modeling we use the SRILM toolkit3 (Stolcke, 2002) with modiﬁed KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 3 Tkns lng1 18,196,080 55,791,641 62,444,507 70,363,304 3,889,215 18,160,857 19,577,329 19,717,885 2,642,772 Tkns lng2 21,184,881 58,403,756 59,811,355 60,583,967 4,100,148 17,788,600 18,926,839 18,849,244 2,319,611 14,833,358 204,837,216 235,177,231 11,196,913 12,886,831 368,154,702 449,279,647 328,840,003 372,627,886 22,520,400 854,353,231 694,394"
W13-2207,P07-2045,0,0.022765,"nd Europarl v7 corpora.4 Note that there is only News Commentary and no Europarl for Russian. We were also able to evaluate several combinations with large parallel corpora: the UN corpus (English, French and Spanish), the Giga French-English corpus and CzEng (Czech-English). We did not use any large corpus for Russian-English. Table 1 shows the sizes of the training data. Corpus cs-en de-en es-en fr-en ru-en de-cs es-cs fr-cs ru-cs Czeng cs-en UN es-en fr-en Giga fr-en The Translation System Both sets of experiments use the same basic framework. The translation system is built around Moses1 (Koehn et al., 2007). Two-way word alignment was computed using GIZA++2 (Och and Ney, 2003), and alignment symmetrization using the growdiag-ﬁnal-and heuristic (Koehn et al., 2003). Weights of the system were optimized using MERT (Och, 2003). No lexical reordering model was trained. For language modeling we use the SRILM toolkit3 (Stolcke, 2002) with modiﬁed KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 3 Tkns lng1 18,196,080 55,791,641 62,444,507 70,363,304 3,889,215 18,160,857 19,577,329 19,717,885 2,642,772 Tkns lng2 21,184,881 58,403,756 59,811,355 60,583,967 4,100,148 17,788,600 18,926,"
W13-2207,P03-1021,0,0.0145917,"ench-English corpus and CzEng (Czech-English). We did not use any large corpus for Russian-English. Table 1 shows the sizes of the training data. Corpus cs-en de-en es-en fr-en ru-en de-cs es-cs fr-cs ru-cs Czeng cs-en UN es-en fr-en Giga fr-en The Translation System Both sets of experiments use the same basic framework. The translation system is built around Moses1 (Koehn et al., 2007). Two-way word alignment was computed using GIZA++2 (Och and Ney, 2003), and alignment symmetrization using the growdiag-ﬁnal-and heuristic (Koehn et al., 2003). Weights of the system were optimized using MERT (Och, 2003). No lexical reordering model was trained. For language modeling we use the SRILM toolkit3 (Stolcke, 2002) with modiﬁed KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 3 Tkns lng1 18,196,080 55,791,641 62,444,507 70,363,304 3,889,215 18,160,857 19,577,329 19,717,885 2,642,772 Tkns lng2 21,184,881 58,403,756 59,811,355 60,583,967 4,100,148 17,788,600 18,926,839 18,849,244 2,319,611 14,833,358 204,837,216 235,177,231 11,196,913 12,886,831 368,154,702 449,279,647 328,840,003 372,627,886 22,520,400 854,353,231 694,394,577 Table 1: Number of sentence pairs and tokens for every l"
W13-2207,spoustova-etal-2010-building,0,0.0166419,"sentences • News commentary from WMT website – 150,217 sentences Table 5: Final BLEU scores. BLEU is truecased computed by the system, BLEUl is the oﬃcial lowercased evaluation by matrix. statmt.org. BLEUt is oﬃcial truecased evaluation. Although lower oﬃcial scores are expected, notice the larger gap in en-fr and cs-fr translation. There seems to be a problem in our French detokenization procedure. 4.1 7 6 • News crawl 2012 – 9,789,861 sentences For Czech: • Czech sides of all the parallel data – 2,566,615 sentences • Data downloaded from Czech news articles9 – 1,531,403 sentences • WebColl (Spoustová et al., 2010) – 4,053,223 sentences Data For the additional Russian-to-Czech systems, we used following parallel data: • PDT 10 – 115,844 sentences • Complete Czech Wikipedia – 3,695,172 sentences • UMC 0.1 (Klyueva and Bojar, 2008) – triparallel set, consisting of news articles – 93,432 sentences • Sentences scraped from Czech social server okoun.cz – 580,249 sentences For English: • data mined from movie subtitles (described in further detail below) – 2,324,373 sentences • English sides of all the paralel data – 4,275,961 sentences • News commentary from WMT website – 150,217 sentences • Czech-Russian pa"
W13-2207,W08-0325,0,0.0132335,"ntences each language). We do not use the News Tests 2008, 2009 and 2011. All parallel and monolingual corpora underwent the same preprocessing. They were tokenized and some characters normalized or cleaned. A set of language-dependent heuristics was applied in an attempt to restore the opening/closing quotation marks (i.e. ""quoted"" → “quoted”) (Zeman, 2012). The data are then tagged and lemmatized. We used the Featurama tagger for Czech and English lemmatization and TreeTagger for German, Spanish, French and Russian lemmatization. All these tools are embedded in the Treex analysis framework (Žabokrtský et al., 2008). The lemmas are used later to compute word alignment. Besides, they are needed to apply “supervised truecasing” to the data: we cast the case of the lemma to the form, relying on our morphological analyzers and taggers to identify proper names, all other words are lowercased. Note that guessing of the true case is only needed for the sentence-initial token. Other words can typically be left in their original form, unless they are uppercased as a form of HIGHLIGHTING. 3.2.2 Larger Monolingual Data Besides the monolingual halves of the parallel corpora, additional monolingual data were provided"
W13-2207,W12-3151,1,0.145185,"Test 2013 set 4 http://www.statmt.org/wmt13/ translation-task.html#download 5 http://www.statmt.org/wmt13/ translation-task.html 1 http://www.statmt.org/moses/ 2 http://code.google.com/p/giza-pp/ 3 http://www-speech.sri.com/projects/srilm/ 86 (3000 sentences each language). We do not use the News Tests 2008, 2009 and 2011. All parallel and monolingual corpora underwent the same preprocessing. They were tokenized and some characters normalized or cleaned. A set of language-dependent heuristics was applied in an attempt to restore the opening/closing quotation marks (i.e. ""quoted"" → “quoted”) (Zeman, 2012). The data are then tagged and lemmatized. We used the Featurama tagger for Czech and English lemmatization and TreeTagger for German, Spanish, French and Russian lemmatization. All these tools are embedded in the Treex analysis framework (Žabokrtský et al., 2008). The lemmas are used later to compute word alignment. Besides, they are needed to apply “supervised truecasing” to the data: we cast the case of the lemma to the form, relying on our morphological analyzers and taggers to identify proper names, all other words are lowercased. Note that guessing of the true case is only needed for the"
W13-2207,J03-1002,0,\N,Missing
W14-3326,D11-1033,0,0.426651,"ain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general dom"
W14-3326,2011.iwslt-evaluation.18,0,0.0458693,"ction 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et"
W14-3326,bojar-etal-2012-joy,1,0.843764,"Missing"
W14-3326,N13-1073,0,0.0271109,"s are trained on the monolingual data in the target language (constrained or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables"
W14-3326,C04-1114,0,0.358032,", 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provi"
W14-3326,E12-3006,0,0.0294557,"Missing"
W14-3326,2005.eamt-1.19,0,0.105464,"nd Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the stan"
W14-3326,2011.iwslt-papers.5,0,0.0956886,"aining phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general domain” here is used to denote data Statistical"
W14-3326,P10-2041,0,0.508413,") or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general"
W14-3326,W08-0320,0,0.122804,"paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; M"
W14-3326,W07-0733,0,0.0769909,"work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation mo"
W14-3326,P03-1021,0,0.0285359,"Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the constrained ones. Linear interpolation outperforms data concatenation quite reliably"
W14-3326,P07-2045,0,0.00503854,"ined or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expecte"
W14-3326,2011.mtsummit-plenaries.5,0,0.0420922,"xts of nonmedical patents in the PatTR collection. Parallel data The parallel data summary is presented in Table 1. The main sources of the medical-domain data for all the language pairs include the EMEA corpus (Tiedemann, 2009), the UMLS metathesaurus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12"
W14-3326,2005.mtsummit-papers.11,0,0.0172802,"and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distribution of the domain-specificity s"
W14-3326,C10-2124,0,0.432112,"arch queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt l"
W14-3326,W02-1405,0,0.131062,"ranslation of search queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training co"
W14-3326,E12-1055,0,0.0150313,"n con unc unc concat interpol concat interpol cs→en 30.87±4.70 32.46±5.05 34.88±5.04 33.82±5.16 de→en 33.21±5.03 33.74±4.97 31.24±5.59 34.19±5.27 en→cs 23.25±4.85 21.56±4.80 22.61±4.91 23.93±5.16 en→de 17.72±4.75 16.90±4.39 19.13±5.66 15.87±11.31 en→fr 28.64±3.77 29.34±3.73 33.08±3.80 31.19±3.73 fr→en 35.56±4.94 35.28±5.26 36.73±4.88 40.25±5.14 Table 4: BLEU scores of query translations. each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. The two language models for sentence scoring are trained with a restricted vocabulary extracted from the in-domain training data as words occurring at least twice (singletons and other words are treated as out-of-vocabulary). In our experiments, we apply this technique to select both monolingual data for language models and parallel data for translation models. Selection of parallel data is based on the English side only. The in-domain models are trained on the monolingual data in the target language"
W14-3326,P13-1135,0,0.0121052,"rus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distri"
W14-3326,wu-wang-2004-improving-domain,0,0.0358057,"approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et"
W14-3326,W12-3151,1,0.842853,"difference) in the FR–EN parallel data and FR monolingual data is illustrated in Figures 1 and 2, respectively.6 The scores (Y axis) are presented for each sentence in increasing order from left to right (X axis). The rest of the preprocessing procedure was applied to all the datasets mentioned above, both parallel and monolingual. The data were tokenized and normalized by converting or omitting some (mostly punctuation) characters. A set of language-dependent heuristics was applied in an attempt to restore and normalize the opening/closing quotation marks, i.e. convert ""quoted"" to “quoted” (Zeman, 2012). The motivation here is twofold: First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses; second, if Moses learns to output directed quotation marks, the subsequent detokenization will be easier. For all systems which translate from German, decompounding is employed to reduce source-side data sparsity. We used BananaSplit for this task (M¨uller and Gurevych, 2006). We perform all training and internal evaluation on lowercased data; we trained recasers to postprocess the final submissions. 6 For the medical domain, constrained"
W14-3326,W12-3102,0,\N,Missing
W14-3326,eck-etal-2004-language,0,\N,Missing
W17-0406,de-marneffe-etal-2014-universal,0,0.0218713,"Missing"
W17-0406,W98-0502,0,0.0604579,"Missing"
W17-0406,P05-1013,0,0.0699442,"treebanks and we do not investigate it further in the present work. Therefore we will focus on the orphan relation in the rest of the paper. Even more radical reduction is stripping (Hankamer and Sag, 1976) where only one argument remains, assuming that the rest would be identical to • The remnant relation does not produce a clear representation if the second clause contains additional modifiers of the elided predicate; • The antecedent of the remnant may not exist in the same sentence; • The annotation style generates many nonprojective and parallel structures, thus reducing parsing quality (Nivre and Nilsson, 2005). The orphan relation is introduced to specify ellipsis more transparently2 in the UD guidelines v2 2 http://universaldependencies.org/u/ overview/specific-syntax.html#ellipsis 3 nsubj > obj > iobj > obl > advmod > csubj > xcomp > ccomp > advcl 50 the previous clause. However, the orphaned argument is usually accompanied at least by an adverb like “too” or “not”. This puts stripping in a gray zone that is not clearly delimited in the UD guidelines. Either we treat the adverb as just a connecting function word, and we attach it to the promoted argument as cc or advmod. Or we treat it as gapping"
W17-1226,C16-1012,0,0.0121703,"ctly, as there are various spelling and morpho211 proach is documented by T¨ackstr¨om et al. (2013). logical differences even between very close languages. Using such shared features allows a parser that was trained on a source treebank to be used directly on target texts; i.e. the source-target “transfer” of the parser is trivial, compared to a sourcetarget transfer of the treebank as described in §2.1. The common abstraction features used by the parser can be linguistically motivated, or induced by mathematical methods such as clustering and vector space representation: 2.3 Other variations Aufrant et al. (2016) combines both main strategies described above by adapting the word order in source sentences to be more similar to that of the target language, e.g. by swapping the order of an attribute and its nominal head; the information about these configurations was extracted from the WALS World Atlas of Language Structures (Dryer and Haspelmath, 2013). Such processing of source language trees fits to the first family of approaches, as it resembles a (very limited) MT preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a"
W17-1226,J92-4003,0,0.101154,"gs: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question"
W17-1226,W06-2920,0,0.108524,"ions whether a tree (and what kind of tree) is a reasonable representation for a sentence structure, and whether all languages do really share their structural properties to such an extent that a single type of representation is viable for all of them. Though such issues deserve intensive attention, and perhaps even more so now when UD have gained such a fascinating momentum, we take the two assumptions simply for granted. Neither do we present the genesis of the current UD collection, preceded by HamleDT treebank collection by Zeman et al. (2014), going back to the CoNLL 2006 and 2007 tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), and to earlier POS standardization efforts. In this overview, we limit ourselves to the scope outlined by the VarDial shared task, whose goal is to develop a parser for a (virtually) underresourced language closely related to a resourcerich language.2 We believe that most of the published approaches could be classified into two broad families which we call tree-transfer-based methods and common-abstraction-based methods. The former project individual dependency trees across the language boundary prior to training a target parser. The latter methods transfer a parser mode"
W17-1226,J03-1002,0,0.00596816,"translation. To reduce the OOV rate, two backoff layers are also stored, the first disregarding the morpho feats, and the second also disregarding the UPOS. An option that we leave for future research is to use the alignment scores provided by the MGA when constructing the translation table. For simplicity, we create only one joint translation table for translating DS into NO. Word-alignment Since the source and target languages in our task are very close to each other, we decided to use the heuristic Monolingual Greedy Aligner (MGA) of Rosa et al. (2012),9 rather than e.g. the usual Giza++ (Och and Ney, 2003) – most standard word aligners ignore word similarity, which we believe to be useful and important in our setting. MGA utilizes the word, lemma, and tag similarity based on Jaro-Winkler distance (Winkler, 1990), and the similarity of relative positions in the sentences, to devise a score for each potential alignment link as a linear combination of these, weighted by pre-set weights. The iterative alignment process then greedily chooses the currently highest scoring pair of words to align in each step; each word can only be aligned once. The process stops when one of the sides is fully aligned,"
W17-1226,P99-1065,0,0.341674,"Missing"
W17-1226,D15-1039,0,0.0140857,"inks. In addition, such alignment typically has a higher amount of one-to-one word alignments, which facilitates tree projection; in case of extremely close languages, as in this paper, the MT system can be constrained to produce only 1:1 translations. There are two additional advantages of the treetransfer-based approach: • the feature set used by the target language parser is independent of the features that are applicable to the source language, • we can easily use only sentence pairs (or tree fragments) with a reasonably high correspondence between source and target structures, as done by Rasooli and Collins (2015). Tree-transfer-based approaches In the tree-transfer-based approaches, a synthetic pseudo-target treebank is created by some sort of projection of individual source trees into the target language. Then a standard monolingual parser can be trained using the pseudo-target treebank in a more or less standard way. As it is quite unlikely that a manually annotated source treebank 2.2 2 Crosslingual transfer is not used only in truly underresourced scenarios, but also in situations in which it is hoped that features explicitly manifested in one language (such as morphological agreement) could boost"
W17-1226,K15-1012,0,0.0123173,"n since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question arises what is the best granularity of shared tags. The more simplified tags, the more languageuniversal information is captured, but the more information is lost at the same time. Moreover, even if two languages share a particular morphological category, e.g. pronoun reflexivity, it is hard to predict whether adding this distinction into the shared tagset"
W17-1226,P15-2040,1,0.922144,"Missing"
W17-1226,W12-4205,1,0.895855,"Missing"
W17-1226,D11-1006,0,0.0460896,"preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a few underresourced languages, choosing the best source language should be ideally automatized too. One could rely on language phylogenetic trees or on linguistic information available e.g. in WALS, or on more mechanized measures, such as KullbackLeibler divergence of POS trigram distributions ˇ (Rosa and Zabokrtsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harm"
W17-1226,L16-1680,0,0.150363,"Missing"
W17-1226,N12-1052,0,0.242999,"Missing"
W17-1226,N13-1126,0,0.0549572,"Missing"
W17-1226,C14-1175,0,0.407656,"et parser. The latter methods transfer a parser model trained directly on the source treebank, but limited only to abstract features shared by both languages. 2.1 with high-quality human-made target translations and high-quality alignment exists, one or more of the necessary components must be approximated. And even if all these data components existed, the task of dependency tree projection would inevitably lead to collisions that have to be resolved heuristically, especially in the case of many-to-one or many-to-many alignments, as investigated e.g. by Hwa et al. (2005) and more recently by Tiedemann (2014) or Ramasamy et al. (2014). This family embraces the following approaches: • using a parallel corpus and projecting the trees through word-alignment links, with authentic texts in both languages but an automatically parsed source side, • using a machine-translated parallel corpus, with only one side containing authentic texts and the other being created by MT; both translation directions have pros and cons: – source-to-target MT allows for using a gold treebank on the source side, – target-to-source MT allows the parser to learn to work with real texts in the target language, for which, in add"
W17-1226,W17-1216,0,0.0957939,"Missing"
W17-1226,N01-1026,0,0.132429,"er is trained on monolingually predicted tags, as explained in §4.1. We have found source-xtag to work well for heterogeneous source data, such as the DS mixture. Conversely, target-xtag proved useful for SK, where the source treebank is much larger than the target data used to train the target tagger. A tagger trained on the large source treebank provides much better tags, which in turn boosts the parsing accuracy, despite the noise from MT and xtag. Note that if no target tagger is available, we must either use target-xtag, or we may project a tagger across the parallel data in the style of Yarowsky and Ngai (2001) and use the resulting tagger in our baseline or source-xtag scenarios.12 We also experimented with cross-tagging of only the UPOS or only the morpho feats, with different setups being useful for different languages. Although the UDPipe tagger can also be trained to perform lemmatization, we have not found any way to obtain and utilize lemmas that would improve the cross-lingual parsing.13 12 Our approach still needs a target tagger to perform the word alignment, but we believe that for very close languages, the word forms alone might be sufficient to obtain a goodenough alignment; or, a diffe"
W17-1226,W17-1201,0,0.0972651,"Missing"
W17-1226,I08-3008,1,0.743945,"tsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual pa"
W17-1226,zeman-2008-reusable,1,0.792983,"eights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obviou"
W17-1320,N13-1049,1,0.828882,"sentation, with the addition of the semantic dashtags and the PATB complete morphological tags (BW) (Buckwalter, 2004). We supplement the trees with additional feature-value pairs representation in the style used in the MADAMIRA morphological analyzer and disambiguator (Pasha et al., 2014). We chose to convert the treebanks through this methodology to allow for the conversion of the existing CATiB treebank that has no parallel in PATB’s constituency representation. In the future, we envision enriching the CATiB treebank with the morphosyntactic features it lacks, using techniques described by Alkuhlani et al. (2013). ways headed either by a conjunction, or, if no conjunction is present, by a punctuation symbol. All conjuncts are at the same tree level. In PAUDT these structures are transformed so that the first conjunct is the head and all subsequent conjuncts are attached to it. Why Another Arabic Universal Dependency Treebank? PAUDT is based on PADT, which is a small treebank, compared to the existing PATB treebank. Our aim is to make use of the automatic conversion of PATB, parts 1, 2, and 3, into a richer version of CATiB, and use it to create NUDAR. This would allow us in the future to convert the r"
W17-1320,E12-2012,0,0.0523355,"Missing"
W17-1320,P09-2056,1,0.958176,"age Processing (NLP) applications, such as automatic summarization, question answering, and machine translation. This motivates the creation of treebanks on which these parsers can be trained. Treebanks have two main different syntactic representations. On one hand, there are phrase structure (constituency) treebanks such as the Penn Treebank (Marcus et al., 1993), and its sister treebanks such as the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) and the Penn Chinese Treebank (Xue et al., 2005). On the other hand, there are dependency treebanks, such as Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), and the Prague Dependency Treebank (PDT) (Hajiˇc et al., 2001). Other treebanks that followed the style of PDT are the Slovene (Džeroski et al., 2006) and the Croatian (Berovi´c et al., 2012) treebanks, as well as the Prague Arabic Dependency Treebank (PADT) (Smrž et al., 2002; Hajiˇc et al., 2004; Smrž et al., 2008). Having these different syntactic representations makes it difficult to compare treebanks, and parsing results (Nilsson et al., 2007). This motivated the creation of the Universal Dependency 2.1 Universal Dependencies UD is an open community effort. It builds on the existing tre"
W17-1320,berovic-etal-2012-croatian,0,0.0407184,"Missing"
W17-1320,W08-1300,0,0.250802,"ollowed the style of PDT are the Slovene (Džeroski et al., 2006) and the Croatian (Berovi´c et al., 2012) treebanks, as well as the Prague Arabic Dependency Treebank (PADT) (Smrž et al., 2002; Hajiˇc et al., 2004; Smrž et al., 2008). Having these different syntactic representations makes it difficult to compare treebanks, and parsing results (Nilsson et al., 2007). This motivated the creation of the Universal Dependency 2.1 Universal Dependencies UD is an open community effort. It builds on the existing treebank structure of the Stanford dependencies (De Marneffe et al., 2006; De Marneffe and Manning, 2008; De Marneffe et al., 2014), as well as the universal Google dependency scheme (McDonald et al., 2013). In addition, it makes use of the Google Universal Parts-of-Speech (POS) Tagset (Petrov et al., 2011), and the morphosyntactic tag set of the interset interlingua (Zeman, 2008). The aim of UD is to facilitate the creation of treebanks in different languages that are consistent in their syntactic representation, while still allowing the extension of the relations to accommodate for language-specific constructs. The target of UD is to facilitate the development of multilingual learning systems,"
W17-1320,de-marneffe-etal-2006-generating,0,0.0509096,"Missing"
W17-1320,de-marneffe-etal-2014-universal,0,0.0604251,"Missing"
W17-1320,dukes-habash-2010-morphological,1,0.870902,"Missing"
W17-1320,D07-1116,1,0.848203,"kens, improved morphology) shared tasks. An extended dataset (282K tokens) was incorporated in the HamleDT collection, where 30 treebanks were first harmonized in the Prague annotation style, later in Stanford dependencies (Rosa et al., 2014). Finally, this dataset was converted to Universal 2 In this paper we use UD to refer to the general shared concept of Universal Dependency representation. For language specific decision and treebanks we will use the name of the treebanks, i.e. PAUDT or NUDAR. 3 All Arabic transliterations are provided in the HabashSoudi-Buckwalter transliteration scheme (Habash et al., 2007b). This scheme extends Buckwalter’s transliteration scheme (Buckwalter, 2002) to increase its readability while maintaining the 1-to-1 correspondence with Arabic orthography as represented in standard encodings of Arabic, i.e., Unicode, CP-1256, etc. The following are the only differences from Buckwalter’s scheme (which is indicated in parenthe (&), Aˇ @ (&lt;), yˆ ø' (}), ¯h è (p), ¯ @ (|), Â @ (>), w ˆ ð' ses): A  (v), ð X (∗), š  ($), Dˇ (Z), ς ¨ (E), γ ¨ (g), ý ø (Y), θH ã  (F), u˜  (N), ˜ı  (K), á  (‘). 167 (Habash and Roth, 2009),4 that converts PATB trees to the CATiB represe"
W17-1320,J93-2004,0,0.060851,"AR. 1 2 Related Work In this section we present the Universal Dependency syntactic representation, as well as some of the most prominent previous efforts on Modern Standard Arabic (MSA) treebanks. Introduction Parsers have been used in many Natural Language Processing (NLP) applications, such as automatic summarization, question answering, and machine translation. This motivates the creation of treebanks on which these parsers can be trained. Treebanks have two main different syntactic representations. On one hand, there are phrase structure (constituency) treebanks such as the Penn Treebank (Marcus et al., 1993), and its sister treebanks such as the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) and the Penn Chinese Treebank (Xue et al., 2005). On the other hand, there are dependency treebanks, such as Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), and the Prague Dependency Treebank (PDT) (Hajiˇc et al., 2001). Other treebanks that followed the style of PDT are the Slovene (Džeroski et al., 2006) and the Croatian (Berovi´c et al., 2012) treebanks, as well as the Prague Arabic Dependency Treebank (PADT) (Smrž et al., 2002; Hajiˇc et al., 2004; Smrž et al., 2008). Having these different"
W17-1320,petrov-etal-2012-universal,0,0.0612175,"Missing"
W17-1320,J13-1008,1,0.821524,"e also compare the result of parsing directly in NUDAR space to parsing in CATiB space then converting to NUDAR representation. For our parsing experiments, we used the MaltParser (Nivre et al., 2006) to train an Arabic dependency parser in the space of both CATiB and NUDAR. We compared the output of the NUDAR parser, to the results of converting the output of the CATiB parser to NUDAR using the system described in Section 3. For the CATiB parser, we used the optimized settings described by Shahrour et al. (2016), and were able to achieve comparable results. We used the gold CATiBex POS tags (Marton et al., 2013), and gold morphological features derived from gold BW tags, to train the parser on the T RAIN dataset of PATB parts 1, 2, and 3. We tested on the T EST dataset of the same treebank parts. The output of the parser was then converted to NUDAR representation. For the NUDAR parser, we ran the MaltOptimizer (Ballesteros and Nivre, 2012) on the full T RAIN dataset of NUDAR. We used the optimized settings to train and run our parser. The results of these experiments are shown in Table 5. The first row shows the result of training the MaltParser on the NUDAR training dataset with the optimized settin"
W17-1320,W15-1821,0,0.0185271,"edicates without a copula are attached as if they were bare nominals. On the other hand, when a copula is involved, we reattach it as a dependent of the non-verbal predicate (in PADT, if the copula is present, it heads the clause). Similarly, prepositions head prepositional phrases in the Prague style but they are attached as modifiers of their nouns in PAUDT. Finally, coordination in the Prague style is alwith over 10 other treebanks scheduled for release in the upcoming version 2.0. The treebanks are in 47 languages, including Swedish (Nivre, 2014), Danish (Johannsen et al., 2015), Finnish (Pyysalo et al., 2015), Estonian (Muischnek et al., 2014), Norwegian (Øvrelid and Hohle, 2016), Croatian (Agi´c and Ljubeši´c, 2015), Persian (Seraji et al., 2016), Bulgarian (Osenova and Simov, 2015), Catalan and Spanish (Alonso and Zeman, 2016), as well as the Prague Arabic Universal Dependency Treebank (PAUDT), among others. 2.2 Arabic Treebanks A number of treebanks exist for MSA. These treebanks vary in terms of their syntactic representation (constituency vs. dependency), richness of annotation, and source of data. We discuss next four treebanks that are relevant to this paper. PATB: The Penn Arabic Treebank"
W17-1320,P13-2017,0,0.0221185,"t al., 2012) treebanks, as well as the Prague Arabic Dependency Treebank (PADT) (Smrž et al., 2002; Hajiˇc et al., 2004; Smrž et al., 2008). Having these different syntactic representations makes it difficult to compare treebanks, and parsing results (Nilsson et al., 2007). This motivated the creation of the Universal Dependency 2.1 Universal Dependencies UD is an open community effort. It builds on the existing treebank structure of the Stanford dependencies (De Marneffe et al., 2006; De Marneffe and Manning, 2008; De Marneffe et al., 2014), as well as the universal Google dependency scheme (McDonald et al., 2013). In addition, it makes use of the Google Universal Parts-of-Speech (POS) Tagset (Petrov et al., 2011), and the morphosyntactic tag set of the interset interlingua (Zeman, 2008). The aim of UD is to facilitate the creation of treebanks in different languages that are consistent in their syntactic representation, while still allowing the extension of the relations to accommodate for language-specific constructs. The target of UD is to facilitate the development of multilingual learning systems, and multilingual NLP, as well as allow for comparative linguistic studies and evaluation (Nivre et al"
W17-1320,rosa-etal-2014-hamledt,1,0.891739,"Missing"
W17-1320,D07-1096,0,0.190039,"Missing"
W17-1320,C16-2048,1,0.86773,"two treebanks. 4 We conducted some experiments to benchmark the parsing scores in the NUDAR treebank. We also compare the result of parsing directly in NUDAR space to parsing in CATiB space then converting to NUDAR representation. For our parsing experiments, we used the MaltParser (Nivre et al., 2006) to train an Arabic dependency parser in the space of both CATiB and NUDAR. We compared the output of the NUDAR parser, to the results of converting the output of the CATiB parser to NUDAR using the system described in Section 3. For the CATiB parser, we used the optimized settings described by Shahrour et al. (2016), and were able to achieve comparable results. We used the gold CATiBex POS tags (Marton et al., 2013), and gold morphological features derived from gold BW tags, to train the parser on the T RAIN dataset of PATB parts 1, 2, and 3. We tested on the T EST dataset of the same treebank parts. The output of the parser was then converted to NUDAR representation. For the NUDAR parser, we ran the MaltOptimizer (Ballesteros and Nivre, 2012) on the full T RAIN dataset of NUDAR. We used the optimized settings to train and run our parser. The results of these experiments are shown in Table 5. The first r"
W17-1320,P81-1022,0,0.745148,"Missing"
W17-1320,zeman-2008-reusable,1,0.772607,"ns makes it difficult to compare treebanks, and parsing results (Nilsson et al., 2007). This motivated the creation of the Universal Dependency 2.1 Universal Dependencies UD is an open community effort. It builds on the existing treebank structure of the Stanford dependencies (De Marneffe et al., 2006; De Marneffe and Manning, 2008; De Marneffe et al., 2014), as well as the universal Google dependency scheme (McDonald et al., 2013). In addition, it makes use of the Google Universal Parts-of-Speech (POS) Tagset (Petrov et al., 2011), and the morphosyntactic tag set of the interset interlingua (Zeman, 2008). The aim of UD is to facilitate the creation of treebanks in different languages that are consistent in their syntactic representation, while still allowing the extension of the relations to accommodate for language-specific constructs. The target of UD is to facilitate the development of multilingual learning systems, and multilingual NLP, as well as allow for comparative linguistic studies and evaluation (Nivre et al., 2016). In its last release of version 1.4, the UD treebank collection contained 64 different treebanks, 1 The noun Nudar PA  nuDAr is Arabic for ‘pure gold’. 166 Proceeding"
W17-1320,W15-5313,0,0.0198134,"in PADT, if the copula is present, it heads the clause). Similarly, prepositions head prepositional phrases in the Prague style but they are attached as modifiers of their nouns in PAUDT. Finally, coordination in the Prague style is alwith over 10 other treebanks scheduled for release in the upcoming version 2.0. The treebanks are in 47 languages, including Swedish (Nivre, 2014), Danish (Johannsen et al., 2015), Finnish (Pyysalo et al., 2015), Estonian (Muischnek et al., 2014), Norwegian (Øvrelid and Hohle, 2016), Croatian (Agi´c and Ljubeši´c, 2015), Persian (Seraji et al., 2016), Bulgarian (Osenova and Simov, 2015), Catalan and Spanish (Alonso and Zeman, 2016), as well as the Prague Arabic Universal Dependency Treebank (PAUDT), among others. 2.2 Arabic Treebanks A number of treebanks exist for MSA. These treebanks vary in terms of their syntactic representation (constituency vs. dependency), richness of annotation, and source of data. We discuss next four treebanks that are relevant to this paper. PATB: The Penn Arabic Treebank (Maamouri et al., 2004; Maamouri et al., 2009) is a Linguistic Data Consortium (LDC) project, for which there are currently 12 parts for MSA. PATB consists of constituency trees,"
W17-1320,pasha-etal-2014-madamira,1,0.916945,"Missing"
W17-1320,nivre-etal-2006-maltparser,0,\N,Missing
W17-1320,L16-1250,0,\N,Missing
W17-1320,L16-1374,0,\N,Missing
W17-1320,L16-1262,1,\N,Missing
W17-6532,L16-1262,1,0.887246,"Missing"
W18-5815,W17-4116,0,0.0877545,"al of a standard written system for the language (Acosta et al., 2013) and impulsed work in parsing, machine translation (Rios, 2016), and speech recognition (Zevallos and Camacho, 2018). Initial research regarding SK has centered in the development of manual annotation tools (Mercado-Gonzales et al., 2018), lexical database creation (Valencia et al., 2018), Spanish-SK parallel corpora creation and initial machine translation experiments (Galarreta et al., 2017). Related to our line of research, work by PereiraNoriega et al. (2017) addresses lemmatization but not morphological categorization. Alva and Oncevay-Marcos (2017) presents initial experiments on spell-checking using proximity of morphemes and syllable patterns extracted from annoIntroduction Linguistic and language technology research on Peruvian native languages have experienced a revival in the last few years. The academic effort was accompanied by an ambitious long term initiative driven by the Peruvian government. This initiative has the objective of systematically documenting as many native languages as possible for preservation purposes (Acosta et al., 2013). So far, writing systems and standardization have been proposed for 19 language families"
W18-5815,galarreta-etal-2017-corpus,0,0.0444498,"ls. Such is the case of Quechua, a native language spoken in South America, for which the robust system developed by (Rios, 2010) paved the way to the proposal of a standard written system for the language (Acosta et al., 2013) and impulsed work in parsing, machine translation (Rios, 2016), and speech recognition (Zevallos and Camacho, 2018). Initial research regarding SK has centered in the development of manual annotation tools (Mercado-Gonzales et al., 2018), lexical database creation (Valencia et al., 2018), Spanish-SK parallel corpora creation and initial machine translation experiments (Galarreta et al., 2017). Related to our line of research, work by PereiraNoriega et al. (2017) addresses lemmatization but not morphological categorization. Alva and Oncevay-Marcos (2017) presents initial experiments on spell-checking using proximity of morphemes and syllable patterns extracted from annoIntroduction Linguistic and language technology research on Peruvian native languages have experienced a revival in the last few years. The academic effort was accompanied by an ambitious long term initiative driven by the Peruvian government. This initiative has the objective of systematically documenting as many na"
W18-5815,E09-2008,0,0.130799,"tice that the matrix verb is chew, and the subordinated clause’s verbs carry the marker xon to indicate that the action was performed by the same agent prior to the action described in the main clause (PSSA: previous event, same subject, A orientation). [ [ Jawen tapon bi-xon ] kobin-a-xon ] Pos3 root:Abs get-PSSA boil-do.T-PSSA naka-kati-kan-ai. chew-Pst4-Pl-Inc “After getting its (i.e., a plant’s) root and boiling it, they chewed it.” Same- or switch- reference marking may also be used to encode different types of discourse (dis)continuity. 3.5 4 The analyzer was implemented using the Foma (Hulden, 2009) toolkit, following the extensive morphological description provided by Valenzuela (2003). Besides segmenting and tagging all morphemes in a word form, the analyzer also categorizes the root and the final token in order to account for any sequence of derivational processes. The analysis is of the form [POS] root[POS.root] morpheme[+Tag] ... and it is illustrated with an example in Table 1. The complete list of abbreviations and symbols used for morphological tagging can be found in the Appendix A of (Valenzuela, 2003). Language specific POS tagset was mapped to the Universal Dependencies (Nivr"
W18-5815,L18-1655,0,0.506909,"pe of such basic tools. Besides downstream applications, they are essential for the construction of annotated corpora, and consequently, for development of other tools. Such is the case of Quechua, a native language spoken in South America, for which the robust system developed by (Rios, 2010) paved the way to the proposal of a standard written system for the language (Acosta et al., 2013) and impulsed work in parsing, machine translation (Rios, 2016), and speech recognition (Zevallos and Camacho, 2018). Initial research regarding SK has centered in the development of manual annotation tools (Mercado-Gonzales et al., 2018), lexical database creation (Valencia et al., 2018), Spanish-SK parallel corpora creation and initial machine translation experiments (Galarreta et al., 2017). Related to our line of research, work by PereiraNoriega et al. (2017) addresses lemmatization but not morphological categorization. Alva and Oncevay-Marcos (2017) presents initial experiments on spell-checking using proximity of morphemes and syllable patterns extracted from annoIntroduction Linguistic and language technology research on Peruvian native languages have experienced a revival in the last few years. The academic effort was"
W18-5815,L16-1262,1,0.847749,"Missing"
W18-6004,W06-2920,0,0.120596,"that they are balanced and representative of the whole treebank according to a number of topological and annotation parameters.30 Then, the gold standard is built by manually checking the output of the automatic conversion of these 994 sentences into the UD style and fixing the mistakes. Finally, we compare the gold standard with (a) the output of our new conversion process and (b) the output of the original conversion process. We compute the rates for the usual evaluation metrics of dependency parsers: LAS (Labeled Attachment Score), LA (Label Accuracy) and UAS (Unlabeled Attachment Score) (Buchholz and Marsi, 2006). Table 1 shows the results together with the accuracy rates for PoS tagging and lemmatisation, as a way to evaluate the harmonisation phase too. Results reveal a general improvement of the quality of conversion. In particular, there is a substantial increase in LAS, while this is smaller for what concerns UAS. This shows that, while the basic TREEX conversion modules are already capable of addressing well the rearrangement of some subtrees required by the conversion to UD, they nonetheless need and greatly benefit from a language-specific fine-tuning, mainly but not only for what concerns the"
W18-6004,L16-1248,0,0.0947416,"ominal subject; cop: copula; advcl: adverbial clause; mark: marker introducing a finite subordinate clause; obl: oblique nominal (see footnote 17); conj: conjunct; cc: coordinating conjunction. The complete list of deprels and their explanations can be found at http://universaldependencies.org/u/dep/ index.html. 21 Ellipsis and apposition are challenging constructions where different UD teams have faced similar problems and sometimes found different, yet compatible, solutions. Discussion about the treatment of such constructions in different languages can be found in (Aranzabe et al., 2014), (Dobrovoljc and Nivre, 2016), (Pyysalo et al., 2015), (Tandon et al., 2016) and (Zeman, 2015). 31 orphan ancestor (in this case sicut for both) and are assigned afun ExD. Since nodes labeled with AuxP, AuxC or Coord can never take the afun ExD, this percolates down the tree to the first content word. Here, this happens from in to tinctione: AuxP AuxC ExD advcl case mark det . . . sicut baptismus in ipsa tinctione . . . Such approach shows some limitations, especially when dealing with coordinating constructions, which are quite tricky when paired with elliptical constructions. Indeed, a priori it is not possible to set a"
W18-6004,L16-1262,1,0.91442,"Missing"
W18-6004,L16-1108,1,0.820817,"D) (Nivre et al., 2016)1 has been including treebanks for ancient languages or historical phases of modern ones. In the current release of UD (2.2), there are treebanks for Ancient Greek, Gothic, Latin, Old Church Slavonic, Old French and Sanskrit. Among these languages, Latin is not only the one provided with most data in UD 2.2 (520K tokens), but also the one with the most treebanks (3). These are PROIEL (Haug and Jøhndal, 2008), which includes the entire New Testament in Latin (the so called Vulgata by Jerome) and texts from the Classical era (199K tokens), the Latin Depen1 2 For instance, Ponti and Passarotti (2016) show the dramatic decrease of accuracy rates provided by a dependency parsing pipeline trained on the IT- TB when applied on texts of the Classical era taken from the LDT. http://universaldependencies.org/ 27 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 27–36 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics guidelines since the beginning of their respective projects (Bamman et al., 2007), while PROIEL has adopted a slightly different style.3 The treebanks had been originally converted into the UD style by means of differen"
W18-6004,W15-1821,0,0.147372,"Missing"
W18-6004,rosa-etal-2014-hamledt,1,0.794526,"Missing"
W18-6004,W16-1716,0,0.0184671,"mark: marker introducing a finite subordinate clause; obl: oblique nominal (see footnote 17); conj: conjunct; cc: coordinating conjunction. The complete list of deprels and their explanations can be found at http://universaldependencies.org/u/dep/ index.html. 21 Ellipsis and apposition are challenging constructions where different UD teams have faced similar problems and sometimes found different, yet compatible, solutions. Discussion about the treatment of such constructions in different languages can be found in (Aranzabe et al., 2014), (Dobrovoljc and Nivre, 2016), (Pyysalo et al., 2015), (Tandon et al., 2016) and (Zeman, 2015). 31 orphan ancestor (in this case sicut for both) and are assigned afun ExD. Since nodes labeled with AuxP, AuxC or Coord can never take the afun ExD, this percolates down the tree to the first content word. Here, this happens from in to tinctione: AuxP AuxC ExD advcl case mark det . . . sicut baptismus in ipsa tinctione . . . Such approach shows some limitations, especially when dealing with coordinating constructions, which are quite tricky when paired with elliptical constructions. Indeed, a priori it is not possible to set a hierarchy of the ExD siblings occurring in a c"
W18-6006,K17-3002,0,0.0357898,"ctions present in the treebanks (Droganova and Zeman, 2017), while also covering different modern languages and providing variation. Decisions are based on the work by Droganova and Zeman (2017) who collected statistics on elliptical constructions that are explicitly marked with orphan relation within the UD treebanks. Relatively high number of elliptical constructions within chosen treebanks is the property of the treebanks rather than the languages. 2.2 In our experiments the main parser used in final experiments as well as labeling the crawl data, is the neural graph-based Stanford parser (Dozat et al., 2017), the winning and state-of-the-art system from the CoNLL-17 Shared Task (Zeman et al., 2017). The secondary parser for labeling the crawl data is UDPipe, a neural transition-based parser, as these parses are already provided together with the crawl data. Both of these parsers include their own part-of-speech tagger, which is trained together (but not jointly) with the dependency parser in all our experiments. In the final self-training web crawl datasets we then keep only deduplicated sentences with identical partof-speech and dependency analyses. All results reported in this paper are measure"
W18-6006,W17-0406,1,0.364789,"2.0. This UD release was used in the CoNLL-17 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2017), giving us a point of comparison to the state-of-the-art. For UD Russian-SynTagRus, we use UD release 2.1, which has a considerably improved annotation of elliptic sentences. For English, which has only a few elliptical sentences in the original treebank, we also utilize in testing a set of elliptical sentences gathered by Schuster et al. (2018). This selection of data strives to maximize the amount of elliptical constructions present in the treebanks (Droganova and Zeman, 2017), while also covering different modern languages and providing variation. Decisions are based on the work by Droganova and Zeman (2017) who collected statistics on elliptical constructions that are explicitly marked with orphan relation within the UD treebanks. Relatively high number of elliptical constructions within chosen treebanks is the property of the treebanks rather than the languages. 2.2 In our experiments the main parser used in final experiments as well as labeling the crawl data, is the neural graph-based Stanford parser (Dozat et al., 2017), the winning and state-of-the-art syste"
W18-6006,L18-1290,1,0.849997,"long coordinated item lists where the ratio drops much lower than average. We of course take into account that a relation type can naturally occur more than once in a sentence, and that it is not ideal to force the ratio close to 1.0. However, as the sampling method tries to mimic the distribution from the original treebank, it should to pick the correct variance while discarding the extremes. Artificial treebanks on elliptical constructions For specifically experimenting on elliptical constructions, we additionally include data from the semi-automatically constructed artificial treebanks by Droganova et al. (2018). These treebanks simulate gapping by removing words in particular coordination constructions, providing data for experimenting with the otherwise very rare construction. For English and Finnish the given datasets are manually curated for grammaticality and fluency, whereas for Czech the quality relies on the rules developed for the process. For Russian and Slovak, which are not part of the original artificial treebank release, we create automatically constructed artificial datasets by running the pipeline developed for the Czech language. Size of the artificial data is shown in Table 1. Czech"
W18-6006,Q17-1031,0,0.0655154,"Department of Future Technologies {droganova,zeman}@ufal.mff.cuni.cz {figint,jmnybl}@utu.fi Abstract of its missing parent, and connecting all remaining core arguments to that promoted one with the orphan relation (see Figure 1). Therefore the dependency parser must learn to predict relations between words that should not usually be connected. Gapping has been studied extensively in theoretical works (Johnson, 2009, 2014; Lakoff and Ross, 1970; Sag, 1976). However, it received almost no attention in NLP works, neither concerned with parsing nor with corpora creation. Among the recent papers, Kummerfeld and Klein (2017) proposed a one-endpoint-crossing graph parser able to recover a range of null elements and trace types, and Schuster (Schuster et al., 2018) proposed two methods to recover elided predicates in sentences with gapping. The aforementioned lack of corpora that would pay attention to gapping, as well as natural relative rarity of gapping, leads to its underrepresentation in training corpora: they do not provide enough examples for the parser to learn gapping. Therefore we investigate methods of enriching the training data with new material from large raw corpora. The present work consist of two p"
W18-6006,P14-1043,0,0.019268,"self-training and tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD English, UD Russian"
W18-6006,N06-1020,0,0.148383,"e apply a method that stands between the standard self-training and tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following"
W18-6006,L16-1262,1,0.858484,"Missing"
W18-6006,D07-1111,0,0.0178248,"agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD English, UD Russian-SynTagRus, and UD Finnish, UD Slovak. With the exception of UD RussianSynTagRus, all our experiments are based on UD release 2.0. This UD release was used in the CoNLL-17 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2017), giving us a point of comparison to the state-of-the-art. For UD Russian-SynTagRus, we use UD release 2.1, which has a considerably improved annot"
W18-6006,N18-1105,0,0.176443,"Missing"
W18-6006,C10-1120,0,0.0266349,"tands between the standard self-training and tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD En"
W18-6006,K17-3009,0,0.0267855,"Missing"
W18-6006,P15-1032,0,0.0171688,"d tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD English, UD Russian-SynTagRus, and UD Fi"
W19-4213,P17-1183,0,0.0863005,"time, and derive operations meant to be applied at the word level instead. These operations are obtained by merging initial character-level operations using the BPE algorithm (Gage, 1994). 3 We encode every string transformation henceforth, action- ai ∈ A as follows: hoperation-position-segmenti. The additional information encoded such as position and segment (characters) involved, allows actions to operate at the word level and act upon a segment of characters instead of a single character. This is a key difference between A and the action sets of most previously proposed neural transducers (Aharoni and Goldberg, 2017; Makarov and Clematide, 2018b,c) which only encode the operation to perform and consume one character at a time. 4.2 Obtaining gold action sequences We discuss now how to deterministically populate A. We start off with operations that act upon one character at a time. We derive these operations with the Damerou-Levenshtein (DL) distance algorithm which adds the transposition operation in addition to the traditional set of the edit distance algorithm. However, the set A of the form hoperation-position-segmenti directly derived by this algorithm is too large and sparse to be learned effectively"
W19-4213,W18-5815,1,0.650671,"ters need to be transformed. Our morphological tagger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hierarchical manner. Even though relative performance according to metrics is below the baseline, experiments show that our models capture important associations between interpretable operation labels and fine-grained morpho-syntax labels. 1 2 Related Work In the last few years, efforts on the analysis of endangered low-resourced languages and the development of basic language tools for them (Rios, 2016; Pereira-Noriega et al., 2017; Cardenas and Zeman, 2018) have once more brought attention into the latent necessity for research of less language-dependent models that are not unreasonably data hungry. On the other hand, more recent efforts have proposed combined strategies to bring together the transducer paradigm and neural architectures (Rastogi et al., 2016; Aharoni and Goldberg, 2016; Lin et al., 2019). For example, the neural transducer proposed by (Aharoni and Goldberg, 2016) presents a sequence to sequence architecture that decodes one character at a time while attending at the input character under a hard-monotonic constrain. However, thei"
W19-4213,P02-1001,0,0.112341,"ned strategies to bring together the transducer paradigm and neural architectures (Rastogi et al., 2016; Aharoni and Goldberg, 2016; Lin et al., 2019). For example, the neural transducer proposed by (Aharoni and Goldberg, 2016) presents a sequence to sequence architecture that decodes one character at a time while attending at the input character under a hard-monotonic constrain. However, their method relies on out-ofIntroduction Tasks related to morphological analysis have been traditionally formulated as string transduction problems tackled by weighted finite state transducers (Mohri, 2004; Eisner, 2002). More recently, however, the problem has been tackled with neural architectures featuring sequence-tosequence architectures (Kann and Sch¨utze, 2016) and neural transducers (Aharoni and Goldberg, 2016; Makarov and Clematide, 2018b,a). In this paper we describe our submission for the SIGMORPHON 2019 Shared Task related to morphological analysis and lemmatization in context (McCarthy et al., 2019). We focus on 1 We release our code at https://github.com/ ronaldahmed/morph-bandit 104 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 104–112"
W19-4213,L18-1293,0,0.0405233,"Missing"
W19-4213,N19-1024,0,0.0162782,"and fine-grained morpho-syntax labels. 1 2 Related Work In the last few years, efforts on the analysis of endangered low-resourced languages and the development of basic language tools for them (Rios, 2016; Pereira-Noriega et al., 2017; Cardenas and Zeman, 2018) have once more brought attention into the latent necessity for research of less language-dependent models that are not unreasonably data hungry. On the other hand, more recent efforts have proposed combined strategies to bring together the transducer paradigm and neural architectures (Rastogi et al., 2016; Aharoni and Goldberg, 2016; Lin et al., 2019). For example, the neural transducer proposed by (Aharoni and Goldberg, 2016) presents a sequence to sequence architecture that decodes one character at a time while attending at the input character under a hard-monotonic constrain. However, their method relies on out-ofIntroduction Tasks related to morphological analysis have been traditionally formulated as string transduction problems tackled by weighted finite state transducers (Mohri, 2004; Eisner, 2002). More recently, however, the problem has been tackled with neural architectures featuring sequence-tosequence architectures (Kann and Sc"
W19-4213,C18-1008,0,0.228717,"ated by several tokens. This can be a crucial piece of information for morphological analysis in context. This type of approach has already been extend effectively to Neural Machine Translation by (Sennrich et al., 2016), who employ simple character n-gram models and a segmentation based on the byte pair encoding (BPE) compression algorithm. This paper presents the submission by the Charles University-University of Malta team to the SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context. We present a lemmatization model based on previous work on neural transducers (Makarov and Clematide, 2018b; Aharoni and Goldberg, 2016). The key difference is that our model transforms the whole word form in every step, instead of consuming it character by character. We propose a merging strategy inspired by Byte-Pair-Encoding that reduces the space of valid operations by merging frequent adjacent operations. The resulting operations not only encode the actions to be performed but the relative position in the word token and how characters need to be transformed. Our morphological tagger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hie"
W19-4213,K18-3008,0,0.206545,"ated by several tokens. This can be a crucial piece of information for morphological analysis in context. This type of approach has already been extend effectively to Neural Machine Translation by (Sennrich et al., 2016), who employ simple character n-gram models and a segmentation based on the byte pair encoding (BPE) compression algorithm. This paper presents the submission by the Charles University-University of Malta team to the SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context. We present a lemmatization model based on previous work on neural transducers (Makarov and Clematide, 2018b; Aharoni and Goldberg, 2016). The key difference is that our model transforms the whole word form in every step, instead of consuming it character by character. We propose a merging strategy inspired by Byte-Pair-Encoding that reduces the space of valid operations by merging frequent adjacent operations. The resulting operations not only encode the actions to be performed but the relative position in the word token and how characters need to be transformed. Our morphological tagger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hie"
W19-4213,N19-1155,0,0.0133955,"er; Anlz = Analyzer We lowercase forms and lemmas before running the DL-distance algorithm. Following the BPE training procedure described by Sennrich et al. (2016), we obtain the list of merged operations from the action sequences derived from the training data. We limit the number of merges to 50. Then, these merges are applied to action sequences on the development and test data. 6.2 Lem that the string is not long enough and, hence, the action is not valid. 6.3 Baseline model We consider the baseline neural model provided by the organizers of the shared task. The architecture, proposed by Malaviya et al. (2019), performs lemmatization and morphological tagging jointly. The morphological tagging module of the model employs an LSTM-based tagger (Heigold et al., 2017), whilst the lemmatizer module employs a sequence-to-sequence architecture with hard attention mechanism (Xu et al., 2015). Training and optimization of details Both the lemmatizer and analyzer models were trained using Adam (Kingma and Ba, 2017), regularized using dropout (Srivastava et al., 2014), and employing an early stopping strategy. We tune the hyper-parameters of both models over the development set of Spanish (es ancora)3 and the"
W19-4213,W18-6011,0,0.0144343,"and (iii) an open challenge over past editions of the shared tasks. We participated in Task II for which a complete sentence of word forms is presented and lemmas and feature bundles (morpho-syntantic description labels) are to be predicted for each token. This task features an outstanding diverse pool of 66 languages from a total of 107 treebanks. Data (forms, lemmas, and feature bundles) are obtained from UniversalDependencies v.2.3 treebanks (Nivre et al., 2018). However, the feature bundles are translated into the UniMorph tagset (Kirov et al., 2018) using the mapping strategy proposed by McCarthy et al. (2018). 4 String transformations at the word level Problem Formulation Let w ∈ V and z ∈ V L be a word type and its corresponding lemma; and let A be a set of string transformation actions. We define the function T : V × Am 7→ V L that receives as input a word form w and a sequence of string transformations a = ha0 , ., ai , .., am i. T iteratively applies the transformations one at a time and returns the resulting string. The objective is to obtain a sequence of actions a such that a form w gets transformed into its lemma z, i.e. T (w, a) = z. 5 System Description In this section we describe the mo"
W19-4213,N16-1076,0,0.0231395,"ssociations between interpretable operation labels and fine-grained morpho-syntax labels. 1 2 Related Work In the last few years, efforts on the analysis of endangered low-resourced languages and the development of basic language tools for them (Rios, 2016; Pereira-Noriega et al., 2017; Cardenas and Zeman, 2018) have once more brought attention into the latent necessity for research of less language-dependent models that are not unreasonably data hungry. On the other hand, more recent efforts have proposed combined strategies to bring together the transducer paradigm and neural architectures (Rastogi et al., 2016; Aharoni and Goldberg, 2016; Lin et al., 2019). For example, the neural transducer proposed by (Aharoni and Goldberg, 2016) presents a sequence to sequence architecture that decodes one character at a time while attending at the input character under a hard-monotonic constrain. However, their method relies on out-ofIntroduction Tasks related to morphological analysis have been traditionally formulated as string transduction problems tackled by weighted finite state transducers (Mohri, 2004; Eisner, 2002). More recently, however, the problem has been tackled with neural architectures featuring"
W19-4213,P16-1162,0,0.276757,"using a neural transducer which consumes more than one character at a time. Our main motivation for this approach stems from neural transducers that normally consume one character at a time using context-enriched representation of characters.1 In language modelling, character-based RNNs have a difficulty capturing long dependencies between characters, especially dependencies in words which are separated by several tokens. This can be a crucial piece of information for morphological analysis in context. This type of approach has already been extend effectively to Neural Machine Translation by (Sennrich et al., 2016), who employ simple character n-gram models and a segmentation based on the byte pair encoding (BPE) compression algorithm. This paper presents the submission by the Charles University-University of Malta team to the SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context. We present a lemmatization model based on previous work on neural transducers (Makarov and Clematide, 2018b; Aharoni and Goldberg, 2016). The key difference is that our model transforms the whole word form in every step, instead of consuming it character by character. We propose a merging strategy"
W19-7717,W13-2322,0,0.0752573,"rallelism sometimes leads to decisions that are normally associated with deeper, semantics-oriented frameworks (the primacy of content words and the second-class citizenship of function words may serve as an example). Many theories and annotation frameworks have been proposed that contain a deep-syntactic, tectogrammatical, or semantic dependency layer; to name just a few: Meaning-Text Theory (Žolkovskij and Mel’čuk, 1965), Functional Generative Description (Sgall, 1967), the Proposition Bank (Kingsbury and Palmer, 2002), Sequoia (Candito and Seddah, 2012), or Abstract Meaning Representation (Banarescu et al., 2013). Names vary and so does the extent of ‘deep’ phenomena that are annotated; the common denominator is that these phenomena are closer to meaning on the meaning-form scale than anything we find in a typical surface-syntactic treebank. By definition, deep representation is more useful for natural language understanding (but it is also more difficult to obtain). Many of the deep frameworks have been applied to more than one language, sometimes just to demonstrate that it is possible; but none of them is anywhere near the number of languages covered by UD. UD itself contains a diffident attempt to"
W19-7717,W17-6507,0,0.380716,"n is potentially useful for semantic role disambiguation, and putting it to the label is supposed to make it more visible; nevertheless, its acquisition from the basic tree is completely deterministic, and there is no attempt to translate the labels to a language-independent description of meaning. Several extensions of the enhanced representation have been proposed. The enhanced++ graphs proposed by Schuster and Manning (2016) extend the set of ellipsis-in-coordination types where null nodes are added; they also suppress quantifying expressions in sentences like a bunch of people are coming. Candito et al. (2017) define the enhanced-alt graphs, which neutralize syntactic alternations, that is, passives, medio-passives, impersonal constructions and causatives. They also suggest to annotate external arguments of other non-finite verb forms than just open infinitival complements and relative clauses: most notably, for participles, even if they are used attributively. Hence in ceux embauchés en 2007 “those hired in 2007”, embauchés heads a non-relative adnominal clause (acl) that modifies the nominal ceux, but at the same time ceux is attached as a passive subject (nsubj:pass) of embauchés. 4 Pre-existing"
W19-7717,C16-1096,0,0.0187428,"2 Related Work Manual semantic annotation is a highly time-consuming process, therefore a number of authors experimented with (semi-)automatic approaches to semantic annotation. Padó (2007) proposed a method that uses parallel corpora to project annotation to transfer semantic roles from English to resource-poorer languages. The experiment was conducted on an English-German corpus. Van der Plas et al. (2011) experimented with joint syntactic-semantic learning aiming at improving the quality of semantic annotations from automatic cross-lingual transfer. An alternative approach was proposed by Exner et al. (2016). Instead of utilizing parallel corpora, they use loosely parallel corpora where sentences are not required to be exact translations of each other. Semantic annotations are transferred from one language to another using sentences aligned by entities. The experiment was conducted using the English, Swedish, and French editions of Wikipedia. Akbik et al. (2015) described a two-stage approach to cross-lingual semantic role labeling (SRL) that was used to generate Proposition Banks for 7 languages. First, they applied a filtered annotation projection to parallel corpora, which was intended to achi"
W19-7717,kingsbury-palmer-2002-treebank,0,0.435036,"UD guidelines have been designed as surfacesyntactic, although their emphasis on cross-linguistic parallelism sometimes leads to decisions that are normally associated with deeper, semantics-oriented frameworks (the primacy of content words and the second-class citizenship of function words may serve as an example). Many theories and annotation frameworks have been proposed that contain a deep-syntactic, tectogrammatical, or semantic dependency layer; to name just a few: Meaning-Text Theory (Žolkovskij and Mel’čuk, 1965), Functional Generative Description (Sgall, 1967), the Proposition Bank (Kingsbury and Palmer, 2002), Sequoia (Candito and Seddah, 2012), or Abstract Meaning Representation (Banarescu et al., 2013). Names vary and so does the extent of ‘deep’ phenomena that are annotated; the common denominator is that these phenomena are closer to meaning on the meaning-form scale than anything we find in a typical surface-syntactic treebank. By definition, deep representation is more useful for natural language understanding (but it is also more difficult to obtain). Many of the deep frameworks have been applied to more than one language, sometimes just to demonstrate that it is possible; but none of them"
W19-7717,W18-6527,0,0.0219996,"e English, Swedish, and French editions of Wikipedia. Akbik et al. (2015) described a two-stage approach to cross-lingual semantic role labeling (SRL) that was used to generate Proposition Banks for 7 languages. First, they applied a filtered annotation projection to parallel corpora, which was intended to achieve higher precision for a target corpus, even if containing fewer labels. Then they bootstrapped and retrained the SRL to iteratively improve recall without reducing precision. This approach was also applied to 7 treebanks from UD release 1.4.1 However, the project seems to be stalled. Mille et al. (2018) proposed the deep datasets that were used in the Shallow and Deep Tracks of the Multilingual Surface Realisation Shared Task (SR’18, SR’19). The Shallow Track datasets consist of unordered syntactic trees with all the word forms replaced with their lemmas; part-of-speech tags and the morphological information are preserved (available for 10 languages). The Deep Track datasets consist of trees that contain only content words linked by predicate-argument edges in the PropBank fashion (available for English, French and Spanish). The datasets were automatically derived from UD trees v.2.0. Gotham"
W19-7717,L16-1262,1,0.868597,"Missing"
W19-7717,W18-6012,0,0.0885664,"ter and Manning (2016) described and evaluated the Stanford Enhancer,3 which is available as a part of the Stanford CoreNLP suite. Nyblom et al. (2013) reported on the Turku Enhancer, a hybrid approach (consisting of rule-based heuristics and machine-learning components) to enhancing Stanford Dependencies of Finnish. The enhancements tackled were conjunct propagation, external subjects, and syntactic functions of relativizers; the first two are thus relevant also in Enhanced UD. Their system achieved F1 score of 93.1; note however that labeled training data is needed for the approach to work. Nivre et al. (2018) compares the Stanford Enhancer with an adapted version of the Turku Enhancer. They trained it on the Finnish labeled data, but in a delexicalized fashion (only non-lexical features were considered). The Turku Enhancer does not predict null nodes, and for external subjects it only considers subject control (or raising), but not object control. On the other hand, Stanford Enhancer only predicts core arguments as controllers while in some languages non-core dependents can control subjects too. Nevertheless, both enhancers are found usable for other languages, as shown on Swedish and Italian. The"
W19-7717,W13-3728,0,0.226937,"7”, embauchés heads a non-relative adnominal clause (acl) that modifies the nominal ceux, but at the same time ceux is attached as a passive subject (nsubj:pass) of embauchés. 4 Pre-existing Enhancing Tools Enhanced UD contains information that cannot be derived automatically from the basic UD tree; additional human input is needed in order to fully disambiguate all situations. Nevertheless, it is believed that automatic ‘enhancers’ can get us relatively far. Schuster and Manning (2016) described and evaluated the Stanford Enhancer,3 which is available as a part of the Stanford CoreNLP suite. Nyblom et al. (2013) reported on the Turku Enhancer, a hybrid approach (consisting of rule-based heuristics and machine-learning components) to enhancing Stanford Dependencies of Finnish. The enhancements tackled were conjunct propagation, external subjects, and syntactic functions of relativizers; the first two are thus relevant also in Enhanced UD. Their system achieved F1 score of 93.1; note however that labeled training data is needed for the approach to work. Nivre et al. (2018) compares the Stanford Enhancer with an adapted version of the Turku Enhancer. They trained it on the Finnish labeled data, but in a"
W19-7717,W18-4902,0,0.0258475,"aph rewriting systems. However, they only focus on two of the five enhancements (external subjects and conjunct propagation), and they only do it for French. Some of their heuristics are very French-specific and they assume that information needed for disambiguation is available in the source annotation (which is the case of the Sequoia French treebank). Several other UD treebanks come from sources where some enhanced annotation is available and can be converted to Enhanced UD. Bouma (2018) demonstrates how original annotations from the Alpino treebank can help enhance the Dutch UD treebanks. Patejuk and Przepiórkowski (2018) discuss conversion from an LFG treebank of Polish and note that not only there is more information than in basic UD, some information cannot be captured even by Enhanced UD. Another example is the distinction between private and shared dependents in coordination: for treebanks converted from Prague-style annotation (Arabic, Czech, Lithuanian, Slovak, Tamil), this distinction is readily available. 5 Data Preparation The first version of Deep UD is based on UD release 2.4 (Nivre et al., 2019) but we intend to generate updates after each future UD release. While we foresee improved semantic anno"
W19-7717,L16-1376,0,0.491206,"d; the common denominator is that these phenomena are closer to meaning on the meaning-form scale than anything we find in a typical surface-syntactic treebank. By definition, deep representation is more useful for natural language understanding (but it is also more difficult to obtain). Many of the deep frameworks have been applied to more than one language, sometimes just to demonstrate that it is possible; but none of them is anywhere near the number of languages covered by UD. UD itself contains a diffident attempt to provide deeper annotations, dubbed the Enhanced Universal Dependencies (Schuster and Manning, 2016). While it is a step in the right direction, it is just the first step: we argue that it should be possible to go deeper. Moreover, Enhanced UD is an optional extension, which is only available in a handful of treebanks (Table 1). Enhanced UD faces the same threat as the other deep frameworks mentioned above: more complex annotation requires more annotation effort, and semantic annotations are often coupled with huge lexical resources such as verb frame dictionaries. Therefore, it is less likely that sufficient manpower will be available to annotate data in a new language. Our principal questi"
W19-7717,P11-2052,0,0.0559535,"Missing"
Y16-2018,I05-1075,0,\N,Missing
Y16-2018,N01-1026,0,\N,Missing
Y16-2018,D10-1056,0,\N,Missing
Y16-2018,petrov-etal-2012-universal,0,\N,Missing
Y16-2018,I08-3008,1,\N,Missing
Y16-2018,W02-2006,0,\N,Missing
Y16-2018,D11-1006,0,\N,Missing
Y16-2018,P13-2112,0,\N,Missing
Y16-2018,P15-2044,0,\N,Missing
Y16-2018,L16-1262,1,\N,Missing
Y16-2018,L16-1497,0,\N,Missing
zeman-2008-reusable,nivre-etal-2006-talbanken05,0,\N,Missing
zeman-2008-reusable,J93-2004,0,\N,Missing
zeman-2008-reusable,W06-2920,0,\N,Missing
zeman-2008-reusable,C00-2143,0,\N,Missing
zeman-2008-reusable,P05-1022,0,\N,Missing
zeman-2008-reusable,I08-3008,1,\N,Missing
zeman-2008-reusable,D07-1096,0,\N,Missing
zeman-2008-reusable,erjavec-2004-multext,0,\N,Missing
zeman-etal-2012-hamledt,zeman-2008-reusable,1,\N,Missing
zeman-etal-2012-hamledt,bosco-etal-2010-comparing,0,\N,Missing
zeman-etal-2012-hamledt,W08-2121,0,\N,Missing
zeman-etal-2012-hamledt,C00-2143,0,\N,Missing
zeman-etal-2012-hamledt,P06-1033,0,\N,Missing
zeman-etal-2012-hamledt,W08-0325,1,\N,Missing
zeman-etal-2012-hamledt,D11-1036,0,\N,Missing
zeman-etal-2012-hamledt,D11-1006,0,\N,Missing
zeman-etal-2012-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
zeman-etal-2012-hamledt,R09-1007,0,\N,Missing
zeman-etal-2012-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
zeman-etal-2012-hamledt,taule-etal-2008-ancora,0,\N,Missing
zeman-etal-2012-hamledt,afonso-etal-2002-floresta,0,\N,Missing
zeman-sarkar-2000-learning,W99-0503,0,\N,Missing
zeman-sarkar-2000-learning,W98-1505,0,\N,Missing
zeman-sarkar-2000-learning,W97-0318,0,\N,Missing
zeman-sarkar-2000-learning,W98-1114,0,\N,Missing
zeman-sarkar-2000-learning,H91-1067,0,\N,Missing
zeman-sarkar-2000-learning,E99-1007,0,\N,Missing
zeman-sarkar-2000-learning,A97-1052,0,\N,Missing
zeman-sarkar-2000-learning,J93-2002,0,\N,Missing
zeman-sarkar-2000-learning,P98-1080,0,\N,Missing
zeman-sarkar-2000-learning,C98-1077,0,\N,Missing
zeman-sarkar-2000-learning,P93-1032,0,\N,Missing
zeman-sarkar-2000-learning,P91-1027,0,\N,Missing
zeman-sarkar-2000-learning,P99-1051,0,\N,Missing
zeman-sarkar-2000-learning,W99-0632,0,\N,Missing
zeman-sarkar-2000-learning,C96-1004,0,\N,Missing
