2021.woah-1.19,Targets and Aspects in Social Media Hate Speech,2021,-1,-1,4,1,80,alexander shvets,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),0,"Mainstream research on hate speech focused so far predominantly on the task of classifying mainly social media posts with respect to predefined typologies of rather coarse-grained hate speech categories. This may be sufficient if the goal is to detect and delete abusive language posts. However, removal is not always possible due to the legislation of a country. Also, there is evidence that hate speech cannot be successfully combated by merely removing hate speech posts; they should be countered by education and counter-narratives. For this purpose, we need to identify (i) who is the target in a given hate speech post, and (ii) what aspects (or characteristics) of the target are attributed to the target in the post. As the first approximation, we propose to adapt a generic state-of-the-art concept extraction model to the hate speech domain. The outcome of the experiments is promising and can serve as inspiration for further work on the task"
2021.nlp4posimpact-1.3,"Cartography of Natural Language Processing for Social Good ({NLP}4{SG}): Searching for Definitions, Statistics and White Spots",2021,-1,-1,5,1,33,paula fortuna,Proceedings of the 1st Workshop on NLP for Positive Impact,0,"The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line, etc. The result is a map of the current NLP4SG research and insights concerning the white spots on this map."
2021.findings-acl.333,Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models,2021,-1,-1,4,0,36,laura perezmayos,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.118,How much pretraining data do language models need to learn syntax?,2021,-1,-1,3,0,36,laura perezmayos,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications: part-of-speech tagging, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost."
2021.eacl-main.120,Evaluating language models for the retrieval and categorization of lexical collocations,2021,-1,-1,3,0.609756,2497,luis anke,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Lexical collocations are idiosyncratic combinations of two syntactically bound lexical items (e.g., {``}heavy rain{''} or {``}take a step{''}). Understanding their degree of compositionality and idiosyncrasy, as well their underlying semantics, is crucial for language learners, lexicographers and downstream NLP applications. In this paper, we perform an exhaustive analysis of current language models for collocation understanding. We first construct a dataset of apparitions of lexical collocations in context, categorized into 17 representative semantic categories. Then, we perform two experiments: (1) unsupervised collocate retrieval using BERT, and (2) supervised collocation classification in context. We find that most models perform well in distinguishing light verb constructions, especially if the collocation{'}s first argument acts as subject, but often fail to distinguish, first, different syntactic structures within the same semantic category, and second, fine-grained semantic categories which restrict the use of small sets of valid collocates for a given base."
2021.eacl-main.191,On the evolution of syntactic information encoded by {BERT}{'}s contextualized representations,2021,-1,-1,4,0,36,laura perezmayos,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"The adaptation of pretrained language models to solve supervised tasks has become a baseline in NLP, and many recent works have focused on studying how linguistic information is encoded in the pretrained sentence representations. Among other information, it has been shown that entire syntax trees are implicitly embedded in the geometry of such models. As these models are often fine-tuned, it becomes increasingly important to understand how the encoded knowledge evolves along the fine-tuning. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semantics-related tasks) in different ways along the fine-tuning process depending on the task."
2020.webnlg-1.1,A Case Study of {NLG} from Multimedia Data Sources: Generating Architectural Landmark Descriptions,2020,-1,-1,11,1,5966,simon mille,Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+),0,"In this paper, we present a pipeline system that generates architectural landmark descriptions using textual, visual and structured data. The pipeline comprises five main components:(i) a textual analysis component, which extracts information from Wikipedia pages; (ii)a visual analysis component, which extracts information from copyright-free images; (iii) a retrieval component, which gathers relevant (property, subject, object) triples from DBpedia; (iv) a fusion component, which stores the contents from the different modalities in a Knowledge Base (KB) and resolves the conflicts that stem from using different sources of information; (v) an NLG component, which verbalises the resulting contents of the KB. We show that thanks to the addition of other modalities, we can make the verbalisation of DBpedia triples more relevant and/or inspirational."
2020.mwe-1.1,{C}oll{F}r{E}n: Rich Bilingual {E}nglish{--}{F}rench Collocation Resource,2020,-1,-1,4,0.952381,16487,beatriz fisas,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,0,"Collocations in the sense of idiosyncratic lexical co-occurrences of two syntactically bound words traditionally pose a challenge to language learners and many Natural Language Processing (NLP) applications alike. Reliable ground truth (i.e., ideally manually compiled) resources are thus of high value. We present a manually compiled bilingual English{--}French collocation resource with 7,480 collocations in English and 6,733 in French. Each collocation is enriched with information that facilitates its downstream exploitation in NLP tasks such as machine translation, word sense disambiguation, natural language generation, relation classification, and so forth. Our proposed enrichment covers: the semantic category of the collocation (its lexical function), its vector space representation (for each individual word as well as their joint collocation embedding), a subcategorization pattern of both its elements, as well as their corresponding BabelNet id, and finally, indices of their occurrences in large scale reference corpora."
2020.msr-1.1,The Third Multilingual Surface Realisation Shared Task ({SR}{'}20): Overview and Evaluation Results,2020,-1,-1,6,1,5966,simon mille,Proceedings of the Third Workshop on Multilingual Surface Realisation,0,"This paper presents results from the Third Shared Task on Multilingual Surface Realisation (SR{'}20) which was organised as part of the COLING{'}20 Workshop on Multilingual Surface Realisation. As in SR{'}18 and SR{'}19, the shared task comprised two tracks: (1) a Shallow Track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (2) a Deep Track where additionally, functional words and morphological information were removed. Moreover, each track had two subtracks: (a) restricted-resource, where only the data provided or approved as part of a track could be used for training models, and (b) open-resource, where any data could be used. The Shallow Track was offered in 11 languages, whereas the Deep Track in 3 ones. Systems were evaluated using both automatic metrics and direct assessment by human evaluators in terms of Readability and Meaning Similarity to reference outputs. We present the evaluation results, along with descriptions of the SR{'}19 tracks, data and evaluation methods, as well as brief summaries of the participating systems. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
2020.lrec-1.126,{T}heme{P}ro: A Toolkit for the Analysis of Thematic Progression,2020,-1,-1,3,1,16870,monica dominguez,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper introduces ThemePro, a toolkit for the automatic analysis of thematic progression. Thematic progression is relevant to natural language processing (NLP) applications dealing, among others, with discourse structure, argumentation structure, natural language generation, summarization and topic detection. A web platform demonstrates the potential of this toolkit and provides a visualization of the results including syntactic trees, hierarchical thematicity over propositions and thematic progression over whole texts."
2020.lrec-1.838,"Toxic, Hateful, Offensive or Abusive? What Are We Really Classifying? An Empirical Analysis of Hate Speech Datasets",2020,-1,-1,3,1,33,paula fortuna,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The field of the automatic detection of hate speech and related concepts has raised a lot of interest in the last years. Different datasets were annotated and classified by means of applying different machine learning algorithms. However, few efforts were done in order to clarify the applied categories and homogenize different datasets. Our study takes up this demand. We analyze six different publicly available datasets in this field with respect to their similarity and compatibility. We conduct two different experiments. First, we try to make the datasets compatible and represent the dataset classes as Fast Text word vectors analyzing the similarity between different classes in a intra and inter dataset manner. Second, we submit the chosen datasets to the Perspective API Toxicity classifier, achieving different performances depending on the categories and datasets. One of the main conclusions of these experiments is that many different definitions are being used for equivalent concepts, which makes most of the publicly available datasets incompatible. Grounded in our analysis, we provide guidelines for future dataset collection and annotation."
W19-8659,Teaching {FORG}e to Verbalize {DB}pedia Properties in {S}panish,2019,0,0,4,1,5966,simon mille,Proceedings of the 12th International Conference on Natural Language Generation,0,"Statistical generators increasingly dominate the research in NLG. However, grammar-based generators that are grounded in a solid linguistic framework remain very competitive, especially for generation from deep knowledge structures. Furthermore, if built modularly, they can be ported to other genres and languages with a limited amount of work, without the need of the annotation of a considerable amount of training data. One of these generators is FORGe, which is based on the Meaning-Text Model. In the recent WebNLG challenge (the first comprehensive task addressing the mapping of RDF triples to text) FORGe ranked first with respect to the overall quality in human evaluation. We extend the coverage of FORGE{'}s open source grammatical and lexical resources for English, so as to further improve the English texts, and port them to Spanish, to achieve a comparable quality. This confirms that, as already observed in the case of SimpleNLG, a robust universal grammar-driven framework and a systematic organization of the linguistic resources can be an adequate choice for NLG applications."
W19-3510,A Hierarchically-Labeled {P}ortuguese Hate Speech Dataset,2019,0,5,4,1,33,paula fortuna,Proceedings of the Third Workshop on Abusive Language Online,0,"Over the past years, the amount of online offensive speech has been growing steadily. To successfully cope with it, machine learning are applied. However, ML-based techniques require sufficiently large annotated datasets. In the last years, different datasets were published, mainly for English. In this paper, we present a new dataset for Portuguese, which has not been in focus so far. The dataset is composed of 5,668 tweets. For its annotation, we defined two different schemes used by annotators with different levels of expertise. Firstly, non-experts annotated the tweets with binary labels ({`}hate{'} vs. {`}no-hate{'}). Secondly, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total. The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception. This hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections. To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pre-trained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome."
P19-1576,Collocation Classification with Unsupervised Relation Vectors,2019,0,0,3,0.784314,2497,luis anke,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Lexical relation classification is the task of predicting whether a certain relation holds between a given pair of words. In this paper, we explore to which extent the current distributional landscape based on word embeddings provides a suitable basis for classification of collocations, i.e., pairs of words between which idiosyncratic lexical relations hold. First, we introduce a novel dataset with collocations categorized according to lexical functions. Second, we conduct experiments on a subset of this benchmark, comparing it in particular to the well known DiffVec dataset. In these experiments, in addition to simple word vector arithmetic operations, we also investigate the role of unsupervised relation vectors as a complementary input. While these relation vectors indeed help, we also show that lexical function classification poses a greater challenge than the syntactic and semantic relations that are typically used for benchmarks in the literature."
D19-6301,The Second Multilingual Surface Realisation Shared Task ({SR}{'}19): Overview and Evaluation Results,2019,0,2,5,1,5966,simon mille,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"We report results from the SR{'}19 Shared Task, the second edition of a multilingual surface realisation task organised as part of the EMNLP{'}19 Workshop on Multilingual Surface Realisation. As in SR{'}18, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed. The shallow track was offered in eleven, and the deep track in three languages. Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity. This report presents the evaluation results, along with descriptions of the SR{'}19 tracks, data and evaluation methods. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
W18-6527,Underspecified {U}niversal {D}ependency Structures as Inputs for Multilingual Surface Realisation,2018,0,1,4,1,5966,simon mille,Proceedings of the 11th International Conference on Natural Language Generation,0,"In this paper, we present the datasets used in the Shallow and Deep Tracks of the First Multilingual Surface Realisation Shared Task (SR{'}18). For the Shallow Track, data in ten languages has been released: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. For the Deep Track, data in three languages is made available: English, French and Spanish. We describe in detail how the datasets were derived from the Universal Dependencies V2.0, and report on an evaluation of the Deep Track input quality. In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process."
W18-6542,Sentence Packaging in Text Generation from Semantic Graphs as a Community Detection Problem,2018,0,1,3,1,80,alexander shvets,Proceedings of the 11th International Conference on Natural Language Generation,0,"An increasing amount of research tackles the challenge of text generation from abstract ontological or semantic structures, which are in their very nature potentially large connected graphs. These graphs must be {``}packaged{''} into sentence-wise subgraphs. We interpret the problem of sentence packaging as a community detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1-score of 0.738."
W18-3601,The First Multilingual Surface Realisation Shared Task ({SR}{'}18): Overview and Evaluation Results,2018,0,11,6,1,5966,simon mille,Proceedings of the First Workshop on Multilingual Surface Realisation,0,"We report results from the SR{'}18 Shared Task, a new multilingual surface realisation task organised as part of the ACL{'}18 Workshop on Multilingual Surface Realisation. As in its English-only predecessor task SR{'}11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed. The shallow track was offered in ten, and the deep track in three languages. Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity. This report presents the evaluation results, along with descriptions of the SR{'}18 tracks, data and evaluation methods. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
L18-1400,Generation of a {S}panish Artificial Collocation Error Corpus,2018,0,0,3,1,29956,sara rodriguezfernandez,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1635,Compilation of Corpora for the Study of the Information Structure{--}Prosody Interface,2018,0,0,4,1,30209,alicia burga,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Comunicacio presentada a: The Language Resources and Evaluation Conferece, celebrada del 7 al 12 de maig de 2018 a Miyazaki, Japo."
W17-6506,Revising the {METU}-Sabanc{\\i} {T}urkish Treebank: An Exercise in Surface-Syntactic Annotation of Agglutinative Languages,2017,-1,-1,3,1,30209,alicia burga,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,None
W17-3517,Shared Task Proposal: Multilingual Surface Realization Using {U}niversal {D}ependency Trees,2017,0,1,3,1,5966,simon mille,Proceedings of the 10th International Conference on Natural Language Generation,0,"We propose a shared task on multilingual Surface Realization, i.e., on mapping unordered and uninflected universal dependency trees to correctly ordered and inflected sentences in a number of languages. A second deeper input will be available in which, in addition, functional words, fine-grained PoS and morphological information will be removed from the input trees. The first shared task on Surface Realization was carried out in 2011 with a similar setup, with a focus on English. We think that it is time for relaunching such a shared task effort in view of the arrival of Universal Dependencies annotated treebanks for a large number of languages on the one hand, and the increasing dominance of Deep Learning, which proved to be a game changer for NLP, on the other hand."
W17-3539,A demo of {FORG}e: the {P}ompeu {F}abra Open Rule-based Generator,2017,0,0,2,1,5966,simon mille,Proceedings of the 10th International Conference on Natural Language Generation,0,"This demo paper presents the multilingual deep sentence generator developed by the TALN group at Universitat Pompeu Fabra, implemented as a series of rule-based graph-transducers for the syntacticization of the input graphs, the resolution of morphological agreements, and the linearization of the trees."
W17-2506,Automatic Extraction of Parallel Speech Corpora from Dubbed Movies,2017,0,0,3,0,16227,alp oktem,Proceedings of the 10th Workshop on Building and Using Comparable Corpora,0,"This paper presents a methodology to extract parallel speech corpora based on any language pair from dubbed movies, together with an application framework in which some corresponding prosodic parameters are extracted. The obtained parallel corpora are especially suitable for speech-to-speech translation applications when a prosody transfer between source and target languages is desired."
S17-2158,{FORG}e at {S}em{E}val-2017 Task 9: Deep sentence generation based on a sequence of graph transducers,2017,0,7,4,1,5966,simon mille,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"We present the contribution of Universitat Pompeu Fabra{'}s NLP group to the SemEval Task 9.2 (AMR-to-English Generation). The proposed generation pipeline comprises: (i) a series of rule-based graph-transducers for the syntacticization of the input graphs and the resolution of morphological agreements, and (ii) an off-the-shelf statistical linearization component."
E17-2108,On the Relevance of Syntactic and Discourse Features for Author Profiling and Identification,2017,0,9,2,1,2827,juan solercompany,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"The majority of approaches to author profiling and author identification focus mainly on lexical features, i.e., on the content of a text. We argue that syntactic and discourse features play a significantly more prominent role than they were given in the past. We show that they achieve state-of-the-art performance in author and gender identification on a literary corpus while keeping the feature set small: the used feature set is composed of only 188 features and still outperforms the winner of the PAN 2014 shared task on author verification in the literary genre."
P16-2081,Semantics-Driven Recognition of Collocations Using Word Embeddings,2016,30,4,4,1,29956,sara rodriguezfernandez,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"L2 learners often produce xe2x80x9cungrammaticalxe2x80x9d word combinations such as, e.g., *give a suggestion or *make a walk. This is because of the xe2x80x9ccollocationalityxe2x80x9d of one of their items (the base) that limits the acceptance of collocates to express a specific meaning (xe2x80x98performxe2x80x99 above). We propose an algorithm that delivers, for a given base and the intended meaning of a collocate, the actual collocate lexeme(s) (make / take above). The algorithm exploits the linear mapping between bases and collocates from examples and generates a collocation transformation matrix which is then applied to novel unseen cases. The evaluation shows a promising line of research in collocation discovery."
L16-1204,A Semi-Supervised Approach for Gender Identification,2016,0,3,2,1,81,juan soler,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In most of the research studies on Author Profiling, large quantities of correctly labeled data are used to train the models. However, this does not reflect the reality in forensic scenarios: in practical linguistic forensic investigations, the resources that are available to profile the author of a text are usually scarce. To pay tribute to this fact, we implemented a Semi-Supervised Learning variant of the k nearest neighbors algorithm that uses small sets of labeled data and a larger amount of unlabeled data to classify the authors of texts by gender (man vs woman). We describe the enriched KNN algorithm and show that the use of unlabeled instances improves the accuracy of our gender identification model. We also present a feature set that facilitates the use of a very small number of instances, reaching accuracies higher than 70{\%} with only 113 instances to train the model. It is also shown that the algorithm also performs well using publicly available data."
L16-1325,Towards Multiple Antecedent Coreference Resolution in Specialized Discourse,2016,22,1,4,1,30209,alicia burga,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Despite the popularity of coreference resolution as a research topic, the overwhelming majority of the work in this area focused so far on single antecedence coreference only. Multiple antecedent coreference (MAC) has been largely neglected. This can be explained by the scarcity of the phenomenon of MAC in generic discourse. However, in specialized discourse such as patents, MAC is very dominant. It seems thus unavoidable to address the problem of MAC resolution in the context of tasks related to automatic patent material processing, among them abstractive summarization, deep parsing of patents, construction of concept maps of the inventions, etc. We present the first version of an operational rule-based MAC resolution strategy for patent material that covers the three major types of MAC: (i) nominal MAC, (ii) MAC with personal / relative pronouns, and MAC with reflexive / reciprocal pronouns. The evaluation shows that our strategy performs well in terms of precision and recall."
L16-1367,Example-based Acquisition of Fine-grained Collocation Resources,2016,29,2,4,1,29956,sara rodriguezfernandez,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Collocations such as {``}heavy rain{''} or {``}make [a] decision{''}, are combinations of two elements where one (the base) is freely chosen, while the choice of the other (collocate) is restricted, depending on the base. Collocations present difficulties even to advanced language learners, who usually struggle to find the right collocate to express a particular meaning, e.g., both {``}heavy{''} and {``}strong{''} express the meaning {`}intense{'}, but while {``}rain{''} selects {``}heavy{''}, {``}wind{''} selects {``}strong{''}. Lexical Functions (LFs) describe the meanings that hold between the elements of collocations, such as {`}intense{'}, {`}perform{'}, {`}create{'}, {`}increase{'}, etc. Language resources with semantically classified collocations would be of great help for students, however they are expensive to build, since they are manually constructed, and scarce. We present an unsupervised approach to the acquisition and semantic classification of collocations according to LFs, based on word embeddings in which, given an example of a collocation for each of the target LFs and a set of bases, the system retrieves a list of collocates for each base and LF."
D16-1111,A Neural Network Architecture for Multilingual Punctuation Generation,2016,19,5,2,0.510295,3435,miguel ballesteros,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,This work was supported by the European Commission under the contract numbers FP7-ICT-/n610411 (MULTISENSOR) and H2020-RIA-645012 (KRISTINA).
C16-2046,{P}raat on the Web: An Upgrade of {P}raat for Semi-Automatic Speech Annotation,2016,6,1,5,1,16870,monica dominguez,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"This paper presents an implementation of the widely used speech analysis tool Praat as a web application with an extended functionality for feature annotation. In particular, Praat on the Web addresses some of the central limitations of the original Praat tool and provides (i) enhanced visualization of annotations in a dedicated window for feature annotation at interval and point segments, (ii) a dynamic scripting composition exemplified with a modular prosody tagger, and (iii) portability and an operational web interface. Speech annotation tools with such a functionality are key for exploring large corpora and designing modular pipelines."
C16-1037,An Automatic Prosody Tagger for Spontaneous Speech,2016,17,2,3,1,16870,monica dominguez,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Speech prosody is known to be central in advanced communication technologies. However, despite the advances of theoretical studies in speech prosody, so far, no large scale prosody annotated resources that would facilitate empirical research and the development of empirical computational approaches are available. This is to a large extent due to the fact that current common prosody annotation conventions offer a descriptive framework of intonation contours and phrasing based on labels. This makes it difficult to reach a satisfactory inter-annotator agreement during the annotation of gold standard annotations and, subsequently, to create consistent large scale annotations. To address this problem, we present an annotation schema for prominence and boundary labeling of prosodic phrases based upon acoustic parameters and a tagger for prosody annotation at the prosodic phrase level. Evaluation proves that inter-annotator agreement reaches satisfactory values, from 0.60 to 0.80 Cohen{'}s kappa, while the prosody tagger achieves acceptable recall and f-measure figures for five spontaneous samples used in the evaluation of monologue and dialogue formats in English and Spanish. The work presented in this paper is a first step towards a semi-automatic acquisition of large corpora for empirical prosodic analysis."
C16-1323,Extending {W}ord{N}et with Fine-Grained Collocational Information via Supervised Distributional Learning,2016,29,2,5,0,15046,luis espinosaanke,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"WordNet is probably the best known lexical resource in Natural Language Processing. While it is widely regarded as a high quality repository of concepts and semantic relations, updating and extending it manually is costly. One important type of relation which could potentially add enormous value to WordNet is the inclusion of collocational information, which is paramount in tasks such as Machine Translation, Natural Language Generation and Second Language Learning. In this paper, we present ColWordNet (CWN), an extended WordNet version with fine-grained collocational information, automatically introduced thanks to a method exploiting linear relations between analogous sense-level embeddings spaces. We perform both intrinsic and extrinsic evaluations, and release CWN for the use and scrutiny of the community."
W15-2107,Towards a multi-layered dependency annotation of {F}innish,2015,14,1,4,1,30209,alicia burga,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,"We present a dependency annotation scheme for Finnish which aims at respecting the multilayered nature of language. We first tackle the annotation of surfacesyntactic structures (SSyntS) as inspired by the Meaning-Text framework. Exclusively syntactic criteria are used when defining the surface-syntactic relations tagset. Our annotation scheme allows for a direct mapping between surface-syntax and a more semantics-oriented representation, in particular predicate-argument structures. It has been applied to a corpus of Finnish, composed of 2,025 sentences related to weather conditions."
R15-1069,Classification of Lexical Collocation Errors in the Writings of Learners of {S}panish,2015,0,0,3,1,29956,sara rodriguezfernandez,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,None
N15-3012,Visualizing Deep-Syntactic Parser Output,2015,19,1,5,1,2827,juan solercompany,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"xe2x80x9cDeep-syntacticxe2x80x9d dependency structures bridge the gap between the surface-syntactic structures as produced by state-of-the-art dependency parsers and semantic logical forms in that they abstract away from surfacesyntactic idiosyncrasies, but still keep the linguistic structure of a sentence. They have thus a great potential for such downstream applications as machine translation and summarization. In this demo paper, we propose an online version of a deep-syntactic parser that outputs deep-syntactic structures from plain sentences and visualizes them using the Brat tool. Along with the deep-syntactic structures, the user can also inspect the visual presentation of the surface-syntactic structures that serve as input to the deep-syntactic parser and that are produced by the joint tagger and syntactic transition-based parser ran in the pipeline before deep-syntactic parsing takes place."
N15-1042,Data-driven sentence generation with non-isomorphic trees,2015,36,15,4,0.746854,3435,miguel ballesteros,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Abstract structures from which the generation naturally starts often do not contain any func- tional nodes, while surface-syntactic struc- tures or a chain of tokens in a linearized tree contain all of them. Therefore, data-driven linguistic generation needs to be able to cope with the projection between non-isomorphic structures that differ in their topology and number of nodes. So far, such a projection has been a challenge in data-driven genera- tion and was largely avoided. We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cas- cade of SVM-classifier based submodules that map in a series of transitions the input struc- tures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered Ancora- UPF corpus."
W14-4416,Classifiers for data-driven deep sentence generation,2014,17,6,3,0.746854,3435,miguel ballesteros,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"State-of-the-art statistical sentence generators deal with isomorphic structures only. Therefore, given that semantic and syntactic structures tend to differ in their topology and number of nodes, i.e., are not isomorphic, statistical generation saw so far itself confined to shallow, syntactic generation. In this paper, we present a series of fine-grained classifiers that are essential for data-driven deep sentence generation in that they handle the problem of the projection of non-isomorphic structures."
W14-3501,Improving Collocation Correction by Ranking Suggestions Using Linguistic Knowledge,2014,23,5,3,1,10809,roberto carlini,Proceedings of the third workshop on {NLP} for computer-assisted language learning,0,"The importance of collocations in the context of second language learning is generally acknowledged. Studies show that the xe2x80x9ccollocation density in learner corpora is nearly the same as in native corpora, i.e., that use of collocations by learners is as common as it is by native speakers, while the collocation error rate in learner corpora is about ten times as high as in native reference corpora. Therefore, CALL could be of great aid to support the learners for better mastering of collocations. However, surprisingly few works address specifically research on CALL-oriented collocation learning assistants that detect miscollocations in the writings of the learners and propose suggestions for their correction or that offer the learner the possibility to verify a word co-occurrence with respect to its correctness as collocation and obtain suggestions for its correction in case it is determined to be a miscollocation. This disregard is likely to be, on the one hand, due to the focus of the CALL research so far on grammatical matters, and, on the other hand, due to the complexity of the problem. In order to be able to provide an adequate correction of a miscollocation, the collocation learning assistant must xe2x80x9cguess the meaning that the learner intended to express. This makes it very different from grammar or spell checkers, which can draw on grammatical respectively orthographic regularities of a language. In this paper, we focus on the problem of the provision of a ranked list of correction suggestions in a context in which the learner submits a collocation for verification and obtains a list of correction suggestions in the case of a miscollocation. We show that the retrieval of the suggestions and their ranking benefits greatly from NLP techniques that provide the syntactic dependency structure and subcategorization information of the word co-occurrences and a weighted Pointwise Mutual Information (PMI) that reflects the fact that in a collocation, it is the base that is subject of the free choice of the speaker, while the occurrence of the collocate is restricted by the base, i.e., that collocations are per se asymmetric."
soler-company-wanner-2014-use,How to Use less Features and Reach Better Performance in Author Gender Identification,2014,8,14,2,0,39281,juan company,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Over the last years, author profiling in general and author gender identification in particular have become a popular research area due to their potential attractive applications that range from forensic investigations to online marketing studies. However, nearly all state-of-the-art works in the area still very much depend on the datasets they were trained and tested on, since they heavily draw on content features, mostly a large number of recurrent words or combinations of words extracted from the training sets. We show that using a small number of features that mainly depend on the structure of the texts we can outperform other approaches that depend mainly on the content of the texts and that use a huge number of features in the process of identifying if the author of a text is a man or a woman. Our system has been tested against a dataset constructed for our work as well as against two datasets that were previously used in other papers."
bouayad-agha-etal-2014-exercise,An Exercise in Reuse of Resources: Adapting General Discourse Coreference Resolution for Detecting Lexical Chains in Patent Documentation,2014,8,6,6,1,39891,nadjet bouayadagha,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Stanford Coreference Resolution System (StCR) is a multi-pass, rule-based system that scored best in the CoNLL 2011 shared task on general discourse coreference resolution. We describe how the StCR has been adapted to the specific domain of patents and give some cues on how it can be adapted to other domains. We present a linguistic analysis of the patent domain and how we were able to adapt the rules to the domain and to expand coreferences with some lexical chains. A comparative evaluation shows an improvement of the coreference resolution system, denoting that (i) StCR is a valuable tool across different text genres; (ii) specialized discourse NLP may significantly benefit from general discourse NLP research."
C14-1133,Deep-Syntactic Parsing,2014,39,11,4,0.746854,3435,miguel ballesteros,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"xe2x80x9cDeep-syntacticxe2x80x9d dependency structures that capture the argumentative, attributive and coordinative relations between full words of a sentence have a great potential for a number of NLPapplications. The abstraction degree of these structures is in-between the output of a syntactic dependency parser (connected trees defined over all words of a sentence and language-specific grammatical functions) and the output of a semantic parser (forests of trees defined over individual lexemes or phrasal chunks and abstract semantic role labels which capture the argument structure of predicative elements, dropping all attributive and coordinative dependencies). We propose a parser that delivers deep syntactic structures as output."
W13-3724,{A}n{C}ora-{UPF}: A Multi-Level Annotation of {S}panish,2013,20,11,3,1,5966,simon mille,Proceedings of the Second International Conference on Dependency Linguistics ({D}ep{L}ing 2013),0,"There is an increasing need for the annotation of multiple types of linguistic information that are rather different in their nature, e.g., word order, morphological features, syntactic and semantic relations, etc. Quite frequently, their annotation is combined in a single structure, which not only results in inadequate annotations of treebanks and consequent low-quality applications trained on them, but also is deficient from a theoretical (linguistic) perspective. We present a new corpus of Spanish annotated on four independent levels, morphology, surface-syntax, deep-syntax and semantics, as well as the methodology that allows for obtaining it with fewer cost while maintaining a high inter-annotator agreement."
W13-2112,Overview of the First Content Selection Challenge from Open Semantic Web Data,2013,4,3,3,1,39891,nadjet bouayadagha,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"In this overview paper we present the outcome of the first content selection challenge from open semantic web data, focusing mainly on the preparatory stages for defining the task and annotating the data. The task to perform was described in the challengexe2x80x99s call as follows: given a set of RDF triples containing facts about a celebrity, select those triples that are reflected in the target text (i.e., a short biography about that celebrity). From the initial nine expressions of interest, finally two participants submitted their systems for evaluation."
I13-1178,Towards the Annotation of {P}enn {T}ree{B}ank with Information Structure,2013,24,5,3,0.285183,16528,bernd bohnet,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Information Structure (IS) determines the xe2x80x9ccommunicativexe2x80x9d segmentation of the meaning of an utterance, which makes it central to the semanticsxe2x80x90syntaxxe2x80x90 intonation interface and therefore also to NLP. Despite this relevance, IS has not received much attention in the context of the majority of the reference treebanks for data-driven NLP that already contain a semantic and syntactic layers of annotation. We present our work in progress on the annotation of the Penn TreeBank with the thematicity dimension of the IS as defined in the Meaning-Text Theory. We experiment with tagging and transitionbased parsing techniques. Especially the latter achieve acceptable accuracy with even very small training samples, which is promising for languages with scarce resources."
W12-1506,Towards a Surface Realization-Oriented Corpus Annotation,2012,25,6,1,1,82,leo wanner,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"Until recently, deep stochastic surface realization has been hindered by the lack of semantically annotated corpora. This is about to change. Such corpora are increasingly available, e.g., in the context of CoNLL shared tasks. However, recent experiments with CoNLL 2009 corpora show that these popular resources, which serve well for other applications, may not do so for generation. The attempts to adapt them for generation resulted so far in a better performance of the realizers, but not yet in a genuinely semantic generation-oriented annotation schema. Our goal is to initiate a debate on how a generation suitable annotation schema should be defined. We define some general principles of a semantic generation-oriented annotation and propose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them."
W12-1525,The Surface Realisation Task: Recent Developments and Future Plans,2012,13,8,4,0,26421,anja belz,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"The Surface Realisation Shared Task was first run in 2011. Two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs. However, the input representations had several shortcomings which we have been aiming to address in the time since. This paper reports on our work to date on improving the input representations and on our plans for the next edition of the SR Task. We also briefly summarise other related developments in NLG shared tasks and outline how the different ideas may be usefully brought together in the future."
W12-1527,Content Selection From Semantic Web Data,2012,15,12,3,1,39891,nadjet bouayadagha,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"So far, there has been little success in Natural Language Generation in coming up with general models of the content selection process. Nonetheless, there has been some work on content selection that employ Machine learning or heuristic search. On the other side, there is a clear tendency in NLG towards the use of resources encoded in standard Semantic Web representation formats. For these reasons, we believe that time has come to propose an initial challenge on content selection from Semantic Web data. In this paper, we briefly outline the idea and plan for the execution of this task."
C12-2082,How Does the Granularity of an Annotation Scheme Influence Dependency Parsing Performance?,2012,19,10,4,1,5966,simon mille,Proceedings of {COLING} 2012: Posters,0,"The common use of a single de facto standard annotation scheme for dependency treebank creation leaves the question open to what extent the performance of an application trained on a treebank depends on this annotation scheme and whether a linguistically richer scheme would imply a decrease of the performance of the application. We investigate the effect of the variation of the number of grammatical relations in a tagset on the performance of dependency parsers. In order to obtain several levels of granularity of the annotation, we design a hierarchical annotation scheme exclusively based on syntactic criteria. The richest annotation contains 60 relations. The more coarse-grained annotations are derived from the richest. As a result, all annotations and thus also the performance of a parser trained on different annotations remain comparable. We carried out experiments with four state-of-the-art dependency parsers. The results support the claim that annotating with more fine-grained syntactic relations does not necessarily imply a significant loss of accuracy. We also show the limits of this approach by giving details on the fine-grained relations that do have a negative impact on the performance of the parsers."
W11-2810,Content selection from an ontology-based knowledge base for the generation of football summaries,2011,19,15,3,1,39891,nadjet bouayadagha,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We present an approach to content selection that works on an ontology-based knowledge base developed independently from the task at hand, i.e., Natural Language Generation. Prior to content selection, a stage akin to signal analysis and data assessment used in the generation from numerical data is performed for identifying and abstracting patterns and trends, and identifying relations between individuals. This new information is modeled as an extended ontology on top of the domain ontology which is populated via inference rules. Content selection leverages the ontology-based description of the domain and is performed throughout the text planning at increasing levels of granularity. It includes a main topic selection phase that takes into account a simple user model, a set of heuristics, and semantic relations that link individuals of the KB. The heuristics are based on weights determined empirically by supervised learning on a corpus of summaries aligned with data. The generated texts are short football match summaries that take into account the user perspective."
W11-2835,{\\textless}{S}tu{M}a{B}a{\\textgreater}: From Deep Representation to Surface,2011,7,11,4,0.470888,16528,bernd bohnet,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We realize the full generation pipeline, from the deep (= semantic) representation (SemR), over the shallow (= surface-syntactic) representation (SSyntR) to the surface. To account systematically for the non-isomorphic projection between SemR and SSyntR, we introduce an intermediate representation: the so-called deep-syntactic representation (DSyntR), which does not contain yet (all) function words (as SemR), but which already contains grammatical function relation labels (as SSyntR)."
bohnet-wanner-2010-open,Open Soucre Graph Transducer Interpreter and Grammar Development Environment,2010,27,19,2,0.470888,16528,bernd bohnet,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Graph and tree transducers have been applied in many NLP areasamong them, machine translation, summarization, parsing, and text generation. In particular, the successful use of tree rewriting transducers for the introduction of syntactic structures in statistical machine translation contributed to their popularity. However, the potential of such transducers is limited because they do not handle graphs and because they consume the source structure in that they rewrite it instead of leaving it intact for intermediate consultations. In this paper, we describe an open source tree and graph transducer interpreter, which combines the advantages of graph transducers and two-tape Finite State Transducers and surpasses the limitations of state-of-the-art tree rewriting transducers. Along with the transducer, we present a graph grammar development environment that supports the compilation and maintenance of graph transducer grammatical and lexical resources. Such an environment is indispensable for any effort to create consistent large coverage NLP-resources by human experts."
mille-wanner-2010-syntactic,Syntactic Dependencies for Multilingual and Multilevel Corpus Annotation,2010,21,4,2,1,5966,simon mille,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The relevance of syntactic dependency annotated corpora is nowadays unquestioned. However, a broad debate on the optimal set of dependency relation tags did not take place yet. As a result, largely varying tag sets of a largely varying size are used in different annotation initiatives. We propose a hierarchical dependency structure annotation schema that is more detailed and more flexible than the known annotation schemata. The schema allows us to choose the level of the desired detail of annotation, which facilitates the use of the schema for corpus annotation for different languages and for different NLP applications. Thanks to the inclusion of semantico-syntactic tags into the schema, we can annotate a corpus not only with syntactic dependency structures, but also with valency patterns as they are usually found in separate treebanks such as PropBank and NomBank. Semantico-syntactic tags and the level of detail of the schema furthermore facilitate the derivation of deep-syntactic and semantic annotations, leading to truly multilevel annotated dependency corpora. Such multilevel annotations can be readily used for the task of ML-based acquisition of grammar resources that map between the different levels of linguistic representation  something which forms part of, for instance, any natural language text generator."
ramos-etal-2010-towards,Towards a Motivated Annotation Schema of Collocation Errors in Learner Corpora,2010,10,18,2,1,46267,margarita ramos,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Collocations play a significant role in second language acquisition. In order to be able to offer efficient support to learners, an NLP-based CALL environment for learning collocations should be based on a representative collocation error annotated learner corpus. However, so far, no theoretically-motivated collocation error tag set is available. Existing learner corpora tag collocation errors simply as lexical errors  which is clearly insufficient given the wide range of different collocation errors that the learners make. In this paper, we present a fine-grained three-dimensional typology of collocation errors that has been derived in an empirical study from the learner corpus CEDEL2 compiled by a team at the Autonomous University of Madrid. The first dimension captures whether the error concerns the collocation as a whole or one of its elements; the second dimension captures the language-oriented error analysis, while the third exemplifies the interpretative error analysis. To facilitate a smooth annotation along this typology, we adapted Knowtator, a flexible off-the-shelf annotation tool implemented as a Prot{\'e}g{\'e} plugin."
C10-1012,Broad Coverage Multilingual Deep Sentence Generation with a Stochastic Multi-Level Realizer,2010,25,42,2,0.470888,16528,bernd bohnet,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increasingly available for multiple languages. We take advantage of them and propose a multilingual deep stochastic sentence realizer that mirrors the state-of-the-art research in semantic parsing. The realizer uses an SVM learning algorithm. For each pair of adjacent levels of annotation, a separate decoder is defined. So far, we evaluated the realizer for Chinese, English, German, and Spanish."
ramos-etal-2008-using,Using Semantically Annotated Corpora to Build Collocation Resources,2008,9,12,3,1,46267,margarita ramos,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present an experiment in extracting collocations from the FrameNet corpus, specifically, support verbs such as direct in Environmentalists directed strong criticism at world leaders. Support verbs do not contribute meaning of their own and the meaning of the construction is provided by the noun; the recognition of support verbs is thus useful in text understanding. Having access to a list of support verbs is also useful in applications that can benefit from paraphrasing, such as generation (where paraphrasing can provide variety). This paper starts with a brief presentation of the notion of lexical function in Meaning-Text Theory, where they fall under the notion of lexical function, and then discusses how relevant information is encoded in the FrameNet corpus. We describe the resource extracted from the FrameNet corpus."
mille-wanner-2008-making,Making Text Resources Accessible to the Reader: the Case of Patent Claims,2008,15,13,2,1,5966,simon mille,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Hardly any other kind of text structures is as notoriously difficult to read as patents. This is first of all due to their abstract vocabulary and their very complex syntactic constructions. Especially the claims in a patent are a challenge: in accordance with international patent writing regulations, each claim must be rendered in a single sentence. As a result, sentences with more than 200 words are not uncommon. Therefore, paraphrasing of the claims in terms the user can understand is of high demand. We present a rule-based paraphrasing module that realizes paraphrasing of patent claims in English as a rewriting task. Prior to the rewriting proper, the module implies the stages of simplification and discourse and syntactic analyses. The rewriting makes use of a full-fledged text generator and consists in a number of genuine generation tasks such as aggregation, selection of referring expressions, choice of discourse markers and syntactic generation. As generator, we use the MATE-work bench, which is based on the Meaning-Text Theory of linguistics."
2008.eamt-1.18,Multilingual summarization in practice: the case of patent claims,2008,15,7,2,1,5966,simon mille,Proceedings of the 12th Annual conference of the European Association for Machine Translation,0,"Hardly any other type of textual material is as difficult to read and comprehend as patents. Especially the claims in a patent reveal very complex syntactic constructions which are difficult to process even for native speakers, let alone for foreigners who do not master well the language in which the patent is written. Therefore, multilingual summarization is very attractive to practitioners in the field. We propose a multilingual summarizer that operates at the Deep-Syntactic Structures (DSyntSs) as introduced in the Meaning-Text Theory. Firstly, the original claims are linguistically simplified and analyzed down to DSyntSs. Then, syntactic and discursive summarization criteria are applied to the DSyntSs to remove summary irrelevant DSyntS-branches. The pruned DSyntS are transferred into DSyntSs of the language in which the summary is to be generated. For the generation of the summary from the transferred DSyntSs, we use the full fledged text generator MATE."
2008.eamt-1.20,Two-step flow in bilingual lexicon extraction from unrelated corpora,2008,12,0,2,0,31297,rogelio nazar,Proceedings of the 12th Annual conference of the European Association for Machine Translation,0,"Fundacio Barcelona Media, Pompeu Fabra UniversityOcata 1. Barcelona.{rogelio.nazar; leo.wanner; jorge.vivaldi}upf.eduAbstract. This paper presents a language independent methodology for automatically extracting bilingual lexicon entries from the web without the need of resources like parallel or comparable corpora, POS tagging, nor an initial bilingual lexicon. It is suitable for specialized domains where bilingual lexicon entries are scarce. The input for the process is a corpus in the source language to use as example of real usage of the units we need to translate. It is a two-step flow process because first we extract single-word units from the source language and then the multi-word units where the initial single units are instantiated. For each of the multi-word units, we see if they appear in texts from the web in the target language. The unit of the target language that appears more frequently across the sets of multi-word units is usually the correct translation of the initial single-word source language entry. Keywords: Bilingual Lexicon Extraction, Specialized Terminology, Machine Translation, Corpus Linguistics, Knowledge-poor methods, statistical methods."
wanner-ramos-2006-local,Local Document Relevance Clustering in {IR} Using Collocation Information,2006,17,1,1,1,82,leo wanner,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"A series of different automatic query expansion techniques has been suggested in Information Retrieval. To estimate how suitable a document term is as an expansion term, the most popular of them use a measure of the frequency of the co-occurrence of this term with one or several query terms. The benefit of the use of the linguistic relations that hold between query terms is often questioned. If a linguistic phenomenon is taken into account, it is the phrase structure or lexical compound. We propose a technique that is based on the restricted lexical cooccurrence (collocation) of query terms. We use the knowledge on collocations formed by query terms for two tasks: (i) document relevance clustering done in the first stage of local query expansion and (ii) choice of suitable expansion terms from the relevant document cluster. In this paper, we describe the first task, providing evidence from first preliminary experiments on Spanish material that local relevance clustering benefits largely from knowledge on collocations."
wanner-etal-2004-enriching,Enriching the {S}panish {E}uro{W}ord{N}et by Collocations,2004,6,2,1,1,82,leo wanner,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Collocations constitute an important type of syntagmatic information whose introduction into WordNets has not yet been addressed. The goal of our work is the integration of the collocational material for the field of emotion nouns encoded in the DIccionario de colocaciones del espanol (DICE) in terms of Lexical Functions into the Spanish part of the EuroWordNet (SpEWN). Two features of collocations are decisive in connection with their representation in SpEWN: (i) they are variant-specific rather than synset-specific and (ii) depending on the degree of their idiosyncrasy, they may be generalized to a certain degree. These features are accounted for by introducing new structures into the SpEWN. These new structures are compatible with the general design principles of the EWN. Given that SpEWN and DICE reveal a diverging distinction of the senses of lexical items, prior to the introduction of collocations from DICE into SpEWN, the senses of the elements of collocations in SpEWN and DICE are aligned."
W03-2315,Deriving the Communicative Structure in Applied {NLG},2003,0,4,1,1,82,leo wanner,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,None
W01-0807,On Using a Parallel Graph Rewriting Formalism in Generation,2001,11,17,2,1,16528,bernd bohnet,Proceedings of the {ACL} 2001 Eighth {E}uropean Workshop on Natural Language Generation ({EWNLG}),0,"In this paper, we present a parallel context sensitive graph rewriting formalism for a dependency-oriented generation grammar. The parallel processing of the input structure makes an explicit presentation of all alternative options for its mapping onto the output structure possible. This allows for the selection of the linguistic realization that suits best the communicative and contextual criteria available."
W00-1436,A development Environment for an {MTT}-Based Sentence Generator,2000,4,33,3,1,16528,bernd bohnet,{INLG}{'}2000 Proceedings of the First International Conference on Natural Language Generation,0,"With the rising standard of the state of the art in text generation and the increase of the number of practical generation applications, it becomes more and more important to provide means for the maintenance of the generator, i.e. its extension, modification, and monitoring by grammarians who are not familiar with its internals. However, only a few sentence and text generators developed to date actually provide these means. One of these generators is KPML (Bateman, 1997). KPML comes with a Development Environment and there is no doubt about the contribution of this environment to the popularity of the systemic approach in generation."
W98-1406,De-Constraining Text Generation,1998,12,11,4,0,38698,stephen beale,Natural Language Generation,0,"We argue that the current, predominantly task-oriented, approaz~hes to modularizing text xe2x80xa2 generation, while plausible and useful conceptually, set up spurious conceptual and operational constraints. We propose a data-driven approach to modularization and illustrate how it eliminates xe2x80xa2 xe2x80xa2the previously ubiquitous constraints on combination of evidence across modules and on xe2x80xa2 control. We also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with realistic texts."
W96-0401,The {H}ealth{D}oc Sentence Planner,1996,21,37,1,1,82,leo wanner,Eighth International Natural Language Generation Workshop,0,None
W94-0316,Building Another Bridge over the Generation Gap,1994,18,12,1,1,82,leo wanner,Proceedings of the Seventh International Workshop on Natural Language Generation,0,"In this paper, we address one of the central problems in text generation: the missing link (the generation gap in Meteer's terms) between the global discourse organization as often provided by text planning modules and the linguistic realization of this organization. We argue that the link should be established by the lexical choice process using resources derived from Mel'cuk's Lexical Functions (LFs). In particular, we demonstrate that sequences of LFs may well serve as lexical discourse structure relations which link up to global discourse relations in the output of a Rhetorical Structure Theory style text planner."
C94-1060,On Lexically Biased Discourse Organization in Text Generation,1994,23,5,1,1,82,leo wanner,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
W90-0105,A collocational based approach to salience-sensitive lexical selection,1990,12,11,1,1,82,leo wanner,Proceedings of the Fifth International Workshop on Natural Language Generation,0,None
