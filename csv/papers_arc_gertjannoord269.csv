2021.wat-1.21,Optimal Word Segmentation for Neural Machine Translation into {D}ravidian Languages,2021,-1,-1,3,1,368,prajit dhar,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"Dravidian languages, such as Kannada and Tamil, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these languages are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from English into four different Dravidian languages. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality."
2020.wmt-1.9,Linguistically Motivated Subwords for {E}nglish-{T}amil Translation: {U}niversity of {G}roningen{'}s Submission to {WMT}-2020,2020,-1,-1,3,1,368,prajit dhar,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and word dropout. Additionally, our experiments show that using a linguistically motivated subword segmentation technique (Ataman et al., 2017) does not consistently outperform the more widely used, non-linguistically motivated SentencePiece algorithm (Kudo and Richardson, 2018), despite the agglutinative nature of Tamil morphology."
2020.wmt-1.130,Data Selection for Unsupervised Translation of {G}erman{--}{U}pper {S}orbian,2020,-1,-1,3,0,13820,lukas edman,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German{--}Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting. We find that we can perform data selection using a pretrained model and show that the quality of a set of sentences or documents can have a great impact on the performance of the UNMT system trained on it. Furthermore, we show that document-level data selection should be preferred for training the XLM model when possible. Finally, we show that there is a trade-off between quality and quantity of the data used to train UNMT systems."
2020.tlt-1.13,{A}lpino{G}raph: A Graph-based Search Engine for Flexible and Efficient Treebank Search,2020,-1,-1,2,0,14365,peter kleiweg,Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories,0,None
2020.lrec-1.680,"A Shared Task of a New, Collaborative Type to Foster Reproducibility: A First Exercise in the Area of Language Science and Technology with {REPROLANG}2020",2020,-1,-1,4,0,8863,antonio branco,Proceedings of the 12th Language Resources and Evaluation Conference,0,"n this paper, we introduce a new type of shared task {---} which is collaborative rather than competitive {---} designed to support and fosterthe reproduction of research results. We also describe the first event running such a novel challenge, present the results obtained, discussthe lessons learned and ponder on future undertakings."
2020.emnlp-main.180,{UD}apter: Language Adaptation for Truly {U}niversal {D}ependency Parsing,2020,34,0,4,1,3854,ahmet ustun,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success."
2020.eamt-1.10,Low-Resource Unsupervised {NMT}: Diagnosing the Problem and Providing a Linguistically Motivated Solution,2020,-1,-1,3,0,13820,lukas edman,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Unsupervised Machine Translation has been advancing our ability to translate without parallel data, but state-of-the-art methods assume an abundance of monolingual data. This paper investigates the scenario where monolingual data is limited as well, finding that current unsupervised methods suffer in performance under this stricter setting. We find that the performance loss originates from the poor quality of the pretrained monolingual embeddings, and we offer a potential solution: dependency-based word embeddings. These embeddings result in a complementary word representation which offers a boost in performance of around 1.5 BLEU points compared to standard word2vec when monolingual data is limited to 1 million sentences per language. We also find that the inclusion of sub-word information is crucial to improving the quality of the embeddings."
W19-4206,"Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.",2019,0,0,4,1,3854,ahmet ustun,"Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"This paper describes our submission to SIGMORPHON 2019 Task 2: Morphological analysis and lemmatization in context. Our model is a multi-task sequence to sequence neural network, which jointly learns morphological tagging and lemmatization. On the encoding side, we exploit character-level as well as contextual information. We introduce a multi-attention decoder to selectively focus on different parts of character and word sequences. To further improve the model, we train on multiple datasets simultaneously and use external embeddings for initialization. Our final model reaches an average morphological tagging F1 score of 94.54 and a lemma accuracy of 93.91 on the test data, ranking respectively 3rd and 6th out of 13 teams in the SIGMORPHON 2019 shared task."
R19-1140,Cross-Lingual Word Embeddings for Morphologically Rich Languages,2019,0,0,3,1,3854,ahmet ustun,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as Turkish and Finnish. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our model includes a simple extension which enables us to exploit morphemes for cross-lingual mapping. We applied our model for the Turkish-Finnish language pair on the bilingual word translation task. Results show that our model outperforms the baseline models by 2{\%} in the nearest neighbour ranking."
L18-1109,A Taxonomy for In-depth Evaluation of Normalization for User Generated Content,2018,9,0,3,1,3851,rob goot,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this work we present a taxonomy of error categories for lexical normalization, which is the task of translating user generated content to canonical language. We annotate a recent normalization dataset to test the practical use of the taxonomy and read a near-perfect agreement. This annotated dataset is then used to evaluate how an existing normalization model performs on the different categories of the taxonomy. The results of this evaluation reveal that some of the problematic categories only include minor transformations, whereas most regular transformations are solved quite well."
J18-4003,{S}quib: Reproducibility in Computational Linguistics: Are We Willing to Share?,2018,10,3,3,0,8503,martijn wieling,Computational Linguistics,0,"This study focuses on an essential precondition for reproducibility in computational linguistics: the willingness of authors to share relevant source code and data. Ten years after Ted Pedersen{'}s influential {``}Last Words{''} contribution in Computational Linguistics, we investigate to what extent researchers in computational linguistics are willing and able to share their data and code. We surveyed all 395 full papers presented at the 2011 and 2016 ACL Annual Meetings, and identified whether links to data and code were provided. If working links were not provided, authors were requested to provide this information. Although data were often available, code was shared less often. When working links to code or data were not provided in the paper, authors provided the code in about one third of cases. For a selection of ten papers, we attempted to reproduce the results using the provided data and code. We were able to reproduce the results approximately for six papers. For only a single paper did we obtain the exact same results. Our findings show that even though the situation appears to have improved comparing 2016 to 2011, empiricism in computational linguistics still largely remains a matter of faith. Nevertheless, we are somewhat optimistic about the future. Ensuring reproducibility is not only important for the field as a whole, but also seems worthwhile for individual researchers: The median citation count for studies with working links to the source code is higher."
D18-1542,Modeling Input Uncertainty in Neural Network Dependency Parsing,2018,0,0,2,1,3851,rob goot,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Recently introduced neural network parsers allow for new approaches to circumvent data sparsity issues by modeling character level information and by exploiting raw data in a semi-supervised setting. Data sparsity is especially prevailing when transferring to non-standard domains. In this setting, lexical normalization has often been used in the past to circumvent data sparsity. In this paper, we investigate whether these new neural approaches provide similar functionality as lexical normalization, or whether they are complementary. We provide experimental results which show that a separate normalization component improves performance of a neural network parser even if it has access to character level information as well as external word embeddings. Further improvements are obtained by a straightforward but novel approach in which the top-N best candidates provided by the normalization component are available to the parser."
2018.gwc-1.30,Simple Embedding-Based Word Sense Disambiguation,2018,-1,-1,2,1,31049,dieke oele,Proceedings of the 9th Global Wordnet Conference,0,"We present a simple knowledge-based WSD method that uses word and sense embeddings to compute the similarity between the gloss of a sense and the context of the word. Our method is inspired by the Lesk algorithm as it exploits both the context of the words and the definitions of the senses. It only requires large unlabeled corpora and a sense inventory such as WordNet, and therefore does not rely on annotated data. We explore whether additional extensions to Lesk are compatible with our method. The results of our experiments show that by lexically extending the amount of words in the gloss and context, although it works well for other implementations of Lesk, harms our method. Using a lexical selection method on the context words, on the other hand, improves it. The combination of our method with lexical selection enables our method to outperform state-of the art knowledge-based systems."
W17-6931,Distributional {L}esk: Effective Knowledge-Based Word Sense Disambiguation,2017,0,2,2,1,31049,dieke oele,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,"We propose a simple, yet effective, Word Sense Disambiguation method that uses a combination of a lexical knowledge-base and embeddings. Similar to the classic Lesk algorithm, it exploits the idea that overlap between the context of a word and the definition of its senses provides information on its meaning. Instead of counting the number of words that overlap, we use embeddings to compute the similarity between the gloss of a sense and the context. Evaluation on both Dutch and English datasets shows that our method outperforms other Lesk methods and improves upon a state-of-theart knowledge-based system. Additional experiments confirm the effect of the use of glosses and indicate that our approach works well in different domains."
W17-5043,The Power of Character N-grams in Native Language Identification,2017,0,5,5,0,10899,artur kulmizev,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we explore the performance of a linear SVM trained on language independent character features for the NLI Shared Task 2017. Our basic system (GRONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as features. We compare this against several ensemble and meta-classifiers in order to examine how the linear system fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution."
W17-0403,Increasing Return on Annotation Investment: The Automatic Construction of a {U}niversal {D}ependency Treebank for {D}utch,2017,11,2,2,0,5827,gosse bouma,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),0,"We present a method for automatically converting the Dutch Lassy Small treebank, a phrasal dependency treebank, to UD. All of the information required to produce accurate UD annotation appears to be available in the underlying annotation. However, we also note that the close connection between POS-tags and dependency labels that is present in UD is missing in the Lassy treebanks. As a consequence, annotation decisions in the Dutch data for such phenomena as nominalization and clausal complements of prepositions seem to differ to some extent from comparable data in English and German. Because the conversion is automatic, we can now also compare three state-of-theart dependency parsers trained on UD Lassy Small with Alpino, a hybrid Dutch parser which produces output that is compatible with the original Lassy annotations."
P17-2078,Parser Adaptation for Social Media by Integrating Normalization,2017,13,1,2,1,3851,rob goot,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This work explores different approaches of using normalization for parser adaptation. Traditionally, normalization is used as separate pre-processing step. We show that integrating the normalization model into the parsing algorithm is more beneficial. This way, multiple normalization candidates can be leveraged, which improves parsing performance on social media. We test this hypothesis by modifying the Berkeley parser; out-of-the-box it achieves an F1 score of 66.52. Our integrated approach reaches a significant improvement with an F1 score of 67.36, while using the best normalization sequence results in an F1 score of only 66.94."
W16-2332,{SMT} and Hybrid systems of the {QTL}eap project in the {WMT}16 {IT}-task,2016,26,3,8,0,17856,rosa gaudio,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the description of 12 systems submitted to the WMT16 IT-task, covering six different languages, namely Basque, Bulgarian, Dutch, Czech, Portuguese and Spanish. All these systems were developed under the scope of the QTLeap project, presenting a common strategy. For each language two different systems were submitted, namely a phrasebased MT system built using Moses, and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was implemented using TectoMT. For 4 of the 6 languages, the TectoMT-based system performs better than the Moses-based one."
N16-1160,Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders,2016,69,16,3,0.952381,16248,simon vsuster,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time."
J16-2007,{O}bituary: In Memoriam: Susan Armstrong,2016,0,0,3,0,2866,pierrette bouillon,Computational Linguistics,0,None
W15-5709,Lexical choice in Abstract Dependency Trees,2015,12,0,2,1,31049,dieke oele,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-2502,Comparison of Coreference Resolvers for Deep Syntax Translation,2015,24,2,3,0,3253,michal novak,Proceedings of the Second Workshop on Discourse in Machine Translation,0,"This work focuses on using anaphora for machine translation with deep-syntactic transfer. We compare multiple coreference resolvers for English in terms of how they affect the quality of pronoun translation in English-Czech and EnglishDutch machine translation systems with deep transfer. We examine which pronouns in the target language depend on anaphoric information, and design rules that take advantage of this information. The resolversxe2x80x99 performance measured by translation quality is contrasted with their intrinsic evaluation results. In addition, a more detailed manual analysis of Englishto-Czech translation was carried out."
S15-2007,{ROB}: Using Semantic Meaning to Recognize Paraphrases,2015,19,2,2,1,3851,rob goot,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Paraphrase recognition is the task of identifying whether two pieces of natural language represent similar meanings. This paper describes a system participating in the shared task 1 of SemEval 2015, which is about paraphrase detection and semantic similarity in twitter. Our approach is to exploit semantically meaningful features to detect paraphrases. An existing state-of-the-art model for predicting semantic similarity is adapted to this task. A wide variety of features is used, ranging from different types of models, to lexical overlap and synset overlap. A maximum entropy classifier is then trained on these features. In addition to the detection of paraphrases, a similarity score is also predicted, using the probabilities of the classifier. To improve the results, normalization is used as preprocessing step. Our final system achieves a F1 score of 0.620 (10th out of 18 teams), and a Pearson correlation of 0.515 (6th out of 13 teams)."
ivanova-van-noord-2014-treelet,Treelet Probabilities for {HPSG} Parsing and Error Correction,2014,24,0,2,0,35341,angelina ivanova,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Most state-of-the-art parsers take an approach to produce an analysis for any input despite errors. However, small grammatical mistakes in a sentence often cause parser to fail to build a correct syntactic tree. Applications that can identify and correct mistakes during parsing are particularly interesting for processing user-generated noisy content. Such systems potentially could take advantage of linguistic depth of broad-coverage precision grammars. In order to choose the best correction for an utterance, probabilities of parse trees of different sentences should be comparable which is not supported by discriminative methods underlying parsing software for processing deep grammars. In the present work we assess the treelet model for determining generative probabilities for HPSG parsing with error correction. In the first experiment the treelet model is applied to the parse selection task and shows superior exact match accuracy than the baseline and PCFG. In the second experiment it is tested for the ability to score the parse tree of the correct sentence higher than the constituency tree of the original version of the sentence containing grammatical error."
C14-1131,From neighborhood to parenthood: the advantages of dependency representation over bigrams in Brown clustering,2014,36,4,2,0.952381,16248,simon vsuster,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We present an effective modification of the popular Brown et al. 1992 word clustering algorithm, using a dependency language model. By leveraging syntax-based context, resulting clusters are better when evaluated against a wordnet for Dutch. The improvements are stable across parameters such as number of clusters, minimum frequency and granularity. Further refinement is possible through dependency relation selection. Our approach achieves a desired clustering quality with less data, resulting in a decrease in cluster creation times."
R11-1049,Adaptability of Lexical Acquisition for Large-scale Grammars,2011,19,1,2,1,33554,kostadin cholakov,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In this paper, we demonstrate the portability of the lexical acquisition (LA) method proposed in Cholakov and van Noord (2010a). Here, LA refers to the acquisition of linguistic descriptions for words which are not listed in the lexicon of a given computational grammar, i.e., words which are unknown to this grammar. The method we discuss was originally developed for the Dutch Alpino system, and the paper shows that the method also applies to the GG (Crysmann, 2003), a computational HPSG grammar of German. The LA method obtains very similar results for German (84% F-measure on learning unknown words). Extending the GG with the lexical entries proposed by the LA method causes an important improvement in parsing accuracy for a test set of sentences containing unknown words. Furthermore, in a smaller experiment, we show that the linguistic knowledge the LA method provides can also be used for sentence generation."
P11-2034,Reversible Stochastic Attribute-Value Grammars,2011,214,20,3,0.833333,21366,daniel kok,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"An attractive property of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking."
P11-1157,Effective Measures of Domain Similarity for Parsing,2011,30,41,2,1,106,barbara plank,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective -- it outperforms random data selection on both languages examined, English and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English."
I11-1086,An Empirical Comparison of Unknown Word Prediction Methods,2011,23,1,2,1,33554,kostadin cholakov,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We compare two types of methods which deal with unknown words in the context of computational grammars. Methods of the first type are based on the idea of supertagging and use a tagger to predict lexical descriptions for unknown tokens in a given input. The second type of methods perform lexical acquisition (LA) which, in the context of this paper, refers to the automatic acquisition of new lexical entries for the lexicon of a given grammar. The methods are compared based on the effect their application has on the parsing coverage and accuracy of the GG grammar of German (Crysmann, 2003). In particular, we adapt the LA method of Cholakov and van Noord (2010) which was originally developed for the Dutch Alpino system to be used with the GG. Its impact on coverage and accuracy on a test corpus of German newspaper texts is compared to the results reported previously on the same corpus for methods which employed a tagger. Furthermore, in a smaller experiment, we show that the linguistic knowledge this LA method provides can also be used for sentence realisation."
W10-2105,Grammar-Driven versus Data-Driven: Which Parsing System Is More Affected by Domain Shifts?,2010,22,13,2,1,106,barbara plank,Proceedings of the 2010 Workshop on {NLP} and Linguistics: Finding the Common Ground,0,"In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and training domain increases. Yet, to which degree do they suffer from this problem, i.e. which kind of parsing system is more affected by domain shifts? Intuitively, grammar-driven systems should be less affected by domain changes. To investigate this hypothesis, an empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques."
zhao-van-noord-2010-pos,{POS} Multi-tagging Based on Combined Models,2010,7,0,2,0,28922,yan zhao,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In the POS tagging task, there are two kinds of statistical models: one is generative model, such as the HMM, the others are discriminative models, such as the Maximum Entropy Model (MEM). POS multi-tagging decoding method includes the N-best paths method and forward-backward method. In this paper, we use the forward-backward decoding method based on a combined model of HMM and MEM. If P(t) is the forward-backward probability of each possible tag t, we first calculate P(t) according HMM and MEM separately. For all tags options in a certain position in a sentence, we normalize P(t) in HMM and MEM separately. Probability of the combined model is the sum of normalized forward-backward probabilities P norm(t) in HMM and MEM. For each word w, we select the best tag in which the probability of combined model is the highest. In the experiments, we use combined model and get higher accuracy than any single model on POS tagging tasks of three languages, which are Chinese, English and Dutch. The result indicates that our combined model is effective."
D10-1088,Using Unknown Word Techniques to Learn Known Words,2010,21,6,2,1,33554,kostadin cholakov,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Unknown words are a hindrance to the performance of hand-crafted computational grammars of natural language. However, words with incomplete and incorrect lexical entries pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar. Such lexical entries are hard to detect and even harder to correct.n n We employ an error miner to pinpoint words with problematic lexical entries. An automated lexical acquisition technique is then used to learn new entries for those words which allows the grammar to parse previously uncovered sentences successfully.n n We test our method on a large-scale grammar of Dutch and a set of sentences for which this grammar fails to produce a parse. The application of the method enables the grammar to cover 83.76% of those sentences with an accuracy of 86.15%."
C10-2018,Acquisition of Unknown Word Paradigms for Large-Scale Grammars,2010,21,6,2,1,33554,kostadin cholakov,Coling 2010: Posters,0,"Unknown words are a major issue for large-scale grammars of natural language. We propose a machine learning based algorithm for acquiring lexical entries for all forms in the paradigm of a given unknown word. The main advantages of our method are the usage of word paradigms to obtain valuable morphological knowledge, the consideration of different contexts which the unknown word and all members of its paradigm occur in and the employment of a full-blown syntactic parser and the grammar we want to improve to analyse these contexts and provide elaborate syntactic constraints. We test our algorithm on a large-scale grammar of Dutch and show that its application leads to an improved parsing accuracy."
W09-2609,A generalized method for iterative error mining in parsing results,2009,5,17,3,0.833333,21366,daniel kok,Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks ({GEAF} 2009),0,"Error mining is a useful technique for identifying forms that cause incomplete parses of sentences. We extend the iterative method of Sagot and de la Clergerie (2006) to treat n-grams of an arbitrary length. An inherent problem of incorporating longer n-grams is data sparseness. Our new method takes sparseness into account, producing n-grams that are as long as necessary to identify problematic forms, but not longer.n n Not every cause for parsing errors can be captured effectively by looking at word n-grams. We report on an algorithm for building more general patterns for mining, consisting of words and part of speech tags.n n It is not easy to evaluate the various error mining techniques. We propose a new evaluation metric which will enable us to compare different error miners."
W09-0107,Parsed Corpora for Linguistics,2009,17,7,1,1,370,gertjan noord,"Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",0,"Knowledge-based parsers are now accurate, fast and robust enough to be used to obtain syntactic annotations for very large corpora fully automatically. We argue that such parsed corpora are an interesting new resource for linguists. The argument is illustrated by means of a number of recent results which were established with the help of parsed corpora."
R09-1012,Combining Finite State and Corpus-based Techniques for Unknown Word Prediction,2009,8,8,2,1,33554,kostadin cholakov,Proceedings of the International Conference {RANLP}-2009,0,"Many NLP systems make use of various lexicons and dictionaries. However, unknown words are a major problem for such resources when applied to real-life data. We propose a method that combines nite state techniques and web queries to deliver possible analyses for a given unknown word and to generate its paradigm. We ensure the general applicability of our approach by applying it to a test set of"
E09-1093,Learning Efficient Parsing,2009,13,11,1,1,370,gertjan noord,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"A corpus-based technique is described to improve the efficiency of wide-coverage high-accuracy parsers. By keeping track of the derivation steps which lead to the best parse for a very large collection of sentences, the parser learns which parse steps can be filtered without significant loss in parsing accuracy, but with an important increase in parsing efficiency. An interesting characteristic of our approach is that it is self-learning, in the sense that it uses unannotated corpora."
W08-1302,Exploring an Auxiliary Distribution Based Approach to Domain Adaptation of a Syntactic Disambiguation Model,2008,19,9,2,0.777778,106,barbara plank,Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,0,"We investigate auxiliary distributions (Johnson and Riezler, 2000) for domain adaptation of a supervised parsing system of Dutch. To overcome the limited target domain training data, we exploit an original and larger out-of-domain model as auxiliary distribution. However, our empirical results exhibit that the auxiliary distribution does not help: even when very little target training data is available the incorporation of the out-of-domain model does not contribute to parsing accuracy on the target domain; instead, better results are achieved either without adaptation or by simple model combination."
oostdijk-etal-2008-coi,From {D}-Coi to {S}o{N}a{R}: a reference corpus for {D}utch,2008,21,31,4,0,13762,nelleke oostdijk,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The computational linguistics community in The Netherlands and Belgium has long recognized the dire need for a major reference corpus of written Dutch. In part to answer this need, the STEVIN programme was established. To pave the way for the effective building of a 500-million-word reference corpus of written Dutch, a pilot project was established. The Dutch Corpus Initiative project or D-Coi was highly successful in that it not only realized about 10{\%} of the projected large reference corpus, but also established the best practices and developed all the protocols and the necessary tools for building the larger corpus within the confines of a necessarily limited budget. We outline the steps involved in an endeavour of this kind, including the major highlights and possible pitfalls. Once converted to a suitable XML format, further linguistic annotation based on the state-of-the-art tools developed either before or during the pilot by the consortium partners proved easily and fruitfully applicable. Linguistic enrichment of the corpus includes PoS tagging, syntactic parsing and semantic annotation, involving both semantic role labeling and spatiotemporal annotation. D-Coi is expected to be followed by SoNaR, during which the 500-million-word reference corpus of Dutch should be built."
W07-2201,Using Self-Trained Bilexical Preferences to Improve Disambiguation Accuracy,2007,26,42,1,1,370,gertjan noord,Proceedings of the Tenth International Conference on Parsing Technologies,0,"A method is described to incorporate bilexical preferences between phrase heads, such as selection restrictions, in a Maximum-Entropy parser for Dutch. The bilexical preferences are modelled as association rates which are determined on the basis of a very large parsed corpus (about 500M words). We show that the incorporation of such self-trained preferences improves parsing accuracy significantly."
W07-2205,The Impact of Deep Linguistic Processing on Parsing Technology,2007,2,7,5,0,1468,timothy baldwin,Proceedings of the Tenth International Conference on Parsing Technologies,0,"As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007), we were asked to discuss our perspectives on the role of current trends in deep linguistic processing for parsing technology. We are particularly interested in the ways in which efficient, broad coverage parsing systems for linguistically expressive grammars can be built and integrated into applications which require richer syntactic structures than shallow approaches can provide. This often requires hybrid technologies which use shallow or statistical methods for pre- or post-processing, to extend coverage, or to disambiguate the output."
W06-2301,"Robust Parsing, Error Mining, Automated Lexical Acquisition, and Evaluation",2006,-1,-1,1,1,370,gertjan noord,Proceedings of the Workshop on {ROMAND} 2006:Robust Methods in Analysis of Natural language Data,0,None
van-noord-etal-2006-syntactic,Syntactic Annotation of Large Corpora in {STEVIN},2006,19,24,1,1,370,gertjan noord,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The construction of a 500-million-word reference corpus of written Dutch has been identified as one of the priorities in the Dutch/Flemish STEVIN programme. For part of this corpus, manually corrected syntactic annotations will be provided. The paper presents the background of the syntactic annotation efforts, the Alpino parser which is used as an important tool for constructing the syntactic annotations, as well as a number of other annotation tools and guidelines. For the full STEVIN corpus, automatically derived syntactic annotations will be provided in a later phase of the programme. A number of arguments is provided suggesting that such a resource can be very useful for applications in information extraction, ontology building, lexical acquisition, machine translation and corpus linguistics."
2006.jeptalnrecital-invite.2,At Last Parsing Is Now Operational,2006,35,144,1,1,370,gertjan noord,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Conf{\\'e}rences invit{\\'e}es,0,"Natural language analysis systems which combine knowledge-based and corpus-based methods are now becoming accurate enough to be used in various applications. We describe one such parsing system for Dutch, known as Alpino, and we show how corpus-based methods are essential to obtain accurate knowledge-based parsers. In particular we show a variety of cases where large amounts of parser output are used to improve the parser."
P04-1057,Error Mining for Wide-Coverage Grammar Engineering,2004,8,55,1,1,370,gertjan noord,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"Parsing systems which rely on hand-coded linguistic descriptions can only perform adequately in as far as these descriptions are correct and complete.The paper describes an error mining technique to discover problems in hand-coded linguistic descriptions for parsing such as grammars and lexicons. By analysing parse results for very large unannotated corpora, the technique discovers missing, incorrect or incomplete linguistic descriptions.The technique uses the frequency of n-grams of words for arbitrary values of n. It is shown how a new combination of suffix arrays and perfect hash finite automata allows an efficient implementation."
W01-1815,Unsupervised {POS}-Tagging Improves Parsing Accuracy and Parsing Efficiency,2001,0,19,2,0,51743,robbert prins,Proceedings of the Seventh International Workshop on Parsing Technologies,0,None
W00-1804,Approximation and Exactness in Finite State {O}ptimality {T}heory,2000,7,23,2,0.952381,42000,dale gerdemann,Proceedings of the Fifth Workshop of the {ACL} Special Interest Group in Computational Phonology,0,None
J00-1005,Treatment of epsilon moves in subset construction,2000,23,30,1,1,370,gertjan noord,Computational Linguistics,0,"The paper discusses the problem of determinizing finite-state automata containing large numbers of emoves. Experiments with finite-state approximations of natural language grammars often give rise to very large automata with a very large number of emoves. The paper identifies and compares a number of subset construction algorithms that treat emoves. Experiments have been performed which indicate that the algorithms differ considerably in practice, both with respect to the size of the resulting deterministic automaton, and with respect to practical efficiency. Furthermore, the experiments suggest that the average number of emoves per state can be used to predict which algorithm is likely to be the fastest for a given input automaton."
E99-1017,Transducers from Rewrite Rules with Backreferences,1999,15,37,2,0.952381,42000,dale gerdemann,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Context sensitive rewrite rules have been widely used in several areas of natural language processing, including syntax, morphology, phonology and speech processing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various algorithms to compile such rewrite rules into finite-state transducers. The present paper extends this work by allowing a limited form of backreferencing in such rules. The explicit use of backreferencing leads to more elegant and general solutions."
W98-1306,Treatment of e-Moves in Subset Construction,1998,14,7,1,1,370,gertjan noord,Finite State Methods in Natural Language Processing,0,"The paper discusses the problem of determinising finite-state automata containing large numbers of e-moves. Experiments with finite-state approximations of natural language grammars often give rise to very large automata with a very large number of e-moves. The paper identifies three subset construction algorithms which treat e-moves. A number of experiments has been performed which indicate that the algorithms differ considerably in practice. Furthermore, the experiments suggest that the average number of e-moves per state can be used to predict which algorithm is likely to perform best for a given input automaton."
W97-1513,Hdrug. A Flexible and Extendible Development Environment for Natural Language Processing.,1997,0,1,1,1,370,gertjan noord,Computational Environments for Grammar Development and Linguistic Engineering,0,None
W97-0614,Grammatical analysis in the {OVIS} spoken-dialogue system,1997,12,10,4,0,5269,markjan nederhof,Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications,0,"We argue that grammatical processing is a viable alternative to concept spotting for processing spoken input in a practical dialogue system. We discuss the structure of the grammar, the properties of the parser, and a method for achieving robustness. We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input."
J97-3004,An Efficient Implementation of the Head-Corner Parser,1997,29,67,1,1,370,gertjan noord,Computational Linguistics,0,"This paper describes an efficient and robust implementation of a bidirectional, head-driven parser for constraint-based grammars. This parser is developed for the OVIS system: a Dutch spoken dialogue system in which information about public transport can be obtained by telephone.After a review of the motivation for head-driven parsing strategies, and head-corner parsing in particular, a nondeterministic version of the head-corner parser is presented. A memorization technique is applied to obtain a fast parser. A goal-weakening technique is introduced, which greatly improves average case efficiency, both in terms of speed and space requirements.I argue in favor of such a memorization strategy with goal-weakening in comparison with ordinary chart parsers because such a strategy can be applied selectively and therefore enormously reduces the space requirements of the parser, while no practical loss in time-efficiency is observed. On the contrary, experiments are described in which head-corner and left-corner parsers implemented with selective memorization and goal weakening outperform standard chart parsers. The experiments include the grammar of the OVIS system and the Alvey NL Tools grammar.Head-corner parsing is a mix of bottom-up and top-down processing. Certain approaches to robust parsing require purely bottom-up processing. Therefore, it seems that head-corner parsing is unsuitable for such robust parsing techniques. However, it is shown how underspecification (which arises very naturally in a logic programming environment) can be used in the head-corner parser to allow such robust parsing techniques. A particular robust parsing model, implemented in OVIS, is described."
P95-1022,The intersection of Finite State Automata and Definite Clause Grammars,1995,12,17,1,1,370,gertjan noord,33rd Annual Meeting of the Association for Computational Linguistics,1,"Bernard Lang defines parsing as the calculation of the intersection of a FSA (the input) and a CFG. Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems, in which parsing takes a word lattice as input (rather than a word string). Furthermore, certain techniques for robust parsing can be modelled as finite state transducers.In this paper we investigate how we can generalize this approach for unification grammars. In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG. It is shown that existing parsing algorithms can be easily extended for FSA inputs. However, we also show that the termination properties change drastically: we show that it is undecidable whether the intersection of a FSA and a DCG is empty (even if the DCG is off-line parsable).Furthermore we discuss approaches to cope with the problem."
P94-1021,Constraint-Based Categorial Grammar,1994,9,15,2,1,5827,gosse bouma,32nd Annual Meeting of the Association for Computational Linguistics,1,"We propose a generalization of Categorial Grammar in which lexical categories are defined by means of recursive constraints. In particular, the introduction of relational constraints allows one to capture the effects of (recursive) lexical rules in a computationally attractive manner. We illustrate the linguistic merits of the new approach by showing how it accounts for the syntax of Dutch cross-serial dependencies and the position and scope of adjuncts in such constructions. Delayed evaluation is used to process grammars containing recursive constraints."
C94-1039,Adjuncts and the Processing of Lexical Rules,1994,3,50,1,1,370,gertjan noord,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The standard HPSG analysis of germanic verb clusters can not explain the observed narrowscope readings of adjuncts in such verb clusters.We present an extension of the HPSG analysis that accounts for the systematic ambiguity of the scope of adjuncts in verb cluster constructions, by treating adjuncts as members of the subcat list. The extension uses powerful recursive lexical rules, implemented as complex constraints. We show how 'delayed evaluation' techiniques from constraint-logic programming can be used to process such lexical rules."
E93-1010,Head-driven Parsing for Lexicalist Grammars: Experimental Results,1993,8,24,2,1,5827,gosse bouma,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present evidence that head-driven parsing strategies lead to efficiency gains over standard parsing strategies, for lexicalist, concatenative and unification-based grammars. A head-driven parser applies a rule only after a phrase matching the head has been derived. By instantiating the head of the rule important information is obtained about the left-hand-side and the other elements of the ritht-hand-side. We have used two different head-driven parsers and a number of standard parsers to parse with lexicalist grammars for English and for Dutch. The results indicate that for important classes of lexicalist grammars it is fruitful to apply parsing strategies which are sensitive to the linguistic notion 'head'."
C92-2105,Self-Monitoring with Reversible Grammars,1992,12,13,2,0,23452,gunter neumann,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We describe a method and its implementation for self-monitoring during natural language generation. In situations of communication where the generation of ambiguous utterances should be avoided our method is able to compute an unambiguous utterance for a given semantic input. The proposed method is based on a very strict integration of parsing and generation. During the monitored generation step, a previously generated (possibly) ambiguous utterance is parsed and the obtained alternative derivation trees are used as a 'guide' for re-generating the utterance. To achieve such an integrated approach the underlying grammar must be reversible."
W91-0103,Towards Uniform Processing of Constraint-based Categorial Grammars,1991,-1,-1,1,1,370,gertjan noord,Reversible Grammar in Natural Language Processing,0,None
P91-1015,Head Corner Parsing for Discontinuous Constituency,1991,17,21,1,1,370,gertjan noord,29th Annual Meeting of the Association for Computational Linguistics,1,"I describe a head-driven parser for a class of grammars that handle discontinuous constituency by a richer notion of string combination than ordinary concatenation. The parser is a generalization of the left-corner parser (Matsumoto et al., 1983) and can be used for grammars written in powerful formalisms such as non-concatenative versions of HPSG (Pollard, 1984; Reape, 1989)."
J90-1004,Semantic-Head-Driven Generation,1990,0,0,2,0.376654,12905,stuart shieber,Computational Linguistics,0,We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applic...
C90-2052,Reversible Unification Based Machine Translation,1990,16,19,1,1,370,gertjan noord,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"In this paper it will be shown how unification grammars can be used to build a reversible machine translation system.Unification grammars are often used to define the relation between strings and meaning representations in a declarative way. Such grammars are sometimes used in a bidirectional way, thus the same grammar is used for both parsing and generation. In this paper I will show how to use bidirectional unification grammars to define reversible relations between language dependent meaning representations. Furthermore it is shown how to obtain a completely reversible MT system using a series of (bidirectional) unification grammars."
P89-1002,A Semantic-Head-Driven Generation Algorithm for Unification-Based Formalisms,1989,16,78,2,0,12905,stuart shieber,27th Annual Meeting of the Association for Computational Linguistics,1,"We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion."
E89-1040,An Approach to Sentence-Level Anaphora in Machine Translation,1989,9,4,1,1,370,gertjan noord,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Theoretical research in the area of machine translation usually involves the search for and creation of an appropriate formalism. An important issue in this respect is the way in which the compositionality of translation is to be defined. In this paper, we will introduce the anaphoric component of the Mimo formalism. It makes the definition and translation of anaphoric relations possible, relations which are usually problematic for systems that adhere to strict compositionality. In Mimo, the translation of anaphoric relations is compositional. The anaphoric component is used to define linguistic phenomena such as wh-movement, the passive and the binding of reflexives and pronouns monolingually. The actual working of the component will be shown in this paper by means of a detailed discussion of wh-movement."
