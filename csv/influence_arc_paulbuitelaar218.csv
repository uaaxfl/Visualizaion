2013.mtsummit-posters.1,W11-2107,0,0.0170933,"guee we were actually able to generate translation models for several languages, i.e. English-German, English-Spanish and English-French (see Figure 3). 4 Experiments and Evaluation Since the FINREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely accessible graphical user interface (GUI), w"
2013.mtsummit-posters.1,W11-1204,0,0.0469957,"Missing"
2013.mtsummit-posters.1,2005.mtsummit-papers.11,0,0.0204063,"a collection of legislative texts of the European Union written between 1950 and now and is available in more than twenty official European languages (Steinberger et al., 2006). The EnglishGerman parallel corpus consists of 1.2 million aligned sentences, and 32 million English and 30 million German tokens. A similar corpus to JRC-Acquis is the Europarl parallel corpus (version 7),8 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 47 million English and 45 million German tokens (Koehn, 2005). Repurchase agreements, Guarantees given, Equity instruments . . . Provisions, Securities, Assets . . . Table 1: Examples of the longest and shortest financial labels in FINREP taxonomy hold 569 monolingual labels in English. FINREP labels are mostly noun phrases, many of which are quite long as can be seen in Figure 1. The longer labels are the product of nominalizing and condensing descriptions of the meaning of the corresponding reporting concept. Each reporting concept has, in addition to its labels, a unique cluster of XBRL identifiers, which are used to tag instances of the concept, e.g"
2013.mtsummit-posters.1,P07-2045,0,0.00944041,"NREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely accessible graphical user interface (GUI), which uses the domain-specific translation models described.19 Figure 3: Translation GUI for the financial domain Figure 3 illustrates the options of the GUI. The interface allows differen"
2013.mtsummit-posters.1,J03-1002,0,0.00432263,"ranslations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely accessible graphical user interface (GUI), which uses the domain-specific translation models described.19 Figure 3: Translation GUI for the financial domain Figure 3 illustrates the options of the GUI. The interface allows different language pairs and different size n-best lists when the translations are generated. Further an output option is available, which generates a downloadable .csv file. The ”Upload dictionary” option allows the user to"
2013.mtsummit-posters.1,P02-1040,0,0.0864417,"es and unigram expressions. Thanks to the extensive multilingual data of Wikipedia and Linguee we were actually able to generate translation models for several languages, i.e. English-German, English-Spanish and English-French (see Figure 3). 4 Experiments and Evaluation Since the FINREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In com"
2013.mtsummit-posters.1,J03-3002,0,0.0189339,"pedia article titles with their multilingual equivalents, Wikipedia holds much more information in the articles themselves. Therefore, exploiting these non-parallel resources in future, as shown by Fiˇser et al. (2011), would clearly help to improve the performance of the translation system. Besides Wikipedia/DBpedia, which can be used for lexicon generation and WSD, the Web itself stores an enormous amount of data, which is often represented in a multilingual way. Therefore a major part of the future work needs to be focused on extraction and alignment on multilingual websites and documents (Resnik and Smith, 2003). In addition to exploiting new resources for statistical machine translation, the manual evaluation for translated labels needs to become the focus of our future work. Although such manual evaluation is time consuming, it provides a closer look into the translation errors. Even through the small manual evaluation of 100 FINREP labels we learned that fine-grained translation error classes have to be formulated. We observed that we have to distinguish between translations with ”one grammatical error” or ”several grammatical errors”. It might also be interesting to classify the types of grammati"
2013.mtsummit-posters.1,2006.amta-papers.25,0,0.0181138,"ltilingual data of Wikipedia and Linguee we were actually able to generate translation models for several languages, i.e. English-German, English-Spanish and English-French (see Figure 3). 4 Experiments and Evaluation Since the FINREP taxonomy is monolingual a straightforward automatic evaluation is not possible. Therefore we randomly chose 100 labels, which were translated into German by an expert. For this experiment we concentrated only on translations from English to German and vice versa. For the automatic evaluation we used the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor18 (Denkowski and Lavie, 2011) algorithms. 18 Moses Toolkit and Graphical User Interface For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). In combination with the Moses Toolkit we built a freely ac"
2013.mtsummit-posters.1,steinberger-etal-2006-jrc,0,0.0679597,".. Unpaid capital which has been called up Figure 1: The financial label Demand deposits and cash equivalents and its ancestors in the financial taxonomy FINREP Length Count 30 1 2 ... 110 1 36 Examples 3.2 Financial assets pledged as collateral, financial assets pledged as collateral for which the transferre has the right to sell or repledge in the absence of default by the reporting institution The parallel corpus JRC-Acquis7 (version 3.0) is a collection of legislative texts of the European Union written between 1950 and now and is available in more than twenty official European languages (Steinberger et al., 2006). The EnglishGerman parallel corpus consists of 1.2 million aligned sentences, and 32 million English and 30 million German tokens. A similar corpus to JRC-Acquis is the Europarl parallel corpus (version 7),8 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 47 million English and 45 million German tokens (Koehn, 2005). Repurchase agreements, Guarantees given, Equity instruments . . . Provisions, Securities, Assets . . . Table 1: Examples of the longest and shortest financial lab"
2013.mtsummit-posters.1,C08-1125,0,0.180485,"d to train a translation and language model. For our current research we used the JRC-Acquis, Europarl and the European Central Bank (ECB) corpora. Finally, in Section 3.4 we describe the procedure to obtain a domain-specific corpus from Linguee and Wikipedia/DBpedia. The results of the translations produced by an SMT trained on these domain-specific resources were compared to SMT results from a system trained on more general resources. Although previous research showed that a translation model built by using a general parallel corpus cannot be used for domain-specific vocabulary translation (Wu et al., 2008), we decided to train a baseline translation model on this existing corpora to illustrate any improvements gained by modelling a new domain-specific corpus for the financial domain. 3.1 The Financial taxonomy - FINREP Under EU law financial institutions such as banks, credit institutions and investment firms must submit periodic reports to national supervisory bodies. The content of these reports is guided by the European Banking Authority4 (EBA) by means of two complementary reporting frameworks: financial reporting (FINREP) and COREP5 (COmmon solvency ratio REPorting) common reporting. These"
2014.amta-researchers.5,P13-1040,0,0.221652,"-linear weights on this specific data. Bilingual term extraction is performed in two steps. First, the source and the target sides of the data are processed by a keyword extractor to identify the most relevant terms in each language. Taking advantage of the parallel data, each monolingual term in the source language is paired with a term in the target language. We perform this step by comparing different techniques, showing that simple approaches based on word alignment and term translation are more robust and more efficient than the state-of-the-art method based on supervised classification (Aker et al., 2013). As regards the integration of the bilingual terms in an SMT system, we cannot apply wellknown approaches (Bouamor et al., 2012) adding the terms to training data or at the end of the phrase table, because in our CAT scenario we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add"
2014.amta-researchers.5,C12-1005,1,0.865188,"ir approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with user-defined translations. Since the manual development of terminological resources is a time intensive and expensive task, our framework continuously builds bilingual terminology knowledge from the already translated sentences. In order to tackle term translation and the out-of-vocabulary issues, Arcan et al. (2012) used the multilingual web to built a parallel domain-specific corpus based on the vocabulary to be translated. Additionally, Arcan et al. (2014) extend their work focusing on disambiguated term extraction using the rich lexical and semantic knowledge of Wikipedia. 8 Conclusion In this paper, we propose a framework to enhance translation quality by exploiting bilingual terms extracted from the parallel sentences daily produced by professional translators. The results show that an SMT model enriched with the identified bilingual terms substantially improves translation quality in terms of BLEU"
2014.amta-researchers.5,W14-4803,1,0.785943,"d. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with user-defined translations. Since the manual development of terminological resources is a time intensive and expensive task, our framework continuously builds bilingual terminology knowledge from the already translated sentences. In order to tackle term translation and the out-of-vocabulary issues, Arcan et al. (2012) used the multilingual web to built a parallel domain-specific corpus based on the vocabulary to be translated. Additionally, Arcan et al. (2014) extend their work focusing on disambiguated term extraction using the rich lexical and semantic knowledge of Wikipedia. 8 Conclusion In this paper, we propose a framework to enhance translation quality by exploiting bilingual terms extracted from the parallel sentences daily produced by professional translators. The results show that an SMT model enriched with the identified bilingual terms substantially improves translation quality in terms of BLEU score over a generic baseline system. Furthermore, we investigate the integration of the extracted bilingual terms into the SMT system. For the f"
2014.amta-researchers.5,2013.mtsummit-papers.5,0,0.459578,"based on word alignment and term translation are more robust and more efficient than the state-of-the-art method based on supervised classification (Aker et al., 2013). As regards the integration of the bilingual terms in an SMT system, we cannot apply wellknown approaches (Bouamor et al., 2012) adding the terms to training data or at the end of the phrase table, because in our CAT scenario we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add bilingual terms into an SMT system in real-time, without the need to stop it. In addition, we compare the cache-based models with a recently developed technique, namely the Realtime Adaptive Translation Systems with cdec (Denkowski et al., 2014), that, based on lexicalized synchronous context-free grammars, takes as input the whole source and post-edited sentences and automatically updates the models. The evaluation of our framework on two different"
2014.amta-researchers.5,W09-0432,0,0.0612499,"passing the new weights to Moses through XML tags for each incoming sentence, which required to extend Moses with this new option. An issue with incremental tuning is the risk of over-fitting of the model on a small development set, when it differs from the test set. In our scenario, this is prevented by the fact that all the sets come from the same document, or from different documents on similar topic in the same project. Although it is important to tune an SMT system on a sufficiently large development set, reasonably good weights can be obtained even if such data are very few, as shown in Bertoldi and Federico (2009). In our framework, it is not possible to concatenate all the previous partitions to enlarge the development set, because the presence of already extracted bilingual terms in the cache-based models would artificially favour the cache-based components during the tuning. 4 Experimental Setting In this Section, we propose a set of experiments aimed at showing the capability of our framework to extract high quality domain-specific bilingual terms from a small amount of parallel data and to integrate them in the translation task. The translation direction considered is from English to Italian. To i"
2014.amta-researchers.5,bouamor-etal-2012-identifying,0,0.221065,"ides of the data are processed by a keyword extractor to identify the most relevant terms in each language. Taking advantage of the parallel data, each monolingual term in the source language is paired with a term in the target language. We perform this step by comparing different techniques, showing that simple approaches based on word alignment and term translation are more robust and more efficient than the state-of-the-art method based on supervised classification (Aker et al., 2013). As regards the integration of the bilingual terms in an SMT system, we cannot apply wellknown approaches (Bouamor et al., 2012) adding the terms to training data or at the end of the phrase table, because in our CAT scenario we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add bilingual terms into an SMT system in real-time, without the need to stop it. In addition, we compare the cache-based models wit"
2014.amta-researchers.5,P11-2031,0,0.0255575,"slations. Furthermore, XML markup cannot handle overlaps between dictionary entries. In our experiments, we found only 15 cases where the entries overlap, whereby we give preference to longer source terms. For each set of partitions, the incremental tuning was run to update the log-linear weights. For a comparison, we also run MERT on each partition starting with flat weights (nonincremental tuning). In Table 3, we report BLEU scores for each partition separately (columns “Part #”), as well as the evaluation on the whole corpus (column “Document level”). The approximate randomization approach Clark et al. (2011) is used to test whether differences among system performances are statistically significant at document level. Results in the table marked with * are statistically significantly better than the baseline with a p-value &lt; 0.05. Comparing the baseline XML markup and the cache-based methods, we notice that the translation performance of cache-based models always outperforms significantly all the other methods in both domains. This is also confirmed at partition level, with few exceptions for the initial partitions. The XML markup performs better than the baseline in both domains, but statistical"
2014.amta-researchers.5,E14-1042,0,0.0177783,"rio we cannot stop the translation service and let translators wait for a long training time. For this reason, we investigate for the first time the integration of cache-based translation and language models (Bertoldi et al., 2013) in the context of terminology embedding comparing them with the XML markup technique. The cache-based model makes it possible to periodically add bilingual terms into an SMT system in real-time, without the need to stop it. In addition, we compare the cache-based models with a recently developed technique, namely the Realtime Adaptive Translation Systems with cdec (Denkowski et al., 2014), that, based on lexicalized synchronous context-free grammars, takes as input the whole source and post-edited sentences and automatically updates the models. The evaluation of our framework on two different domains (IT and medical) suggests that: (i) an SMT model enriched with the identified bilingual terms substantially improves translation quality in terms of BLEU score over a generic SMT system; (ii) strategies to integrate terminology need to take into consideration also the surrounding context of a translated term; (iii) in order to take advantage of the continuous appending of new info"
2014.amta-researchers.5,N13-1073,0,0.0382693,"altime cdec) an online model adaptation system. Differently from the cache-based approach, it automatically extracts new translation rules from the whole source and post-edited sentences and adds them to the translation grammar. This system takes advantage of cdec (Dyer et al., 2010), a standalone decoder, aligner, and learning framework for SMT. cdec allows us to train word-based and phrase-based models, as well as models based on lexicalized synchronous content-free grammars (SCFG), which was used in our experiment. The adaptation of cdec to work in real time requires the use of Fast Align (Dyer et al., 2013) to perform on-the-fly word alignment between source and post-edited sentences. This makes possible the incremental addition of information to the translation models after a sentence is translated. Furthermore, Realtime cdec adapts the Bayesian language model using the hierarchical Pitman-Yor process approach, whereby MIRA (Chiang, 2012) is used to optimize the discriminative parameters of the decoder. In our experiments we use the Realtime cdec similarly to the scenario described in Section 3.2. Each sentence pair (source, post-edition) from partitionn−1 is added to the model and used by MIRA"
2014.amta-researchers.5,P10-4002,0,0.0287658,"ed also by the results reported in Table 3, showing that translation quality is generally lower than for the IT domain. 6 Cache-Based Model vs. Online Adaptation Model with cdec To complete our evaluation, we compare the XML markup and the cache-based approach with the Realtime Adaptive Translation Systems with cdec,9 (henceforth Realtime cdec) an online model adaptation system. Differently from the cache-based approach, it automatically extracts new translation rules from the whole source and post-edited sentences and adds them to the translation grammar. This system takes advantage of cdec (Dyer et al., 2010), a standalone decoder, aligner, and learning framework for SMT. cdec allows us to train word-based and phrase-based models, as well as models based on lexicalized synchronous content-free grammars (SCFG), which was used in our experiment. The adaptation of cdec to work in real time requires the use of Fast Align (Dyer et al., 2013) to perform on-the-fly word alignment between source and post-edited sentences. This makes possible the incremental addition of information to the translation models after a sentence is translated. Furthermore, Realtime cdec adapts the Bayesian language model using"
2014.amta-researchers.5,R11-1017,1,0.891393,"Missing"
2014.amta-researchers.5,C14-2028,1,0.840113,"monolingual term extraction tool as well as the most suitable bilingual alignment approach, we use freely available data, which were manually annotated to better evaluate all the intermediate steps of the experiment. Two datasets belonging to the IT domain, namely a portion of GNOME project data (4,3K tokens)5 and KDE Data (9,5K),6 are used for domain-specific term extraction. The whole framework, including the machine translation part, is tested on a subset of the EMEA corpus (Tiedemann, 2009) for the medical domain (18K tokens) and an IT corpus (18K), extracted from a software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann"
2014.amta-researchers.5,2012.amta-papers.22,0,0.10194,"Missing"
2014.amta-researchers.5,W12-3154,0,0.0264004,"i´c (2012) extract terms and lexicon entries from SMT phrase tables. In their approach they apply linguistic, lexicon and frequency filters to obtain good lexicon entries. Similarly, we also access the phrase table to build our bilingual terminology, whereby our filter relies on the term and sentence lookup approach. Furthermore, there has been research done on the integration of domain-specific parallel data into SMT, e.g. dictionaries or bilingual terminology, either by retraining new and general parallel resources or adding new entries to the phrase table (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012; Pinnis et al., 2012). Furthermore, Okita and Way (2010) investigate the effect of integrating bilingual terminology in the training step of an SMT system, and analyse in particular the performance of a word aligner sensitive to multi-word expressions and translation smoothing. As opposed to their approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with us"
2014.amta-researchers.5,itagaki-aikawa-2008-post,0,0.0258275,"d general parallel resources or adding new entries to the phrase table (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012; Pinnis et al., 2012). Furthermore, Okita and Way (2010) investigate the effect of integrating bilingual terminology in the training step of an SMT system, and analyse in particular the performance of a word aligner sensitive to multi-word expressions and translation smoothing. As opposed to their approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT output and automatically swap them with user-defined translations. Since the manual development of terminological resources is a time intensive and expensive task, our framework continuously builds bilingual terminology knowledge from the already translated sentences. In order to tackle term translation and the out-of-vocabulary issues, Arcan et al. (2012) used the multilingual web to built a parallel domain-specific corpus based on the vocabulary to be translated. Additionally, Arcan et al. (2014) extend their work focusing on disam"
2014.amta-researchers.5,S10-1004,0,0.0174807,"where no or little training data are available, we chose three unsupervised terminology extractors supporting different languages. 4 http://www.alchemyapi.com/products/features/keyword-extraction/ Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 55 KX is a terminology extractor, which combines frequency information and part-of-speech patterns of n-grams to identify the most relevant terms in a corpus. It is freely available for English and Italian and was the first-ranked unsupervised system in the Semeval2010 task on keyword extraction (Kim et al., 2010). TWSC follows an approach which is very similar to KX, integrating morpho-syntactic patterns with statistical features. One of the main differences w.r.t. KX is the implementation of different co-occurrence statistics to rank term candidates, and the treatment of nested terms. Nevertheless, we expect the performance of these two tools to be very similar. A third system considered is AlchemyAPI. This commercial tool employs sophisticated statistical algorithms and linguistic approaches to analyse textual content and extract topic keywords, but no further implementation details are given. 2.2 B"
2014.amta-researchers.5,2005.mtsummit-papers.11,0,0.0114654,"software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), obtaining a training corpus of 37M tokens and a development set of ∼25K tokens. The generic SMT system used in all our experiments is trained on this merged general resource. The difference in size between the specific and the generic data is evident, i.e. approximately few thousands vs. more than 30 million tokens. For both domains, this reflects a real CAT scenario, where only a small quantity of domain-specific data is available. Manual Terminology Annotation In order to evaluate the quality of the bilingual terms, we create a terminological gold st"
2014.amta-researchers.5,P07-2045,0,0.0124687,"shown significant productivity gains when human translators post-edit machine translation output rather than translating documents from scratch. This evidence has raised interest in the integration of machine translation systems within CAT software. In this context, an important open issue is how to support translators with domain-specific information when dealing with highly specific texts, i.e. manuals coming from different domains (information technology (IT), legal, agriculture, etc.). Translation tools such as Google Translate,1 Bing Translator2 or open source SMT systems such as Moses (Koehn et al., 2007) trained on generic data are the most common solutions, but they often result in unsatisfactory translations. A valuable alternative to support professional translators is represented by online terminology resources, e.g. IATE,3 which are continuously updated and can be easily queried. However, the manual use of these services can be very time demanding when working with a CAT tool. For these reasons, the automatic identification and integration of bilingual domain-specific terms into an SMT system is a crucial step towards increasing translation quality of high-specific texts in a CAT environ"
2014.amta-researchers.5,W02-1405,0,0.154471,"al terminology. Thurmair and Aleksi´c (2012) extract terms and lexicon entries from SMT phrase tables. In their approach they apply linguistic, lexicon and frequency filters to obtain good lexicon entries. Similarly, we also access the phrase table to build our bilingual terminology, whereby our filter relies on the term and sentence lookup approach. Furthermore, there has been research done on the integration of domain-specific parallel data into SMT, e.g. dictionaries or bilingual terminology, either by retraining new and general parallel resources or adding new entries to the phrase table (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012; Pinnis et al., 2012). Furthermore, Okita and Way (2010) investigate the effect of integrating bilingual terminology in the training step of an SMT system, and analyse in particular the performance of a word aligner sensitive to multi-word expressions and translation smoothing. As opposed to their approach, we do not have prior knowledge about the bilingual terminology, since we extract it on the fly based on the document to be translated. As a post-processing step, Itagaki and Aikawa (2008) propose a way to identify terminology translations from SMT"
2014.amta-researchers.5,2013.mtsummit-wptp.10,0,0.0652808,"Missing"
2014.amta-researchers.5,J03-1002,0,0.00565761,"or domain-specific term extraction. The whole framework, including the machine translation part, is tested on a subset of the EMEA corpus (Tiedemann, 2009) for the medical domain (18K tokens) and an IT corpus (18K), extracted from a software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), obtaining a training corpus of 37M tokens and a development set of ∼25K tokens. The generic SMT system used in all our experiments is trained on this merged general resource. The difference in size between the specific and the generic data is evident, i.e. approximately few thousands vs. more than 30 million tokens. For both domai"
2014.amta-researchers.5,P02-1040,0,0.092848,"reement following Landis and Koch (1977). This annotation effort resulted in the identification of 874 domain-specific bilingual terms in the two datasets.7 5 Evaluation In this Section, we report the quality of monolingual term extraction and the bilingual alignment. For each domain we evaluate the performance obtained by applying different approaches to the integration of bilingual terms into an SMT system. Evaluation of the extracted monolingual and bilingual terms is performed on the manually annotated KDE and GNOME datasets by calculating precision, recall and f-measure. The BLEU metric (Papineni et al., 2002) is used to automatically evaluate the translation quality of the EMEA and the IT manual datasets. 5.1 Monolingual Term Extraction Our first evaluation concerns monolingual term extraction from English and Italian documents provided by the KX, AlchemyAPI and TWSC extraction tools. As shown in Table 1, KX tends to overgenerate when extracting English terms. It extracts the highest number of expressions, which results in a high recall, but low precision. On the other hand, TWSC extracts the least English terms. Based on F1, we observe that AlchemyAPI is the best performing tool when extracting E"
2014.amta-researchers.5,S10-1036,0,0.0136457,"lel data), while the second is the creation of bilingual terminology starting from the monolingual ones. In order to obtain the best possible performance, we compare different approaches in both steps. At the monolingual level, we test the extraction using three unsupervised term extraction tools. For bilingual alignment, we compare different alignment strategies. The two steps are detailed in the following subsections. 2.1 Monolingual Terminology Extraction In order to find the best performing approach to identify monolingual terms, we compare three available term extractors: the KX toolkit (Pianta and Tonelli, 2010), TWSC (Pinnis et al., 2012) and AlchemyAPI.4 Given our experimental scenario, where no or little training data are available, we chose three unsupervised terminology extractors supporting different languages. 4 http://www.alchemyapi.com/products/features/keyword-extraction/ Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 55 KX is a terminology extractor, which combines frequency information and part-of-speech patterns of n-grams to identify the most relevant terms in a corpus. It is freely available for English and Italian and was the fi"
2014.amta-researchers.5,W09-2907,0,0.0426805,"Missing"
2014.amta-researchers.5,steinberger-etal-2006-jrc,0,0.0430484,"an IT corpus (18K), extracted from a software user manual (Federico et al., 2014). Each corpus is split in partitions of around 3,000 tokens, i.e. the daily workload of a professional translator in post-editing, resulting in 6 partitions each. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage of the generic SMT system, we merged parts of JRCAcquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), obtaining a training corpus of 37M tokens and a development set of ∼25K tokens. The generic SMT system used in all our experiments is trained on this merged general resource. The difference in size between the specific and the generic data is evident, i.e. approximately few thousands vs. more than 30 million tokens. For both domains, this reflects a real CAT scenario, where only a small quantity of domain-specific data is available. Manual Terminology Annotation In order to evaluate the quality of the bilingual terms, we create"
2014.amta-researchers.5,2012.eamt-1.59,0,0.0531867,"Missing"
2014.amta-researchers.5,W10-2602,0,0.0457622,"Missing"
2014.amta-researchers.5,vintar-fiser-2008-harvesting,0,0.0420124,"Missing"
2014.amta-researchers.5,O03-1003,0,0.20189,"Missing"
2014.amta-researchers.5,O04-2001,0,\N,Missing
2014.amta-researchers.5,C94-1084,0,\N,Missing
2014.amta-researchers.5,N10-1062,0,\N,Missing
2020.figlang-1.22,E06-1042,0,0.0739157,"aphor identification explicitly analyses the tenor and the relation between The levels of processing metaphors should be taken into consideration when designing and developing a computational model to identify metaphors and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor identification. Stanford CoreNLP is used to generate the dependencies. Zayed’s Tweets (Zayed et al., 2019) datasets are utilised for relation-level metaphor identification. Approaches addressing the task on the wordlevel are not fairly comparable to relation-level approaches since ea"
2020.figlang-1.22,W07-0104,0,0.0372383,"Missing"
2020.figlang-1.22,P06-4020,0,0.0317125,"aches using this dataset. In this paper, we introduce the first adapted version of the VUAMC. Furthermore, we adapt the TroFi and the TSV datasets to better suit relation-level metaphor processing. Related Work 4 This work is inspired by Tsvetkov et al. (2014) and Shutova et al. (2016) who attempted to adapt existing word-level metaphor identification datasets to suit their relation-level (phrase-level) identification approaches. Shutova et al. (2010) was the first to create an annotated dataset for relation-level metaphor identification. The Robust Accurate Statistical Parsing (RASP) parser (Briscoe et al., 2006) was utilised to extract verb-subject and verb-direct object grammar relations from the British National Corpus (BNC) (Burnard, 2007). The dataset comprises around 62 verb-noun pairs of metaphoric expressions, where the verb is used metaphorically given the complement noun (tenor). The TroFi dataset, which was designed to classify particular literal and metaphoric verbs on the word-level, was adapted by Tsvetkov et al. (2014) in order to extract metaphoric expressions on the relation-level. The authors parsed the original dataset using the Turbo dependency parser (Martins et al., 2010) to extr"
2020.figlang-1.22,E17-2084,0,0.0131724,"n the wordlevel, to suit relation-level metaphor identification of verb-noun relations. Verb-direct object and verbs-subject dependencies were extracted and filtered yielding a dataset of 647 verb–noun pairs, out of which 316 instances are metaphorical and 331 instances are literal. To the best of our knowledge, there is no attempt to adapt the benchmark VU Amsterdam metaphor corpus, referred to as VUAMC, to suit relation-level metaphor identification. This has discouraged other researchers focusing on relationlevel approaches to employ this dataset such as the work done by Rei et al. (2017), Bulat et al. (2017), Shutova et al. (2016) and Tsvetkov et al. (2014) who did not evaluate or compare their approaches using this dataset. In this paper, we introduce the first adapted version of the VUAMC. Furthermore, we adapt the TroFi and the TSV datasets to better suit relation-level metaphor processing. Related Work 4 This work is inspired by Tsvetkov et al. (2014) and Shutova et al. (2016) who attempted to adapt existing word-level metaphor identification datasets to suit their relation-level (phrase-level) identification approaches. Shutova et al. (2010) was the first to create an annotated dataset for r"
2020.figlang-1.22,D14-1082,0,0.0115165,"e) labelled as a metaphor regardless of its tenor since it is word-by-word classification. Therefore, in order to adapt them to suit relation-level processing, the associated target domain words (tenor) need to be identified. 3 1. select the benchmark dataset which is originally annotated on the word-level; 2. extract particular grammatical relations focusing on the vehicle as the head of the relation (e.g. the verb in a dobj or adjective in amod relation); 4. verify the correctness of the retrieved relations and the assigned gold label. In this work, we employ the Stanford dependency parser (Chen and Manning, 2014) to identify grammar relations. The recurrent neural network (RNN) parser, pre-trained on the WSJ corpus, is used from within the Stanford CoreNLP toolkit (Manning et al., 2014). For the VUAMC adaptation, as discussed in Section 4, we utilise the training and test splits provided by the NAACL metaphor shared task in the Verbs track. We focus on this track since we are interested in verb-noun relations. The verbs dataset consists of 17,240 annotated verbs in the training set and 5,874 annotated verbs in the test set. First, we retrieved the original sentences of these verbs from the VUAMC since"
2020.figlang-1.22,W19-4444,0,0.0230971,"guage in terms of linguistic metaphors such as “shattered my emotions”,“break his soul”,“crushed her happiness”, “fragile emotions” and “brittle feelings”. Due to their nebulous nature, metaphors are quite challenging to comprehend and process by humans, let alone computational models. This intrigued many researchers to develop various automatic techniques to process metaphor in text. Metaphor processing has many potential applications, either as part of natural language processing (NLP) tasks such as machine translation (Koglin and Cunha, 2019), text simplification (Wolska and Clausen, 2017; Clausen and Nastase, 2019) and sentiment analysis (Rentoumi et al., 2012) or in more general discourse analysis use cases such as in analysing political discourse (Charteris-Black, 2011), financial reporting (Ho and Cheng, 2016) and health communication (Semino et al., 2018). The computational processing of metaphors can be divided into two tasks, namely metaphor identification and its interpretation. While the former is concerned with recognising the metaphoric word or expression in a given sentence, the latter focuses on discerning the meaning of the metaphor. Metaphor identification is studied more extensively than"
2020.figlang-1.22,W18-0907,0,0.0375345,"Missing"
2020.figlang-1.22,P14-5010,0,0.00253319,"et domain words (tenor) need to be identified. 3 1. select the benchmark dataset which is originally annotated on the word-level; 2. extract particular grammatical relations focusing on the vehicle as the head of the relation (e.g. the verb in a dobj or adjective in amod relation); 4. verify the correctness of the retrieved relations and the assigned gold label. In this work, we employ the Stanford dependency parser (Chen and Manning, 2014) to identify grammar relations. The recurrent neural network (RNN) parser, pre-trained on the WSJ corpus, is used from within the Stanford CoreNLP toolkit (Manning et al., 2014). For the VUAMC adaptation, as discussed in Section 4, we utilise the training and test splits provided by the NAACL metaphor shared task in the Verbs track. We focus on this track since we are interested in verb-noun relations. The verbs dataset consists of 17,240 annotated verbs in the training set and 5,874 annotated verbs in the test set. First, we retrieved the original sentences of these verbs from the VUAMC since the shared task released their ids and the corresponding gold labels. This yielded around 10,570 sentences in both sets. Then, we parsed these sentences using the Stanford pars"
2020.figlang-1.22,D10-1004,0,0.0110796,"parser (Briscoe et al., 2006) was utilised to extract verb-subject and verb-direct object grammar relations from the British National Corpus (BNC) (Burnard, 2007). The dataset comprises around 62 verb-noun pairs of metaphoric expressions, where the verb is used metaphorically given the complement noun (tenor). The TroFi dataset, which was designed to classify particular literal and metaphoric verbs on the word-level, was adapted by Tsvetkov et al. (2014) in order to extract metaphoric expressions on the relation-level. The authors parsed the original dataset using the Turbo dependency parser (Martins et al., 2010) to extract subject-verb-object (SVO) grammar relations. The final dataset consists of 953 metaphorical and 656 literal instances. In the same work, Tsvetkov et al. also prepared a relationlevel metaphor identification dataset, referred to as the TSV dataset, focusing on adjective-noun grammar relations. We will further describe this dataset in Section 4. More recently, Shutova et al. (2016) adapted the benchmark MOH dataset, which was initially Datasets As mentioned in Section 2, the widely used benchmark datasets for word-level metaphor identification are TroFi, VUAMC and MOH datasets, while"
2020.figlang-1.22,S16-2003,0,0.0668767,"Missing"
2020.figlang-1.22,L18-1243,0,0.0110519,"comprehension of metaphors. Thus, processing metaphors on the word-level could be seen as a more general approach where the tenor of the metaphor is not explicitly highlighted as well as the relation between the source and the target domains. On the other hand, relation-level metaphor identification explicitly analyses the tenor and the relation between The levels of processing metaphors should be taken into consideration when designing and developing a computational model to identify metaphors and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor ident"
2020.figlang-1.22,D17-1162,0,0.0117747,"metaphoric verbs on the wordlevel, to suit relation-level metaphor identification of verb-noun relations. Verb-direct object and verbs-subject dependencies were extracted and filtered yielding a dataset of 647 verb–noun pairs, out of which 316 instances are metaphorical and 331 instances are literal. To the best of our knowledge, there is no attempt to adapt the benchmark VU Amsterdam metaphor corpus, referred to as VUAMC, to suit relation-level metaphor identification. This has discouraged other researchers focusing on relationlevel approaches to employ this dataset such as the work done by Rei et al. (2017), Bulat et al. (2017), Shutova et al. (2016) and Tsvetkov et al. (2014) who did not evaluate or compare their approaches using this dataset. In this paper, we introduce the first adapted version of the VUAMC. Furthermore, we adapt the TroFi and the TSV datasets to better suit relation-level metaphor processing. Related Work 4 This work is inspired by Tsvetkov et al. (2014) and Shutova et al. (2016) who attempted to adapt existing word-level metaphor identification datasets to suit their relation-level (phrase-level) identification approaches. Shutova et al. (2010) was the first to create an an"
2020.figlang-1.22,N16-1020,0,0.253398,"rs and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor identification. Stanford CoreNLP is used to generate the dependencies. Zayed’s Tweets (Zayed et al., 2019) datasets are utilised for relation-level metaphor identification. Approaches addressing the task on the wordlevel are not fairly comparable to relation-level approaches since each task deals with metaphor identification differently. Therefore, given the distinction of the tasks definition, the tradition of previous work in this area is to compare the wordlevel metaphor identification appro"
2020.figlang-1.22,C10-1113,0,0.0966965,"Missing"
2020.figlang-1.22,W18-0903,0,0.0112705,"ric or literal given the context. Many approaches are designed to identify metaphors of different syntactic types on the word-level but the most frequently studied ones are verbs. In this paper, we are interested in relation-level metaphor identification focusing on the data availability for this level of processing. The next section explains, in detail, the difference between wordlevel and relation-level metaphor analysis highlighting the research gap that we aim to tackle. 2 the source and the target domains. Figure 1 illustrates the difference between the levels of metaphor identification. Stowe and Palmer (2018) highlighted the importance of integrating syntax and semantics to process metaphors in text. Through a corpus-based analysis focusing on verb metaphors, the authors showed that the type of syntactic construction (dependency/grammar relation) a verb occurs in influences its metaphoricity. Relation-level metaphor processing requires an extra step to identify the grammatical relations (i.e. dependencies) that highlight both the tenor and the vehicle. Thus, it might be seen that processing metaphors on the word-level is more straight forward and raises the question: why do we need relation-level"
2020.figlang-1.22,P14-1024,0,0.21611,"eloping a computational model to identify metaphors and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor identification. Stanford CoreNLP is used to generate the dependencies. Zayed’s Tweets (Zayed et al., 2019) datasets are utilised for relation-level metaphor identification. Approaches addressing the task on the wordlevel are not fairly comparable to relation-level approaches since each task deals with metaphor identification differently. Therefore, given the distinction of the tasks definition, the tradition of previous work in this area is to com"
2020.figlang-1.22,W17-5035,0,0.0235765,"ressed in our everyday language in terms of linguistic metaphors such as “shattered my emotions”,“break his soul”,“crushed her happiness”, “fragile emotions” and “brittle feelings”. Due to their nebulous nature, metaphors are quite challenging to comprehend and process by humans, let alone computational models. This intrigued many researchers to develop various automatic techniques to process metaphor in text. Metaphor processing has many potential applications, either as part of natural language processing (NLP) tasks such as machine translation (Koglin and Cunha, 2019), text simplification (Wolska and Clausen, 2017; Clausen and Nastase, 2019) and sentiment analysis (Rentoumi et al., 2012) or in more general discourse analysis use cases such as in analysing political discourse (Charteris-Black, 2011), financial reporting (Ho and Cheng, 2016) and health communication (Semino et al., 2018). The computational processing of metaphors can be divided into two tasks, namely metaphor identification and its interpretation. While the former is concerned with recognising the metaphoric word or expression in a given sentence, the latter focuses on discerning the meaning of the metaphor. Metaphor identification is st"
2020.findings-emnlp.36,W13-0901,0,0.0148166,"uestion-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models"
2020.findings-emnlp.36,J91-1003,0,0.827084,"ation in a novel way to condition the neural network computation on the contextualised features of the given expression. The idea of affine transformations has been used in NLP-related tasks such as visual question-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzal"
2020.findings-emnlp.36,D18-1060,0,0.0842365,"ere a traditional fully-connected feed-forward neural network is trained using pre-trained word embeddings. The authors highlighted the limitation of this approach when dealing with short and noisy conversational texts. As part of the NAACL 2018 Metaphor Shared Task (Leong et al., 2018), many researchers proposed neural models that mainly employ LSTMs (Hochreiter and Schmidhuber, 1997) with pre-trained word embeddings to identify metaphors on the word-level. The best performing systems are: THU NGN (Wu et al., 2018), OCOTA (Bizzoni and Ghanimifard, 2018) and bot.zen (Stemle and Onysko, 2018). Gao et al. (2018) were the first to employ the deep contextualised word representation ELMo (Peters et al., 2018), combined with pre-trained GloVe (Pennington et al., 2014) embeddings to train bidirectional LSTM-based models. The authors introduced a sequence labelling model and a single-word classification model for verbs. They showed that incorporating the context-dependent representation of ELMo with context-independent word embeddings improved metaphor identification. Mu et al. (2019) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attem"
2020.findings-emnlp.36,W15-1403,0,0.0459358,"resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word and relation levels. Word-"
2020.findings-emnlp.36,W06-3506,0,0.0721933,"n used in NLP-related tasks such as visual question-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards em"
2020.findings-emnlp.36,W13-0908,0,0.0160554,"rom rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word and relation levels. Word-Level Processing: Do Dinh and Gurevych (2016) were the first to utilise a neural architecture to identify metaphors. They approached the problem as sequence labelling where a traditional fully-connected feed-forward neural netw"
2020.findings-emnlp.36,P82-1020,0,0.740393,"Missing"
2020.findings-emnlp.36,P18-1113,0,0.217732,"Missing"
2020.findings-emnlp.36,W13-0907,0,0.0169984,"17), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses"
2020.findings-emnlp.36,W15-4650,0,0.277476,"2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word"
2020.findings-emnlp.36,W14-2302,0,0.0189081,"ng, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural mode"
2020.findings-emnlp.36,P19-1378,0,0.0806395,"9) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attempt to exploit wider context to improve metaphor detection in addition to other word representations including GLoVe, ELMo and skip-thought (Kiros et al., 2015). Mao et al. (2018, 2019) explored the idea of selectional preferences violation (Wilks, 1978) in a neural architecture to identify metaphoric words. Mao’s proposed approaches emphasised the importance of the context to identify metaphoricity by employing context-dependent and context-independent word embeddings. Mao et al. (2019) also proposed employing multi-head attention to compare the targeted word representation with its context. An interesting approach was introduced by Dankers et al. (2019) to model the interplay between metaphor identification and emotion regression. The authors introduced multiple multi-task learning techniques that employ hard and soft parameter sharing methods to optimise LSTM-based and BERT-based models. Relation-Level Processing: Shutova et al. (2016) focused on identifying the metaphoricity of adjective/verb-noun pairs. This work employed multimodal embeddings of visual and linguistic fe"
2020.findings-emnlp.36,P19-1385,0,0.0179672,"ne transformations. In order to integrate the interaction of the metaphor components in the identification process, we utilise affine transformation in a novel way to condition the neural network computation on the contextualised features of the given expression. The idea of affine transformations has been used in NLP-related tasks such as visual question-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and S"
2020.findings-emnlp.36,S16-2003,0,0.391371,"Missing"
2020.findings-emnlp.36,W13-0904,0,0.274685,"sing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research t"
2020.findings-emnlp.36,N19-1059,0,0.522106,"rming systems are: THU NGN (Wu et al., 2018), OCOTA (Bizzoni and Ghanimifard, 2018) and bot.zen (Stemle and Onysko, 2018). Gao et al. (2018) were the first to employ the deep contextualised word representation ELMo (Peters et al., 2018), combined with pre-trained GloVe (Pennington et al., 2014) embeddings to train bidirectional LSTM-based models. The authors introduced a sequence labelling model and a single-word classification model for verbs. They showed that incorporating the context-dependent representation of ELMo with context-independent word embeddings improved metaphor identification. Mu et al. (2019) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attempt to exploit wider context to improve metaphor detection in addition to other word representations including GLoVe, ELMo and skip-thought (Kiros et al., 2015). Mao et al. (2018, 2019) explored the idea of selectional preferences violation (Wilks, 1978) in a neural architecture to identify metaphoric words. Mao’s proposed approaches emphasised the importance of the context to identify metaphoricity by employing context-dependent and context-independent word embeddings. Ma"
2020.findings-emnlp.36,D14-1162,0,0.0823574,"Missing"
2020.findings-emnlp.36,N18-1202,0,0.0374712,"rd embeddings. The authors highlighted the limitation of this approach when dealing with short and noisy conversational texts. As part of the NAACL 2018 Metaphor Shared Task (Leong et al., 2018), many researchers proposed neural models that mainly employ LSTMs (Hochreiter and Schmidhuber, 1997) with pre-trained word embeddings to identify metaphors on the word-level. The best performing systems are: THU NGN (Wu et al., 2018), OCOTA (Bizzoni and Ghanimifard, 2018) and bot.zen (Stemle and Onysko, 2018). Gao et al. (2018) were the first to employ the deep contextualised word representation ELMo (Peters et al., 2018), combined with pre-trained GloVe (Pennington et al., 2014) embeddings to train bidirectional LSTM-based models. The authors introduced a sequence labelling model and a single-word classification model for verbs. They showed that incorporating the context-dependent representation of ELMo with context-independent word embeddings improved metaphor identification. Mu et al. (2019) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attempt to exploit wider context to improve metaphor detection in addition to other word representati"
2020.findings-emnlp.36,W16-1103,0,0.029423,"Missing"
2020.findings-emnlp.36,D17-1162,0,0.10501,"del the interplay between metaphor identification and emotion regression. The authors introduced multiple multi-task learning techniques that employ hard and soft parameter sharing methods to optimise LSTM-based and BERT-based models. Relation-Level Processing: Shutova et al. (2016) focused on identifying the metaphoricity of adjective/verb-noun pairs. This work employed multimodal embeddings of visual and linguistic features. Their model employs the cosine similarity of the candidate expression components based on word embeddings to classify metaphors using an optimised similarity threshold. Rei et al. (2017) introduced a supervised similarity network to detect adjective/verb-noun metaphoric expressions. Their system utilises word gating, vector representation mapping and a weighted similarity function. Pre-trained word embeddings and attribute-based embeddings (Bulat et al., 2017) were employed as features. This work explicitly models the interaction between the metaphor components. Gating is used to modify the vector of the verb/adjective based on the noun, however the surrounding context is ignored by feeding only the candidates as input to the neural model which might lead to loosing important"
2020.findings-emnlp.36,N16-1020,0,0.128659,"ches emphasised the importance of the context to identify metaphoricity by employing context-dependent and context-independent word embeddings. Mao et al. (2019) also proposed employing multi-head attention to compare the targeted word representation with its context. An interesting approach was introduced by Dankers et al. (2019) to model the interplay between metaphor identification and emotion regression. The authors introduced multiple multi-task learning techniques that employ hard and soft parameter sharing methods to optimise LSTM-based and BERT-based models. Relation-Level Processing: Shutova et al. (2016) focused on identifying the metaphoricity of adjective/verb-noun pairs. This work employed multimodal embeddings of visual and linguistic features. Their model employs the cosine similarity of the candidate expression components based on word embeddings to classify metaphors using an optimised similarity threshold. Rei et al. (2017) introduced a supervised similarity network to detect adjective/verb-noun metaphoric expressions. Their system utilises word gating, vector representation mapping and a weighted similarity function. Pre-trained word embeddings and attribute-based embeddings (Bulat e"
2020.findings-emnlp.36,N13-1118,0,0.238925,"ification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word and relation levels. Word-Level Processing: Do Dinh and Gurevych (2016) were the first to utilise a neural architecture to identify metaphors. They approached the problem as sequence labelling where a traditional fully-connected fee"
2020.findings-emnlp.36,C10-1113,0,0.0827264,"Missing"
2020.findings-emnlp.36,shutova-teufel-2010-metaphor,0,0.210902,"l expression, whereas in word-level identification only the source domain words (vehicle) are labelled. These levels of analysis (paradigms) are already established in literature and adopted by previous research in this area as will be explained in Section 2. The majority of existing approaches, as well as the available datasets, pertaining to metaphor processing focus on the metaphorical usage of verbs and adjectives either on the word or relation levels. This is because these syntactic types exhibit metaphoricity more frequently than others according to corpus-based analysis (Cameron, 2003; Shutova and Teufel, 2010). Inspired by the works on visual reasoning, we use the candidate expression of certain grammatical relations, represented by deep contextualised features, as an auxiliary input to modulate our computational model. Affine transformations can be utilised to process one source of information in the context of another. In our case, we want to integrate: 1) the deep contextualised-features of the candidate expression (represented by ELMo sentence embeddings) with 2) the syntactic/semantic features of a given sentence. Based on this task, affine transformations have a similar role to attention but"
2020.lrec-1.254,C14-1029,0,0.0381497,"Missing"
2020.lrec-1.254,S14-2004,0,0.605952,"sly carried out, and a project with the Insight Centre for Data Analytics2 (Galway, Ireland) was established to analyse and provide findings in the comments written in natural language from the open-ended questions of the 2017 and 2018 editions of the survey. From this work resulted several outcomes, among which the creation of a framework specific to the field of health and hospital care for classifying the different complaints and compliment into categories. Those categories are divided into three types following the Activity, Resource and Context (ARC) methodology defined in Ordenes et al. (2014) (see section 2.1. for more details on the approach). From the project was also 1 2 https://yourexperience.ie/about/who-we-are/ https://www.insight-centre.org/ produced a dataset of comments manually annotated with the ARC framework categories mentioned above. The contribution provided in this paper is twofold. We first introduce an intermediate level term extraction approach to patient experience analysis, with the claim that this level of granularity can help in identifying important and particular aspects of complaints and compliments, and make them more easily actionable than typical broad"
2020.lrec-1.254,W19-5118,0,0.0203481,"Missing"
2020.lrec-1.254,zhang-etal-2008-comparative,0,0.527649,"The second step makes use of scoring functions to calculate the relatedness between the noun phrase and the domain. Several functions following different approaches are available to choose from in Saffron. Some of them rely essentially on occurrence frequency, some on reference corpora, etc. (see (Astrakhantsev, 2018) for a detailed review of different types of scoring). Ranking and Filtering: These scores obtained are then used to rank the candidate terms by relevance. An individual scoring function can be chosen, or a few of them combined with a voting algorithm approach to aggregate them (Zhang et al., 2008). A threshold can be specified by selecting top N terms, or terms observing a minimum scoring value, to obtain the final list of terms selected for the task ordered from the most relevant to the least relevant for the domain of the dataset. 4.2.2. Settings In this study, we are looking for intermediate level terms which, as opposed to high level terms, capture more precise facets on the complaints and compliments and help to provide a deeper analysis. With this in mind, we defined the following settings for our experiment. We allow terms of minimum two and maximum five words, as terms that are"
2020.lrec-1.285,S15-2151,1,0.901758,"Missing"
2020.lrec-1.285,S16-1168,1,0.908624,"namely the extraction of food-drug and herb-drug interactions. Keywords: Taxonomy Extraction, Knowledge Graphs, Knowledge Graph Pruning 1. Introduction Taxonomies are useful tools for content organisation, navigation, and information retrieval, providing valuable input for semantically intensive tasks such as question answering (Ojokoh and Adebisi, 2019) and textual entailment (Fawei et al., 2019). Recent efforts on setting common grounds for taxonomy extraction evaluation in the context of SemEval have focused on extracting hierarchical relations from unstructured text (Bordea et al., 2015; Bordea et al., 2016; Jurgens and Pilehvar, 2016). In the past few years, this task attracted an increasing amount of interest from the research community. But the relatively low evaluation results show that this is still a challenging task, despite efforts to simplify the task by focusing on the more simple subtask of hypernym relations extraction rather than constructing a full fledged taxonomy and by providing datasets that mainly cover concepts with a relatively high frequency in the target corpus (Camacho-Collados et al., 2018). Instead of extracting hierarchical relations from text alone, in this work, we a"
2020.lrec-1.285,W19-5013,1,0.826035,"ug interactions from scientific articles. Foods and medicinal plants, for example grapefruit juice and St. John’s Wort, potentially cause clinically-significant drug interactions in a similar way that combining drugs can lead to undesired drug-drug interactions. In this context, the goal is to automatically extract application-specific taxonomies from the Wikipedia category hierarchy for concepts that describe foods, plants and drugs in relation to diseases, signs, symptoms and medical specialties. Identifying these hierarchies is useful for constructing relevant corpora (Bordea et al., 2018; Bordea et al., 2019), entity extraction, and for drug interaction organisation and visualisation. But selecting a taxonomy of relevant Wikipedia categories is not trivial for this application because many of these concepts have several uses outside of our application domain. For instance, exploring Category:Foods, we discover Category:Ducks that points to Category:Fictional Ducks, including Donald Duck with all the films, television series and video games that are related, but clearly out of scope for our application. Similarly, Category:Blood sausages is linked to Category:Vampirism through the category Category"
2020.lrec-1.285,S18-1115,0,0.0177312,"ve focused on extracting hierarchical relations from unstructured text (Bordea et al., 2015; Bordea et al., 2016; Jurgens and Pilehvar, 2016). In the past few years, this task attracted an increasing amount of interest from the research community. But the relatively low evaluation results show that this is still a challenging task, despite efforts to simplify the task by focusing on the more simple subtask of hypernym relations extraction rather than constructing a full fledged taxonomy and by providing datasets that mainly cover concepts with a relatively high frequency in the target corpus (Camacho-Collados et al., 2018). Instead of extracting hierarchical relations from text alone, in this work, we address the problem of adapting and re-using existing taxonomical structures from general-purpose knowledge graphs such as the Wikipedia knowledge graph (Kapanipathi et al., 2014). The sheer vastness of Wikipedia’s domain coverage recommends it as a reliable source of semi-structured information. Manually-curated and high-quality information about hierarchical relations is readily available for a wide range of domains. Hence, the taxonomy extraction task should be made more feasible provided that there are methods"
2020.lrec-1.285,S16-1169,0,0.0194974,"n of food-drug and herb-drug interactions. Keywords: Taxonomy Extraction, Knowledge Graphs, Knowledge Graph Pruning 1. Introduction Taxonomies are useful tools for content organisation, navigation, and information retrieval, providing valuable input for semantically intensive tasks such as question answering (Ojokoh and Adebisi, 2019) and textual entailment (Fawei et al., 2019). Recent efforts on setting common grounds for taxonomy extraction evaluation in the context of SemEval have focused on extracting hierarchical relations from unstructured text (Bordea et al., 2015; Bordea et al., 2016; Jurgens and Pilehvar, 2016). In the past few years, this task attracted an increasing amount of interest from the research community. But the relatively low evaluation results show that this is still a challenging task, despite efforts to simplify the task by focusing on the more simple subtask of hypernym relations extraction rather than constructing a full fledged taxonomy and by providing datasets that mainly cover concepts with a relatively high frequency in the target corpus (Camacho-Collados et al., 2018). Instead of extracting hierarchical relations from text alone, in this work, we address the problem of adaptin"
2020.lrec-1.285,velardi-etal-2012-new,1,0.85696,"Missing"
2020.lrec-1.712,C18-1021,0,0.0135773,"ession is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work, we view metaphor interpretation as a definition generation (explanation) task focusing on finding out the meaning of a given metaphoric expression and explain it in literal words. There are a variety of applications that can benefit from interpreting metaphors, including language learning and text simplification (Barbu et al., 2015; Wolska and Clausen, 2017; Bingel et al., 2018) as well as lexical resources creation and development (Krek et al., 2018). Approach lexical substitution (Shutova et al., 2010) Metaphor brush aside accusation Interpretation reject paraphrase generation (Bizzoni and Lappin, 2018) The crowd was a river in the street. The crowd was large and impetuous in the street. definition generation (Martin, 1990) How do I kill the process? to terminate computer process. Table 1: Metaphor interpretation approaches with examples from previous studies. Manually annotating a dataset for metaphor interpretation (either to provide a definition/explanation or t"
2020.lrec-1.712,W18-0906,0,0.0616811,"le datasets for metaphor interpretation, which in turn hinders the development of this topic. There are several approaches to address metaphor interpretation among which: 1. Lexical Substitution (lexical paraphrasing) where the metaphoric word/phrase is replaced with its literal counterpart to clarify its semantic meaning. This task is viewed as single-word (lexical) substitution (Shutova, 2010; Shutova et al., 2012; Bollegala and Shutova, 2013); 2. Paraphrase Generation (inference of meaning) where the full sentence including the metaphoric expression is transformed using more literal words (Bizzoni and Lappin, 2018); 3. Definition Generation (interpretation or definition assignment) where a full interpretation (explanation) of the metaphoric expression is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work, we view metaphor interpretation as a definition generation (explanation) task focusing on finding out the meaning of a given metaphoric expression and explain it in literal words. There are a variety of applications that can ben"
2020.lrec-1.712,S16-2003,0,0.0556793,"Missing"
2020.lrec-1.712,D14-1162,0,0.0833958,"Missing"
2020.lrec-1.712,shutova-teufel-2010-metaphor,0,0.0234301,"the automatically generated list of candidates. External knowledge resources including machine readable dictionaries and lexicons have been widely used in WSD (Ide and V´eronis, 1993; Agirre and Stevenson, 2007; Navigli, 2009). We will discuss the criteria of choosing the resources utilised in this work in Section 3.. Linguistic metaphors can be expressed in various syntactic structures. The majority of previous work focused on modelling verbal and adjectival metaphoric expression (Shutova, 2015). Corpus studies showed that verbs are the most frequent metaphorical expressions (Cameron, 2003; Shutova and Teufel, 2010) which encouraged the majority of systems pertained to metaphor processing to focus on the metaphorical usage of verbs. Thus, in this work, we focus on verb-direct object metaphoric expressions. We create our dataset of metaphor definitions by interpreting around 1,500 metaphoric expression identified in an existing tweets dataset (Zayed et al., 2019) and providing their literal meaning. To the best of our knowledge, there is no publicly available annotated dataset of this kind and we believe that this resource will be invaluable for the development and evaluation of computational models for m"
2020.lrec-1.712,C12-2109,0,0.064216,"Missing"
2020.lrec-1.712,N16-1020,0,0.194735,"Missing"
2020.lrec-1.712,N10-1147,0,0.108488,"onsciously grasp such interaction, asking a human annotator to translate such a cognitive process and interpret a metaphoric expression is a very demanding task. This is the reason behind the lack of publicly available datasets for metaphor interpretation, which in turn hinders the development of this topic. There are several approaches to address metaphor interpretation among which: 1. Lexical Substitution (lexical paraphrasing) where the metaphoric word/phrase is replaced with its literal counterpart to clarify its semantic meaning. This task is viewed as single-word (lexical) substitution (Shutova, 2010; Shutova et al., 2012; Bollegala and Shutova, 2013); 2. Paraphrase Generation (inference of meaning) where the full sentence including the metaphoric expression is transformed using more literal words (Bizzoni and Lappin, 2018); 3. Definition Generation (interpretation or definition assignment) where a full interpretation (explanation) of the metaphoric expression is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work,"
2020.lrec-1.712,J15-4002,0,0.0231285,"hat given a metaphoric verb the goal (of the human annotator) is to identify its closest (literal) meaning among the automatically generated list of candidates. External knowledge resources including machine readable dictionaries and lexicons have been widely used in WSD (Ide and V´eronis, 1993; Agirre and Stevenson, 2007; Navigli, 2009). We will discuss the criteria of choosing the resources utilised in this work in Section 3.. Linguistic metaphors can be expressed in various syntactic structures. The majority of previous work focused on modelling verbal and adjectival metaphoric expression (Shutova, 2015). Corpus studies showed that verbs are the most frequent metaphorical expressions (Cameron, 2003; Shutova and Teufel, 2010) which encouraged the majority of systems pertained to metaphor processing to focus on the metaphorical usage of verbs. Thus, in this work, we focus on verb-direct object metaphoric expressions. We create our dataset of metaphor definitions by interpreting around 1,500 metaphoric expression identified in an existing tweets dataset (Zayed et al., 2019) and providing their literal meaning. To the best of our knowledge, there is no publicly available annotated dataset of this"
2020.lrec-1.712,W17-5035,0,0.0220775,"on) of the metaphoric expression is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work, we view metaphor interpretation as a definition generation (explanation) task focusing on finding out the meaning of a given metaphoric expression and explain it in literal words. There are a variety of applications that can benefit from interpreting metaphors, including language learning and text simplification (Barbu et al., 2015; Wolska and Clausen, 2017; Bingel et al., 2018) as well as lexical resources creation and development (Krek et al., 2018). Approach lexical substitution (Shutova et al., 2010) Metaphor brush aside accusation Interpretation reject paraphrase generation (Bizzoni and Lappin, 2018) The crowd was a river in the street. The crowd was large and impetuous in the street. definition generation (Martin, 1990) How do I kill the process? to terminate computer process. Table 1: Metaphor interpretation approaches with examples from previous studies. Manually annotating a dataset for metaphor interpretation (either to provide a defin"
2020.semeval-1.208,Q17-1010,0,0.0160571,"Missing"
2020.semeval-1.208,2020.sltu-1.25,1,0.784813,"tatseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average functional words count per tweet Average Hashtags per tweet Stats 9,075,418 15.64 5.61 0.08 Table 1: Data statistics of the OffensEval 2020 dataset survey different types of features have been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than tra"
2020.semeval-1.208,2020.sltu-1.28,0,0.0231748,"tatseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average functional words count per tweet Average Hashtags per tweet Stats 9,075,418 15.64 5.61 0.08 Table 1: Data statistics of the OffensEval 2020 dataset survey different types of features have been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than tra"
2020.semeval-1.208,N19-1423,0,0.0118949,"e better at text classification. Rhanoui et al. (2019) has designed such an approach for sentiment analysis. In their research, they combined multiple outputs of a convolutional filter to form a vector representation of text, which was then fed to a BiLSTM. Dynamic meta-embedding (DME) and Contextualised DME (CDME) introduced by Kiela et al. (2018) has shown significant improvement in a variety of natural language processing tasks such as natural language inference, sentiment classification and image-caption retrieval. Pre-trained Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019) based models (BERT) have performed exceptionally well in many natural language processing tasks such as text classification, natural language generation and machine translation. We designed our experiments based on CNN + BiLSTM, CDME, DME and BERT. 3 Methodology In this section, we are giving insights about the methodology followed to process and pseudo-label the data in Subsection 3.1 and Subsection 3.2 respectively. 3.1 Data Statistics and Pre-processing Steps The data statistics in Table 1 show that on average, a high number of functional words3 per tweet are present in the dataset. We dec"
2020.semeval-1.208,S19-2107,0,0.0132446,"been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an imbalanced dataset by using models with Synthetic Minority Over-sampling technique (SMOTE). Singh and Chand (2019) uses sequence to sequence models combined with long short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) cla"
2020.semeval-1.208,D18-1176,0,0.0142837,"short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) class. Hybrid approaches which combine a recurrent neural network (RNN) and a CNN have been proven to be better at text classification. Rhanoui et al. (2019) has designed such an approach for sentiment analysis. In their research, they combined multiple outputs of a convolutional filter to form a vector representation of text, which was then fed to a BiLSTM. Dynamic meta-embedding (DME) and Contextualised DME (CDME) introduced by Kiela et al. (2018) has shown significant improvement in a variety of natural language processing tasks such as natural language inference, sentiment classification and image-caption retrieval. Pre-trained Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019) based models (BERT) have performed exceptionally well in many natural language processing tasks such as text classification, natural language generation and machine translation. We designed our experiments based on CNN + BiLSTM, CDME, DME and BERT. 3 Methodology In this section, we are giving insights about the methodology foll"
2020.semeval-1.208,D14-1162,0,0.0834896,"OLID dataset to select the classifier which can be used to label the OffensEval 2020 dataset. CNN+BiLSTM: In this architecture, we extracted the abstract features from the text vector using a one dimensional (1D) convolution network and 1D maxpooling, which later has been fed to the BiLSTM to form a vector representation of the tweet. This vectorised text represents the word with its long and short term context in a vector space. Abstract and prominent features captured by the CNN have been contextualised with time steps with the RNN. We used pre-trained GloVe 50d Twitter crawled embeddings (Pennington et al., 2014). 1600 DME and CDME: We used DME and CDME as an ensemble of embeddings to study if it gives better results with the OLID dataset. This architecture takes advantage of both Word2vec (Mikolov et al., 2013) and FastText (Bojanowski et al., 2017) embeddings to learn the vector representation of the word. The Word2vec and FastText embedding of the same words, later on, are projected on the embedding matrix space. These projected vectors are later concatenated. Weights of each embedding have been learned as a hyperparameter using the self-attention model. Unlike DME, CDME has BiLSTM incorporated in"
2020.semeval-1.208,S19-2091,0,0.0286542,"Missing"
2020.semeval-1.208,S19-2136,0,0.0156389,"0), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an imbalanced dataset by using models with Synthetic Minority Over-sampling technique (SMOTE). Singh and Chand (2019) uses sequence to sequence models combined with long short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) class. Hybrid approaches which combine a recurrent neural network (RNN) and a CNN have been proven to be better at text classification. Rhanoui et al. (2019) has designed such an approach for senti"
2020.semeval-1.208,2020.trac-1.7,1,0.75126,"its label prediction performance (offensive OFF, not-offensive NOT) and not on these scores, it was up to the participant to decide how to best map the scores to labels. We trained different text classifiers on the OLID (Zampieri et al., 2019a) dataset to predict the labels of the OffensEval 2020. This pseudo-labelling approach avoids the efforts involved in the manual adjustment of threshold values of average confidence (Avg conf ) and average standard deviation (Avg std). 2 Related work There has been significant research on aggression detection, hate speech detection (Ranjan et al., 2016; Rani et al., 2020; Jose et al., 2020) and cyberbullying (Schmidt and Wiegand, 2017). Based on this This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-statseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The"
2020.semeval-1.208,W17-1101,0,0.0277812,"ensive NOT) and not on these scores, it was up to the participant to decide how to best map the scores to labels. We trained different text classifiers on the OLID (Zampieri et al., 2019a) dataset to predict the labels of the OffensEval 2020. This pseudo-labelling approach avoids the efforts involved in the manual adjustment of threshold values of average confidence (Avg conf ) and average standard deviation (Avg std). 2 Related work There has been significant research on aggression detection, hate speech detection (Ranjan et al., 2016; Rani et al., 2020; Jose et al., 2020) and cyberbullying (Schmidt and Wiegand, 2017). Based on this This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-statseveryone-should-read/#75d1769a60ba 2 https://dictionary.cambridge.org/us/dictionary/english/hate-speech 1598 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average fun"
2020.semeval-1.208,S19-2128,0,0.0104818,"support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an imbalanced dataset by using models with Synthetic Minority Over-sampling technique (SMOTE). Singh and Chand (2019) uses sequence to sequence models combined with long short term memory (LSTM) network, gated recurrent unit (GRU) and Bidirectional LSTM (BiLSTM) to classify a given tweet into an offensive (OFF) or not-offensive (NOT) class. Hybrid approaches which combine a recurrent neural network (RNN) and a CNN have been proven to be better at text classification. Rhanoui et al. (2019) has designed such an approach for sentiment analysis. In their research, they combined multiple outputs of a convolutional filter to form a vector representation of text, which was then fed to a BiLSTM. Dynamic meta-embeddi"
2020.semeval-1.208,2020.trac-1.6,1,0.769081,"Evaluation, pages 1598–1604 Barcelona, Spain (Online), December 12, 2020. Parameters The number of tweets/samples Average word count per tweet Average functional words count per tweet Average Hashtags per tweet Stats 9,075,418 15.64 5.61 0.08 Table 1: Data statistics of the OffensEval 2020 dataset survey different types of features have been employed by previous works including surface, word generalization, sentiment-based (Chakravarthi et al., 2020a; Chakravarthi et al., 2020b), lexical, codemixed (Priyadharshini et al., 2020), linguistic, knowledge-based and multimodal information features (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) as well. Traditional machine learning (ML) approaches such as support vector machines (SVM) by Perell´o et al. (2019) can be trained on hate speech tweets by identifying n-grams features which could be improved further by combining word embedding with sentiment features. Research by Kebriaei et al. (2019) shows how a convolutional neural network (CNN) shows higher macro averaged F1-score than traditional ML approaches such as SVM, random forest (RF) and naive Bayes (NB). (Rajendran et al., 2019) uses an ensemble of classifiers to classify the offensive text in an"
2020.semeval-1.208,N19-1144,0,0.0360412,"Missing"
2020.semeval-1.208,S19-2010,0,0.0417452,"Missing"
2020.trac-1.6,W18-4411,0,0.0114961,"for offensive content detection, consisting of 743 memes which are annotated with an offensive or not-offensive label. II We used this dataset to implement a multimodal offensive content classifier for memes. III We addressed issues associated with multimodal classification and data collection for memes. 2. Offensive Content Offensive content intends to upset or embarrasses people by being rude or insulting (Drakett et al., 2018). Past work on offensive content detection focused on hate speech detection (Schmidt and Wiegand, 2017; Ranjan et al., 2016; Jose et al., 2020), aggression detection (Aroyehun and Gelbukh, 2018), trolling (Mojica de la Vega and Ng, 2018), and cyberbullying (Arroyo-Fern´andez et al., 2018). In the case of images, offensive content has been studied to detect nudity (Arentz and Olstad, 2004; Kakumanu et al., 2007; Tian et al., 2018), sexually explicit content, objects used to promote violence, and racially inappropriate content (Connie et al., 2018; Gandhi et al., 2019). https://www.lexico.com/en/definition/meme 32 features of the meme. 3. Related work The related section covers the work done in identifying offensive content in text and image. It also describes the research done in the"
2020.trac-1.6,W18-4417,0,0.0412797,"Missing"
2020.trac-1.6,W19-6809,1,0.680023,"ch as sadness, anger, disgust would be classified as negative. On the other hand, the sentences which hint happiness and surprise would be categorized in positive classes and the rest of the memes are treated as neutral. Their dataset is not publicly available. While our work is the first to create a dataset for the memes to detect offensive content using voluntary annotators. 3.5. MultiOFF Dataset 4.2. Data Collection and Annotation We constructed the MultiOFF dataset by manually annotating the data into either the offensive or non-offensive category. The annotators, which used Google Forms (Chakravarthi et al., 2019; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a), were given instructions to label if a given meme is offensive or non-offensive based on the image and text associated with it. The guidelines about the annotation task are as follows: I The reviewer must review the meme as shown in Figure 6a in two categories either offensive or Nonoffensive. Summary Most of the studies mentioned above focus on meme classification on a single modality. The ones that have been dealing with multimodal content rely on machine learning approaches that require handcrafted features derived from the data to c"
2020.trac-1.6,2020.sltu-1.25,1,0.820212,"st would be classified as negative. On the other hand, the sentences which hint happiness and surprise would be categorized in positive classes and the rest of the memes are treated as neutral. Their dataset is not publicly available. While our work is the first to create a dataset for the memes to detect offensive content using voluntary annotators. 3.5. MultiOFF Dataset 4.2. Data Collection and Annotation We constructed the MultiOFF dataset by manually annotating the data into either the offensive or non-offensive category. The annotators, which used Google Forms (Chakravarthi et al., 2019; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a), were given instructions to label if a given meme is offensive or non-offensive based on the image and text associated with it. The guidelines about the annotation task are as follows: I The reviewer must review the meme as shown in Figure 6a in two categories either offensive or Nonoffensive. Summary Most of the studies mentioned above focus on meme classification on a single modality. The ones that have been dealing with multimodal content rely on machine learning approaches that require handcrafted features derived from the data to classify the observations. I"
2020.trac-1.6,2020.sltu-1.28,1,0.771907,"st would be classified as negative. On the other hand, the sentences which hint happiness and surprise would be categorized in positive classes and the rest of the memes are treated as neutral. Their dataset is not publicly available. While our work is the first to create a dataset for the memes to detect offensive content using voluntary annotators. 3.5. MultiOFF Dataset 4.2. Data Collection and Annotation We constructed the MultiOFF dataset by manually annotating the data into either the offensive or non-offensive category. The annotators, which used Google Forms (Chakravarthi et al., 2019; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a), were given instructions to label if a given meme is offensive or non-offensive based on the image and text associated with it. The guidelines about the annotation task are as follows: I The reviewer must review the meme as shown in Figure 6a in two categories either offensive or Nonoffensive. Summary Most of the studies mentioned above focus on meme classification on a single modality. The ones that have been dealing with multimodal content rely on machine learning approaches that require handcrafted features derived from the data to classify the observations. I"
2020.trac-1.6,L18-1585,0,0.113528,"Missing"
2020.trac-1.6,D14-1162,0,0.0821,"Missing"
2020.trac-1.6,2020.trac-1.7,1,0.842073,"to understand the content from a single modality (He et al., 2016). Therefore, it is important to consider both modalities to understand the meaning or intention of the meme. Unfortunately, memes are responsible for spreading hatred in society, because of which there is a requirement to automatically identify memes with offensive content. But due to its multimodal nature, memes which often are the combination of text and image are difficult to regulate by automatic filtering. Offensive or abusive content on social media can be explicit or implicit (Waseem et al., 2017; Watanabe et al., 2018; Rani et al., 2020) and could be classified as explicitly offensive or abusive if it is unambiguously identified as such. As an example, it might contain racial, homophobic, or other offending slurs. In the case of implicit offensive or abusive content, the actual meaning is often obscured by the use of ambiguous terms, sarcasm, lack of profanity, hateful terms, or other means. As they fall under this criterion, memes can be categorized as implicit offensive content. Hence it is difficult to classify them as offensive for human annotators 1 I We created the MultiOFF dataset for offensive content detection, consi"
2020.trac-1.6,W17-1101,0,0.0330318,"lt to classify them as offensive for human annotators 1 I We created the MultiOFF dataset for offensive content detection, consisting of 743 memes which are annotated with an offensive or not-offensive label. II We used this dataset to implement a multimodal offensive content classifier for memes. III We addressed issues associated with multimodal classification and data collection for memes. 2. Offensive Content Offensive content intends to upset or embarrasses people by being rude or insulting (Drakett et al., 2018). Past work on offensive content detection focused on hate speech detection (Schmidt and Wiegand, 2017; Ranjan et al., 2016; Jose et al., 2020), aggression detection (Aroyehun and Gelbukh, 2018), trolling (Mojica de la Vega and Ng, 2018), and cyberbullying (Arroyo-Fern´andez et al., 2018). In the case of images, offensive content has been studied to detect nudity (Arentz and Olstad, 2004; Kakumanu et al., 2007; Tian et al., 2018), sexually explicit content, objects used to promote violence, and racially inappropriate content (Connie et al., 2018; Gandhi et al., 2019). https://www.lexico.com/en/definition/meme 32 features of the meme. 3. Related work The related section covers the work done in"
2020.trac-1.6,2020.semeval-1.99,0,0.131682,"Missing"
2020.trac-1.6,W12-2103,0,0.0836443,"ez et al., 2018). In the case of images, offensive content has been studied to detect nudity (Arentz and Olstad, 2004; Kakumanu et al., 2007; Tian et al., 2018), sexually explicit content, objects used to promote violence, and racially inappropriate content (Connie et al., 2018; Gandhi et al., 2019). https://www.lexico.com/en/definition/meme 32 features of the meme. 3. Related work The related section covers the work done in identifying offensive content in text and image. It also describes the research done in the area of meme analysis as well as multimodality. 3.1. Offensive Content in Text Warner and Hirschberg (2012) model offensive language by developing a Support Vector Machine (SVM) classifier, which takes in features manually derived from the text and classifies if the given text is abusive or not. Djuric et al. (2015) have used n-gram features to classify if the speech is abusive or not. There are many text-based datasets available for aggression identification (Watanabe et al., 2018), hate speech identification (Davidson et al., 2017) and Offensive language detection (Wiegand et al., 2018; Zampieri et al., 2019). Amongst the work mentioned, Watanabe et al. (2018) relies on unigrams and pattern of th"
2020.trac-1.6,W17-3012,0,0.0188561,"al nature of the meme, it is often difficult to understand the content from a single modality (He et al., 2016). Therefore, it is important to consider both modalities to understand the meaning or intention of the meme. Unfortunately, memes are responsible for spreading hatred in society, because of which there is a requirement to automatically identify memes with offensive content. But due to its multimodal nature, memes which often are the combination of text and image are difficult to regulate by automatic filtering. Offensive or abusive content on social media can be explicit or implicit (Waseem et al., 2017; Watanabe et al., 2018; Rani et al., 2020) and could be classified as explicitly offensive or abusive if it is unambiguously identified as such. As an example, it might contain racial, homophobic, or other offending slurs. In the case of implicit offensive or abusive content, the actual meaning is often obscured by the use of ambiguous terms, sarcasm, lack of profanity, hateful terms, or other means. As they fall under this criterion, memes can be categorized as implicit offensive content. Hence it is difficult to classify them as offensive for human annotators 1 I We created the MultiOFF dat"
2020.trac-1.6,S19-2010,0,0.0232044,"the area of meme analysis as well as multimodality. 3.1. Offensive Content in Text Warner and Hirschberg (2012) model offensive language by developing a Support Vector Machine (SVM) classifier, which takes in features manually derived from the text and classifies if the given text is abusive or not. Djuric et al. (2015) have used n-gram features to classify if the speech is abusive or not. There are many text-based datasets available for aggression identification (Watanabe et al., 2018), hate speech identification (Davidson et al., 2017) and Offensive language detection (Wiegand et al., 2018; Zampieri et al., 2019). Amongst the work mentioned, Watanabe et al. (2018) relies on unigrams and pattern of the text for detecting hate speech. These patterns are carefully crafted manually and then provided to machine learning models for further classification. Wiegand et al. (2018; Zampieri et al. (2019) deals with the classification of hateful tweets in the German language and addresses some of the issues in identifying offensive content. All this research puts more weight on features of single modality i.e. text and manual feature extraction. We work on memes which have more than one modality, i.e. image and t"
2020.webnlg-1.15,W14-3348,0,0.0312581,"sing the Adam optimizer with a learning rate of 0.001. 6 Results In this section, we report the results of our experiments on the validation set of the WebNLG+ corpus. Since at the time of writing, we do not have access to the official WebNLG+ reference lexicalisations in the test set, to evaluate performance on the unseen categories of data, we treat Artist, Athlete, CelestialBody, Company, MeanOfTransportation and Politician as unseen categories and 140 Table 2 shows the results of automatic evaluation in terms of three commonly used evaluation metrics, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). The LSTM and transformer baseline models achieve a BLEU score of 35.0 and 36.8 respectively across all categories of data and a score of about 54 on the seen categories. However, there is a significant drop in the performance on the unseen categories, which shows that end-toend trained systems do not generalise well to new and unseen domains of data. The results from finetuning the T5 model in Table 2 indicate that transfer learning is hugely beneficial in the context of RDFto-text generation as it achieves significant gains over the baselines right out of the b"
2020.webnlg-1.15,2020.webnlg-1.7,0,0.227841,"Missing"
2020.webnlg-1.15,W17-3518,0,0.147703,"e (Gatt and Krahmer, 2018). However, there has been a shift recently towards end-to-end architectures using neural networks to convert data in the input to text in a natural language in the output. In our submission, we employ an end-to-end approach using the T5 model architecture (Raffel et al., 2020) which is pre-trained on a large corpus of text scraped from the Web. We fine-tune the T5 model on the WebNLG+ corpus and explore 1 RDF - Resource Description Framework various pre-training and pre-processing strategies to improve the performance of our system. 2 Background The WebNLG challenge (Gardent et al., 2017) was created with the goal of producing a common benchmark to compare “microplanners”, i.e, generation systems that verbalise non-linguistic content to text in some human language. In 2017, the challenge received a mix of submissions built using template or grammar-based pipeline, statistical machine translation (SMT) and neural machine translation (NMT) frameworks. The test set used for final evaluation was split into two subsets, seen and unseen. The first subset contained data from the categories that were also present in the training set while the second included new data from unseen categ"
2020.webnlg-1.15,2020.inlg-1.14,0,0.018246,"NMT and SMT-based systems mostly outperformed the rule-based pipeline sytems in terms of BLEU and TER score. However, the scores for the NMT-based systems dropped significantly on the unseen categories while the rule-based systems were able to generalise better on the new and unseen domains. Further work by Castro Ferreira et al. (2019) compared pipeline-based and end-to-end architectures and their findings also suggest that the systems which are trained end-to-end are comparable to pipeline methods on the seen categories but do not generalise to new and unseen domains of data. More recently, Kale (2020) have shown that applying transfer learning using an end-to-end pretrained model such as T5 achieves state-of-the-art results on three benchmark datasets for data-totext generation and performs well even on out-ofdomain inputs in the unseen categories of data. The T5 model (Raffel et al., 2020) follows a transformer-based encoder-decoder architecture 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), c Dublin, Ireland (Virtual), 18 December 2020, pages 137–143, 2020 Association for Computational Linguistics Attribution 4.0 International. Tripleset 200 Pu"
2020.webnlg-1.15,P17-4012,0,0.02376,"F-triples given in the input. 139 5 Experimental Setup We adopt the WebNLG baseline system (Gardent et al., 2017) as one of the baseline architectures for our experiments, which is a vanilla sequence-tosequence LSTM model with attention (Bahdanau et al., 2015) where the RDF-triples in the input are linearised as a sequence and the output text is tokenised before training. We use another baseline based on the transformer architecture (Vaswani et al., 2017) similar to the end-to-end architecture setup by Castro Ferreira et al. (2019). These baseline models are trained using the OpenNMT library (Klein et al., 2017). We use the default parameters for two baseline models. Two hidden layers and 500 units per hidden layer with input feeding (Luong et al., 2015) enabled and word embeddings of size 500-dimensions are used for the LSTM neural model. Dropout is applied with value 0.3 and the LSTM model is trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-h"
2020.webnlg-1.15,D18-2012,0,0.0119781,"s not appear to be any significant improvements with the pre-trained model. The addition of &lt;SUB>, &lt;PRED> and &lt;OBJ> tags in the T5+tags model improves the BLEU score for unseen categories by more than 2 points from 37.4 to 39.5. However, for seen categories, there is a drop of about of 0.9. Information about entity types from DBpedia also appears to be useful for the unseen categories, improving the BLEU score from 37.4 to 38.9 for the T5+types model. However, it also leads to a performance drop by about 4 points for each metric in the case of seen categories. The T5 model uses SentencePiece (Kudo and Richardson, 2018) for subword tokenisation to handle unknown and rare tokens, such as the multi-word predicates in this corpus. However, Data Coverage Relevance Correctness Text Structure Fluency 92.892 (0.17) 92.066 (0.127) 92.063 (0.116) 95.442 (0.251) 93.784 (0.161) 92.588 (0.113) 94.061 (0.161) 94.392 (0.139) 91.794 (0.19) 90.138 (0.13) 92.053 (0.189) 94.149 (0.256) 87.4 (0.039) 85.737 (-0.064) 91.588 (0.258) 92.105 (0.254) 82.43 (0.011) 80.941 (-0.143) 88.898 (0.233) 89.846 (0.279) 95.296 (0.28) 90.253 (0.065) 91.253 (0.059) 95.491 (0.264) 94.568 (0.153) 89.568 (-0.043) 94.512 (0.178) 94.142 (0.135) 93.59"
2020.webnlg-1.15,D15-1166,0,0.0122312,"tures for our experiments, which is a vanilla sequence-tosequence LSTM model with attention (Bahdanau et al., 2015) where the RDF-triples in the input are linearised as a sequence and the output text is tokenised before training. We use another baseline based on the transformer architecture (Vaswani et al., 2017) similar to the end-to-end architecture setup by Castro Ferreira et al. (2019). These baseline models are trained using the OpenNMT library (Klein et al., 2017). We use the default parameters for two baseline models. Two hidden layers and 500 units per hidden layer with input feeding (Luong et al., 2015) enabled and word embeddings of size 500-dimensions are used for the LSTM neural model. Dropout is applied with value 0.3 and the LSTM model is trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with value 0.1 and the model is trained using Adam optimizer (Kingma and"
2020.webnlg-1.15,P02-1040,0,0.109128,"ctively with a batchsize of 32 using the Adam optimizer with a learning rate of 0.001. 6 Results In this section, we report the results of our experiments on the validation set of the WebNLG+ corpus. Since at the time of writing, we do not have access to the official WebNLG+ reference lexicalisations in the test set, to evaluate performance on the unseen categories of data, we treat Artist, Athlete, CelestialBody, Company, MeanOfTransportation and Politician as unseen categories and 140 Table 2 shows the results of automatic evaluation in terms of three commonly used evaluation metrics, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). The LSTM and transformer baseline models achieve a BLEU score of 35.0 and 36.8 respectively across all categories of data and a score of about 54 on the seen categories. However, there is a significant drop in the performance on the unseen categories, which shows that end-toend trained systems do not generalise well to new and unseen domains of data. The results from finetuning the T5 model in Table 2 indicate that transfer learning is hugely beneficial in the context of RDFto-text generation as it achieves significant gains o"
2020.webnlg-1.15,W17-4770,0,0.0903171,"Missing"
2020.webnlg-1.15,2020.acl-main.704,0,0.0407051,"with a pretraining strategy relevant for this task. For our final submission to the WebNLG+ challenge 2020, we train a “base” variant of the T5 model using data from the entire training set of the WebNLG+ corpus. Before fine-tuning the T5base model, we split the multi-word predicates and add &lt;SUB>, &lt;PRED> and &lt;OBJ> tags for subjects, predicates and objects respectively. Table 3 shows the automatic evaluation results for our submission using the GERBIL NLG framework (Moussalem et al., 2020) on the WebNLG+ test set in terms of chrf++ (Popovi´c, 2017), BERT score (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) along with BLEU, METEOR and TER scores. Our 141 system ranks among the top 5 for most of these evaluation metrics across all categories. In terms of BLEU score, our submission achieves scores of 58.26 for seen categories and 45.52 for the unseen categories. For the test set containing unseen entities, our system achieves the highest BLEU score of 52.76 and ranks among the top two for most of the automatic evaluation metrics. Table 4 shows results of human evaluation on the WebNLG+ test set for our submission along with two baselines and the reference lexicalisation. For the evaluation, human"
2020.webnlg-1.15,2006.amta-papers.25,0,0.0404603,"ing rate of 0.001. 6 Results In this section, we report the results of our experiments on the validation set of the WebNLG+ corpus. Since at the time of writing, we do not have access to the official WebNLG+ reference lexicalisations in the test set, to evaluate performance on the unseen categories of data, we treat Artist, Athlete, CelestialBody, Company, MeanOfTransportation and Politician as unseen categories and 140 Table 2 shows the results of automatic evaluation in terms of three commonly used evaluation metrics, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). The LSTM and transformer baseline models achieve a BLEU score of 35.0 and 36.8 respectively across all categories of data and a score of about 54 on the seen categories. However, there is a significant drop in the performance on the unseen categories, which shows that end-toend trained systems do not generalise well to new and unseen domains of data. The results from finetuning the T5 model in Table 2 indicate that transfer learning is hugely beneficial in the context of RDFto-text generation as it achieves significant gains over the baselines right out of the box. Even though the baseline t"
2020.webnlg-1.15,2020.emnlp-demos.6,0,0.0329574,"Missing"
2020.webnlg-1.6,W14-3348,0,0.0112758,"models are trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with value 0.1 and the model 2 6 Results and Discussion In this section, we report the results of our experiments in terms of two commonly used evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). We also report scores in terms of precision and recall of the input entities covered in the output generations. For statistical significance, we use MultEval (Clark et al., 2011) to perform bootstrap resampling and report the results on three test sets consisting of instances from seen, unseen and all categories in Table 3. For the test set consisting of seen categories, using KGEs shows consistent improvement over the baseline models for both LSTM and transformer architectures. This improvement is observed in both cases whether the delexicalisation step is performed or not. For the models t"
2020.webnlg-1.6,N18-1029,0,0.0447332,"Missing"
2020.webnlg-1.6,W17-3518,0,0.269612,"tput. This trend is largely inspired by the success of the end-to-end approaches in the related task of machine translation as well as the availability of large corpora for data-to-text generation such as the WikiBio (Lebret et al., 2016) or the ROTOWIRE (Wiseman et al., 2017) datasets, which contain input data in the form of a table consisting of rows and columns. However, the structure and representation of the input data can vary significantly depending on the task at hand. For example, the input can also be a knowledge graph (KG) represented as a set of RDF-triples like the WebNLG corpus (Gardent et al., 2017) or a dialogue-act-based meaning representation like the E2E dataset (Novikova et al., 2017). In this work, we employ pre-trained knowledge graph embeddings (KGEs) for data-to-text generation with a model which is trained in an end-toend fashion using an encoder-decoder style neural network architecture. These embeddings have been shown to be useful in similar end-to-end architectures especially in domain-specific and underresourced scenarios for machine translation (Moussallem et al., 2019). We focus on the WebNLG corpus which contains RDF-triples paired with verbalisations in English. We com"
2020.webnlg-1.6,E17-2068,0,0.0462123,"semantically-enriched also take into account the associated semantic information. Approaches where relationships are interpreted as displacements operating on the lowdimensional embeddings of the entities, have been implemented within the TransE toolkit (Bordes et al., 2013). RDF2Vec (Ristoski and Paulheim, 2016) uses language modelling approaches for unsupervised feature extraction from sequences of words and adapts them to RDF graphs. Cochez et al. (2017) exploited the Global Vectors algorithm in RDF2Vec to compute embeddings from the co-occurrence matrix of entities and relations. However, Joulin et al. (2017b) showed that a BoW based approach with the fastText algorithm (Joulin et al., 2017a) generates state-of-the-art results in KGEs. For data-to-text generation, Chen et al. (2019) have shown that leveraging external knowledge is useful in generating text from Wikipedia infoboxes. In our work, we incorporate pre-trained KGEs based on the fastText model with an end-toend approach for the data-to-text generation. Table 2: Example of an input tripleset paired with reference text in the output (top) and corresponding delexicalised version (bottom). input paired with 42,873 lexicalisations in the out"
2020.webnlg-1.6,D19-1052,0,0.0348328,"Missing"
2020.webnlg-1.6,P17-4012,0,0.0126405,"ation on the test set we take the checkpoint with the best BLEU score on the validation set. Experimental Setup We follow the WebNLG baseline system (Gardent et al., 2017) as one of the baseline architectures for our experiments, which is a vanilla sequenceto-sequence LSTM model with attention where the RDF triples in the input are linearised as a sequence and the output text is tokenised before training. We use another baseline based on the transformer architecture similar to the end-to-end architecture setup by Castro Ferreira et al. (2019). The models are trained using the OpenNMT library (Klein et al., 2017). We use the default parameters for the baseline model: two hidden layers with 500 LSTM units per hidden layer and word embeddings of 500 dimensions. Dropout is applied with value 0.3 and the LSTM models are trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with val"
2020.webnlg-1.6,D19-1299,0,0.0180416,"mbeddings of the entities, have been implemented within the TransE toolkit (Bordes et al., 2013). RDF2Vec (Ristoski and Paulheim, 2016) uses language modelling approaches for unsupervised feature extraction from sequences of words and adapts them to RDF graphs. Cochez et al. (2017) exploited the Global Vectors algorithm in RDF2Vec to compute embeddings from the co-occurrence matrix of entities and relations. However, Joulin et al. (2017b) showed that a BoW based approach with the fastText algorithm (Joulin et al., 2017a) generates state-of-the-art results in KGEs. For data-to-text generation, Chen et al. (2019) have shown that leveraging external knowledge is useful in generating text from Wikipedia infoboxes. In our work, we incorporate pre-trained KGEs based on the fastText model with an end-toend approach for the data-to-text generation. Table 2: Example of an input tripleset paired with reference text in the output (top) and corresponding delexicalised version (bottom). input paired with 42,873 lexicalisations in the output. We follow the same structure for splitting the dataset into training and test sets as defined in the challenge. The final evaluation is done on a test set split into seen an"
2020.webnlg-1.6,D16-1128,0,0.0617342,"Missing"
2020.webnlg-1.6,N19-1236,0,0.0270887,"Missing"
2020.webnlg-1.6,W17-5525,0,0.054177,"Missing"
2020.webnlg-1.6,P02-1040,0,0.106413,"pplied with value 0.3 and the LSTM models are trained with stochastic gradient descent, starting with a learning rate of 1.0 and learning rate decay enabled. For the transformer model, the encoder-decoder setup contains 6 layers with 512 hidden units. The word embeddings are 512-dimensional and the feed-forward sublayers are 2048-dimensional. Each multi-head attention sublayer consists of 8 attention heads. Dropout is applied with value 0.1 and the model 2 6 Results and Discussion In this section, we report the results of our experiments in terms of two commonly used evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). We also report scores in terms of precision and recall of the input entities covered in the output generations. For statistical significance, we use MultEval (Clark et al., 2011) to perform bootstrap resampling and report the results on three test sets consisting of instances from seen, unseen and all categories in Table 3. For the test set consisting of seen categories, using KGEs shows consistent improvement over the baseline models for both LSTM and transformer architectures. This improvement is observed in both cases whether the delexicalisation ste"
2020.webnlg-1.6,D14-1162,0,0.0860599,"fashion using an encoder-decoder style neural network architecture. These embeddings have been shown to be useful in similar end-to-end architectures especially in domain-specific and underresourced scenarios for machine translation (Moussallem et al., 2019). We focus on the WebNLG corpus which contains RDF-triples paired with verbalisations in English. We compare the use of KGEs to two baseline models – the standard sequenceto-sequence model with attention (Bahdanau et al., 2015) and the transformer model (Vaswani et al., 2017). We also do a comparison with pre-trained GloVe word-embeddings (Pennington et al., 2014). 2 Related Work Castro Ferreira et al. (2019) have compared pipeline-based and end-to-end architectures for data-to-text generation on the WebNLG corpus. Their findings suggest that the systems which are trained end-to-end are comparable to pipeline methods on seen data categories but do not generalise to new and unseen domains of data. Marcheggiani and Perez-Beltrachini (2018) proposed an encoder based on graph convolutional networks to exploit the structure in the input for an end-to-end system which showed a slight improvement over the standard LSTM encoder. However, their test set did not"
2020.wildre-1.2,K19-1096,0,0.0110666,"015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in detecting objects in the image. The majority of memes do not"
2020.wildre-1.2,2018.gwc-1.10,1,0.835805,"Missing"
2020.wildre-1.2,W19-7101,1,0.674614,"is work at https: //github.com/sharduls007/TamilMemes. 2. (a) Example 3 (b) Example 4 Figure 2: Examples of troll and not-troll memes. memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that t"
2020.wildre-1.2,W19-6809,1,0.756237,"is work at https: //github.com/sharduls007/TamilMemes. 2. (a) Example 3 (b) Example 4 Figure 2: Examples of troll and not-troll memes. memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that t"
2020.wildre-1.2,2020.sltu-1.25,1,0.812156,"ation of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score. Keywords: Tamil dataset, memes classification, trolling, Indian language data 1. Introduction often obscure due to fused image-text representation. The content in Indian memes might be written in English, in a native language (native or foreign script), or in a mixture of languages and scripts (Ranjan et al., 2016; Chakravarthi et al., 2018; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b). This adds another challenge to the meme classification problem. Traditional media content distribution channels such as television, radio or newspapers are monitored and scrutinized for their content. Nevertheless, social media platforms on the Internet opened the door for people to contribute, leave a comment on existing content without any moderation. Although most of the time, the internet users are harmless, some produce offensive content due to anonymity and freedom provided by social networks. Due to this freedom, people are becoming creative in their joke"
2020.wildre-1.2,2020.sltu-1.28,1,0.822974,"ation of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score. Keywords: Tamil dataset, memes classification, trolling, Indian language data 1. Introduction often obscure due to fused image-text representation. The content in Indian memes might be written in English, in a native language (native or foreign script), or in a mixture of languages and scripts (Ranjan et al., 2016; Chakravarthi et al., 2018; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b). This adds another challenge to the meme classification problem. Traditional media content distribution channels such as television, radio or newspapers are monitored and scrutinized for their content. Nevertheless, social media platforms on the Internet opened the door for people to contribute, leave a comment on existing content without any moderation. Although most of the time, the internet users are harmless, some produce offensive content due to anonymity and freedom provided by social networks. Due to this freedom, people are becoming creative in their joke"
2020.wildre-1.2,W17-3001,0,0.0240583,"ga and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and the"
2020.wildre-1.2,W15-5948,0,0.024406,"memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that tend to be offensive, provocative, or menacing (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018)”. Their main function"
2020.wildre-1.2,W18-4409,0,0.0195459,"pinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in detecting objects in the i"
2020.wildre-1.2,W18-4401,0,0.126908,"g in memes has yet to be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a serious threat which shames people or spreads hatred towards people or a particular community (Kumar et al., 2018; Rani et al., 2020; Suryawanshi et al., 2020). There have been several studies on moderating trolling, however, for a social media administrator memes are hard to monitor as they are region-specific. Furthermore, their meaning is (a) Example 1 (b) Example 2 Figure 1: Examples of Indian memes. 7 In Figure 1, Example 1 is written in Tamil with two images and Example 2 is written in English and Tamil (Roman Script) with two images. In the first example, the meme is trolling about the “Vim dis-washer” soap. The information in Example 1 can be translated into English as “the price of a lemon is fi"
2020.wildre-1.2,malmasi-zampieri-2017-detecting,0,0.0213379,"4 would be “Sorry my friend (girl)”. As this example does not contain any provoking or offensive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 201"
2020.wildre-1.2,W18-3504,0,0.0321038,"and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to"
2020.wildre-1.2,P16-2065,0,0.0169659,"troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer"
2020.wildre-1.2,K15-1032,0,0.0178824,"sive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (20"
2020.wildre-1.2,R15-1058,0,0.0223515,"sive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (20"
2020.wildre-1.2,L18-1585,0,0.0292017,"Missing"
2020.wildre-1.2,P18-2031,0,0.0198473,"2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in dete"
2020.wildre-1.2,2020.trac-1.7,1,0.686215,"o be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a serious threat which shames people or spreads hatred towards people or a particular community (Kumar et al., 2018; Rani et al., 2020; Suryawanshi et al., 2020). There have been several studies on moderating trolling, however, for a social media administrator memes are hard to monitor as they are region-specific. Furthermore, their meaning is (a) Example 1 (b) Example 2 Figure 1: Examples of Indian memes. 7 In Figure 1, Example 1 is written in Tamil with two images and Example 2 is written in English and Tamil (Roman Script) with two images. In the first example, the meme is trolling about the “Vim dis-washer” soap. The information in Example 1 can be translated into English as “the price of a lemon is five Rupees”, whereby"
2020.wildre-1.2,2020.trac-1.6,1,0.747165,"ial networks. Due to this freedom, people are becoming creative in their jokes by making memes. Although memes are meant to be humorous, sometimes it becomes threatening and offensive to specific people or community. On the Internet, a troll is a person who upsets or starts a hatred towards people or community. Trolling is the activity of posting a message via social media that is intended to be offensive, provocative, or menacing to distract which often has a digressive or off-topic content with the intent of provoking the audience (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Suryawanshi et al., 2020). Despite this growing body of research in natural language processing, identifying trolling in memes has yet to be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a seri"
2020.wildre-1.2,Q14-1006,0,0.0309954,"r balance male and female annotators). Based on Landis and Koch (1977) and given the inherent obscure nature of memes, we got fair agreement amongst the annotators. K= 4.5. Data Statistics We collected 2,969 memes, of which most are images with text embedded on them. After the annotation, we learned that the majority (1,951) of these were annotated as troll memes, and 1,018 as not-troll memes. Furthermore, we observed that memes, which have more than one image have a high probability of being a troll, whereas those with only one image are likely to be not-troll. We included Flickr30K2 images (Young et al., 2014) to the not-troll category to address the class imbalance. Flickr30K is only added to training, while the test set is randomly chosen from our dataset. In all our experiments the test set remains the same. 5. 6. We experimented with ResNet and MobileNet. The variation in experiments comes in terms of the data on which the models have been trained on, while the test set (300 memes) remained the same for all experiments. In the first variation, TamilMemes in Table 1, we trained the ResNet and MobileNet models on our Tamil meme dataset(2,669 memes). The second variation, i.e. TamilMemes + ImageNe"
2021.deelio-1.8,S12-1052,0,0.255205,", we explore if causal knowledge is useful for question answering and present strategies on how to enhance a pretrained language model with causal knowledge. There is limited work on incorporating external causal knowledge to improve question answering and no prior work on using causal knowledge to improve multiple-choice question answering. The task of causal question answering aims to reason about cause and effects over a provided real or hypothetical premise. Specifically, we explore the multiple-choice formulation of this task in the context of the COPA (Choice of Plausible Alternatives) (Gordon et al., 2012b) and WIQA (What If Reasoning over Procedural Text) (Tandon et al., 2019) benchmark tasks. COPA and WIQA are both challenging causal reasoning tasks. WIQA requires reasoning on hypothetical perturbations to procedural descriptions of events. Consider the example in Figure 1. To answer the hypothetical question about the downstream effect of an increase of ash and cloud on the environment, the model must be able to causally link Event 3 (about ash clouds) to Event 5 (erupted materials disturb the environment). If provided a causal fact such as (ash clouds, cause-effect, environmental disturban"
2021.deelio-1.8,D19-6004,0,0.0213172,"COPA was first introduced as a SemEval 2012 shared task (Gordon et al., 2012a). COPA consists of a premise and two alternatives. The task is to identify which alternative is most likely the cause or effect of the provided premise. Background commonsense causal knowledge is required to successfully answer questions as there is limited lexical overlap between the premise and alternatives. The COPA dataset consists of 1,000 questions, broken into 500 development and 500 test questions. Recent pretrained models such as BERT and RoBERTa have seen improved performance on the COPA dataset. However, Kavumba et al. (2019) found that these models exploited superficial cues such as the token frequency in the correct answers. To mitigate this effect, Kavumba et al. expanded the development set to include mirror instances to balance the lexical distribution between correct and incorrect answers. For each set of alternatives, the mirror instance introduces a new premise, where the previous correct alternative is now incorrect. This new dataset, called COPA-Balanced, also categorized the test set into easy and hard groups. The easy group consists of 190 questions where RoBERTa-Large and BERT-Large could answer corre"
2021.deelio-1.8,P19-1470,0,0.0291252,"d causal facts. CauseNet consists of about 12 million concepts and 11.5 million relations extracted from Wikipedia and ClueWeb12 1 . ConceptNet (Speer et al., 2017), a public knowledge graph, consists of 36 relations and includes a causes relation. The ATOMIC (Sap et al., 2019) knowledge base consists of 877k textual descriptions of inferential knowledge organized around event prompts and agent-centric activities. ATOMIC describes the social and commonsense knowledge of these events along nine if-then relations which describe the event’s causes and effects on other agents/participants. COMET (Bosselut et al., 2019) is a language model adaptation framework that is trained on ATOMIC and ConceptNet to generate novel commonsense facts and construct robust commonsense knowledge bases. This paper uses CauseNet as its primary source for causal knowledge as it contains a broad and deep set of causal facts (including descriptions of physical processes relevant to WIQA). Next we provide a summary of the question answering tasks which require causal reasoning. The task of binary causal question answering poses questions of cause and effect as yes/no questions (i.e. Could X cause Y?). Hassanzadeh et al. evaluate th"
2021.deelio-1.8,N19-1112,0,0.156096,"bout causality. Causal facts are generally extracted from natural language descriptions. For example, the statement Global warming is caused primarily by human activities such as coal-burning power plants would yield the causal fact factories cause global warming. These causal facts can also be described explicitly in a knowledge base or expressed formally as triples with an explicit cause-effect relation. For exRecent model-based approaches for question answering tasks have primarily focused on finetuning pretrained transformer-based language models, such as BERT (Devlin et al.) and RoBERTa (Liu et al., 2019c), on task-specific datasets. These language models have been found to contain transferable linguistic knowledge (Liu et al., 2019a) and general knowledge (Petroni et al., 2019) that are effective for most downstream natural language processing (NLP) tasks. For more complex tasks, such as causal reasoning, pretrained language models are often limited as they lack the specific external background knowledge required to effectively reason about causality. 70 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"
2021.deelio-1.8,2021.ccl-1.108,0,0.0745963,"Missing"
2021.deelio-1.8,D19-1005,0,0.0291255,"nowledge improves RoBERTa’s performance to nearly match the current state-of-the-art (SOTA) and improve upon the SOTA in specific sub-categories such as in-paragraph and out-of-paragraph reasoning. • Premise: Air pollution in the city worsened. What was the CAUSE of this? 2 • Alternative 1: Factories increased their production. Related Work Enhancing language models with external knowledge (in the form of a knowledge graph or knowledge base) remains an open problem. Several promising strategies have emerged for injecting knowledge into large language models as part of the pretraining process. Peters et al. (2019) present the Knowledge Attention and Recontextualization • Alternative 2: Factories shut down. Lexically, there is limited information in the premise and alternatives that the model can exploit to answer the question. To successfully answer this question, the model requires both background 71 (KAR) layer which can be inserted into a neural language model architecture and used to train knowledge enhanced contextual embeddings. Liu et al. (2019b) introduce the K-BERT model which learns knowledge enabled representations from sentence trees that consist of inputs augmented with knowledge triples."
2021.deelio-1.8,D16-1014,0,0.0390732,"Missing"
2021.gem-1.13,W05-0909,0,0.443849,"hich consists of 6 layers each in the encoder and decoder with a multi-head attention sub-layer consisting of 8 attention heads. The word embeddings have a dimension of 512 and the fully-connected feed-forward sublayers are 2048dimensional. Pre-training on DBpedia abstracts is done on a single Nvidia GeForce GTX 1080 Ti GPU for 10 epochs with a batch size of 8 using the Adam optimizer with a learning rate of 0.001. All the other hyperparameter values are set to their default values. Table 1 shows scores for the output generations on the validation set for BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004). We find random masking to perform the best in terms of automatic evaluation metrics compared to specifically masking entities or predicates, though the results are not statistically significantly different. Furthermore, in our experiments we compare the results when additional tags are added to the input either as entity types from DBpedia or NER tags from spaCy or just the <SUB>, <PRED> and <OBJ> tags. For this, we use the T5-base model with approximately 220 million parameters. This model consists of 12 layers each in the encoder and decoder with 12 attention heads"
2021.gem-1.13,2020.webnlg-1.7,0,0.0545537,"Missing"
2021.gem-1.13,N19-1423,0,0.0305047,"onGen, hence scores on some subsets are not shown. The evaluation metrics are divided into different categories measuring lexical similarity, semantic equivalence, diversity and system characteristics. Popular metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-1/2/L (Lin, 2004) are used for lexical similarity, while recently proposed metrics such as 2 https://github.com/GEM-benchmark/ GEM-metrics BERTScore (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) which rely on sentence embeddings from pre-trained contextualised embedding models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are used for evaluating semantic equivalence. To account for the diverse outputs, Shannon Entropy (Shannon et al., 1950) is calculated over unigrams and bigrams (H1 , H2 ) along with the mean segmented type token ratio over segment lengths of 100 (MSTTR) (Johnson, 1944). Furthermore, the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that appear once across the entire test output (Unique1,2 ) is calculated (Li et al., 2018). The size of the output vocabulary (|V|) and the mean length of the generated output te"
2021.gem-1.13,W19-8652,0,0.0360191,"Missing"
2021.gem-1.13,W17-3518,0,0.37912,"ions for NLG have relied on rulebased systems designed using a modular pipeline approach (Gatt and Krahmer, 2018). However, recently approaches based on neutral networks with an encoder-decoder architecture trained in an endto-end fashion have gained popularity. These typically follow the paradigm of pre-training on a large corpus followed by fine-tuning on a task specific dataset and have been shown to achieve state-of-theart results on many natural language tasks (Raffel In our participation in the GEM benchmark, we submit outputs for four datasets including DART (Nan et al., 2021), WebNLG (Gardent et al., 2017; Castro Ferreira et al., 2020), E2E (Novikova et al., 2017; Duˇsek et al., 2019) and CommonGen (Lin et al., 2020). We use the pre-trained T5-base model architecture (Raffel et al., 2020) for our submission implemented using the transformers library from Hugging Face (Wolf et al., 2020). We first train on monolingual data before fine-tuning on the task-specific dataset. For DART and WebNLG, we use abstracts from DBpedia (Auer et al., 2007) for training while for the other two datasets, we use monolingual target-side references for pre-training with a masked language modeling objective. We expe"
2021.gem-1.13,2020.acl-main.703,0,0.0996048,"Missing"
2021.gem-1.13,2020.findings-emnlp.165,0,0.0344977,"wever, recently approaches based on neutral networks with an encoder-decoder architecture trained in an endto-end fashion have gained popularity. These typically follow the paradigm of pre-training on a large corpus followed by fine-tuning on a task specific dataset and have been shown to achieve state-of-theart results on many natural language tasks (Raffel In our participation in the GEM benchmark, we submit outputs for four datasets including DART (Nan et al., 2021), WebNLG (Gardent et al., 2017; Castro Ferreira et al., 2020), E2E (Novikova et al., 2017; Duˇsek et al., 2019) and CommonGen (Lin et al., 2020). We use the pre-trained T5-base model architecture (Raffel et al., 2020) for our submission implemented using the transformers library from Hugging Face (Wolf et al., 2020). We first train on monolingual data before fine-tuning on the task-specific dataset. For DART and WebNLG, we use abstracts from DBpedia (Auer et al., 2007) for training while for the other two datasets, we use monolingual target-side references for pre-training with a masked language modeling objective. We experiment with different masking strategies where we mask entities and predicates (for DART), meaning representation"
2021.gem-1.13,W04-1013,0,0.0287175,"Missing"
2021.gem-1.13,2021.ccl-1.108,0,0.0645923,"Missing"
2021.gem-1.13,W17-5525,0,0.0544369,"Missing"
2021.gem-1.13,P02-1040,0,0.110933,"rs library (Wolf et al., 2020) which consists of 6 layers each in the encoder and decoder with a multi-head attention sub-layer consisting of 8 attention heads. The word embeddings have a dimension of 512 and the fully-connected feed-forward sublayers are 2048dimensional. Pre-training on DBpedia abstracts is done on a single Nvidia GeForce GTX 1080 Ti GPU for 10 epochs with a batch size of 8 using the Adam optimizer with a learning rate of 0.001. All the other hyperparameter values are set to their default values. Table 1 shows scores for the output generations on the validation set for BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004). We find random masking to perform the best in terms of automatic evaluation metrics compared to specifically masking entities or predicates, though the results are not statistically significantly different. Furthermore, in our experiments we compare the results when additional tags are added to the input either as entity types from DBpedia or NER tags from spaCy or just the <SUB>, <PRED> and <OBJ> tags. For this, we use the T5-base model with approximately 220 million parameters. This model consists of 12 layers each in the encoder a"
2021.gem-1.13,2020.webnlg-1.15,1,0.807329,"dom masking Table 3: Results from automatic evaluation on the E2E validation set with different masking strategies on monolingual data for pre-training using the T5-base model. Table 4: Results from automatic evaluation on the CommonGen validation set with different masking strategies on monolingual data for pre-training using the T5base model. Since the entire WebNLG (en) corpus is already included the DART dataset without any modifications, we use the same model as defined in §2.1 without any further fine-tuning to generate outputs on the WebNLG (en) dataset. Our overall approach is same as Pasricha et al. (2020) for the WebNLG+ challenge 2020 except here we use additional 6,678 DBpedia abstracts for pre-training and the larger DART dataset for fine-tuning which results in a higher scores for automatic evaluation metrics. masking appears to perform better though the differences in terms of automatic evaluation metrics are not significantly different. For our submission to the GEM benchmark, we use the same model architecture and hyperparameter values as described previously for DART to generate the output submissions on the E2E test set and challenge sets. This model is first pre-trained on the monoli"
2021.gem-1.13,2020.acl-main.704,0,0.0137475,"of writing we do not have access to all the references in the test set as well as the challenge sets for DART and CommonGen, hence scores on some subsets are not shown. The evaluation metrics are divided into different categories measuring lexical similarity, semantic equivalence, diversity and system characteristics. Popular metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-1/2/L (Lin, 2004) are used for lexical similarity, while recently proposed metrics such as 2 https://github.com/GEM-benchmark/ GEM-metrics BERTScore (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) which rely on sentence embeddings from pre-trained contextualised embedding models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are used for evaluating semantic equivalence. To account for the diverse outputs, Shannon Entropy (Shannon et al., 1950) is calculated over unigrams and bigrams (H1 , H2 ) along with the mean segmented type token ratio over segment lengths of 100 (MSTTR) (Johnson, 1944). Furthermore, the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that appear once across the entire test output (Unique1,2 ) i"
2021.gem-1.13,W19-2303,0,0.0120652,"evaluating semantic equivalence. To account for the diverse outputs, Shannon Entropy (Shannon et al., 1950) is calculated over unigrams and bigrams (H1 , H2 ) along with the mean segmented type token ratio over segment lengths of 100 (MSTTR) (Johnson, 1944). Furthermore, the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that appear once across the entire test output (Unique1,2 ) is calculated (Li et al., 2018). The size of the output vocabulary (|V|) and the mean length of the generated output texts are reported as system characteristics (Sun et al., 2019). Compared to the baselines described in the GEM benchmark (Gehrmann et al., 2021), we observe higher scores in our submissions for automatic metrics on the CommonGen and DART datasets while scoring lower on the cleaned E2E and WebNLG (en) datasets especially on the test and challenge subsets for both E2E and WebNLG. 152 4 Conclusion We presented a description of the system submitted by NUIG-DSI to the GEM benchmark 2021. We participated in the modeling shared task and submitted outputs on four datasets for data-to-text generation including DART, WebNLG (en), E2E and CommonGen using the T5-bas"
bordea-etal-2012-expertise,D10-1108,0,\N,Missing
bordea-etal-2012-expertise,S10-1030,1,\N,Missing
buitelaar-etal-2004-evaluation,W03-1302,1,\N,Missing
buitelaar-etal-2004-evaluation,E03-2012,1,\N,Missing
buitelaar-etal-2004-evaluation,A00-1031,0,\N,Missing
buitelaar-etal-2004-evaluation,vintar-etal-2002-efficient,1,\N,Missing
buitelaar-etal-2004-towards,A00-1031,0,\N,Missing
buitelaar-etal-2004-towards,buitelaar-etal-2004-evaluation,1,\N,Missing
buitelaar-etal-2006-ontology,callmeier-etal-2004-deepthought,1,\N,Missing
buitelaar-etal-2014-hot,C00-2136,0,\N,Missing
buitelaar-etal-2014-hot,S10-1055,0,\N,Missing
buitelaar-etal-2014-hot,W00-0738,0,\N,Missing
C12-1005,W11-2107,0,0.0157244,"ted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators"
C12-1005,W11-1204,0,0.0405268,"Missing"
C12-1005,P07-2045,0,0.0139662,"translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators (Section 4.5). 4.1 Translation System: Moses Toolkit For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language 12 Meteor configuration: exact, stem, paraphrase 73 model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). 4.2 Translating aligned UK – German GAAP labels The UK GAAP is a monolingual ontology which holds 142 financial labels. With the help of the German equivalent, i.e. German GAAP, we aligned 16 German labels with the English o"
C12-1005,J03-1002,0,0.00499596,"and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators (Section 4.5). 4.1 Translation System: Moses Toolkit For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language 12 Meteor configuration: exact, stem, paraphrase 73 model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). 4.2 Translating aligned UK – German GAAP labels The UK GAAP is a monolingual ontology which holds 142 financial labels. With the help of the German equivalent, i.e. German GAAP, we aligned 16 German labels with the English ones, stored in the UK GAAP. This allowed us to do a small automatic evaluation, regardless of the low number of labels to be translated. Scoring Metric Source # correct BLEU-2 BLEU-4 NIST TER Meteor JRC-Acquis ECB Linguee+Wikipe"
C12-1005,P02-1040,0,0.0881958,"rman one. These translation pairs were used to suggest the SMT system to choose the extracted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to"
C12-1005,2006.amta-papers.25,0,0.0395008,"the SMT system to choose the extracted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cro"
C12-1005,steinberger-etal-2006-jrc,0,0.328224,"Missing"
C12-1005,C08-1125,0,0.0916576,"ties, Charges, Balance, Capital, Reserves . . . 2 1 Table 1: Examples for financial labels in the UK GAAP # of labels 30 20 10 0 1 2 3 4 5 7 6 Length of a label 8 9 10 11 Figure 1: Label length of the UK GAAP ontology 3.2 JRC-Acquis The general parallel corpus JRC-Acquis3 was used as baseline training data. This corpus is available in almost every EU official language (except Irish), and is a collection of legislative texts written between 1950 and now. Although previous research showed, that a training model built by using a general resource cannot be used to translate domain-specific terms (Wu et al., 2008), we decided to evaluate the translations on these resources to illustrate any improvement steps from a general resource to specialised domain resources. 3.3 European Central Bank Corpus For comparison with JRC-Acquis, we also did experiments using the European Central Bank Corpus4 , which contains a financial vocabulary. The multilingual corpus is generated by extracting the website and documentation from the European Central Bank and is aligned among 19 European languages. For our research we used the English-German language pair, which consists of 113,171 sentence pairs or 2.8 million Engli"
C12-1005,zesch-etal-2008-extracting,0,0.0118582,"a, they used thresholds to avoid storing undesirable categories. Müller and Gurevych (2008) used 68 Wikipedia and Wiktionary as knowledge bases to integrate semantic knowledge into Information Retrieval. Their models, text semantic relatedness (for Wikipedia) and word semantic relatedness (for Wiktionary), are compared to a statistical model implemented in Lucene. In their approach to bilingual retrieval, they use the cross-language links in Wikipedia, which improved the retrieval performance in their experiment, especially when the machine translation system generated incorrect translations. Zesch et al. (2008) address the issues in accessing the largest collaborative resources: Wikipedia and Wiktionary. They describe several modules and APIs for converting a Wikipedia XML Dump into a more suitable format. Instead of parsing the large Wikipedia XML Dump, they suggest to store the Dump into a database, which significantly increases the performance in retrieval time of queries. 3 Experimental Data We are investigating the problem of translating a domain-specific vocabulary, therefore our experiments started with an analysis of the financial terms stored in the investigated ontology. With these extract"
C16-1010,2014.amta-researchers.5,1,0.855557,"Translations The SMT system is configured to return the t highest scoring translations, according to its model, and we select the translation as the most frequent translation of the context among this t-best list. In our experiments, we combined this with m disambiguations to give tm candidate translations from which the candidate is chosen. Target Side Lookup (TSL) We can also utilize the translation of our context into the target language xliT from the parallel corpus, however this cannot be applied directly as we do not know which word(s) in xliT correspond to the input and previous work (Arcan et al., 2014) has shown that automatic inference of this alignment (e.g., with GIZA++) can seriously affect performance. Instead we filter contexts to those that generate a translation candidate, wklT , such that wklT ∈ xliT , i.e., the machine translation agrees with the gold-standard translation for this context. 4 Experimental Setting This section gives an overview on the multilingual resources and the translation toolkit used in our experiment. Furthermore, we give insights into SMT evaluation techniques, considering the translation direction of the English WordNet entries into Italian, Slovene, Spanis"
C16-1010,P15-1069,1,0.524332,"ingwn.linguistic-lod.org/ 98 Since all these approaches use word alignment information, they are not able to generate any translation equivalents for multi-word expressions (MWE). In contrast, our approach use an SMT system trained on a large amount of parallel sentences, which allows us to align possible MWEs, such as commercial loan or take a breath, between source and target language. Furthermore, we engage the idea of identifying relevant contextual information to support an SMT system translating short expressions, which showed better performance compared to approaches without a context. Arcan et al. (2015) built small domainspecific translation models for ontology translation from relevant sentence pairs that were identified in a parallel corpus based on the ontology labels to be translated. With this approach they improve the translation quality over the usage of large generic translation models. Since the generation of translation models can be computational expensive, Arcan et al. (2016) use large generic translation models to translate ontology labels, which were placed into a disambiguated context. With this approach the authors demonstrate translation quality improvement over commercial s"
C16-1010,P13-1133,0,0.0310493,"orm of concepts, where new concepts may be added even if they are not represented (yet) in the Princeton WordNet or even lexicalized in English (e.g., many languages have distinct gendered role words, such as ‘male teacher’ and ‘female teacher’, but these meanings are not distinguished in English). Previous studies of generating non-English wordnets combined Wiktionary knowledge with existing wordnets to extend them or to create new ones (de Melo and Weikum, 2009). Bond and Paik (2012) describe in their work the creation of the Open Multilingual Wordnet and its extension with other resources (Bond and Foster, 2013). A different approach to expand English WordNet synsets with lexicalizations in other languages was proposed in de Melo and Weikum (2012). The authors do not directly match concepts in the two different language resources, but demonstrate an approach that learns how to determine the best translation for English synsets by taking bilingual dictionaries, structural information of the English WordNet and corpus frequency information into account. With the growing amount of parallel data, Kazakov and Shahid (2009) show an approach to acquire a set of synsets from parallel corpora. The synsets are"
C16-1010,2016.gwc-1.9,1,0.90782,"f a sentence. As a motivating example, we consider the word vessel, which is a member of three synsets in Princeton WordNet, whereby the most frequent translation, e.g., as given by Google Translate, is Schiff in German and nave in Italian, corresponding to i608331 ‘a craft designed for water transportation’. For the second sense, i65336 ‘a tube in which a body fluid circulates’, we assume that we know the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We use the CILI identifiers for synsets (Bond et al., 2016) 97 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 97–108, Osaka, Japan, December 11-17 2016. German translation for this sense is Gefäß. In our approach we look for sentences in a parallel corpus, where the words vessel and Gefäß both occur and obtain a context such as ‘blood vessel’ that allows the SMT system to translate this sense correctly. This alone is not sufficient as Gefäß is also a translation of i60834 ‘an object used as a container’, however in Italian these two senses are distinct (vaso and recipiente respective"
C16-1010,S07-1054,0,0.0243835,"kipedia entry, the authors use Google Translate to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language. The use of parallel corpora has been previously exploited for word sense disambiguation, for example to construct sense-tagged corpora in another language (Ng et al., 2003) or by using translations as a method to discriminate senses (Ide et al., 2002). It has been shown that the combination of these techniques can improve supervised word sense disambiguation (Chan et al., 2007). A similar approach to the one proposed in this paper is that of Tufi¸s et al. (2004), where they show that using the interlingual index of WordNet with the help of parallel text can improve word sense disambiguation of a monolingual approach and we generalize this result to generate wordnets for new languages. 3 Methodology Our approach takes the advantage of the increasing amount of parallel corpora in combination with wordnets in languages other than English for sense disambiguation, which will help us to improve automatic translations of English WordNet entries. We assume that we have a m"
C16-1010,P11-2031,0,0.0293983,"c produces good correlation with human judgement at the sentence or segment level. chrF3 is a character n-gram metric, which has shown very good correlations with human judgements on the WMT2015 shared metric task (Stanojevi´c et al., 2015), especially when translating from English into morphologically rich(er) languages. As there are multiple translations available for each sense in the target wordnet we use all translations as multiple references for BLEU, for the other two metrics we compare only to the most frequent member of the synset. The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value < 0.05. 5 Evaluation In this section we present the evaluation of the translated English WordNet words into Italian, Slovene, Spanish and Croatian. We evaluate the quality of translations of the WordNet entries based on the provided contextual information as well as the impact on the number of languages and their effect on wordsense disambiguation. 5.1 Translation Quality Evaluation Based on Contextual Information Our main evaluation focuses on the importance of identifying relevant cont"
C16-1010,W14-3348,0,0.0558521,"corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations.6 The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), this is in most cases only precision, so in addition we also report other metrics. METEOR (Metric for Evaluation of T"
C16-1010,eisele-chen-2010-multiun,0,0.142157,"word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and"
C16-1010,W11-2123,0,0.0256256,"37M 43M Table 2: Statistics on parallel data for translation model training and word-sense disambiguation. (parallel resources used for training the translation models1 and/or word-sense disambiguation2 ) The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for ide"
C16-1010,W02-0808,0,0.183729,"Missing"
C16-1010,W09-4202,0,0.0213779,"the creation of the Open Multilingual Wordnet and its extension with other resources (Bond and Foster, 2013). A different approach to expand English WordNet synsets with lexicalizations in other languages was proposed in de Melo and Weikum (2012). The authors do not directly match concepts in the two different language resources, but demonstrate an approach that learns how to determine the best translation for English synsets by taking bilingual dictionaries, structural information of the English WordNet and corpus frequency information into account. With the growing amount of parallel data, Kazakov and Shahid (2009) show an approach to acquire a set of synsets from parallel corpora. The synsets are obtained by comparing aligned words in parallel corpora in several languages. Similarly, the sloWNet for Slovene (Fišer, 2007) and Wolf for French (Sagot and Fišer, 2008) are constructed using a multilingual corpus and word alignment techniques in combination with other existing lexical resources. 2 The Polylingual WordNet is available at http://polylingwn.linguistic-lod.org/ 98 Since all these approaches use word alignment information, they are not able to generate any translation equivalents for multi-word e"
C16-1010,N03-1017,0,0.0314089,"Missing"
C16-1010,2005.mtsummit-papers.11,0,0.191404,"del learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and"
C16-1010,P03-1058,0,0.073926,"ge of Wikipedia. This is done by assigning WordNet synsets to Wikipedia entries, and making these relations multilingual through the interlingual links. For languages, which do not have the corresponding Wikipedia entry, the authors use Google Translate to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language. The use of parallel corpora has been previously exploited for word sense disambiguation, for example to construct sense-tagged corpora in another language (Ng et al., 2003) or by using translations as a method to discriminate senses (Ide et al., 2002). It has been shown that the combination of these techniques can improve supervised word sense disambiguation (Chan et al., 2007). A similar approach to the one proposed in this paper is that of Tufi¸s et al. (2004), where they show that using the interlingual index of WordNet with the help of parallel text can improve word sense disambiguation of a monolingual approach and we generalize this result to generate wordnets for new languages. 3 Methodology Our approach takes the advantage of the increasing amount of par"
C16-1010,J03-1002,0,0.00552686,"7M 296M 377M 130M 378M 302M 34M 33M 13M 37M 43M Table 2: Statistics on parallel data for translation model training and word-sense disambiguation. (parallel resources used for training the translation models1 and/or word-sense disambiguation2 ) The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we"
C16-1010,P02-1040,0,0.103798,"inberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations.6 The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), this is in most cases only precision, so in addition we also report other metrics"
C16-1010,W15-3049,0,0.0522913,"Missing"
C16-1010,2016.gwc-1.43,0,0.0636712,"Missing"
C16-1010,W15-3031,0,0.0649935,"Missing"
C16-1010,E12-1015,0,0.17713,"ZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calcu"
C16-1010,C04-1192,0,0.101426,"Missing"
D15-1258,esuli-sebastiani-2006-sentiwordnet,0,0.029461,"ce-holders, or skip n-grams, 2. Sentiment Features: Given that suggestions are being extracted from customer reviews, which are otherwise mostly used for sentiment analysis, a relation between sentiments and suggestions can be suspected. It can be observed From the Figure 3, 4 suggestions do not seem to always carry one particular sentiment, but different sentiments at different instances. We compare three types of sentiment related features: a) Manually tagged sentiments: These annotations were provided with the used sentiment analysis datasets. b) Sentiwordnet score summation: SentiWordNet (Esuli and Sebastiani, 2006) sentiment score summation of all the words in a sentence. No sense disambiguation is performed, and all synset scores are summed up for each word. c) Normalised sentiwordnet score summation: These scores are the sum of all the sentiment scores of the words in a given sentence, normalised over the number of words carrying non-neutral sentiment score. 3. Information about the subject/s of a statement: This feature captures the presence of nsubj dependency (Marneffe and Manning, 2164 Features Baseline (best generic features) + patterns + sentiments (manual) + sentiments (score) + sentiments (nor"
D15-1258,N03-1033,0,0.00503191,"f the arguments of this dependency. Often a reviewer addresses the reader when giving a suggestion. For the sentence, If you do end up here, be sure to specify a room at the back of the hotel, the nsubj dependency is nsubj(do, you). The feature value would be VBP-PRP in this case. On the other hand, this suggestion could also have been, Be sure to specify a room at the back of the hotel.. In this case the feature value would be null. If more than one nsubj dependency is present, the POS pair of each of them will be included in the feature value. Experimental Setup: We use the Stanford Parser (Toutanova et al., 2003) for obtaining part of speech and dependency information. Stemming did not effect the results. Stopwords were used using a customised stopword list. We employ the LibSVM library (Chang and Lin, 2011) for Support Vector Machine classifiers (SVM), as impleResults and Discussion We evaluate the proposed features using 10-fold cross validation. As indicated in section 5, we consider best performing set of generic features as the baseline, which is: uni, bi grams and unigrams of pos tags. Table 5 summarises the classifier performance with the addition of special features, measured in Precision, Rec"
D15-1258,W15-0115,1,0.834677,"Work Only a few attempts have been made to study suggestion mining, and there is an unavailability of benchmark datasets. Therefore, suggestion mining still remains a young area of study. • Suggestion Mining from Customer Reviews As mentioned in section 1, there have been some attempts to extract suggestions for improvements in products from customer reviews. Ramanand et. al. (2010) used manually formulated patterns to extract wishes regarding improvements in products. Brun and Hagege (2013) also used manually formulated rules to extract suggestions for improvements from the product reviews. Negi and Buitelaar (2015) studied linguistic nature of suggestions and wishes for improvements and performed experiments in order to assert that these contain subjunctive mood. These works do not acknowledge the fact that reviews can also contain suggestions for other customers. One major drawback of previous works on customer reviews is the public unavailability of evaluation datasets. 2161 • Other Domains Two other lines of work extracted suggestions from domains other than reviews. Dong et. al. (2013) performed detection of suggestions for product improvement using tweets. They used a statistical classifier, with f"
D15-1258,W10-0207,0,0.189263,"Missing"
declerck-etal-2004-towards,W03-1905,1,\N,Missing
declerck-etal-2004-towards,buitelaar-etal-2004-towards,1,\N,Missing
declerck-etal-2004-towards,broeder-etal-2004-large,1,\N,Missing
declerck-etal-2004-towards,capstick-etal-2002-collate,1,\N,Missing
E03-2012,A00-1031,0,0.0199281,"ation 2. Semantic Relation Annotation Results Displaying Search I Engine  Figure 1. System Architecture 3 MeSH: Medical Subject Headings (http://www.nlm.nih.gov/mesh/meshhome.html) 231 ates the intermediary data representation, and a back-end tier consisting of a search engine system to provide the retrieval technology (see Figure 1.). 2.1 Query and Document Annotation The middle tier annotation module consists of more subtiers representing an advanced annotation system that automatically identifies a number of relevant linguistic and semantic features. Components for part-of-speech tagging (Brants, 2000), morphological analysis (Petitpierre and Russell., 1995), phrase tagging (chunking) (Skut and Brants., 1998), concept and semantic relations annotation are being loosely integrated, through input-output markup interfaces, and generate an intermediary XML representation (Vintar et al., 2001) of the input data (see Figure 2.). Semantic annotation represents the primary information that the retrieval system is using. Crossing the language barrier from a query in one language to the document collection in another language is done via concept codes as an interlingua representation. The multilingua"
E03-2012,W98-1117,0,0.0275225,"Missing"
E03-2012,vintar-etal-2002-efficient,1,0.7706,"Missing"
E06-2010,callmeier-etal-2004-deepthought,1,0.811114,"n extraction see e.g. Maedche et al., 2002; Lopez and Motta, 2004; Müller et al., 2004; Nirenburg and Raskin, 2004). The SOBA system consists of a web crawler, linguistic annotation components and a component for the transformation of linguistic annotations into an ontology-based representation. The web crawler acts as a monitor on relevant web domains (i.e. the FIFA2 and UEFA3 web sites), automatically downloads relevant documents from them and sends them to a linguistic annotation web service. Linguistic annotation and information extraction is based on the Heart-of-Gold (HoG) architecture (Callmeier et al. 2004), which provides a uniform and flexible infrastructure for building multilingual applications that use semantics- and XML-based natural language processing components. The linguistically annotated documents are further processed by the transformation component, which generates a knowledge base of soccer-related entities (players, teams, etc.) and events (matches, goals, etc.) by mapping annotated entities or events to ontology classes and their properties. Finally, an automatic hyperlinking component is used for the visualization of extracted entities and events. This component is based on the"
kasper-etal-2004-integrated,N03-1025,0,\N,Missing
kasper-etal-2004-integrated,steffen-2004-n,1,\N,Missing
L16-1066,P15-1059,0,0.0600247,"and meaning. The method proposed in this work models the growth pattern of a scientific topic to predict its distribution in the future. Although approaches based on topic models have received a lot of attention, they still require a considerable amount of manual work and expert knowledge to identify a collective topic for human-readable labels. Therefore, in this work we rely on previous work on keyphrase extraction to identify scientific topics (Bordea et al., 2013). Recent analysis of keyphrase creation dynamics shows that they are frequently used in scientific discourse to signal novelty (Adar and Datta, 2015). Solutions based on citation analysis are less appropriate for contemporary analysis of emergent trends, as this type of data is less robust for recent documents and can not be applied as soon as documents become available. Where the majority of previous approaches apply retrospective analysis, by mapping the evolution of scientific topics over time (Zhou et al., 2006; He et al., 2009; Bolelli et al., 2009), in this paper, we address the problem of actually predicting temporal distribution of the keywords at some point in the future. We also generate a dataset based on all of the LREC1 confer"
L16-1090,2007.mtsummit-papers.5,0,0.0604561,"em for English to and from White Hmong (Hmong-Mien language). They built their system from dictionary entries and translations of introductions or phrases from localisation projects. To extend their parallel resources, they manually searched for Hmong phrases and its translations on the web, whereby they collected around 45,000 parallel sentences in overall. A different deployment of SMT systems in an under-resourced scenario was shown in Lewis et al. (2011) and Lewis (2010) as a consequence of the earthquake crisis in Haiti supporting emergency responders to find trapped people. Differently, Babych et al. (2007) compare results between a direct transfer of an SMT system (source→target language) and translations via a cognate language (source→pivot→target language). Their approach focused on Slavic languages with Russian as the pivot language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with translating Irish language was shown in Scannell (2006). The rule-based system was developed for translations of closely related languages, Irish (Gaeilge) and"
L16-1090,2003.mtsummit-papers.9,0,0.117195,"Missing"
L16-1090,W14-3348,0,0.0162316,"was extracted from corpora annotated with *). In addition to the publicly available parallel corpora, the Acadamh na hOllscola´ıochta Gaeilge12 at the National University of Ireland, Galway (NUIG) provided us with translations of second level textbooks (Cuimhne na dT´eacsleabhar) in the domain of economics and geography. The data resource, funded by An Chomhairle um Oideachas Gaeltachta agus Gaelscola´ıochta (COGG), holds around 350,000 parallel sentences or 6M English and 6.5M Irish words, respectively. 5. Evaluation Here, we report results based on the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and the chrF3 (Popovi´c, 2015) metric for automatic evaluation of translations. Additionally, we perform a manual evaluation of the translations into Irish. BLEU is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with standard exact word (or phrase"
L16-1090,W11-2123,0,0.0576013,"he development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the target language. It also provides information on the current translation performance of IRIS in terms of the evaluation metric BLEU. Furthermore, it gives detailed information about the used data for the translation models accessed by IRI"
L16-1090,N03-1017,0,0.0503569,"atures are preserved in such a standardization process (Scannell, 2014). For example, it may be desirable to standardize spelling and orthography, but to preserve dialectal vocabulary and grammar. This is just one of many factors limiting the development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the ta"
L16-1090,P07-2045,0,0.0189343,"tandardize spelling and orthography, but to preserve dialectal vocabulary and grammar. This is just one of many factors limiting the development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the target language. It also provides information on the current translation performance of IRIS in terms of the eva"
L16-1090,W11-2164,0,0.0274951,"nslation improvement or using a pivot language to overcome the data sparseness. With the aim of language preservation, Lewis and Yang (2012) show an SMT system for English to and from White Hmong (Hmong-Mien language). They built their system from dictionary entries and translations of introductions or phrases from localisation projects. To extend their parallel resources, they manually searched for Hmong phrases and its translations on the web, whereby they collected around 45,000 parallel sentences in overall. A different deployment of SMT systems in an under-resourced scenario was shown in Lewis et al. (2011) and Lewis (2010) as a consequence of the earthquake crisis in Haiti supporting emergency responders to find trapped people. Differently, Babych et al. (2007) compare results between a direct transfer of an SMT system (source→target language) and translations via a cognate language (source→pivot→target language). Their approach focused on Slavic languages with Russian as the pivot language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with t"
L16-1090,2010.eamt-1.37,0,0.0195681,"using a pivot language to overcome the data sparseness. With the aim of language preservation, Lewis and Yang (2012) show an SMT system for English to and from White Hmong (Hmong-Mien language). They built their system from dictionary entries and translations of introductions or phrases from localisation projects. To extend their parallel resources, they manually searched for Hmong phrases and its translations on the web, whereby they collected around 45,000 parallel sentences in overall. A different deployment of SMT systems in an under-resourced scenario was shown in Lewis et al. (2011) and Lewis (2010) as a consequence of the earthquake crisis in Haiti supporting emergency responders to find trapped people. Differently, Babych et al. (2007) compare results between a direct transfer of an SMT system (source→target language) and translations via a cognate language (source→pivot→target language). Their approach focused on Slavic languages with Russian as the pivot language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with translating Irish"
L16-1090,J03-1002,0,0.00563437,"l vocabulary and grammar. This is just one of many factors limiting the development of Irish language resources and tools for computational linguistics, and therefore the language has been defined as a less-resourced language (Piotrowski, 2012) in this domain. 4. English-Irish Machine Translation Development Here we present IRIS, an English-Irish translation system, which is based on a widely used phrase-based SMT framework (Koehn et al., 2003). For generating the translation models, we use the statistical translation toolkit Moses (Koehn et al., 2007). Word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with KenLM (Heafield, 2011). The monolingual and parallel corpora described in Section 4.2. are progressively added to the IRIS training set. This allows us to evaluate the performance of the system (Section 5.) at each point new data is added. 4.1. IRIS Framework IRIS’ bilingual interface (Figure 1) allows the user to enter English or Irish sentences that are to be translated into the target language. It also provides information on the current translation performance of IRIS in terms of the evaluation metric BLEU. Furthermore, it gives detailed informat"
L16-1090,P02-1040,0,0.104513,"corpora (the evaluation data set was extracted from corpora annotated with *). In addition to the publicly available parallel corpora, the Acadamh na hOllscola´ıochta Gaeilge12 at the National University of Ireland, Galway (NUIG) provided us with translations of second level textbooks (Cuimhne na dT´eacsleabhar) in the domain of economics and geography. The data resource, funded by An Chomhairle um Oideachas Gaeltachta agus Gaelscola´ıochta (COGG), holds around 350,000 parallel sentences or 6M English and 6.5M Irish words, respectively. 5. Evaluation Here, we report results based on the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and the chrF3 (Popovi´c, 2015) metric for automatic evaluation of translations. Additionally, we perform a manual evaluation of the translations into Irish. BLEU is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along"
L16-1090,W15-3049,0,0.0598333,"Missing"
L16-1090,W14-4605,0,0.166414,"language. The results showed the efficiency of the usage of dictionaries, grammars as well as lexical and syntactic similarities of closely related languages for translation improvements. An early work dealing with translating Irish language was shown in Scannell (2006). The rule-based system was developed for translations of closely related languages, Irish (Gaeilge) and Scottish Gaelic (G`aidhlig), respectively. The translation system is based on a bilingual lexicon, which performs part-of-speech tagging, word sense disambiguation and a syntactic/lexical transfer. This work was expanded in Scannell (2014), focusing on overcoming the orthographical differences between the languages. As an additional task, the author casts the text normalisation problem as an SMT problem and applies the statistical models for normalisation of historical Irish text. The most recent work on a domain-specific English-Irish SMT is shown in Dowling et al. (2015), aiming to help Irish government with their translation tasks. 1 http://www.meta-net.eu/whitepapers/ key-results-and-cross-language-comparison 2 http://server1.nlp.insight-centre.org/ iris/ 3. Irish language Irish is a VSO language on the Celtic branch of the"
L16-1090,skadins-etal-2014-billions,0,0.0331127,"Missing"
L16-1090,W15-3031,0,0.0620098,"Missing"
L16-1090,tiedemann-2012-parallel,0,0.0905896,"Missing"
L16-1385,mendes-etal-2012-dbpedia,0,0.433891,"Wikipedia concepts consisting of 175 million surface forms that refer to 7.6 million entities derived from Wikipedia article titles, anchor text from inter-Wikipedia links, nonWikipedia web pages linking to Wikipedia articles and nonWikipedia web pages to non-Wikipedia pages for topics that have corresponding Wikipedia articles(Spitkovsky and Chang, 2012). DBpedia lexicalization dataset was released as a part of DBpedia Spotlight project. It contains alternative names for entities and concepts in the DBpedia project with several scores estimating the association strength between name and URI(Mendes et al., 2012). Similar to DBpedia Spotlight, the AIDA project developed a dictionary called ’YAGO means’ for use in entity disambiguation and linking. It is constructed by extracting link anchors, re-direction and disambiguation page links in Wikipedia (Yosef et al., 2011). A slight variant of YAGO Means, Redirect Disambiguation Mapping (RDM) dictionary contains the additional entries with alternative labels of DBpedia entities. For example, the label ”Berlin (2009 film)” has been converted to just ”Berlin” (Steinmetz et al., 2013). 3. Wikipedia as a Lexical Resource To construct a dictionary from Wikipedi"
L16-1385,spitkovsky-chang-2012-cross,0,0.0312833,"e KB. 2. Related Work Several approaches exist for generating an entity-linking dictionary from Wikipedia or other knowledge resources. We review some of the well known entity linking dictionaries in the following. Google released a cross-lingual dictionary for English Wikipedia concepts consisting of 175 million surface forms that refer to 7.6 million entities derived from Wikipedia article titles, anchor text from inter-Wikipedia links, nonWikipedia web pages linking to Wikipedia articles and nonWikipedia web pages to non-Wikipedia pages for topics that have corresponding Wikipedia articles(Spitkovsky and Chang, 2012). DBpedia lexicalization dataset was released as a part of DBpedia Spotlight project. It contains alternative names for entities and concepts in the DBpedia project with several scores estimating the association strength between name and URI(Mendes et al., 2012). Similar to DBpedia Spotlight, the AIDA project developed a dictionary called ’YAGO means’ for use in entity disambiguation and linking. It is constructed by extracting link anchors, re-direction and disambiguation page links in Wikipedia (Yosef et al., 2011). A slight variant of YAGO Means, Redirect Disambiguation Mapping (RDM) dictio"
L18-1149,P15-1069,1,0.827864,"ord-alignment and machine translation approaches, and compared the results with the proposed semantics transfer approach, focusing on the semantic coherence of the generated translations between Spanish, French and German. Sajous et al. (2010) enriched Wiktionary by relying on similarity measures based on random walks through the graphs extracted from its lexical networks. In their final step they engaged users in collaborative editing in order to validate the resource. A different approach for translation and disambiguation of domainspecific expressions stored in knowledge bases was shown in Arcan et al. (2015), where the authors identified relevant in-domain parallel sentences and used them to train a small but domain-aware SMT system. Ordan et al. (2017) demonstrated an approach for bilingual dictionary creation using different translation directions within a loop. In contrast, de Melo and Weikum (2012) did not match concepts with SMT, but showed a machine learning approach which determines the best translation for English WordNet synsets by taking bilingual dictionaries, structural information of WordNet and corpus frequency information into account. Similarly, the multilingual disambiguation of"
L18-1149,C16-1010,1,0.797131,"idge the gap between language-specific information and the language-independent semantic content (Gracia et al., 2012). Since manual multilingual translation and evaluation of knowledge bases is a very time-consuming and expensive process, we apply SMT to automatically translate domain-specific expressions and demonstrate its validity by translating the IATE entries. While an SMT system can only return the most frequent or dominant translation when given a term by itself, it has been showed that SMT provides strong word sense disambiguation when the word is given in the context of a sentence (Arcan et al., 2016a; Arcan et al., 2016b). As a motivating example, we consider the word vessel, which appears several times in the IATE repository, whereby the most frequent translation into German is Schiff, with the meaning of ‘a craft designed for water transportation’, e.g., as given by Google Translate.3 To overcome the issue of obtaining translations for vessel in other languages, and also in different domains (in the sense of blood vessel, for instance), we aim to identify (several) parallel sentences, which hold the terminological entries in the targeted domain, and use their context to translate them"
L18-1149,P11-2031,0,0.0290034,"luation data set to reach an estimate of the translation’s overall quality. METEOR (Denkowski and Lavie, 2014) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with exact word (or phrase) matching it uses additional features, i.e., stemming, paraphrasing and synonymy matching. chrF3 (Popovi´c, 2015) is a character n-gram metric, which has shown very good correlations with human judgements, especially when translating from English into morphologically rich languages (Stanojevi´c et al., 2015). The approximate randomization approach (Clark et al., 2011) is used to test whether differences among system performances are statistically significant. 5. Results In this section, we present the evaluation of the translated IATE entries into several languages not initially included in this resource, and how existing IATE terms have been exploited for our purposes in the parallel corpora used in this work.7 In addition to this, we illustrate the enhancing of IATE RDF resource with additional contextual information 7 We randomly selected 2,000 terms, although not all target terms are represented in each language for evaluation. 933 # of Terms Bulgarian"
L18-1149,declerck-etal-2006-multilingual,0,0.117658,"Missing"
L18-1149,W14-3348,0,0.0177264,"3M 163M 938k 1M 687k 561k 1M 1M 640k 826k 1M 180k 626k 1M 934k 421k 389k 218k 976k 1M 1M 1M 543k 631k 687k 1M 1M 1M 1M 1M 1M 1M 2M 1M 271k 1M 3M 1M 833k 653k 380k 1M 1M 1M 1M 1M 1M 1M Table 3: Statistics on parallel data for translation model training and word-sense disambiguation. data set of reference translations. Considering the shortness of the entries in IATE, we report scores based on the unigram overlap (BLEU-1). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. METEOR (Denkowski and Lavie, 2014) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with exact word (or phrase) matching it uses additional features, i.e., stemming, paraphrasing and synonymy matching. chrF3 (Popovi´c, 2015) is a character n-gram metric, which has shown very good correlations with human judgements, especially when translating from English into morphologically rich languages (Stanojevi´c et al., 2015). The approximate randomization approach (Clark et al., 2011) is used to test whether differences among system performances are statistically significan"
L18-1149,eisele-chen-2010-multiun,0,0.0159357,"nts, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). BLEU (Papineni et al., 2002) is calculated for individual translated segments (n-grams) by comparing them wi"
L18-1149,W11-2123,0,0.0166436,"the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel c"
L18-1149,N03-1017,0,0.00891204,"uation techniques. 4.1. IATE - Inter-Active Terminology for Europe IATE is the terminology database of the EU with its objective of supporting the EU translators and creating a terminology resource to ensure standardisation throughout all institutions. It incorporates the various terminology databases into one database containing approximately one million multilingual entries in English (Table 2).5 Recent domains that have been extensively covered include the financial crisis, environment, fisheries and migration. 4.2. Statistical Machine Translation Our approach is based on phrase-based SMT (Koehn et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 20"
L18-1149,P07-2045,0,0.0132526,"s, environment, fisheries and migration. 4.2. Statistical Machine Translation Our approach is based on phrase-based SMT (Koehn et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 201"
L18-1149,2005.mtsummit-papers.11,0,0.0830539,"learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and r"
L18-1149,W11-1013,1,0.819266,"Missing"
L18-1149,J03-1002,0,0.00757023,"n et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). S"
L18-1149,P02-1040,0,0.108804,"Missing"
L18-1149,W15-3049,0,0.0629611,"Missing"
L18-1149,W15-3031,0,0.0609754,"Missing"
L18-1149,E12-1015,0,0.0499086,"(Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). BLEU (Papineni et al., 2002) is calculated for individual translated segments (n-grams) by comparing them with a 5 6 Based on IATE TBX file - IATE export 16032017.tbx http://opus.nlpl.eu/"
L18-1192,W17-5223,1,0.825919,"ing n rating scale annotations results in comaparisons. n(n−1) 2 1200 Dimension Spearman Correlation (Regression Models) F1 (Comparison Models) Valence Arousal Dominance Surprise 0.72 0.64 0.53 0.42 0.72 0.69 0.71 0.63 Average 0.58 0.69 7. Table 5: Cross validation results for rating scale regression models and comparison classification models. 5. Predictive Model As further verification of the utility of the data, we built two supervised models, one each from the 5-point rating scale and pairwise comparison annotations. For the rating scale data, regressions were built using the approach in (Andryushechkin et al., 2017). This model consists of an ensemble of two supervised models: an SVR (Support Vector Machine Regression) with n-gram and several custom features (see (Andryushechkin et al., 2017)) and a BiLSTM (Bidirectional Long-Short Term Memory) model utilising 100 dimensional Glove word embeddings trained on tweets (Pennington et al., 2014). For the comparison data, an SVM (Support Vector Machine) was built using the same Glove word embeddings as features. The cross validation results shown in Table 5 indicate that supervised modelling can be effective for predicting emotions using this data. 6. Conclusi"
L18-1192,E17-2092,0,0.0433944,"ular with dimensional annotations. Existing text corpora with dimensional emotion annotations include Affective Norms for English Texts (Bradley and Lang, 2007), a collection of 120 generic texts with VAD annotations; a collection of 2,895 Facebook posts annotated by just two annotators with valence and arousal dimensions (Preotiuc-Pietro et al., 2016). Yu et.al. (2016) presented a collection of 2009 Chinese sentences from various online texts, again annotated with valence and arousal only. Subsequent to our annotation efforts, several further annotated data sets have been published: EMOBANK (Buechel and Hahn, 2017), a collection of ten thousand texts from diverse sources, but not including tweets, and data for the upcoming “Affect in Tweets” task for SemEval 20181 which presents tweets annotated for valence, arousal and dominance in English, Spanish and Arabic. In addition, two recent data sets annotated with emotion intensity in Ekman emotion categories have also been released: data for the WASSA emotion intensity detection competition (Mohammad and Bravo-Marquez, 2017), 1,500 to 2,000 tweets for each of the four Ekman emotions joy, anger, sadness and fear; and further data from SemEval 2018. Several a"
L18-1192,P17-2074,0,0.368721,"ordinal scale, such as the SAM manikins (Bradley and Lang, 1994). It has been argued that human estimations of relative values are more consistent than when assigning an absolute value (Metallinou and Narayanan, 2013; Yannakakis et al., 2017). To address this, Martinez et.al. (2014) suggest that ranked annotations not be treated as absolute values, and instead treated as ordinal, and used, for example, to train ranking estimators. Another approach is to perform relative annotations directly, such as best/worst scaling, where the highest and lowest ranked tweets are chosen from a set of four (Kiritchenko and Mohammad, 2017). Pairwise tweet comparisons are another option, however we are not aware of this approach being used previously in the emotion annotation literature as it requires a large number of annotations to acquire a reasonable ranking. In this work, we present a collection of 2,019 tweets annotated following the four dimensional emotion representation scheme of Fontaine et.al. (2007). We further assess the relative merits of annotations on a ranking scale vs. comparisons, providing annotations using both a 5 point rank1197 1 http://alt.qcri.org/semeval2018 ing scale and pairwise comparisons2 . An init"
L18-1192,W17-5205,0,0.147475,"annotated with valence and arousal only. Subsequent to our annotation efforts, several further annotated data sets have been published: EMOBANK (Buechel and Hahn, 2017), a collection of ten thousand texts from diverse sources, but not including tweets, and data for the upcoming “Affect in Tweets” task for SemEval 20181 which presents tweets annotated for valence, arousal and dominance in English, Spanish and Arabic. In addition, two recent data sets annotated with emotion intensity in Ekman emotion categories have also been released: data for the WASSA emotion intensity detection competition (Mohammad and Bravo-Marquez, 2017), 1,500 to 2,000 tweets for each of the four Ekman emotions joy, anger, sadness and fear; and further data from SemEval 2018. Several approaches to annotating emotion expressed in text on a continuous scale have been used. Probably the most common utilises an ordinal scale, such as the SAM manikins (Bradley and Lang, 1994). It has been argued that human estimations of relative values are more consistent than when assigning an absolute value (Metallinou and Narayanan, 2013; Yannakakis et al., 2017). To address this, Martinez et.al. (2014) suggest that ranked annotations not be treated as absolu"
L18-1192,passonneau-2004-computing,0,0.163964,"rics operate on a similar scale (i.e.: values for conceptually similar annotation differences should be the same). In this work, we do not attempt to empirically evaluate these disagreement metrics beyond comparison of agreement values on the presented data sets. 4.1. Annotation Difference Metrics Categorical Annotations (Multiple Categories Allowed) There are several metrics that have been applied to categorical annotations with multiple categories allowed. The Jacccard set similarity metric (Jaccard, 1912) is the ratio between the sizes of the intersection and union of the sets. Passonneau (Passonneau, 2004) observed that if one annotator is inclined to provide, in general, more labels than another annotator, you should consider any extra labels from the prolific annotator as less indicative of disagreement, proposing a simple difference metric that attempts to capture this idea (see below). Passonneau later proposed a combination of the two metrics (Passonneau, 2006), capturing the granularity of the Jaccard metric and the motivating principle of his previous proposal. He named this metric MASI (Measuring Agreement on Set-valued Items). In the formulae below, A and B refer to two annotations of"
L18-1192,passonneau-2006-measuring,0,0.0544999,"ral metrics that have been applied to categorical annotations with multiple categories allowed. The Jacccard set similarity metric (Jaccard, 1912) is the ratio between the sizes of the intersection and union of the sets. Passonneau (Passonneau, 2004) observed that if one annotator is inclined to provide, in general, more labels than another annotator, you should consider any extra labels from the prolific annotator as less indicative of disagreement, proposing a simple difference metric that attempts to capture this idea (see below). Passonneau later proposed a combination of the two metrics (Passonneau, 2006), capturing the granularity of the Jaccard metric and the motivating principle of his previous proposal. He named this metric MASI (Measuring Agreement on Set-valued Items). In the formulae below, A and B refer to two annotations of a data element (tweet in our case), with each a set of annotated categories. Jacc(A, B) = 1 − |A ∩ B| |A ∪ B|  0    0.3 Pass(A, B) =  0.6    1 A=B A ⊂ B or B ⊂ A A ∩ B 6= ∅ A∩B =∅ Masi(A, B) = 1 − Jacc(A, B) × Pass(A, B) Another scenario, where the above metrics could be seen as overly pessimistic, is as follows: in cases where an anno1199 Metric Wood Masi"
L18-1192,D14-1162,0,0.0824505,"Missing"
L18-1192,W16-0404,0,0.0606471,"tion of positive/negative sentiment, while more nuanced emotion representation models have received relatively little attention. In particular, there has been a lack of quality annotated resources for model building and evaluation in that space (Mohammad, 2016) and in particular with dimensional annotations. Existing text corpora with dimensional emotion annotations include Affective Norms for English Texts (Bradley and Lang, 2007), a collection of 120 generic texts with VAD annotations; a collection of 2,895 Facebook posts annotated by just two annotators with valence and arousal dimensions (Preotiuc-Pietro et al., 2016). Yu et.al. (2016) presented a collection of 2009 Chinese sentences from various online texts, again annotated with valence and arousal only. Subsequent to our annotation efforts, several further annotated data sets have been published: EMOBANK (Buechel and Hahn, 2017), a collection of ten thousand texts from diverse sources, but not including tweets, and data for the upcoming “Affect in Tweets” task for SemEval 20181 which presents tweets annotated for valence, arousal and dominance in English, Spanish and Arabic. In addition, two recent data sets annotated with emotion intensity in Ekman emo"
L18-1192,N16-1066,0,0.0670196,"Missing"
L18-1324,S16-1168,1,0.928904,"Missing"
L18-1324,S16-1205,0,0.0176911,"most important goals of such research and several methods going back to (Hearst, 1992) have been proposed for this task. A recent such system to use this is the TAXI system (Panchenko et 2059 al., 2016), which combined simple string substring metrics with Hearst-like patterns learned from text, which are then constructed into a taxonomy using Tarjan’s algorithm (Tarjan, 1972). A different approach, by (Tan et al., 2016), used the endocentricity of a term, that is if a term contains another term, e.g., ‘fish’ in ‘goldfish’, whether this indicates a hypernymlike relationship. The QASSIT system (Cleuziou and Moreno, 2016) used the genetic algorithm in order to learn taxonomic relations, however performance across domains was poor. BabelNet (Navigli and Ponzetto, 2012), a wide coverage dictionary, has been used both as a source of information about taxonomic relations (Maitra and Das, 2016) and also itself was constructed using automatic taxonomy learning (Navigli et al., 2011). However, the focus of this has been mostly on single words as would be found in a dictionary and less on the kind of multi-word terminology that can be used to describe specialist domains as in this paper. Finally, there has been some w"
L18-1324,P14-1113,0,0.0231681,"rformance across domains was poor. BabelNet (Navigli and Ponzetto, 2012), a wide coverage dictionary, has been used both as a source of information about taxonomic relations (Maitra and Das, 2016) and also itself was constructed using automatic taxonomy learning (Navigli et al., 2011). However, the focus of this has been mostly on single words as would be found in a dictionary and less on the kind of multi-word terminology that can be used to describe specialist domains as in this paper. Finally, there has been some work on the use of word embeddings to predict hypernym relations, such as in (Fu et al., 2014), where a linear project function from a word embedding to its hypernym was constructed. The possibility of combining this with syntactic patterns for hypernym discovery has also been investigated (Shwartz et al., 2016). 3. Methodology Our methodology is based on the use of multiple features that can be extracted from the labels or an associated corpus. We then learn to combine these using a supervised learning approach. In this section we will present the methodology for the features we used. 3.1. Data While, the TexEval task has recently given a baseline, by which performance on this task ca"
L18-1324,C92-2082,0,0.587432,"ection of texts and secondly, in the taxonomy learning step these terms are grouped into a hierarchical structure. The first step is well explored and recent strong results have been shown on this task (Astrakhantsev, 2016; Buitelaar et al., 2013) and as such we shall not focus on it in the course of this article. The second task is however much less well-explored and this is the focus of this article, and so we assume that the terms have already been identified by an approach such as those outlines above. The task of taxonomy extraction is closely related to tasks such as hypernym detection (Hearst, 1992) or ontology learning (Buitelaar et al., 2005), in which a structured representation of concepts should be learned. However, the task of taxonomy extraction does not have the formal nature 1 http://www.acm.org/publications/ class-2012 that either of these tasks in that the terms only need to be loosely associated. For examples, taxonomies frequently place terms under broader concepts that do not match the strict requirements of ontological subsumption (Gangemi et al., 2002) that would be required from an ontology, e.g., grouping “Kalman Filter” is under “Filtering” in the IEEE taxonomy, where"
L18-1324,S16-1204,0,0.0127014,"earned from text, which are then constructed into a taxonomy using Tarjan’s algorithm (Tarjan, 1972). A different approach, by (Tan et al., 2016), used the endocentricity of a term, that is if a term contains another term, e.g., ‘fish’ in ‘goldfish’, whether this indicates a hypernymlike relationship. The QASSIT system (Cleuziou and Moreno, 2016) used the genetic algorithm in order to learn taxonomic relations, however performance across domains was poor. BabelNet (Navigli and Ponzetto, 2012), a wide coverage dictionary, has been used both as a source of information about taxonomic relations (Maitra and Das, 2016) and also itself was constructed using automatic taxonomy learning (Navigli et al., 2011). However, the focus of this has been mostly on single words as would be found in a dictionary and less on the kind of multi-word terminology that can be used to describe specialist domains as in this paper. Finally, there has been some work on the use of word embeddings to predict hypernym relations, such as in (Fu et al., 2014), where a linear project function from a word embedding to its hypernym was constructed. The possibility of combining this with syntactic patterns for hypernym discovery has also b"
L18-1324,S16-1206,0,0.0322349,"Missing"
L18-1324,P16-1226,0,0.0441135,"Missing"
L18-1383,P06-4018,0,0.0626103,"olve real-world problems. However, it is frequently the case that these components are developed independently and thus far integration of these services is far from trivial. The installation of these services can act as a significant barrier to entry for NLP developers and even once developed these pipelines can be opaque and brittle. These issues are of course endemic to software development and until recently could only be solved by integrating all components within a single development model, for example, such as integrating all NLP tools using the Python language as has been done by NLTK(Bird, 2006). An alternative model has arisen in the form of Web services that provide integration between multiple components through clear and defined protocols such as REST. However, Web services have generally not been adopted by researchers or industry, in part due to the fact that the remote nature of the computation can lead to issues with the availability of services (as external services are often down) and the speed of these services (as sending requests to servers creates significant bottlenecks). In this paper, we propose a new platform called Teanga1 , which aims to achieve the best of both w"
L18-1383,P10-4004,0,0.0340285,"rchitecture, multiple frameworks, toolkits, and suites have been created, and each of them 2 ‘teanga’ [""tj aNg@] means ‘language’ in Irish 2410 https://json-ld.org/ • Use URIs to identify all types of data items, for example, if we have a dataset of papers, we would use a unique URI for each paper. uses a different approach to creating interoperability among all their services, and, by that, reduce the amount of manual work needed to process data. Among these are the LAPPS Grid (Ide et al., 2015) and its Galaxy front-end (Ide et al., 2016), GernEdiT: A Graphical Tool for GermaNet Development (Henrich and Hinrichs, 2010), Language Grid: An Infrastructure for Intercultural Collaboration (Ishida, 2006), and Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004). Some problem with the applications of other platforms is that some of them only run on a desktop machine or rely on a platform-specific program, e.g. Eclipse plugins. For example, in the case of UIMA, it’s only a middleware architecture to be taken into account while developing a new NLP tool. For example, it doesn’t provide the user with an interface to process data. UIMA also is like GATE when it comes to the complexity of"
L18-1383,L16-1073,0,0.0291147,"eads to the encoding of the data. 3. Related Work In the domain of NLP architecture, multiple frameworks, toolkits, and suites have been created, and each of them 2 ‘teanga’ [""tj aNg@] means ‘language’ in Irish 2410 https://json-ld.org/ • Use URIs to identify all types of data items, for example, if we have a dataset of papers, we would use a unique URI for each paper. uses a different approach to creating interoperability among all their services, and, by that, reduce the amount of manual work needed to process data. Among these are the LAPPS Grid (Ide et al., 2015) and its Galaxy front-end (Ide et al., 2016), GernEdiT: A Graphical Tool for GermaNet Development (Henrich and Hinrichs, 2010), Language Grid: An Infrastructure for Intercultural Collaboration (Ishida, 2006), and Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004). Some problem with the applications of other platforms is that some of them only run on a desktop machine or rely on a platform-specific program, e.g. Eclipse plugins. For example, in the case of UIMA, it’s only a middleware architecture to be taken into account while developing a new NLP tool. For example, it doesn’t provide the user with an int"
miguel-buitelaar-2008-domain,P95-1026,0,\N,Missing
N13-2006,S12-1095,1,0.710787,"Missing"
N13-2006,W12-4201,0,0.0119607,", the corpus was used to generate feature vectors on the basis of the contextual information provided by surrounding words. Finally we calculate the semantic similarity between the extracted information from the parallel corpus and the ontology vocabulary. Related Work Word sense disambiguation (WSD) systems generally perform on the word level, for an input word they generate the most probable meaning. On the other hand, state of the art translation systems operate on sequences of words. This discrepancy between unigrams versus n-grams was first described in (Carpuat and Wu, 2005). Likewise, (Apidianaki et al., 2012) use a WSD classifier to generate a probability distribution of phrase pairs and to build a local language model. They show that the classifier does not only improve the translation of ambiguous words, but also the translation of neighbour words. We investigate this discrepancy as part of our research in enriching the ontology label translation with ontological information. Similar to their work we incorporate the idea of enriching the translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate a lexicon that predicts the bag of o"
N13-2006,P05-1048,0,0.0285816,"table and language model. Further, the corpus was used to generate feature vectors on the basis of the contextual information provided by surrounding words. Finally we calculate the semantic similarity between the extracted information from the parallel corpus and the ontology vocabulary. Related Work Word sense disambiguation (WSD) systems generally perform on the word level, for an input word they generate the most probable meaning. On the other hand, state of the art translation systems operate on sequences of words. This discrepancy between unigrams versus n-grams was first described in (Carpuat and Wu, 2005). Likewise, (Apidianaki et al., 2012) use a WSD classifier to generate a probability distribution of phrase pairs and to build a local language model. They show that the classifier does not only improve the translation of ambiguous words, but also the translation of neighbour words. We investigate this discrepancy as part of our research in enriching the ontology label translation with ontological information. Similar to their work we incorporate the idea of enriching the translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate"
N13-2006,W11-2107,0,0.0138112,"780169232 0.0358268041 0.0341965597 0.0273327211 0.0266209669 Europarl parallel corpus into smaller (n-gram) training sets, whereby no training set outperforms significantly the baseline approach. Table 5: Top five re-ranked translations after calculating the Jaccard similarity 5 Evaluation Our evaluation was conducted on the translations generated by the baseline approach, using only Europarl, and the ontology-enhanced translations of financial labels. We undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor6 (Denkowski and Lavie, 2011) algorithms. 5.1 Baseline Evaluation of general corpus At the beginning of our experiment, we translated the financial labels with the Moses Toolkit, where the translation model was generated from the English-German Europarl aligned corpus. The results are shown in Table 7 as baseline. 5.2 Baseline Evaluation of filtered general corpus A second evaluation on translations was done on a filtered Europarl corpus, depending if a sentence holds the vocabulary of the ontology to be translated. We generated five training sets, based on n-grams of the ontology vocabulary (from unigram to 5-gram) appea"
N13-2006,P03-1054,0,0.00365331,"logy German GAAP targeted label: Equity-equivalent partner loans contextual information: capital (6), reserve (3), loss (3), balance sheet (2) . . . currency translation (1), negative consolidation difference (1), profit (1) Table 3: Contextual information for the financial label Equity-equivalent partner loans 4.4 To compare the contextual information extracted from Europarl a similar approach was applied to the vocabulary in the German GAAP ontology. First, to avoid unnecessary segments, e.g. provisions for or losses from executory, we parsed the financial ontology with the Stanford parser (Klein and Manning, 2003) and extracted meaningful segments from the ontology labels. This step was done primarily to avoid comparing all possible n-gram segments with the filtered segments extracted from the Europarl corpus (cf. Subsection 4.2). With the syntactical information given by the Stanford parser we extracted a set of noun segments for the ontology labels, which we defined by the rules shown in Table 2. # 1 2 3 4 5 executory contracts (pattern 3), provisions for expected losses and expected losses from executory contracts (pattern 5). In the next step, for all 2794 labels from the financial ontology, a uniq"
N13-2006,2005.mtsummit-papers.11,0,0.0128442,"rom a linguistic point of view. They are used in financial or accounting reports as unique financial expressions or identifiers to organise and retrieve the reported information automatically. Therefore it is important to translate these financial labels with exact meaning preservation. 3 http://www.xbrl.de/ 3.2 Europarl As a baseline approach we used the Europarl parallel corpus,4 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 40 million English and 43 million German tokens (Koehn, 2005). Although previous research showed that a translation model built by using a general parallel corpus cannot be used for domain-specific vocabulary translation (Wu et al., 2008), we decided to train a baseline translation model on this general corpus to illustrate any improvement steps gained by enriching the standard approach with the semantic information of the ontology vocabulary and structure. 4 Experiment Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromt"
N13-2006,P07-2045,0,0.00540875,"h with the semantic information of the ontology vocabulary and structure. 4 Experiment Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromthe Europarl corpus a new resource with contextual information of surrounding words as feature vectors (Section 4.2). A similar approach was done with the ontology structure and vocabulary (Section 4.3). 4.1 Moses toolkit To translate the English financial labels into German, we used the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. 4.2 Building the contextual-semantic resource from the parallel corpus Europarl To enhance the baseline approach with additional semantic information, we built a new resource of contextual information from Europarl. From the original phrase table, which was generated from the Europarl corpus, we used the subphrase table, which was generated to translate the German GAAP financial ontology in the baseline approach. Although this sub-ph"
N13-2006,D09-1022,0,0.030141,"bed in (Carpuat and Wu, 2005). Likewise, (Apidianaki et al., 2012) use a WSD classifier to generate a probability distribution of phrase pairs and to build a local language model. They show that the classifier does not only improve the translation of ambiguous words, but also the translation of neighbour words. We investigate this discrepancy as part of our research in enriching the ontology label translation with ontological information. Similar to their work we incorporate the idea of enriching the translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate a lexicon that predicts the bag of output words from the bag of input words. In their research, no alignment between input and output words is used, words are chosen based on the input context. The word predictions of the input sentences are considered as an additional feature that is used in the decoding process. This feature defines a new probability score that favours the translation hypothesis containing words, which were predicted by the lexicon model. Similarly, (Patry and Langlais, 2011) train a model by translating a bagof-words. In contrast to their work, our approach uses b"
N13-2006,W11-1013,0,0.271968,"nput and output words is used, words are chosen based on the input context. The word predictions of the input sentences are considered as an additional feature that is used in the decoding process. This feature defines a new probability score that favours the translation hypothesis containing words, which were predicted by the lexicon model. Similarly, (Patry and Langlais, 2011) train a model by translating a bagof-words. In contrast to their work, our approach uses bag-of-word information to enrich the missing contextual information that arises from translating ontology labels in isolation. (McCrae et al., 2011) exploit in their research 41 3 Data sets 3.1 Financial ontology For our experiment we used the financial ontology German GAAP (Generally Accepted Accounting Practice),3 which holds 2794 concepts with labels in German and English. Balance sheet ... Total equity and liabilities ... Equity Equity-equivalent partner loans Revenue reserves Legal reserve ... Legal reserve, of which transferred from prior year net retained profits Figure 1: The financial label Equity-equivalent partner loans and its neighbours in the German GAAP ontology As seen in Figure 1 the financial labels do not correspond to"
N13-2006,J03-1002,0,0.00307123,"ent Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromthe Europarl corpus a new resource with contextual information of surrounding words as feature vectors (Section 4.2). A similar approach was done with the ontology structure and vocabulary (Section 4.3). 4.1 Moses toolkit To translate the English financial labels into German, we used the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. 4.2 Building the contextual-semantic resource from the parallel corpus Europarl To enhance the baseline approach with additional semantic information, we built a new resource of contextual information from Europarl. From the original phrase table, which was generated from the Europarl corpus, we used the subphrase table, which was generated to translate the German GAAP financial ontology in the baseline approach. Although this sub-phrase table holds only segments necessary to translate the financial labels, it sti"
N13-2006,P02-1040,0,0.0858052,"Target label Eigenkapital Equity Kapitalbeteiligung Gleichheit Gerechtigkeit Jaccard 0.0780169232 0.0358268041 0.0341965597 0.0273327211 0.0266209669 Europarl parallel corpus into smaller (n-gram) training sets, whereby no training set outperforms significantly the baseline approach. Table 5: Top five re-ranked translations after calculating the Jaccard similarity 5 Evaluation Our evaluation was conducted on the translations generated by the baseline approach, using only Europarl, and the ontology-enhanced translations of financial labels. We undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor6 (Denkowski and Lavie, 2011) algorithms. 5.1 Baseline Evaluation of general corpus At the beginning of our experiment, we translated the financial labels with the Moses Toolkit, where the translation model was generated from the English-German Europarl aligned corpus. The results are shown in Table 7 as baseline. 5.2 Baseline Evaluation of filtered general corpus A second evaluation on translations was done on a filtered Europarl corpus, depending if a sentence holds the vocabulary of the ontology to be translated. We generated f"
N13-2006,I11-1074,0,0.0123468,"e translation model with neighbour words information, whereby we extend the window to 5-grams. (Mauser et al., 2009) generate a lexicon that predicts the bag of output words from the bag of input words. In their research, no alignment between input and output words is used, words are chosen based on the input context. The word predictions of the input sentences are considered as an additional feature that is used in the decoding process. This feature defines a new probability score that favours the translation hypothesis containing words, which were predicted by the lexicon model. Similarly, (Patry and Langlais, 2011) train a model by translating a bagof-words. In contrast to their work, our approach uses bag-of-word information to enrich the missing contextual information that arises from translating ontology labels in isolation. (McCrae et al., 2011) exploit in their research 41 3 Data sets 3.1 Financial ontology For our experiment we used the financial ontology German GAAP (Generally Accepted Accounting Practice),3 which holds 2794 concepts with labels in German and English. Balance sheet ... Total equity and liabilities ... Equity Equity-equivalent partner loans Revenue reserves Legal reserve ... Legal"
N13-2006,2006.amta-papers.25,0,0.0348069,"leichheit Gerechtigkeit Jaccard 0.0780169232 0.0358268041 0.0341965597 0.0273327211 0.0266209669 Europarl parallel corpus into smaller (n-gram) training sets, whereby no training set outperforms significantly the baseline approach. Table 5: Top five re-ranked translations after calculating the Jaccard similarity 5 Evaluation Our evaluation was conducted on the translations generated by the baseline approach, using only Europarl, and the ontology-enhanced translations of financial labels. We undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor6 (Denkowski and Lavie, 2011) algorithms. 5.1 Baseline Evaluation of general corpus At the beginning of our experiment, we translated the financial labels with the Moses Toolkit, where the translation model was generated from the English-German Europarl aligned corpus. The results are shown in Table 7 as baseline. 5.2 Baseline Evaluation of filtered general corpus A second evaluation on translations was done on a filtered Europarl corpus, depending if a sentence holds the vocabulary of the ontology to be translated. We generated five training sets, based on n-grams of the ontology"
N13-2006,C08-1125,0,0.0214557,"tion automatically. Therefore it is important to translate these financial labels with exact meaning preservation. 3 http://www.xbrl.de/ 3.2 Europarl As a baseline approach we used the Europarl parallel corpus,4 which holds proceedings of the European Parliament in 21 European languages. We used the English-German parallel corpus with around 1.9 million aligned sentences and 40 million English and 43 million German tokens (Koehn, 2005). Although previous research showed that a translation model built by using a general parallel corpus cannot be used for domain-specific vocabulary translation (Wu et al., 2008), we decided to train a baseline translation model on this general corpus to illustrate any improvement steps gained by enriching the standard approach with the semantic information of the ontology vocabulary and structure. 4 Experiment Since ontology labels (or label segments) translated by the Moses toolkit (Section 4.1) do not have much contextual information, we addressed this lack of information and generated fromthe Europarl corpus a new resource with contextual information of surrounding words as feature vectors (Section 4.2). A similar approach was done with the ontology structure and"
P15-1069,C12-1005,1,0.864122,"ds of the current label are combined with the related words of its direct parent in the ontology. The usage of the ontology hierarchy allows us to take advantage of the specific vocabulary of the related words in the computation of the cosine similarity. Given a label and a source sentence from the generic corpus, related words and their weights are extracted from both of them and used as entries of the vectors passed to the cosine similarity. The most similar source sentence and the label should share the largest number of related words (largest cosine similarity). 3.2 We engage the idea of (Arcan et al., 2012) where the authors provide to the SMT system unambiguous terminology identified in Wikipedia to improve the translations of labels in the financial domain. To disambiguate Wikipedia entries with translations into different domains, they query the repository for analysing the n-gram overlap between the financial labels and the Wikipedia entries and store the frequency of categories which are associated with the matched entry. In a final step they extract only bilingual Wikipedia entries, which are associated with the most frequent Wikipedia categories identified in the previous step. Since the"
P15-1069,W14-4803,1,0.799714,"Missing"
P15-1069,W14-3348,0,0.0275309,"domainspecific models to evaluate different approaches when combining generic and domain-specific data together. We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA12 (Tiedemann, 2009), which holds specific medical parallel data extracted from the European Medicines Agency documents and websites. Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) algorithms.15 BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Considering the shortness of the labels, we report scores based on the bi-gram overlap (BLEU-2) and the standard four-gram overlap (BLEU-4). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on the harmonic mean of precision and recal"
P15-1069,2014.amta-researchers.5,1,0.895111,"its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual 710 this reason, we improve it by extending the semantic information of labels using a technique for computing vector representations of words. The technique is based on a neural network that analyses the textual data provided as input and provides as output a list of semantically related words (Mikolov et al., 2013). Each input string is vectorized using the surrounding context and compared to other vectorized sets of words (from the training data) in a multi-dimensi"
P15-1069,eck-etal-2004-language,0,0.0142663,"t labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select Translation performed on 25.02.2015 709 knowledge. This research showed that using the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wi"
P15-1069,D11-1033,0,0.0218699,"tions of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wikipedia article. relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown i"
P15-1069,2013.mtsummit-papers.5,0,0.0612,"l corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilingual knowledge is not present in the parallel dataset. To complement the previous approaches we access DBpedia6 as a multilingual lexical resource. 4 Wikipedia dump id enwiki-20141106 https://code.google.com/p/word2vec/ 6 http://wiki.dbpedia.org/Downloads2014 5 711 Model (Bertoldi et al., 2013) approach. 4 Experimental Setting The Fill-Up model has been developed to address a common scenario where a large generic background model exists, and only a small quantity of domain-specific data can be used to build a translation model. Its goal is to leverage the large coverage of the background model, while preserving the domain-specific knowledge coming from the domain-specific data. For this purpose the generic and the domain-specific translation models are merged. For those translation candidates that appear in both models, only one instance is reported in the Fill-Up model with the lar"
P15-1069,W11-2131,0,0.013545,"4) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown in (Cuong and Sima’an, 2014), where the authors propose a latent domain translation model to distinguish between hidden in- and out-of-domain data. (Gasc´o et al., 2012) and (Bicici and Yuret, 2011) sub-sample sentence pairs whose source has most overlap with the evaluation dataset. Different from these approaches, we do not embed any specific in-domain knowledge to the generic corpus, from which sentence selection is performed. Furthermore, none of these methods explicitly exploit the ontological hierarchy for label disambiguation and are not specifically designed to deal with the characteristics of ontology labels. 3 Methodology We propose an approach that uses the ontology labels to be translated to select the most relevant parallel sentences from a generic parallel corpus. Since onto"
P15-1069,E12-1016,0,0.0314405,"Missing"
P15-1069,2011.iwslt-evaluation.18,0,0.0125056,"rated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilingual knowledge is not present in the parallel dataset. To complement the previous approaches we access DBpedia6 as a multilingual lexical resource. 4 Wikipedia dump id enwiki-20141106 https://code.google.com/p/word2vec/ 6 http://wiki.dbpedia.org/Downloads2014 5 711 Model (Bertoldi et al., 2013) approach. 4 Experimental Setting The Fill-Up model has been developed to address a common scenario where a large generic background m"
P15-1069,bouamor-etal-2012-identifying,0,0.0126692,"dia entry has an equivalent in the target language, i.e. German, we use the bilingual information for the lexical enhancement of the SMT system. 3.3 Integration of Domain-Specific Knowledge into SMT After the identification of domain-specific bilingual knowledge, it has to be integrated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilingual knowledge is not present in the parallel dataset. To complement the previous approaches we access DBpedia6 as a multilingual lexical resource."
P15-1069,W12-3154,0,0.0682286,"e translations using a small, but ontology-specific SMT system. We learned that using external SMT services often results in wrong translations of labels, because the external SMT services are not able to adapt to the specificity of the ontology. Avoiding existing multilingual resources, which enables a simple replacement of source and target labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select Translation performed on 25.02.2015 709 knowledge. This research showed tha"
P15-1069,P11-2031,0,0.0431554,"perplexity score gives a notion of how well the probability model based on the ontology vocabulary predicts a sample, which is in our case each sentence in the generic corpus. Second, we use the method shown in (Hildebrand et al., 2005), where the authors use a method 13 11 tf-idf – term frequency-inverse document frequency http://iate.europa.eu/downloadTbx.do 15 METEOR configuration: exact, stem, paraphrase For reproducibility and future evaluation we take the first one-third part of each corpus. 12 http://opus.lingfil.uu.se/EMEA.php 14 713 The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value < 0.05. 5 Dataset Type Evaluation of Ontology Labels In this Section, we report the translation quality of ontology labels based on translation systems learned from different sentence selection methods. Additionally, we perform experiments training an SMT system on the combination of in- and outdomain knowledge. The final approach enhances a domain-specific translation system with lexical knowledge identified in IATE or DBpedia. 5.1 Size BLEU-2 BLEU-4 METEOR Generic dataset 1.9M EMEA dat"
P15-1069,2005.eamt-1.19,0,0.0505213,"Missing"
P15-1069,C14-1182,0,0.0444323,"Missing"
P15-1069,declerck-etal-2006-multilingual,0,0.649534,"Missing"
P15-1069,D14-1014,0,0.0128301,"that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown in (Cuong and Sima’an, 2014), where the authors propose a latent domain translation model to distinguish between hidden in- and out-of-domain data. (Gasc´o et al., 2012) and (Bicici and Yuret, 2011) sub-sample sentence pairs whose source has most overlap with the evaluation dataset. Differen"
P15-1069,P02-1040,0,0.0948144,"tem in combination with the smaller domainspecific models to evaluate different approaches when combining generic and domain-specific data together. We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA12 (Tiedemann, 2009), which holds specific medical parallel data extracted from the European Medicines Agency documents and websites. Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) algorithms.15 BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Considering the shortness of the labels, we report scores based on the bi-gram overlap (BLEU-2) and the standard four-gram overlap (BLEU-4). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on t"
P15-1069,2005.mtsummit-papers.11,0,0.0478815,"age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English Generic Dataset (out-domain) German based on tf-idf 13 to select the most relevant sentences. This widely-used method in information retrieval tells us how important a word is to a document, whereby each sentence from the generic corpus is treated as a document. Finally, we compare our approach wi"
P15-1069,P07-2045,0,0.00491823,"uses an ageing of the existing translation candidates and hence their re-scoring; in case of re-insertion of a phrase pair, the old value is set to the initial value. Similarly to the CBTM, the local language model is built to give preference to the provided target expressions. Each entry stored in CBLM is associated with a decaying function of the age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lin"
P15-1069,W09-2907,0,0.0250648,"Missing"
P15-1069,W02-1405,0,0.0100846,"ous entries, the cosine similarity gives more weight to the Wikipedia entry, which is closer to our preferred domain. Finally, if the Wikipedia entry has an equivalent in the target language, i.e. German, we use the bilingual information for the lexical enhancement of the SMT system. 3.3 Integration of Domain-Specific Knowledge into SMT After the identification of domain-specific bilingual knowledge, it has to be integrated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilin"
P15-1069,steinberger-etal-2006-jrc,0,0.0179494,"sociated with a decaying function of the age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English Generic Dataset (out-domain) German based on tf-idf 13 to select the most relevant sentences. This widely-used method in information retrieval tells us how important a word is to a document, whereby each sentence from the generic corpus is treated as a document. Finally"
P15-1069,D07-1036,0,0.0550878,"Missing"
P15-1069,W11-1013,0,0.59252,"Missing"
P15-1069,tiedemann-2012-parallel,0,0.0488733,"Missing"
P15-1069,P10-2041,0,0.0156483,"sing the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wikipedia article. relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target sid"
P15-1069,2011.iwslt-papers.6,0,0.0139458,"resource, Wikipedia with its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual 710 this reason, we improve it by extending the semantic information of labels using a technique for computing vector representations of words. The technique is based on a neural network that analyses the textual data provided as input and provides as output a list of semantically related words (Mikolov et al., 2013). Each input string is vectorized using the surrounding context and compared to other vectorized sets of words (from the training data"
P15-1069,C08-1125,0,0.0116254,"w to gain adequate translations using a small, but ontology-specific SMT system. We learned that using external SMT services often results in wrong translations of labels, because the external SMT services are not able to adapt to the specificity of the ontology. Avoiding existing multilingual resources, which enables a simple replacement of source and target labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select Translation performed on 25.02.2015 709 knowledge."
P15-1069,J03-1002,0,0.00748084,"n case of re-insertion of a phrase pair, the old value is set to the initial value. Similarly to the CBTM, the local language model is built to give preference to the provided target expressions. Each entry stored in CBLM is associated with a decaying function of the age of insertion into the model. Both models are used as additional features of the log-linear model in the SMT system. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7 http://www.who.int/classifications/ icd/en/ 8 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9 http://www.statmt.org/europarl/ 10 http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English Generic Dataset (out-domain) German"
P15-1069,2014.amta-researchers.15,0,0.0265574,"Missing"
qasemizadeh-etal-2012-semi,councill-etal-2008-parscit,0,\N,Missing
qasemizadeh-etal-2012-semi,I08-7002,0,\N,Missing
qasemizadeh-etal-2012-semi,bird-etal-2008-acl,0,\N,Missing
qasemizadeh-etal-2012-semi,W97-0313,0,\N,Missing
qasemizadeh-etal-2012-semi,W00-1308,0,\N,Missing
qasemizadeh-etal-2012-semi,C92-2070,0,\N,Missing
qasemizadeh-etal-2012-semi,C02-1154,0,\N,Missing
qasemizadeh-etal-2012-semi,E06-3004,0,\N,Missing
qasemizadeh-etal-2012-semi,W03-3017,0,\N,Missing
qasemizadeh-etal-2012-semi,J08-3010,0,\N,Missing
raileanu-etal-2002-evaluation,J96-2004,0,\N,Missing
raileanu-etal-2002-evaluation,vintar-etal-2002-efficient,1,\N,Missing
S01-1012,H92-1045,0,0.154876,"Missing"
S01-1012,W93-0106,0,0.11459,"Missing"
S01-1012,P91-1019,0,0.10593,"es like MD, for the medical domain, or ML, for meteorology are used to define which senses of a word are used in which domains. Three of the senses of the word high for instance correspond to three different domains: music (a high tone), drugs (the experience of being high) and meteorology (a high pressure area). 49 (doctor#l, hospital#!; operate#?) and across subhierarchies, as in the sports domain (life_form#l: athlete# I; physical_object#l: game_equipment#l; act#2: sport#l; location#!: playing_field#l). 2.2 Topic Signatures and Variation The topic specific context models as constructed by (Guthrie et al. 1991) can be viewed as ""signatures"" of the topic in question. Such topic signatures can, however, be constructed even without the use of subject codes by generating them (semi-) automatically from a lexical resource and then validating them on topic specific corpora (Hearst and Schi.itze 1993). An extension of this idea is to treat senses, or rather WordNet synsets, as topics for which a signature can be constructed. One approach to this is to retrieve relevant documents through search engines on the web by defining queries for each synset (Agirre et al. 2000, Agirre et al. 2001). For instance, the"
S01-1012,magnini-cavaglia-2000-integrating,0,0.0284556,"Missing"
S01-1012,W00-1101,0,\N,Missing
S01-1012,W00-1322,0,\N,Missing
S01-1012,W00-1326,0,\N,Missing
S10-1030,N04-4005,0,0.015292,"jacent keywords are merged into keyphrases in a post-processing step. The frequency of noun phrase heads is exploited by Barker and Cornacchia (2000), using noun phrases as candidates and ranking them based on term frequency and term length. Kea is a supervised system that uses all n-grams of a certain length, a Naive Bayes classifier and tf-idf and position features (Frank et al., 1999). Turney (2000) introduces Extractor, a supervised system that selects stems and stemmed n-grams as candidates and tunes its parameters (mainly related to frequency, position, length) with a genetic algorithm. Hulth (2004) experiments with three types of candidate terms (i.e., n-grams, noun phrase chunks and part-of-speech tagged words Introduction Keyphrases provide users overwhelmed by the richness of information currently available with useful insight into document content but at the same time they are a valuable input for a variety of NLP applications such as summarization, clustering and searching. The SemEval 2010 competition included a task targeting the Automatic Keyphrase Extraction from Scientific Articles (Kim et al., 2010). Given a set of scientific articles participants are required to assign to ea"
S10-1030,C08-2021,0,0.039054,"Missing"
S10-1030,W03-1805,0,0.324952,"Missing"
S10-1030,W04-3252,0,\N,Missing
S10-1030,S10-1004,0,\N,Missing
S12-1095,P94-1019,0,\N,Missing
S12-1095,S12-1051,0,\N,Missing
S14-1006,S12-1051,0,0.0140741,"ined above. The relatedness score between two natural language texts is calculated by computing cosine product of their corresponding ESA vectors. Total number of articles (N) 438379 110900 46035 23608 13718 8322 5241 3329 2126 1368 Table 1: The total number of retrieved articles for different values of K 3 Investigation of ESA model Although Gortton et al. (2011) has shown that ESA performance on document pairs does not get affected by using different number of Wikipedia articles, we further examine it for word-word and phrase-phrase pairs. We use three different datasets WN353, SemEvalOnWN (Agirre et al., 2012) and Lee50. WN353 contains 353 word pairs, SemEvalOnWN consists of 750 short phrase/sentence pairs, and Lee50 is a collection of 50 document pairs. All these datasets contain relatedness scores given by human annotators. We evaluate ESA model on these three datasets against different number of Wikipedia articles. In order to select different number of Wikipedia articles, we sort them according to the total number of unique words appearing in each article. We select N articles, where N is total number of articles which have at least K unique words. Table 1 shows the total number of retrieved ar"
S14-1006,P11-1009,0,0.0279384,"oncept-concept correlation to improve the ESA model. Hassan and Mihalcea (2011) proposed a model similar to ESA, which builds the high dimensional vector of salient concepts rather than explicit concepts. Gortton et 52 4 Context Enrichment Context enrichment is performed by concatenating the context defining text to the given word before building the ESA vector. Therefore, instead of building the ESA vector of a word, the vector is built for the short text that is obtained after concatenating the related context. This is similar to classical query expansion task (Aggarwal and Buitelaar, 2012; Pantel and Fuxman, 2011), where related concepts are concatenated with a query to improve the information retrieval performance. We propose three different methods to obtain related context: 1) WordNetbased Context Enrichment 2) Wikipedia-based Context Enrichment, and 3) WikiDefinition-based Context Enrichment. 4.1 WordNet-based Context Enrichment WordNet-based context enrichment uses the WordNet synonyms to obtain the context, and concatenates them into the given word to build the ESA vector. However, WordNet may contain more than one synset for a word, where each synset represents a different semantic sense. Theref"
S14-1006,S12-1095,1,\N,Missing
S14-2058,W02-1011,0,0.0291152,"ment polarities of the words in the clause. This work analyses various syntactic and lexical features for sentence level aspect based sentiment analysis. The task focuses on detection of a writer’s sentiment towards an aspect which is explicitly mentioned in a sentence. The target sentiment polarities are positive, negative, conflict and neutral. We use a supervised learning approach, evaluate various features and report accuracies which are much higher than the provided baselines. Best features include unigrams, clauses, dependency relations and SentiWordNet polarity scores. 1 2 Related Work Pang et al. (2002) proved that unigrams and bigrams, adjectives and part of speech tags are important features for a machine learning based sentiment classifier. Later, verbs and adjectives were also identified as important features (Chesley, 2006). Meena and Prabhakar (2007) performed sentence level sentiment analysis using rules based on clauses of a sentence. However, in our case we cannot simply consider the adjectives and verbs as features, since they might relate to different aspects. For example, in the sentence ‘The pizza is the best if you like thin crusted pizza.’, sentiment towards ‘pizza’ is positiv"
S14-2058,S14-2004,0,0.0312329,"like’ would be the sentiment verb. Therefore, only those adjectives and verbs which relate to the target aspect, can be considered as the indicator of their polarity. Wilson et al. (2009) showed that the words which share certain dependency relations with aspect terms, tend to indicate the sentiments expressed towards those terms. Saif et al. (2012) showed the co-relation between topic and sentiment polarity in tweets, and asserted that majority of people tend to express similar sentiments towards same topics, especially in the case of positive sentiments. The baseline approach for this task (Pontiki et al., 2014) also associates polarity with aspect terms. Therefore, we also consider aspect term as a potential feature. Our approach for this task is based on our observation of the data, with a provenance of the above mentioned findings. Introduction The term aspect refers to the features or aspects of a product, service or topic being discussed in a text. The task of detection of sentiment towards these aspects involves two major processing steps, identifying the aspects in the text and identifying the sentiments towards these aspects. Our work describes a submitted system in the Aspect Based Sentiment"
S14-2058,J09-3003,0,0.0783309,"Missing"
S14-2058,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S15-1010,S14-1006,1,0.670657,"or implicit concepts. Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) utilizes the concepts which are explicitly derived under human cognition like Wikipedia concepts (articles). However, Latent Semantic Analysis (LSA) derives a latent concept space by performing dimensionality reduction (Landauer et al., 1998). Gabrilovich and Markovitch (2007) introduced ESA model in which Wikipedia and Open Directory Project were used to obtain the explicit concepts, however, Wikipedia has been a popular choice in further ESA implementations (Polajnar et al., 2013; Gottron et al., 2011; Aggarwal et al., 2014). ESA represents the semantics of a word with a high dimensional vector over the Wikipedia concepts. The tf-idf weight of the word with the textual content under a Wikipedia concept reflects the magnitude Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 92–100, Denver, Colorado, June 4–5, 2015. # 1 2 3 4 5 Table 1: Top 5 Wikipedia concepts for “football” and “soccer” in the ESA vector football FIFA Football History of association football Football in England Association football soccer History of soccer in the United States Soccer in the Unit"
S15-1010,N09-1003,0,0.0595084,"Missing"
S15-1010,H92-1045,0,0.458359,"not create the link for every surface form appearing in the article content. For instance, “Apple” occurs 213 times in “Steve Jobs” Wikipedia page in our corpus, but only 7 out of these 213 are linked to the “Apple Inc.” Wikipedia page. The term frequency of “Apple” is calculated without considering the partial string matches, for example, we do not count if “apple” appears as a substring of any annotated text segment like “Apple Store” or “Apple Lisa”. To obtain the actual frequency of every hyperlink for computing the magnitude of the dimension, we apply “one sense per discourse” heuristic (Gale et al., 1992), which assumes that a term tends to have the same meaning in the same discourse. We link every additional un-linked occurrence of the text segment with the same hyperlink appearing most of the times for the same segment in the article. The total number of hyperlinks possible in our corpus would be equal to the total number of Wikipedia articles i.e. 3,635,833. 5.1 In order to evaluate the concept relatedness measures, we performed our experiments on the gold standard benchmark dataset KORE (Hoffart et al., 2012). The KORE dataset consists of 21 seed Wikipedia concepts selected from the YAGO k"
S15-1010,O97-1002,0,0.656163,"Missing"
S15-1010,P94-1019,0,\N,Missing
S15-2151,baroni-bernardini-2004-bootcat,0,0.157355,"not submit a system for the Chemical domain and the QASSIT team, which submitted only one run for the WordNet Chemical taxonomy. Next, we will provide a short description of each approach in alphabetical order, discussing corpora collection and the approaches adopted for relation discovery and taxonomy construction. INRIASAC (supervised) Corpus: Wikipedia search using terms; Relation discovery: substring inclusion, lexico-syntactic patterns, co-occurrence information based on sentences and documents; Taxonomy construction: none. LT3 (unsupervised) Corpus: web corpus constructed using BootCat (Baroni and Bernardini, 2004) using the provided terms as seed terms; Re906 lation discovery: lexico-syntactic patterns, morphological structure of compound terms, WordNet lookup (Lefever et al., 2014); Taxonomy construction: none. ntnu (unsupervised) Corpus: Wikipedia and WordNet definitions; Relation discovery: hypernym extraction from definitions, WordNet lookup, Wikipedia categories, similarity between keywords; Taxonomy construction: none. QASIT (semi-supervised) Corpus: Wikipedia, DBpedia; Relation discovery: lexico-syntactic patterns, co-occurrence information; Taxonomy construction: Learning Pretopological Spaces"
S15-2151,P14-1089,1,0.666588,"produced two kinds of gold standard taxonomies. WordNet taxonomy Concepts and relationships in the WordNet hypernym-hyponym hierarchy rooted on the corresponding root concept. Combined taxonomy Domain-specific terms and relations from well-known, publicly available, taxonomies other than WordNet: CheBI1 for Chemicals, “The Google product taxonomy”2 for Foods, the “Material Handling Equipment”3 taxonomy for Equipment, and the “Taxonomy of Fields and their Subfields”4 for Science. Hypernym-hyponym relationships were also gathered from a general purpose resource, the Wikipedia Bitaxonomy (WiBi) (Flati et al., 2014), using a semi-automatic approach. For each domain we first manually identified domain sub-hierarchies from WiBi (W ); Second we automatically searched for the terms of W in common with the corresponding gold standard G. For each common term t we added in G the taxonomy rooted on t from W . Table 1 shows the resulting number of vertices |V |, i.e., the number of terms given to the participants, and the number of edges |E |of the produced gold standard taxonomies for the four target domains. Finally, test data consists of eight lists of domain concepts, for which participants were asked to outp"
S15-2151,P05-1014,0,0.027733,"Missing"
S15-2151,C92-2082,0,0.513933,"ethods for taxonomy enrichment and construction. Recently, the task of taxonomy learning from text, also called taxonomy induction, has received an increased interest in the natural language processing Taxonomy learning can be divided into three main subtasks: term extraction, relation discovery, and taxonomy construction. Term extraction is a relatively well-known task, hence we decided to abstract from this stage and provide a common ground for the next steps by making available the list of terms beforehand. Most approaches for relation discovery from text rely on lexico-syntactic patterns (Hearst, 1992; Kozareva et al., 2008), co-occurrence information (Sanderson and Croft, 1999), substring inclusion (Nevill-Manning et al., 1999), or exploit semantic relations provided in textual definitions (Navigli and Velardi, 2010). Any asymmetrical relation that indicates subordination between two terms can be considered, but here the focus is mainly on hyponym-hypernym relations. Depending on the approach selected, the task may or may not require large amounts of text to extract relations between terms, therefore no corpus is provided as part of the shared dataset. This stage usually produces a large"
S15-2151,D10-1108,0,0.307974,"Missing"
S15-2151,P08-1119,0,0.413658,"onomy enrichment and construction. Recently, the task of taxonomy learning from text, also called taxonomy induction, has received an increased interest in the natural language processing Taxonomy learning can be divided into three main subtasks: term extraction, relation discovery, and taxonomy construction. Term extraction is a relatively well-known task, hence we decided to abstract from this stage and provide a common ground for the next steps by making available the list of terms beforehand. Most approaches for relation discovery from text rely on lexico-syntactic patterns (Hearst, 1992; Kozareva et al., 2008), co-occurrence information (Sanderson and Croft, 1999), substring inclusion (Nevill-Manning et al., 1999), or exploit semantic relations provided in textual definitions (Navigli and Velardi, 2010). Any asymmetrical relation that indicates subordination between two terms can be considered, but here the focus is mainly on hyponym-hypernym relations. Depending on the approach selected, the task may or may not require large amounts of text to extract relations between terms, therefore no corpus is provided as part of the shared dataset. This stage usually produces a large number of noisy, inconsi"
S15-2151,P10-1134,1,0.252601,"omy learning can be divided into three main subtasks: term extraction, relation discovery, and taxonomy construction. Term extraction is a relatively well-known task, hence we decided to abstract from this stage and provide a common ground for the next steps by making available the list of terms beforehand. Most approaches for relation discovery from text rely on lexico-syntactic patterns (Hearst, 1992; Kozareva et al., 2008), co-occurrence information (Sanderson and Croft, 1999), substring inclusion (Nevill-Manning et al., 1999), or exploit semantic relations provided in textual definitions (Navigli and Velardi, 2010). Any asymmetrical relation that indicates subordination between two terms can be considered, but here the focus is mainly on hyponym-hypernym relations. Depending on the approach selected, the task may or may not require large amounts of text to extract relations between terms, therefore no corpus is provided as part of the shared dataset. This stage usually produces a large number of noisy, inconsistent relations, that assign multiple parents to a node and that contain cycles, i.e., sequences of vertices that start and end at the same vertex. Hence, the third stage of taxonomy learning, taxo"
S15-2151,velardi-etal-2012-new,1,0.815328,"t output taxonomies 6 7 Domain Root concept Chemicals Equipment Food Science chemical equipment food science Combined taxonomies WordNet taxonomies |V| |E| |V| |E| 17584 612 1156 452 24817 615 1587 465 1351 475 1486 429 1387 485 1533 441 comparative evaluation manual quality assessment Figure 1: The task workflow. 2010; Navigli et al., 2011; Wang et al., 2013). To address the inherent complexity of evaluating taxonomy quality, several methods have been considered in the past including manual evaluation by domain experts, structural evaluation, and automatic evaluation against a gold standard (Velardi et al., 2012). In this task, all these existing evaluation approaches are considered, using a voting scheme to aggregate the results for the final ranking of the systems. We introduce four new domains that have not previously been considered for this task, covering general knowledge domains such as food and equipment and technical domains such as chemicals and science. For each domain, we provide a gold standard taxonomy gathered exclusively from WordNet (Fellbaum, 2005), as well as a gold standard taxonomy that combines terms and relations gathered from other domain-specific sources. 2 Task workflow In th"
S15-2151,J13-3007,1,0.671564,"irected cycles (self loop included). We then use an approach based on the Tarjan algorithm (Tarjan, 1972) to calculate the number of connected components in S. Finally, we compute the number of intermediate nodes as the number of nodes |VS |− |LS |where LS is the set of leaf nodes in S. A leaf node is a node with out-degree = 0. 2.2.3 Comparison against Gold Standard Previous datasets for evaluating taxonomy extraction (Kozareva et al., 2008) mainly rely on WordNet to gather gold standards from several general knowledge domains, such as animals, plants, and vehicles. The datasets proposed in (Velardi et al., 2013) enrich this experimental setting by including two specialized domains, Virus and Artificial Intelligence, that have low coverage in WordNet. A limitation of these datasets is that currently there is no gold standard taxonomy for these domains, therefore only a manual evaluation is possible. The dataset introduced here, instead, covers four new domains, providing two separate gold standards for each domain: one collected from WordNet, a general purpose resource, and a second one that combines relations from domain-specific resources and from a collaborative resource, Wikipedia, for a higher co"
S16-1110,S15-1010,1,0.835823,"r sentence. We examine several methods that can be used to learn these alignments including word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and models based on deep learning that have been suggested for machine translation (Bahdanau et al., 2014; Cho et al., 2014). In addition, we look into recent models for sentence and document similarity that can leverage the large amount of loosely aligned text in particular those based on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and recent extensions aimed at generating orthogonal representations (McCrae et al., 2013; Aggarwal et al., 2015). While these novel techniques alone can achieve high performance on the task, we note that simple metrics such as the number of overlapping terms can produce reasonable performance. For added robustness we combine features based on simple metrics with novel methods explored in this work as a multi-feature regression problem, which we solve by means of an M5 Decision tree (Wang and Witten, 1996; Quinlan, 1992). The rest of the paper is structured as follows: we present our system in Section 2. We then present both our internal evaluation results and the official Task 1 results in Section 3 and"
S16-1110,S14-2010,0,0.0136638,"ard and soft alignments, and the ESA similarity, we combine all of our features into a single vector and thus transform the problem into that of a traditional regression task. We experimented with various classifiers using the Weka toolkit (Hall et al., 2009) and found that in nearly all experiments, the strongest performance was obtained using the M5 Decision Tree method (Wang and Witten, 1996; Quinlan, 1992) and so we adopted this for all our experiments. 3 Evaluation 3.1 Internal Evaluation 2.4 ESA Similarity We conducted a series of evaluations using data from previous SemEval challenges (Agirre et al., 2014) as a baseline as shown in Table 1. These results present the following configurations using 10-fold cross-validation: Gabrilovich and Markovitch (2007) introduced the ESA model that represents the semantics of a word Sultan Only (-DF) Using baseline features, which are also used in all experiments, and Sultan et 0 αij = αij df (ai )df (bj ) 715 al.’s (2014a) aligner. Without accounting for term document frequency (see Section 2.3.3) Sultan Only As above only with document frequency included as a feature Sultan + Jacana Including the Jacana aligner (Section 2.2) Sultan + WordSim Including the"
S16-1110,P14-1023,0,0.0203315,"t uses Conditional Random Field (CRF) model to globally decode the best alignment. It uses features based on WordNet and part-of-speech tags. 2.3 1 https://github.com/arunjeyapal/ GreedyStringTiling 713 Soft Alignment 2.3.1 WordSim Semantic relatedness measures can be directly used to compute the soft alignments between the sentences. In this approach, we compare the pretrained neural word embeddings to compute the relatedness between words across both the sentences, thus producing the soft alignment matrix. We use cosine similarity for this purpose. We use the neural embeddings5 developed by Baroni et al. (2014). 2 Source and Target Length The length (in tokens) of each of the two strings Hard alignment https://github.com/ma-sultan/ monolingual-word-aligner 3 For example in the pair “Being against nukes does not mean not wanting to use nukes” and “Being against using nukes means not wanting to use nukes” the systems differed in the alignment of the word “use” 4 https://github.com/chetannaik/jacana 5 Best predict vectors on http://clic.cimec. unitn.it/composes/semantic-vectors.html Dataset Method Sultan Only (-DF) Sultan Only Sultan + Jacana Sultan + WordSim Sultan + WordSim + WordPairs All aligners A"
S16-1110,D14-1179,0,0.020235,"Missing"
S16-1110,S15-2046,0,0.038739,"Missing"
S16-1110,D13-1179,1,0.847973,"ther word in the other sentence. We examine several methods that can be used to learn these alignments including word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and models based on deep learning that have been suggested for machine translation (Bahdanau et al., 2014; Cho et al., 2014). In addition, we look into recent models for sentence and document similarity that can leverage the large amount of loosely aligned text in particular those based on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and recent extensions aimed at generating orthogonal representations (McCrae et al., 2013; Aggarwal et al., 2015). While these novel techniques alone can achieve high performance on the task, we note that simple metrics such as the number of overlapping terms can produce reasonable performance. For added robustness we combine features based on simple metrics with novel methods explored in this work as a multi-feature regression problem, which we solve by means of an M5 Decision tree (Wang and Witten, 1996; Quinlan, 1992). The rest of the paper is structured as follows: we present our system in Section 2. We then present both our internal evaluation results and the official Task 1"
S16-1110,D14-1162,0,0.0793427,"es correspond to each other. This is quite successful in many cases where many words have the same lemma, however when synonymous and semantically similar terms are used, it is much harder to construct alignment. For this reason, we propose the use of soft alignments, where instead of producing a hard linking between individual words in the sentence, we instead produce a score indicating how likely one word in a sentence is to be aligned to another word in the other sentence. We examine several methods that can be used to learn these alignments including word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and models based on deep learning that have been suggested for machine translation (Bahdanau et al., 2014; Cho et al., 2014). In addition, we look into recent models for sentence and document similarity that can leverage the large amount of loosely aligned text in particular those based on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and recent extensions aimed at generating orthogonal representations (McCrae et al., 2013; Aggarwal et al., 2015). While these novel techniques alone can achieve high performance on the task, we note that simple metrics such as the number of over"
S16-1110,Q14-1018,0,0.0688326,"computing text similarity, and also evaluate different methods to produce it. The main features used by our system are based on alignment and Explicit Semantic Analysis. Our system was above the median scores for 4 out of the 5 datasets at SemEval 2016 STS Task 1. 1 Introduction Semantic textual similarity is the task of deciding if two sentences express a similar or identical meaning and requires a deep understanding of a sentence and its meaning in order to achieve high performance. Recent successful approaches to this problem have been based on the idea of creating monolingual alignments (Sultan et al., 2014a) indicating which words in each of the two sentences correspond to each other. This is quite successful in many cases where many words have the same lemma, however when synonymous and semantically similar terms are used, it is much harder to construct alignment. For this reason, we propose the use of soft alignments, where instead of producing a hard linking between individual words in the sentence, we instead produce a score indicating how likely one word in a sentence is to be aligned to another word in the other sentence. We examine several methods that can be used to learn these alignmen"
S16-1110,S14-2039,0,0.0173864,"computing text similarity, and also evaluate different methods to produce it. The main features used by our system are based on alignment and Explicit Semantic Analysis. Our system was above the median scores for 4 out of the 5 datasets at SemEval 2016 STS Task 1. 1 Introduction Semantic textual similarity is the task of deciding if two sentences express a similar or identical meaning and requires a deep understanding of a sentence and its meaning in order to achieve high performance. Recent successful approaches to this problem have been based on the idea of creating monolingual alignments (Sultan et al., 2014a) indicating which words in each of the two sentences correspond to each other. This is quite successful in many cases where many words have the same lemma, however when synonymous and semantically similar terms are used, it is much harder to construct alignment. For this reason, we propose the use of soft alignments, where instead of producing a hard linking between individual words in the sentence, we instead produce a score indicating how likely one word in a sentence is to be aligned to another word in the other sentence. We examine several methods that can be used to learn these alignmen"
S16-1110,P13-2123,0,0.0376981,"Missing"
S16-1168,S16-1205,0,0.102525,"stem introduces hypernym endocentricity as a useful property for hypernym identification (Tan, 2016). Often multi-word hyponyms are endocentric constructions which contains a word that fulfills the same function as one part of its word. E.g. an ”apple pie” is essentially a ”pie”. The number of multi-words terms that are endocentric in English is investigated and whether this endocentric property can be used to generate entity links to connect terms in the Wikipedia list of list. QASSIT A semi-supervised methodology is used for the acquisition of lexical taxonomies based on genetic algorithms (Cleuziou and Moreno, 2016). It is based on the theory of pretopology that offers a powerful formalism to model semantic relations and transforms a list of terms into a structured term space by combining different discriminant criteria. In particular, rare but accurate pieces of knowledge are used to parameterize the different criteria defining the pretopological term space. Then, a structuring algorithm is used to transform the pretopological space into a lexical taxonomy. 5.1 Monolingual Subtasks (English) Table 2 presents the results of the structural analysis for English, giving an overview of the structural measure"
S16-1168,P14-1089,0,0.0222979,"luding monolingual subtasks for hypernym identification and taxonomy construction in English, as well as two corresponding multilingual subtasks that cover Dutch, French and Italian. 3 Dataset Creation We selected three target domains (i.e. Environment, Food and Science) with three root concepts (i.e. “environment”, “food” and “science”, respectively). Then, for each domain we considered different sources for gathering gold standard taxonomies, including a multilingual thesaurus, Eurovoc1 , a large lexical database of English, WordNet, and a general purpose resource, the Wikipedia Bitaxonomy (Flati et al., 2014). We also considered other domainspecific resources including “The Google product taxonomy” 2 for Food, and the “Taxonomy of Fields and their Subfields” 3 for Science. English taxonomies The English gold standard taxonomies are collected from each of the sources 1 Eurovoc: http://eurovoc.europa.eu/drupal/ http://www.google.com/basepages/ producttype/taxonomy.en-US.txt 3 http://sites.nationalacademies.org/PGA/ Resdoc/PGA_044522 2 1082 described above as follows. Gold standards are gathered from WordNet by selecting concepts and relationships in the hypernym-hyponym hierarchy rooted on the corre"
S16-1168,P05-1014,0,0.125948,"duced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold standard taxonomies and by manual quality assessment of novel relations. 1 Introduction Taxonomies are useful tools for content organisation, navigation, and retrieval, providing valuable input for semantically intensive tasks such as question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). In general, a hierarchical relation is any asymmetrical relation that indicates subordination between two terms, but in this task we focus on hyponym-hypernym relations. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions"
S16-1168,S15-2152,0,0.120027,"sks such as question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). In general, a hierarchical relation is any asymmetrical relation that indicates subordination between two terms, but in this task we focus on hyponym-hypernym relations. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pat"
S16-1168,C92-2082,0,0.623288,"trieval, providing valuable input for semantically intensive tasks such as question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). In general, a hierarchical relation is any asymmetrical relation that indicates subordination between two terms, but in this task we focus on hyponym-hypernym relations. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promisin"
S16-1168,D10-1108,0,0.643764,"Paul Buitelaar* *Insight Centre for Data Analytics National University of Ireland, Galway name.surname@insight-centre.org **LT3, Language and Translation Technology Team, Ghent University, Belgium name.surname@ugent.be Abstract usually produces a large number of noisy, inconsistent relations, which assign multiple parents to a node and contain cycles. Hence, the third stage of taxonomy learning, taxonomy construction, focuses on the overall structure of the resulting graph and aims to organise terms in a hierarchical structure, more specifically a directed acyclic graph (Velardi et al., 2013; Kozareva and Hovy, 2010). This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016. This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them. TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold stan"
S16-1168,N15-1098,0,0.027616,"rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchical relations from text and subsequent taxonomy construction, therefore we make the assumption that a list of terms is readily available. This simplifies evaluation by providing a common ground for all the s"
S16-1168,S16-1204,0,0.074038,"icipated in the shared task, but only two systems participated in the multilingual subtasks. Two of the systems that participated in the monolingual subtask alone did not submit runs for the food domain, which has the largest number of nodes. Overall, 62 system runs were submitted by the five teams, 36 for the multilingual subtasks and 26 for the monolingual subtasks. Next, we provide a short description of each approach starting with the two systems that participated in the multilingual subtasks. JUNLP The JUNLP system makes use of an external linguistic resource for hypernym identification (Maitra and Das, 2016). This resource is the BabelNet semantic network that connects concepts and named entities in a very large network of semantic relations, called Babel synsets (Navigli and Ponzetto, 2010). To make sure that no relations that were used to construct the gold standards are considered, only relations that mention Wikipedia as a source were selected, discarding relations from all the other sources. Additionally, the system makes use of two string inclusion heuristics. The first heuristic checks if any of the terms provided by the organisers is included as a substring in another term. The second heu"
S16-1168,N13-1090,0,0.00284317,"with lexico-syntactic patterns. No databases or linguistic resources beyond trial data and raw text corpora mentioned above are used. For the taxonomy construction subtasks, the system makes use of an unsupervised graph pruning approach based on the Tarjan algorithm, connecting the resulting disconnected components to the root of the graph. NUIG-UNLP The system implements a semisupervised method that finds hypernym candidates for the provided noun phrases by representing them as distributional vectors. Roughly, this method assumes that hypernyms may be induced by adding a 1087 vector offset (Mikolov et al., 2013; Rei and Briscoe, 2014) to the corresponding hyponym representation generated by GloVe over a Wikipedia dump. The vector offset is obtained as the average offset between 200 pairs of hyponym-hypernym in the same vector space selected from trial data. USAAR This system introduces hypernym endocentricity as a useful property for hypernym identification (Tan, 2016). Often multi-word hyponyms are endocentric constructions which contains a word that fulfills the same function as one part of its word. E.g. an ”apple pie” is essentially a ”pie”. The number of multi-words terms that are endocentric i"
S16-1168,P10-1023,0,0.0313702,"s for the food domain, which has the largest number of nodes. Overall, 62 system runs were submitted by the five teams, 36 for the multilingual subtasks and 26 for the monolingual subtasks. Next, we provide a short description of each approach starting with the two systems that participated in the multilingual subtasks. JUNLP The JUNLP system makes use of an external linguistic resource for hypernym identification (Maitra and Das, 2016). This resource is the BabelNet semantic network that connects concepts and named entities in a very large network of semantic relations, called Babel synsets (Navigli and Ponzetto, 2010). To make sure that no relations that were used to construct the gold standards are considered, only relations that mention Wikipedia as a source were selected, discarding relations from all the other sources. Additionally, the system makes use of two string inclusion heuristics. The first heuristic checks if any of the terms provided by the organisers is included as a substring in another term. The second heuristic considers terms that have a considerable overlap, for instance Chocolate Pudding and Vanilla Pudding although their hypernym (i.e., Pudding) is not mentioned in the list of terms."
S16-1168,S16-1206,0,0.269983,"Missing"
S16-1168,S16-1202,0,0.203096,"from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchical relations from text and subsequent taxonomy construction, therefore we make the assumption that a list of terms is readily available. This simplifies evaluation by providing a common ground for all the systems, but participants are allowed to add additional nodes, i.e. terms, in the hierarchy as they consider appropriate. To avoid the need for term extraction, terms are extracted from existing taxonomies, providing participants with a domain lexicon that has to be organised in a hierarchical structure. 1081 Proceedings"
S16-1168,W14-1608,0,0.0698067,"from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchi"
S16-1168,C14-1097,0,0.021659,"Missing"
S16-1168,E14-4008,0,0.0519906,"ns. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatical"
S16-1168,S16-1203,0,0.215836,"a semisupervised method that finds hypernym candidates for the provided noun phrases by representing them as distributional vectors. Roughly, this method assumes that hypernyms may be induced by adding a 1087 vector offset (Mikolov et al., 2013; Rei and Briscoe, 2014) to the corresponding hyponym representation generated by GloVe over a Wikipedia dump. The vector offset is obtained as the average offset between 200 pairs of hyponym-hypernym in the same vector space selected from trial data. USAAR This system introduces hypernym endocentricity as a useful property for hypernym identification (Tan, 2016). Often multi-word hyponyms are endocentric constructions which contains a word that fulfills the same function as one part of its word. E.g. an ”apple pie” is essentially a ”pie”. The number of multi-words terms that are endocentric in English is investigated and whether this endocentric property can be used to generate entity links to connect terms in the Wikipedia list of list. QASSIT A semi-supervised methodology is used for the acquisition of lexical taxonomies based on genetic algorithms (Cleuziou and Moreno, 2016). It is based on the theory of pretopology that offers a powerful formalis"
S16-1168,J13-3007,0,0.759765,"rdea*, Els Lefever**, Paul Buitelaar* *Insight Centre for Data Analytics National University of Ireland, Galway name.surname@insight-centre.org **LT3, Language and Translation Technology Team, Ghent University, Belgium name.surname@ugent.be Abstract usually produces a large number of noisy, inconsistent relations, which assign multiple parents to a node and contain cycles. Hence, the third stage of taxonomy learning, taxonomy construction, focuses on the overall structure of the resulting graph and aims to organise terms in a hierarchical structure, more specifically a directed acyclic graph (Velardi et al., 2013; Kozareva and Hovy, 2010). This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016. This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them. TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by"
S16-1168,C14-1212,0,0.0211349,"ification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchical relations from text and subsequent taxonomy construction, therefore we make the assumption that a list of terms is readily available. This simplifies evaluation by providing a common"
S16-1168,S15-2151,1,\N,Missing
S16-2022,N09-1030,0,0.356116,"Missing"
S16-2022,D14-1181,0,0.0168353,"Missing"
S16-2022,D14-1162,0,0.0769314,"Missing"
S16-2022,W10-0207,0,0.418212,"Missing"
S16-2022,S15-2079,0,0.0366853,"Missing"
S16-2022,P15-1130,0,0.0606505,"Missing"
S16-2022,P14-2050,0,0.0680427,"Missing"
S16-2022,P14-5010,0,0.00856172,"Missing"
S16-2022,D15-1258,1,0.526414,"Missing"
S16-2022,P14-1023,0,\N,Missing
S19-2151,S19-2218,0,0.0466855,"Missing"
S19-2151,S19-2210,0,0.0641595,"Missing"
S19-2151,S19-2208,0,0.0277785,"Missing"
S19-2151,S19-2216,0,0.111195,"Missing"
S19-2151,S19-2219,0,0.0266766,"Missing"
S19-2151,S19-2221,0,0.0437169,"Missing"
S19-2151,S19-2220,0,0.0761106,"Missing"
S19-2151,S19-2212,0,0.0600722,"Missing"
S19-2151,S19-2214,0,0.0331482,"Missing"
S19-2151,S19-2211,0,0.0396443,"Missing"
S19-2151,S19-2224,0,0.0924461,"Missing"
S19-2151,S19-2215,0,0.171118,"nces in the test dataset 2. The highest F-score for subtask B was 0.858, where the ratio of suggestion and non-suggestion sentences in the test set was higher than subtask A. Top 3 systems: BERT (Devlin et al., 2018) pre-trained language model remains the common method in the top three systems submitted in subtask A, which is one of the state of the art statistical language models. However, the most interesting results are provided by the best performing system in subtask B, which uses a rule based classifier, where rules comprise of both words and POS tags. The devised rule-based classifier (Potamias et al., 2019) assigns confidence scores to sentences on the basis of lexical patterns organised in pre-specified categories and lexical lists corresponding to each subtask. This rule based system also performed fairly well in subtask A, where it achieved rank 5. Recall suggestion (Rsugg ): The fraction of suggestion class instances which are correctly identified out of the total number of suggestions. Rsugg = True Positives / (True Positives + False Negatives) F1 score for the suggestion class is: F1sugg = 2 * (Psugg * Rsugg ) / (Psugg + Rsugg ) Baseline System A rule based classifier is employed using the"
S19-2151,S19-2225,0,0.042145,"Missing"
S19-2151,S16-1003,0,0.0785784,"Missing"
S19-2151,P16-3018,1,0.827946,"ructured data in order to aid recommender systems, or the summarisation of suggestion forums where suggestion providers often tend to provide context in their responses (Figure 1) which gets repetitive over a large number of responses relating to the same entity. The task of automatic identification of suggestions in a given text is referred to as suggestion mining (Brun and Hagege, 2013). Studies performed on suggestion mining have defined it as a sentence classification task, where class prediction has to be made on each sentence of a given text, classes being suggestion and non suggestion (Negi, 2016). State of the art opinion mining systems have mostly focused on identifying sentiment polarity of the text. Therefore, suggestion mining remains a very less explored problem as compared to sentiment analysis, specially in the context of recent advancements in neural network based approaches for feature learning and transfer learning. We present the pilot SemEval task on Suggestion Mining. The task consists of subtasks A and B, where we created labeled data from feedback forum and hotel reviews respectively. Subtask A provides training and test data from the same domain, while Subtask B evalua"
S19-2151,W10-0207,0,0.584944,"Missing"
S19-2151,S15-2078,0,0.0376761,"Therefore, we introduce this pilot shared task to disseminate suggestion mining benchmarks and evaluate state of the art methods for text classification on domain specific and cross domain training scenarios. The datasets released as a part of the shared task include the domains hotel reviews and software developers suggestion forum (see Table 1). Suggestion mining faces similar text processing challenges as other sentence or short text classification tasks related to opinion mining and subjectivity analysis, such as stance detection (Mohammad et al., 2016), or tweet sentiment classification (Rosenthal et al., 2015). Some of the observed challenges in suggestion mining are elaborated below: • Context dependency: In some cases, context plays a major role in determining whether a sentence is a suggestion or not. For example, ‘There is a parking garage on the corner of the Forbes showroom.’ can be labeled as a suggestion (for parking space) when it appears in a restaurant review and a human annotator gets to read the full review. However, the same sentence would not be labeled as a suggestion if the text is aimed to describe the surroundings of the Forbes showroom. • Long and complex sentences: Often, a sug"
S19-2151,S16-2022,1,0.194667,"ions: Text from social media and other sources usually contains figurative use of language, which demands pragmatic understanding from the models. For example, ‘Try asking for extra juice at breakfast - its 22 euros!!!!!’ is more of a sarcasm than a suggestion. Therefore, a sentence framed as a typical suggestions may not always be a suggestion and vice versa. A variety of linguistic strategies used in suggestions also make this task interesting from a computational linguistics perspective and labeled datasets can be leveraged for linguistic studies as well. datasets(Negi and Buitelaar, 2015; Negi et al., 2016). A few other works also evaluated statistical classifiers (Wicaksono and Myaeng, 2012; Dong et al., 2013), which employed mostly manually identified features, however only two other works (Wicaksono and Myaeng, 2012; Dong et al., 2013) provided their datasets. Suggestion mining still lacks well defined annotation guidelines, a multi-domain and cross-domain approach to the problem and benchmark datasets, which we address in our recent work (Negi et al., 2018). Therefore, we introduce this pilot shared task to disseminate suggestion mining benchmarks and evaluate state of the art methods for te"
S19-2151,S19-2217,0,0.0389942,"Missing"
S19-2151,S19-2152,0,0.0383878,"Missing"
S19-2151,S19-2226,0,0.0414685,"Missing"
S19-2151,S19-2222,0,0.0612199,"Missing"
vintar-etal-2002-efficient,ide-etal-2000-xces,0,\N,Missing
vintar-etal-2002-efficient,A97-1034,0,\N,Missing
vintar-etal-2002-efficient,W00-1003,0,\N,Missing
vintar-etal-2002-efficient,A00-1031,0,\N,Missing
W00-0103,C94-2113,0,0.048395,"Missing"
W00-0103,W97-0802,0,0.0816301,"classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), while addressing some previous shortcomings. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), but takes into account some shortcomings as pointed out in (Krymolowski and Roth 1998) (Peters, Peters and Vossen 1998) (Tomuro 1998). Whereas the original approach identified a small set of top-level synsets for grouping together lexical items, Introduction This paper presents an algorithm for finding systematic polysemous classes in WordNet (Miller et al 1990) and GermaNet (Hamp and Feldweg 1997) -- a semantic database for German similar to WordNet. The introduction of such classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity i As pointed out in (Wilks 99), earlier work in AI on &apos;Polaroid Words&apos; (Hirst 87) and &apos;Word Experts&apos; (Small 81) advocated a similar, incremental approach to sense representation and interpretation. In line with this, the CoreLex approach discussed here provides a large scale inventory of systematically polysemous lexical items with und"
W00-0103,W98-0717,0,0.0885414,"many applications, for instance in document categorization, information retrieval, and information extraction, it may be sufficient to know if a given word belongs to a certain class (underspecified sense) rather than to know which of its (related) senses exactly to pick. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), while addressing some previous shortcomings. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), but takes into account some shortcomings as pointed out in (Krymolowski and Roth 1998) (Peters, Peters and Vossen 1998) (Tomuro 1998). Whereas the original approach identified a small set of top-level synsets for grouping together lexical items, Introduction This paper presents an algorithm for finding systematic polysemous classes in WordNet (Miller et al 1990) and GermaNet (Hamp and Feldweg 1997) -- a semantic database for German similar to WordNet. The introduction of such classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity i As pointed out in (W"
W00-0103,W98-0715,0,0.160359,"nformation retrieval, and information extraction, it may be sufficient to know if a given word belongs to a certain class (underspecified sense) rather than to know which of its (related) senses exactly to pick. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), while addressing some previous shortcomings. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), but takes into account some shortcomings as pointed out in (Krymolowski and Roth 1998) (Peters, Peters and Vossen 1998) (Tomuro 1998). Whereas the original approach identified a small set of top-level synsets for grouping together lexical items, Introduction This paper presents an algorithm for finding systematic polysemous classes in WordNet (Miller et al 1990) and GermaNet (Hamp and Feldweg 1997) -- a semantic database for German similar to WordNet. The introduction of such classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity i As pointed out in (Wilks 99), earlier work in AI on &apos;Polaroid Words"
W03-1302,P91-1034,0,0.19303,"Missing"
W03-1302,J98-1001,0,0.0213934,"Missing"
W03-1302,W99-0508,0,0.0434869,"Missing"
W03-1302,vintar-etal-2002-efficient,1,0.675331,"be used for disambiguation. 2.2 The Springer Corpus of Medical Abstracts The experiments and implementations of WSD described in this paper were all carried out on a parallel corpus of English-German medical scientific abstracts obtained from the Springer Link web site.3 The corpus consists approximately of 1 million tokens for each language. Abstracts are from 41 medical journals, each of which constitutes a relatively homogeneous medical sub-domain (e.g. Neurology, Radiology, etc.). The corpus was automatically marked up with morphosyntactic and semantic information, ˇ as described by Spela Vintar et al. (2002). In brief, whenever a token is encountered in the corpus that is listed as a term in UMLS, the document is annotated with the CUI under which that term is listed. Ambiguity is introduced by this markup process because the lexical resources often list a particular term as a possible realisation of more than one concept or CUI, as with the trauma example above, in which case the document is annotated with all of these possible CUI’s. The number of tokens of UMLS terms included by this annotation process is given in Table 1. The table shows how many tokens were found by the annotation process, l"
W03-1302,H93-1052,0,0.136614,"l, the bilingual method correctly find the meanings of approximately one fifth of the ambiguous terms, and makes only a few mistakes for English but many more for German. 4 Collocational disambiguation By a ‘collocation’ we mean a fixed expression formed by a group of words occuring together, such as blood vessel or New York. (For the purposes of this paper we only consider contiguous multiword expressions which are listed in UMLS.) There is a strong and well-known tendency for words to express only one sense in a given collocation. This property of words was first described and quantified by Yarowsky (1993), and has become known generally as the ‘One Sense Per Collocation’ property. Yarowsky (1995) used the one sense per collocation property as an essential ingredient for an unsupervised Word-Sense Disambiguation algorithm. For example, the collocations plant life and manufacturing plant are used as ‘seed-examples’ for the living thing and building senses of plant, and these examples can then be used as high-precision training data to perform more general high-recall disambiguation. While Yarowsky’s algorithm is unsupervised (the algorithm does not need a large collection of annotated training e"
W03-1302,P95-1026,0,0.352314,"us terms, and makes only a few mistakes for English but many more for German. 4 Collocational disambiguation By a ‘collocation’ we mean a fixed expression formed by a group of words occuring together, such as blood vessel or New York. (For the purposes of this paper we only consider contiguous multiword expressions which are listed in UMLS.) There is a strong and well-known tendency for words to express only one sense in a given collocation. This property of words was first described and quantified by Yarowsky (1993), and has become known generally as the ‘One Sense Per Collocation’ property. Yarowsky (1995) used the one sense per collocation property as an essential ingredient for an unsupervised Word-Sense Disambiguation algorithm. For example, the collocations plant life and manufacturing plant are used as ‘seed-examples’ for the living thing and building senses of plant, and these examples can then be used as high-precision training data to perform more general high-recall disambiguation. While Yarowsky’s algorithm is unsupervised (the algorithm does not need a large collection of annotated training examples), it still needs direct human intervention to recognise which ambiguous terms are ame"
W08-0625,A00-1031,0,0.136461,"s can be assigned to terms that allow to investigate the most likely expressed (and hence queried) relations between them. For this purpose we need access to a representative corpus of texts that at the same time reflects the joint view of anatomy, spatial aspects of radiology and disease that we are targeting. Patient records would be our first choice, but due to strict anonymization requirements these are difficult to obtain. We therefore constructed a corpus based on the Wikipedia Categories Anatomy and Radiology. We then ran all text sections of each corpus through a part-ofspeech tagger (Brants, 2000) to extract all nouns in the corpus and to compute a relevance score (chisquare) for each by comparing anatomy and radiology frequencies with those in the British Na114 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 114–115, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tional Corpus. A next step will be to parse and annotate sentences with predicate-structure information, which may then be used for relation extraction along the lines of (Schutz and Buitelaar, 2005). 3 FMA Terms FMA Term lateral anterior artery anterior spinal ar"
W09-4508,E99-1003,0,0.0161923,"rts. The rest of this paper is organized as follows. Next section discusses related work. Then materials and methods used are introduced and the clinical query derivation approach is explained in detail. This is followed by the discussion of the results of comparing the query patterns with the clinical questions corpus. The clinical experts’ assessment is reported followed by conclusion and future directions. 2. Related Work Clinical query derivation can be viewed as a special case of term-relation extraction. Related approaches from the medical domain are reported by Bourigault and Jacquemin [2] and Le Moigno et al. [9] which, however, are independent of medical image semantics. Price and Delcambre [11] propose to model the clinical queries as binary relations on query topics (e.g. relation (topic1, topic2)). The relations in the queries are then matched against relations in the documents. In their extended model [12] the ‘semantic components’, which are terms and expressions characteristic for certain types of documents, are used as arguments to the same query relations (e.g. relation(semantic component1, semantic component 2)). Later, the semantic components are used as mediators t"
W09-4508,A00-1031,0,0.050192,"sease’ and ‘Hodgkin’s Disease’). We refer to the version from February 2009. 3.2 Data The anatomy, radiology and disease corpora based on Wikipedia were constructed from the Anatomy, Radiology and Diseases sections of Wikipedia. Actual patient records would have been the first choice, but due to strict anonymization requirements they are difficult to obtain. Thus, Wikipedia corpora served as an initial step. To set up the three corpora the related web pages were downloaded and a specific XML version for them was generated. The text sections of the XML files were run through the TnT POS parser [3] using PENN Treebank Tagset to extract all nouns and adjectives in the corpus. The reason for including adjectives is based on our observations with the concept labels. Especially for anatomy domain, the adjectives carry information that can be significant for medical decisions, for example, when determining whether an image is related to the right or to the left ventricle of the heart. Therefore, throughout the paper, when we talk about concepts, we refer to both adjectives and nouns. Then a relevance score (chi-square) for each noun and adjective was computed by comparing their frequencies i"
W09-4508,W08-0625,1,0.832316,"sults, such as documents or Web resources, which are matched by the template of the question he selected. Again the most significant difference between this work and ours is that the former assumes that the clinical queries or at least their components are already identified, whereas our objective is first to identify the queries (or their components) based on ontologies and statistical analysis. Related work on biomedical data sets and corpora include ‘i2b2’5 on clinical data and the GENIA6 corpus. All these corpora have been designed to extract terms and their interrelations as described in [4]. This is the approach which we also follow with our query pattern derivation technique. These resources mainly concentrate on one domain such as genes or clinical reports. In contrast, the corpora that are established for this work i.e. the statistical analysis of ontology concepts and subsequent relation extraction, are designed to provide a common viewpoint of diseases, anatomy and radiology. Finally, there has been work on collecting clinical questions gathered from healthcare providers in clinical settings, which are available online under the Clinical Questions Collection7. This is also"
W12-4210,W11-1204,0,0.043027,"Missing"
W12-4210,P07-2045,0,0.00794763,"ailable knowledge5 . With the heavily interlinked information base, Wikipedia forms a rich lexical and semantic resource. Besides a large amount of articles, it also holds a hierarchy of Categories that Wikipedia Articles are tagged with. It includes knowledge about named entities, domain-specific terms and word senses. Furthermore, the redirect system of Wikipedia articles can be used as a dictionary for synonyms, spelling variations and abbreviations. 3.5 Translation System: Moses For generating translations from English into German and vice versa, the statistical translation toolkit Moses (Koehn et al., 2007) was used to build the training model and for decoding. For this approach, a phrase-based approach was taken instead of a tree based model. Further, we aimed at improving the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with 5 http://en.wikipedia.org/wiki/Wikipedia:Size_comparison the GIZA++ toolkit (Och and Ney, 2003), whereby the 5-gram language model was built by SRILM (Stolcke, 2002). 4 Domain-specific Resource Generation In this section, two different types of data and the approach of buil"
W12-4210,2005.mtsummit-papers.11,0,0.00794548,"esults of Google Translate. This finding leads us to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation can help to improve translation of domain-specific terms. 1 Introduction Our research on translation of ontology vocabularies is motivated by the challenge of translating domainspecific terms with restricted or no additional textual context that in other cases can be used for translation improvement. For our experiment we started by translating financial terms with baseline systems trained on the EuroParl (Koehn, 2005) corpus and the JRC-Acquis (Steinberger et al., 2006) corpus. Although both resources contain a large amount of parallel data, the translations were not satisfying. To improve the translations of the financial ontology vocabulary we built a new parallel resource, which was generated using Linguee1 , an online translation query service. With this data, we could train a small system, which produced better translations than the baseline model using only general resources. Since the manual development of terminological resources is a time intensive and expensive task, we used Wikipedia as a backgr"
W12-4210,W05-0909,0,0.0424273,". As an example, we examined the Category Accounting terminology and stored the English Wikipedia Title Balance sheet with the German equivalent Wikipedia Title Bilanz. At the end of the lexicon generation we examined 5228 Wikipedia Articles, which were tagged with one or more financial Categories. From this set of Articles we were able to generate a terminological lexicon with 3228 English-German entities. 5 Evaluation Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST (Doddington, 2002), BLEU (Papineni et al., 2002), and Meteor (Lavie and Agarwal, 2005) algorithms. To further study any translation improvements of our experiment, we also used Google Translate8 in translating 63 financial xEBR terms (cf. Section 3.1) from English into German and from German into English. 5.1 Interpretation of Evaluation Metrics In our experiments translation models built from a general resource performed worst. These re8 Translations were generated on February 2012. 90 Scoring Metric Source # correct BLEU NIST Google Translate JRC-Acquis EuroParl Linguee Lexical substitution Linguee+Wiki 21 9 5 15 4 22 0.452 0.127 0.021 0.364 0.006 0.348 4.830 2.458 1.307 3.93"
W12-4210,J03-1002,0,0.00271255,"lling variations and abbreviations. 3.5 Translation System: Moses For generating translations from English into German and vice versa, the statistical translation toolkit Moses (Koehn et al., 2007) was used to build the training model and for decoding. For this approach, a phrase-based approach was taken instead of a tree based model. Further, we aimed at improving the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with 5 http://en.wikipedia.org/wiki/Wikipedia:Size_comparison the GIZA++ toolkit (Och and Ney, 2003), whereby the 5-gram language model was built by SRILM (Stolcke, 2002). 4 Domain-specific Resource Generation In this section, two different types of data and the approach of building them will be presented. Section 4.1 gives an overview of generating a parallel resource from Linguee, which was used in generating a new domain-specific training model. In Section 4.2 a detailed description is given how we extracted terms from Wikipedia for generating a domain-specific lexicon. 4.1 Domain-specific parallel corpus generation To build a new training model that is specialised on our xEBR ontology, w"
W12-4210,P02-1040,0,0.0827818,"ikipedia knowledge base also existed. As an example, we examined the Category Accounting terminology and stored the English Wikipedia Title Balance sheet with the German equivalent Wikipedia Title Bilanz. At the end of the lexicon generation we examined 5228 Wikipedia Articles, which were tagged with one or more financial Categories. From this set of Articles we were able to generate a terminological lexicon with 3228 English-German entities. 5 Evaluation Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST (Doddington, 2002), BLEU (Papineni et al., 2002), and Meteor (Lavie and Agarwal, 2005) algorithms. To further study any translation improvements of our experiment, we also used Google Translate8 in translating 63 financial xEBR terms (cf. Section 3.1) from English into German and from German into English. 5.1 Interpretation of Evaluation Metrics In our experiments translation models built from a general resource performed worst. These re8 Translations were generated on February 2012. 90 Scoring Metric Source # correct BLEU NIST Google Translate JRC-Acquis EuroParl Linguee Lexical substitution Linguee+Wiki 21 9 5 15 4 22 0.452 0.127 0.021 0."
W12-4210,C08-1125,0,0.0513713,"Missing"
W12-4210,zesch-etal-2008-extracting,0,0.0196027,"Missing"
W12-4210,steinberger-etal-2006-jrc,0,\N,Missing
W12-4210,W07-0734,0,\N,Missing
W13-5502,baccianella-etal-2010-sentiwordnet,0,0.0652912,"Missing"
W13-5502,strapparava-valitutti-2004-wordnet,1,\N,Missing
W14-4803,P13-1040,0,0.100306,"Missing"
W14-4803,2013.mtsummit-posters.1,1,0.807058,"Missing"
W14-4803,W04-2214,0,0.0190318,"o each linked term in a text, after that we obtain the most frequent domain and filter out the terms that are out of scope. In the example above, the term mouse is accepted because it belongs to the domain computer science, as the majority of terms (mouse, pop up menu and Gnome panel), while the term key in the domain music is rejected. The large number of languages and domains to cover prevents us from using standard text classification techniques to categorize the document. For this reason, we implemented an approach based on the mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia categories are created and assigned by different human editors, and are therefore less rigorous, coherent and consistent than usual ontologies. In addition, the Wikipedia’s category hierarchy forms a cyclic graph (Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity, allows us reducing the number of domains to few tens instead of"
W14-4803,2011.iwslt-evaluation.18,0,0.0335625,"d identify their equivalents using Wikipedia cross-lingual links. For this purpose we extend The Wiki Machine API,2 a tool for linking terms in text to Wikipedia pages, adding two more components able to first identify domain-specific terms, and to find their translations in a target language. The identified bilingual terms are then compared with those obtained by TaaS (Skadinˇs et al., 2013). The embedding of the domain-specific terms into an SMT system is performed by use of the XML markup approach, which uses the terms as preferred translation candidates at run time, and the Fill-Up model (Bisazza et al., 2011), which emphasizes phrase pairs extracted from the bilingual terms. Our results show that the performance of our technique and TaaS are comparable in the identification of monolingual and bilingual domain-specific terms. From the machine translation point of view, our experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach. Thiswork workisis licensed licenced under Attribution 4.0 International License. Page numbers and proceedings footer This under aaCrea"
W14-4803,bouamor-etal-2012-identifying,0,0.139922,"erwise, we return the most frequent alternative form of the term in the target language. From the previous example, the system is able to return the Italian page Mouse and all terms used in the Italian Wikipedia to express this concept of Mouse in computer science. Using this information, the term mouse is paired with its translation into Italian. 2.2 Integration of Bilingual Terms into SMT A straightforward approach for adding bilingual terms to the SMT system consists of concatenating the training data and the terms. Although it has been shown to perform better than more complex techniques (Bouamor et al., 2012), it is still affected by major disadvantages that limits its use in real applications. In particular, when small amounts of bilingual terms are concatenated with a large training dataset, terms with ambiguous translations are penalised, because the most frequent and general translations often receive the highest probability, which drives the SMT system to ignore specific translations. In this paper, we focus on two techniques that give more priority to specific translations than generic ones: the Fill-Up model and the XML markup approach. The Fill-Up model has been developed to address a comm"
W14-4803,P11-2031,0,0.0368264,"search path” in English) is translated with a completely wrong translation. In the next Section we evaluate whether the automatic identified bilingual terms can improve the performance of an SMT system and if it is robust to the aforementioned errors. 4.3 Embedding Terminology into SMT Our further experiments focused on the automatic evaluation of the translation quality of the EMEA, GNOME and KDE test sets (Table 4). The obtained bilingual terminology from TaaS and The Wiki Machine was embedded through the Fill-Up and XML markup approaches. The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value &lt; 0.05. The parameters of the baseline method and the Fill-Up models were optimized on the development set. Injecting the obtained TaaS bilingual terms improves the BLEU score in several cases. XML markup outperforms the general baseline approach in three (out of eight) datasets, whereby three of them are statistically significant (GNOME En→It, KDE anno En↔It). Embedding the same bilingual terminology into the Fill-Up model helped to outperform the baseline approach for all test sets, wh"
W14-4803,W07-0717,0,0.0906092,"Missing"
W14-4803,J09-4007,1,0.827322,"roach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages. The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense, a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The third step enriches the linked terms using information extracted from Wikipedia and LOD resources. The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e., orthographical and morphological variants, synonyms, and related terms), images, topic, type, cross language links, etc. For example, in the text “click right mouse key to pop up menu and Gnome panel”, The"
W14-4803,W12-3154,0,0.180428,"formance is similar to the scores obtained using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En→It and 27.02 for It→En, which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of the two term identification methods and suggests a novel research direction. 5 Related Work The main focus of our research is on bilingual term identification and the embedding of this knowledge into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that an SMT system built by using a large general resource cannot be used to translate domain-specific terms, we have to provide the system domain-specific lexical knowledge. Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string 28 matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Bes"
W14-4803,W07-0733,0,0.227517,"Missing"
W14-4803,P07-2045,0,0.00742184,"translation by the resource it was provided. In our case, we favour first the translation provided by ETB. If no translation is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting the term extraction approach, TaaS requires manual specification of the source and target languages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system"
W14-4803,2005.mtsummit-papers.11,0,0.123656,"uages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identification tools and term em"
W14-4803,W02-1405,0,0.803922,"Missing"
W14-4803,W13-5630,0,0.0323917,"Missing"
W14-4803,N07-1025,0,0.0357079,"age for the specific language and domain. The first step identifies and ranks the terms by relevance using a simple statistical approach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages. The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense, a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The third step enriches the linked terms using information extracted from Wikipedia and LOD resources. The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e., orthographical and morphological variants, synonyms, and related te"
W14-4803,2011.iwslt-papers.6,0,0.197482,"Missing"
W14-4803,J03-1002,0,0.00531712,"slation provided by ETB. If no translation is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting the term extraction approach, TaaS requires manual specification of the source and target languages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating rem"
W14-4803,P02-1040,0,0.0894061,"approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identification tools and term embedding methods for the two domains: IT and the medical domain. For evaluating the extracted monolingual and bilingual terms, we calculate precision, recall and f-measure using the manually labelled KDE anno and GNOME datasets. In addition, we perform a manual inspection of a subset of the bilingual identified terms. The BLEU metric (Papineni et al., 2002) was used to automatically evaluate the quality of the translations. The metric calculates the overlap of n-grams between the SMT system output and a reference translation, provided by a professional translator. 4.1 Monolingual Term Identification In Table 1, the column ’Ident.’ represents the number of identified terms for each tool, whereby we observed TaaS always extracts more terms than The Wiki Machine. While extracting Italian terms, TaaS extracts twice as more terms as The Wiki Machine, which can be explained by the overall lower 5 6 In the rest of the paper, we refer to the annotated p"
W14-4803,W09-2907,0,0.30525,"Missing"
W14-4803,steinberger-etal-2006-jrc,0,0.177866,"ication of the source and target languages, the domain, and the source document. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identifi"
W14-4803,tiedemann-2012-parallel,0,0.0212487,"cument. Since we focused on the IT and medical domains we set the options to ’Information and communication technology’ and ’Medicine and pharmacy’, respectively. For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit (Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage, we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of ∼37M tokens and a development set of ∼10K tokens. In our experiments, an instance of Moses trained on the generic parallel dataset was used in three different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the Fill-Up method as background translation model. 4 Evaluation In this Section, we report the performance of the different term identification tools and term embedding methods for the two domains: IT"
W14-4803,C08-1125,0,0.0692236,"or It→En. This performance is similar to the scores obtained using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En→It and 27.02 for It→En, which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of the two term identification methods and suggests a novel research direction. 5 Related Work The main focus of our research is on bilingual term identification and the embedding of this knowledge into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that an SMT system built by using a large general resource cannot be used to translate domain-specific terms, we have to provide the system domain-specific lexical knowledge. Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string 28 matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the"
W14-4803,W07-0201,0,0.0128413,"e, pop up menu and Gnome panel), while the term key in the domain music is rejected. The large number of languages and domains to cover prevents us from using standard text classification techniques to categorize the document. For this reason, we implemented an approach based on the mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia categories are created and assigned by different human editors, and are therefore less rigorous, coherent and consistent than usual ontologies. In addition, the Wikipedia’s category hierarchy forms a cyclic graph (Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity, allows us reducing the number of domains to few tens instead of some hundred thousands (800,000 23 categories in the English Wikipedia) and does not require any language-specific training data. Wikipedia categories that contain more pages (∼1,000) have been manually mapped to WordNet domains. The domain for a term is"
W15-0115,W12-3802,0,0.0285742,"lity is a grammatical category that allows the expression of aspects related to the attitude of a speaker towards his statement, in terms of degree of certainty, reliability, subjectivity, sources of information, and perspective (Morante and Sporleder, 2012). Subjunctive mood originated from the typological studies of modality (Palmer, 1986; Dudman, 1988; Portner, 2009). Some works equate its presence with ‘counterfactuality’(Palmer, 1986), while some do not (Anderson, 1951). Other concepts like ‘event modality’, ‘irrealis’ (Palmer, 1986), have definitions similar to that of subjunctive mood. Benamara et al. (2012) studied modality and negation for French language, with an objective to examine its effect on sentiment polarity. Narayanan et al. (2009) performed sentiment analysis on conditional sentences. Our objective however is inclined towards wish and suggestion detection, rather than sentiment analysis. Wish Detection: Goldberg et al. (2009) performed wish detection on datasets obtained from political discussion forums and product reviews. They automatically extracted sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. Suggestion Detection: R"
W15-0115,N09-1030,0,0.728756,"1986; Dudman, 1988; Portner, 2009). Some works equate its presence with ‘counterfactuality’(Palmer, 1986), while some do not (Anderson, 1951). Other concepts like ‘event modality’, ‘irrealis’ (Palmer, 1986), have definitions similar to that of subjunctive mood. Benamara et al. (2012) studied modality and negation for French language, with an objective to examine its effect on sentiment polarity. Narayanan et al. (2009) performed sentiment analysis on conditional sentences. Our objective however is inclined towards wish and suggestion detection, rather than sentiment analysis. Wish Detection: Goldberg et al. (2009) performed wish detection on datasets obtained from political discussion forums and product reviews. They automatically extracted sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. Suggestion Detection: Ramanand et al. (2010) pointed out that wish is a broader category, which might not bear suggestions every time. They performed suggestion detection, where they focussed only on suggestion bearing wishes, and used manually formulated syntactic patterns for their detection. Brun (2013) also extracted suggestions from product reviews and"
W15-0115,P03-1054,0,0.0292697,"5 n/a n/a 0.78 0.59 0.82 n/a n/a Recall 0.65 0.34 0.67 n/a n/a 0.21 0.31 0.25 n/a n/a AUC 0.76 0.63 0.78 0.73 0.80 0.60 0.64 0.62 0.47 0.56 Table 2: Results of Wish Detection and Comparison with Goldberg et. al. 2009 Data products Features unigrams subjunctive unigrams,subjunctive Precision 0.29 0.29 0.33 Recall 0.02 0.11 0.02 AUC 0.51 0.54 0.51 Table 3: Results of Suggestion Detection We also obtain classification results of the combination of these features with the standard unigram features (Table 2, 3). To obtain the part of speech and dependency information, we use Stanford Parser 3.3.1 (Klein and Manning, 2003). Word stemming is not performed. We use the LibSVM implementation of SVM classifier (EL-Manzalawy and Honavar, 2005). The parameter values of SVM classifiers are: SVM type = C-SVC, Kernel Function = Radial Basis Function. Features are ranked using the Info- Gain feature selection algorithm (Mitchell, 1997). Top 1000 features are used in all the experiments ie. the size of feature vector is not more than 1000. 5 Subjunctive Feature Evaluation Goldberg et al. (2009) evaluated their approach using a 10 fold cross validation on their datasets. In order to compare subjunctive features against thei"
W15-0115,J12-2001,0,0.0607685,"ne said that to teddy at the meeting yesterday. Perhaps I should have stopped at 8 or 9 years old. I would like to know if you re a purist or a hypocrite. I wish it were summer. I suggest that Dawn drive the car. But if it weren’t so big, it wouldn’t be nearly so fun. Table 1: Examples of Suggestions, Wishes, and Subjunctive Mood 2 Related work Mood and Modality: Modality is a grammatical category that allows the expression of aspects related to the attitude of a speaker towards his statement, in terms of degree of certainty, reliability, subjectivity, sources of information, and perspective (Morante and Sporleder, 2012). Subjunctive mood originated from the typological studies of modality (Palmer, 1986; Dudman, 1988; Portner, 2009). Some works equate its presence with ‘counterfactuality’(Palmer, 1986), while some do not (Anderson, 1951). Other concepts like ‘event modality’, ‘irrealis’ (Palmer, 1986), have definitions similar to that of subjunctive mood. Benamara et al. (2012) studied modality and negation for French language, with an objective to examine its effect on sentiment polarity. Narayanan et al. (2009) performed sentiment analysis on conditional sentences. Our objective however is inclined towards"
W15-0115,D09-1019,0,0.182209,"of degree of certainty, reliability, subjectivity, sources of information, and perspective (Morante and Sporleder, 2012). Subjunctive mood originated from the typological studies of modality (Palmer, 1986; Dudman, 1988; Portner, 2009). Some works equate its presence with ‘counterfactuality’(Palmer, 1986), while some do not (Anderson, 1951). Other concepts like ‘event modality’, ‘irrealis’ (Palmer, 1986), have definitions similar to that of subjunctive mood. Benamara et al. (2012) studied modality and negation for French language, with an objective to examine its effect on sentiment polarity. Narayanan et al. (2009) performed sentiment analysis on conditional sentences. Our objective however is inclined towards wish and suggestion detection, rather than sentiment analysis. Wish Detection: Goldberg et al. (2009) performed wish detection on datasets obtained from political discussion forums and product reviews. They automatically extracted sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. Suggestion Detection: Ramanand et al. (2010) pointed out that wish is a broader category, which might not bear suggestions every time. They performed suggestion"
W15-0115,W10-0207,0,0.87494,"resent in such sentences may not necessarily contribute to the actual sentiment of the sentence, for example ‘I wish it tasted as amazing as it looked’ is not positive. While this is considered as a challenge for sentiment analysis, we adopt a different perspective, and discover benefits of the presence of subjunctive mood in opinionated text. Apart from the expression of criticism and satisfaction in customer reviews, reviews might include suggestions for improvements. Suggestions can either be expressed explicitly (Brun, 2013), or by expressing wishes regarding new features and improvements(Ramanand et al., 2010) (Table 1). Extraction of suggestions goes beyond the scope of sentiment analysis, and also complements it by providing another valuable information that is worth analyzing. Table 1 presents some examples of occurrence of subjunctive mood collected from different forums on English grammar1 . There seems to be a high probability of the occurrence of subjunctive mood in wish and suggestion expressing sentences. This observation can be exploited for the tasks of wish detection (Ramanand et al., 2010), and suggestion extraction (Brun, 2013). To the best of our knowledge, subjunctive mood has never"
W15-4205,calzolari-etal-2012-lre,0,0.098711,"ar}@insight-centre.org Abstract statistical taggers, statistical parsers, and statistical machine translation systems) or they require lexico-semantic resources as background knowledge to perform some task (e.g. word sense disambiguation). As the number of language resources available keeps growing, the task of discovering and finding resources that are pertinent to a particular task becomes increasingly difficult. While there are a number of repositories that collect and index metadata of language resources, such as META-SHARE (Federmann et al., 2012), CLARIN (Broeder et al., 2010), LRE-Map (Calzolari et al., 2012), Datahub.io1 and OLAC (Simons and Bird, 2003), they do not provide a complete solution to the discovery problem for two reasons. First, integrated search over all these different repositories is not possible, as they use different data models, different vocabularies and expose different interfaces and APIs. Second, these repositories must strike a balance between quality and coverage, either opting for coverage at the expense of quality of metadata, or vice versa. When collecting metadata from multiple resources, we understand that there are two principal challenges: property harmonization an"
W15-4205,choukri-etal-2012-using,0,0.0195543,"x XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want to submit the resources accompanying papers submitted to conferences. Most fields in LRE-Map consist of a text field with some prespecified options to select and a thorough analysis of the results has been conducted (Mariani et al., 2014). Similarly, the Open Linguistics Working Group (Chiarcos et al., 2012) has been collecting language resources published as linked data in a Related Work Interoperability of metadata is an important pr"
W15-4205,de-marneffe-etal-2006-generating,0,0.0158775,"Missing"
W15-4205,federmann-etal-2012-meta,0,0.277944,"Missing"
W15-4205,broeder-etal-2010-data,0,0.0785251,"Missing"
W15-4205,mariani-etal-2014-facing,0,0.0288608,"Missing"
W15-4205,piperidis-2012-meta,0,0.0544122,"oaches have been pursued to collect metadata of resources. Large consortium-led projects and initiatives such as the CLARIN projects and METANET have attempted to create metadata standards for representing linguistic data. Interoperability of the data stemming from these two repositories is however severely limited due to incompatibilities in their data models. META-SHARE favors a qualitative approach in which a relatively complex XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want t"
W15-4205,Q14-1019,1,\N,Missing
W15-4205,gavrilidou-etal-2012-meta,0,\N,Missing
W18-0910,D17-1316,0,0.023835,"Missing"
W18-0910,P16-1018,0,0.0363228,"Missing"
W18-0910,E06-1042,0,0.0359598,"enses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work, Shutova and Sun (2013) investigated the use of hierarchical graph factorization clustering to derive a network of concepts in order to learn metaphorical associations in an unsupervised way which then was used as features to identify metaphors. We consider the work introduced by Shutova et al. (2010) as a baseline for our proposed approach, thus we are going to explain its reimplementation details in subsection 3.3. Birke and Sarkar (2006) introduced TroFi, which is considered the first statistical system to identify the metaphorical senses of verbs in a semi-supervised way. The authors adapted a statistical similarity-based word sense disambiguation approach to cluster literal and non-literal senses. A predefined set of seed sentences is utilised to compute the similarity between a given sentence and the seed sentences. 3 3.1 Hypothesis Our hypothesis in this work is that a given candidate should have common characteristics and semantic features with some positive examples of metaphors. However, simply calculating the similari"
W18-0910,W13-0907,0,0.021453,"weighted cosine similarity function is used to automatically select the important vector dimensions for the metaphor detection task. The authors experimented with different pretrained word representations, namely skip-gram model and an attribute-based model. Two different datasets, which were referred to as the TSV dataset 2 82 the WordNet lexicographer name of the words first sense the state-of-the-art semi-supervised system used as our baseline system. baum, 1998) have been employed to develop supervised systems to detect metaphors (K¨oper and Schulte im Walde, 2017; Tsvetkov et al., 2013; Hovy et al., 2013; Turney et al., 2011). Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manne"
W18-0910,P06-4020,0,0.0117563,"The cosine similarity between the candidates “break promise” and “break glass” and the top 10 metaphoric seeds in the seed set using a pre-trained Word2Vec word embedding model on Google News dataset. 2009) and consists of 62 metaphoric verb-noun pairs (more details are given in section 4). Spectral clustering (Meila and Shi, 2001) is used to cluster the abstract concepts (nouns) and the concrete concepts (verbs) then an association (mapping) is drawn between the two clusters using the seed set. The candidate extraction component employs the Robust Accurate Statistical Parsing (RASP) parser (Briscoe et al., 2006) to extract verb-subject and verb-direct object grammar relations. After that, the linked clusters (through the seed set) is used to identify potential metaphoric candidates. The filtering component is finally used to filter out these candidates based on a selectional preferences strength (SPS) measure (Resnik, 1993). The verbs exhibiting weak selectional preferences are considered to have lower metaphorical potential. An SPS threshold was set experimentally to be 1.32, thus, the candidates which verbs have an SPS value below this threshold are discarded. Dts = d(vt , vs ) ∀vs ∈ S This gives a"
W18-0910,W17-1903,0,0.025657,"Missing"
W18-0910,E17-2084,0,0.055548,"l., 2016). In this work, a word or an expression is a metaphor if it has at least one basic/literal sense (more concrete, physical) and a secondary metaphoric sense (abstract, 1 These examples could be found in the United Nations Parallel Corpus (Ziemski et al., 2016). 81 Proceedings of the Workshop on Figurative Language Processing, pages 81–90 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics (Tsvetkov et al., 2013) and the MOH dataset (Mohammad et al., 2016), were used to train the system and optimise its parameters as well as to assess its performance. Bulat et al. (2017) is a recent approach that investigated whether property-based semantic word representation can provide better concept generalisation for detecting metaphors than dense linguistic representation. The authors proposed property-based vectors through cross-modal mapping between dense linguistic representations and a property-norm semantic space. The authors built a count-based distributional vector and employed a skip-gram model trained on Wikipedia articles as their dense linguistic representations. The property-norm semantic space is obtained from the property-norm dataset (McRae et al., 2005)."
W18-0910,P14-5010,0,0.00246872,"n the clusters or not. And if the candidate’s noun appeared in a noun cluster but this cluster was not mapped to the cluster where the verb occurs the candidate will be discarded. 4 System Architecture As described in Figure 1 below, our system consists of three main components: a parser, a seed set of metaphoric expressions and a pre-trained word embedding model. Parser: Since our aim is to identify metaphors on the phrase-level, the Stanford parser is used to extract the grammar relations in a given sentence. We used the recurrent neural network (RNN) parser in the Stanford CoreNLP toolkit (Manning et al., 2014) to extract dependencies focusing on verb-subject and verb-direct object grammar relations. Seed Set: We used the seed set of Shutova et al. (2010) to act as our set of existing known metaphoric expressions (positive examples). The seed set consists of 62 verb-subject and verb-direct object phrases where the verb is used metaphorically3 . These seeds are extracted originally from a subset of the BNC corpus which contains 761 sentences. These sentences were annotated for grammatical relations to extract the specified grammar relations which are then filtered and manually annotated for metaphori"
W18-0910,N13-1118,0,0.0178746,"Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work, Shutova and Sun (2013) investigated the use of hierarchical graph factorization clustering to derive a network of concepts in order to learn metaphorical associations in an unsupervised way which then was used as features to identify metaphors. We consider the work introduced by Shutova et al. (2010) as a baseline for our proposed approach, thus we are going to explain its reimplementation details in subsection 3.3. Birke and Sarkar (2006) introduced TroFi, which is considered the first statistical system to identify the metaphorical senses of verbs in a semi-supervised way. The authors adapted a statistical simila"
W18-0910,C10-1113,0,0.304198,"sed to automatically select the important vector dimensions for the metaphor detection task. The authors experimented with different pretrained word representations, namely skip-gram model and an attribute-based model. Two different datasets, which were referred to as the TSV dataset 2 82 the WordNet lexicographer name of the words first sense the state-of-the-art semi-supervised system used as our baseline system. baum, 1998) have been employed to develop supervised systems to detect metaphors (K¨oper and Schulte im Walde, 2017; Tsvetkov et al., 2013; Hovy et al., 2013; Turney et al., 2011). Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work, Shutova and Sun (2013"
W18-0910,S16-2003,0,0.0591525,"Missing"
W18-0910,W13-0906,0,0.115478,"taphoric expressions such as “...eradicate poverty”, “...root out the causes of poverty”, or “...the roots of poverty are...”1 (Lakoff and Johnson, 1980; Veale et al., 2016). In this work, a word or an expression is a metaphor if it has at least one basic/literal sense (more concrete, physical) and a secondary metaphoric sense (abstract, 1 These examples could be found in the United Nations Parallel Corpus (Ziemski et al., 2016). 81 Proceedings of the Workshop on Figurative Language Processing, pages 81–90 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics (Tsvetkov et al., 2013) and the MOH dataset (Mohammad et al., 2016), were used to train the system and optimise its parameters as well as to assess its performance. Bulat et al. (2017) is a recent approach that investigated whether property-based semantic word representation can provide better concept generalisation for detecting metaphors than dense linguistic representation. The authors proposed property-based vectors through cross-modal mapping between dense linguistic representations and a property-norm semantic space. The authors built a count-based distributional vector and employed a skip-gram model trained o"
W18-0910,D14-1162,0,0.0825187,"7371 question 0.8462 question 0.9424 promise Cand. N glass Table 2: The cosine distance between the verbs and nouns of the candidates “break promise” and “break glass” verses the verbs and the nouns of the top 10 metaphoric seeds in the seed set using a pre-trained Word2Vec word embedding model on Google News dataset. metaphors in the seed set are “mend marriage, break agreement, cast doubt, and stir excitement”. Word Embedding Model: This work utilises distributional vector representation of word meaning to calculate semantic similarity between a candidate and a seed set. Word2Vec and GloVe (Pennington et al., 2014) are two widely used word embeddings algorithms to construct embeddings vectors based on the distributional hypothesis (Firth, 1957) but using different machine learning techniques. In this work, we investigated the effect of using different pre-trained models and similarity measures as shown in detail in the next section. This is one of the limitations of this system; a candidate is either in the clusters or not. And if the candidate’s noun appeared in a noun cluster but this cluster was not mapped to the cluster where the verb occurs the candidate will be discarded. 4 System Architecture As"
W18-0910,D11-1063,0,0.0287244,"milarity function is used to automatically select the important vector dimensions for the metaphor detection task. The authors experimented with different pretrained word representations, namely skip-gram model and an attribute-based model. Two different datasets, which were referred to as the TSV dataset 2 82 the WordNet lexicographer name of the words first sense the state-of-the-art semi-supervised system used as our baseline system. baum, 1998) have been employed to develop supervised systems to detect metaphors (K¨oper and Schulte im Walde, 2017; Tsvetkov et al., 2013; Hovy et al., 2013; Turney et al., 2011). Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work"
W18-0910,D17-1162,0,0.0467174,"ammatical relations. In this paper, we are interested in phrase-level linguistic metaphor detection, focusing on verbnoun phrases (grammatical relations) by employing semantic representation of word meaning. Therefore, due to space limitation, we will discuss the most relevant research in this regard in this section. An extensive literature review is presented in (Zhou et al., 2007; Shutova, 2015). Some recent work on metaphor detection has been looking into the utilization of semantic representations through word embeddings representations to design supervised systems for metaphor detection (Rei et al., 2017; Bulat et al., 2017; Shutova et al., 2016). Our approach also utilises such representations but in a semi-supervised manner to avoid the need for large training corpora. Rei et al. (2017) introduced a neural network architecture to detect adjective-noun and verb-noun metaphoric constructions. Their system comprises three main components which are: word gating, vector representation mapping and a weighted similarity function. The word gating is used to model the association between the properties of the source and target domains which is done via a non-linear transformation of the word embeddi"
W18-0910,J15-4002,0,0.018927,"identification” which is concerned with recognising (detecting) the metaphoric expressions in the input text. Metaphor detection could be done on the word-level (token-level) or on the phrase-level by extracting grammatical relations. In this paper, we are interested in phrase-level linguistic metaphor detection, focusing on verbnoun phrases (grammatical relations) by employing semantic representation of word meaning. Therefore, due to space limitation, we will discuss the most relevant research in this regard in this section. An extensive literature review is presented in (Zhou et al., 2007; Shutova, 2015). Some recent work on metaphor detection has been looking into the utilization of semantic representations through word embeddings representations to design supervised systems for metaphor detection (Rei et al., 2017; Bulat et al., 2017; Shutova et al., 2016). Our approach also utilises such representations but in a semi-supervised manner to avoid the need for large training corpora. Rei et al. (2017) introduced a neural network architecture to detect adjective-noun and verb-noun metaphoric constructions. Their system comprises three main components which are: word gating, vector representatio"
W18-0910,L16-1561,0,0.0163589,"be transferred to another concept’s sense such as “poverty” by exploiting the properties of the first concept. This then can be expressed in our everyday language in terms of linguistic metaphoric expressions such as “...eradicate poverty”, “...root out the causes of poverty”, or “...the roots of poverty are...”1 (Lakoff and Johnson, 1980; Veale et al., 2016). In this work, a word or an expression is a metaphor if it has at least one basic/literal sense (more concrete, physical) and a secondary metaphoric sense (abstract, 1 These examples could be found in the United Nations Parallel Corpus (Ziemski et al., 2016). 81 Proceedings of the Workshop on Figurative Language Processing, pages 81–90 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics (Tsvetkov et al., 2013) and the MOH dataset (Mohammad et al., 2016), were used to train the system and optimise its parameters as well as to assess its performance. Bulat et al. (2017) is a recent approach that investigated whether property-based semantic word representation can provide better concept generalisation for detecting metaphors than dense linguistic representation. The authors proposed property-based vectors through c"
W18-0910,N16-1020,0,0.254651,"emantic space. The authors built a count-based distributional vector and employed a skip-gram model trained on Wikipedia articles as their dense linguistic representations. The property-norm semantic space is obtained from the property-norm dataset (McRae et al., 2005). The TSV dataset is used to train and test a support vector machine (SVM) classifier to classify adjective-noun pairs using the introduced cognitively salient properties as features. An interesting approach, which employed multi-model embeddings of visual and linguistic features to detect metaphoricity in text, is introduced by Shutova et al. (2016). The proposed approach obtained linguistic word embeddings using a log-linear skip-gram model trained on Wikipedia text and obtained visual embeddings using a deep convolutional neural network trained on image data. This was done for both the words and phrases of adjective-noun and verb-noun pairs individually. Then, the cosine similarity function has been employed to measure the distance between the phrase vector and the corresponding vectors of its constituent words. Metaphor classification is done based on an optimised threshold output of the cosine similarity function. The authors used th"
W18-3107,daudert-2017-analysing,1,0.731608,"lj et al., 2010). As interpretability comes at the cost of flexibility, accuracy, or efficiency (Ribeiro et al., 2016), the consideration of the trade-off between classifier types becomes essential. This is notably the case for automated trading and medical diagnosis (Caruana et al., 2015) where the application of a ”black box” algorithm can pose a significant risk. Although potentially less powerful, machine learning approaches based on simpler algorithms allow for the identification of the components responsible for the achieved prediction. This work is inspired by the proposal described in Daudert (2017); specifically, it exploits the idea of utilising a combination of multiple sentiments. Our work conducts the first step into a new direction by focusing on the achievement of a superior sentiment classification trough the exploitation of the relations between different sentiments. (Sinha, 2014). Good news tend to lift markets and increase optimism, bad news tend to lower markets (Schuster, 2003; Van De Kauter et al., 2015). Not only news are an important factor for the markets. In 2011, Bollen et al. (2011) showed that changes in public mood reflect value shifts in the Dow Jones Industrial In"
W18-6216,P11-1015,0,0.0561651,"ilarity between word pairs, instead of only utilising a word frequency measure, thus, capable of capturing opinions and sentiments that are implicitly expressed in a text and, overall, contributing to improved clustering. Tang et al. (2016) focused on learning word embeddings defined not only by context but also by sentiment. Their approach is able to better capture nearest neighboring vectors not only through their semantic similarity but also favoring the same sentiment polarity. This novel idea of utilising word embeddings to better capture polarity in documents was initially brought up by Maas et al. (2011). The work described in this paper aims to address the existing knowledge gap concerning the application of distributional semantics for sentiment linking and assigning. tion, factors affecting people’s sentiment rise. This includes other people’s textually-expressed sentiment since information is not always presented in a neutral manner. However, the relation between sentiments across different data sources, how they affect each other, and how this can be leveraged for sentiment classification has not been investigated yet. 2.1 Linking Sentiments Across Data Sources Daudert et al. (2018) goes"
W18-6216,P04-3031,0,0.129096,"aches employing a threshold for determining the relevance of a news to a microblog’s sentiment (approach 1 and 3) aim at improving the sentiment linking since they fully discard news below a certain similarity value. The remaining two context-based approaches using a weighting scheme are reducing the impact of less relevant news on a microblog’s sentiment and are, hence, aiming at improving the assigning of sentiment. This occurs in multiple steps: First, URLs in microblogs as well as news titles and descriptions are removed. Second, microblogs are tokenised employing the NLTK TweetTokenizer (Bird and Loper, 2004); news titles and descriptions are tokenized using the Stanford CoreNLP Tokenizer (Manning et al., 2014). matching pieces of text according to predefined criteria such entities, text intersections, or a degree of textual similarity. Hereby, we assume that linked sentiments are either influenced by the same cause or affecting each other. Conveyance - The conveyance of sentiment describes the influence of the sentiment of one text on the sentiment of another. Sentiment is (indirectly) fully or partially transfered from a piece of text A to a piece of text B. Assigning - The assigning of sentimen"
W18-6216,P14-5010,0,0.00268818,"1 and 3) aim at improving the sentiment linking since they fully discard news below a certain similarity value. The remaining two context-based approaches using a weighting scheme are reducing the impact of less relevant news on a microblog’s sentiment and are, hence, aiming at improving the assigning of sentiment. This occurs in multiple steps: First, URLs in microblogs as well as news titles and descriptions are removed. Second, microblogs are tokenised employing the NLTK TweetTokenizer (Bird and Loper, 2004); news titles and descriptions are tokenized using the Stanford CoreNLP Tokenizer (Manning et al., 2014). matching pieces of text according to predefined criteria such entities, text intersections, or a degree of textual similarity. Hereby, we assume that linked sentiments are either influenced by the same cause or affecting each other. Conveyance - The conveyance of sentiment describes the influence of the sentiment of one text on the sentiment of another. Sentiment is (indirectly) fully or partially transfered from a piece of text A to a piece of text B. Assigning - The assigning of sentiment models the conveyance of sentiment from a text to another. Given two linked sentiments and the hypothe"
W18-6216,W18-3107,1,0.769325,"itude of instantly available informa107 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 107–115 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Tsagkias et al. (2011)). Abel et al. (2011) suggests five different approaches of linking news to tweets: 1) a strict URL-based strategy, 2) a lenient URL-based strategy, 3) a bag-of-words strategy, 4) a hashtag-based strategy, and 5) an entity-based strategy. Strategy 5) comes close to what has been explored by Daudert et al. (2018) whereby our approach is inspired by 3), employing it as an addon to 5). Other related research considering the combination of semantic similarity and sentiment analysis are (Tang et al., 2016; Poria et al., 2016). Poria et al. (2016) developed a Latent Dirichlet Allocation algorithm considering the semantic similarity between word pairs, instead of only utilising a word frequency measure, thus, capable of capturing opinions and sentiments that are implicitly expressed in a text and, overall, contributing to improved clustering. Tang et al. (2016) focused on learning word embeddings defined no"
W18-6216,N13-1090,0,0.00469325,"gt. weighted, and Agg. aggregated. A p-value &lt; 0.01 is achieved for all models with the exception of TS Thr. on subset B (marked with * ) which achieves a p-value &lt; 0.05. The classifications based on microblog messages and context-based news sentiment are represented in gray (columns 4-7). tween a news and a microblog. We choose different tokenizers for microblogs and news as the TweetTokenizer is specifically made for microblogs while news require a tokenizer adapted to a different structure and length. Third, we convert the Stanford GloVe Twitter model (Pennington et al., 2014) to Word2Vec (Mikolov et al., 2013a) and obtain the word embeddings. Having the word embeddings for microblogs and news in place, the subsequent processing varied depending on the context-based approach. 3.3 N ST BAm = s(n1 )+s(n2 ) 2 (1) The first context-based approach generates the NSTBA as an average of the sentiments of the microblog-related news articles. Document embeddings are retrieved for each microblog and news by averaging the word embeddings (Kartsaklis, 2014). We employ the cosine similarity as measure since vector offsets have been shown to be effective (Mikolov et al., 2013b). To be considered as context-relate"
W18-6216,P13-1024,0,0.0253146,"rch employs a distributional semantics approach to remove noise in terms of microblog-unrelated news sentiment although dealing with the same entity. To the best of our knowledge, only the previously mentioned work has started investigating the relations between the sentiments and leveraged them for microblog sentiment classification, hence, there is no research on the use of distributional semantics for sentiment linking. On the other hand, research targeting the field of semantic enrichment is available and it is particularly relevant when addressing the linking of news and microblogs (e.g. Guo et al. (2013); Wei et al. (2014); Abel et al. (2011); 3 Methodology The work performed is divided into two parts: the preparation of the data, and its use in a Machine Learning (ML) prediction model. Throughout this paper, we implement the methodology described by Daudert et al. (2018), utilising the same datasets (section 3.1) and experimental setup (section 3.4). We extend their previous work by improving the method to link a news sentiment to a microblog as well as to assign a news sentiment to a microblog (section 3.2). The aim of this research is to explore the relation of sentiments between news and"
W18-6216,S17-2152,0,0.039622,"Missing"
W18-6216,D14-1162,0,0.0834072,"milarity. Thr. represents threshold, Wgt. weighted, and Agg. aggregated. A p-value &lt; 0.01 is achieved for all models with the exception of TS Thr. on subset B (marked with * ) which achieves a p-value &lt; 0.05. The classifications based on microblog messages and context-based news sentiment are represented in gray (columns 4-7). tween a news and a microblog. We choose different tokenizers for microblogs and news as the TweetTokenizer is specifically made for microblogs while news require a tokenizer adapted to a different structure and length. Third, we convert the Stanford GloVe Twitter model (Pennington et al., 2014) to Word2Vec (Mikolov et al., 2013a) and obtain the word embeddings. Having the word embeddings for microblogs and news in place, the subsequent processing varied depending on the context-based approach. 3.3 N ST BAm = s(n1 )+s(n2 ) 2 (1) The first context-based approach generates the NSTBA as an average of the sentiments of the microblog-related news articles. Document embeddings are retrieved for each microblog and news by averaging the word embeddings (Kartsaklis, 2014). We employ the cosine similarity as measure since vector offsets have been shown to be effective (Mikolov et al., 2013b)."
W18-6216,C14-1083,0,0.0242823,"ibutional semantics approach to remove noise in terms of microblog-unrelated news sentiment although dealing with the same entity. To the best of our knowledge, only the previously mentioned work has started investigating the relations between the sentiments and leveraged them for microblog sentiment classification, hence, there is no research on the use of distributional semantics for sentiment linking. On the other hand, research targeting the field of semantic enrichment is available and it is particularly relevant when addressing the linking of news and microblogs (e.g. Guo et al. (2013); Wei et al. (2014); Abel et al. (2011); 3 Methodology The work performed is divided into two parts: the preparation of the data, and its use in a Machine Learning (ML) prediction model. Throughout this paper, we implement the methodology described by Daudert et al. (2018), utilising the same datasets (section 3.1) and experimental setup (section 3.4). We extend their previous work by improving the method to link a news sentiment to a microblog as well as to assign a news sentiment to a microblog (section 3.2). The aim of this research is to explore the relation of sentiments between news and microblogs, hence,"
W97-0205,H92-1022,0,0.0665959,"Missing"
W97-0205,J90-1003,0,0.00883602,"assification of nouns that are known correspond to those that are unknown. An additional measure of the effectiveness of the classifter is measuring the recall on classification of all nouns, known and unknown. This number seems to correlate with the size of the corpus, in larger corpora more nouns are being classified, but not necessarily more correctly. Correct classification rather seems to depend on the homogeneity of the corpus: if it is written in one style, with one theme and so The classifier uses mutual information (MI) scores rather than the raw frequences of the occurring patterns (Church and Hanks, 1990). Computing MI scores is by now a standard procedure for measuring the co-occurrence between objects relative to their overall occurrence. MI is defined in general as follows: y) I ix y) = log2 P(x) P(y) on. We can use this definition to derive an estimate of the connectedness between words, in terms of collocations (Smadja, 1993), but also in terms of phrases and grammatical relations (Hindle, 1990). For instance the co-occurrence of verbs and the heads of their NP objects iN: size of the corpus, i.e. the number of stems): Recall of the classifier (percentage of all nouns that are classified"
W97-0205,P90-1034,0,0.104384,"Missing"
W97-0205,J93-1007,0,0.0342526,"Missing"
W97-0205,C92-2070,0,0.223363,"Missing"
W97-0205,W91-0209,0,\N,Missing
W97-0205,P88-1012,0,\N,Missing
wolff-etal-2014-missed,macklovitch-russell-2000-whats,0,\N,Missing
wolff-etal-2014-missed,steinberger-etal-2012-dgt,0,\N,Missing
wolff-etal-2014-missed,P06-2111,0,\N,Missing
