2020.acl-main.157,N18-2004,0,0.318085,"ble benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain. The entire dataset is released for future research2 . 1 Introduction Apart from constituting an interesting task on its own, stance detection has been identified as a crucial sub-step towards many other NLP tasks (Mohammad et al., 2017). In fact, stance detection is the core component of fake news detection (Pomerleau and Rao, 2017), fact-checking (Vlachos and Riedel, 2014; Baly et al., 2018), and rumor verification (Zubiaga et al., 2018b). Despite its importance, stance detection suffers from the lack of a large dataset which would allow for reliable comparison between models. We aim at filling this gap by presenting Will-They-Won’tThey (WT– WT), a large dataset of English tweets targeted at stance detection for the rumor verification task. We constructed the dataset based on tweets, since Twitter is a highly relevant platform for rumour verification, which is popular with the public as well as politicians and enterprises (Gorrell et al., 2019). To make the dataset representative"
2020.acl-main.157,S17-2144,0,0.0136497,"r and Perella, 2004; Piesse et al., 2013). In this sense, the analysis of the evolution of opinions and concerns expressed by users about a possible M&A deal, from its early stage to its closing (or its rejection) stage, is a process similar to rumor verification (Zubiaga et al., 2018a). Moreover, despite the wide interest, most research in the intersection of NLP and finance has so far focused on sentiment analysis, text mining and thesauri/taxonomy generation (Fisher et al., 2016; Hahn et al., 2018; El-Haj et al., 2018). While sentiment (Chan and Chong, 2017) and targetedsentiment analysis (Chen et al., 2017) have an undisputed importance for analyzing financial markets, research in stance detection takes on a crucial role: in fact, being able to model the market’s perception of the merger might ultimately contribute to explaining stock price re-valuation. We make the following three contributions. Firstly, we construct and release WT– WT, a large, expert-annotated Twitter stance detection dataset. With its 51,284 tweets, the dataset is an order of magnitude larger than any other stance detection dataset of user-generated data, and could be used to train and robustly compare neural models. To our"
2020.acl-main.157,E17-2088,0,0.202834,"ost operations, there is a clear correlation between the relative proportion of refuting and supporting samples and the merger being approved or blocked by the US Department of Justice. Commenting tweets are more frequent than supporting over all operations: this is in line with previous findings in financial microblogging (Žnidaršiˇc et al., 2018). 2.6 Comparison with Existing Corpora The first dataset for Twitter stance detection collected 4,870 tweets on 6 political events (Mohammad et al., 2016a) and was later used in SemEval2016 (Mohammad et al., 2016b). Using the same annotation schema, Inkpen et al. (2017) released a corpus on the 2016 US election annotated for multi-target stance. In the scope of P HEME, a large project on rumor resolution (Derczynski and Bontcheva, 2014), Zubiaga et al. (2015) stanceannotated 325 conversational trees discussing 9 breaking news events. The dataset was used in RumourEval 2017 (Derczynski et al., 2017) and was later extended with 1,066 tweets for RumourEval 2019 (Gorrell et al., 2019). Following the same procedure, Aker et al. (2017) annotated 401 tweets on mental disorders (Table 3). This makes the proposed dataset by far the largest publicly available dataset"
2020.acl-main.157,S17-2083,0,0.0453654,"Missing"
2020.acl-main.157,C18-1288,0,0.0226635,"efforts on stance detection (Hanselowski et al., 2018). Moreover, w.r.t. stance datasets where unrelated samples were randomly generated (Pomerleau and Rao, 2017; Hanselowski et al., 2018), we report a slightly 6 The average κ was weighted by the number of samples annotated by each pair. The standard deviation of the κ scores between single annotator pairs is 0.074. Label Distribution The distribution of obtained labels for each operation is reported in Table 2. Differences in label distribution between events are usual, and have been observed in other stance corpora (Mohammad et al., 2016a; Kochkina et al., 2018). For most operations, there is a clear correlation between the relative proportion of refuting and supporting samples and the merger being approved or blocked by the US Department of Justice. Commenting tweets are more frequent than supporting over all operations: this is in line with previous findings in financial microblogging (Žnidaršiˇc et al., 2018). 2.6 Comparison with Existing Corpora The first dataset for Twitter stance detection collected 4,870 tweets on 6 political events (Mohammad et al., 2016a) and was later used in SemEval2016 (Mohammad et al., 2016b). Using the same annotation s"
2020.acl-main.157,L16-1623,0,0.53442,"served in other research efforts on stance detection (Hanselowski et al., 2018). Moreover, w.r.t. stance datasets where unrelated samples were randomly generated (Pomerleau and Rao, 2017; Hanselowski et al., 2018), we report a slightly 6 The average κ was weighted by the number of samples annotated by each pair. The standard deviation of the κ scores between single annotator pairs is 0.074. Label Distribution The distribution of obtained labels for each operation is reported in Table 2. Differences in label distribution between events are usual, and have been observed in other stance corpora (Mohammad et al., 2016a; Kochkina et al., 2018). For most operations, there is a clear correlation between the relative proportion of refuting and supporting samples and the merger being approved or blocked by the US Department of Justice. Commenting tweets are more frequent than supporting over all operations: this is in line with previous findings in financial microblogging (Žnidaršiˇc et al., 2018). 2.6 Comparison with Existing Corpora The first dataset for Twitter stance detection collected 4,870 tweets on 6 political events (Mohammad et al., 2016a) and was later used in SemEval2016 (Mohammad et al., 2016b). Us"
2020.acl-main.157,S16-1003,0,0.140981,"Missing"
2020.acl-main.157,D14-1162,0,0.088668,"Missing"
2020.acl-main.157,C18-1203,0,0.0561482,"et al., 2014), which are shared between tweets and targets and kept fixed during training. 3.2 Results and Discussion Results of experiments are reported in Table 4. Despite its simple architecture, SiamNet obtains the best performance in terms of both averaged and weighted averaged F1 scores. In line with previous findings (Mohammad et al., 2017), the SVM model constitutes a very strong and robust baseline. The relative gains in performance of CrossNet w.r.t. BiCE, and of HAN w.r.t. TAN, consistently reflect results obtained by such models on the SemEval 2016-Task 6 corpus (Xu et al., 2018; Sun et al., 2018). Moving to single labels classification, analysis of the confusion matrices shows a relevant number of misclassifications between the support and comment classes. Those classes have been found difficult to discriminate in other datasets as well (Hanselowski et al., 2018). The presence of linguistic features, as in the HAN model, may help in spotting the nuances in the tweet’s argumentative structure which allow for its correct classification. This may hold true also for the refute class, the least common and most difficult to discriminate. Unrelated samples in WT– WT could be about the involv"
2020.acl-main.157,S16-1067,0,0.0688256,"Missing"
2020.acl-main.157,W14-2508,0,0.0159342,"s a high-quality and reliable benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain. The entire dataset is released for future research2 . 1 Introduction Apart from constituting an interesting task on its own, stance detection has been identified as a crucial sub-step towards many other NLP tasks (Mohammad et al., 2017). In fact, stance detection is the core component of fake news detection (Pomerleau and Rao, 2017), fact-checking (Vlachos and Riedel, 2014; Baly et al., 2018), and rumor verification (Zubiaga et al., 2018b). Despite its importance, stance detection suffers from the lack of a large dataset which would allow for reliable comparison between models. We aim at filling this gap by presenting Will-They-Won’tThey (WT– WT), a large dataset of English tweets targeted at stance detection for the rumor verification task. We constructed the dataset based on tweets, since Twitter is a highly relevant platform for rumour verification, which is popular with the public as well as politicians and enterprises (Gorrell et al., 2019). To make the da"
2020.acl-main.157,P18-2118,0,0.048404,"Missing"
2020.acl-main.157,H05-1044,0,0.279691,"Missing"
2020.acl-main.157,N16-1174,0,0.0419379,"Missing"
2020.acl-main.157,C16-1230,0,0.0462877,"Missing"
2020.acl-main.157,I13-1191,0,\N,Missing
2020.acl-main.157,N15-1178,0,\N,Missing
2020.acl-main.157,C18-1158,0,\N,Missing
2020.acl-main.157,S19-2147,0,\N,Missing
2020.acl-main.157,P18-2123,0,\N,Missing
2020.acl-main.157,S17-2089,0,\N,Missing
2020.coling-tutorials.2,Q17-1010,0,0.0473549,"hniques in NLP, in the broad sense. We will start by conventional word embeddings (e.g., Word2Vec and GloVe) and then move to other types of embeddings, such as sense-specific and graph alternatives. We will finalize with an overview of the trending contextualized representations (e.g., ELMo and BERT) and explain their potential and impact in NLP. 1 Description In this tutorial we will start by providing a historical overview on word-level vector space models, and word embeddings in particular. Word embeddings (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017)) have proven to be powerful keepers of prior knowledge to be integrated into downstream Natural Language Processing (NLP) applications. However, despite their flexibility and success in capturing semantic properties of words, the effectiveness of word embeddings are generally hampered by an important limitation, known as the meaning conflation deficiency: the inability to discriminate among different meanings of a word. A word can have one meaning (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanings depending on the context: an animal"
2020.coling-tutorials.2,P12-1092,0,0.073822,"ated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992). As a practical wa"
2020.coling-tutorials.2,D15-1200,0,0.0243516,"space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992). As a practical way to deal with the knowledge-acquisition bottleneck, an emerging branch of"
2020.coling-tutorials.2,K17-1012,1,0.841481,"s of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992). As a practical way to deal with the knowledge-acquisition bottleneck, an emerging branch of research has focused on directly integrating unsupe"
2020.coling-tutorials.2,K16-1006,0,0.130901,"n a 2D semantic space around the ambiguous word mouse. Having the word, with its different meanings, represented as a single point (vector) results in pulling together of semantically unrelated words, such as computer and rabbit. Figure 2: A general illustration of contextualized word embeddings and how they are integrated in NLP models. A language modelling component is responsible for analyzing the context of the target word (cell in the figure) and generating its dynamic embedding. i.e., their representations dynamically change depending on the context in which a word appears. Context2vec (Melamud et al., 2016) and ELMo (Peters et al., 2018a) are some of the early examples for this type of representation. These models represent the context of a target word by extracting the embedding of a word in context from a bi-directional LSTM language model. The latter further proposed a seamless integration into neural NLP systems, as depicted in Figure 2. More recently, Transformers (Vaswani et al., 2017) have proven very effective for encoding contextualized knowledge, thanks to their self-attention mechanism (Figure 3). BERT (Devlin et al., 2018), which is based on Transformers, has revolutionized the field"
2020.coling-tutorials.2,D14-1113,0,0.026328,"A word can have one meaning (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanings depending on the context: an animal or a computer device. Hence, mouse is said to be ambiguous. In fact, according to the Principle of Economical Versatility of Words (Zipf, 1949), frequent words tend to have more senses. Moreover, this meaning conflation can have additional negative impacts on accurate semantic modeling, e.g., semantically unrelated words that are similar to different senses of a word are pulled towards each other in the semantic space (Neelakantan et al., 2014; Pilehvar and Collier, 2016). In our example, the two semantically-unrelated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches hav"
2020.coling-tutorials.2,D14-1162,0,0.096322,"el synthesis of the main embedding techniques in NLP, in the broad sense. We will start by conventional word embeddings (e.g., Word2Vec and GloVe) and then move to other types of embeddings, such as sense-specific and graph alternatives. We will finalize with an overview of the trending contextualized representations (e.g., ELMo and BERT) and explain their potential and impact in NLP. 1 Description In this tutorial we will start by providing a historical overview on word-level vector space models, and word embeddings in particular. Word embeddings (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017)) have proven to be powerful keepers of prior knowledge to be integrated into downstream Natural Language Processing (NLP) applications. However, despite their flexibility and success in capturing semantic properties of words, the effectiveness of word embeddings are generally hampered by an important limitation, known as the meaning conflation deficiency: the inability to discriminate among different meanings of a word. A word can have one meaning (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanin"
2020.coling-tutorials.2,N18-1202,0,0.249928,"ambiguous word mouse. Having the word, with its different meanings, represented as a single point (vector) results in pulling together of semantically unrelated words, such as computer and rabbit. Figure 2: A general illustration of contextualized word embeddings and how they are integrated in NLP models. A language modelling component is responsible for analyzing the context of the target word (cell in the figure) and generating its dynamic embedding. i.e., their representations dynamically change depending on the context in which a word appears. Context2vec (Melamud et al., 2016) and ELMo (Peters et al., 2018a) are some of the early examples for this type of representation. These models represent the context of a target word by extracting the embedding of a word in context from a bi-directional LSTM language model. The latter further proposed a seamless integration into neural NLP systems, as depicted in Figure 2. More recently, Transformers (Vaswani et al., 2017) have proven very effective for encoding contextualized knowledge, thanks to their self-attention mechanism (Figure 3). BERT (Devlin et al., 2018), which is based on Transformers, has revolutionized the field of representation learning an"
2020.coling-tutorials.2,D18-1179,0,0.122379,"ambiguous word mouse. Having the word, with its different meanings, represented as a single point (vector) results in pulling together of semantically unrelated words, such as computer and rabbit. Figure 2: A general illustration of contextualized word embeddings and how they are integrated in NLP models. A language modelling component is responsible for analyzing the context of the target word (cell in the figure) and generating its dynamic embedding. i.e., their representations dynamically change depending on the context in which a word appears. Context2vec (Melamud et al., 2016) and ELMo (Peters et al., 2018a) are some of the early examples for this type of representation. These models represent the context of a target word by extracting the embedding of a word in context from a bi-directional LSTM language model. The latter further proposed a seamless integration into neural NLP systems, as depicted in Figure 2. More recently, Transformers (Vaswani et al., 2017) have proven very effective for encoding contextualized knowledge, thanks to their self-attention mechanism (Figure 3). BERT (Devlin et al., 2018), which is based on Transformers, has revolutionized the field of representation learning an"
2020.coling-tutorials.2,D16-1174,1,0.84262,"ng (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanings depending on the context: an animal or a computer device. Hence, mouse is said to be ambiguous. In fact, according to the Principle of Economical Versatility of Words (Zipf, 1949), frequent words tend to have more senses. Moreover, this meaning conflation can have additional negative impacts on accurate semantic modeling, e.g., semantically unrelated words that are similar to different senses of a word are pulled towards each other in the semantic space (Neelakantan et al., 2014; Pilehvar and Collier, 2016). In our example, the two semantically-unrelated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individu"
2020.coling-tutorials.2,N10-1013,0,0.0548651,", the two semantically-unrelated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992"
2020.coling-tutorials.2,P15-1173,0,0.041538,"Missing"
2020.emnlp-main.584,P12-1092,0,0.16532,"nseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinctions at the granularity level defined by the inventory. Stanford Contextual Word Similarity (Huang et al., 2012) is one of the first datasets that focuses on ambiguity but outside the boundaries of sense inventories, and as a similarity measurement between two words in their contexts. Pilehvar and Camacho-Collados (2019) highlighted some of the limitations of the dataset that prevent a reliable evaluation, and proposed the Word-in-Context (WiC) dataset. WiC is the closest dataset to ours, which provides around 10K instances (1400 instances for 1184 unique target nouns and verbs in the test set), but for the English language only. 2.2 Cross-lingual NLP A prerequisite for research on a language is the ava"
2020.emnlp-main.584,isahara-etal-2008-development,0,0.0482702,"on 3.2.2). 3.2.1 Multilingual WordNet WordNet (Miller, 1995) is the de facto sense inventory for English WSD. The resource was originally built as an English lexical database in 1995, but since then there have been many efforts to extend it to other languages (Bond and Paik, 2012). We took advantage of these extensions to construct XL-WiC. In particular, we processed the WordNet versions of Bulgarian (Simov and Osenova, 2010), Chinese (Huang et al., 2010), Croatian (Raffaelli et al., 2008), Danish (Pedersen et al., 2009), Dutch (Postma et al., 2016), Estonian (Vider and Orav, 2002), Japanese (Isahara et al., 2008), Korean (Yoon et al., 2009) and Farsi (Shamsfard et al., 2010).1 Farsi: Semi-automatic extraction. FarsNet v3.0 (Shamsfard et al., 2010) comprises 30K synsets with over 100K word entries. Many of these synsets are mapped to the English database; however, each synset provides just one example usage for a target word. This prevents us from applying the automatic extraction of positive examples. Therefore, we utilized a semi-automatic procedure for the construction of the Farsi set. To this end, for each word, we extracted all example usages from 1 We tried other WordNet versions such as Albania"
2020.emnlp-main.584,P18-4020,0,0.0372913,"Missing"
2020.emnlp-main.584,W17-3204,0,0.0247615,"Missing"
2020.emnlp-main.584,2020.acl-main.653,0,0.125474,"Missing"
2020.emnlp-main.584,P19-1124,0,0.0199364,"Missing"
2020.emnlp-main.584,S15-2049,0,0.612119,"ense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinctions at the granularity level defined by the inventory. Stanford Contextual Word Similarity (Huang et al., 2012) is one of the first datasets that focuses on ambiguity but outside the boun"
2020.emnlp-main.584,Q14-1019,1,0.840733,"is a benchmark for inventory-independent evaluation of WSD models (Section 2.1), while the multilingual nature of the dataset makes it an interesting resource for experimenting with crosslingual transfer (Section 2.2). 2.1 Word Sense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing th"
2020.emnlp-main.584,S13-2040,0,0.39929,"und 10K instances (1400 instances for 1184 unique target nouns and verbs in the test set), but for the English language only. 2.2 Cross-lingual NLP A prerequisite for research on a language is the availability of relevant evaluation benchmarks. Given its importance, construction of multilingual datasets has always been considered as a key contribution in NLP research and numerous benchmarks exist for a wide range of tasks, such as semantic parsing (Hershcovich et al., 2019), word similarity (Camacho-Collados et al., 2017; Barzegar et al., 2018), sentence similarity (Cer et al., 2017), or WSD (Navigli et al., 2013; Moro and Navigli, 2015b). A more recent example is XTREME (Hu et al., 2020), a benchmark that covers around 40 languages in nine syntactic and semantic tasks. On the other hand, pre-trained language models have recently proven very effective in transferring knowledge in cross-lingual NLP tasks (Devlin et al., 2019; Conneau et al., 2020). This has further magnified the requirement for rigorous multilingual benchmarks that can be used as basis for this direction of research (Artetxe et al., 2020b). 3 XL-WiC: The Benchmark In this section, we describe the procedure we followed to construct the"
2020.emnlp-main.584,P02-1040,0,0.109067,"Missing"
2020.emnlp-main.584,N19-1128,1,0.608713,"Missing"
2020.emnlp-main.584,2016.gwc-1.43,0,0.0469731,"Missing"
2020.emnlp-main.584,W14-0105,0,0.0340426,"Missing"
2020.emnlp-main.584,2020.acl-demos.14,0,0.0324095,"Missing"
2020.emnlp-main.584,E17-1010,1,0.882283,"tended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinctions at the granularity level defined by the inventory. Stanford Contextual Word Similarity (Huang et al., 2012) is one of the first datasets that focuses on ambiguity but outside the boundaries of sense inventories, and as a similarity m"
2020.emnlp-main.584,2020.emnlp-main.285,1,0.825453,"g their multilingual counterparts by a large margin. 2 Related Work XL-WiC is a benchmark for inventory-independent evaluation of WSD models (Section 2.1), while the multilingual nature of the dataset makes it an interesting resource for experimenting with crosslingual transfer (Section 2.2). 2.1 Word Sense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as re"
2020.emnlp-main.584,N09-4007,0,0.0225585,"ur framework is based on the original WiC dataset, which we extend to multiple languages. 3.1 English WiC Each instance of the original WiC dataset (Pilehvar and Camacho-Collados, 2019) is composed of a target word (e.g., justify) and two sentences where the target word occurs (e.g., “Justify the margins” and “The end justifies the means”). The task is a binary classification: to decide whether the same sense of the target word (justify) was intended in the two contexts or not. The dataset was built using example sentences from resources such as Wiktionary, WordNet (Miller, 1995) and VerbNet (Schuler et al., 2009). 3.2 XL-WiC We followed Pilehvar and Camacho-Collados (2019) and constructed XL-WiC based on example usages of words in sense inventories. Example usages are curated in a way to be self contained and clearly distinguishable across different senses of a word; hence, they provide a reliable basis for the binary classification task. Specifically, for a word 7194 Lang. Target Word Sentence 1 Sentence 2 Label EN Beat We beat the competition. Agassi beat Becker in the tennis championship. True DA ET FR KO ZH FA Tro Ruum Causticit´e ᆯᄅ ᅳ ᄐ ᆷ ᅵ 發 Jeg tror p˚a det, min mor fortalte. ¨ Uhel hetkel olin"
2020.emnlp-main.584,2020.acl-demos.6,0,0.0235878,"t evaluation of WSD models (Section 2.1), while the multilingual nature of the dataset makes it an interesting resource for experimenting with crosslingual transfer (Section 2.2). 2.1 Word Sense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinct"
2020.emnlp-main.584,simov-osenova-2010-constructing,0,0.0220283,"w i and sw where i = 6 j) are paired as a negative instance j (False label). We leveraged two main sense inventories for this extension: Multilingual WordNet (Section 3.2.1) and Wiktionary (Section 3.2.2). 3.2.1 Multilingual WordNet WordNet (Miller, 1995) is the de facto sense inventory for English WSD. The resource was originally built as an English lexical database in 1995, but since then there have been many efforts to extend it to other languages (Bond and Paik, 2012). We took advantage of these extensions to construct XL-WiC. In particular, we processed the WordNet versions of Bulgarian (Simov and Osenova, 2010), Chinese (Huang et al., 2010), Croatian (Raffaelli et al., 2008), Danish (Pedersen et al., 2009), Dutch (Postma et al., 2016), Estonian (Vider and Orav, 2002), Japanese (Isahara et al., 2008), Korean (Yoon et al., 2009) and Farsi (Shamsfard et al., 2010).1 Farsi: Semi-automatic extraction. FarsNet v3.0 (Shamsfard et al., 2010) comprises 30K synsets with over 100K word entries. Many of these synsets are mapped to the English database; however, each synset provides just one example usage for a target word. This prevents us from applying the automatic extraction of positive examples. Therefore,"
2020.emnlp-main.584,tiedemann-2012-parallel,0,0.0727061,"Missing"
2020.emnlp-main.584,2020.eamt-1.61,0,0.026207,"Missing"
2020.findings-emnlp.365,N15-1178,0,0.0169534,"the interplay between social media and news sources has been widely studied in other research fields, such as journalism studies (Johnson et al., 2018; Orellana-Rodriguez and Keane, 2018), very little work exists in computer science (Dredze et al., 2016), and notably, none considering SD. 2 Background The Task. SD is the task of automatically identifying the opinion expressed in a text with respect to a target (Mohammad et al., 2017). Note that SD constitutes a related, but different task than both sentiment analysis and textual entailment. The first considers the emotions conveyed in a text (Alhothali and Hoey, 2015; Tang et al., 2016), while in the second, the goal is to predict whether a logical implication exists between two sentences (Bowman et al., 2015). Consider the following example: • Target: Aetna will merge with Humana • Text: Aetna & Humana CEOs met again to talk about deal, can’t stand those bla-bla people!!! The text’s sentiment is negative, as the author is complaining about the meeting; concerning entailment, it is positive: the target entails the text because, in order to merge, two companies need to discuss the deal; finally, its stance is commenting, as it is just talking about the mer"
2020.findings-emnlp.365,N18-2004,0,0.0513448,"Missing"
2020.findings-emnlp.365,D15-1075,0,0.0379889,"; Orellana-Rodriguez and Keane, 2018), very little work exists in computer science (Dredze et al., 2016), and notably, none considering SD. 2 Background The Task. SD is the task of automatically identifying the opinion expressed in a text with respect to a target (Mohammad et al., 2017). Note that SD constitutes a related, but different task than both sentiment analysis and textual entailment. The first considers the emotions conveyed in a text (Alhothali and Hoey, 2015; Tang et al., 2016), while in the second, the goal is to predict whether a logical implication exists between two sentences (Bowman et al., 2015). Consider the following example: • Target: Aetna will merge with Humana • Text: Aetna & Humana CEOs met again to talk about deal, can’t stand those bla-bla people!!! The text’s sentiment is negative, as the author is complaining about the meeting; concerning entailment, it is positive: the target entails the text because, in order to merge, two companies need to discuss the deal; finally, its stance is commenting, as it is just talking about the merger, without expressing the orientation that it will happen (or not). SD as a Sub-Task. SD is often integrated into rumor verification (Zubiaga et"
2020.findings-emnlp.365,2020.acl-main.157,1,0.917189,"ble 1). The term M&A refers to the process by which the ownership of a company (the target) is transferred to another company (the buyer). An M&A process (merger) ranges from informal talks between the companies to the closing of the deal; high secrecy is involved and discussions are usually not publicly disclosed during its early stages (Bruner and Perella, 2004). Thus, the analysis of the evolution of opinions and concerns about a potential merger is a process similar to rumor verification (Zubiaga et al., 2018b). Notably, the news articles in S TANDER discuss the same targets as in WT– WT (Conforti et al., 2020), a large Twitter SD dataset: thus, their union provides aligned signals from both authoritative (articles) and user-generated (tweets) sources, constituting the first resource of this kind for SD. In this paper, we make the following contributions: (1) We construct S TANDER, a large expertannotated news dataset 1 labeled for SD and finegrained ER. To our knowledge, it is the first news SD dataset to provide evidence snippets, along with their exact location in the corresponding article. (2) We provide detailed statistics of our data, as well as the first diachronic analysis of the sources of"
2020.findings-emnlp.365,W02-0109,0,0.178069,"Missing"
2020.findings-emnlp.365,L16-1623,0,0.0270988,"tion (Lillie and Middelboe, 2019) and automated fact-checking (Popat et al., 2017; Thorne and Vlachos, 2018; Baly et al., 2018): in this context, textual entailment is sometimes preferred to SD as the penultimate sub-step before verification (Thorne et al., 2018). Twitter SD. Traditionally, research on SD focused on user-generated data, such as blogs and commenting sections on websites (Skeppstedt et al., 2017; Hercig et al., 2017), apps (Vamvas and Sennrich, 2020), and Facebook posts (Klenner et al., 2017); above all, mainly due to the handiness of its API, Twitter was used as a data source (Mohammad et al., 2016; Zubiaga et al., 2016; Inkpen et al., 2017; Aker et al., 2017; Conforti et al., 2020). News SD. At the time of writing, a very small number of SD datasets collecting news have been released, usually building on platforms originally developed by professional journalists, like Emergent (Ferreira and Vlachos, 2016) or Snopes (Hanselowski et al., 2019). Note that in Twitter SD, the task consists of defining the stance of a tweet with respect to a short target (usually a named entity like Hillary Clinton (Inkpen et al., 2017), or a concept like feminism (Mohammad et al., 2016)); in news SD, on the"
2020.findings-emnlp.365,D14-1162,0,0.0822115,"Missing"
2020.semeval-1.3,C18-1139,0,0.0425101,"imLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differe"
2020.semeval-1.3,2020.semeval-1.37,0,0.0306792,"h Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, especially in the English Subtask 2. However, in order to optimise their system, they made many more submissions than allowed in the competition; we therefore leave them out of the official ranking. With a more multilingual approach, BabelEncoding (Costella Pessutto et al., 2020) proposed a solution in which they translated the contexts and target words to many languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings"
2020.semeval-1.3,2020.lrec-1.720,1,0.566046,"Missing"
2020.semeval-1.3,2020.semeval-1.38,0,0.0192666,"l at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they achieved a second place in the English Subtask 1 and won the Finnish Subtask 1. 8 Conclusion We resented the SemEval-2020 Task on Graded Word Similarity in Context and introduced our new dataset CoSimLex. We provided the motivation behind their design choices and described the annotation process. The task received a good number of submissions and system description papers (15 and"
2020.semeval-1.3,S17-2002,1,0.894409,"me word, and labelled as to whether the word sense in the two examples/contexts is the same or different. This forces engagement with the context; it also creates a task in which context-independent models like word2vec “would perform no better than a random baseline”; and inter-rater agreement scores are much more healthy. However, as the dataset focuses on discrete word senses, it cannot capture graded effects of context. These datasets are also available only in English. Multi-lingual similarity datasets exist: in SemEval2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity, Camacho-Collados et al. (2017) used five different languages, and even used pairs in which each word was presented in a different language. A more recent Multi-SimLex dataset (Vuli´c et al., 2020) comprises similarity ratings for 1,888 concept pairs aligned across 13 typologically diverse languages. However, the pairs in both datasets were annotated out of context, preventing analysis of contextual effects. 38 3 Task Description Our dataset is based on pairs of words from SimLex-999 (Hill et al., 2015). Each instance is a naturallyoccurring context, taken from Wikipedia, in which both words in the pair appear, labelled wit"
2020.semeval-1.3,2020.semeval-1.35,0,0.0334679,"al influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categories, and significantly improving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created"
2020.semeval-1.3,P19-4007,1,0.829844,"e to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts"
2020.semeval-1.3,2020.semeval-1.5,0,0.204677,"Missing"
2020.semeval-1.3,P19-1285,0,0.02456,"ty scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask"
2020.semeval-1.3,N19-1423,0,0.0961329,"w dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in iso"
2020.semeval-1.3,J13-3003,0,0.173635,"luation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words as they occur in context; sp"
2020.semeval-1.3,2020.semeval-1.34,0,0.0283968,"roving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they a"
2020.semeval-1.3,2020.semeval-1.17,0,0.0328571,"nally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They proposed a system in which they calculated K-Means inspired centroids from the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed prediction"
2020.semeval-1.3,2020.semeval-1.16,0,0.0416405,"rom the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, su"
2020.semeval-1.3,J15-4004,0,0.527307,"used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential"
2020.semeval-1.3,P12-1092,0,0.573882,"eam tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of S"
2020.semeval-1.3,2020.emnlp-main.283,0,0.0166095,"submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The"
2020.semeval-1.3,P19-1569,0,0.0310581,"notator against the average of the rest. JUSTMasters is not part of the official ranking since they were able to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new"
2020.semeval-1.3,2020.semeval-1.33,0,0.0312792,"set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They pr"
2020.semeval-1.3,2020.semeval-1.36,0,0.0172085,"ss is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, esp"
2020.semeval-1.3,Q17-1022,1,0.867881,"Missing"
2020.semeval-1.3,N18-1202,0,0.17532,"system description papers. A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datas"
2020.semeval-1.3,N19-1128,1,0.870958,"ew resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words"
2020.semeval-1.3,2020.semeval-1.18,0,0.0318793,"languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings. Their idea is that the translation not only brings new resources but the process itself can produce useful information, for example to disambiguate. The approach works very well for the less resourced languages, being clearly the best system in that category, in both Subtask 1 and 2. Their system won Subtask 1 and 2 for Croatian (by a healthy margin) and 2 for Slovene, ending third in the Slovene Subtask 1 and third and second in the two Finnish ones. The MultiSem team (Soler and Apidianaki, 2020) collected 5 different datasets in order to fine-tune their BERT models, most of them automatically generated from previous datasets to increase contextual influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categ"
2020.semeval-1.3,2020.semeval-1.19,0,0.0366991,"l discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the si"
2020.semeval-1.3,2020.lrec-1.582,1,0.835923,"Missing"
2020.semeval-1.3,W17-0228,0,0.0235541,"ilarity of words and the effect that context has on it. Good context-independent models could theoretically give reasonably competitive results in this task, however we still expect context-dependent models to have a considerable advantage. 4 Dataset CoSimLex (Armendariz et al., 2020) is based on pairs of words from SimLex-999 (Hill et al., 2015); the reliability and common use of SimLex makes it a good starting point and allows comparison of judgements and model outputs to the context-independent case. For Croatian and Finnish we use existing translations of SimLex-999 (Mrkši´c et al., 2017; Venekoski and Vankka, 2017; Kittask, 2019). In the case of Slovene, we have produced our own new translation,1 following Mrkši´c et al. (2017)’s methodology for Croatian. The dataset consists of 340 pairs in English, 112 in Croatian, 111 in Slovene and 24 in Finnish. Each pair is rated within two different contexts, giving a total of 1174 scores of contextual similarity. This poses a difficult task: to find suitable, organically occurring contexts; this task is even more challenging for languages with less resources, and as a result the selection of pairs is different for each language. Each line of CoSimLex is made of"
2020.semeval-1.3,2020.cl-4.5,1,0.887547,"Missing"
2021.acl-short.73,S14-2010,0,0.0379037,"Missing"
2021.acl-short.73,S16-1081,0,0.0327213,"Missing"
2021.acl-short.73,S12-1051,0,0.0082807,"g a clustering step helps us to eliminate the local dominant directions of each cluster. We will show in Section 5 that different linguistic knowledge is encoded in the dominant directions of various clusters. Moreover, numerical results show that in comparison with the global approach, our method can make the embedding space more isotropic, even when the fewer number of PCs are nulled out. 4 Experiments We carried out experiments on the following benchmarks. As for Semantic Textual Similarity (STS), which is the main benchmark for our experiments, we experimented with STS 2012-2016 datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016), the SICK-Relatedness dataset (SICK-R) (Marelli et al., 2014), and the STS benchmark (STS-B). For the STS task, we report results for GPT-2, BERT, and RoBERTa. We also experimented with a number 577 Model STS 2012 STS 2013 STS 2014 STS 2015 STS 2016 SICK-R STS-B Baseline GPT-2 BERT-base RoBERTa-base 26.49 42.87 33.09 30.25 59.21 56.44 35.74 59.75 46.76 41.25 62.85 55.44 46.40 63.74 60.88 45.05 58.69 61.28 24.8 47.4 56.0 Global approach GPT-2 BERT-base RoBERTa-base 51.42 54.62 51.59 69.71 70.39 73.57 55.91 60.34 60.70 60.35 63.73 66.72 62.12 69.37 69.34 59.22 63.68 65."
2021.acl-short.73,S13-1004,0,0.0225306,"Missing"
2021.acl-short.73,Q16-1028,0,0.132043,"on of data in general (Huang et al., 2018; Cogswell et al., 2016). From the geometric point of view, a space is called isotropic if the vectors within that space are uniformly distributed in all directions. Lacking isotropy in the embedding space affects not only the optimization procedure (e.g., model’s accuracy and convergence time) but also the expressiveness of the embedding space; hence, improving the isotropy of the embedding space can lead to performance improvements (Wang et al., 2020; Ioffe and Szegedy, 2015). We measure the isotropy of embedding space using the partition function of Arora et al. (2016): ? (?) = ? Õ ?? ? ?? (1) ?=1 where ? is a unit vector, ? ? is the corresponding embedding for the ? ? ℎ word in the embedding matrix W ∈ IRN×D , N is the number of words in the vocabulary, and D is the embedding size. Arora et al. (2016) showed that ? (?) can be approximated using a constant for isotropic embedding spaces. Therefore, for the set ?, which is the set of eigenvectors of W? W, in the following equation, I(W) would be close to one for a perfectly isotropic space (Mu and Viswanath, 2018). I(W) = 2.1 ???? ∈? ? (?) ???? ∈? ? (?) (2) Analyzing Isotropy in pre-trained CWRs Using the ab"
2021.blackboxnlp-1.29,N19-1112,0,0.0237596,"t layers of XLNet and BERT when tested on Wikipedia examples. XLNet shows considerable norm disparities across different layers. for a theoretically justified method, Minimum Description Length (MDL) probing, in our layer-wise analysis. 2.3 MDL Probing Conventional probes (such as Conneau et al., 2018; Tenney et al., 2019b; Jawahar et al., 2019) leave unclear whether the classifier identifies linguistic knowledge in the representations or learns the task itself (Hewitt and Liang, 2019). Hence, researchers had to limit the size of the dataset (Zhang and Bowman, 2018) or the probe’s complexity (Liu et al., 2019) to make sure the probe is not learning the task itself. However, probes based on information theory enable us to obtain more interpretable and reliable probing results. The goal of information-theoretic probing is to measure to what extent representations encode a specific linguistic knowledge and how much effort is required to extract it. Voita and Titov (2020) combined the final quality of the probe classifier and the difficulty of achieving it by reformulating probes to a data transmission problem. If N number of representations are given, we plan to send their corresponding labels with a"
2021.blackboxnlp-1.29,silveira-etal-2014-gold,0,0.0178643,"Missing"
2021.blackboxnlp-1.29,D19-1448,0,0.0244675,"Missing"
2021.eacl-main.140,W09-2420,0,0.113846,"Missing"
2021.eacl-main.140,N19-1423,0,0.0634982,"Missing"
2021.eacl-main.140,D12-1129,0,0.0223218,"el to (1) disambiguate the word in context without an external sense inventory, (2) deal with unseen instances and incomplete data, and (3) transfer the intrinsic knowledge (gained on general domain data) into a specific domain. 2 Related Work Word Sense Disambiguation. The task of WSD consists of associating a word in context with its most appropriate entry in a given sense inventory (Navigli, 2009), e.g., WordNet. For WSD there are many associated datasets (Raganato et al., 2017; Vial et al., 2018; R¨oder et al., 2018; Ling et al., 2015), including domain-specific ones (Agirre et al., 2009; Faralli and Navigli, 2012). The main difference between WSD and its re-formulation TSV is that for TSV the availability of a sense inventory is not required. Instead of associating a word in context with its most appropriate sense, the usage of a single given sense in the provided context is to be verified. Systems that aim to solve the proposed task are therefore not required to model all senses of the target word, but only a single sense instead. This facilitates the development of systems for specific domains or settings, as no general-domain knowledge resource is required to perform this task. For instance, an Indo"
2021.eacl-main.140,N18-2017,0,0.0532666,"Missing"
2021.eacl-main.140,D19-1355,0,0.118417,"an perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense inventory (which would include senses fo"
2021.eacl-main.140,E17-2068,0,0.0180845,"Missing"
2021.eacl-main.140,Q15-1023,0,0.0278143,"domains. Therefore, this dataset aims at evaluating the ability of a model to (1) disambiguate the word in context without an external sense inventory, (2) deal with unseen instances and incomplete data, and (3) transfer the intrinsic knowledge (gained on general domain data) into a specific domain. 2 Related Work Word Sense Disambiguation. The task of WSD consists of associating a word in context with its most appropriate entry in a given sense inventory (Navigli, 2009), e.g., WordNet. For WSD there are many associated datasets (Raganato et al., 2017; Vial et al., 2018; R¨oder et al., 2018; Ling et al., 2015), including domain-specific ones (Agirre et al., 2009; Faralli and Navigli, 2012). The main difference between WSD and its re-formulation TSV is that for TSV the availability of a sense inventory is not required. Instead of associating a word in context with its most appropriate sense, the usage of a single given sense in the provided context is to be verified. Systems that aim to solve the proposed task are therefore not required to model all senses of the target word, but only a single sense instead. This facilitates the development of systems for specific domains or settings, as no general-"
2021.eacl-main.140,2020.acl-main.465,0,0.0138816,"biguation algorithms that do not require modeling the entirety of a sense inventory. This characteristic also provides a crucial advantage in enterprise and domain-specific settings as it facilitates the development of systems which are only aimed at modelling the domain at hand. Moreover, having these out-of-domain test instances makes our benchmark more robust and generalisable, preventing (or making it harder) for statistical models to learn spurious correlations from the training set, which has been proven to be an issue in standard NLP tasks (Poliak et al., 2018; Gururangan et al., 2018; Linzen, 2020). In our initial experiments we found that current state-of-the-art disambiguation techniques based on pre-trained language models such as BERT are very accurate at handling ambiguity, even in specialised domains. However, there is still room for improvement as highlighted by the gap with the human performance. This benchmark therefore opens up avenues for future research on domain-transfer and on developing general-purpose solutions which can perform well on a variety of domains without the need for large amounts of training data. As future work, we are planning to further investigate and ana"
2021.eacl-main.140,P19-1569,0,0.11729,"results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense"
2021.eacl-main.140,N19-1128,1,0.828173,"Missing"
2021.eacl-main.140,S18-2023,0,0.0565736,"Missing"
2021.eacl-main.140,E17-1010,1,0.93906,"gs. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense inventory (which would include senses for the fruit and the tree) and a technology-specific sense inventory (which would include the sense for Apple Inc. the technology"
2021.eacl-main.140,N04-1002,0,0.225153,"Missing"
2021.eacl-main.140,L18-1166,0,0.017952,"-specific instances from three different domains. Therefore, this dataset aims at evaluating the ability of a model to (1) disambiguate the word in context without an external sense inventory, (2) deal with unseen instances and incomplete data, and (3) transfer the intrinsic knowledge (gained on general domain data) into a specific domain. 2 Related Work Word Sense Disambiguation. The task of WSD consists of associating a word in context with its most appropriate entry in a given sense inventory (Navigli, 2009), e.g., WordNet. For WSD there are many associated datasets (Raganato et al., 2017; Vial et al., 2018; R¨oder et al., 2018; Ling et al., 2015), including domain-specific ones (Agirre et al., 2009; Faralli and Navigli, 2012). The main difference between WSD and its re-formulation TSV is that for TSV the availability of a sense inventory is not required. Instead of associating a word in context with its most appropriate sense, the usage of a single given sense in the provided context is to be verified. Systems that aim to solve the proposed task are therefore not required to model all senses of the target word, but only a single sense instead. This facilitates the development of systems for spe"
2021.eacl-main.140,2019.gwc-1.14,0,0.020377,"ough these models can perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense inventory (which wo"
2021.emnlp-main.61,2020.acl-main.385,0,0.02412,"s the saliency score of an input token to classiIn what follows in the paper, we use the analyfier’s decision. Note that using attention weights sis method discussed in this section to find those for this purpose can be misleading given that raw tokens that play the central role in different surattention weights do not necessarily correspond to face (Section 4), syntactic (Sections 5 and 6.1) and the importance of individual token representations semantic (Section 6.3) probing tasks. Based on (Serrano and Smith, 2019; Jain and Wallace, 2019; these tokens we then investigate the reasons behind Abnar and Zuidema, 2020; Kobayashi et al., 2020). performance variations across layers. 794 Figure 1: Absolute normalized saliency scores for the top-4 most attributed (high frequency, > 128) tokens across five different layers.2 4 Sentence Length In this surface-level task we probe the representation of a given sentence in order to estimate its size, i.e., the number of words (not tokens) in it. To this end, we used SentEval’s SentLen dataset, but changed the formulation from the original classification objective to a regression one which allows a better generalization due to its fine-grained setting. The diagnosti"
2021.emnlp-main.61,2020.emnlp-main.263,0,0.248326,"suggest evaluating probes with alternative criteria (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020; Zhu and Rudzicz, 2020). We extend the layer-wise analysis to the token level in search for distinct and meaningful subspaces in BERT’s representation space that can explain the performance trends in various probing tasks. To this end, we leverage the attribution method (Simonyan et al., 2013; Sundararajan et al., 2017; Smilkov et al., 2017) which has recently proven effective for analytical studies in NLP (Li et al., 2016; Yuan et al., 2019; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, Authors marked with a star (? ) contributed equally. 1 2021; Voita et al., 2021). Our analysis on a set of Code is available at https://github.com/ hmohebbi/explain-probing-results surface, syntax, and semantic probing tasks (Con792 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 792–806 c November 7–11, 2021. 2021 Association for Computational Linguistics neau et al., 2018) shows that BERT usually encodes the knowledge required for addressing these tasks within specific token representations, particularly at higher layers. For instan"
2021.emnlp-main.61,2020.blackboxnlp-1.14,0,0.024804,"nuances of the task and hence suggest evaluating probes with alternative criteria (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020; Zhu and Rudzicz, 2020). We extend the layer-wise analysis to the token level in search for distinct and meaningful subspaces in BERT’s representation space that can explain the performance trends in various probing tasks. To this end, we leverage the attribution method (Simonyan et al., 2013; Sundararajan et al., 2017; Smilkov et al., 2017) which has recently proven effective for analytical studies in NLP (Li et al., 2016; Yuan et al., 2019; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, Authors marked with a star (? ) contributed equally. 1 2021; Voita et al., 2021). Our analysis on a set of Code is available at https://github.com/ hmohebbi/explain-probing-results surface, syntax, and semantic probing tasks (Con792 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 792–806 c November 7–11, 2021. 2021 Association for Computational Linguistics neau et al., 2018) shows that BERT usually encodes the knowledge required for addressing these tasks within specific token representations, particularly at h"
2021.emnlp-main.61,2020.acl-main.493,0,0.0196159,"t layers. In contrast, in this paper we explore the role of token representations in the final performance. More recently, Klafka and Ettinger (2020) investigated the extent of information that can be recovered from each word representation in a sentence about the other words. Apart from using different probing tasks and methodologies, most notably they relied solely on classifier’s performance score, whereas we make conclusion based on the most contributed token representations. that there exists a linear subspace that approximately encodes all syntactic tree distances. In a follow-up study, Chi et al. (2020) showed that similar syntactic subspaces exist for languages other than English in the multilingual BERT and that these subspaces are shared among languages to some extent. This corroborated the finding of Pires et al. (2019) that multilingual BERT has common subspaces across different languages that capture various linguistic knowledge. As for semantic subspaces, Wiedemann et al. (2019) showed that BERT places the contextualized representations of polysemous words into different regions of the embedding space, thereby capturing sense distinctions. Similarly, Reif et al. (2019) studied BERT’s"
2021.emnlp-main.61,W19-4828,0,0.0170912,"emantic abnormalities, and to distinctively separate grammatical number and tense subspaces. 1 1 Introduction Recent years have seen a surge of interest in pretrained language models, highlighted by extensive research around BERT (Devlin et al., 2019) and its derivatives. One strand of research has focused on enhancing existing models with the primary objective of improving downstream performance on various NLP tasks (Liu et al., 2019b; Lan et al., 2019; Yang et al., 2019). Another strand analyzes the behaviour of these models with the hope of getting better insights for further developments (Clark et al., 2019; Kovaleva et al., 2019; Jawahar et al., 2019; Tenney et al., 2019; Lin et al., 2019). Probing is one of the popular analysis methods, often used for investigating the encoded knowledge in language models (Conneau et al., 2018; Tenney et al., 2018). This is typically carried out by training a set of diagnostic classifiers that predict a specific linguistic property based on the representations obtained from different layers. Recent works in probing language models demonstrate that initial layers are responsible for encoding low-level linguistic information, such as part of speech and positiona"
2021.emnlp-main.61,L18-1269,0,0.0164272,"l., 2021). In particular, Voita et al. (2021) adopted a variant of Layer-wise Relevance Propagation (Bach et al., 2015) to evaluate the relative contributions of source and target tokens to the generation process in Neural Machine Translation predictions. To our knowledge, this is the first time that attribution methods are employed for layerwise probing of pre-trained language models. 3 Methodology Our analytical study was mainly carried out on Representation subspaces. In addition to layer- a set of sentence-level probing tasks from SentEwise representations, subspaces that encode spe- val (Conneau and Kiela, 2018). The benchmark cific linguistic knowledge, such as syntax, have consists of several single-sentence evaluation tasks. been a popular area of study. By designing a struc- Each task provides 100k instances for training and tural probe, Hewitt and Manning (2019) showed 10k for test, all balanced across target classes. We 793 used the test set examples for our evaluation and indepth analysis. Following the standard procedure for this benchmark, we trained a diagnostic classifier for each task. The classifier takes sentence representations as its input and predicts the specific property intended f"
2021.emnlp-main.61,2020.acl-main.434,0,0.0214449,"ely occupying initial, middle and higher layers. In a similar study, Tenney et al. (2019) employed the edge probing tasks defined by Tenney et al. (2018) to show the hierarchy of encoded knowledge through layers. Moreover, they observed that while most of the syntactic information can be localized in a few layers, semantic knowledge tends to spread across the entire network. Both studies were aimed at discovering the extent of linguistic information encoded across different layers. In contrast, in this paper we explore the role of token representations in the final performance. More recently, Klafka and Ettinger (2020) investigated the extent of information that can be recovered from each word representation in a sentence about the other words. Apart from using different probing tasks and methodologies, most notably they relied solely on classifier’s performance score, whereas we make conclusion based on the most contributed token representations. that there exists a linear subspace that approximately encodes all syntactic tree distances. In a follow-up study, Chi et al. (2020) showed that similar syntactic subspaces exist for languages other than English in the multilingual BERT and that these subspaces ar"
2021.emnlp-main.61,2020.emnlp-main.574,0,0.0832711,"n input token to classiIn what follows in the paper, we use the analyfier’s decision. Note that using attention weights sis method discussed in this section to find those for this purpose can be misleading given that raw tokens that play the central role in different surattention weights do not necessarily correspond to face (Section 4), syntactic (Sections 5 and 6.1) and the importance of individual token representations semantic (Section 6.3) probing tasks. Based on (Serrano and Smith, 2019; Jain and Wallace, 2019; these tokens we then investigate the reasons behind Abnar and Zuidema, 2020; Kobayashi et al., 2020). performance variations across layers. 794 Figure 1: Absolute normalized saliency scores for the top-4 most attributed (high frequency, > 128) tokens across five different layers.2 4 Sentence Length In this surface-level task we probe the representation of a given sentence in order to estimate its size, i.e., the number of words (not tokens) in it. To this end, we used SentEval’s SentLen dataset, but changed the formulation from the original classification objective to a regression one which allows a better generalization due to its fine-grained setting. The diagnostic classifier receives ave"
2021.emnlp-main.61,D19-1445,0,0.0160834,"s, and to distinctively separate grammatical number and tense subspaces. 1 1 Introduction Recent years have seen a surge of interest in pretrained language models, highlighted by extensive research around BERT (Devlin et al., 2019) and its derivatives. One strand of research has focused on enhancing existing models with the primary objective of improving downstream performance on various NLP tasks (Liu et al., 2019b; Lan et al., 2019; Yang et al., 2019). Another strand analyzes the behaviour of these models with the hope of getting better insights for further developments (Clark et al., 2019; Kovaleva et al., 2019; Jawahar et al., 2019; Tenney et al., 2019; Lin et al., 2019). Probing is one of the popular analysis methods, often used for investigating the encoded knowledge in language models (Conneau et al., 2018; Tenney et al., 2018). This is typically carried out by training a set of diagnostic classifiers that predict a specific linguistic property based on the representations obtained from different layers. Recent works in probing language models demonstrate that initial layers are responsible for encoding low-level linguistic information, such as part of speech and positional information, whereas"
2021.emnlp-main.61,2020.acl-main.492,0,0.0175567,"near transformation under which distances between word embeddings correspond to their sense-level relationships. Our work extends these studies by revealing other types of surface, syntactic, and high-level semantic subspaces and linguistic features using a pattern-finding approach on different types of probing tasks. Attribution methods. Recently, there has been a surge of interest in using attribution methods to open up the blackbox and explain the decision makings of pre-trained language models, from developing methods and libraries to visualize inputs’ contributions (Ribeiro et al., 2016; Han et al., 2020; Wallace et al., 2019; Tenney et al., 2020) to applying them into fine-tuned models on downstream tasks (Atanasova et al., 2020; Wu and Ong, 2021; Voita et al., 2021). In particular, Voita et al. (2021) adopted a variant of Layer-wise Relevance Propagation (Bach et al., 2015) to evaluate the relative contributions of source and target tokens to the generation process in Neural Machine Translation predictions. To our knowledge, this is the first time that attribution methods are employed for layerwise probing of pre-trained language models. 3 Methodology Our analytical study was mainly carried"
2021.emnlp-main.61,N16-1082,0,0.222461,"play a significant role in learning nuances of the task and hence suggest evaluating probes with alternative criteria (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020; Zhu and Rudzicz, 2020). We extend the layer-wise analysis to the token level in search for distinct and meaningful subspaces in BERT’s representation space that can explain the performance trends in various probing tasks. To this end, we leverage the attribution method (Simonyan et al., 2013; Sundararajan et al., 2017; Smilkov et al., 2017) which has recently proven effective for analytical studies in NLP (Li et al., 2016; Yuan et al., 2019; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, Authors marked with a star (? ) contributed equally. 1 2021; Voita et al., 2021). Our analysis on a set of Code is available at https://github.com/ hmohebbi/explain-probing-results surface, syntax, and semantic probing tasks (Con792 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 792–806 c November 7–11, 2021. 2021 Association for Computational Linguistics neau et al., 2018) shows that BERT usually encodes the knowledge required for addressing these tasks within"
2021.emnlp-main.61,W19-4825,0,0.0371755,"Missing"
2021.emnlp-main.61,N19-1112,0,0.197106,"T tends to encode meaningful knowledge in specific token representations (which are often ignored in standard classification setups), allowing the model to detect syntactic and semantic abnormalities, and to distinctively separate grammatical number and tense subspaces. 1 1 Introduction Recent years have seen a surge of interest in pretrained language models, highlighted by extensive research around BERT (Devlin et al., 2019) and its derivatives. One strand of research has focused on enhancing existing models with the primary objective of improving downstream performance on various NLP tasks (Liu et al., 2019b; Lan et al., 2019; Yang et al., 2019). Another strand analyzes the behaviour of these models with the hope of getting better insights for further developments (Clark et al., 2019; Kovaleva et al., 2019; Jawahar et al., 2019; Tenney et al., 2019; Lin et al., 2019). Probing is one of the popular analysis methods, often used for investigating the encoded knowledge in language models (Conneau et al., 2018; Tenney et al., 2018). This is typically carried out by training a set of diagnostic classifiers that predict a specific linguistic property based on the representations obtained from different"
2021.emnlp-main.61,2020.acl-main.420,0,0.022478,"s, these studies do not go further to investigate the reasons behind the layer-wise behavior and the role played by token representations. Analyzing the shortcomings of pre-trained language models requires a scrutiny beyond the mere performance (e.g., accuracy or F-score) in a given probing task. This is particularly important as recent studies point out that the diagnostic classifier (applied to the model’s outputs) might itself play a significant role in learning nuances of the task and hence suggest evaluating probes with alternative criteria (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020; Zhu and Rudzicz, 2020). We extend the layer-wise analysis to the token level in search for distinct and meaningful subspaces in BERT’s representation space that can explain the performance trends in various probing tasks. To this end, we leverage the attribution method (Simonyan et al., 2013; Sundararajan et al., 2017; Smilkov et al., 2017) which has recently proven effective for analytical studies in NLP (Li et al., 2016; Yuan et al., 2019; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, Authors marked with a star (? ) contributed equally. 1 2021; Voita et al., 2021). Our"
2021.emnlp-main.61,P19-1493,0,0.0242443,"rd representation in a sentence about the other words. Apart from using different probing tasks and methodologies, most notably they relied solely on classifier’s performance score, whereas we make conclusion based on the most contributed token representations. that there exists a linear subspace that approximately encodes all syntactic tree distances. In a follow-up study, Chi et al. (2020) showed that similar syntactic subspaces exist for languages other than English in the multilingual BERT and that these subspaces are shared among languages to some extent. This corroborated the finding of Pires et al. (2019) that multilingual BERT has common subspaces across different languages that capture various linguistic knowledge. As for semantic subspaces, Wiedemann et al. (2019) showed that BERT places the contextualized representations of polysemous words into different regions of the embedding space, thereby capturing sense distinctions. Similarly, Reif et al. (2019) studied BERT’s ability to distinguish different word senses in different contexts. Using the probing approach of Hewitt and Manning (2019), they also found that there exists a linear transformation under which distances between word embeddi"
2021.emnlp-main.61,D19-1410,0,0.0244158,"cribe how sentence representations were computed in our experiments. Then, we discuss our approach for measuring the attribution of individual token representations to classifier’s decision. 3.1 Sentence Representation For computing sentence representations for layer l, we opted for a simple unweighted averaging (hlAvg ) of all input tokens (except for padding and [ CLS ] token). This choice was due to our observation that the mean pooling strategy retains or improves [ CLS ] performance in most layers in our probing tasks (cf. Appendix A.1 for more details). This corroborates the findings of Reimers and Gurevych (2019) who observed a similar trend on sentence similarity and inference tasks. Moreover, the mean pooling strategy simplifies our measuring of each token’s attribution, discussed next. Our evaluations are based on the pre-trained BERT (base-uncased, 12-layer, 768-hidden size, 12-attention head, 110M parameters) obtained from the HuggingFace’s Transformers library (Wolf et al., 2020). We followed the recommended hyperparameters by Jawahar et al. (2019) to train the diagnostic classifiers for each layer. In addition to BERT, we carried out our evaluations on RoBERTa (Liu et al., 2019b, base, 125M par"
2021.emnlp-main.61,N16-3020,0,0.0603066,"that there exists a linear transformation under which distances between word embeddings correspond to their sense-level relationships. Our work extends these studies by revealing other types of surface, syntactic, and high-level semantic subspaces and linguistic features using a pattern-finding approach on different types of probing tasks. Attribution methods. Recently, there has been a surge of interest in using attribution methods to open up the blackbox and explain the decision makings of pre-trained language models, from developing methods and libraries to visualize inputs’ contributions (Ribeiro et al., 2016; Han et al., 2020; Wallace et al., 2019; Tenney et al., 2020) to applying them into fine-tuned models on downstream tasks (Atanasova et al., 2020; Wu and Ong, 2021; Voita et al., 2021). In particular, Voita et al. (2021) adopted a variant of Layer-wise Relevance Propagation (Bach et al., 2015) to evaluate the relative contributions of source and target tokens to the generation process in Neural Machine Translation predictions. To our knowledge, this is the first time that attribution methods are employed for layerwise probing of pre-trained language models. 3 Methodology Our analytical study"
2021.emnlp-main.61,P19-1282,0,0.0213486,"the output labels. This is usually referred tokens in a sentence to the classification decision. to as the saliency score of an input token to classiIn what follows in the paper, we use the analyfier’s decision. Note that using attention weights sis method discussed in this section to find those for this purpose can be misleading given that raw tokens that play the central role in different surattention weights do not necessarily correspond to face (Section 4), syntactic (Sections 5 and 6.1) and the importance of individual token representations semantic (Section 6.3) probing tasks. Based on (Serrano and Smith, 2019; Jain and Wallace, 2019; these tokens we then investigate the reasons behind Abnar and Zuidema, 2020; Kobayashi et al., 2020). performance variations across layers. 794 Figure 1: Absolute normalized saliency scores for the top-4 most attributed (high frequency, > 128) tokens across five different layers.2 4 Sentence Length In this surface-level task we probe the representation of a given sentence in order to estimate its size, i.e., the number of words (not tokens) in it. To this end, we used SentEval’s SentLen dataset, but changed the formulation from the original classification objective to"
2021.emnlp-main.61,2020.emnlp-main.744,0,0.0165426,"go further to investigate the reasons behind the layer-wise behavior and the role played by token representations. Analyzing the shortcomings of pre-trained language models requires a scrutiny beyond the mere performance (e.g., accuracy or F-score) in a given probing task. This is particularly important as recent studies point out that the diagnostic classifier (applied to the model’s outputs) might itself play a significant role in learning nuances of the task and hence suggest evaluating probes with alternative criteria (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020; Zhu and Rudzicz, 2020). We extend the layer-wise analysis to the token level in search for distinct and meaningful subspaces in BERT’s representation space that can explain the performance trends in various probing tasks. To this end, we leverage the attribution method (Simonyan et al., 2013; Sundararajan et al., 2017; Smilkov et al., 2017) which has recently proven effective for analytical studies in NLP (Li et al., 2016; Yuan et al., 2019; Bastings and Filippova, 2020; Atanasova et al., 2020; Wu and Ong, Authors marked with a star (? ) contributed equally. 1 2021; Voita et al., 2021). Our analysis on a set of Cod"
2021.findings-emnlp.261,Q16-1028,0,0.0263131,"hich are the reason for anisotropic distribution. The distribution is more uniform and isotropic if the extent of elongation is similar across different directions (the most and the least elongated directions). With this in mind, Mu and Viswanath (2018) proposed a measurement to quantify the embedding space isotropy employing PCs as follows: I(W) = minu∈U F (u) maxu∈U F (u) (1) where U is the set of all eigenvectors of the word embedding matrix, and F (u) is the following partition function: F (u) = N X Tw i eu (2) i=1 where N is the number of word embeddings and wi is the ith word embedding. Arora et al. (2016) demonstrated that for a perfectly isotropic embedding space, F (u) could be approximated by a constant. The value of I(W) is closer to one for the more isotropic embedding spaces. 3.2 Methodology We study the changes applied to the embedding space by fine-tuning from the perspective of isotropy. In this regard, we take several approaches explained as follows. Zero-mean. This method simply transfers all the embeddings to the center. Clustering+ZM. Here, we first cluster embeddings and then separately make each cluster zeromean (Cai et al., 2021). These two approaches give us a precise picture"
2021.findings-emnlp.261,W19-3621,0,0.0221724,"r, local structures in pre-trained contextual word representations (CWRs), such as those encoding token types or frequency, undergo a massive change during fine-tuning. Our experiments show dramatic growth in the number of elongated directions in the embedding space, which, in contrast to pre-trained CWRs, carry the essential linguistic knowledge in the fine-tuned embedding space, making existing isotropy enhancement methods ineffective. 1 Introduction semantic expressiveness of embedding space, the latter reflects the unwanted social bias in training data (Li et al., 2020; Garg et al., 2018; Gonen and Goldberg, 2019). The representation degeneration problem is another issue that limits their linguistic capacity. Gao et al. (2019) showed that the weight tying trick (Inan et al., 2017) in the pre-training procedure is mainly responsible for the degeneration problem in the embedding space. In such a case, the embeddings occupy a narrow cone in the space (Ethayarajh, 2019). Several approaches have been proposed to improve the isotropy of pre-trained models, which in turn boosts the representation power and downstream performance of CWRs (Zhang et al., 2020; Wang et al., 2020). However, previous studies have m"
2021.findings-emnlp.261,2020.acl-main.177,0,0.0361823,"Missing"
2021.findings-emnlp.261,2020.aacl-main.11,0,0.0382764,"lly for semantic downstream tasks. et al., 2021). Moreover, the results show that some Cosine similarity-based metrics have usually linguistic information is surprisingly eliminated by been employed for assessing the isotropy of emthis procedure (Mosbach et al., 2020). Studies on bedding spaces where a near-zero cosine similarity the multi-head attention structure suggest a similar between random embeddings indicates isotropic trend in their patterns during fine-tuning; in higher layers, attention weights experience more signifi- distribution. However, Rajaee and Pilehvar (2021) cant changes (Hao et al., 2020). More detailed anal- demonstrated that these metrics might not be reliable for calculating isotropy since, in some cases, ysis on self-attention modules indicates that dense the cosine similarity of random words is zero while and value projection matrices have heavily been affected by fine-tuning (Radiya-Dixit and Wang, their distribution is not uniform. Hence, we uti2020). However, geometric analysis on the em- lize another metric based on Principal Components (PCs). bedding space and changes applied to the structure of embeddings during fine-tuning are aspects that As we mentioned before, a"
2021.findings-emnlp.261,N19-1419,0,0.0296593,"capturing that null out dominant directions) have the linguistic knowledge. They have shown that presame positive outcome for the fine-tuned modtrained representations are able to encode various els as it has for the pre-trained ones? linguistic properties (Tenney et al., 2019a; Talmor et al., 2020; Goodwin et al., 2020; Wu et al., 2020; • How does the distribution of CWRs change Zhou and Srikumar, 2021; Chen et al., 2021; Tenupon fine-tuning? ney et al., 2019b), among others, syntactic, such as part of speech (Liu et al., 2019a) and dependency To answer these questions, we consider the tree (Hewitt and Manning, 2019), and semantic, semantic textual similarity (STS) as the target such as word senses (Reif et al., 2019) and seman- task and leverage the metric proposed by Mu and tic dependency (Wu et al., 2021). Viswanath (2018) for measuring isotropy. The preDespite their significant potential, pre-trained trained BERT and RoBERTa (Liu et al., 2019b) representations suffer from important weaknesses. underperform static embeddings on STS, while Frequency and gender bias are two well-known fine-tuning significantly boosts their performance, problems in CWRs. While the former hurts the suggesting the considera"
2021.findings-emnlp.261,2020.emnlp-main.733,0,0.0321926,"lt in isotropy enhancements. Moreover, local structures in pre-trained contextual word representations (CWRs), such as those encoding token types or frequency, undergo a massive change during fine-tuning. Our experiments show dramatic growth in the number of elongated directions in the embedding space, which, in contrast to pre-trained CWRs, carry the essential linguistic knowledge in the fine-tuned embedding space, making existing isotropy enhancement methods ineffective. 1 Introduction semantic expressiveness of embedding space, the latter reflects the unwanted social bias in training data (Li et al., 2020; Garg et al., 2018; Gonen and Goldberg, 2019). The representation degeneration problem is another issue that limits their linguistic capacity. Gao et al. (2019) showed that the weight tying trick (Inan et al., 2017) in the pre-training procedure is mainly responsible for the degeneration problem in the embedding space. In such a case, the embeddings occupy a narrow cone in the space (Ethayarajh, 2019). Several approaches have been proposed to improve the isotropy of pre-trained models, which in turn boosts the representation power and downstream performance of CWRs (Zhang et al., 2020; Wang e"
2021.findings-emnlp.261,N19-1112,0,0.0216915,"• Does isotropy enhancement (using methods such as BERT (Devlin et al., 2019), in capturing that null out dominant directions) have the linguistic knowledge. They have shown that presame positive outcome for the fine-tuned modtrained representations are able to encode various els as it has for the pre-trained ones? linguistic properties (Tenney et al., 2019a; Talmor et al., 2020; Goodwin et al., 2020; Wu et al., 2020; • How does the distribution of CWRs change Zhou and Srikumar, 2021; Chen et al., 2021; Tenupon fine-tuning? ney et al., 2019b), among others, syntactic, such as part of speech (Liu et al., 2019a) and dependency To answer these questions, we consider the tree (Hewitt and Manning, 2019), and semantic, semantic textual similarity (STS) as the target such as word senses (Reif et al., 2019) and seman- task and leverage the metric proposed by Mu and tic dependency (Wu et al., 2021). Viswanath (2018) for measuring isotropy. The preDespite their significant potential, pre-trained trained BERT and RoBERTa (Liu et al., 2019b) representations suffer from important weaknesses. underperform static embeddings on STS, while Frequency and gender bias are two well-known fine-tuning significantly boo"
2021.findings-emnlp.261,2021.ccl-1.108,0,0.0373912,"Missing"
2021.findings-emnlp.261,2020.blackboxnlp-1.4,0,0.041842,"Missing"
2021.findings-emnlp.261,2020.emnlp-main.552,0,0.0430932,"Missing"
2021.findings-emnlp.261,2020.findings-emnlp.227,0,0.0234161,"near-zero cosine similarity for randomly sampled words. Following the research line in understanding the Contextual embedding spaces are known to lack reasons behind the outstanding performance of pre-trained language models and their capabilities, the desirable isotropy property (Rajaee and Pilehvar, 2021; Ethayarajh, 2019). Gao et al. (2019) most recent investigations on fine-tuning have been called the defect the representation degeneration done through probing tasks and by evaluating the encoded linguistic knowledge (Merchant et al., problem and attributed it mainly to the weight ty2020; Mosbach et al., 2020; Talmor et al., 2020; ing trick (Press and Wolf, 2017) and the language modeling as the objective of the training. Under Yu and Ettinger, 2021). These studies demonstrate such a circumstance, random word embeddings that most changes in fine-tuning are applied to the upper layers, such that those layers encode task- are highly similar to one another while shaping a specific knowledge, while lower layers are respon- narrow cone in the space. Clearly, anisotropic distribution hurts the expressiveness of the embedding sible for the core linguistic phenomenon (Durrani space, especially for semanti"
2021.findings-emnlp.261,D14-1162,0,0.103564,"n a fine-tuned model, we choose Semantic Textual Similarity (STS) as the target task considering STS-Benchmark dataset (Cer et al., 2017). STS is a semantic regression task in which the model needs to determine the similarity of two sentences in a paired sample. The label is a continuous range in 0 to 5. The interesting point about STS, which makes it a reasonable choice for our analyses, is that the performance of pre-trained LMs is drastically low on this task (Reimers and Gurevych, 2019). In fact, BERT and RoBERTa’s contextual representations under-perform static embeddings, such as Glove (Pennington et al., 2014) in this task. Moreover, the [CLS] token, which is usually considered a sentence representation for classification tasks, has a lower performance than simple averaging over all tokens of a sentence. However, finetuning, whether with [CLS] token or mean-pooling method, can dramatically enhance the performance (Reimers and Gurevych, 2019). 3.4 Experimental Setup We analyze the influence of fine-tuning on the embedding space of the base versions of BERT and RoBERTa. Both models have similar transformerbased architectures, while RoBERTa has been trained with more training data and a slight differe"
2021.findings-emnlp.261,W19-4302,0,0.0397007,"Missing"
2021.findings-emnlp.261,E17-2025,0,0.0302728,"s. Following the research line in understanding the Contextual embedding spaces are known to lack reasons behind the outstanding performance of pre-trained language models and their capabilities, the desirable isotropy property (Rajaee and Pilehvar, 2021; Ethayarajh, 2019). Gao et al. (2019) most recent investigations on fine-tuning have been called the defect the representation degeneration done through probing tasks and by evaluating the encoded linguistic knowledge (Merchant et al., problem and attributed it mainly to the weight ty2020; Mosbach et al., 2020; Talmor et al., 2020; ing trick (Press and Wolf, 2017) and the language modeling as the objective of the training. Under Yu and Ettinger, 2021). These studies demonstrate such a circumstance, random word embeddings that most changes in fine-tuning are applied to the upper layers, such that those layers encode task- are highly similar to one another while shaping a specific knowledge, while lower layers are respon- narrow cone in the space. Clearly, anisotropic distribution hurts the expressiveness of the embedding sible for the core linguistic phenomenon (Durrani space, especially for semantic downstream tasks. et al., 2021). Moreover, the result"
2021.findings-emnlp.261,2021.acl-short.73,1,0.916645,"The preDespite their significant potential, pre-trained trained BERT and RoBERTa (Liu et al., 2019b) representations suffer from important weaknesses. underperform static embeddings on STS, while Frequency and gender bias are two well-known fine-tuning significantly boosts their performance, problems in CWRs. While the former hurts the suggesting the considerable change that CWRs un3042 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3042–3049 November 7–11, 2021. ©2021 Association for Computational Linguistics dergo during fine-tuning (Reimers and Gurevych, 2019; Rajaee and Pilehvar, 2021). Our analysis on the fine-tuned embedding space of BERT and RoBERTa demonstrates that word representations are highly anisotropic across all layers. An evaluation specifically carried out on the [CLS] tokens approves a similar pattern but to a greater extent. Moreover, experimental results show fading of local clustered areas in pre-trained CWRs during fine-tuning, which could be a possible reason for the improved performance. Interestingly, the fine-tuning procedure can change the linguistic knowledge encoded in dominant directions of embedding space from unnecessary information to the essen"
2021.findings-emnlp.261,D19-1410,0,0.302467,"18) for measuring isotropy. The preDespite their significant potential, pre-trained trained BERT and RoBERTa (Liu et al., 2019b) representations suffer from important weaknesses. underperform static embeddings on STS, while Frequency and gender bias are two well-known fine-tuning significantly boosts their performance, problems in CWRs. While the former hurts the suggesting the considerable change that CWRs un3042 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3042–3049 November 7–11, 2021. ©2021 Association for Computational Linguistics dergo during fine-tuning (Reimers and Gurevych, 2019; Rajaee and Pilehvar, 2021). Our analysis on the fine-tuned embedding space of BERT and RoBERTa demonstrates that word representations are highly anisotropic across all layers. An evaluation specifically carried out on the [CLS] tokens approves a similar pattern but to a greater extent. Moreover, experimental results show fading of local clustered areas in pre-trained CWRs during fine-tuning, which could be a possible reason for the improved performance. Interestingly, the fine-tuning procedure can change the linguistic knowledge encoded in dominant directions of embedding space from unnecess"
2021.findings-emnlp.261,2020.tacl-1.48,0,0.133853,"oved performance achieved by fine-tuning pre-trained language models (LMs) be attributed to the increased isotropy of the embedding space? Recently, several studies have focused on the remarkable potential of pre-trained language models, • Does isotropy enhancement (using methods such as BERT (Devlin et al., 2019), in capturing that null out dominant directions) have the linguistic knowledge. They have shown that presame positive outcome for the fine-tuned modtrained representations are able to encode various els as it has for the pre-trained ones? linguistic properties (Tenney et al., 2019a; Talmor et al., 2020; Goodwin et al., 2020; Wu et al., 2020; • How does the distribution of CWRs change Zhou and Srikumar, 2021; Chen et al., 2021; Tenupon fine-tuning? ney et al., 2019b), among others, syntactic, such as part of speech (Liu et al., 2019a) and dependency To answer these questions, we consider the tree (Hewitt and Manning, 2019), and semantic, semantic textual similarity (STS) as the target such as word senses (Reif et al., 2019) and seman- task and leverage the metric proposed by Mu and tic dependency (Wu et al., 2021). Viswanath (2018) for measuring isotropy. The preDespite their significant pot"
2021.findings-emnlp.261,P19-1452,0,0.0248626,"stions: • Can the improved performance achieved by fine-tuning pre-trained language models (LMs) be attributed to the increased isotropy of the embedding space? Recently, several studies have focused on the remarkable potential of pre-trained language models, • Does isotropy enhancement (using methods such as BERT (Devlin et al., 2019), in capturing that null out dominant directions) have the linguistic knowledge. They have shown that presame positive outcome for the fine-tuned modtrained representations are able to encode various els as it has for the pre-trained ones? linguistic properties (Tenney et al., 2019a; Talmor et al., 2020; Goodwin et al., 2020; Wu et al., 2020; • How does the distribution of CWRs change Zhou and Srikumar, 2021; Chen et al., 2021; Tenupon fine-tuning? ney et al., 2019b), among others, syntactic, such as part of speech (Liu et al., 2019a) and dependency To answer these questions, we consider the tree (Hewitt and Manning, 2019), and semantic, semantic textual similarity (STS) as the target such as word senses (Reif et al., 2019) and seman- task and leverage the metric proposed by Mu and tic dependency (Wu et al., 2021). Viswanath (2018) for measuring isotropy. The preDespite"
2021.findings-emnlp.261,2020.acl-main.383,0,0.0202189,"e-trained language models (LMs) be attributed to the increased isotropy of the embedding space? Recently, several studies have focused on the remarkable potential of pre-trained language models, • Does isotropy enhancement (using methods such as BERT (Devlin et al., 2019), in capturing that null out dominant directions) have the linguistic knowledge. They have shown that presame positive outcome for the fine-tuned modtrained representations are able to encode various els as it has for the pre-trained ones? linguistic properties (Tenney et al., 2019a; Talmor et al., 2020; Goodwin et al., 2020; Wu et al., 2020; • How does the distribution of CWRs change Zhou and Srikumar, 2021; Chen et al., 2021; Tenupon fine-tuning? ney et al., 2019b), among others, syntactic, such as part of speech (Liu et al., 2019a) and dependency To answer these questions, we consider the tree (Hewitt and Manning, 2019), and semantic, semantic textual similarity (STS) as the target such as word senses (Reif et al., 2019) and seman- task and leverage the metric proposed by Mu and tic dependency (Wu et al., 2021). Viswanath (2018) for measuring isotropy. The preDespite their significant potential, pre-trained trained BERT and Ro"
2021.findings-emnlp.261,2021.findings-acl.201,0,0.0406867,"n to lack reasons behind the outstanding performance of pre-trained language models and their capabilities, the desirable isotropy property (Rajaee and Pilehvar, 2021; Ethayarajh, 2019). Gao et al. (2019) most recent investigations on fine-tuning have been called the defect the representation degeneration done through probing tasks and by evaluating the encoded linguistic knowledge (Merchant et al., problem and attributed it mainly to the weight ty2020; Mosbach et al., 2020; Talmor et al., 2020; ing trick (Press and Wolf, 2017) and the language modeling as the objective of the training. Under Yu and Ettinger, 2021). These studies demonstrate such a circumstance, random word embeddings that most changes in fine-tuning are applied to the upper layers, such that those layers encode task- are highly similar to one another while shaping a specific knowledge, while lower layers are respon- narrow cone in the space. Clearly, anisotropic distribution hurts the expressiveness of the embedding sible for the core linguistic phenomenon (Durrani space, especially for semantic downstream tasks. et al., 2021). Moreover, the results show that some Cosine similarity-based metrics have usually linguistic information is s"
2021.findings-emnlp.261,2020.findings-emnlp.46,0,0.0297493,"ning data (Li et al., 2020; Garg et al., 2018; Gonen and Goldberg, 2019). The representation degeneration problem is another issue that limits their linguistic capacity. Gao et al. (2019) showed that the weight tying trick (Inan et al., 2017) in the pre-training procedure is mainly responsible for the degeneration problem in the embedding space. In such a case, the embeddings occupy a narrow cone in the space (Ethayarajh, 2019). Several approaches have been proposed to improve the isotropy of pre-trained models, which in turn boosts the representation power and downstream performance of CWRs (Zhang et al., 2020; Wang et al., 2020). However, previous studies have mainly focused on the anisotropy of pre-trained language models. Here, we investigate the impact of fine-tuning on isotropy. Specifically, We try to answer the following questions: • Can the improved performance achieved by fine-tuning pre-trained language models (LMs) be attributed to the increased isotropy of the embedding space? Recently, several studies have focused on the remarkable potential of pre-trained language models, • Does isotropy enhancement (using methods such as BERT (Devlin et al., 2019), in capturing that null out dominant"
2021.findings-emnlp.261,2021.naacl-main.401,0,0.025574,"ed isotropy of the embedding space? Recently, several studies have focused on the remarkable potential of pre-trained language models, • Does isotropy enhancement (using methods such as BERT (Devlin et al., 2019), in capturing that null out dominant directions) have the linguistic knowledge. They have shown that presame positive outcome for the fine-tuned modtrained representations are able to encode various els as it has for the pre-trained ones? linguistic properties (Tenney et al., 2019a; Talmor et al., 2020; Goodwin et al., 2020; Wu et al., 2020; • How does the distribution of CWRs change Zhou and Srikumar, 2021; Chen et al., 2021; Tenupon fine-tuning? ney et al., 2019b), among others, syntactic, such as part of speech (Liu et al., 2019a) and dependency To answer these questions, we consider the tree (Hewitt and Manning, 2019), and semantic, semantic textual similarity (STS) as the target such as word senses (Reif et al., 2019) and seman- task and leverage the metric proposed by Mu and tic dependency (Wu et al., 2021). Viswanath (2018) for measuring isotropy. The preDespite their significant potential, pre-trained trained BERT and RoBERTa (Liu et al., 2019b) representations suffer from important weak"
2021.hackashop-1.1,2020.acl-main.370,0,0.0400492,"dversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus. Costanza Conforti1 , Jakob Berndt2 , Mohammad Taher Pilehvar1,3 , Marco Basaldella1 , Chryssi Giannitsarou2 , Flavio Toxvaerd2 , Nigel Collier1 1 Language Technology Lab, University of Cambridge 2 Faculty of Economics, University of Cambridge 3 Tehran Institute for Advanced Studies, Iran {cc918,jb2088}@cam.ac.uk Abstract datasets, which are especially expensive to obtain for items such as news articles. As a consequence, following research on other text classification tasks such as sentiment analysis (Du et al., 2020), research in SD investigated effective methods for cross-domain SD, where the scarcity of data for a specific dataset is supplemented with stance-annotated data from other domains. In this context, preliminary research in adversarial domain adaptation obtained promising results for both Twitter (Wang et al., 2020) and news (Xu et al., 2019) SD. In this paper, we focus on the new task of crossgenre SD: we consider adversarial knowledge transfer from two datasets, WT– WT and S TANDER, which collect samples in the same domain (i.e. the financial domain), but which belong to different genres (i.e"
2021.hackashop-1.1,C18-1158,0,0.153682,"able online; moreover, user-generated data tend to be relatively short and compact, and thus more affordable to annotate and process. Starting from popular shared tasks such as Pomerleau and Rao (2017), SD on complex and articulated input, such as news articles, has gained increasing popularity. Notably, effective news SD would constitute an invaluable tool to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). In line with the general trend in NLP, deep learning-based models have long since established state-of-the-art results in news SD (Hanselowski et al., 2018). Notably, training neural networks relies heavily on the availability of large labeled 2 An Aligned Multi-Genre Stance Detection Corpus In this work, we rely on two recently released datasets for news and Twitter SD: the S TANDER corpus for the news genre (Conforti et al., 2020a), and the WT– WT corpus for Twitter (Conforti et al., 2020b). Both corpora collect samples discussing four mergers and acquisition (M&A) operations in the healthcare industry (Table 2): an M&A operation, or merger, is the process in which a company (the buyer) attempts to acquire the ownership of another company (the"
2021.hackashop-1.1,2020.findings-emnlp.365,1,0.850925,"g popularity. Notably, effective news SD would constitute an invaluable tool to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). In line with the general trend in NLP, deep learning-based models have long since established state-of-the-art results in news SD (Hanselowski et al., 2018). Notably, training neural networks relies heavily on the availability of large labeled 2 An Aligned Multi-Genre Stance Detection Corpus In this work, we rely on two recently released datasets for news and Twitter SD: the S TANDER corpus for the news genre (Conforti et al., 2020a), and the WT– WT corpus for Twitter (Conforti et al., 2020b). Both corpora collect samples discussing four mergers and acquisition (M&A) operations in the healthcare industry (Table 2): an M&A operation, or merger, is the process in which a company (the buyer) attempts to acquire the ownership of another company (the target). A merger succeeds if ownership of the target is transferred, but can fail at any stage of discussions or can be blocked by authorities due to, e.g., antitrust concerns (Bruner and Perella, 2004). 1 Proceedings of the EACL Hackashop on News Media Content Analysis and Aut"
2021.hackashop-1.1,D19-1410,0,0.0290299,"Missing"
2021.hackashop-1.1,C18-1283,0,0.0208441,"sed on user-generated data, such as Twitter or Reddit (Gorrell et al., 2019): this is mainly due to the abundance of such data, which are usually freely available online; moreover, user-generated data tend to be relatively short and compact, and thus more affordable to annotate and process. Starting from popular shared tasks such as Pomerleau and Rao (2017), SD on complex and articulated input, such as news articles, has gained increasing popularity. Notably, effective news SD would constitute an invaluable tool to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). In line with the general trend in NLP, deep learning-based models have long since established state-of-the-art results in news SD (Hanselowski et al., 2018). Notably, training neural networks relies heavily on the availability of large labeled 2 An Aligned Multi-Genre Stance Detection Corpus In this work, we rely on two recently released datasets for news and Twitter SD: the S TANDER corpus for the news genre (Conforti et al., 2020a), and the WT– WT corpus for Twitter (Conforti et al., 2020b). Both corpora collect samples discussing four mergers and acquisition (M&A) operations in the health"
2021.mrl-1.10,P19-1070,0,0.060336,"Missing"
2021.mrl-1.10,P16-1085,1,0.809987,"226,036 92,202 1,175 1,151 1,155 1,931 1,656 1,467 1,481 1,706 4,272 Nouns Verbs Adj. Adv. EN Test - SemEval-13 FA Table 1: Number of sense-annotated instances in the benchmark datasets after cleaning and unification. RAW counts correspond to number of instances in the original datasets before cleaning and unification. ventories such as WordNet (e.g. semantic relations, sense glosses, distributions, etc.) for inference. For the last decade, the supervised approach has been the dominant paradigm for WSD (Raganato et al., 2017), either the conventional featurebased systems (Zhong and Ng, 2010; Iacobacci et al., 2016), LSTM-driven techniques (Melamud et al., 2016; Yuan et al., 2016), or the more recent trend empowered by pre-trained language models (Loureiro and Jorge, 2019; Scarlini et al., 2020b). In the latter approaches, feature extraction strategies where sense embeddings are determined by averaging a word’s contextualised representations have proven surprisingly effective (Loureiro et al., 2021), even in multilingual settings (Bevilacqua and Navigli, 2020; Raganato et al., 2020). We extend this simple idea to the cross-lingual setting, showing that the vanilla contextualized sense embeddings achievin"
2021.mrl-1.10,S07-1004,0,0.152133,"Missing"
2021.mrl-1.10,P19-1569,1,0.886816,"bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the knowledge acquisition bottleneck for less-resourced languages. In this paper, we aim at investigating this opportunity. To this end, we build upon recent research on cross-lingual transfer to compute contextualized sense embeddings and verify if semantic distinctions in the English language are transferable to oth"
2021.mrl-1.10,K16-1006,0,0.2984,"67 1,481 1,706 4,272 Nouns Verbs Adj. Adv. EN Test - SemEval-13 FA Table 1: Number of sense-annotated instances in the benchmark datasets after cleaning and unification. RAW counts correspond to number of instances in the original datasets before cleaning and unification. ventories such as WordNet (e.g. semantic relations, sense glosses, distributions, etc.) for inference. For the last decade, the supervised approach has been the dominant paradigm for WSD (Raganato et al., 2017), either the conventional featurebased systems (Zhong and Ng, 2010; Iacobacci et al., 2016), LSTM-driven techniques (Melamud et al., 2016; Yuan et al., 2016), or the more recent trend empowered by pre-trained language models (Loureiro and Jorge, 2019; Scarlini et al., 2020b). In the latter approaches, feature extraction strategies where sense embeddings are determined by averaging a word’s contextualised representations have proven surprisingly effective (Loureiro et al., 2021), even in multilingual settings (Bevilacqua and Navigli, 2020; Raganato et al., 2020). We extend this simple idea to the cross-lingual setting, showing that the vanilla contextualized sense embeddings achieving outstanding results in the monolingual setti"
2021.mrl-1.10,W04-0807,0,0.318541,"Missing"
2021.mrl-1.10,W04-0800,0,0.416298,"Missing"
2021.mrl-1.10,H93-1061,0,0.495972,"Missing"
2021.mrl-1.10,Q14-1019,0,0.0939957,"Missing"
2021.mrl-1.10,S13-2040,0,0.021605,"an one person) Cross-Lingual Neural Language Model Input Most Similar Figure 1: Overview of the proposed method for multilingual zero-shot word sense disambiguation. The example sentence presented (‘There is a bench in the hall’, in English) is using a different language (target) than our sense inventory (source), but using multilingual language models and lemma mappings, we demonstrate how it’s still possible to perform disambiguation, using either variations of our method (sense and synset strategies). 3.2 Multilingual SemEval test sets We considered two multilingual datasets: SemEval 2013 (Navigli et al., 2013a) available for English, French and Italian, and SemEval 2015 (Moro and Navigli, 2015), available for English, French, Italian, Spanish and German. These datasets were annotated with BabelNet (Navigli and Ponzetto, 2012), a resource that contains WordNet, among other linked sense inventories. Therefore, from each dataset we simply considered those disambiguated instances that could be mapped to PWN 3.0, while the rest of instances were removed. We also rely on BabelNet to gather a representative set of candidate senses for any given target word. 3.3 FarsNet To extend the evaluation set beyond"
2021.mrl-1.10,S07-1011,0,0.160123,"Missing"
2021.mrl-1.10,N19-4009,0,0.0133653,"age, we first datasets from the unified WSD evaluation framegather all the candidate synsets in the source lanwork of Raganato et al. (2017).7 As expected, the guage from Babelnet. Then each candidate synset 4 Following Loureiro and Jorge (2019), we consider tokenis associated with one or more senses. For examlevel embeddings as the average of sub-token embeddings, ple, we can find two candidate PWN senses for which is computed as the sum of embeddings from the last 4 the word presente (present in Spanish). The first layers of the corresponding NLM. 5 Our code is based on the Fairseq toolkit (Ott et al., 2019). sense corresponds to the PWN synset “intermeWe run our experiments on a single RTX 2070, with a runtime diate between past and future&quot; and the second to under 2 hours for generating all embeddings used in this work. 6 “being or existing in a specified place&quot;. Finally, We experimented with NLMs trained exclusively on Italwe compute the cosine distance from the contex- ian (i.e. UmBERTo-CC and dbmdz-IT-XXL), but found that the senses learned using those models do not consistently tualized embeddings of target word (presente) to outperform the MFS baseline on both test sets. 7 all the candidate"
2021.mrl-1.10,W14-0132,0,0.218083,"n lexical semantics. Currently, the dominant WSD paradigm is the supervised approach (Raganato et al., 2017), which highly relies on sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred a"
2021.mrl-1.10,P19-1493,0,0.0537941,"Missing"
2021.mrl-1.10,E17-1010,1,0.78396,"e across languages and test them on the benchmark. Experimental results show that this contextualized knowledge can be effectively transferred to similar languages through pre-trained multilingual language models, to the extent that they can outperform monolingual representations learned from existing language-specific data. 1 Introduction Word Sense Disambiguation (WSD) is an indispensable component of language understanding (Navigli, 2009); hence, it has been one of the most studied long-standing problems in lexical semantics. Currently, the dominant WSD paradigm is the supervised approach (Raganato et al., 2017), which highly relies on sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated ("
2021.mrl-1.10,2020.emnlp-main.584,1,0.853486,"Missing"
2021.mrl-1.10,2020.lrec-1.706,1,0.733327,"tly, the dominant WSD paradigm is the supervised approach (Raganato et al., 2017), which highly relies on sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et"
2021.mrl-1.10,2020.lrec-1.723,0,0.141421,"Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the kn"
2021.mrl-1.10,L18-1268,0,0.0164441,"sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportuniti"
2021.mrl-1.10,N18-1202,0,0.350205,"nowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the knowledge acquisition bottleneck for less-resourced languages. In this paper, we aim at investigating this opportunity. To this end, we build upon recent research on cross-lingual transfer to compute contextualized sense embeddings and verify if semantic distinctions in the English langua"
2021.mrl-1.10,2019.gwc-1.14,0,0.0154849,"992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the knowledge acquisition bottleneck for less-resourced languages. In this paper, we aim at investigating this opportunity. To this end, we build upon recent research on cross-lingual transfer to compute contextualized sense embeddings and verify if semantic distinctions in the English language are transferable to other languages. The c"
2021.mrl-1.10,D17-1207,0,0.0294596,"annotations. However, as usual in NLP, most of As training corpora we used SemCor (Miller et al., the sense-annotated corpora are dedicated to the 1993), which consists of a collection of English English language only. Nonetheless, recent work documents annotated with PWN senses. Despite on cross-lingual word embeddings has shown that its age, SemCor remains the standard training corit is possible to reliably align monolingual seman- pus for WSD due to its large number of manual tic spaces with minimal or no supervision (Artetxe sense annotations. There have been several efforts et al., 2017; Zhang et al., 2017; Conneau et al., towards providing sense annotations for translated 2018). Moreover, pre-trained language models, versions of SemCor (Petrolito and Bond, 2014a). like BERT (Devlin et al., 2019), have been shown Consequently, we also considered the Italian verto be effective in transferring knowledge across lan- sion of SemCor included in MultiSemCor (Benguages (Lample and Conneau, 2019; Pires et al., tivogli and Pianta, 2005), which is the language 2019; Artetxe et al., 2020). In this paper we build with most PWN annotations available. MultiSemon these ideas to take the best of both worlds. I"
2021.mrl-1.10,P10-4014,0,0.0465707,",176 1,448 3,498 RAW 226,036 92,202 1,175 1,151 1,155 1,931 1,656 1,467 1,481 1,706 4,272 Nouns Verbs Adj. Adv. EN Test - SemEval-13 FA Table 1: Number of sense-annotated instances in the benchmark datasets after cleaning and unification. RAW counts correspond to number of instances in the original datasets before cleaning and unification. ventories such as WordNet (e.g. semantic relations, sense glosses, distributions, etc.) for inference. For the last decade, the supervised approach has been the dominant paradigm for WSD (Raganato et al., 2017), either the conventional featurebased systems (Zhong and Ng, 2010; Iacobacci et al., 2016), LSTM-driven techniques (Melamud et al., 2016; Yuan et al., 2016), or the more recent trend empowered by pre-trained language models (Loureiro and Jorge, 2019; Scarlini et al., 2020b). In the latter approaches, feature extraction strategies where sense embeddings are determined by averaging a word’s contextualised representations have proven surprisingly effective (Loureiro et al., 2021), even in multilingual settings (Bevilacqua and Navigli, 2020; Raganato et al., 2020). We extend this simple idea to the cross-lingual setting, showing that the vanilla contextualized"
2021.starsem-1.9,P17-1171,0,0.0216167,"or ParsFEVER, the inter-annotator agreement and annotators’ agreement against the dataset are 0.92 and 0.87 based on accuracy, respectively. 3.4 Dataset Statistics Table 4 lists the distribution of instances across the three classes in the training, development, and test sets. Unlike FEVER which only includes mutated claims, in ParsFEVER we consider both mutated and original claims to improve training. 102 4 Experiments Following Thorne et al. (2018), we implemented a full pipeline system for fact verification and extraction with the following three modules: 1. A document retrieval component (Chen et al., 2017) to find the most relevant page to a specific claim. 2. A sentence retrieval module to extract the evidence sentence (DrQA-based sentence retrieval module). Split SUP REF NEI Training Dev Test 6,253 841 853 4,008 824 833 5,685 861 863 Total 7,947 5,665 7,409 Model Table 4: Distribution of instances in ParsFEVER across the three classes: SUPPORTED ( SUP ), RE FUTED ( REF ), and N OT E NOUGH I NFO ( NEI ). The statistics are for the pruned dataset, i.e., after omitting claims which are ambiguous or contain typo (around 1,885 samples). Both mutated and original claims are included in the dataset."
2021.starsem-1.9,2020.findings-emnlp.309,0,0.0613026,"Missing"
2021.starsem-1.9,P14-5010,0,0.00251165,"ection. This dataset contains 12.8K human-labeled instances. Dataset Performing accurate fact-checking at scale requires a high-quality dataset along with the necessary algorithms and models. While there is a significant volume of research on the algorithms and models, they are generally language-agnostic. However, the datasets must be developed for each language independently. In this work, while using FEVER as a baseline, we modify their approach to make it more suitable for low-resource languages like Farsi. Thorne et al. (2018) processed the June 2017 Wikipedia dump with Stanford CoreNLP (Manning et al., 2014) to collect sentences from the introductory sections of approximately 5K popular pages. In addition to this set of primary pages, all the related (secondary) pages2 are retrieved. Following this procedure, we manually selected a set of 358 articles from the most popular Farsi pages crawled from fa.wikipedia.org. While FEVER provides an annotation tool, it leverages proprietary services which are not publicly available. Hence we developed our own Wikipedia crawler and annotation tools, which we release along with our dataset and annotation guidelines. Table 1 shows two samples from ParsFEVER. I"
2021.starsem-1.9,D16-1244,0,0.0428612,"FUTED ( REF ), and N OT E NOUGH I NFO ( NEI ). The statistics are for the pruned dataset, i.e., after omitting claims which are ambiguous or contain typo (around 1,885 samples). Both mutated and original claims are included in the dataset. 3. Two recognizing textual entailment (RTE) models are used to classify the claim based on collected evidences as SUPPORTED, RE FUTED , or NOT E NOUGH I NFO . These models are based on MLP (Riedel et al., 2017), with a single hidden layer that benefits term frequencies and TF-IDF cosine similarity between the claim and evidence, and Decomposable Attention (Parikh et al., 2016, DA). Given that NOT ENOUGH I NFO instances are not associated with any evidence, they cannot be used for training the RTE models. To address this issue, Thorne et al. (2018) proposed two alternatives solutions: sampling a sentence (as evidence) from the nearest page to the claim (NP) or using the document retrieval component to uniformly select a random sentence (as evidence) from Wikipedia (RS). 4.1 Results We customized the system based on Farsi. Following Thorne et al. (2018), we set k = 5 (k nearest documents to the claim for document retrieval) and l = 5 (top l-most similar sentences fr"
2021.starsem-1.9,C18-1283,0,0.0171628,"tains similar downstream performance, indicating the quality of the dataset. We release the dataset and the annotation guidelines at https://github. com/Zarharan/ParsFEVER. 1 Introduction The spread of false information can lead to severe social and political problems (Wang, 2017). It would be extremely difficult to detect and track false information manually, given that the abundance of available technology has made it possible for these to be produced at scale and disseminated rapidly. Therefore, there has been a lot of interest in developing natural language technologies for fact-checking (Thorne and Vlachos, 2018). Unfortunately, similarly to many other fields of NLP that rely on manually curated datasets, fact-checking has remained restricted to a few high-resource languages for which large-scale annotated datasets are available. 1 The annotators who were responsible for training and leading other annotators. 99 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 99–104 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics 2 Related Work 3 The only related datasets in Farsi are those of Zarharan et al. (2019) and Zamani et al. (2017)."
2021.starsem-1.9,N18-1074,0,0.0473832,"Missing"
2021.starsem-1.9,P17-2067,0,0.0306256,"a dataset widely used for fact extraction and verification in English. The dataset consists of around 185K claims generated by modifying sentences extracted from Wikipedia. The claims are classified as SUPPORTED, REFUTED, and NOT E NOGH I NFO. Despite being based on FEVER, our dataset has some fundamental differences that aim at making a more challenging benchmark for low-resourced languages. In the following section, we elaborate on the construction procedure of our dataset and the differences it has to that used for FEVER. Other related datasets include HOVER (Jiang et al., 2020) and LIAR (Wang, 2017). HOVER is a dataset for many-hop fact extraction and claim verification. Unlike our dataset, which consists of single sentence claims, HOVER includes claims from one sentence up to one paragraph. It consists of 26K claims with SUPPORTED or NOTSUPPORTED labels. LIAR was instead derived from the short statements extracted from POLITIFACT. COM for fake news detection. This dataset contains 12.8K human-labeled instances. Dataset Performing accurate fact-checking at scale requires a high-quality dataset along with the necessary algorithms and models. While there is a significant volume of research"
2021.wassa-1.19,N18-2004,0,0.028242,"Missing"
2021.wassa-1.19,W17-0904,0,0.0176178,"Missing"
2021.wassa-1.19,N10-1004,0,0.0507836,"ity, Sentiment and Social Media Analysis, pages 181–187 April 19, 2021. ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal fro"
2021.wassa-1.19,P09-1113,0,0.0298862,"f the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 181–187 April 19, 2021. ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a"
2021.wassa-1.19,D18-1045,0,0.0667885,"Missing"
2021.wassa-1.19,P17-2090,0,0.0199156,"imple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal from silver data. 4. We predict the ID test data with the system trained in 3. Related Work on Stance Detection SD is a widely investigated field in NLP. Starting from Mohammad et al. (2017), research in SD focused on the analysis of Twitter posts. Another research direction explored the classification of Twitter users with respect to given topics, like political independence (Darwish et al., 2019). Work on other types of user-generated data includes SD on parenting blogs (Skeppstedt et al., 2017), political posts on newspapers websites (Hanselowski et al., 2018), posts on online debate forums on various topics (Hasan and Ng, 2014) and posts on wordpress blogs (Simaki et al., 2017). SD has been also integrated into Fake News Detection (Pomerleau and Rao, 2017) and constitutes an"
2021.wassa-1.19,C14-1168,0,0.0284561,"tional Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 181–187 April 19, 2021. ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data"
2021.wassa-1.19,C18-1158,0,0.0464177,"Missing"
2021.wassa-1.19,P18-1096,0,0.0234355,". ©2021 Association for Computational Linguistics to well-structured input: the only noise are potential errors in synthetic labeling. Our approach fits into the broad family of weaklyand semi-supervised frameworks which have been adopted to tackle domain adaptation (DAda) problems (Søgaard, 2013). In recent literature, such methods have been applied with mixed success to many tasks, ranging from named entity recognition (Fries et al., 2017) to relation extraction (Mintz et al., 2009), tagging (Plank et al., 2014), parsing (McClosky et al., 2010), and sentiment analysis (Blitzer et al., 2007; Ruder and Plank, 2018; Ratner et al., 2020). In this paper, we propose to apply weakly supervision to SD, by adopting the extremely simple and inexpensive framework described above. Figure 1: Pipeline of the framework. Rectangular boxes: gold annotations; cornered boxes: unlabeled/synthetic annotations; green lines: elements which are passed from different stages of the pipeline. 3 3. We train a new system on both gold OOD and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal from silver data. 4. We predict the ID test data with the system trained"
2021.wassa-1.19,D14-1083,0,0.024934,"We predict the ID test data with the system trained in 3. Related Work on Stance Detection SD is a widely investigated field in NLP. Starting from Mohammad et al. (2017), research in SD focused on the analysis of Twitter posts. Another research direction explored the classification of Twitter users with respect to given topics, like political independence (Darwish et al., 2019). Work on other types of user-generated data includes SD on parenting blogs (Skeppstedt et al., 2017), political posts on newspapers websites (Hanselowski et al., 2018), posts on online debate forums on various topics (Hasan and Ng, 2014) and posts on wordpress blogs (Simaki et al., 2017). SD has been also integrated into Fake News Detection (Pomerleau and Rao, 2017) and constitutes an important step in the rumor verification pipeline (Zubiaga et al., 2018b): in this framework, popular shared tasks focused on SD of rumorous tweets (Gorrell et al., 2018) and Reddit posts (Gorrell et al., 2018). These works analyze tweets in a tree-shaped stream (Zubiaga et al., 2015). Note that SD constitutes a related but different task than sentiment analysis (Mohammad et al., 2017): the latter focuses on the polarity expressed w.r.t. a topic"
2021.wassa-1.19,P16-1009,0,0.113015,"Missing"
2021.wassa-1.19,E17-2088,0,0.018627,"ments in performance, with gains in F1 scores ranging from +3.4 to +5.1. 1 Introduction Stance Detection (SD) is a widely investigated task (Mohammad et al., 2017), which constitutes an important component of many complex NLP problems, ranging from fake news detection to rumour verification (Vlachos and Riedel, 2014; Baly et al., 2018; Zubiaga et al., 2018b). Since from early works (Agrawal et al.), research on SD focused on user-generated content, ranging from blogs and commenting sections on websites (Hercig et al.), to Reddit or Facebook posts (Klenner et al.) and, above all, Twitter data (Inkpen et al., 2017; Zubiaga et al., 2018a). Recently, Conforti et al. (2020) released WillThey-Won’t-They (WT– WT), a very large corpus of stance-annotated tweets discussing five US mergers and acquisitions (M&A) operations spanning over two industries: healthcare and entertainment. M&A is a general term that refers to the process in which the ownership of companies are transferred. Such process has many stages that range from informal talks to the closing of a deal, and discussions may not be publicly disclosed until a formal agreement is signed (Bruner 2 Cross-Target Generalization with Synthetically Annotate"
2021.wassa-1.19,W17-5801,0,0.0265033,"and synthetic ID data: in this way, the system is exposed to a gold signal from the OOD data and to a noisy but ID signal from silver data. 4. We predict the ID test data with the system trained in 3. Related Work on Stance Detection SD is a widely investigated field in NLP. Starting from Mohammad et al. (2017), research in SD focused on the analysis of Twitter posts. Another research direction explored the classification of Twitter users with respect to given topics, like political independence (Darwish et al., 2019). Work on other types of user-generated data includes SD on parenting blogs (Skeppstedt et al., 2017), political posts on newspapers websites (Hanselowski et al., 2018), posts on online debate forums on various topics (Hasan and Ng, 2014) and posts on wordpress blogs (Simaki et al., 2017). SD has been also integrated into Fake News Detection (Pomerleau and Rao, 2017) and constitutes an important step in the rumor verification pipeline (Zubiaga et al., 2018b): in this framework, popular shared tasks focused on SD of rumorous tweets (Gorrell et al., 2018) and Reddit posts (Gorrell et al., 2018). These works analyze tweets in a tree-shaped stream (Zubiaga et al., 2015). Note that SD constitutes"
2021.wassa-1.19,D17-1215,0,0.0232299,"VSHealth buys Aetna CVS #PBM has resulted in delays in therapy, switches, etc all documented. Terrible! The sentiment of the tweet w.r.t. the target is negative: the user believes that the merger would harm patients; however, its stance is comment, as it is Comparison with previous work on Data Augmentation and Domain Adaptation. Note that this framework differs from data augmentation (DAug) strategies adopted to supply for small training data, like in question answering (Kafle et al.), machine translation (Fadaee et al.) distillation (Tang et al., 2019), or for adversarial sample generation (Jia and Liang, 2017). Such techniques, inspired by DAug in speech recognition and computer vision (Chatfield et al., 2014), work by deformating gold samples to generate new artificial samples (for example, by random token masking, or POS- or semantics-based token replacement). Our approach differs in a number of aspects: 1. In DAug the goal is to enlarge a set of initial ID data; here, we assume we don’t have any ID training data, but only OOD; 2. For this reason, while DAug helps to cope with data sparsity, our approach is also useful for domain shifts; 3. In DAug, sample generation might introduce two kinds of"
2021.wassa-1.19,W14-2508,0,0.0225027,"offer great benefits. In this paper, we apply a weakly supervised framework to enhance cross-target generalization through synthetically annotated data. We focus on Twitter SD and show experimentally that integrating synthetic data is helpful for cross-target generalization, leading to significant improvements in performance, with gains in F1 scores ranging from +3.4 to +5.1. 1 Introduction Stance Detection (SD) is a widely investigated task (Mohammad et al., 2017), which constitutes an important component of many complex NLP problems, ranging from fake news detection to rumour verification (Vlachos and Riedel, 2014; Baly et al., 2018; Zubiaga et al., 2018b). Since from early works (Agrawal et al.), research on SD focused on user-generated content, ranging from blogs and commenting sections on websites (Hercig et al.), to Reddit or Facebook posts (Klenner et al.) and, above all, Twitter data (Inkpen et al., 2017; Zubiaga et al., 2018a). Recently, Conforti et al. (2020) released WillThey-Won’t-They (WT– WT), a very large corpus of stance-annotated tweets discussing five US mergers and acquisitions (M&A) operations spanning over two industries: healthcare and entertainment. M&A is a general term that refer"
D16-1174,W06-3814,0,0.0438754,"Missing"
D16-1174,S14-2098,0,0.111441,"Missing"
D16-1174,J10-4006,0,0.140542,"Missing"
D16-1174,N15-1059,1,0.907262,"Missing"
D16-1174,D14-1110,0,0.440814,"Missing"
D16-1174,P11-1144,0,0.0530254,"Missing"
D16-1174,N15-1184,0,0.102007,"Missing"
D16-1174,N13-1092,0,0.160035,"Missing"
D16-1174,J15-4004,0,0.111179,"Missing"
D16-1174,P12-1092,0,0.738492,"Missing"
D16-1174,P15-1010,1,0.875169,"Missing"
D16-1174,N15-1070,0,0.467518,"Missing"
D16-1174,S14-2003,1,0.911966,"Missing"
D16-1174,S14-2072,0,0.0877215,"Missing"
D16-1174,D14-1113,0,0.443083,"Missing"
D16-1174,W11-0122,0,0.0701096,"Missing"
D16-1174,D14-1162,0,0.077956,"Missing"
D16-1174,S14-2093,0,0.0654737,"Missing"
D16-1174,N10-1013,0,0.382386,"Missing"
D16-1174,P15-1173,0,0.594965,"Missing"
D16-1174,C14-1016,0,0.34374,"Missing"
D16-1174,W09-3206,0,0.091897,"Missing"
D16-1174,P14-2089,0,0.0678899,"Missing"
D16-1174,J16-2003,0,\N,Missing
D18-1169,P07-1056,0,0.0314137,"Missing"
D18-1169,Q17-1010,0,0.131523,"e evaluate the performance of some of recent models on our dataset. These techniques can be broadly classified into two categories. The first group exploits the knowledge encoded for a rare word in external lexical resources (Section 5.2.1), whereas the second induces embeddings for rare words by extending the semantics of its subword units (Section 5.2.2). 5.2.1 5.2.2 Subword models Resource-based models fall short of inducing embeddings for words that are not covered in the lexical resource. Subword models alleviate this limitation by breaking the word into its subword (Pinter et al., 2017; Bojanowski et al., 2017) or morphological units (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015) and induce an embedding by composing the information available for these. FastText (Bojanowski et al., 2017) is one of the popular approaches of this type. The model first splits the unseen word into character ngrams (by default, 3- to 6-grams) and then computes the unseen word’s embedding as the centroid of the embeddings of these character n-grams (which are available as a result of a specific training). We also report results for Mimick (Pinter et al., 2017), one of the most recent subword models. T"
D18-1169,S17-2002,1,0.857853,"liable dataset. 3.2 Construction Procedure The following four-phase procedure was used to construct the dataset: (1) A set of 660 rare words were carefully selected from a wide range of domains; (2) For each of these initial words, a pairing word was manually selected according to a randomly sampled score from the similarity scale (Section 3.2.2); (3) All pairs were scored by 8 annotators; (4) A final adjudication was performed to address disagreements (Section 3.2.3). 3.2.1 Similarity scale We adopted the five-point Likert scale used for the annotation of the datasets in SemEval-2017 Task 2 (Camacho-Collados et al., 2017). The task reported high IAA scores which reflects the welldefinedness and clarity of the scale. We provided annotators with the concise guideline shown in Table 1, along with several examples. Given the continuity of the scale, the annotators were given flexibility to select values in between the five points, whenever appropriate, with a step size of 0.5. The annotators were asked in the guidelines to make sure they were familiar with all common meanings of the word (as defined by WordNet or other online dictionaries). To facilitate the annotation, the annotators were provided with the defini"
D18-1169,D16-1235,0,0.0863574,"Missing"
D18-1169,D17-1030,0,0.0740015,"Missing"
D18-1169,J15-4004,0,0.122077,"our evaluation of mainstream word embeddings and recent word representation techniques on the dataset. Finally, concluding remarks are mentioned in Section 6. 2 Related Work Word similarity datasets have been one of the oldest, still most prominent, benchmarks for the evaluation and comparison of semantic representation techniques. As a result, several word similarity datasets have been constructed during the past few decades; to name a few: RG-65 (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2002), YP-130 (Yang and Powers, 2005), MEN-3K (Bruni et al., 2014), SimLex-999 (Hill et al., 2015), and SimVerb-3500 (Gerz et al., 2016). Many of these English word similarity datasets have been translated to other languages to create frameworks for multilingual (Leviant and Reichart, 2015) or crosslingual (CamachoCollados et al., 2017) semantic representation techniques. However, these datasets mostly target words that occur frequently in generic texts and, as a result, are not suitable for the evaluation of subword or rare word representation models. One may opt for transforming a frequent-word benchmarks into an artificial rare word dataset by downsampling the dataset’s words in the und"
D18-1169,W04-1213,1,0.707379,"Missing"
D18-1169,2005.mtsummit-papers.11,0,0.0111694,"Missing"
D18-1169,P14-2050,0,0.0581858,"challenging rare word benchmark should ideally reflect this phenomenon. To verify this in our dataset, we experimented with a set of commonly used word embeddings trained on corpora with billions of tokens. Table 4 provides correlation performance results for different embedding sets on the RW and C ARD -660 datasets. Specifically, we considered different variants of Word2vec7 (Mikolov et al., 7 https://code.google.com/archive/p/word2vec/ 2013) and Glove8 (Pennington et al., 2014), two commonly-used word embeddings that are trained on massively large text corpora; Dependencybased embeddings9 (Levy and Goldberg, 2014) which extends the Skip-gram model to handle dependency-based contexts; LexVec10 (Salle et al., 2016) which improves the Skip-gram model to better handle frequent words; and ConceptNet Numberbatch11 (Speer et al., 2017) which exploits lexical knowledge from multiple resources, such as Wiktionary and WordNet, and was the best performing system in SemEval 2017 Task 2. In the last two rows of the Table we also report results for two hybrid embeddings constructed by combining the pre-trained Freebase Word2vec, which mostly comprises named entities, with two of the best performing embeddings evalua"
D18-1169,P17-1015,0,0.0255784,"Missing"
D18-1169,W13-3512,0,0.603424,"models in a downstream NLP system, despite being very important, does not provide a solid base for comparing different models, given that small variations in the architecture, parameter setting, or initialisation can lead to performance differences. Moreover, such an evaluation would reflect the “suitability” of representations for that specific configuration and for that particular task, and might not be conclusive for other settings. As far as generic evaluation is concerned, existing benchmarks generally target frequent words. An exception is the Stanford Rare Word (RW) Similarity dataset (Luong et al., 2013) which has been the standard evaluation benchmark for rare word representation techniques for the past few years. In Section 2.1, we will provide an in-depth analysis of RW and highlight that crowdsourcing the annotations, with no rigorous checkpoints, has compromised the reliability of the dataset. This is mainly reflected by the low inter-annotator agreement (IAA), a performance ceiling which is easily surpassed by many existing models. To overcome this barrier and to fill the gap for a reliable benchmark for the evaluation of subword and rare word representation techniques, we introduce a n"
D18-1169,P11-1015,0,0.193517,"Missing"
D18-1169,D14-1162,0,0.0947241,"lier in the Introduction, it is not possible to enumerate the entire vocabulary of a natural language, even if massive corpora are used. A challenging rare word benchmark should ideally reflect this phenomenon. To verify this in our dataset, we experimented with a set of commonly used word embeddings trained on corpora with billions of tokens. Table 4 provides correlation performance results for different embedding sets on the RW and C ARD -660 datasets. Specifically, we considered different variants of Word2vec7 (Mikolov et al., 7 https://code.google.com/archive/p/word2vec/ 2013) and Glove8 (Pennington et al., 2014), two commonly-used word embeddings that are trained on massively large text corpora; Dependencybased embeddings9 (Levy and Goldberg, 2014) which extends the Skip-gram model to handle dependency-based contexts; LexVec10 (Salle et al., 2016) which improves the Skip-gram model to better handle frequent words; and ConceptNet Numberbatch11 (Speer et al., 2017) which exploits lexical knowledge from multiple resources, such as Wiktionary and WordNet, and was the best performing system in SemEval 2017 Task 2. In the last two rows of the Table we also report results for two hybrid embeddings construct"
D18-1169,E17-2062,1,0.778842,"and estimate the embedding for a rare word by exploiting different types of lexical knowledge encoded for it in the resource. The definition centroid model of Lazaridou et al. (2017) takes WordNet word glosses (definitions) as semantic clue. An embedding is induced for an unseen word by averaging the content words’ embeddings in its definition.12 The definition LSTM strategy of Bahdanau et al. (2017) extends the centroid model by encoding the definition using an LSTM network (Hochreiter and Schmidhuber, 1997), in order to better capture the semantics and word order in the definition. SemLand (Pilehvar and Collier, 2017) also uses WordNet, but takes a different approach which benefits from the graph structure of WordNet. For an unseen word, SemLand extracts the set of its semantically related words from WordNet and induces an embedding for the unseen word by combining pre-trained embeddings for the related words. 5.3 Experimental Setup We report results for the five techniques discussed in Sections 5.2.2 and 5.2.1. We used two of the best performing embedding sets, i.e., Glove cased CC and ConceptNet Numberbatch, to train the models (except FastText for which we use the pre-trained WikiNews subword embeddings"
D18-1169,D17-1010,0,0.0965811,"In this experiment, we evaluate the performance of some of recent models on our dataset. These techniques can be broadly classified into two categories. The first group exploits the knowledge encoded for a rare word in external lexical resources (Section 5.2.1), whereas the second induces embeddings for rare words by extending the semantics of its subword units (Section 5.2.2). 5.2.1 5.2.2 Subword models Resource-based models fall short of inducing embeddings for words that are not covered in the lexical resource. Subword models alleviate this limitation by breaking the word into its subword (Pinter et al., 2017; Bojanowski et al., 2017) or morphological units (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015) and induce an embedding by composing the information available for these. FastText (Bojanowski et al., 2017) is one of the popular approaches of this type. The model first splits the unseen word into character ngrams (by default, 3- to 6-grams) and then computes the unseen word’s embedding as the centroid of the embeddings of these character n-grams (which are available as a result of a specific training). We also report results for Mimick (Pinter et al., 2017), one of the mos"
D18-1169,D16-1264,0,0.129264,"Missing"
D18-1169,P16-2068,0,0.0346199,"experimented with a set of commonly used word embeddings trained on corpora with billions of tokens. Table 4 provides correlation performance results for different embedding sets on the RW and C ARD -660 datasets. Specifically, we considered different variants of Word2vec7 (Mikolov et al., 7 https://code.google.com/archive/p/word2vec/ 2013) and Glove8 (Pennington et al., 2014), two commonly-used word embeddings that are trained on massively large text corpora; Dependencybased embeddings9 (Levy and Goldberg, 2014) which extends the Skip-gram model to handle dependency-based contexts; LexVec10 (Salle et al., 2016) which improves the Skip-gram model to better handle frequent words; and ConceptNet Numberbatch11 (Speer et al., 2017) which exploits lexical knowledge from multiple resources, such as Wiktionary and WordNet, and was the best performing system in SemEval 2017 Task 2. In the last two rows of the Table we also report results for two hybrid embeddings constructed by combining the pre-trained Freebase Word2vec, which mostly comprises named entities, with two of the best performing embeddings evaluated on the dataset. Given that the word embeddings are not comparable across two different spaces, we"
D18-1169,D15-1033,0,0.0430563,"Missing"
D18-1169,N15-1186,0,0.0191791,"oadly classified into two categories. The first group exploits the knowledge encoded for a rare word in external lexical resources (Section 5.2.1), whereas the second induces embeddings for rare words by extending the semantics of its subword units (Section 5.2.2). 5.2.1 5.2.2 Subword models Resource-based models fall short of inducing embeddings for words that are not covered in the lexical resource. Subword models alleviate this limitation by breaking the word into its subword (Pinter et al., 2017; Bojanowski et al., 2017) or morphological units (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015) and induce an embedding by composing the information available for these. FastText (Bojanowski et al., 2017) is one of the popular approaches of this type. The model first splits the unseen word into character ngrams (by default, 3- to 6-grams) and then computes the unseen word’s embedding as the centroid of the embeddings of these character n-grams (which are available as a result of a specific training). We also report results for Mimick (Pinter et al., 2017), one of the most recent subword models. The technique learns a mapping function from strings to embeddings by training a Bi-LSTM netw"
D18-1221,D15-1177,1,0.862559,"andom walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al., 2017). The crucial difference of this work is that the ambiguity resolution mechanism is part of the compositional model itself, and the sense embeddings are trained simultaneously with the rest of the parameters. A close work is by Cheng and Kartsaklis (2015), who used a siamese network with an integrated disambiguation mechanism for paraphrase detection. For more information on multi-sense embeddings see (CamachoCollados and Pilehvar, 2018). Methodology Fig. 1 provides a high-level illustration of our methodology, consisting of two stages: (1) the KB graph is extended with weighted textual features, and an artificial “corpus” of random walks is created and used as input to the skipgram model (Mikolov et al., 2013) for generating an enhanced KB space—this part is covered in §3.1; (2) the transformation from text to entities is performed by a super"
D18-1221,Q16-1002,0,0.134813,"rgescale text-to-entity mapping and entity classification tasks, with state of the art results. 1 Introduction The task of associating a well-defined action, concept or piece of knowledge to a natural language utterance or text is a common problem in natural language processing and generic artificial intelligence (Tellex et al., 2011), and can emerge in many different forms. In NLP, the ability to code text into an entity of a knowledge graph finds applications in tasks such as question answering and information retrieval, or any task that involves some form of mapping a definition to a term (Hill et al., 2016; Rimell et al., 2016). Further, it can be invaluable in providing solutions to domain-specific challenges, for example medical concept normalisation (Limsopatham and Collier, 2016) and identification of adverse drug reactions (O’Connor et al., 2014). This paper details a model for efficiently mapping unrestricted text at the level of phrases and sentences to the entities of a knowledge base ∗ This paper is dedicated to the memory of Euripides Kartsaklis, a man who loved technology. (KB)—a task also referred to as text grounding or normalisation. The model aims at characterising short focused"
D18-1221,D13-1166,1,0.898941,"Missing"
D18-1221,C12-2054,1,0.886801,"Missing"
D18-1221,W13-3513,1,0.849346,"nts, optic, dendrite, rubiaceae, nonparametric, meninges, deviation, anesthetics Sense 2. tableware, meal, expectation, heartily, kitchen, hum, eating, forestay, suitors, croupier, companionship, restaurant, dishes, candles, cup, tea Sense 3. reassigned, projective, ultracentrifuge, polemoniaceous, thyronine, assumptions, lymphocyte, atomic, difficulties, intracellular, virgil, elementary, cartesian form of word sense disambiguation when composing word vectors can provide consistent improvements on end tasks such as sentence similarity and paraphrase detection (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2013). 6 Table 5: Derived senses for word table, visualised as lists of nearest neighbouring words in the vector space. the “body part” sense of the word. In our model, homonymy issues are resolved by design: each point in the target space corresponds to a welldefined unambiguous concept or synset. Further, the attentional mechanism of Fig. 4 handles subtle variations of each distinct sense due to polysemy. The effectiveness of the textual feature mechanism was demonstrated in every task we attempted, but to different extents. As our tuning on the dev sets showed, for tasks closer to textto-entity"
D18-1221,D11-1049,0,0.0303607,"Missing"
D18-1221,D15-1200,0,0.021254,"Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al., 2017). The crucial difference of this work is that the ambiguity resolution mechanism is part of the compositional model itself, and the sense embeddings are trained simultaneously with the rest of the parameters. A close work is by Cheng and Kartsaklis (2015), who used a siamese network with an integrated disambiguation mechanism for paraphrase detection. For more information on multi-sense embeddings see (CamachoCollados and Pilehvar, 2018). Methodology Fig. 1 provides a high-level illustration of our methodology, consisting of two stages: (1) the KB graph is extended with"
D18-1221,D15-1194,1,0.844041,"results demonstrate the effectiveness of our methods by improving the current state of the art. 2 Background Aligning meaning between text and entities in a knowledge graph is a task traditionally based on heuristic methods exploiting text features such as string matching, word weighting, syntactic relations, or dictionary lookups (McCallum et al., 2005; Lu et al., 2011; O’Connor et al., 2014). Machine learning techniques have been also exploited in various forms, for example Leaman et al. (2013) use a pairwise learning-to-rank technique to learn the similarity between different terms, while Limsopatham and Collier (2015) apply statistical machine translation to “translate” social media text to domain-specific terminology. There is little work based on neural networks; the most relevant to us 1 https://www.snomed.org/snomed-ct is a study by Hill et al. (2016), who tested a number of compositional neural architectures trained to approximate word embeddings on a reverse dictionary task. Compared to their work, this paper proposes the use of a distinct target space for representing ontological knowledge, where every entity in the graph lives. The goal of a graph embedding method is to embed components of a knowle"
D18-1221,P16-1096,1,0.859784,"piece of knowledge to a natural language utterance or text is a common problem in natural language processing and generic artificial intelligence (Tellex et al., 2011), and can emerge in many different forms. In NLP, the ability to code text into an entity of a knowledge graph finds applications in tasks such as question answering and information retrieval, or any task that involves some form of mapping a definition to a term (Hill et al., 2016; Rimell et al., 2016). Further, it can be invaluable in providing solutions to domain-specific challenges, for example medical concept normalisation (Limsopatham and Collier, 2016) and identification of adverse drug reactions (O’Connor et al., 2014). This paper details a model for efficiently mapping unrestricted text at the level of phrases and sentences to the entities of a knowledge base ∗ This paper is dedicated to the memory of Euripides Kartsaklis, a man who loved technology. (KB)—a task also referred to as text grounding or normalisation. The model aims at characterising short focused texts, such as definitions or tweets. Given a medical KB, for example, a tweet of the form “Can’t sleep, too tired to think straight” would be mapped to the entity Insomnia, while i"
D18-1221,P17-1170,1,0.865639,"an et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al., 2017). The crucial difference of this work is that the ambiguity resolution mechanism is part of the compositional model itself, and the sense embeddings are trained simultaneously with the rest of the parameters. A close work is by Cheng and Kartsaklis (2015), who used a siamese network with an integrated disambiguation mechanism for paraphrase detection. For more information on multi-sense embeddings see (CamachoCollados and Pilehvar, 2018). Methodology Fig. 1 provides a high-level illustration of our methodology, consisting of two stages: (1) the KB graph is extended with weighted textual featur"
D18-1221,N10-1013,0,0.112582,"Missing"
D18-1221,D14-1113,0,0.0162897,"of using textual features to improve the entity vectors is not well explored, and most of the existing work focuses again on the representation of relations (Xie et al., 2016; Wang et al., 2014; Wang and Li, 2016) as opposed to entities. Closer to us is the work of Yamada et al. (2017) and Yang et al. (2015), with the latter to incorporate text features in the concept embeddings by exploiting matrix factorisation properties. Representing the meaning of words using a number of sense vectors is an old and well-established idea in NLP—see for example (Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 . . . nT and a context window size c, skipgram maximises the following quantity: KB space Text data T 1X T Skipgram Textual features such as word similarity, while in the few works based in real end tasks, disambiguation is usually treated as a prior stand-alone step (Kartsaklis and Sadrzadeh, 2013; Li and Jurafsky, 2015; Pilehvar et al.,"
D18-1221,D14-1167,0,0.0246019,"dom walk-based methods are not the only way to construct graph spaces—alternatives include factorisation (Ahmed et al., 2013) and deep autoencoders (Wang et al., 2016)—they have been found very effective in capturing multiple aspects of the graph structure (Wang et al., 2017; Goyal and Ferrara, 2017). The current paper proposes a random walk generation strategy that improves and complements existing approaches. The idea of using textual features to improve the entity vectors is not well explored, and most of the existing work focuses again on the representation of relations (Xie et al., 2016; Wang et al., 2014; Wang and Li, 2016) as opposed to entities. Closer to us is the work of Yamada et al. (2017) and Yang et al. (2015), with the latter to incorporate text features in the concept embeddings by exploiting matrix factorisation properties. Representing the meaning of words using a number of sense vectors is an old and well-established idea in NLP—see for example (Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then us"
D18-1221,P16-1219,0,0.0345946,"Missing"
D18-1221,Q17-1028,0,0.0235835,"factorisation (Ahmed et al., 2013) and deep autoencoders (Wang et al., 2016)—they have been found very effective in capturing multiple aspects of the graph structure (Wang et al., 2017; Goyal and Ferrara, 2017). The current paper proposes a random walk generation strategy that improves and complements existing approaches. The idea of using textual features to improve the entity vectors is not well explored, and most of the existing work focuses again on the representation of relations (Xie et al., 2016; Wang et al., 2014; Wang and Li, 2016) as opposed to entities. Closer to us is the work of Yamada et al. (2017) and Yang et al. (2015), with the latter to incorporate text features in the concept embeddings by exploiting matrix factorisation properties. Representing the meaning of words using a number of sense vectors is an old and well-established idea in NLP—see for example (Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014). However, most of the relevant research is evaluated on intrinsic tasks 1960 MSE Sentence space an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model (Mikolov et al., 2013). For a random walk of nodes n1 n2 ."
D18-1250,S17-2097,0,0.412028,"2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the test set but never appeared in the training data. relations and naturally good at modeling long distance relations within sequential language data. Approaches include Mehryary et al. (2016) with the original RNN and Li et al. (2017); Ammar et al. (2017); Zhou et al. (2018) with RNNs having LSTM units which are used to extend the range of context. Apart from sentences themselves, RNNbased models often take as input information extracted from dependency trees, such as shortest dependency paths (SDP) (Mehryary et al., 2016; Ammar et al., 2017), or even whole trees (Li et al., 2017). Since RNNs and CNNs each have their own distinct advantages, a few models have combined both in a single neural architecture (Cai et al., 2016; Zhang et al., 2018). 3 3.1 Materials and Methods Gold Standard Corpora As noted above, our experiments used six wellknown"
D18-1250,A00-1011,0,0.210112,"presented a deep neural network model, in which each component is capable of taking advantage of a particular type of major linguistic or architectural feature. The model is robust and adaptable across different relation types in various domains without any architectural changes. 2. We investigated the impact of different components and features on the final performance, therefore, providing insights on which model components and features are useful for future research. 2 Related Works We focus here on supervised approaches to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural"
D18-1250,S17-2091,0,0.208025,"rformance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the test set but never appeared in the training data. relations"
D18-1250,S17-2168,0,0.0353796,"Missing"
D18-1250,Q17-1010,0,0.0254341,". These two characteristics indicate the importance of understanding the mechanisms by which neural networks can generalize, i.e. make accurate predictions on novel instances. 3.2 Model Architecture Our ‘Man for All SeasonS’ (MASS) model comprises an embeddings layer, multi-channel bidirectional Long Short-Term Memory (BLSTM) 2268 and, Ddir is the orientation of the dependency vector i.e. from left-to-right or vice versa in the order of the SDP. Both are initialized randomly. For word representation, we take advantage of four types of information, including: • FastText pre-trained embeddings (Bojanowski et al., 2017) are the 300dimensional vectors that represent words as the sum of the skip-gram vector and character n-gram vectors to incorporate sub-word information. • WordNet embeddings are in the form of onehot vectors that determine which sets in the 45 standard WordNet super-senses the tokens belong to. • Character embeddings are denoted by C, containing 76 entries for 26 letters in uppercase and lowercase forms, punctuation, and numbers. Each character cj ∈ C is randomly initialized. They will be used to generate the token’s character-based embeddings. Figure 2: The architecture of MASS model for rel"
D18-1250,H05-1091,0,0.273299,"hes to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, suc"
D18-1250,P16-1072,0,0.430697,"as the possibility of having selection bias raise a question about the true capability of state-of-the-art methods for relation classification. In addition, despite such a wealth of studies, it still remains unclear which approach is superior and which factors set the limits on performance. For example, heuristic post-processing rules have been seen to significantly boost relation classification performance on several benchmarks; yet, they cannot be relied upon to generalize across domains. The novel approach we present in this paper draws inspiration from neural hybrid models such as that of Cai et al. (2016). In this work, we present a large-scale analysis of state-of-theart neural network architectures on six benchmark datasets which represent a variety of language domains and semantic types. As a means of comparison against reported system performance, we propose a novel multi-channel long short term memory (Hochreiter and Schmidhuber, 1997, LSTM) model combined with a Convolutional Neural Network (Kim, 2014, CNN) that takes advantage of all major linguistic and architectural features cur2266 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2266–2277"
D18-1250,S13-2057,0,0.203226,"Missing"
D18-1250,P17-2057,0,0.0176958,"ation types: sentence (i) shows a Synonym-of relation, represented by an abbreviation pattern, which is very different from the predicate relation Cause-effect in (ii). Introduction Determining the semantic relation between pairs of named entity mentions, i.e. relation classification, is useful in many fact extraction applications, ranging from identifying adverse drug reactions (Gurulingappa et al., 2012; Dandala et al., 2017), extracting drug abuse events (Jenhani et al., 2016), improving the access to scientific literature (G´abor et al., 2018), question answering (Lukovnikov et al., 2017; Das et al., 2017) to major life events extraction (Li et al., 2014; Cavalin et al., 2016). With a multitude of possible relation types, it is critical to understand how systems will behave in a variety of settings (see Table 1 for an example). † ∗ &lt;e1>Three-dimensional digital subtraction angiographic&lt;/e1> (&lt;e2>3D-DSA&lt;/e2>) images from diagnostic cerebral angiography were obtained ... Contributed equally & Names are in alphabetical order Corresponding author To the best of our knowledge, almost all relation classification models introduced so far have been experimentally validated on only a few datasets - ofte"
D18-1250,W16-3002,0,0.145651,"Missing"
D18-1250,S18-1111,0,0.0632716,"Missing"
D18-1250,W09-2415,0,0.261995,"Missing"
D18-1250,D15-1176,0,0.0351817,"edding, POS tag embedding and the character embedding. These four types of word-related information are fed into eight separate LSTMs, independently from each other during recurrent propagation. words. These four types of word-related information are fed into eight separate LSTMs (four for each direction) independently from each other during recurrent propagation. These four BLSTM channels are illustrated in Figure 3. The morphological surface information is represented with character-based embedding using a BLSTM, in which the forward and backward LSTM hidden states are jointly concatenated (Ling et al., 2015; Dang et al., 2018). For other layers, the LSTM hidden states are concatenated separately as the forward and the backward vector to form two final embeddings for each token as follows: f wWi = f wF Ti ⊕ f wW Ni ⊕ Chari ⊕ f wP OSi bwWi = bwF Ti ⊕ bwW Ni ⊕ Chari ⊕ bwP OSi 3.2.3 CNN with dependency unit Similar to Cai et al. (2016), the Convolutional Neural Networks (CNNs) in our model utilize Dependency Units (DU) to model the SDP. DU has the form of [wi − dii+1 − wi+1 ], in which wi , wi+1 are two adjacent tokens and dii+1 is the dependency between them. As a result, the low-dimensional forwar"
D18-1250,W16-3009,0,0.267568,"Missing"
D18-1250,D14-1181,0,0.0336199,"ral benchmarks; yet, they cannot be relied upon to generalize across domains. The novel approach we present in this paper draws inspiration from neural hybrid models such as that of Cai et al. (2016). In this work, we present a large-scale analysis of state-of-theart neural network architectures on six benchmark datasets which represent a variety of language domains and semantic types. As a means of comparison against reported system performance, we propose a novel multi-channel long short term memory (Hochreiter and Schmidhuber, 1997, LSTM) model combined with a Convolutional Neural Network (Kim, 2014, CNN) that takes advantage of all major linguistic and architectural features cur2266 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2266–2277 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics rently employed. We designate this as a ‘Man for All SeasonS’ (MASS) model because it incorporates many popular elements reported by state of the art systems on individual datasets. The main contributions of the paper are: 1. We presented a deep neural network model, in which each component is capable of taki"
D18-1250,S17-2171,0,0.224581,"se of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Thr"
D18-1250,W16-3005,0,0.0391341,"Missing"
D18-1250,P09-1113,0,0.139683,"e of major linguistic or architectural feature. The model is robust and adaptable across different relation types in various domains without any architectural changes. 2. We investigated the impact of different components and features on the final performance, therefore, providing insights on which model components and features are useful for future research. 2 Related Works We focus here on supervised approaches to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were amon"
D18-1250,W15-1506,0,0.0358419,"ised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are ano"
D18-1250,D14-1214,0,0.0128033,"on, represented by an abbreviation pattern, which is very different from the predicate relation Cause-effect in (ii). Introduction Determining the semantic relation between pairs of named entity mentions, i.e. relation classification, is useful in many fact extraction applications, ranging from identifying adverse drug reactions (Gurulingappa et al., 2012; Dandala et al., 2017), extracting drug abuse events (Jenhani et al., 2016), improving the access to scientific literature (G´abor et al., 2018), question answering (Lukovnikov et al., 2017; Das et al., 2017) to major life events extraction (Li et al., 2014; Cavalin et al., 2016). With a multitude of possible relation types, it is critical to understand how systems will behave in a variety of settings (see Table 1 for an example). † ∗ &lt;e1>Three-dimensional digital subtraction angiographic&lt;/e1> (&lt;e2>3D-DSA&lt;/e2>) images from diagnostic cerebral angiography were obtained ... Contributed equally & Names are in alphabetical order Corresponding author To the best of our knowledge, almost all relation classification models introduced so far have been experimentally validated on only a few datasets - often only one. This is despite the availability of e"
D18-1250,S10-1057,0,0.19607,"Missing"
D18-1250,C16-1238,0,0.15285,"and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the"
D18-1250,N18-1080,0,0.235816,"p learning have stimulated interest in applying neural architectures to the task. Convolutional Neural Networks (CNNs) (Nguyen and Grishman, 2015) were among early approaches to be applied. Following in this direction, (Lee et al., 2017) achieved state of the art performance on the ScienceIE task of SemEval 2017. Other recent variations of CNN architectures include a CNN with an attention mechanism in Shen and Huang (2016) and a CNN combined with maximum entropy in Gu et al. (2017). Various auxiliary information has been reported to improve the performance of CNNs, such as the document graph (Verga et al., 2018) and position embeddings (Shen and Huang, 2016; Lee et al., 2017; Verga et al., 2018). Recurrent Neural Networks (RNNs) are another approach to capturing Figure 1: The statistics of corpora used in our experiments. Three aspects are considered: the distribution of relation types, the distribution of Out-Of-Vocabulary (OOV) in the test set and the distribution of new entity pairs (NP) that appeared in the test set but never appeared in the training data. relations and naturally good at modeling long distance relations within sequential language data. Approaches include Mehryary et al. (2016) wi"
D18-1250,D15-1062,0,0.0709902,"eddings layer is followed by multi-channel bi-directional LSTM layers, two parallel CNNs and three softmax classifiers. The model’s input makes use of words and dependencies along the SDP going from the first entity to the second one using both forwards and backwards sequences. layers, two parallel Convolutional Neural Network (CNN) layers and three sof tmax classifiers. The MASS model’s architecture is depicted in Figure 2. MASS makes use of words and dependencies along the SDP going from the first entity to the second one using both forwards and backwards sequences. As is standard practice (Xu et al., 2015; Cai et al., 2016; Mehryary et al., 2016; Panyam et al., 2018) an entity pair is classified as having a relation if and only if the SDP between them is classified as having that relation. 3.2.1 Embeddings layer Despite the presence of inter-sentential relations in the six corpora we make the simplifying assumption that relations occur only between entities (or nominals) in the same sentence. We model each such sentence using a dependency path. In order to classify novel dependency paths we represent a dependency relation di as a vector Di that is the concatenation of two vectors as follow: Di"
D18-1250,P09-1115,0,0.305376,"nent is capable of taking advantage of a particular type of major linguistic or architectural feature. The model is robust and adaptable across different relation types in various domains without any architectural changes. 2. We investigated the impact of different components and features on the final performance, therefore, providing insights on which model components and features are useful for future research. 2 Related Works We focus here on supervised approaches to relation classification. Alternatives include hand built patterns (Aone and Ramos-Santacruz, 2000), unsupervised approaches (Yan et al., 2009) and distantly supervised approaches (Mintz et al., 2009). Traditional supervised and kernel-based approaches have made use of a full range of linguistic features (Miwa et al., 2010) such as orthography, character n-grams, chunking as well as vertex and edge walks over the dependency graph. Hand crafting and modeling with such complex feature sets remains a challenge although performance tends to increase with the amount of syntactic information (Bunescu and Mooney, 2005). Recent successes in deep learning have stimulated interest in applying neural architectures to the task. Convolutional Neu"
D19-5612,P18-1082,0,0.0300316,"(Yelp: 8, Yahoo: 5), which illustrates that the overlap between q (z) and p(z) shrinks further as C grows. 3.3 Text Generation To empirically examine how channel capacity translates into generative capacity of the model, we experimented with the C -VAELSTM models from Table 1. To generate a novel sentence, after a model was trained, a latent variable z is sampled from the prior distribution and then transformed into a sequence of words by the decoder p(x|z). During decoding for generation we try three decoding schemes: (i) Greedy: which selects the most probable word at each step, (ii) Top-k (Fan et al., 2018): which at each step samples from the K most probable words, and (iii) Nucleus Sampling (NS) (Holtzman et al., 2019): which at each step samples from a flexible subset of most probable words chosen based on their cumulative mass (set by a threshold p, where p = 1 means sampling from the full distribution). While similar to Topk, the benefit of NS scheme is that the vocabulary size at each time step of decoding varies, a property that encourages diversity and avoids degenerate text patterns of greedy or beam search decoding (Holtzman et al., 2019). We experiment with NS (p = {0.5, 0.9}) and Top"
D19-5612,D18-1151,0,0.139365,"el capacities. Finally, we run some experiments to find if any form of syntactic information is encoded in the latent space. For all experiments, we use the objective function of eqn. 2 with = 1. We do not use larger s because the constraint KL = C is always satisfied. 3 Corpora We use 5 different corpora covering different domains and size through this section: Yelp and Yahoo Yang et al. (2017) both have (100k,10k,10k) sentences in (train, dev, test) sets and 20k words in vocabulary, Children’s Book Test (CBT; Weston et al. (2016)) has (192k,10k,12k) sentences and 12k vocab, Wikipedia (WIKI; Marvin and Linzen (2018)) has (2m,270k,270k) sentences and 20k vocab, and WebText (Radford et al., 2019) has (1m,23k,24k) sentences and 22k vocab. 4 Models We examine three VAE architectures, covering a range of decoding strengths to examine if the objective function in eqn. 2 is immune to posterior collapse regardless of the choice of 3 can be seen as a Lagrange multiplier and any value that allows for constraint satisfaction (R = C) is fine. 4 Corpora and preprocessing scripts will be released. 5 We attribute the difference in performance across our models to the non-optimal selection of training hyperparameters, a"
D19-5612,D17-1066,0,0.0307558,"h powerful autoregressive decoders, such as LSTMs, the internal decoder’s cells are likely to suffice for representing the sentence, leading to a sub-optimal solution where the decoder ignores the inferred latent code z. This allows the encoder to become independent of x, an issue known as posterior collapse (q (z|x) ⇡ p(z)) where the inference network produces uninformative latent variables. Several solutions have been proposed to address the posterior collapse issue: (i) Modifying the architecture of the model by weakening decoders (Bowman et al., 2016; Miao et al., 2015; Yang et al., 2017; Semeniuta et al., 2017), or introducing additional connections between the encoder and decoder to enforce the dependence between x and z (Zhao et al., 2017; Goyal et al., 2017; Dieng et al., 2018); (ii) Using more flexible or multimodal priors (Tomczak and Welling, 2017; Xu and Durrett, 2018); (iii) Alternating the training by focusing on the inference network in the earlier stages (He et al., 2019), or augmenting amortized optimization of VAEs with instancebased optimization of stochastic variational inference (Kim et al., 2018; Marino et al., 2018). All of the aforementioned approaches impose one or more of the fo"
D19-5612,D18-1480,0,0.0189516,"an issue known as posterior collapse (q (z|x) ⇡ p(z)) where the inference network produces uninformative latent variables. Several solutions have been proposed to address the posterior collapse issue: (i) Modifying the architecture of the model by weakening decoders (Bowman et al., 2016; Miao et al., 2015; Yang et al., 2017; Semeniuta et al., 2017), or introducing additional connections between the encoder and decoder to enforce the dependence between x and z (Zhao et al., 2017; Goyal et al., 2017; Dieng et al., 2018); (ii) Using more flexible or multimodal priors (Tomczak and Welling, 2017; Xu and Durrett, 2018); (iii) Alternating the training by focusing on the inference network in the earlier stages (He et al., 2019), or augmenting amortized optimization of VAEs with instancebased optimization of stochastic variational inference (Kim et al., 2018; Marino et al., 2018). All of the aforementioned approaches impose one or more of the following limitations: restraining the choice of decoder, modifying the training algorithm, or requiring a substantial alternation of the objective function. As exceptions to these, -VAE (Razavi et al., 2019) and -VAE (Higgins et al., 2017) aim to avoid the posterior coll"
E17-2062,N06-2001,0,0.0431668,"e of the comparison work, which generally focus on morphologically complex words, can induce representations for such terms. This advantage enables us to train embeddings in general domain, for which text are available abundantly, and specialise them to specific domains for which large amounts of training data might not be available. We also note that our system did not provide full coverage of the words in the two datasets, missing several words which Related Work Recent research on representation induction for rare words has mainly focused on the case of infrequent morphological variations (Alexandrescu and Kirchhoff, 2006) and has tried to address the problem by resorting to information available for subword units. A morphological analyzer, such as Morfessor (Creutz and Lagus, 2007), is usually used in a pre-processing step to break inflected words into their morphological structures. Representations are then induced for morphologically complex words from their morphemes either by combining recursive neural networks (Luong et al., 2013) or using log-bilinear language models (Botha and Blunsom, 2014). Lazaridou et al. (2013) induced embeddings for complex words by adapting phrase composition models, whereas Sori"
E17-2062,P16-1156,0,0.0370722,"Missing"
E17-2062,N15-1184,0,0.0374917,"rds from their morphemes either by combining recursive neural networks (Luong et al., 2013) or using log-bilinear language models (Botha and Blunsom, 2014). Lazaridou et al. (2013) induced embeddings for complex words by adapting phrase composition models, whereas Soricut and Och (2015) automatically constructed 4 391 http://www.ihtsdo.org/snomed-ct References a morphological graph by exploiting regularities within a word embedding space. In the latter case, the representations were inferred by analyzing morphological transformations in the graph. Also related to our work is the retrofitting (Faruqui et al., 2015) of pre-trained embeddings by exploiting semantic lexical resources. Despite being effective in improving the representations for seen words, the retrofitting approaches are generally unable to induce new embeddings to address the unseen words problem. Cotterell et al. (2016) designed an extension of the retrofitting procedure that uses morphological resources to generate vectors for forms not observed in the training data. A common strand in all these works is that they assume that the training corpus covers the morpheme or other morphological variations of an unseen word. As a result, they f"
E17-2062,N13-1092,0,0.135544,"Missing"
E17-2062,N15-1186,0,0.152514,"to word representation (Turney and Pantel, 2010) is highly reliant on the availability of large amounts of training data and falls short of effectively modeling rare words that appear only a handful of times in the training corpus. Several efforts have been made to address this deficiency by expanding the coverage through inducing representations for rare words. Recent work has mainly focused on morphologically complex rare words has often tried to alleviate the problem by spreading the available knowledge across words that share the same morpheme (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). However, these techniques are unable to induce representations for words whose morphemes are not seen during training, such as infrequent domain specific terms. Importantly, the coverage issue is more evident when representations trained 2 Embeddings for Rare Words The objective is to expand the vocabulary of a given set of pre-trained word embeddings W by adding rare words.1 To achieve this goal, we leverage a lexical resource S that provides a better coverage of rare words or belongs to a specific domain and hence can be used to specialise W to that target domain. Our approach has two phas"
E17-2062,W13-3512,0,0.830758,"words. The prominent distributional approach to word representation (Turney and Pantel, 2010) is highly reliant on the availability of large amounts of training data and falls short of effectively modeling rare words that appear only a handful of times in the training corpus. Several efforts have been made to address this deficiency by expanding the coverage through inducing representations for rare words. Recent work has mainly focused on morphologically complex rare words has often tried to alleviate the problem by spreading the available knowledge across words that share the same morpheme (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). However, these techniques are unable to induce representations for words whose morphemes are not seen during training, such as infrequent domain specific terms. Importantly, the coverage issue is more evident when representations trained 2 Embeddings for Rare Words The objective is to expand the vocabulary of a given set of pre-trained word embeddings W by adding rare words.1 To achieve this goal, we leverage a lexical resource S that provides a better coverage of rare words or belongs to a specific domain and hence can be used to specialise W"
E17-2062,D14-1162,0,0.0834882,"extracted from a lexical resource in that domain. Parameter θ adjusts the contribution of initial embedding. Setting the parameter to zero reduces the formulation to that of inducing an embedding for an unseen word. In the next section, we discuss how the parameters were set in our experiments. 3 Experiments As evaluation framework, we used word similarity. To verify the ability of the approach in inducing embeddings in both general and specific domains, we carried out two different experiments. Embeddings. We used three different pretrained word embeddings: (1) G LOV E embeddings trained by Pennington et al. (2014) on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), (2) W 2 V- GN, Word2vec (Mikolov et al., 2013) trained on the Google News dataset (vocab: 3M, dim: 300), and (3) W 2 V-250 K, the same Word2vec embeddings with a vocabulary of 250K most frequent words. We opted for these embeddings mainly for their popularity but we note that the proposed approach is equally applicable to any other vector representation. Embedding induction Parameters. In experiments, whenever we had access to frequency statistics in the training data, we considered words with frequency < 10K as rare and induced their repre"
E17-2062,D16-1174,1,0.888426,"Missing"
E17-2062,W16-2902,1,0.889807,"Missing"
E17-2062,W09-3204,0,0.083506,"Missing"
E17-2062,P13-1149,0,\N,Missing
J14-4005,N09-1003,0,0.040159,"Missing"
J14-4005,agirre-de-lacalle-2004-publicly,0,0.0376862,"Missing"
J14-4005,W04-3204,0,0.0823896,"Missing"
J14-4005,E09-1005,0,0.123671,"Our method can be considered an extension of the vicinity-based approach as it replaces its pseudosense selection technique with a graph-based similarity measure. 845 Computational Linguistics Volume 40, Number 4 This expands the search space for finding pseudosenses from a small set of surrounding synsets to virtually all synsets in WordNet. In order to measure semantic similarity we used the PPR (Haveliwala 2002) algorithm, a graph-based technique that has been used previously as a core component for semantic similarity (Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be used to estimate a probability distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behind our selecting a graph-based similarity measure was that the alternative contextbased methods, such as Lin’s (1998) measure, have been shown to require a widecoverage sense-tagged data set in order to"
J14-4005,P01-1005,0,0.140669,"Missing"
J14-4005,D08-1007,0,0.0258306,"Missing"
J14-4005,E06-1018,0,0.0430439,"kind semantically aware pseudowords, in that they aim at listing senses that are in specific relations to each other, thus mirroring the relations existing between the senses of real words in the lexicon. For example, the lack-insufficiency relation is encoded in the pseudoword for deficiency, which would not be possible if we generated a random pseudoword. Semantically aware pseudowords enable the generation of artificially annotated data sets that have similar properties to their real counterparts and this makes them particularly suitable for the evaluation of WSD and Induction algorithms (Bordag 2006; Jurgens and Stevens 2011; Di Marco and Navigli 2013). In fact, in a real sense-annotated data set different senses of a word appear in distinct contexts. The extent of this distinction, however, depends on the semantic relatedness of the corresponding senses. The intuition behind semantically aware pseudowords is that they model each sense of an ambiguous word through a semantically similar monosemous representative that should appear naturally in contexts that are similar to those of its corresponding real sense. For this reason, these pseudowords should be expected to result in data sets w"
J14-4005,P10-1046,0,0.0468098,"Missing"
J14-4005,S07-1054,0,0.107006,"Missing"
J14-4005,W06-1663,0,0.0915187,"ith polysemy to a maximum of 35.9 at polysemy 10 (the absolute difference is on average 28.2 in this configuration). This divergence in the polysemy-wise performance of our two systems in the Nat-Nat configuration shows that IMS, in addition to being particularly good at this configuration, is able to further extend its lead over UKB at higher polysemy degrees. B.2 Performance by Pseudosense Node Degree As discussed in Section 6.4, UKB adopts the PPR algorithm, a variant of eigenvector centrality, whose behavior highly depends on the structure of the graph it is applied to. Previous research (Cuadros and Rigau 2006; Navigli 2008; Navigli and Lapata 2010) has shown that a denser graph with a large number of semantic relations benefits the eigenvector centrality-based approaches, enabling them to provide more accurate disambiguation judgments. These evaluations, however, were carried out on the WordNet graph leveraged for the disambiguation of instances from the SemCor data set. In this section, we perform a similar analysis but on a much larger scale, that is, in a setting with hundreds of thousands of disambiguation instances and using a much denser graph. Essentially, the graphs used in our experiments"
J14-4005,S07-1015,0,0.221659,"Missing"
J14-4005,C08-1021,0,0.240614,"Cor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are ma"
J14-4005,E03-1071,0,0.0172557,"training data. Finally, in Section 6.6 we describe the evaluation measures used in our experiments. 6.1 Corpus We sampled all the sentences for pseudosense tagging from the English Gigaword corpus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The corpus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sentences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&C tagger (Curran and Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection As a result of our similarity-based approach, we could generate as many pseudowords as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two reasons that will be explained shortly, we only considered a reliable subset of these pseudowords for generating the data sets for our experiments. Firstly, we did not consider nouns with polysemy degree higher than 12 in our experiments, as it is not possible to perform a reliable analysis on such"
J14-4005,J13-3008,1,0.731833,"Missing"
J14-4005,P07-1028,0,0.0249181,"Missing"
J14-4005,J10-4007,0,0.027172,"Missing"
J14-4005,W00-1322,0,0.345091,"Missing"
J14-4005,D12-1129,1,0.78837,"hich the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest different distribution. To verify this, we studied the variability of sense distribution across texts belonging to different domains. We started from a data set of sense-annotated documents from 30 different domains provided by Faralli and Navigli (2012). We then estimated the average sense distribution of all nouns across documents, shown in Figure 3 (light columns) for polysemy degrees 2 to 12 as sorted according to WordNet sense order. As can be seen, the average sense distribution across domains is not skewed, in contrast to the natural sense distribution (dark columns in the figure). In addition, this configuration models a setting in which the system is not effectively provided with knowledge of all senses in the test set. Uni-Uni. This configuration assumes a system with the same amount of knowledge for all senses, tested on a task in"
J14-4005,D07-1061,0,0.0182407,"Missing"
J14-4005,W11-2214,0,0.139974,"flating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale, ¨ Church, and Yarowsky (1992a) and Schutze (1992) as a means of generating large amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unam¨ biguous words picked out to be in the same range of occurrence frequency (Schutze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compared with the human-generated pseudowords (Gaustad 2001), thus leading to an op"
J14-4005,P10-1155,0,0.0143506,"a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zhong and Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models,2 this approach can only provide training examples for about one third of ambiguous nouns in WordNet, more than half of which have only one of their senses covered. Middle-ground approaches have also been proposed that either mix arbitrary sensetagged corpora with a small amount of tagged data for the domain of interest (Khapra et al. 2010), or estimate the sense distribution of the new domain data set with the help of parallel corpora (Chan and Ng 2005b, 2007), thus relieving the knowledge acquisition bottleneck. However, domain adaptation approaches typically suffer from lower disambiguation performance and still require annotated data for the domain of interest. 2.2 Knowledge-Based WSD and the Knowledge Acquisition Bottleneck Knowledge-based WSD systems are equally affected by the knowledge acquisition bottleneck, as they exploit the knowledge and structure of lexical knowledge bases in carrying out the disambiguation task. T"
J14-4005,kilgarriff-rosenzweig-2000-english,0,0.0172803,"our experiments to cover a wide range of possible real-world scenarios. In Section 5, we identified two different sense distributions according to which we could produce pseudosense-tagged corpora, namely, the uniform distribution and the natural one. In our experiments, we considered all four possible ways of combining the sense distributions of training–test data—that is, Natural-Natural (Nat-Nat), Uniform-Uniform (Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the following rationale for each of them: Nat-Nat. This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig 2000; McCarthy et al. 2004), in which senses are naturally distributed according to the same distribution both in the training and the test data sets. Nat-Uni. This configuration trains a WSD system with a natural distribution but applies it to texts for which the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be th"
J14-4005,J98-1006,0,0.148303,"Missing"
J14-4005,W02-1006,0,0.034262,"representatives for the two mainstream WSD paradigms, that is, supervised and knowledge-based WSD. 6.4.1 Supervised: It Makes Sense (IMS). In our experiments we used IMS (Zhong and Ng 2010) as the representative supervised WSD system. IMS is a publicly available English all-words WSD system achieving state-of-the-art results on several Senseval and SemEval tasks.11 The system classifies words in context using linear support vector machines. The context (a sentence in our case) is represented as a standard vector of features including parts of speech, surrounding words, and local collocations (Lee and Ng 2002). For each of the four configurations (see Section 6.3) and for each pseudoword, IMS was trained with the corresponding training set and the learned word expert model was then applied to the test set. In our experiments, we used the default configuration of IMS where the system adopts a linear SVM classifier with L2-loss function. 6.4.2 Knowledge-Based: UKB. As the state-of-the-art knowledge-based WSD system, we used UKB.12 UKB is a publicly available graph-based WSD system that exploits a preexisting lexical knowledge base (Agirre, Lopez de Lacalle, and Soroa 2014). UKB provides an implementa"
J14-4005,C00-1072,0,0.013541,"02 0.034 0.025 heroin drug hard cocaine user the list similarSynsets to select a pseudosense. However, the mode statistics in the table suggests that even when minFreq is set to a large value, most of the pseudosenses are picked out from the highest-ranking positions in the similarSynsets list. 3.3 Topic Signature-Based Pseudowords As an alternative means of finding suitable monosemous representatives for word senses with the PPR algorithm, we propose using automatically generated topic signatures. Topic signatures (TS) are weighted topical vectors that are associated with senses or concepts (Lin and Hovy 2000). The dimensions of these vectors are the words in the vocabulary and their weights determine the relatedness of each of these words to the target word sense. These vectors can be obtained automatically from large corpora or the Web with the help of monosemous relatives. In order to generate a TS-based pseudoword for a word w, we first sort the weighted vectors associated with the senses of w. Then, from each of these vectors, we select the monosemous word with highest relatedness (i.e., largest weight) which satisfies the minimum frequency constraint. The generation process of the TS-based ps"
J14-4005,W04-0804,0,0.0365061,"enses. In the following two sections we illustrate two corpus sampling strategies used in our experiments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemC"
J14-4005,P06-1058,0,0.147271,"ords in terms of disambiguation difficulty than random 3 http://www.nlm.nih.gov/mesh. 841 Computational Linguistics Volume 40, Number 4 pseudowords. However, this approach requires a specific hierarchical lexicon and falls short of creating many pseudowords with high polysemy. More recent work has focused on the identification of monosemous representatives in the surroundings of a sense, that is, selected among concepts directly related to the given sense. Senses of a real ambiguous word have been modeled by picking out the most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al. 2006). Pseudowords are then constructed by conflating these morphemes accordingly. However, this method leverages a specific Chinese hierarchical lexicon, in which different levels of the hierarchy correspond to different levels of sense granularity. A more flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous words in WordNet. For each particular sense, they search its surroundings in the WordNet graph in order to find an unambiguous representative for that sense. Unfortunately, as we discuss in detail in the next section, none of these proposals can enable a large-scale e"
J14-4005,H94-1020,0,0.205432,"he evaluation measures used in our experiments. 6.1 Corpus We sampled all the sentences for pseudosense tagging from the English Gigaword corpus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The corpus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sentences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&C tagger (Curran and Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection As a result of our similarity-based approach, we could generate as many pseudowords as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two reasons that will be explained shortly, we only considered a reliable subset of these pseudowords for generating the data sets for our experiments. Firstly, we did not consider nouns with polysemy degree higher than 12 in our experiments, as it is not possible to perform a reliable analysis on such degrees given that very few pseudowords can be ge"
J14-4005,Q13-1013,0,0.0142671,"ments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we show in Table 13, SemCor provi"
J14-4005,P04-1036,0,0.0101617,"range of possible real-world scenarios. In Section 5, we identified two different sense distributions according to which we could produce pseudosense-tagged corpora, namely, the uniform distribution and the natural one. In our experiments, we considered all four possible ways of combining the sense distributions of training–test data—that is, Natural-Natural (Nat-Nat), Uniform-Uniform (Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the following rationale for each of them: Nat-Nat. This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig 2000; McCarthy et al. 2004), in which senses are naturally distributed according to the same distribution both in the training and the test data sets. Nat-Uni. This configuration trains a WSD system with a natural distribution but applies it to texts for which the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest"
J14-4005,mihalcea-2002-bootstrapping,0,0.0561184,"f the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bootstrapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploit"
J14-4005,N07-1025,0,0.029214,"wledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a r"
J14-4005,W04-0807,0,0.0895033,"Missing"
J14-4005,H93-1061,0,0.69434,"ation for Computational Linguistics Computational Linguistics Volume 40, Number 4 difficulty of capturing knowledge in a computer-usable form (Buchanan and Wilkins 1993). Unfortunately, providing knowledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzett"
J14-4005,moro-etal-2014-annotating,1,0.879132,"Missing"
J14-4005,Q14-1019,1,0.377703,"onary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amount"
J14-4005,D12-1128,1,0.768948,"t efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system’s performance. In this article we address t"
J14-4005,S13-2035,1,0.902507,"Missing"
J14-4005,otrusina-smrz-2010-new,0,0.194066,"cused on the identification of monosemous representatives in the surroundings of a sense, that is, selected among concepts directly related to the given sense. Senses of a real ambiguous word have been modeled by picking out the most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al. 2006). Pseudowords are then constructed by conflating these morphemes accordingly. However, this method leverages a specific Chinese hierarchical lexicon, in which different levels of the hierarchy correspond to different levels of sense granularity. A more flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous words in WordNet. For each particular sense, they search its surroundings in the WordNet graph in order to find an unambiguous representative for that sense. Unfortunately, as we discuss in detail in the next section, none of these proposals can enable a large-scale evaluation framework for WSD, mainly because they suffer from coverage issues that prevent the creation of wide-coverage sense-annotated data sets. In this article we propose new pseudoword generation techniques that allow for the creation of thousands of artificial words having sufficient occurrence coverage"
J14-4005,P13-1132,1,0.4551,"Missing"
J14-4005,N13-1130,1,0.83194,"terms of disambiguation difficulty, representativeness, and distinguishability of the artificial senses. We leverage our semantically aware pseudowords to create, for the first time, a large-scale evaluation framework for WSD. Using this framework, we are able to perform an experimental comparison of state-of-the-art systems for supervised and knowledge-based WSD on a very large data set made up of millions of sense-tagged sentences. Our large-scale framework enables us to carry out an in-depth analysis of the factors and conditions that determine the systems’ performance. In our recent work (Pilehvar and Navigli 2013), we presented an approach for the generation of semantically aware pseudowords, called similarity-based pseudowords. At the core of this approach was the Personalized PageRank algorithm (Haveliwala 1 Although in our experiments we focus on nouns only, the same approach can potentially be used for any other open-class part of speech. 838 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD 2002) on the WordNet graph, which was utilized to find the most semantically similar monosemous representative for a given sense of a real ambiguous word. The main strength of the"
J14-4005,P10-1154,1,0.777793,". 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004;"
J14-4005,S07-1016,0,0.0431579,"Missing"
J14-4005,S13-1003,0,0.221754,"Missing"
J14-4005,D08-1027,0,0.120403,"Missing"
J14-4005,W04-0811,0,0.453984,". We then illustrate how we leverage our pseudowords to generate large sense-tagged data sets in Section 5. The experimental set-up for pseudoword-based WSD is described in Section 6. Experimental results as well as the findings are presented and discussed in Section 7. Finally, we provide concluding remarks in Section 8. 2. Related Work 2.1 Supervised WSD and the Knowledge Acquisition Bottleneck Over the last few decades, WSD systems have been suffering from disappointingly low performance, especially in an all-words setting in which one has to cover the entire lexicon of the given language (Snyder and Palmer 2004; Pradhan et al. 2007a). In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bo"
J14-4005,P14-1122,1,0.706979,"s in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with differ"
J14-4005,W13-0215,0,0.0273342,"Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2"
J14-4005,H05-1069,0,0.0328538,"eacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zho"
J14-4005,H93-1052,0,0.20931,"Missing"
J14-4005,P95-1026,0,0.298171,"In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bootstrapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have al"
J14-4005,P10-4014,0,0.844732,"dably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the sys"
J14-4005,N03-2023,0,\N,Missing
J14-4005,P07-1007,0,\N,Missing
J14-4005,J14-1003,0,\N,Missing
J14-4005,P10-2013,0,\N,Missing
N13-1130,E09-1005,0,0.0380075,"algorithm. PPR basically computes the probability according to which a random walker at a specific node in a graph would visit an arbitrary node in the same graph. The algorithm estimates, for a specific node in a graph, a probability distribution (called PPR vector) which determines the importance of any given node in the graph for that specific node. When applied to a semantic graph, this importance can be interpreted as semantic similarity. PPR has previously been used as a core component for semantic similarity1 (Hughes and Ramage, 2007; Agirre et al., 2009) and Word Sense Disambiguation (Agirre and Soroa, 2009). Algorithm 1 shows the procedure for the generation of our similarity-based pseudowords. The algorithm takes an ambiguous word w as input, and outputs its corresponding similarity-based pseudoword Pw whose ith pseudosense models the ith sense of w, together with a confidence score which we detail below. Given w, the algorithm iterates over the synsets corresponding to its individual senses (lines 4-13) and finds the most suitable pseudosenses for Pw . For 1 Top-ranking synsets will contain words which are most likely similar to the target sense, whereas we move to a graded notion of relatedne"
N13-1130,N09-1003,0,0.0783016,"Missing"
N13-1130,P01-1005,0,0.167094,"Missing"
N13-1130,D08-1007,0,0.0339981,"ed data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficu"
N13-1130,E06-1018,0,0.218099,"sks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the nu"
N13-1130,P10-1046,0,0.0568983,"the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is c"
N13-1130,J13-3008,1,0.6768,"Missing"
N13-1130,P07-1028,0,0.0603859,"nse annotated data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of"
N13-1130,J02-3001,0,0.00953271,"these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts. 1 Introduction A fundamental problem in computational linguistics is the paucity of manually annotated data, such as part-of-speech tagged sentences, treebanks, and logical forms, which exist only for few languages (Ide et al., 2010). A case in point is the lack of abundant sense annotated data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order"
N13-1130,D07-1061,0,0.156147,"larity measure we selected the Personalized PageRank (Haveliwala, 2002, PPR) algorithm. PPR basically computes the probability according to which a random walker at a specific node in a graph would visit an arbitrary node in the same graph. The algorithm estimates, for a specific node in a graph, a probability distribution (called PPR vector) which determines the importance of any given node in the graph for that specific node. When applied to a semantic graph, this importance can be interpreted as semantic similarity. PPR has previously been used as a core component for semantic similarity1 (Hughes and Ramage, 2007; Agirre et al., 2009) and Word Sense Disambiguation (Agirre and Soroa, 2009). Algorithm 1 shows the procedure for the generation of our similarity-based pseudowords. The algorithm takes an ambiguous word w as input, and outputs its corresponding similarity-based pseudoword Pw whose ith pseudosense models the ith sense of w, together with a confidence score which we detail below. Given w, the algorithm iterates over the synsets corresponding to its individual senses (lines 4-13) and finds the most suitable pseudosenses for Pw . For 1 Top-ranking synsets will contain words which are most likely"
N13-1130,P10-2013,0,0.0484543,"f pseudowords, i.e., artificial words which model real polysemous words. Our approach simultaneously addresses the two important issues that hamper the generation of large pseudosense-annotated datasets: semantic awareness and coverage. We evaluate these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts. 1 Introduction A fundamental problem in computational linguistics is the paucity of manually annotated data, such as part-of-speech tagged sentences, treebanks, and logical forms, which exist only for few languages (Ide et al., 2010). A case in point is the lack of abundant sense annotated data, which hampers the performance and coverage of lexical semantic tasks such as Word Sense Disambiguation (Navigli, 2009; Navigli, 2012, WSD) and semantic role labeling (Gildea and Jurafsky, 2002). A possible way to break this bottleneck is to use pseudowords, i.e., artificial words constructed by conflating a set of unambiguous words, with the aim of modeling polysemy in real ambiguous words. The idea of pseudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in o"
N13-1130,P06-1058,0,0.762166,"y proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the number of distinct pseudowords an algorithm can generate. When coupled with the semantic awareness issue, wide coverage is hampered by the difficulty in generating thousands of pseudowords which mimic existing polysemous words. Unfortunately, none of the existing approaches to the generation of pseudowords can meet both these challenges simultaneously, and this has hind"
N13-1130,W04-0807,0,0.21108,"Missing"
N13-1130,N03-2023,0,0.834,"seudowords was originally proposed by Gale et al. (1992) and Sch¨utze (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the number of distinct pseudowords an algorithm can generate. When coupled with the semantic awareness issue, wide coverage is hampered by the difficulty in generating thousands of pseudowords which mimic existing polysemous words. Unfortunately, none of the existing approaches to the generation of pseudowords can meet both these challenges simultaneously,"
N13-1130,otrusina-smrz-2010-new,0,0.483014,"e (1992) for WSD evaluation, but later found application in other tasks such as selectional preferences (Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010), Word Sense Induction (Bordag, 2006; Di Marco and Navigli, 2013) or studies Semantic awareness corresponds to the constraint that pseudowords, in order to be realistic, are expected to have senses which are in a semantic relationship (thus modeling systematic polysemy). Recent work has focused on this issue and, by exploiting either specific lexical hierarchies (Nakov and Hearst, 2003; Lu et al., 2006), or the WordNet structure (Otrusina and Smrz, 2010), have succeeded in generating pseudowords which are comparable to real words in terms of disambiguation difficulty. The second challenge is coverage, which corresponds to the number of distinct pseudowords an algorithm can generate. When coupled with the semantic awareness issue, wide coverage is hampered by the difficulty in generating thousands of pseudowords which mimic existing polysemous words. Unfortunately, none of the existing approaches to the generation of pseudowords can meet both these challenges simultaneously, and this has hindered the generation of a large pseudosense-annotated"
N13-1130,H93-1052,0,0.0609921,"ccurrence frequency in the corpus C. This minimum frequency corresponds to the number of annotated sentences that are requested for the task of interest which will exploit the resulting annotated corpus. An immediate way of generating a pseudoword would be to randomly select its constituents from the set of all monosemous words given by a lexicon (e.g., WordNet). However, constructing a pseudoword by merely combining a random set of unambiguous words selected on the basis of their falling in the same range of occurrence frequency (Sch¨utze, 1992), or leveraging homophones and OCR ambiguities (Yarowsky, 1993), does not provide a suitable model of a real polysemous word (Gaustad, 2001; Nakov and Hearst, 2003). This is because in the real world different senses, unless they are homonymous, share some semantic or pragmatic relation. Therefore, random pseudowords will typically model only homonymous distinctions (such as the centimeter vs. curium senses of cm), while they will fall short of modeling systematic polysemy (such as the lack vs. insufficiency senses of deficiency). 2.1 Semantically-aware Pseudowords In order to cope with the above-mentioned limits of random pseudowords, an artificial word"
N13-1130,P10-4014,0,0.0806447,"Missing"
N15-1059,agirre-de-lacalle-2004-publicly,0,0.185389,"Missing"
N15-1059,N09-1003,0,0.248006,"Missing"
N15-1059,P13-4021,0,0.0165032,"Missing"
N15-1059,P14-1023,0,0.056742,"W (Pilehvar et al., 2013) are WordNet-based approaches that leverage the structural information of WordNet for the computation of semantic similarity. Most similar to our work are Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007, ESA), which represents a word in a high-dimensional space of Wikipedia articles, and Salient Semantic Analysis (Hassan and Mihalcea, 2011, SSA), which leverages the linking of concepts within Wikipedia articles for generating semantic profiles of words. Word2Vec (Mikolov et al., 2013) and PMI-SVD are the best predictive and cooccurrence models obtained by Baroni et al. (2014) on a 2.8 billion-token corpus that also includes the English Wikipedia.4 Word2Vec is based on neural network context prediction models (Mikolov et al., 2013), whereas PMI-SVD is a traditional cooccurrence based vector wherein weights are calculated by means of Pointwise Mutual Information (PMI) and the vector’s dimension is reduced to 500 by singular value decomposition (SVD). We use the DKProSimilarity (B¨ar et al., 2013) implementation of Lin and ESA in order to evaluate these measures on the WS-Sim dataset. 4.1.3 Results Table 1 shows the Pearson correlation of the different similarity mea"
N15-1059,F14-1032,1,0.865084,"Missing"
N15-1059,J06-1003,0,0.530067,"ty reduction and an effective weighting scheme, our representation attains state-of-the-art performance on multiple datasets in two standard benchmarks: word similarity and sense clustering. We are releasing our vector representations at http://lcl.uniroma1.it/nasari/. 1 Introduction Obtaining accurate semantic representations of individual word senses or concepts is vital for several applications in Natural Language Processing (NLP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by"
N15-1059,E06-1002,0,0.170695,"Missing"
N15-1059,R13-1022,0,0.205557,"kolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation technique, called NASARI, which exploits the knowledge available in both types of"
N15-1059,D10-1113,0,0.017686,"LP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained fr"
N15-1059,D08-1094,0,0.101817,"Missing"
N15-1059,P05-1045,0,0.00430959,"having, on average, 2.6 words. 571 3.2.2 Concept extraction If the two input words w1 and w2 are not found in the same synonym set in S, we proceed by obtaining their sets of senses Cw1 and Cw2 , respectively. Depending on the type of wi , we use two different resources for obtaining Cwi : the WordNet sense inventory and Wikipedia. WordNet words. When the word wi is defined in the WordNet sense inventory and is not a named entity (line 6 in Algorithm 1), we set Cwi as all the WordNet synsets that contain wi , i.e., Cwi = {synset s ∈ WordNet : wi ∈ s}. We use Stanford Named Entity Recognizer (Finkel et al., 2005) in our experiments. WordNet OOV and named entities. For named entities and words that do not exist in WordNet’s vocabulary (OOV) we construct the set Cwi by exploiting Wikipedia’s piped links (line 10 in Algorithm 1). To this end, we take as elements of Cwi the Wikipedia pages of the hyperlinks which have wi as their surface form, i.e., piped-links (wi ). If |Cwi |&gt; 5, we prune Cwi to its top-5 pages in terms of their number of ingoing links. Our choice of Wikipedia as a source for named entities is due to its higher coverage in comparison to WordNet. 4 Experiments We evaluated NASARI on two"
N15-1059,E14-1044,1,0.624945,"ea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Pilehvar et al., 2013), or Wikipedia (Gabrilovich and Markovitch, 2007; Mihalcea, 2007). None of these techniques, however, combine knowledge from multiple types of resource, making their representations resourcespecific and also prone to sparsity. In contrast, our method is based on the complementary knowledge of two different resources and their interlinking, leading to richer semantic representations that are also applicable across resources. Most similar to our combination of complementary knowledge is the work of Franco-Salvador et al. (2014) for crosslingual document retrieval. 575 Concept similarity. Concept similarity techniques are mainly limited to the knowledge that their underlying lexical resources provide. For instance, methods designed for measuring semantic similarity of WordNet synsets (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) usually leverage lexicographic or structural information in this lexical resource. Similarly, Wikipedia-based approaches (Hassan and Mihalcea, 2011; Strube and Ponzetto, 2006; Milne and Witten, 2008) do not usually benefit from the expert-based lexico-semant"
N15-1059,J15-4004,0,0.102299,"Missing"
N15-1059,P12-1092,0,0.261884,"ept representation. Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their representation on the know"
N15-1059,N07-1025,0,0.142037,"et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation technique, called NASARI, which exploits the knowledge avail"
N15-1059,D14-1113,0,0.0504506,"Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their representation on the knowledge derived from resource"
N15-1059,J07-2002,0,0.141103,"Missing"
N15-1059,P14-1044,1,0.829179,"ple datasets in two standard benchmarks: word similarity and sense clustering. We are releasing our vector representations at http://lcl.uniroma1.it/nasari/. 1 Introduction Obtaining accurate semantic representations of individual word senses or concepts is vital for several applications in Natural Language Processing (NLP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, u"
N15-1059,P13-1132,1,0.923416,"y using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation techniqu"
N15-1059,P10-1040,0,0.341448,"esource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 20"
N15-1059,D13-1136,0,0.0100905,"ntic similarity. Concept representation. Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their repres"
N15-1169,baccianella-etal-2010-sentiwordnet,0,0.0145331,"onstruction procedure is accurate and has a significant impact on a WordNet-based algorithm encountering novel lemmas. 1 Introduction Semantic knowledge bases are an essential, enabling component of many NLP applications. A notable example is WordNet (Fellbaum, 1998), which encodes a taxonomy of concepts and semantic relations between them. As a result, WordNet has enabled a wide variety of NLP techniques such as Word Sense Disambiguation (Agirre et al., 2014), information retrieval (Varelas et al., 2005), semantic similarity (Pedersen et al., 2004; B¨ar et al., 2013), and sentiment analysis (Baccianella et al., 2010). However, semantic knowledge bases such as WordNet are expensive to produce; as a result, their scope and domain are often constrained by the resources available and may omit highly-specific concepts or lemmas, as well as new terminology that emerges after their construction. For example, WordNet does not contain the nouns “stepmom,” “broadband,” and “prequel.” Because of the coverage limitations of WordNet, several approaches have attempted to enrich WordNet with new relations and concepts. One group of approaches has enriched WordNet by aligning its structure with that of other resources su"
N15-1169,P13-4021,0,0.0369554,"Missing"
N15-1169,S12-1004,0,0.0169502,"in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to this work. Other related work has attempted to tap resources such as Wikipedia for automatically constructing new ontologies (Suchanek et al., 2007; Dandala et al., 2012; Moro and Navigli, 2012; Meyer and Gurevych, 2012), extending existing ones through either alignment-based methods (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014) or inferring the positions of new senses by their shared attributes which are extracted from text (Reisinger and Pas¸ca, 2009). Extension and alignment approaches based on Wikipedia are limited mainly to noun concepts in Wikipedia; furthermore, these techniques cannot be directly applied to Wiktionary because its lack of taxonomic structure would prevent adding most OOV data to the existing WordNet taxonomy. 6 Conclusion"
N15-1169,S14-2003,1,0.815024,"lt, the median error is likely an overestimate of the expected error for the C ROWN construction process. 4.2 Application-based evaluation Semantic similarity is one of the core features of many NLP applications. The second evaluation measures the performance improvement of using C ROWN instead of WordNet for measuring semantic similarity when faced with slang or OOV lemmas. Notably, prior semantic similarity benchmarks such as SimLex-999 (Hill et al., 2014) and the ESL test questions (Turney, 2001) have largely omitted these types of words. However, the recent dataset of SemEval-2014 Task 3 (Jurgens et al., 2014) includes similarity judgments between a WordNet sense and a word not defined in WordNet’s vocabulary or with a slang interpretation not present in WordNet. 1462 Table 3: The Pearson correlation performance of ADW when using the WordNet and C ROWN semantic networks on the word-to-sense test dataset of SemEval-2014 Task 3. We also show results for the string-based baseline system (GST) and for the best participating system in the word-to-sense comparison type of Task 3. 4.2.1 Methodology Semantic similarity was measured using the similarity algorithm of Pilehvar et al. (2013), ADW,3 which first"
N15-1169,S14-2072,0,0.0136046,"ge is due only to the differences between the two networks. Performance is measured using Pearson correlation with the gold standard judgments. 4.2.2 Results Of the 60 OOV lemmas and 38 OOV slang terms in the test data, 51 and 26 were contained in C ROWN, respectively. Table 3 shows the Pearson correlation performance of ADW in the two settings for all lemmas in the dataset, and for three subsets of the dataset: OOV, slang, and regular lemmas, the latter of which are in WordNet; the bottom rows show the performance of the Task’s best participating system for the word-to-sense comparison type (Kashyap et al., 2014) and the most competi3 https://github.com/pilehvar/ADW 4 http://wordnet.princeton.edu/glosstag.shtml tive baseline, based on Greedy String Tiling (GST) (Wise, 1996). ADW sees large performance improvements in the OOV and slang words when using C ROWN instead of WordNet, which are both statistically significant at p<0.01. The overall improvement of ADW would place it as the fifth best system in this comparison type of Task 3. The performance on regular in-WordNet and OOV lemmas is approximately equal, indicating the high accuracy of OOV hypernym attachment in C ROWN. Notably, on OOV and Slang,"
N15-1169,P14-5010,0,0.00416277,"ove Wiktionary markup. The extracted texts were then partitioned into two sets: (1) those expressing a lexicalization, e.g., “1337” is an alternative spelling of “elite” and (2) those indicating a definition. Novel lexicalizations that are not already handled by the WordNet morphological analyzer (Morphy) were added to the lexicalization exception lists in C ROWN. Definitions are processed using two methods to identify a set of candidate lemmas whose senses might be identical or near to the appropriate hypernym synset. First, candidates are obtained by parsing the gloss with Stanford CoreNLP (Manning et al., 2014) and extract1460 ing the head word and all other words joined to it by a conjunction. Second, additional candidates are collected from the first hyperlinked term or phrase in the gloss, which is similar to the approach of Navigli and Velardi (2010) for hypernym extraction in Wikipedia. Candidates are then filtered to ensure that (1) they have the same part of speech as the definition’s term and (2) they are defined in WordNet, which is necessary for the attachment. 3.2 Structural and Lexical Attachment Three types of structural or lexical heuristics were used to attach OOV lemmas when the appr"
N15-1169,Q13-1013,0,0.0786187,"rticle titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to this work. Other related work has attempted to tap resources such as Wikipedia for automatically constructing new ontologies (Suchanek et al., 2007; Dandala et al., 2012; Moro and Navigli, 2012; Meyer and Gurevych, 2012), extending existing ones through either alignment-based methods (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014) or inferring the positions of new senses by their shared attributes which are extracted from text (Reisinger and Pas¸ca, 2009). Extension and alignment approaches based on Wikipedia are limited mainly to noun concepts in Wikipedia; furthermore, these techniques cannot be directly applied to Wiktionary because its lack of taxonomic structure would prevent adding most OOV data to the existing WordNet taxonomy. 6 Conclusion This work has introduced C ROWN version 1.0, a new extension of WordNet that merges sense definitions from Wiktionary to add new hypernym and ant"
N15-1169,I11-1099,0,0.0344856,"ectly applied. 3 Extending WordNet C ROWN is created by identifying lemmas that are out of vocabulary (OOV) in WordNet but have one or more associated glosses in Wiktionary. A new synset is created for that lemma and a hypernym relation is added to the appropriate WordNet synset. The C ROWN attachment process rates hypernym candidates using two methods. First, where possible, we exploit structural or morphological information to identify highly-probable candidates. Second, following previous work on resource alignment showing that lexical overlap accurately measures gloss semantic similarity (Meyer and Gurevych, 2011; Navigli and Ponzetto, 2012), candidates are found by measuring the similarity of the Wiktionary gloss with the glosses of synsets found by a constrained search of the WordNet graph. We note that attaching OOV lemmas by first aligning WordNet and Wiktionary is not possible due to relation sparsity within Wiktionary, where most OOV words would not be connected to the aligned network. Following, we first describe the Wiktionary preprocessing steps and then detail both OOV attachment methods. 3.1 Preprocessing Wiktionary was parsed using JWKTL (Zesch et al., 2008) to extract the text associated"
N15-1169,miller-gurevych-2014-wordnet,0,0.302185,"Missing"
N15-1169,P10-1134,0,0.0363326,"already handled by the WordNet morphological analyzer (Morphy) were added to the lexicalization exception lists in C ROWN. Definitions are processed using two methods to identify a set of candidate lemmas whose senses might be identical or near to the appropriate hypernym synset. First, candidates are obtained by parsing the gloss with Stanford CoreNLP (Manning et al., 2014) and extract1460 ing the head word and all other words joined to it by a conjunction. Second, additional candidates are collected from the first hyperlinked term or phrase in the gloss, which is similar to the approach of Navigli and Velardi (2010) for hypernym extraction in Wikipedia. Candidates are then filtered to ensure that (1) they have the same part of speech as the definition’s term and (2) they are defined in WordNet, which is necessary for the attachment. 3.2 Structural and Lexical Attachment Three types of structural or lexical heuristics were used to attach OOV lemmas when the appropriate data was available. First, Wikisaurus or Wiktionary synonym relations create sets of mutually-synonymous lemmas, which may contain OOV lemmas. The common hypernym of these lemmas is estimated by computing the most frequent hypernym synset f"
N15-1169,P14-1044,1,0.77811,"in are often constrained by the resources available and may omit highly-specific concepts or lemmas, as well as new terminology that emerges after their construction. For example, WordNet does not contain the nouns “stepmom,” “broadband,” and “prequel.” Because of the coverage limitations of WordNet, several approaches have attempted to enrich WordNet with new relations and concepts. One group of approaches has enriched WordNet by aligning its structure with that of other resources such as Wikipedia or Wiktionary (RuizCasado et al., 2005; Navigli and Ponzetto, 2012; Miller and Gurevych, 2014; Pilehvar and Navigli, 2014). However, because these approaches identify corresponding lemmas with identical lexicalizations, they are often unable to directly add novel lemmas to the existing taxonomic structure. The second group of approaches performs taxonomy induction to learn hypernymy relationships between words (Moro and Navigli, 2012; Meyer and Gurevych, 2012). However, these approaches often produce separate taxonomies from WordNet, which are also generally not readily accessible as resources. We introduce a new resource C ROWN (CommunityenRiched Open WordNet) that extends the existing WordNet taxonomy, more tha"
N15-1169,P13-1132,1,0.78939,"emEval-2014 Task 3 (Jurgens et al., 2014) includes similarity judgments between a WordNet sense and a word not defined in WordNet’s vocabulary or with a slang interpretation not present in WordNet. 1462 Table 3: The Pearson correlation performance of ADW when using the WordNet and C ROWN semantic networks on the word-to-sense test dataset of SemEval-2014 Task 3. We also show results for the string-based baseline system (GST) and for the best participating system in the word-to-sense comparison type of Task 3. 4.2.1 Methodology Semantic similarity was measured using the similarity algorithm of Pilehvar et al. (2013), ADW,3 which first represents a given linguistic item (such as a word or a concept) using random walks over the WordNet semantic network, where random walks are initialized from the synsets associated with that item. The similarity between two linguistic items is accordingly computed in terms of the similarity of their corresponding representations. ADW is an ideal candidate for measuring the impact of C ROWN for two reasons. First, the algorithm obtains state-of-the-art performance on both wordbased and sense-based benchmarks using only WordNet as a knowledge source. Second, the method is bo"
N15-1169,W08-0507,0,0.0921622,"ly significant at p<0.01. The overall improvement of ADW would place it as the fifth best system in this comparison type of Task 3. The performance on regular in-WordNet and OOV lemmas is approximately equal, indicating the high accuracy of OOV hypernym attachment in C ROWN. Notably, on OOV and Slang, the unsupervised ADW, when coupled with the additional information in C ROWN , produces competitive results with the best performing system, which is a multi-feature supervised system utilizing extensive external dictionaries and distributional methods. 5 Related Work Most related is the work of Poprat et al. (2008), who attempted to automatically build an extension of WordNet with biomedical terminology; however, they were unsuccessful in constructing the resource. Other work has attempted to leverage distributional similarity techniques (Snow et al., 2006) or exploit the structured information in Wikipedia (Ruiz-Casado et al., 2005; Toral et al., 2008; Ponzetto and Navigli, 2009; Yamada et al., 2011) in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Dist"
N15-1169,P09-1070,0,0.194356,"Missing"
N15-1169,P06-1101,0,0.338597,"rnym attachment in C ROWN. Notably, on OOV and Slang, the unsupervised ADW, when coupled with the additional information in C ROWN , produces competitive results with the best performing system, which is a multi-feature supervised system utilizing extensive external dictionaries and distributional methods. 5 Related Work Most related is the work of Poprat et al. (2008), who attempted to automatically build an extension of WordNet with biomedical terminology; however, they were unsuccessful in constructing the resource. Other work has attempted to leverage distributional similarity techniques (Snow et al., 2006) or exploit the structured information in Wikipedia (Ruiz-Casado et al., 2005; Toral et al., 2008; Ponzetto and Navigli, 2009; Yamada et al., 2011) in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to t"
N15-1169,toral-etal-2008-named,0,0.380664,"additional information in C ROWN , produces competitive results with the best performing system, which is a multi-feature supervised system utilizing extensive external dictionaries and distributional methods. 5 Related Work Most related is the work of Poprat et al. (2008), who attempted to automatically build an extension of WordNet with biomedical terminology; however, they were unsuccessful in constructing the resource. Other work has attempted to leverage distributional similarity techniques (Snow et al., 2006) or exploit the structured information in Wikipedia (Ruiz-Casado et al., 2005; Toral et al., 2008; Ponzetto and Navigli, 2009; Yamada et al., 2011) in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to this work. Other related work has attempted to tap resources such as Wikipedia for automatically c"
N15-1169,I11-1098,0,0.518362,"Missing"
N15-1169,zesch-etal-2008-extracting,0,0.045225,"Missing"
N15-1169,J14-1003,0,\N,Missing
N15-3016,S12-1051,0,0.104282,"Missing"
N15-3016,P13-4021,0,0.0281113,"Missing"
N15-3016,P11-2087,0,0.0331341,"tion Semantic similarity quantifies the extent of shared semantics between two linguistics items, e.g., between deer and moose or cat and a feline mammal. Lying at the core of many Natural Language Processing systems, semantic similarity measurement plays an important role in their overall performance and effectiveness. Example applications of semantic similarity include Information Retrieval (Hliaoutakis et al., 2006), Word Sense Disambiguation (Patwardhan et al., 2003), paraphrase recogni76 tion (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009) or simplification (Biran et al., 2011), machine translation evaluation (Lavie and Denkowski, 2009), tweet search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-ba"
N15-3016,J06-1003,0,0.0525352,"r correlations on RG-65 and accuracy, i.e., the number of correctly identified synonyms, on TOEFL. We show results for two sets of vectors: full vectors of size 118K and truncated vectors of size 5000 which are provided as a part of the package. As can be seen, despite reducing the space requirement by more than 15 times, our compressed vectors obtain high performance on both the datasets, matching those of the full vectors on the TOEFL dataset and also the cosine measure. 79 Related Work As the de facto standard lexical database, WordNet has been used widely in measuring semantic similarity. Budanitsky and Hirst (2006) provide an overview of WordNet-based similarity measures. WordNet::Similarity, a software developed by Pedersen et al. (2004), provides a Perl implementation of a number of these WordNet-based measures. UMLS::Similarity is an adaptation of WordNet::Similarity to the Unified Medical Language System (UMLS) which can be used for measuring the similarity and relatedness of terms in the biomedical domain (McInnes et al., 2009). Most of these WordNet-based measures suffer from two major drawbacks: (1) they usually exploit only the subsumption relations in WordNet; and (2) they are limited to measur"
N15-3016,P10-4006,0,0.0241543,"nes et al., 2009). Most of these WordNet-based measures suffer from two major drawbacks: (1) they usually exploit only the subsumption relations in WordNet; and (2) they are limited to measuring the semantic similarity of pairs of synsets with the same part of speech. ADW improves both issues by obtaining rich and unified representations for individual synsets, enabling effective comparison of arbitrary word senses or concepts, irrespective of their part of speech. Distributional semantic similarity measures have also attracted a considerable amount of research attention. The S-Space Package (Jurgens and Stevens, 2010) is an evaluation benchmark and a development framework for word space algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997). The package is integrated in DKProSimilarity (B¨ar et al., 2013), a more recently developed package geared towards semantic similarity of textual items. DKProSimilarity provides an opensource implementation of several semantic similarity techniques, from simple string-based measures such as character n-gram overlap, to more sophisticated vector-based measures such as Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). ADW was shown to impro"
N15-3016,P11-1076,0,0.0290749,"a feline mammal. Lying at the core of many Natural Language Processing systems, semantic similarity measurement plays an important role in their overall performance and effectiveness. Example applications of semantic similarity include Information Retrieval (Hliaoutakis et al., 2006), Word Sense Disambiguation (Patwardhan et al., 2003), paraphrase recogni76 tion (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009) or simplification (Biran et al., 2011), machine translation evaluation (Lavie and Denkowski, 2009), tweet search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-based semantic similarity approach which was originally proposed by Pilehvar et al. (2013). The measure is based on the Personalized PageRan"
N15-3016,N04-3012,0,0.100431,"vectors: full vectors of size 118K and truncated vectors of size 5000 which are provided as a part of the package. As can be seen, despite reducing the space requirement by more than 15 times, our compressed vectors obtain high performance on both the datasets, matching those of the full vectors on the TOEFL dataset and also the cosine measure. 79 Related Work As the de facto standard lexical database, WordNet has been used widely in measuring semantic similarity. Budanitsky and Hirst (2006) provide an overview of WordNet-based similarity measures. WordNet::Similarity, a software developed by Pedersen et al. (2004), provides a Perl implementation of a number of these WordNet-based measures. UMLS::Similarity is an adaptation of WordNet::Similarity to the Unified Medical Language System (UMLS) which can be used for measuring the similarity and relatedness of terms in the biomedical domain (McInnes et al., 2009). Most of these WordNet-based measures suffer from two major drawbacks: (1) they usually exploit only the subsumption relations in WordNet; and (2) they are limited to measuring the semantic similarity of pairs of synsets with the same part of speech. ADW improves both issues by obtaining rich and u"
N15-3016,P14-1044,1,0.796084,"nguage Processing systems, semantic similarity measurement plays an important role in their overall performance and effectiveness. Example applications of semantic similarity include Information Retrieval (Hliaoutakis et al., 2006), Word Sense Disambiguation (Patwardhan et al., 2003), paraphrase recogni76 tion (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009) or simplification (Biran et al., 2011), machine translation evaluation (Lavie and Denkowski, 2009), tweet search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-based semantic similarity approach which was originally proposed by Pilehvar et al. (2013). The measure is based on the Personalized PageRank (PPR) algorithm (Haveliwala et al., 2002) applied on the Wo"
N15-3016,P13-1132,1,0.909651,"t search (Sriram et al., 2010), question answering (Mohler et al., 2011), and lexical resource alignment (Pilehvar and Navigli, 2014). Owing to its crucial importance a large body of research has been dedicated to semantic similarity. This has resulted in a diversity of similarity measures, ranging from corpus-based methods that leverage the statistics obtained from massive corpora, to knowledge-based techniques that exploit the knowledge encoded in various semantic networks. Align, Disambiguate, and Walk (ADW) is a knowledge-based semantic similarity approach which was originally proposed by Pilehvar et al. (2013). The measure is based on the Personalized PageRank (PPR) algorithm (Haveliwala et al., 2002) applied on the WordNet graph (Miller et al., 1990), and can be used to compute the similarity between arbitrary linguistic items, all the way from word senses to texts. Pilehvar et al. (2013) reported state-of-the-art performance on multiple evaluation benchmarks belonging to different lexical levels: senses, words, and sentences. In this demonstration we present an open-source implementation of our system together with a Java API and a Web interface for online measurement of semantic similarity. We a"
N18-6004,J14-1003,0,0.064392,"Missing"
N18-6004,P13-2095,0,0.0248108,"Missing"
N18-6004,E17-2036,1,0.850307,"lson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel"
N18-6004,D14-1110,0,0.0233225,"WordNet or DBpedia. We will explain some of the current challenges in Word Sense Disambiguation and Entity Linking, as key tasks in natural language understanding which also enable a direct integration of knowledge from lexical resources. We will explain some knowledge-based and supervised methods for these tasks which play a decisive role in connecting lexical resources and text data (Zhong and Ng, 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be"
N18-6004,D15-1084,0,0.0128731,"la and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resourc"
N18-6004,R15-1025,0,0.0176738,"and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and t"
N18-6004,D16-1041,1,0.860585,"designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as"
N18-6004,C16-1323,1,0.854218,"designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as"
N18-6004,D11-1142,0,0.0139666,"nitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignme"
N18-6004,E12-1059,0,0.0325611,"rdi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. E"
N18-6004,P16-1191,0,0.0154848,", 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be obtained in many downstream applications. At the same time, these resources can be further enriched and better exploited by making use of NLP techniques. In this context, the main motivation of this tutorial is to show how Natural Language Processing and Lexical Resources have interacted so far, and a view towards potential scenarios in the near future. The tutorial is then divided in two main blocks. First, w"
N18-6004,W03-1028,0,0.0995272,"lexical resources and NLP. Additionally, we will summarize existing attempts in this direction, such as modeling linguistic phenomena like terminology, definitions and glosses, examples and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approa"
N18-6004,N15-1169,1,0.850096,"ignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. Effectively leveraging linguistically expressible cues with their associated knowledge remains a difficult task. Knowledge may be extracted from (roughly) three types of resources (Hovy et al., 2013): unstructured, e.g. text corpora; semistructured, such as encyclopedic collaborative repositories like Wikipedia and Wiktionary, or structured, which include lexic"
N18-6004,S16-1169,1,0.907779,"Missing"
N18-6004,Q13-1013,0,0.0234379,"g general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. Effectively leveraging linguistically expressible cues with their associated knowledge remains a difficult task. Knowledge may be extracted from (roughly) three types of resources (Hovy et al., 2013): u"
N18-6004,P10-1134,0,0.0128014,"like terminology, definitions and glosses, examples and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify ("
N18-6004,D16-1174,1,0.853113,"Sense Disambiguation and Entity Linking, as key tasks in natural language understanding which also enable a direct integration of knowledge from lexical resources. We will explain some knowledge-based and supervised methods for these tasks which play a decisive role in connecting lexical resources and text data (Zhong and Ng, 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be obtained in many downstream applications. At the same time, these resources can be f"
N18-6004,P17-1170,1,0.839468,"Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be obtained in many downstream applications. At the same time, these resources can be further enriched and better exploited by making use of NLP techniques. In this context, the main motivation of this tutorial is to show how Natural Language Processing and Lexical Resources have interacted so far, and a view towards potential scenarios in the near future. The tutorial is then divided in two main blocks. First, we delve into NLP for C"
N18-6004,P14-1044,1,0.834755,"opic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. Effectively leveraging linguistically expressible cues with their associated knowledge remains a difficult task. Knowledge may be extracted from (roughly) three types of resources (Hovy et al., 2013): unstructured, e.g. text corpora;"
N18-6004,P15-1173,0,0.0571924,"Missing"
N18-6004,vivaldi-rodriguez-2010-finding,0,0.0115787,"ources and NLP. Additionally, we will summarize existing attempts in this direction, such as modeling linguistic phenomena like terminology, definitions and glosses, examples and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic r"
N18-6004,P10-4014,0,0.0279959,"s (Hovy et al., 2013): unstructured, e.g. text corpora; semistructured, such as encyclopedic collaborative repositories like Wikipedia and Wiktionary, or structured, which include lexicographic resources like WordNet or DBpedia. We will explain some of the current challenges in Word Sense Disambiguation and Entity Linking, as key tasks in natural language understanding which also enable a direct integration of knowledge from lexical resources. We will explain some knowledge-based and supervised methods for these tasks which play a decisive role in connecting lexical resources and text data (Zhong and Ng, 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekov"
N18-6004,D12-1104,0,\N,Missing
N18-6004,J04-2002,0,\N,Missing
N18-6004,S15-2151,0,\N,Missing
N18-6004,P15-1010,1,\N,Missing
N18-6004,N13-1092,0,\N,Missing
N18-6004,N15-1164,0,\N,Missing
N18-6004,S16-1168,0,\N,Missing
N18-6004,K16-1006,0,\N,Missing
N18-6004,P16-1085,1,\N,Missing
N18-6004,C16-1217,0,\N,Missing
N18-6004,E17-1010,1,\N,Missing
N18-6004,D17-1120,0,\N,Missing
N18-6004,P16-1226,0,\N,Missing
N19-1128,N19-1423,0,0.160101,"Missing"
N19-1128,D18-1200,0,0.0163851,"as a human-level performance upperbound, is 0.52. Moreover, most of the instances in SCWS have context pairs with different target words.14 This makes it possible to test context-independent models, which only considers word pairs in isolation, on the dataset. Importantly, such a context-independent model can easily surpass the human-level performance upperbound. For instance, we computed the performance of the Google News Word2vec pre-trained word embeddings (Mikolov et al., 2013b) on the dataset to be 0.65 (ρ), which is significantly higher than the optimistic IRA for the dataset. In fact, Dubossarsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling effect, which had not been controlled for in similarity datasets. In contrast, a context-insensitive word embedding model would perform no better than a random baseline on our dataset. Related work 5 The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015)"
N19-1128,P16-1191,0,0.0428877,"an word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In the next section we show that the pruning resulted in a significant boost in the clarity of the dataset. 2.2 Quality check To verify the quality and the difficulty of the dataset and to estimate the human-level performance upperbound, we randomly sampled four sets of 100 instances from the test set, with an overlap of 50 instances between two of the annotators. Each set was assigned to an annotator who was asked to label each instance based on whether they thought the two occurrences of the word referred to the same meaning or not.6 The annotators were not provided"
N19-1128,J15-4004,0,0.0755782,"rsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling effect, which had not been controlled for in similarity datasets. In contrast, a context-insensitive word embedding model would perform no better than a random baseline on our dataset. Related work 5 The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015), in which the task is to automatically estimate the semantic similarity of word pairs. Ideally, the estimated similarity scores should have high correlation with those given by human annotators. However, there is a fundamental difference between SCWS and other word similarity datasets: each word in SCWS is associated with a context which triggers a specific meaning of the word. The unique property of the dataset makes it a suitable benchmark for multi-prototype and contextualized word embeddings. However, in the following, we highlight some of the limitations of the dataset which hinder its s"
N19-1128,P12-1092,0,0.578031,"resented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carry out impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline). Despite providing a suitable means of verifying the effectiveness of the embeddings, the downstream evaluation cannot replace generic evaluations as it is difficult to isolate the impact of embeddings from many other factors involved, including the algorithmic configuration and parameter setting of the system. To our knowledge, the Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) is the only existing benchmark that specifically focuses on the dynamic nature of word semantics.2 In Section 4 we will explain the limitations of this dataset for the evaluation of recent work in the literature. In this paper we propose WiC, a novel dataset that provides a high-quality benchmark for the evaluation of context-sensitive word embeddings. WiC provides multiple interesting characteristics: (1) it is suitable for evaluating a wide range of techniques, including contextualized word and sense representation and word sense disambiguation; (2) it is framed as a binary classification d"
N19-1128,P06-1014,0,0.103333,"r initial training dataset. Semi-automatic check. Even though very few in number, all resources (even exprt-based ones) contain errors such as incorrect part-of-speech tags or ill-formed examples. Moreover, the extraction of examples and the mappings across resources were not always accurate. In order to have as few resource-specific and mapping errors as possible, all training, development and test sets were semi-automatically post-processed, either with small fixes whenever possible or by removing problematic instances otherwise. 2.1.2 Pruning WordNet is known to be a fine-grained resource (Navigli, 2006). Often, different senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et"
N19-1128,D14-1113,0,0.0631549,"1 Introduction One of the main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears. Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this doma"
N19-1128,W16-1620,0,0.074152,"main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears. Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this domain either perform evalua"
N19-1128,N18-1202,0,0.484341,"(potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this domain either perform evaluations on word similarity datasets (in which words are presented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carry out impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline). Despite providing a suitable means of ver"
N19-1128,P17-1170,1,0.883717,"rd (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In the next section we show that the pruning resulted in a significant boost in the clarity of the dataset. 2.2 Quality check To verify the quality and the difficulty of the dataset and to estimate the human-level performance upperbound, we randomly sampled four sets of 100 instances from the test set, with an overlap of 50 instances between two of the annotators. Each set was assigned to an annotator who was asked to label each instance based on whether they thought the two occurrences of the word referred to the same meaning or not.6 The annotators were not provided with knowledge from any"
N19-1128,D16-1174,1,0.933732,"Missing"
N19-1128,K17-1012,1,0.948555,"ferent senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017). We adopted a simple strategy and 4 Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been"
N19-1128,P13-1132,1,0.771028,"igli, 2006). Often, different senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017). We adopted a simple strategy and 4 Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet s"
N19-1128,N10-1013,0,0.0928172,"://pilehvar.github.io/wic/. 1 Introduction One of the main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears. Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluatio"
N19-1128,K16-1006,0,0.0996134,"rrespond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this domain either perform evaluations on word similarity datasets (in which words are presented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carry out impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline). Despite providing a"
N19-1128,P11-1097,0,0.0396217,"Missing"
N19-1128,P13-2125,0,0.0404721,"for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In the next section we show that the pruning resulted in a significant boost in the clarity of the dataset. 2.2 Quality check To verify the quality and the difficulty of the dataset and to estimate the human-level performance upperbound, we randomly sampled four sets of 100 instances from the test set, with an overlap of 50 instances between two of the annotators. Each set was assigned to an annotator who was asked to label each instance based on whether they thought the two occurrences of the word referred to the same meaning or not.6 The"
N19-1128,D07-1107,0,0.0152477,"ained resource (Navigli, 2006). Often, different senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017). We adopted a simple strategy and 4 Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coar"
N19-1128,S17-1004,0,0.0213614,"this dataset for the evaluation of recent work in the literature. In this paper we propose WiC, a novel dataset that provides a high-quality benchmark for the evaluation of context-sensitive word embeddings. WiC provides multiple interesting characteristics: (1) it is suitable for evaluating a wide range of techniques, including contextualized word and sense representation and word sense disambiguation; (2) it is framed as a binary classification dataset, in which, unlike SCWS, identical words are paired with each other (in different con2 With a similar goal in mind but focused on hypernymy, Vyas and Carpuat (2017) developed a benchmark to assess the capability of automatic systems to detect hypernymy relations in context. 1267 Proceedings of NAACL-HLT 2019, pages 1267–1273 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics F There’s a lot of trash on the bed of the river — I keep a glass of water next to my bed when I sleep F Justify the margins — The end justifies the means T Air pollution — Open a window and let in some air T The expanded window will give us time to catch the thieves — You have a two-hour window of clear weather to finish working on the la"
N19-1128,J16-2003,0,\N,Missing
N19-1196,P18-1229,0,0.0173912,"ions the model makes to infer a path, i.e A.D = average depth × average branch. Multi-Sense LSTM (MS-LSTM): Kartsaklis et al. (2018) proposed a model that achieves stateof-the-art results on the text-to-entity mapping on the Snomed CT 7 dataset. The approach uses a novel multi-sense LSTM, augmented with an attention mechanism, to project the definition to the ontology vector space. Additionally, for a better alignment between the two vector spaces, the authors augmented the ontology graph with textual features. 3.2 To perform evaluation of the models described above we used Ancestor-F1 score (Mao et al., 2018). This metric compares the ancestors (is − amodel ) of the predicted node with the ancestors (is − agold ) of the gold node in the taxonomy. we constrain each entity to have only one parent node. The edges between the other parent nodes are removed.6 Path Representations. We also experiment with two path representations. Our first approach, text2nodes, uses the label of an entity (cf. Section 1) to represent a path. This is not efficient since the decoder of the model needs to select between all of the entities in an ontology and also requires more parameters in the model. Our second approach,"
N19-1222,P16-1191,0,0.12356,"rds in WordNet 3.0. 2.2.1 Supersenses It is widely acknowledged that sense distinctions in WordNet inventory are too fine-grained for most NLP applications (Hovy et al., 2013). For instance, for the noun star, WordNet 3.0 lists eight senses, among which two celestial body senses (as an “astronomical object” and that “visible, as a point of light, from the Earth”), and three person senses (“skillful person”, “lead actor”, and “performing artist”). This fine level of sense distinction is often more than that required by the target downstream application (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In our experiments, we used WordNet’s lexicographer files (lexnames2 ) in order to reduce sense granularity. Created by the curators of WordNet 1 https://code.google.com/archive/p/ word2vec/ 2 https://wordnet.princeton.edu/man/ lexnames.5WN.html 2152 WN-seen Concept Mapping top-10 top-100 top-10 top-100 top-10 top-100 RNN cosine ranking 0.656 0.694 0.824 0.836 0.150 0.162 0.310 0.352 0.230 0.335 0.480 0.630 BoW cosine ranking 0.642 0.706 0.820 0.872 0.250 0.310 0.416 0.474 0.280 0.390 0.590 0.735 RNN cosine ranking 0.742 0.668 0.854 0.840 0.164 0.180 0.336 0.372 0.275 0.325 0.505 0.615 BoW c"
N19-1222,C14-1048,0,0.0213215,"dividual meanings of words, usually referred to as word senses. Sense distinctions might be given by an external sense inventory, such as WordNet (Fellbaum, 1998). An inventory-based sense representation technique exploits the knowledge encoded in the resource to construct representations (Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Pilehvar and Collier, 2016). Alternatively, senses can be automatically induced in an unsupervised manner by analyzing the diversity of contexts in which a word appears (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan ˇ et al., 2014; Guo et al., 2014; Suster et al., 2016). Regardless of how senses are obtained, the integration of sense representations into NLP systems is not a straightforward process. Hence, they have often been evaluated on artificial tasks such as word similarity. This is also due to lack of suitable evaluation benchmarks for sense representation techniques. Pilehvar and Camacho-Collados (2019) recently proposed a dataset, The Word-inContext (WiC), which provides a challenging, yet reliable, benchmark for the purpose. Few attempts have been made at the integration of sense representation into downstream applications. Li"
N19-1222,Q16-1002,0,0.523695,"description or definition (Brown and McNeill, 1966; Zock and Bilac, 2004). For example, given “a crystal of snow”, the system has to return the word snowflake. The task is closely related to the “tip of the tongue” problem where an individual recalls some general features about a word but cannot retrieve that from memory. Therefore, a reverse dictionary system can be particularly useful to writers and translators when they cannot recall a word in time or are unsure how to express an idea they want to convey. 2.1 Evaluation framework Our experiments are based on the reverse dictionary model of Hill et al. (2016) which leverages a standard neural architecture in order to map dictionary definitions to representations of the words defined by those definitions. Specifically, they proposed two neural architectures for mapping the definition of word t to its word embedding et . Let Dt be the sequence of words in t’s definition, i.e., Dt = {w1 , w2 , . . . , wn }, with their corresponding embeddings {v1 , v2 , . . . , vn }. The two models differ in the way they process Dt . In the bag-of-words (BoW) model, Dt is taken as a bag 2151 Proceedings of NAACL-HLT 2019, pages 2151–2156 c Minneapolis, Minnesota, Jun"
N19-1222,P12-1092,0,0.0629571,"rts by computing distinct representations for individual meanings of words, usually referred to as word senses. Sense distinctions might be given by an external sense inventory, such as WordNet (Fellbaum, 1998). An inventory-based sense representation technique exploits the knowledge encoded in the resource to construct representations (Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Pilehvar and Collier, 2016). Alternatively, senses can be automatically induced in an unsupervised manner by analyzing the diversity of contexts in which a word appears (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan ˇ et al., 2014; Guo et al., 2014; Suster et al., 2016). Regardless of how senses are obtained, the integration of sense representations into NLP systems is not a straightforward process. Hence, they have often been evaluated on artificial tasks such as word similarity. This is also due to lack of suitable evaluation benchmarks for sense representation techniques. Pilehvar and Camacho-Collados (2019) recently proposed a dataset, The Word-inContext (WiC), which provides a challenging, yet reliable, benchmark for the purpose. Few attempts have been made at the integration of sense r"
N19-1222,P17-1170,1,0.853159,"is also due to lack of suitable evaluation benchmarks for sense representation techniques. Pilehvar and Camacho-Collados (2019) recently proposed a dataset, The Word-inContext (WiC), which provides a challenging, yet reliable, benchmark for the purpose. Few attempts have been made at the integration of sense representation into downstream applications. Li and Jurafsky (2015) experimented with unsupervised sense representations in tasks such as part-of-speech tagging and named entity recognition, with mixed results. Also related to our work are the proposals of Flekova and Gurevych (2016) and Pilehvar et al. (2017) to disambiguate the input text and replace word embeddings with sense embeddings for the intended senses. Our results for supersenses corroborates the findings of Pilehvar et al. (2017) who found reducing finegranularity of senses beneficial to some settings. A more recent branch of research investigates the construction of dynamic word embeddings that can adapt according to the context in which they appear (Peters et al., 2018; Devlin et al., 2018). One of the objectives of this research has been to bypass the integration difficulties of sense representations into downstream models. These so"
N19-1222,D16-1174,1,0.897729,"Missing"
N19-1222,N10-1013,0,0.27577,"nse distributional representation (word embedding). Importantly, this setting ignores the fact that a word can be polysemous, i.e., it can take multiple (possibly unrelated) meanings. Representing a word with all its possible meanings as a single point (vector) in the embedding space, the socalled meaning conflation deficiency (CamachoCollados and Pilehvar, 2018), can hinder system’s semantic effectiveness. To address this deficiency, many techniques have been put forward over the past few years, the most prominent of which is sense representation or multi-prototype embedding (Sch¨utze, 1998; Reisinger and Mooney, 2010). However, as a general trend, these representations are usually evaluated either on generic benchmarks, such as word similarity, or on sense-centered tasks such Reverse Dictionary Reverse dictionary, conceptual dictionary, or concept lookup is the task of returning a word given its description or definition (Brown and McNeill, 1966; Zock and Bilac, 2004). For example, given “a crystal of snow”, the system has to return the word snowflake. The task is closely related to the “tip of the tongue” problem where an individual recalls some general features about a word but cannot retrieve that from"
N19-1222,P15-1173,0,0.0416169,"Missing"
N19-1222,P11-1097,0,0.0512128,"Missing"
N19-1222,J98-1004,0,0.812139,"Missing"
N19-1222,N15-1070,0,0.0202461,"in the task of reverse dictionary mapping. We leave the experiments with other sense representation techniques to future work. 5 Related work Sense representations address the meaning conflation deficiency of their word-based counterparts by computing distinct representations for individual meanings of words, usually referred to as word senses. Sense distinctions might be given by an external sense inventory, such as WordNet (Fellbaum, 1998). An inventory-based sense representation technique exploits the knowledge encoded in the resource to construct representations (Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Pilehvar and Collier, 2016). Alternatively, senses can be automatically induced in an unsupervised manner by analyzing the diversity of contexts in which a word appears (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan ˇ et al., 2014; Guo et al., 2014; Suster et al., 2016). Regardless of how senses are obtained, the integration of sense representations into NLP systems is not a straightforward process. Hence, they have often been evaluated on artificial tasks such as word similarity. This is also due to lack of suitable evaluation benchmarks for sense representatio"
N19-1222,P13-2125,0,0.0320085,"for the 155K unique words in WordNet 3.0. 2.2.1 Supersenses It is widely acknowledged that sense distinctions in WordNet inventory are too fine-grained for most NLP applications (Hovy et al., 2013). For instance, for the noun star, WordNet 3.0 lists eight senses, among which two celestial body senses (as an “astronomical object” and that “visible, as a point of light, from the Earth”), and three person senses (“skillful person”, “lead actor”, and “performing artist”). This fine level of sense distinction is often more than that required by the target downstream application (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In our experiments, we used WordNet’s lexicographer files (lexnames2 ) in order to reduce sense granularity. Created by the curators of WordNet 1 https://code.google.com/archive/p/ word2vec/ 2 https://wordnet.princeton.edu/man/ lexnames.5WN.html 2152 WN-seen Concept Mapping top-10 top-100 top-10 top-100 top-10 top-100 RNN cosine ranking 0.656 0.694 0.824 0.836 0.150 0.162 0.310 0.352 0.230 0.335 0.480 0.630 BoW cosine ranking 0.642 0.706 0.820 0.872 0.250 0.310 0.416 0.474 0.280 0.390 0.590 0.735 RNN cosine ranking 0.742 0.668 0.854 0.840 0.164 0.180 0.336 0.372"
N19-1222,D15-1200,0,0.0202121,"14; Suster et al., 2016). Regardless of how senses are obtained, the integration of sense representations into NLP systems is not a straightforward process. Hence, they have often been evaluated on artificial tasks such as word similarity. This is also due to lack of suitable evaluation benchmarks for sense representation techniques. Pilehvar and Camacho-Collados (2019) recently proposed a dataset, The Word-inContext (WiC), which provides a challenging, yet reliable, benchmark for the purpose. Few attempts have been made at the integration of sense representation into downstream applications. Li and Jurafsky (2015) experimented with unsupervised sense representations in tasks such as part-of-speech tagging and named entity recognition, with mixed results. Also related to our work are the proposals of Flekova and Gurevych (2016) and Pilehvar et al. (2017) to disambiguate the input text and replace word embeddings with sense embeddings for the intended senses. Our results for supersenses corroborates the findings of Pilehvar et al. (2017) who found reducing finegranularity of senses beneficial to some settings. A more recent branch of research investigates the construction of dynamic word embeddings that"
N19-1222,N16-1160,0,0.0416779,"Missing"
N19-1222,N18-1121,0,0.0548688,"Missing"
N19-1222,W04-2105,0,0.174801,"8), can hinder system’s semantic effectiveness. To address this deficiency, many techniques have been put forward over the past few years, the most prominent of which is sense representation or multi-prototype embedding (Sch¨utze, 1998; Reisinger and Mooney, 2010). However, as a general trend, these representations are usually evaluated either on generic benchmarks, such as word similarity, or on sense-centered tasks such Reverse Dictionary Reverse dictionary, conceptual dictionary, or concept lookup is the task of returning a word given its description or definition (Brown and McNeill, 1966; Zock and Bilac, 2004). For example, given “a crystal of snow”, the system has to return the word snowflake. The task is closely related to the “tip of the tongue” problem where an individual recalls some general features about a word but cannot retrieve that from memory. Therefore, a reverse dictionary system can be particularly useful to writers and translators when they cannot recall a word in time or are unsure how to express an idea they want to convey. 2.1 Evaluation framework Our experiments are based on the reverse dictionary model of Hill et al. (2016) which leverages a standard neural architecture in orde"
N19-1222,D14-1113,0,0.0728202,"Missing"
N19-1222,N18-1202,0,0.0471281,"tasks such as part-of-speech tagging and named entity recognition, with mixed results. Also related to our work are the proposals of Flekova and Gurevych (2016) and Pilehvar et al. (2017) to disambiguate the input text and replace word embeddings with sense embeddings for the intended senses. Our results for supersenses corroborates the findings of Pilehvar et al. (2017) who found reducing finegranularity of senses beneficial to some settings. A more recent branch of research investigates the construction of dynamic word embeddings that can adapt according to the context in which they appear (Peters et al., 2018; Devlin et al., 2018). One of the objectives of this research has been to bypass the integration difficulties of sense representations into downstream models. These socalled contextualised word embeddings can easily be replaced with conventional static word embeddings in neural-based NLP systems. This integration has proven beneficial to a wide range of NLP applications. Pilehvar and CamachoCollados (2019) carried out an analysis on the sense distinguishing capability of contextualised embeddings, showing that, despite their successful application to downstream applications, these embeddings"
N19-1222,N19-1128,1,0.771525,"inegranularity of senses beneficial to some settings. A more recent branch of research investigates the construction of dynamic word embeddings that can adapt according to the context in which they appear (Peters et al., 2018; Devlin et al., 2018). One of the objectives of this research has been to bypass the integration difficulties of sense representations into downstream models. These socalled contextualised word embeddings can easily be replaced with conventional static word embeddings in neural-based NLP systems. This integration has proven beneficial to a wide range of NLP applications. Pilehvar and CamachoCollados (2019) carried out an analysis on the sense distinguishing capability of contextualised embeddings, showing that, despite their successful application to downstream applications, these embeddings are not very powerful in capturing distinct meanings of words. 6 Conclusions We provided an analysis on the impact of addressing the meaning conflation deficiency of word embeddings on the performance of a downstream NLP application, i.e., reverse dictionary mapping. Through a set of experiments we showed that a simple migration from words to senses can significantly improve the ability of the system in sem"
P13-1132,N09-1003,0,0.0486211,"Missing"
P13-1132,S12-1051,0,0.29581,"ky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, which enables meaningful similarity comparisons across different scales of text or lexical levels. Second, by operating at the sense level, a unified approach is able to identify the semantic similarities that exist independently of the text’s lexical forms and any semantic ambiguity therein. For example, consider the sentences: t1. A manager fired the worker. t2. An emplo"
P13-1132,S12-1059,0,0.0305138,"Missing"
P13-1132,C10-1005,0,0.0137954,"Missing"
P13-1132,J12-1003,0,0.0112526,"the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely"
P13-1132,P11-2087,0,0.0296674,", word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic simila"
P13-1132,J06-1003,0,0.937818,"2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense infor"
P13-1132,W05-1203,0,0.293606,"vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional features. As can be seen from Table 3, our alignmentbased disambiguatio"
P13-1132,J13-3008,1,0.741191,"Missing"
P13-1132,P08-2067,0,0.0268098,"an and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, w"
P13-1132,J12-1005,0,0.0406973,"Missing"
P13-1132,N06-2015,0,0.0347272,"(Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4 Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz Onto Noun Verb 0.406 0.522 0.421 0.544 0.418 0.531 0.370 0.455 0.218 0.396 Noun 0.450 0.483 0.478 NA NA SE-2 Verb 0.465 0.482 0.473 NA NA Adj 0.484 0.531 0.501 0.473 0.371 Onto + SE"
P13-1132,D07-1061,0,0.146501,"andom walks beginning at that node will produce a frequency distribution over the nodes in the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala,"
P13-1132,O97-1002,0,0.0927604,"icks the most relevant sense of the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our"
P13-1132,S01-1004,0,0.0108658,"t::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4 Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz Onto Noun Verb 0.406 0.522 0.421 0.544 0.418 0.531 0.370 0.455"
P13-1132,P98-2127,0,0.0658275,"f the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional feat"
P13-1132,magnini-cavaglia-2000-integrating,0,0.0196462,"Missing"
P13-1132,W06-2503,0,0.0213638,"my of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWW O 0.868 Method RCos RW O RJac SVM ODE Table 6: Spearman’s ρ correlation coefficients with human judgments on the RG-65 dataset. ADWJac , ADWW O , and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources s"
P13-1132,P06-1014,1,0.932281,"tories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWW O 0.868 Method RCos RW O RJac SVM ODE Table 6: Spearman’s ρ correlation coefficients with human judgments on the RG-65 dataset. ADWJac , ADWW O , and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic sig"
P13-1132,N04-3012,0,0.0975539,"respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs"
P13-1132,N13-1130,1,0.864485,"Missing"
P13-1132,W09-3204,0,0.101622,"the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala, 2002). Let M be the adjacency matrix for the WordNet network, where edges connect senses"
P13-1132,2003.mtsummit-papers.42,0,0.0659828,"ot include the mistakes made when the Jaccard measure was used as they vary with the k value. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideally, a measure’s similarity judgments are expected to be highly correlated with those of humans. To be consistent with the previous literature (Hughes and Ramage, 2007; Agirre et al., 2009), we used Spearman’s rank correlation in our experiment. that of Rapp (2003) uses word senses, an approach that is outperformed by our method. The errors produced by our system were largely the result of sense locality in the WordNet network. Table 5 highlights the incorrect responses. The synonym mistakes reveal cases where senses of the two words are close in WordNet, indicating some relatedness. For example, percentage may be interpreted colloquially as monetary value (e.g., “give me my percentage”) and elicits the synonym of profit in the economic domain, which ADW incorrectly selects as a synonym. 4.1 4.3 Experimental Setup Our alignment-based sense disambiguatio"
P13-1132,D07-1107,0,0.352211,"ation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity (Agirre et al., 2012) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by Landauer and Dumais (1997), which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset (Rubenstein and Goodenough, 1965), and finally (3) surpassing the performance of Snow et al. (2007) in a sensecoarsening task that measures sense similarity. 2 A Unified Semantic Representation We propose a representation of any lexical item as a distribution over a set of word senses, referred to as the item’s semantic signature. We begin with a formal description of the representation at the sense level (Section 2.1). Following this, we describe our alignment-based disambiguation algorithm which enables us to produce sense-based semantic signatures for those lexical items (e.g., words or sentences) which are not sense annotated (Section 2.2). Finally, we propose three methods for comparin"
P13-1132,J11-2003,0,0.012897,"t multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Hal"
P13-1132,S12-1060,0,0.0804335,"Missing"
P13-1132,P95-1026,0,0.288603,"of the context words in both texts. To find this maximum we use an alignment procedure which, for each word type wi in item T1 , assigns wi to the sense that has the maximal similarity to any sense of the word types in the compared text T2 . Algorithm 1 formalizes the alignment process, which produces a sense disambiguated representation as a result. Senses are compared in terms of their semantic signatures, denoted as function R. We consider multiple definitions of R, defined later in Section 2.3. As a part of the disambiguation procedure, we leverage the one sense per discourse heuristic of Yarowsky (1995); given all the word types in two compared lexical items, each type is assigned a single sense, even if it is used multiple times. Additionally, if the same word type appears in both sentences, both will always be mapped to the same sense. Although such a sense assignment is potentially incorrect, assigning both types to the same sense results in a representation that does no worse than a surface-level comparison. We illustrate the alignment-based disambiguation procedure using the two example sentences t1 and t2 given in Section 1. Figure 1(a) illustrates example alignments of the first sense"
P13-1132,agirre-de-lacalle-2004-publicly,0,\N,Missing
P13-1132,C98-2122,0,\N,Missing
P14-1044,E09-1005,0,0.00560636,"nk (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corresponding definitions d1 and d2 in the respective res"
P14-1044,D07-1061,0,0.0346618,"lk graph algorithm, for calculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first rep"
P14-1044,W98-0710,0,0.132083,"ely for aligning new pairs of resources for which no training data is available, with state-of-the-art performance. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying"
P14-1044,de-melo-weikum-2010-providing,0,0.0430328,"Missing"
P14-1044,R09-1039,0,0.0170718,"rs of resources for which no training data is available, with state-of-the-art performance. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two conce"
P14-1044,D12-1129,1,0.0403312,"and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corresponding definitions d1 and d2 in the respective resources L1 and L2 (Figure 1(a"
P14-1044,Q13-1013,0,0.114181,"WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into semantic graphs before such graph-based approaches can be applied to them. To do this, recent work has proposed graph construction by monosemous linking, where a concept is linked to all the concepts associated with the monosemous words in its definition (Matuschek and Gurevych, 2013). However, this alignment method still involves tuning of parameters which are highly dependent on the characteristics of the generated graphs and, hence, requires hand-crafted sense alignments for the specific pair of resources to be aligned, a task which has to be replicated every time the resources are updated. In this paper we propose a unified approach to aligning arbitrary pairs of lexical resources which is independent of their specific structure. Thanks to a novel modeling of the sense entries and an effective ontologization algorithm, our approach also fares well when resources lack r"
P14-1044,P14-1089,1,0.0410164,"raphs on the basis of their shortest distance. To gain more insight into the effectiveness of our 5 Related Work Resource ontologization. Having lexical resources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A recent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) provided approaches for"
P14-1044,I11-1099,0,0.414615,"Robust Approach to Aligning Heterogeneous Lexical Resources Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {pilehvar,navigli}@di.uniroma1.it Abstract Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have f"
P14-1044,E12-1059,0,0.245016,"of a conifer”. The definition contains two content words: fruitn and conifern . The latter word is monosemous in Wiktionary, hence we directly connect cone4n to the only sense of conifern . The noun fruit, however, has 5 senses in Wiktionary. We therefore measure the similarity between the definition of cone4n and all the 5 definitions of fruit and introduce a link from cone4n to the sense of fruit which yields the maximal similarity value (defined as “(botany) The seedbearing part of a plant...”). (WP), Wiktionary (WT), and OmegaWiki (OW). We utilized the DKPro software (Zesch et al., 2008; Gurevych et al., 2012) to access the information in the foregoing three resources. For WP, WT , OW we used the dump versions 20090822, 20131002, and 20131115, respectively. 4 For ontologizing WT and OW, the bag of content words W is given by the content words in sense definitions and, if available, additional related words obtained from lexicon relations (see Section 3). In WT, both of these are in word surface form and hence had to be disambiguated. For OW , however, the encoded relations, though relaEvaluation measures. We followed previous work (Navigli and Ponzetto, 2012; Matuschek and Gurevych, 2013) and evalu"
P14-1044,C12-1108,0,0.0563781,"Missing"
P14-1044,P06-1014,1,0.806119,"Missing"
P14-1044,W11-0122,0,0.189115,"g Heterogeneous Lexical Resources Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {pilehvar,navigli}@di.uniroma1.it Abstract Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into s"
P14-1044,P13-1132,1,0.379781,"alculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tranDefinitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corres"
P14-1044,W08-2231,0,0.173234,"ormalizes the alignment process: the algorithm takes as input the semantic graphs G1 and G2 corresponding to the two resources, as explained above, and produces as output an alignment in the form of a set A of concept pairs. The algorithm iterates over all concepts c1 ∈ V1 and, for each of them, obtains the set of concepts C ⊂ V2 , which can be considered as alignment candidates for c1 (line 3). For a concept c1 , alignment candidates in G2 usually consist of every concept c2 ∈ V2 that shares at least one lexicalization with c1 in the same part of speech tag, i.e., LG1 (c1 ) ∩ LG2 (c2 ) 6= ∅ (Reiter et al., 2008; Meyer and Gurevych, 2011). Once the set of target candidates C for a source concept c1 is obtained, the alignment task can be cast as that of identifying those concepts in C to which c1 should be aligned. To do this, the algorithm calculates the similarity between c1 and each c2 ∈ C (line 5). If their similarity score exceeds a certain value denoted by θ 469 Figure 1: The process of measuring the similarity of a pair of concepts across two resources. The method consists of two components: definitional and structural similarities, each measuring a similarity score for the given concept pair."
P15-1010,N09-1003,0,0.305209,"Missing"
P15-1010,N13-1092,0,0.0947633,"Missing"
P15-1010,P14-1023,0,0.389493,"of-the-art performance on multiple datasets. 1 Introduction The much celebrated word embeddings represent a new branch of corpus-based distributional semantic model which leverages neural networks to model the context in which a word is expected to appear. Thanks to their high coverage and their ability to capture both syntactic and semantic information, word embeddings have been successfully applied to a variety of NLP tasks, such as Word Sense Disambiguation (Chen et al., 2014), Machine Translation (Mikolov et al., 2013b), Relational Similarity (Mikolov et al., 2013c), Semantic Relatedness (Baroni et al., 2014) and Knowledge Representation (Bordes et al., 2013). However, word embeddings inherit two important limitations from their antecedent corpusbased distributional models: (1) they are unable to 1 95 http://paraphrase.org/#/download Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 95–105, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics dataset for the training of sense embeddings. of lexical-semantic knowledge bases, makes embeddings significan"
P15-1010,C14-1048,0,0.0182086,"yer and hence significantly speeding up the training process. Other related work includes GloVe (Pennington et al., 2014), which is an effort to make the vector dimensions in word embeddings explicit, and the approach of Bordes et al. (2013), which trains word embeddings on the basis of relationship information derived from WordNet. Several techniques have been proposed for transforming word embeddings to the sense level. Chen et al. (2014) leveraged word embeddings in Word Sense Disambiguation and investigated the possibility of retrofitting embeddings with the resulting disambiguated words. Guo et al. (2014) exploited parallel data to automatically generate sense-annotated data, based on the fact that different senses of a word are usually translated to different words in another language (Chan and Ng, 2005). The automatically-generated senseannotated data was later used for training sensespecific word embeddings. Huang et al. (2012) 6 Conclusions and Future Work We proposed an approach for obtaining continuous representations of individual word senses, referred to as sense embeddings. Based on the proposed sense embeddings and the knowledge obtained from a large-scale lexical resource, i.e., Bab"
P15-1010,J15-4004,0,0.0681504,"Missing"
P15-1010,P12-1092,0,0.556672,"Navigli Department of Computer Science Sapienza University of Rome {iacobacci,pilehvar,navigli}@di.uniroma1.it Abstract model distinct meanings of a word as they conflate the contextual evidence of different meanings of a word into a single vector; and (2) they base their representations solely on the distributional statistics obtained from corpora, ignoring the wealth of information provided by existing semantic resources. Several research works have tried to address these problems. For instance, basing their work on the original sense discrimination approach of Reisinger and Mooney (2010), Huang et al. (2012) applied K-means clustering to decompose word embeddings into multiple prototypes, each denoting a distinct meaning of the target word. However, the sense representations obtained are not linked to any sense inventory, a mapping that consequently has to be carried out either manually, or with the help of sense-annotated data. Another line of research investigates the possibility of taking advantage of existing semantic resources in word embeddings. A good example is the Relation Constrained Model (Yu and Dredze, 2014). When computing word embeddings, this model replaces the original co-occurre"
P15-1010,J06-1003,0,0.0667063,"a pair of input words w1 and w2 . The algorithm also takes as its inputs the similarity strategy and the weighted similarity parameter α (Section 3.3.1) along with a graph vicinity factor flag (Section 3.3.2). 14: 15: 16: 17: 18: 3.3.1 Similarity measurement strategy We take two strategies for calculating the similarity of the given words w1 and w2 . Let Sw1 and Sw2 be the sets of senses associated with the two respective input words w1 and w2 , and let s~i be the sense embedding vector of the sense si . In the first strategy, which we refer to as closest, we follow the conventional approach (Budanitsky and Hirst, 2006) and measure the similarity of the two words as the similarity of their closest senses, i.e.: Simclosest (w1 , w2 ) = max T (s~1 , s~2 ) s1 ∈Sw1 s2 ∈Sw2 Sw1 ← getSenses(w1 ), Sw2 ← getSenses(w2 ) if Str is closest then sim ← -1 else sim ← 0 end if for each s1 ∈ Sw1 and s2 ∈ Sw2 do if Vic is true then tmp ← T ∗ (s~1 ,s~2 ) else tmp ← T (s~1 ,s~2 ) end if if Str is closest then sim ← max (sim, tmp) else sim ← sim + tmpα × d(s1 ) × d(s2 ) end if end for two words are assigned the similarity judgement of 6.27, which is slightly above the middle point in the similarity scale [0,10] of the dataset."
P15-1010,S12-1047,0,0.284454,"factor flag α parameter for the weighted strategy Output: The similarity between w1 and w2 Vector comparison For comparing vectors, we use the Tanimoto distance. The measure is a generalization of Jaccard similarity for real-valued vectors in [-1, 1]: T (w~1 , w~2 ) = kw~1 k2 w~1 · w~2 + kw~2 k2 − w~1 · w~2 (1) 1: where w~1 · w~2 is the dot product of the vectors w~1 and w~2 and kw~1 k is the Euclidean norm of w~1 . Rink and Harabagiu (2013) reported consistent improvements when using vector space metrics, in particular the Tanimoto distance, on the SemEval-2012 task on relational similarity (Jurgens et al., 2012) in comparison to several other measures that are designed for probability distributions, such as Jensen-Shannon divergence and Hellinger distance. 3.3 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Word similarity 13: We show in Algorithm 1 our procedure for measuring the semantic similarity of a pair of input words w1 and w2 . The algorithm also takes as its inputs the similarity strategy and the weighted similarity parameter α (Section 3.3.1) along with a graph vicinity factor flag (Section 3.3.2). 14: 15: 16: 17: 18: 3.3.1 Similarity measurement strategy We take two strategies for calculating the sim"
P15-1010,W14-1618,0,0.0376758,"in terms of the closest word embeddings among all the corresponding synonyms obtained with the expansion procedure (cf. Section 3.1). A comparison of word and sense embeddings in the vanilla setting (with neither the expansion procedure nor graph vicinity factor) indicates the consistent advantage gained by moving from word Comparison systems. We compare our results against six other systems and the PMI baseline provided by the task organizers. As for systems that use word embeddings for measuring relational similarity, we report results for RNN-1600 (Mikolov et al., 2013c) and PairDirection (Levy and Goldberg, 2014). We also report results for UTD-NB and UTD-SVM (Rink and Harabagiu, 2012), which rely on lexical pattern classification based on Na¨ıve Bayes and Support Vector Machine classifiers, respectively. UTD-LDA (Rink and Harabagiu, 2013) is another system presented by the same authors that casts the task as a selectional preferences one. Finally, we show the performance of Com (Zhila et al., 2013), a system that combines Word2vec, lexical patterns, and knowledge base information. Similarly to the word similarity experiments, we also report a baseline based on word embeddings (Word2vec) trained on th"
P15-1010,D14-1110,0,0.812054,"ve semantic similarity measurement. We evaluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets. 1 Introduction The much celebrated word embeddings represent a new branch of corpus-based distributional semantic model which leverages neural networks to model the context in which a word is expected to appear. Thanks to their high coverage and their ability to capture both syntactic and semantic information, word embeddings have been successfully applied to a variety of NLP tasks, such as Word Sense Disambiguation (Chen et al., 2014), Machine Translation (Mikolov et al., 2013b), Relational Similarity (Mikolov et al., 2013c), Semantic Relatedness (Baroni et al., 2014) and Knowledge Representation (Bordes et al., 2013). However, word embeddings inherit two important limitations from their antecedent corpusbased distributional models: (1) they are unable to 1 95 http://paraphrase.org/#/download Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 95–105, c Beijing, China, July 26-31, 2015. 2015 Association for"
P15-1010,S12-1055,0,0.0233277,"yms obtained with the expansion procedure (cf. Section 3.1). A comparison of word and sense embeddings in the vanilla setting (with neither the expansion procedure nor graph vicinity factor) indicates the consistent advantage gained by moving from word Comparison systems. We compare our results against six other systems and the PMI baseline provided by the task organizers. As for systems that use word embeddings for measuring relational similarity, we report results for RNN-1600 (Mikolov et al., 2013c) and PairDirection (Levy and Goldberg, 2014). We also report results for UTD-NB and UTD-SVM (Rink and Harabagiu, 2012), which rely on lexical pattern classification based on Na¨ıve Bayes and Support Vector Machine classifiers, respectively. UTD-LDA (Rink and Harabagiu, 2013) is another system presented by the same authors that casts the task as a selectional preferences one. Finally, we show the performance of Com (Zhila et al., 2013), a system that combines Word2vec, lexical patterns, and knowledge base information. Similarly to the word similarity experiments, we also report a baseline based on word embeddings (Word2vec) trained on the same corpus and with the same settings as S ENS E MBED. 102 adopted a si"
P15-1010,W13-0118,0,0.0802148,"ow the nth sense of the word with part of speech x as wordxn . 6 97 3.2 Algorithm 1 Word Similarity Input: Two words w1 and w2 Str, the similarity strategy Vic, the graph vicinity factor flag α parameter for the weighted strategy Output: The similarity between w1 and w2 Vector comparison For comparing vectors, we use the Tanimoto distance. The measure is a generalization of Jaccard similarity for real-valued vectors in [-1, 1]: T (w~1 , w~2 ) = kw~1 k2 w~1 · w~2 + kw~2 k2 − w~1 · w~2 (1) 1: where w~1 · w~2 is the dot product of the vectors w~1 and w~2 and kw~1 k is the Euclidean norm of w~1 . Rink and Harabagiu (2013) reported consistent improvements when using vector space metrics, in particular the Tanimoto distance, on the SemEval-2012 task on relational similarity (Jurgens et al., 2012) in comparison to several other measures that are designed for probability distributions, such as Jensen-Shannon divergence and Hellinger distance. 3.3 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Word similarity 13: We show in Algorithm 1 our procedure for measuring the semantic similarity of a pair of input words w1 and w2 . The algorithm also takes as its inputs the similarity strategy and the weighted similarity parameter α ("
P15-1010,N13-1090,0,0.830715,"aluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets. 1 Introduction The much celebrated word embeddings represent a new branch of corpus-based distributional semantic model which leverages neural networks to model the context in which a word is expected to appear. Thanks to their high coverage and their ability to capture both syntactic and semantic information, word embeddings have been successfully applied to a variety of NLP tasks, such as Word Sense Disambiguation (Chen et al., 2014), Machine Translation (Mikolov et al., 2013b), Relational Similarity (Mikolov et al., 2013c), Semantic Relatedness (Baroni et al., 2014) and Knowledge Representation (Bordes et al., 2013). However, word embeddings inherit two important limitations from their antecedent corpusbased distributional models: (1) they are unable to 1 95 http://paraphrase.org/#/download Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 95–105, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics dataset for the t"
P15-1010,H93-1061,0,0.085931,"supervised process, usually based on neural networks. In contrast to word embeddings, which obtain a single model for potentially ambiguous words, sense embeddings are continuous representations of individual word senses. In order to be able to apply word embeddings techniques to obtain representations for individual word senses, large sense-annotated corpora have to be available. However, manual sense annotation is a difficult and time-consuming process, i.e., the so-called knowledge acquisition bottleneck. In fact, the largest existing manually sense annotated dataset is the SemCor corpus (Miller et al., 1993), whose creation dates back to more than two decades ago. In order to alleviate this issue, we leveraged a state-of-the-art Word Sense Disambiguation (WSD) algorithm to automatically generate large amounts of sense-annotated corpora. In the rest of Section 2, first, in Section 2.1, we describe the sense inventory used for S ENS E M BED . Section 2.2 introduces the corpus and the disambiguation procedure used to sense annotate this corpus. Finally in Section 2.3 we discuss how we leverage the automatically sense-tagged 2.2 Generating a sense-annotated corpus As our corpus we used the September-"
P15-1010,P10-1040,0,0.0492224,"ddings are vector space models (VSM) that represent words as real-valued vectors in a low-dimensional (relative to the size of the vocabulary) semantic space, usually referred to as the continuous space language model. The conventional way to obtain such representations is to compute a term-document occurrence matrix on large corpora and then reduce the dimensionality of the matrix using techniques such as singular value decomposition (Deerwester et al., 1990; Bullinaria and Levy, 2012, SVD). Recent predictive techniques (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007; Turian et al., 2010; Mikolov et al., 2013a) replace the conventional two-phase approach with a single supervised process, usually based on neural networks. In contrast to word embeddings, which obtain a single model for potentially ambiguous words, sense embeddings are continuous representations of individual word senses. In order to be able to apply word embeddings techniques to obtain representations for individual word senses, large sense-annotated corpora have to be available. However, manual sense annotation is a difficult and time-consuming process, i.e., the so-called knowledge acquisition bottleneck. In"
P15-1010,Q14-1019,1,0.573841,"Missing"
P15-1010,P14-2089,0,0.682129,"on the original sense discrimination approach of Reisinger and Mooney (2010), Huang et al. (2012) applied K-means clustering to decompose word embeddings into multiple prototypes, each denoting a distinct meaning of the target word. However, the sense representations obtained are not linked to any sense inventory, a mapping that consequently has to be carried out either manually, or with the help of sense-annotated data. Another line of research investigates the possibility of taking advantage of existing semantic resources in word embeddings. A good example is the Relation Constrained Model (Yu and Dredze, 2014). When computing word embeddings, this model replaces the original co-occurrence clues from text corpora with the relationship information derived from the Paraphrase Database1 (Ganitkevitch et al., 2013, PPDB), an automatically extracted dataset of paraphrase pairs. However, none of these techniques have simultaneously solved both above-mentioned issues, i.e., inability to model polysemy and reliance on text corpora as the only source of knowledge. We propose a novel approach, called S ENS E MBED, which addresses both drawbacks by exploiting semantic knowledge for modeling arbitrary word sens"
P15-1010,D14-1162,0,0.121745,"degree of analogy between the two pairs 1: Swa ← getSenses(wa ), Swb ← getSenses(wb ) 2: (s∗a , s∗b ) ← argmaxsa ∈Swa T (s~a , s~b ) 3: Swc ← getSenses(wc ), Swd ← getSenses(wd ) 4: sc , s~d ) (s∗c , s∗d ) ← argmax sc ∈Swc T (~ 5: 4.1.2 Comparison systems We compare the performance of our similarity measure against twelve other approaches. As regards traditional distributional models, we report the best results computed by Baroni et al. (2014) for PMI-SVD, a system based on Pointwise Mutual Information (PMI) and SVD-based dimensionality reduction. For word embeddings, we report the results of Pennington et al. (2014, GloVe) and Collobert and Weston (2008). GloVe is an alternative way for learning embeddings, in which vector dimensions are made explicit, as opposed to the opaque meaning of the vector dimensions in Word2vec. The approach of Collobert and Weston (2008) is an embeddings model with a deeper architecture, designed to preserve more complex knowledge as distant relations. We also show results for the word embeddings trained by Baroni et al. (2014). The authors first constructed a massive corpus by combining several large corpora. Then, they trained dozens of different Word2vec models by varying"
P15-1010,N13-1120,0,0.307465,"nal similarity Relational similarity evaluates the correspondence between relations (Medin et al., 1990). The task can be viewed as an analogy problem in which, given two pairs of words (wa , wb ) and (wc , wd ), the goal is to compute the extent to which the relations of wa to wb and wc to wd are similar. Sense embeddings are suitable candidates for measuring this type of similarity, as they represent relations between senses as linear transformations. Given this property, the relation between a pair of words can be obtained by subtracting their corresponding normalized embeddings. Following Zhila et al. (2013), the relational similarity between two pairs of word (wa , wb ) and (wc , wd ) is accordingly calculated as: ANALOGY (w ~a , w~b , w~c , w~d ) = T (w~b − w~a , w~d − w~c ) (6) We show the procedure for measuring the relational similarity in Algorithm 2. The algorithm first finds the closest senses across the two word pairs: s∗a and s∗b for the first pair and s∗c and s∗d for the second. The analogy vector representations are accordingly computed as the difference between the sense embeddings of the corresponding closest senses. Finally, the relational similarity is computed as the similarity o"
P15-1010,P13-1132,1,0.752755,"d2vec. The approach of Collobert and Weston (2008) is an embeddings model with a deeper architecture, designed to preserve more complex knowledge as distant relations. We also show results for the word embeddings trained by Baroni et al. (2014). The authors first constructed a massive corpus by combining several large corpora. Then, they trained dozens of different Word2vec models by varying the system’s training parameters and reported the best performance obtained on each dataset. As representatives for graph-based similarity techniques, we report results for the state-of-theart approach of Pilehvar et al. (2013) which is based on random walks on WordNet’s semantic network. Moreover, we present results for the graph-based approach of Zesch et al. (2008), which compares a pair of words based on the path lengths on Wiktionary’s semantic network. We also compare our word similarity measure against the multi-prototype models of Reisinger and Mooney (2010) and Huang et al. (2012), and against the approaches of Yu and Dredze (2014) and Chen et al. (2014), which enhance word embeddings with semantic knowledge derived from PPDB and WordNet, respectively. Finally, we report results for word embeddings, as our"
P15-1010,N10-1013,0,0.698982,"Missing"
P15-1072,E09-1005,0,0.756023,"not consider those BabelNet synsets that are not associated with Wikipedia pages. WordNet sense inventory. Similarly, when restricted to the WordNet inventory, we discard those BabelNet synsets that do not contain a WordNet synset. In this setting, we also leverage relations from WordNet’s semantic network and its disambiguated glosses3 in order to obtain a richer set of Wikipedia articles in the sub-corpus construction. The enrichment of the semantic network with the disambiguated glosses has been shown to be beneficial in various graph-based disambiguation tasks (Navigli and Velardi, 2005; Agirre and Soroa, 2009; Pilehvar et al., 2013). 4 Experiments We assess the reliability of M UFFIN in two standard evaluation benchmarks: semantic similarity (Section 4.1) and Word Sense Disambiguation (Section 4.2). 4.1 Datasets Semantic Similarity As our semantic similarity experiment we opted for word similarity, which is one of the most popular evaluation frameworks in lexical semantics. Given a pair of words, the task in word similarity is to automatically judge their semantic similarity and, ideally, this judgement should be close to that given by humans. 4.1.2 Comparison systems Monolingual. We benchmark our"
P15-1072,N09-1003,0,0.0629266,"Missing"
P15-1072,P14-1023,0,0.440805,"ine-interpretable form, is a fundamental problem in Natural Language Processing (NLP). The Vector Space Model (VSM) is a prominent approach for semantic representation, with widespread popularity in numerous NLP applications. The prevailing methods for the computation of a vector space representation are based on distributional semantics (Harris, 1954). However, these approaches, whether in their conventional co-occurrence based form (Salton et al., 1975; Turney and Pantel, 2010; Landauer and Dooley, 2002), or in their newer predictive branch (Collobert and Weston, 2008; Mikolov et al., 2013; Baroni et al., 2014), suffer from a major drawback: they are unable to model individual word senses or concepts, as they conflate 1. Multilingual: it enables sense representation in dozens of languages; 2. Unified: it represents a linguistic item, irrespective of its language, in a unified seman741 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 741–751, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: Our procedure for constructing a multilingual vec"
P15-1072,F14-1032,1,0.762091,"on multiple datasets and settings in both frameworks, which confirms the reliability and flexibility of our representations. 2 2.2 Methodology Vector construction: lexical specificity Lexical specificity (Lafon, 1980) is a statistical measure based on the hypergeometric distribution. Due to its efficiency in extracting a set of highly relevant words from a sub-corpus, the measure has recently gained popularity in different NLP applications, such as textual data analysis (Lebart et al., 1998), term extraction (Drouin, 2003), and domain-based term disambiguation (Camacho-Collados et al., 2014; Billami et al., 2014). We leverage lexical specificity to compute the weights in our vectors. In our earlier work (Camacho-Collados et al., 2015), we conducted different experiments which demonstrated the improvement that lexical specificity can provide over the popular term frequency-inverse document frequency weighting scheme (Jones, 1972, tf-idf ). Lexical specificity computes the vector weights for an item, i.e., a word or a set of words, by comparing and contrasting its contextual information with a reference corpus. In our setting, we take the whole Wikipedia as our reference corpus RC (we use the October 20"
P15-1072,E09-1013,0,0.0487404,"al Semantic Representation of Concepts Jos´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches h"
P15-1072,D09-1124,0,0.0465708,"the RG-65 dataset (Rubenstein and Goodenough, 1965) as our monolingual word similarity dataset. The dataset comprises 65 English word pairs which have been manually annotated by several annotators according to their similarity on a scale of 0 to 4. We also perform evaluations on the French (Joubarne and Inkpen, 2011) and German (Gurevych, 2005) adaptations of this dataset. c∈Cw 7: return cˆ Thanks to the use of BabelNet, our approach is applicable to arbitrary languages. For the task of WSD, we focus on two major sense inventories integrated in BabelNet: Wikipedia and WordNet. Cross-lingual. Hassan and Mihalcea (2009) developed two sets of cross-lingual datasets based on the English MC-30 (Miller and Charles, 1991) and WordSim-353 (Finkelstein et al., 2002) datasets, for four different languages: English, German, Romanian, and Arabic. However, the construction procedure they adopted, consisting of translating the pairs to other languages while preserving the original similarity scores, has led to inconsistencies in the datasets. For instance, the Spanish dataset contains the identical pair mediodiamediodia with a similarity score of 3.42 (in the scale [0,4]). Additionally, the datasets contain several orth"
P15-1072,N15-1059,1,0.795724,"esentation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual"
P15-1072,D14-1110,0,0.232776,"Missing"
P15-1072,P12-1092,0,0.153897,"-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model indi"
P15-1072,D07-1061,0,0.0271844,"e of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Ou"
P15-1072,N15-1184,0,0.0879558,"Missing"
P15-1072,I05-1067,0,0.360848,"4.1.1 Input: a target word w and a document d (context of w) Output: cˆ, the intended sense of w 1: for each concept c ∈ Cw 2: scorec ← 0 3: for each lemma l ∈ d 4: if l ∈ lexc then −1 5: scorec ← scorec + rank(l, lexc ) 6: cˆ ← arg max scorec Monolingual. We picked the RG-65 dataset (Rubenstein and Goodenough, 1965) as our monolingual word similarity dataset. The dataset comprises 65 English word pairs which have been manually annotated by several annotators according to their similarity on a scale of 0 to 4. We also perform evaluations on the French (Joubarne and Inkpen, 2011) and German (Gurevych, 2005) adaptations of this dataset. c∈Cw 7: return cˆ Thanks to the use of BabelNet, our approach is applicable to arbitrary languages. For the task of WSD, we focus on two major sense inventories integrated in BabelNet: Wikipedia and WordNet. Cross-lingual. Hassan and Mihalcea (2009) developed two sets of cross-lingual datasets based on the English MC-30 (Miller and Charles, 1991) and WordSim-353 (Finkelstein et al., 2002) datasets, for four different languages: English, German, Romanian, and Arabic. However, the construction procedure they adopted, consisting of translating the pairs to other lang"
P15-1072,S13-2042,0,0.0480006,"Missing"
P15-1072,S10-1003,0,0.0437631,"vided with suitable amounts of sense-annotated data, their applicability is limited to those words and languages for which such data is available, practically limiting them to a small subset of words mainly in the English language. Knowledge-based approaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniqu"
P15-1072,S13-2029,0,0.0397807,"ts of sense-annotated data, their applicability is limited to those words and languages for which such data is available, practically limiting them to a small subset of words mainly in the English language. Knowledge-based approaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniques: knowledge-based and s"
P15-1072,S13-2043,0,0.0202365,"pproaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniques: knowledge-based and supervised 748 word senses. Thanks to its effective combination of distributional statistics and structured knowledge, the approach can compute efficient representations of arbitrary word senses, with high coverage and irrespect"
P15-1072,D14-1162,0,0.0877904,"lados et al., 2015). However, these techniques are either limited in the languages to which they can be applied, or in their applicability to tasks other than semantic similarity (Navigli and Ponzetto, 2012b). Corpus-based techniques are more flexible, enabling the training of models on corpora other than English. However, these approaches, either in their conventional co-occurrence based form (Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997; Turney and Pantel, 2010; Bullinaria and Levy, 2012), or the more recent predictive models (Mikolov et al., 2013; Collobert and Weston, 2008; Pennington et al., 2014), are restricted in two ways: (1) they cannot be used to compare word senses; and (2) they cannot be directly applied to cross-lingual semantic similarity. Though the first problem has been solved by multi-prototype models (Huang et al., 2012), or by the sense-specific representations obtained as a result of exploiting WordNet glosses (Chen et al., 2014), the second problem remains unaddressed. In contrast, our approach models word senses and concepts effectively, while providing a unified representation for different languages that enables cross-lingual semantic similarity. 6 Acknowledgments"
P15-1072,P14-1044,1,0.783795,"and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages in comparison to the previous VSM techniques: Semantic representa"
P15-1072,P13-1132,1,0.862073,"andard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages in comparison to the previous VSM te"
P15-1072,H93-1061,0,0.184777,"(SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Word Sense Disambiguation Wikipedia In this setting, we selected the SemEval 2013 allwords WSD task (Navigli et al., 2013) as our evaluation benchmark. The task provides datasets for five different languages: Italian, English, French, Spanish and German. There are on average 1123 words to disambiguate in each language’s dataset. As compa"
P15-1072,Q14-1019,1,0.944398,"and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Word Sense Disambiguation Wikipedia In this setting, we selected the SemEval 2013 allwords WSD task (Navigli et al., 2013) as our evaluation benchmark. The task provides datasets for five dif"
P15-1072,W09-3204,0,0.0161049,"Missing"
P15-1072,J91-1002,0,0.380594,"However, the approach is limited to the WSD and Entity Linking tasks. In contrast, our approach is global as it can be used in different NLP tasks, including WSD. Semantic similarity. Semantic similarity of word pairs is usually computed either on the basis of the structural properties of lexical databases and thesauri, or by comparing vectorial representations of words learned from massive text corpora. Structural approaches usually measure the similarity on the basis of the distance information on semantic networks, such as WordNet (Budanitsky and Hirst, 2006), or thesauri, such as Roget’s (Morris and Hirst, 1991; Jarmasz and Szpakowicz, 2003). The semantic network of WordNet has also been used in more sophisticated techniques such as those based on random graph walks (Ramage et al., 2009; Pilehvar et al., 2013), or coupled with the complementary knowledge from Wikipedia (Camacho-Collados et al., 2015). However, these techniques are either limited in the languages to which they can be applied, or in their applicability to tasks other than semantic similarity (Navigli and Ponzetto, 2012b). Corpus-based techniques are more flexible, enabling the training of models on corpora other than English. However,"
P15-1072,N10-1013,0,0.149374,"on of Concepts Jos´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully u"
P15-1072,W09-3206,0,0.0188389,"that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages"
P15-1072,S13-2040,1,0.922763,"oreover, M UFFINpivot attains the best results among the pivot systems on all datasets, confirming the reliability of our system in the monolingual setting. We note that since the cross-lingual datasets were built by translating the word pairs in the original English RG-65 dataset, the pivot-based comparison systems proved to be highly competitive, outperforming the CL-MSR2.0 system by a considerable margin. 4.2 4.2.1 4.2.2 WordNet As regards the WordNet disambiguation task, we take as our benchmark the two recent SemEval English all-words WSD tasks: the SemEval-2013 task on Multilingual WSD (Navigli et al., 2013) and the SemEval-2007 English Lexical Sample, SRL and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2"
P15-1072,P10-4014,0,0.479385,"e take as our benchmark the two recent SemEval English all-words WSD tasks: the SemEval-2013 task on Multilingual WSD (Navigli et al., 2013) and the SemEval-2007 English Lexical Sample, SRL and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Wor"
P15-1072,S07-1016,0,\N,Missing
P15-1072,W09-2413,0,\N,Missing
P15-1072,J06-1003,0,\N,Missing
P15-2001,agirre-de-lacalle-2004-publicly,0,0.0794886,"Missing"
P15-2001,J15-4004,0,0.0289188,"Missing"
P15-2001,P14-1023,0,0.0149352,"he automatic procedure on different languages. Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-anno"
P15-2001,N15-1059,1,0.712727,"nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and"
P15-2001,S14-2003,1,0.67505,"ization of the approach which would be capable of automatically constructing reliable cross-lingual similarity datasets for any pair of languages. Scoring the dataset Twelve native Spanish speakers were asked to evaluate the similarity for the Spanish translations. In order to obtain a more global distribution of judges, we included judges both both Spain and Latin America. As far as the Farsi dataset was concerned, twelve Farsi native speakers scored the newly translated pairs. The guidelines provided to the annotators were based on the recent SemEval task on Cross-Level Semantic Similarity (Jurgens et al., 2014), which provides clear indications in order to distinguish similarity and relatedness. The annotators were allowed to give scores from 0 to 4, with a step size of 0.5. Table 1 shows example pairs with their corresponding scores from the English and the newly created Spanish and Farsi versions of the RG65 dataset. As we can see from the table, the scores across languages are not necessarily identical, with small, in a few cases significant, differences between the corresponding scores. This is due to the fact that associated senses with words do not hold one-to-one correspondence across differe"
P15-2001,P15-1072,1,0.362867,"nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and"
P15-2001,E14-1044,1,0.813372,"e English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-l"
P15-2001,N07-1025,0,0.0174777,"Missing"
P15-2001,I05-1067,0,0.376725,"´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. W"
P15-2001,P11-1076,0,0.0297043,"Kennedy and Hirst (2012) for the automatic construction of cross-lingual datasets from aligned monolingual datasets. Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had The paper is structured as follows. We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2. We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3. Section 4 provides the details of our algorithm for the automatic construction o"
P15-2001,D09-1124,0,0.216669,"@di.uniroma1.it Abstract in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an"
P15-2001,P14-1044,1,0.385004,"igned monolingual datasets. Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had The paper is structured as follows. We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2. We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3. Section 4 provides the details of our algorithm for the automatic construction of the cross-lingual datasets. We report the results of the evaluation performed on the generated"
P15-2001,P13-1132,1,0.716354,"vide an evaluation of the automatic procedure on different languages. Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al.,"
P16-1085,E09-1005,0,0.0130029,"hoCollados et al., 2015a). Babelfy is a multilingual knowledge-based WSD and Entity Linking algorithm based on the semantic network of BabelNet. Muffin is a multilingual sense representation technique that combines the structural knowledge derived from semantic networks with the distributional statistics obtained from text corpora. The system uses sense-based representations for performing WSD. Camacho-Collados et al. (2015a) also proposed a hybrid system that averages the disambiguation scores of IMS with theirs (shown as “Muffin + IMS” in our tables). We also report the results for UKB w2w (Agirre and Soroa, 2009), another knowledge-based WSD approach based on Personalized PageRank (Haveliwala, 2002). Finally, we also carried out experiments with the pre-trained models6 that are proSystem SE2 SE3 SE7 MFS baseline 71.6 70.3 65.8 Babelfy Muffin Muffin + IMS UBK w2w IMS (pre-trained models) IMS (SemCor) IMS (OMSTI) − − − − 77.5 73.0 76.6 68.3 − − 65.3 74.0 70.8 73.3 62.7 66.0 68.5 56.0 66.5 64.2 67.7 IMS + Word2vec (SemCor) 74.2 70.1 68.6 IMS + Word2vec (OMSTI) 77.7 74.1 71.5 Table 4: F1 performance in the nouns subsets of different all-words WSD datasets. vided with the IMS toolkit, as well as IMS traine"
P16-1085,W06-1669,0,0.0124011,"Missing"
P16-1085,P98-1013,0,0.118219,"of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption that a word’s context can provide enough evidence for its disambiguation. Since manual sense annotation is a difficult and time-consuming process, something known as the ”knowledge acquisition bottleneck” (Pilehvar and Navigli, 2014), supervised methods are not scalable and th"
P16-1085,P14-1023,0,0.0329689,"e highlight each cell according to the relative performance gain in comparison to the IMS baseline (top row in the table). ferent dimensionalities and learning techniques: Word2vec embeddings trained on Wikipedia, with the Skip-gram model for dimensionalities 50, 300 and 500 (for comparison reasons) and CBOW with 300 dimensions, Word2vec trained on the Google News corpus with 300 dimensions and the Skipgram model, the 300 dimensional embeddings of GloVe, and the 50 dimensional C&W embeddings. Additionally we include experiments on a non-embedding model, a PMI-SVD vector space model trained by Baroni et al. (2014). Table 6 lists the performance of our system with different word representations in vector space on the Senseval-2 English Lexical Sample task. The results corroborate the findings of Levy et al. (2015) that Skip-gram is more efficient in captur904 Word representations Dim. Concatenation Combination strategy Average Fractional Exponential Skip-gram - GoogleNews GloVe CBOW - Wiki Skip-gram - Wiki 300 300 300 300 65.5 61.7 65.1 65.2 65.5 66.3 65.4 65.6 69.4 66.7 68.9 68.9 69.6 68.3 68.8 69.7 PMI - SVD - Wiki Skip-gram - Wiki 500 500 65.5 65.1 65.3 65.6 67.3 69.1 66.8 69.9 50 50 58.6 65.0 67.3 6"
P16-1085,N13-1092,0,0.0156057,"Missing"
P16-1085,P10-1156,0,0.0196722,"upervised WSD system. We provide an analysis of the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used exten"
P16-1085,E09-1013,0,0.0179041,"al. (2013) are good representatives for this category of systems. We provide more information on IMS in Section 4.1. 3.2 Unsupervised methods These methods create their own annotated corpus. The underlying assumption is that similar senses occur in similar contexts, therefore it is possible to group word usages according to their shared meaning and induce senses. These methods lead to the difficulty of mapping their induced senses into a sense inventory and they still require manual intervention in order to perform such mapping. Examples of this approach were studied by Agirre et al. (2006), Brody and Lapata (2009), Manandhar et al. (2010), Van de Cruys and Apidianaki (2011) and Di Marco and Navigli (2013). 3.3 Semi-supervised methods Other methods, called semi-supervised, take a middle-ground approach. Here, a small manuallyannotated corpus is usually used as a seed for bootstrapping a larger annotated corpus. Examples of these approaches were presented by Mihalcea and Faruque (2004). A second option is to use a wordaligned bilingual corpus approach, based on the assumption that an ambiguous word in one language could be unambiguous in the context of a second language, hence helping to annotate the sen"
P16-1085,P12-1092,0,0.0166034,"ton et al. (2014, GloVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be in"
P16-1085,S07-1053,0,0.0186831,"f a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captures the ordered sequences of words which tend to appear around the target word (Firth, 1957). Though not very popular, syntactic relations have also been studied as a possible feature (Stetina et al., 1998) in WSD. More sophisticated features have also been studied. Examples are distributional semantic models, such as Latent Semantic Analysis (Van de Cruys and Apidianaki, 2011) and Latent Dirichlet Allocation (Cai et al., 2007). Inasmuch as they are the dominant distributional semantic model, word embeddings have also been applied as features to WSD systems. In this paper we study different methods through which word embeddings can be used as WSD features. 3.6 3.6.1 Concatenation Concatenation is our first strategy, which is inspired by the model of Bengio et al. (2003). This method consists of concatenating the vectors of the words surrounding a target word into a larger vector that has a size equal to the aggregated dimensions of all the individual embeddings. Let wij be the weight associated with the ith dimensio"
P16-1085,P15-1010,1,0.832227,"t features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e.,"
P16-1085,P15-1072,1,0.928323,"oVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexi"
P16-1085,N15-1059,1,0.852577,"oVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexi"
P16-1085,D14-1110,0,0.715913,"significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features. 1 Introduction 2 Embeddings represent words, or concepts in a low-dimensional continuous space. These vectors capture useful syntactic and semantic information, such as regularities in language, where relationships are characterized by a relation-specific vector offset. The ability of embeddings to capture knowledge has been exploited in several tasks, such as Machine Translation (Mikolov et al., 2013, MT), Sentiment Analysis (Socher et al., 2013), Word Sense Disambiguation (Chen et al., 2014, WSD) and Language Understanding (Mesnil et al., 2013). Supervised WSD is based on the hypothesis that contextual information provides a Word Embeddings An embedding is a representation of a topological object, such as a manifold, graph, or field, in a certain space in such a way that its connectivity or algebraic properties are preserved (Insall et al., 2015). Presented originally by Bengio et al. (2003), word embeddings aim at representing, i.e., embedding, the ideal semantic space of words in a real-valued continuous vector space. In contrast to traditional distributional techniques, such"
P16-1085,W02-1006,0,0.0190147,"ord embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captures the ordered sequences of words which tend to appear around the target word (Firth, 1957). Though not very popula"
P16-1085,Q15-1016,0,0.0471663,"kipedia, with the Skip-gram model for dimensionalities 50, 300 and 500 (for comparison reasons) and CBOW with 300 dimensions, Word2vec trained on the Google News corpus with 300 dimensions and the Skipgram model, the 300 dimensional embeddings of GloVe, and the 50 dimensional C&W embeddings. Additionally we include experiments on a non-embedding model, a PMI-SVD vector space model trained by Baroni et al. (2014). Table 6 lists the performance of our system with different word representations in vector space on the Senseval-2 English Lexical Sample task. The results corroborate the findings of Levy et al. (2015) that Skip-gram is more efficient in captur904 Word representations Dim. Concatenation Combination strategy Average Fractional Exponential Skip-gram - GoogleNews GloVe CBOW - Wiki Skip-gram - Wiki 300 300 300 300 65.5 61.7 65.1 65.2 65.5 66.3 65.4 65.6 69.4 66.7 68.9 68.9 69.6 68.3 68.8 69.7 PMI - SVD - Wiki Skip-gram - Wiki 500 500 65.5 65.1 65.3 65.6 67.3 69.1 66.8 69.9 50 50 58.6 65.0 67.3 65.7 62.9 68.3 64.3 68.6 Collobert & Weston Skip-gram - Wiki Table 6: F1 percentage performance on the Senseval-2 English Lexical Sample dataset with different word representations models, vector dimensio"
P16-1085,J13-3008,1,0.769446,"Missing"
P16-1085,S01-1001,0,0.935565,"cations which consist of 11 features around the target word. IMS uses a linear support vector machine (SVM) as its classifier. 4.2 Experiments 5.1 Lexical Sample WSD Experiment The lexical sample WSD tasks provide training datasets in which different occurrences of a small set of words are sense annotated. The goal is for a WSD system to analyze the contexts of the individual senses of these words and to capture clues that can be used for distinguishing different senses of a word from each other at the test phase. Datasets. As our benchmark for the lexical sample WSD, we chose the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-2007 (Pradhan et al., 2007) English Lexical Sample WSD tasks. The former two cover nouns, verbs and adjectives in their datasets whereas the latter task focuses on nouns and verbs Embedding Features We take the real-valued word embeddings as new features of IMS and introduce them into the system without performing any further modifications. 2 3 900 code.google.com/archive/p/word2vec/ http://ronan.collobert.com/senna/ Task Senseval-2 (SE2) Senseval-3 (SE3) SemEval-07 (SE7) Training Test noun verb adjective noun verb adjective 4851 3593 13287 356"
P16-1085,N15-1184,0,0.0796841,"ects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are b"
P16-1085,W04-0838,0,0.0214036,"methods lead to the difficulty of mapping their induced senses into a sense inventory and they still require manual intervention in order to perform such mapping. Examples of this approach were studied by Agirre et al. (2006), Brody and Lapata (2009), Manandhar et al. (2010), Van de Cruys and Apidianaki (2011) and Di Marco and Navigli (2013). 3.3 Semi-supervised methods Other methods, called semi-supervised, take a middle-ground approach. Here, a small manuallyannotated corpus is usually used as a seed for bootstrapping a larger annotated corpus. Examples of these approaches were presented by Mihalcea and Faruque (2004). A second option is to use a wordaligned bilingual corpus approach, based on the assumption that an ambiguous word in one language could be unambiguous in the context of a second language, hence helping to annotate the sense in the first language (Ng and Lee, 1996). Word Sense Disambiguation Natural language is inherently ambiguous. Most commonly-used words have several meanings. In order to identify the intended meaning of a word one has to analyze the context in which it appears by directly exploiting information from raw texts. The task of automatically assigning predefined meanings to wor"
P16-1085,W04-0807,0,0.145187,"ound the target word. IMS uses a linear support vector machine (SVM) as its classifier. 4.2 Experiments 5.1 Lexical Sample WSD Experiment The lexical sample WSD tasks provide training datasets in which different occurrences of a small set of words are sense annotated. The goal is for a WSD system to analyze the contexts of the individual senses of these words and to capture clues that can be used for distinguishing different senses of a word from each other at the test phase. Datasets. As our benchmark for the lexical sample WSD, we chose the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-2007 (Pradhan et al., 2007) English Lexical Sample WSD tasks. The former two cover nouns, verbs and adjectives in their datasets whereas the latter task focuses on nouns and verbs Embedding Features We take the real-valued word embeddings as new features of IMS and introduce them into the system without performing any further modifications. 2 3 900 code.google.com/archive/p/word2vec/ http://ronan.collobert.com/senna/ Task Senseval-2 (SE2) Senseval-3 (SE3) SemEval-07 (SE7) Training Test noun verb adjective noun verb adjective 4851 3593 13287 3566 3953 8987 755 314 − 1740 1807 2559"
P16-1085,P10-1154,1,0.425395,"Missing"
P16-1085,N10-1013,0,0.0253751,"Missing"
P16-1085,H94-1046,0,0.21226,"om word embeddings for improved WSD performance: (1) the system of Taghipour and Ng (2015), which combines word embeddings of Collobert and Weston (2008) using the concatenation strategy (cf. Section 3.6) and introduces the combined embeddings as a new feature in addition to the standard WSD features in IMS; and (2) AutoExtend (Rothe and Sch¨utze, 2015), which constructs a whole new set of features based on vectors made from words, senses and synsets of WordNet and incorporates them in IMS. Training corpus. As our training corpus we opted for two available resources: SemCor and OMSTI. SemCor (Miller et al., 1994) is a manually sense-tagged corpus created by the WordNet project team at Princeton University. The dataset is a subset of the English Brown Corpus and comprises around 360,000 words, providing annotations for more than 200K content words.4 OM5.1.1 Lexical sample WSD results Table 2 shows the F1 performance of the different systems on the three lexical sample datasets. As can be seen, the IMS + Word2vec system improves 4 We used automatic mappings to WordNet 3.0 provided in web.eecs.umich.edu/∼mihalcea/downloads.html. 901 STI5 (One Million Sense-Tagged for Word Sense Disambiguation and Inducti"
P16-1085,P15-1173,0,0.045228,"Missing"
P16-1085,C12-1109,0,0.00916756,"the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations repre"
P16-1085,S13-1003,0,0.272531,"Missing"
P16-1085,W04-0811,0,0.414476,"SD results Table 2 shows the F1 performance of the different systems on the three lexical sample datasets. As can be seen, the IMS + Word2vec system improves 4 We used automatic mappings to WordNet 3.0 provided in web.eecs.umich.edu/∼mihalcea/downloads.html. 901 STI5 (One Million Sense-Tagged for Word Sense Disambiguation and Induction) was constructed based on the DSO corpus (Ng and Lee, 1996) and provides annotations for around 42K different nouns, verbs, adjectives, and adverbs. Datasets. As benchmark for this experiment, we considered the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), and SemEval-2007 (Pradhan et al., 2007) English allwords tasks. There are 2474, 2041, and 465 words for which at least one of the occurrences has been sense annotated in the Senseval-2, Senseval-3 and SemEval-2007 datasets, respectively. System SE2 SE3 SE7 MFS baseline 60.1 62.3 51.4 IMS (Zhong and Ng, 2010) Taghipour and Ng (2015) IMS (pre-trained models) IMS (SemCor) IMS (OMSTI) 68.2 − 67.7 62.5 67.0 67.6 68.2 67.5 65.0 66.4 58.3 − 58.0 56.5 57.6 IMS + Word2vec (SemCor) 63.4 65.3 57.8 IMS + Word2vec (OMSTI) 68.3 68.2 59.1 Table 3: F1 performance on different English allwords WSD datasets."
P16-1085,E06-1016,0,0.0363847,"veraging word embeddings as WSD features in a supervised WSD system. We provide an analysis of the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the ne"
P16-1085,D13-1170,0,0.00332672,"beddings alone, if designed properly, can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features. 1 Introduction 2 Embeddings represent words, or concepts in a low-dimensional continuous space. These vectors capture useful syntactic and semantic information, such as regularities in language, where relationships are characterized by a relation-specific vector offset. The ability of embeddings to capture knowledge has been exploited in several tasks, such as Machine Translation (Mikolov et al., 2013, MT), Sentiment Analysis (Socher et al., 2013), Word Sense Disambiguation (Chen et al., 2014, WSD) and Language Understanding (Mesnil et al., 2013). Supervised WSD is based on the hypothesis that contextual information provides a Word Embeddings An embedding is a representation of a topological object, such as a manifold, graph, or field, in a certain space in such a way that its connectivity or algebraic properties are preserved (Insall et al., 2015). Presented originally by Bengio et al. (2003), word embeddings aim at representing, i.e., embedding, the ideal semantic space of words in a real-valued continuous vector space. In contrast t"
P16-1085,Q14-1019,1,0.916961,"training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captu"
P16-1085,W98-0701,0,0.0818822,"s to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-speech (POS) tags of the neighboring words have also been used extensively as a WSD feature. Local collocations represent another standard feature that captures the ordered sequences of words which tend to appear around the target word (Firth, 1957). Though not very popular, syntactic relations have also been studied as a possible feature (Stetina et al., 1998) in WSD. More sophisticated features have also been studied. Examples are distributional semantic models, such as Latent Semantic Analysis (Van de Cruys and Apidianaki, 2011) and Latent Dirichlet Allocation (Cai et al., 2007). Inasmuch as they are the dominant distributional semantic model, word embeddings have also been applied as features to WSD systems. In this paper we study different methods through which word embeddings can be used as WSD features. 3.6 3.6.1 Concatenation Concatenation is our first strategy, which is inspired by the model of Bengio et al. (2003). This method consists of"
P16-1085,P96-1006,0,0.0718105,"Van de Cruys and Apidianaki (2011) and Di Marco and Navigli (2013). 3.3 Semi-supervised methods Other methods, called semi-supervised, take a middle-ground approach. Here, a small manuallyannotated corpus is usually used as a seed for bootstrapping a larger annotated corpus. Examples of these approaches were presented by Mihalcea and Faruque (2004). A second option is to use a wordaligned bilingual corpus approach, based on the assumption that an ambiguous word in one language could be unambiguous in the context of a second language, hence helping to annotate the sense in the first language (Ng and Lee, 1996). Word Sense Disambiguation Natural language is inherently ambiguous. Most commonly-used words have several meanings. In order to identify the intended meaning of a word one has to analyze the context in which it appears by directly exploiting information from raw texts. The task of automatically assigning predefined meanings to words in contexts, known as Word Sense Disambiguation, is a fundamental task in computational lexical semantics (Navigli, 2009). There are four conventional approaches to 1 Supervised methods 3.4 Knowledge-based methods These methods are based on existing lexical resou"
P16-1085,N15-1035,0,0.774649,"ivides each dimension by 2W since the number of context words is twice the window size: Word Embeddings as WSD features Word embeddings have become a prominent technique in distributional semantics. These methods leverage neural networks in order to model the contexts in which a word is expected to appear. Thanks to their ability in efficiently learning the semantics of words, word embeddings have been applied to a wide range of NLP applications. Several studies have also investigated their integration into the Word Sense Disambiguation setting. These include the works of Zhong and Ng (2010), Taghipour and Ng (2015), Rothe and Sch¨utze (2015), and Chen et al. (2014), which leverage embeddings for supervised (the former three) and ei = I+W X j=I−W j6=I wij 2W 3.6.3 Fractional decay Our third strategy for constructing a feature vector on the basis of the context word embeddings is inspired by the way Word2vec combines the words in the context. Here, the importance of a word 899 for our representation is assumed to be inversely proportional to its distance from the target word. Hence, surrounding words are weighted based on their distance from the target word: ei = I+W X wij j=I−W j6=I We carried out experi"
P16-1085,W06-2501,0,0.00847417,"tion of different methods of leveraging word embeddings as WSD features in a supervised WSD system. We provide an analysis of the impact of different parameters in the training of embeddings on the WSD performance. We consider four different strategies for integrating a pre-trained word embedding in a supervised WSD system, discussed in what follows. dently of annotated data and can exploit the graph structure of semantic networks to identify the most suitable meanings. These methods are able to obtain wide coverage and good performance using structured knowledge, rivaling supervised methods (Patwardhan and Pedersen, 2006; Mohammad and Hirst, 2006; Agirre et al., 2010; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Moro et al., 2014; Chen et al., 2014; Camacho-Collados et al., 2015a). 3.5 Standard WSD features As was analyzed by Lee and Ng (2002), conventional WSD systems usually make use of a fixed set of features to model the context of a word. The first feature is based on the words in the surroundings of the target word. The feature usually represents the local context as a binary array, where each position represents the occurrence of a particular word. Part-of-s"
P16-1085,P11-1148,0,0.0143733,"Missing"
P16-1085,D14-1162,0,0.119484,"utational Linguistics, pages 897–907, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics feed-forward neural network capable of predicting a word given the words preceding (i.e., leading up to) that word. Collobert and Weston (2008) presented a much deeper model consisting of several layers for feature extraction, with the objective of building a general architecture for NLP tasks. A major breakthrough occurred when Mikolov et al. (2013) put forward an efficient algorithm for training embeddings, known as Word2vec. A similar model to Word2vec was presented by Pennington et al. (2014, GloVe), but instead of using latent features for representing words, it makes an explicit representation produced from statistical calculation on word countings. Numerous efforts have been made to improve different aspects of word embeddings. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 20"
P16-1085,P14-2089,0,0.0162442,"s. One way to enhance embeddings is to represent more finegrained semantic items, such as word senses or concepts, given that conventional embeddings conflate different meanings of a word into a single representation. Several research studies have investigated the representation of word senses, instead of words (Reisinger and Mooney, 2010; Huang et al., 2012; Camacho-Collados et al., 2015b; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015). Another path of research is aimed at refining word embeddings on the basis of additional information from other knowledge resources (Faruqui et al., 2015; Yu and Dredze, 2014). A good example of this latter approach is that proposed by Faruqui et al. (2015), which improves pre-trained word embeddings by exploiting the semantic knowledge from resources such as PPDB1 (Ganitkevitch et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption"
P16-1085,J14-4005,1,0.814294,"et al., 2013), WordNet (Miller, 1995) and FrameNet (Baker et al., 1998). In the following section we discuss how embeddings can be integrated into an important lexical semantic task, i.e., Word Sense Disambiguation. 3 WSD which we briefly explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption that a word’s context can provide enough evidence for its disambiguation. Since manual sense annotation is a difficult and time-consuming process, something known as the ”knowledge acquisition bottleneck” (Pilehvar and Navigli, 2014), supervised methods are not scalable and they require repetition of a comparable effort for each new language. Currently, the best performing WSD systems are those based on supervised learning. It Makes Sense (Zhong and Ng, 2010, IMS) and the system of Shen et al. (2013) are good representatives for this category of systems. We provide more information on IMS in Section 4.1. 3.2 Unsupervised methods These methods create their own annotated corpus. The underlying assumption is that similar senses occur in similar contexts, therefore it is possible to group word usages according to their shared"
P16-1085,P10-4014,0,0.418016,"explain in the following. 3.1 These methods make use of manually senseannotated data, which are curated by human experts. They are based on the assumption that a word’s context can provide enough evidence for its disambiguation. Since manual sense annotation is a difficult and time-consuming process, something known as the ”knowledge acquisition bottleneck” (Pilehvar and Navigli, 2014), supervised methods are not scalable and they require repetition of a comparable effort for each new language. Currently, the best performing WSD systems are those based on supervised learning. It Makes Sense (Zhong and Ng, 2010, IMS) and the system of Shen et al. (2013) are good representatives for this category of systems. We provide more information on IMS in Section 4.1. 3.2 Unsupervised methods These methods create their own annotated corpus. The underlying assumption is that similar senses occur in similar contexts, therefore it is possible to group word usages according to their shared meaning and induce senses. These methods lead to the difficulty of mapping their induced senses into a sense inventory and they still require manual intervention in order to perform such mapping. Examples of this approach were s"
P16-1085,S07-1016,0,\N,Missing
P16-1085,C98-1013,0,\N,Missing
P16-1085,J14-1003,0,\N,Missing
P16-1085,S10-1011,0,\N,Missing
P17-1115,P14-1023,0,0.0792466,"Missing"
P17-1115,S07-1109,0,0.543854,"ink it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable performance with a sm"
P17-1115,H05-1091,0,0.0262661,"Missing"
P17-1115,P14-2009,0,0.0385077,"Missing"
P17-1115,S07-1033,0,0.813479,"the current NER technology does not have a solution. We think it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 feature"
P17-1115,D15-1162,0,0.0470937,"by the whole sentence (or paragraph), rather it is a small and focused “predicate window” pointed to by the entity’s head dependency. In other words, most of the sentence is not only superfluous for the task, it actually lowers the accuracy of the model due to irrelevant input. This is particularly important in metonymy resolution as the entity’s surface form is not taken into consideration, only its context. In Figure 1, we show the process of extracting the Predicate Window from a sample sentence (more examples are available in the Appendix). We start by using the SpaCy dependency parser by Honnibal and Johnson (2015), which is the fastest in the world, open source and highly customisable. Each dependency tree provides the following features: dependency labels and entity head dependency. Rather than using most of the tree, we only use a single local head dependency relationship to point to the predicate. Leveraging a dependency parser helps PreWin with selecting the minimum relevant input to the model while discarding irrelevant input, which may cause the neural model to behave unpredictably. Finally, the entity itself is never used as input in any of our methods, we only rely on context. PreWin then extra"
P17-1115,P09-2079,0,0.0176808,"Missing"
P17-1115,S07-1031,0,0.0140227,"solution. We think it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable pe"
P17-1115,D12-1017,0,0.781192,"Missing"
P17-1115,P15-2047,0,0.0332998,"Missing"
P17-1115,W02-1027,0,0.8191,"m Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable performance with a small neural network typically trained in no more than 5 epochs, minimal training data, a basic dependency parser and the new PreWin method by being highly discriminating in choosing signal over noise. 9 1255 http://homepages.inf.ed.ac.uk/mnissim/mascara/ 7 References Conclusions and Future Work We showed how a minimalist neural approach can replace substantial external resources, handcrafted features and how the PreWin method can even ignore most of the paragraph where the ent"
P17-1115,S07-1007,0,0.904465,"(2009) used dependency parsing to explore how features based on syntactic dependency relations can be used to improve performance on opinion mining. In unsupervised lymphoma (type of cancer) classification, Luo et al. (2014) constructed a sentence graph from the results of a two-phase dependency parse to mine pathology reports for the relationships between medical concepts. Our methods also exploit the versatility of dependency parsing to leverage information about the sentence structure. 2.1 SemEval 2007 Dataset Our main standard for performance evaluation is the SemEval 2007 Shared Task 8 (Markert and Nissim, 2007) dataset first introduced in Nissim and Markert (2003b). Two types of entities were evaluated, organisations and locations, randomly retrieved from the British National Corpus (BNC). 1249 We only use the locations dataset, which comprises a train (925 samples) and a test (908 samples) partition. For medium evaluation, the classes are literal (geographical territories and political entities), metonymic (place-for-people, place-forproduct, place-for-event, capital-for-government or place-for-organisation) and mixed (metonymic and literal frames invoked simultaneously or unable to distinguish). T"
P17-1115,K16-1006,0,0.0211996,"olov et al., 2013; Mesnil et al., 2013; Baroni et al., 2014; Collobert et al., 2011). We evaluate this method (its 5 and 10-word variant) alongside PreWin and Paragraph. 4.4 Paragraph Baseline The paragraph baseline method extends the “immediate” one by taking 50 words from each side of the entity as the input to the classifier. In practice, this extends the feature window to include extrasentential evidence in the paragraph. This ap6 https://github.com/milangritta/Minimalist-LocationMetonymy-Resolution 7 http://nlp.stanford.edu/projects/glove/ 1252 proach is also popular in machine learning (Melamud et al., 2016; Zhang et al., 2016). 4.5 Ensemble of Models In addition to a single best performing model, we have combined several models trained on different data and/or using different model configurations. For the SemEval test, we combined three separate models trained on the newly annotated CoNLL dataset and the training data for SemEval. For the ReLocaR test, we once again let three models vote, trained on CooNLL and ReLocaR data. 5 Results We evaluate all methods using three datasets for training (ReLocaR, SemEval, CoNLL) and two for testing (ReLocaR, SemEval). Due to inherent randomness in the deep"
P17-1115,D09-1095,0,0.79472,"Missing"
P17-1115,S07-1101,0,0.706817,"nology does not have a solution. We think it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achie"
P17-1115,P03-1008,0,0.586561,"a-based MR dataset called ReLocaR to address the training data shortage. (3) We make an annotated subset of the CoNLL 2003 (NER) Shared Task available for extra MR training data, alongside models, tools and other data. 1248 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1248–1259 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1115 2 Related Work Some of the earliest work on MR that used an approach similar to our method (machine learning and dependency parsing) was by Nissim and Markert (2003a). The decision list classifier with backoff was evaluated using syntactic head-modifier relations, grammatical roles and a thesaurus to overcome data sparseness and generalisation problems. However, the method was still limited for classifying unseen data. Our method uses the same paradigm but adds more features, a different machine learning architecture and a better usage of the parse tree structure. Much of the later work on MR comes from the SemEval 2007 Shared Task 8 (Markert and Nissim, 2007) and later by Markert and Nissim (2009). The feature set of Nissim and Markert (2003a) was updat"
P17-1115,D14-1162,0,0.0777928,"Missing"
P17-1115,J14-4005,1,0.897615,"Missing"
P17-1115,S07-1093,0,0.726835,"for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”. A better output might be something like the following: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. 6.2 Simplicity and Minimalism Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features. We managed to achieve comparable performance with a small neural netwo"
P17-1115,W10-4001,0,0.0596201,"Missing"
P17-1170,R13-1022,0,0.0177951,"oth tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been prop"
P17-1170,baccianella-etal-2010-sentiwordnet,0,0.0519029,"Missing"
P17-1170,J15-2004,0,0.0193871,"ce of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie revi"
P17-1170,D15-1041,0,0.0141171,"s of their input. The word level functionality can affect the performance of these systems in two ways: (1) it can hamper their efficiency in handling words that are not encountered frequently during training, such as multiwords, inflections and derivations, and (2) it can restrict their semantic understanding to the level of words, with all their ambiguities, and thereby prevent accurate capture of the intended meanings. The first issue has recently been alleviated by techniques that aim to boost the generalisation power of NLP systems by resorting to sub-word or character-level information (Ballesteros et al., 2015; Kim et al., 2016). The second limitation, however, has not yet been studied sufficiently. A reasonable way to handle word ambiguity, and hence to tackle the second issue, is to semantify the input text: transform it from its surface-level semantics to the deeper level of word senses, i.e. their intended meanings. We take a step in this direction by designing a pipeline that enables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) a"
P17-1170,D16-1041,1,0.0551113,"a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embed"
P17-1170,W13-5003,0,0.0210547,"se distinctions can be beneficial to both tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representati"
P17-1170,N16-1163,0,0.0312474,"were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. C"
P17-1170,D14-1067,0,0.00713165,"onfigurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic netw"
P17-1170,L16-1269,1,0.895177,"bedding of its corresponding word. Owing to its reliance on WordNet’s semantic network, DeConf is limited to generating only those word senses that are covered by this lexical resource. We propose to use Wikipedia in order to expand the vocabulary of the computed word senses. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential"
P17-1170,E17-2036,1,0.840387,"Missing"
P17-1170,D14-1110,0,0.0816486,"Missing"
P17-1170,D13-1184,0,0.0200958,"biguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses i"
P17-1170,W16-2501,0,0.00621644,"ved over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the"
P17-1170,P16-1191,0,0.660744,"r many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimental setup. 5.1 Experimental set"
P17-1170,C14-1048,0,0.0139087,"al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmar"
P17-1170,P82-1020,0,0.792916,"Missing"
P17-1170,Q15-1023,0,0.0247776,"(ˆ s)} 11: return Disambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input d"
P17-1170,P12-1092,0,0.0165482,"unami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has o"
P17-1170,P11-1015,0,0.0188444,"rk for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even"
P17-1170,P15-1010,1,0.869344,"he shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to che"
P17-1170,N15-1070,0,0.0148647,"have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense em"
P17-1170,N15-1164,0,0.0284146,"he word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) propo"
P17-1170,N15-1011,0,0.00802726,"nses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th An"
P17-1170,P14-5010,0,0.00165544,"for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the input semantic network, N . Our algorithm then selects the best candidates iteratively. In each iteration, the 1 As defined in the underlying sense inventory, up to trigrams. We used Stanford CoreNLP (Manning et al., 2014) for tokenization, Part-of-Speech (PoS) tagging and lemmatization. 1858 Figure 2: Simplified graph-based representation of a sample sentence. Figure 3: Text classification model architecture. candidate sense that has the highest graph degree maxDeg is chosen as the winning sense: maxDeg = max |{(s, s0 ) ∈ E : s0 ∈ S}| s∈S (1) After each iteration, when a candidate sense sˆ is selected, all the possible candidate senses of the corresponding word (i.e. getLex(ˆ s)) are removed from E (line 10 in the algorithm). Figure 2 shows a simplified version of the graph for a sample sentence. The algorithm"
P17-1170,K16-1006,0,0.012425,"inking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in"
P17-1170,P14-1062,0,0.00336837,"nables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based mod"
P17-1170,D14-1181,0,0.123887,"of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 P"
P17-1170,D15-1200,0,0.0215655,"14; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to check the benefit that can be gained by replacing word embeddings with sense embeddings in multiple tasks. With the help of two simple disambiguation algorithms, unsupervised sense embeddings were integrated into various downstream applications, with varying degrees of success. Given the interdependency of sense representation and disambiguation in this model, it is very difficult to introduce alternative algorithms into its pipeline, either to benefit from the state of the art, or to carry out an evaluation. Instead, our pipeline provides the ad"
P17-1170,P16-1096,1,0.782735,"can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the i"
P17-1170,P06-1014,1,0.24475,"sent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embeddings. 4.2 Pre-trained Supersense Embeddings It has been argued that WordNet sense distinctions are too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. I"
P17-1170,D14-1113,0,0.0183534,"et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative"
P17-1170,P04-1035,0,0.00916293,"for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases"
P17-1170,P05-1015,0,0.141168,"sults in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose val"
P17-1170,D14-1162,0,0.119517,"mantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al."
P17-1170,D16-1174,1,0.850136,"Missing"
P17-1170,J14-4005,1,0.680885,"isambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retriev"
P17-1170,D16-1018,0,0.0905951,"ifferent NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the pr"
P17-1170,E17-1010,1,0.0268406,"3.6 83.2 IMDB 87.7 87.4 PL05 77.3 76.6 PL04 67.9 67.4 Stanford 91.8 91.3 Wikipedia 83.1 88.0 75.9† 67.1 91.0 WordNet Wikipedia 84.4 83.1 88.0 88.4∗ 75.9 75.8 66.2 69.3∗ 91.4† 91.0 85.5 88.3 80.2 72.5 93.1 83.4 88.3 79.2 69.7† 92.6 Wikipedia 83.8 87.0† 79.2 73.1 92.3 WordNet 85.2 88.8 79.5 73.8 92.7† Wikipedia 84.2 87.9 78.3† 72.6 92.2 Word Pre-trained Sense Supersense WordNet Table 4: Accuracy performance on five polarity detection datasets. Given that polarity datasets are balanced17 , we do not report F1 which would have been identical to accuracy. texts is a known issue (Moro et al., 2014; Raganato et al., 2017), the tackling of which remains an area of exploration. spective of the classification task. We attribute this to two main factors: 1. Sparsity: Splitting a word into multiple word senses can have the negative side effect that the corresponding training data for that word is distributed among multiple independent senses. This reduces the training instances per word sense, which might affect the classifier’s performance, particularly when senses are semantically related (in comparison to fine-grained senses, supersenses address this issue to some extent). 2. Disambiguation quality: As also ment"
P17-1170,N10-1013,0,0.011305,"nce of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research"
P17-1170,P15-1173,0,0.0610169,"Missing"
P17-1170,P11-1097,0,0.160583,"Missing"
P17-1170,N15-1099,0,0.0142975,"these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system by transforming its i"
P17-1170,J98-1004,0,0.448429,"Missing"
P17-1170,P13-2125,0,0.0526578,"re too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimenta"
P17-1170,P13-1045,0,0.0139818,"sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even distribution of positive and negative labels. 5.3.2 Results Table 4 lists accuracy performance of our classification model and all its varian"
P17-1170,N16-1160,0,0.0557682,"Missing"
P17-1170,D15-1167,0,0.00767,"applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th Annual Meeting of the"
P17-1170,C14-1016,0,0.0219186,"fter their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or"
P17-1170,J17-1002,0,0.0225265,"s. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012)."
P17-1170,D15-1243,0,0.0130565,"no improvement is observed over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification whi"
P17-1170,J14-2007,0,0.0411995,"word-level functionality of these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system"
P17-1170,P15-1032,0,0.018956,"ayer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic network of WordNet (Miller, 1995),"
P17-1170,E17-1109,0,0.0294874,"Missing"
P17-1170,P10-4014,0,0.0202993,"ph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges am"
P17-1170,D13-1141,0,0.0158915,"riments with two configurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach explo"
P17-1170,D07-1107,0,\N,Missing
P17-1170,K17-1012,1,\N,Missing
P18-1119,W17-4417,0,0.0177368,"n-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing. 1 Introduction Geocoding1 is a specific case of text geolocation, which aims at disambiguating place references in text. For example, Melbourne can refer to more than ten possible locations and a geocoder’s task is to identify the place coordinates for the intended Melbourne in a context such as “Melbourne hosts one of the four annual Grand Slam tennis tournaments.” This is central to the success of tasks such as indexing and searching documents by geography (Bhargava et al., 2017), geospatial 1 Also called Toponym Resolution in related literature. analysis of social media (Buchel and Pennington, 2017), mapping of disease risk using integrated data (Hay et al., 2013), and emergency response systems (Ashktorab et al., 2014). Previous geocoding methods (Section 2) have leveraged lexical semantics to associate the implicit geographic information in natural language with coordinates. These models have achieved good results in the past. However, focusing only on lexical features, to the exclusion of other feature spaces such as the Cartesian Coordinate System, puts a ceiling"
P18-1119,C12-1064,0,0.0405819,"Missing"
P18-1119,W16-1721,0,0.509013,"Missing"
P18-1119,N16-1122,0,0.0197966,"nd, we introduced MapVec, an algorithm and a container for encoding context locations in geodesic vector space. We showed how CamCoder, using lexical and MapVec features, outperformed both approaches, achieving a new SOTA. MapVec remains effective with various machine learning frameworks (Random Forest, CNN and MLP) and substantially improves accuracy when combined with other neural models (LSTMs). Finally, we introduced GeoVirus, an open-source dataset that helps facilitate geoparsing evaluation across more diverse domains with different lexical-geographic distributions (Flatow et al., 2015; Dredze et al., 2016). Tasks that could benefit from our methods include social media placing tasks (Choi et al., 2014), inferring user location on Twitter (Zheng et al., 2017), geolocation of images based on descriptions (Serdyukov et al., 2009) and detecting/analyzing incidents from social media (Berlingerio et al., 2013). Future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents. Acknowledgements We gratefully acknowledge the funding support of the Natural Environment Research Council (NERC) PhD Studentship NE/M009"
P18-1119,D10-1124,0,0.370464,"Missing"
P18-1119,D16-1235,0,0.0266525,"Missing"
P18-1119,P17-1115,1,0.916968,"(named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geoparsing. The first step, usually referred to as Geotagging, is a Named Entity Recognition component which extracts all location references in a given text. This phase may optionally include metonymy resolution, see (Zhang and Gelernter, 2015; Gritta et al., 2017a). The goal of geocoding is to choose the correct coordinates for a location mention from a set of candidates. Gritta et al. (2017b) provided a comprehensive survey of five recent geoparsers. The authors established an evaluation framework, with a new dataset, for their experimental analysis. We use this evaluation framework in our experiments. We briefly describe the methodology of each geocoder featured in our evaluation (names are capitalised and appear in italics) as well as survey the related work in geocoding. Computational methods in geocoding broadly divide into rule-based, statistica"
P18-1119,P80-1024,0,0.366307,"Missing"
P18-1119,D15-1162,0,0.0551982,"Missing"
P18-1119,K16-1006,0,0.0204518,"Missing"
P18-1119,D14-1162,0,0.0808156,"Missing"
P18-1119,D17-1016,0,0.36786,"t al. (2015) supplemented lexical features, represented as a bag-of-words, with an exhaustive set of manually generated geographic features and spatial heuristics such as geospatial containment and geodesic distances between entities. The ranking of locations was learned with LambdaMART (Burges, 2010). Unlike our geocoder, the addition of geographic features did not significantly improve scores, reporting: “The geo-specific features seem to have a limited impact over a strong baseline system.” Unable to obtain a codebase, their results feature in Table 1. The latest neural network approaches (Rahimi et al., 2017) with normalised bag-of-word representations have achieved SOTA scores when augmented with social network data for Twitter document (user’s concatenated tweets) geolocation (Bakerman et al., 2018). 3 Methodology Figure 1 shows our new geocoder CamCoder implemented in Keras (Chollet, 2015). The lexical part of the geocoder has three inputs, from the top: Context Words (location mentions excluded), Location Mentions (context words excluded) and the Target Entity (up to 15 words long) to be Figure 1: The CamCoder neural architecture. It is possible to split CamCoder into a Lexical (top 3 inputs)"
P18-1119,P16-4022,0,0.801543,"Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics that exploits both lexical and geographic knowledge producing SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014"
P18-1119,N15-1153,0,0.188813,"Association for Computational Linguistics that exploits both lexical and geographic knowledge producing SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objec"
P18-1119,D12-1137,0,0.506942,"15 - 20, 2018. 2018 Association for Computational Linguistics that exploits both lexical and geographic knowledge producing SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences i"
P18-1119,P13-1144,0,0.766793,"example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geoparsing. The first step, usually referred to as Geotagging, is a Named Entity Recognition component which extracts all location references in a given text. This phase may optionally include metonymy resolution, see (Zhang and Gelernter, 2015; Gritta et al., 2017a). The goal of geocoding is to choose the correct coordinates for a location mention from a set of candidates. Gritta et al. (2017b) provided a comprehensiv"
P18-1119,D14-1039,0,0.419312,"GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geoparsing. The first step, usually referred to as Ge"
P18-1119,P11-1096,0,0.577584,"SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics. 2 Background Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014). This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014). Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared. Geocoding is typically the second step in Geopa"
S14-2003,S12-1051,0,0.573556,"alone. 1 Introduction Given two linguistic items, semantic similarity measures the degree to which the two items have the same meaning. Semantic similarity is an essential component of many applications in Natural Language Processing (NLP), and similarity measurements between all types of text as well as between word senses lend themselves to a variety of NLP tasks such as information retrieval (Hliaoutakis et al., 2006) or paraphrasing (Glickman and Dagan, 2003). Semantic similarity evaluations have largely focused on comparing similar types of lexical items. Most recently, tasks in SemEval (Agirre et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Nav"
S14-2003,S14-2001,0,0.0683247,"Missing"
S14-2003,J08-4004,0,0.0124731,"e average similarity value of non-OOV items. Baseline scores were made public after the evaluation period ended. Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) (Wise, 1996) was added after the evaluation period concluded. Unlike LCS, GST better handles the transpositions of tokens across the two texts and can still report high similarity when encountering reordered text. The minimum match length for GST was set to 6. inter-annotator correlations of 0.377–0.832. However, we note that Pearson correlation and Krippendorff’s α are not directly comparable (Artstein and Poesio, 2008), as annotators’ scores may be correlated, but completely disagree. Second, the two-phase construction process produced values that were evenly distributed across the rating scale, shown in Figure 1 as the distribution of the values for all data sets. However, we note that this creation procedure was very resource intensive and, therefore, semi-automated or crowdsourcing-based approaches for producing high-quality data will be needed to expand the size of the data in future CLSS-based evaluations. Nevertheless, as a pilot task, the manual effort was essential for ensuring a rigorouslyconstruct"
S14-2003,W13-3806,0,0.0852038,"se 1 were rated for their similarity according to the scale described in Section 2.2. An initial pilot study showed that crowdsourcing was only moderately effective for producing these ratings with high agreement. Furthermore, the texts used in Task 3 came from a variety of genres, such as scientific domains, which some workers had difficulty understanding. While we note that crowdsourcing has been used in prior STS tasks for generating similarity scores (Agirre et al., 2012; Agirre et al., 2013), both tasks’ efforts encountered lower worker score correlations on some portions of the dataset (Diab, 2013), suggesting that crowdsourcing may not be reliable for judging the similarity of certain types of text. See Section 3.5 for additional details. Therefore, to ensure high quality, the first two organizers rated all items independently. Because the sentence-to-phrase and phrase-to-word comparisons contain slang and idiomatic language, a third American English mother tongue annotator was added for those data sets. The third annotator was compensated e250 for their assistance. Annotators were allowed to make finer-grained distinctions in similarity using multiples of 0.25. For all items, when any"
S14-2003,P06-1014,1,0.050394,"Missing"
S14-2003,N13-1092,0,0.0642978,"Missing"
S14-2003,J14-4005,1,0.692332,"re et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a). Task 3 was designed with three main objectives. First, the task should include multiple types of comparison in order to assess each type’s difficulty and whether specialized resources are needed for each. Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types. Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text- and sense"
S14-2003,P14-1044,1,0.162584,"re et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a). Task 3 was designed with three main objectives. First, the task should include multiple types of comparison in order to assess each type’s difficulty and whether specialized resources are needed for each. Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types. Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text- and sense"
S14-2003,ide-suderman-2004-american,0,0.00893705,"from specific domains, social media, and text with idiomatic or slang language. Table 3 summarizes the corpora and their distribution across the test and training sets for each comparison type, with a high-level description of the genre of the data. We briefly describe the corpora next. The WikiNews, Reuters 21578, and Microsoft Research (MSR) Paraphrase corpora are all drawn from newswire text, with WikiNews being authored by volunteer writers and the latter two corpora written by professionals. Travel Guides was drawn from the Berlitz travel guides data in the Open American National Corpus (Ide and Suderman, 2004) and includes very verbose sentences 1 Annotation materials along with all training and test data are available on the task website http://alt.qcri. org/semeval2014/task3/. 18 4 – Very Similar The two items have very similar meanings and the most important ideas, concepts, or actions in the larger text are represented in the smaller text. Some less important information may be missing, but the smaller text is a very good summary of the larger text. 3 – Somewhat Similar The two items share many of the same important ideas, concepts, or actions, but include slightly different details. The smalle"
S14-2003,S12-1046,0,0.00660937,"download?” is captured in the phrase “streaming vintage movies for free”, or how similar is “circumscribe” to the phrase “beating around the bush.” Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality. Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications. Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy and Navigli, 2009), summarizaThis paper introduces a new SemEval task on Cross-Level Semantic Similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Highquality data sets were constructed for four comparison types using multi-stage annotation procedures with a graded scale of similarity. Nineteen teams submitted 38 systems. Most systems surpassed the baseline performance, with several attaining high perform"
S14-2003,S12-1047,1,0.484906,"l task for evaluating the capabilities of systems at measuring all types of semantic similarity, independently of the size of the text. To accomplish this objective, systems were presented with items from four comparison types: (1) paragraph to sentence, (2) sentence to phrase, (3) phrase to word, and (4) word to sense. Given a pair of items, a system must assess the degree to which the meaning of the larger item is captured in the smaller item. WordNet 3.0 was chosen as the sense inventory (Fellbaum, 1998). 2.2 Task Data Rating Scale 3.1 Following previous SemEval tasks (Agirre et al., 2012; Jurgens et al., 2012), Task 3 recognizes that two items’ similarity may fall within a range of similarity values, rather than having a binary notion of similar or dissimilar. Initially a six-point (0–5) scale similar to that used in the STS tasks was considered (Agirre et al., 2012); however, annotators found difficulty in deciding between the lower-similarity options. After multiple revisions and feedback from a group of initial annotators, we developed a five-point Likert scale for rating a pair’s similarity, shown in Table 1.1 The scale was designed to systematically order a broad range of semantic relations: s"
S14-2003,S01-1004,0,0.0226851,"Missing"
S14-2003,S10-1004,0,0.00761061,"vintage movies for free”, or how similar is “circumscribe” to the phrase “beating around the bush.” Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality. Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications. Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy and Navigli, 2009), summarizaThis paper introduces a new SemEval task on Cross-Level Semantic Similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Highquality data sets were constructed for four comparison types using multi-stage annotation procedures with a graded scale of similarity. Nineteen teams submitted 38 systems. Most systems surpassed the baseline performance, with several attaining high performance for multiple comparison types. Further,"
S14-2003,2005.mtsummit-papers.11,0,0.00244139,"ce on Wikipedia. Food reviews were drawn from the SNAP Amazon Fine Food Reviews data set (McAuley and Leskovec, 2013) and are customer-authored reviews for a variety of food items. Fables were taken from a collection of Aesop’s Fables. The Yahoo! Answers corpus was derived from the Yahoo! Answers data set, which is a collection of questions and answers from the Community Question Answering (CQA) site; the data set is notable for having the highest degree of ungrammaticality in our test set. SMT Europarl is a collection of texts from the English-language proceedings of the European parliament (Koehn, 2005); Europarl data was also used in the PPDB corpus (Ganitkevitch et al., 2013), from which phrases were extracted. Wikipedia was used to generate two phrase data sets from (1) extracting the definitional portion of an article’s initial sentence, e.g., “An [article name] is a [definition],” and (2) captions for an article’s images. Web queries were gathered from online sources of realworld queries. Last, the first and second authors generated slang and idiomatic phrases based on expressions contained in Wiktionary. 3.2 Annotation Process A two-phase process was used to produce the test and traini"
S14-2003,S13-1004,0,\N,Missing
S16-1169,E09-1005,0,0.04607,"stems was done according to their F1 scores. 4 Systems Five teams submitted 13 systems, where each team’s systems were variations on a common architecture. No system utilized resource-specific features beyond the gloss (e.g., the Wiktionary markup) and so all systems were ultimately submitted in the constrained category. Systems were compared against two baselines. 4.1 Participants The MSejrKU systems build definitional representations based on skip-gram vectors trained on Wikipedia data and incorporates syntactic features. 1098 Words in a candidate gloss are disambiguated using the method of Agirre and Soroa (2009) and then a classifier predicts the goodness of fit for a candidate attachment synset related to those in the gloss. The Duluth systems perform string matching to compare a definition with each of the glosses in WordNet. Given a new definition, systems differ in which words are included from the WordNet synset for comparison: Duluth2 uses only the words in the definition after stopword removal, while Duluth1 extends Duluth2 by including words from the hypernyms of the compared synset. Duluth3 extends Duluth1 with words from the hyponyms but also takes the step of breaking each definition into"
S16-1169,S12-1051,0,0.0334628,"4’s datasets. A system’s task is to identify, for a new word sense, the target synset and the corresponding operation. incorporating polysemy into the task by requiring systems to specify a concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the"
S16-1169,S13-1004,0,0.0211258,"m’s task is to identify, for a new word sense, the target synset and the corresponding operation. incorporating polysemy into the task by requiring systems to specify a concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such a"
S16-1169,S15-2151,0,0.0811161,"-markup) for performing the integration and may use additional information from any dictionary, including the one from which the target word sense had been obtained, e.g., Wiktionary. • Constrained: the system might use any resource other than dictionaries. We allowed each team to submit up to three runs per system type to let them explore different configurations, features, or parameter settings in the official rankings. 2.2 Related Tasks Task 14 directly relates to three branches of prior tasks in SemEval. First, two recent tasks have evaluated automatic methods for constructing taxonomies (Bordea et al., 2015; Bordea et al., 2016). In these tasks, participants are presented with word pairs –but no glosses– and tasked with organizing the words into hypernym relationships. Task 14 provides the next step in such evaluations by explicitly Lemma geoscience POS noun mudslide noun euthanize verb changing room noun Apple noun own verb Definition Any of several sciences that deal with the Earth A mixed drink consisting of vodka, Kahlua and Bailey’s. To submit (a person or animal) to euthanasia. A room, especially in a gym, designed for people to change their clothes. An American multinational technology co"
S16-1169,S16-1168,0,0.0965396,"Missing"
S16-1169,J06-1003,0,0.735663,"ntended target synset and the one outputted by the system. However, we recognize that links in the taxonomy do not necessarily represent uniform semantic distances, since siblings that are deep in the hierarchy tend to be more related to one another. Hence, a direct edge-counting approach might not provide a reliable basis for the evaluation of the attachment accuracy. Interestingly, the attachment accuracy evaluation can be cast as a WordNet-based semantic similarity measurement in which the goal is to compute the similarity between two concepts based on the structural properties of WordNet (Budanitsky and Hirst, 2006b), most important of which is the distance between the two. Therefore, we measure accuracy using the Wu and Palmer (1994, Wu&P) semantic similarity measure, defined as: 2 · depthLCS (1) depth1 + depth2 where depth1 and depth2 are the depths of the two concepts in WordNet’s subsumption hierarchy (hypernymy/hyponymy relations) and DepthLCS is the depth of their least common subsumer, i.e., the most specific concept which is an ancestor of both the concepts. For each instance in the test set for which the system made a prediction, we measure the Wu&P similarity of the output attachment and the c"
S16-1169,esuli-sebastiani-2006-sentiwordnet,0,0.146338,"Missing"
S16-1169,P08-1017,0,0.239662,"Missing"
S16-1169,N06-2015,0,0.0476498,"concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover"
S16-1169,P15-1010,1,0.838753,"h2 uses only the words in the definition after stopword removal, while Duluth1 extends Duluth2 by including words from the hypernyms of the compared synset. Duluth3 extends Duluth1 with words from the hyponyms but also takes the step of breaking each definition into character tri-grams to capture surface-form regularities. The UMNDuluth team performs a similar approach but weights gloss similarity by favoring specific kinds of terms, such as those that are longer and those that appear in WordNet. The TALN systems project the definition of the novel term into a vector space using S ENS E MBED (Iacobacci et al., 2015). Then this vector is compared with the vectors for senses in WordNet to find the closest match. System variations address issues when words have no associated vectors and how to select between candidate attachments. The JRC system uses a form of second-order similarity by representing each definition as a vector over the synsets that contain its words. New terms are attached by finding the WordNet synset whose definition has maximal cosine similarity. The VCU systems adopt multiple approaches based on textual similarity. Run1 uses a secondorder expansion by representing a definition using fre"
S16-1169,S13-2049,1,0.852284,"n evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover the different meanings of a word (Manandhar et al., 2010; Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013). However, the new senses discovered by these methods were never integrated into any taxonomy, making them difficult to use and relate to existing concepts. Task 14 provides a natural next step for WSI pairs, should any novel induced senses be matched with a gloss describing it. 3 Task Data Given that WordNet 3.0 offers wide coverage of common concepts, the majority of novel concepts to be integrated are likely to come from topical domains, informal expressions, and neologisms. Therefore, the dataset for Task 14 was constructed to contain concepts from a wide varie"
S16-1169,N15-1169,1,0.89615,"Missing"
S16-1169,S14-2003,1,0.871791,"t and the corresponding operation. incorporating polysemy into the task by requiring systems to specify a concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate."
S16-1169,E12-1060,0,0.0286342,"e coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2016, pages 1092–1102, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics new automated approaches for using the definitions in these resource to expand WordNet with new concepts. Accordingly, the task provides a high-quality dataset of one thousand definitions from a wide range of domains to be added to the WordNet hierarchy, either by adding them as new concept"
S16-1169,S10-1011,0,0.016173,"rent evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover the different meanings of a word (Manandhar et al., 2010; Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013). However, the new senses discovered by these methods were never integrated into any taxonomy, making them difficult to use and relate to existing concepts. Task 14 provides a natural next step for WSI pairs, should any novel induced senses be matched with a gloss describing it. 3 Task Data Given that WordNet 3.0 offers wide coverage of common concepts, the majority of novel concepts to be integrated are likely to come from topical domains, informal expressions, and neologisms. Therefore, the dataset for Task 14 was constructed to conta"
S16-1169,P14-5010,0,0.00666983,"Missing"
S16-1169,S13-2035,0,0.0242744,"uring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover the different meanings of a word (Manandhar et al., 2010; Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013). However, the new senses discovered by these methods were never integrated into any taxonomy, making them difficult to use and relate to existing concepts. Task 14 provides a natural next step for WSI pairs, should any novel induced senses be matched with a gloss describing it. 3 Task Data Given that WordNet 3.0 offers wide coverage of common concepts, the majority of novel concepts to be integrated are likely to come from topical domains, informal expressions, and neologisms. Therefore, the dataset for Task 14 was constructed to contain concepts from a wide variety of domains and to include"
S16-1169,P13-1132,1,0.912458,"Missing"
S16-1169,W08-0507,0,0.045568,"troduction Semantic networks and ontologies are key resources in Natural Language Processing. Of these resources, WordNet (Fellbaum, 1998), the de facto standard lexical database of English, has remained in widespread use over the past two decades, with a broad range of applications such as Word Sense Disambiguation (Navigli, 2009), Query expansion Hence, a variety of techniques have tried to tackle the coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2"
S16-1169,P06-1101,0,0.281153,"etworks and ontologies are key resources in Natural Language Processing. Of these resources, WordNet (Fellbaum, 1998), the de facto standard lexical database of English, has remained in widespread use over the past two decades, with a broad range of applications such as Word Sense Disambiguation (Navigli, 2009), Query expansion Hence, a variety of techniques have tried to tackle the coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2016, pages 1092–110"
S16-1169,toral-etal-2008-named,0,0.058103,"ies are key resources in Natural Language Processing. Of these resources, WordNet (Fellbaum, 1998), the de facto standard lexical database of English, has remained in widespread use over the past two decades, with a broad range of applications such as Word Sense Disambiguation (Navigli, 2009), Query expansion Hence, a variety of techniques have tried to tackle the coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2016, pages 1092–1102, c San Diego, Cali"
S16-1169,tsvetkov-etal-2014-augmenting-english,0,0.0676202,"Missing"
S16-1169,P94-1019,0,\N,Missing
S16-1169,I11-1098,0,\N,Missing
S17-2002,N09-1003,0,0.413176,"Missing"
S17-2002,W13-3520,0,0.0139707,"eline system we included the results of the concept and entity embeddings of NASARI (Camacho-Collados et al., 2016). These embeddings were obtained by exploiting knowledge from Wikipedia and WordNet coupled with general domain corpus-based Word2Vec embeddings (Mikolov et al., 2013). We performed the evaluation with the 300-dimensional English embedded vectors (version 3.0)9 and used them for all languages. For the comparison within and • Subtask 1. The common corpus for subtask 1 was the Wikipedia corpus of the target language. Specifically, systems made use of the Wikipedia dumps released by Al-Rfou et al. (2013).6 • Subtask 2. The common corpus for subtask 2 was the Europarl parallel corpus7 . This corpus is available for all languages except 6 https://sites.google.com/site/rmyeid/ projects/polyglot 7 http://opus.lingfil.uu.se/Europarl. php 8 http://opus.lingfil.uu.se/ OpenSubtitles2016.php 9 http://lcl.uniroma1.it/nasari/ 20 System English r Luminoso run2 0.78 Luminoso run1 0.78 0.78 QLUT run1∗ hhu run1∗ 0.71 HCCL run1∗ 0.68 NASARI (baseline) 0.68 hhu run2∗ 0.66 QLUT run2∗ 0.67 RUFINO run1∗ 0.65 0.60 Citius run2 l2f run2 (a.d.) 0.64 l2f run1 (a.d.) 0.64 Citius run1∗ 0.57 MERALI run1∗ 0.59 Amateur ru"
S17-2002,J06-1003,0,0.0958441,"0 0.60 0.48 0.53 0.44 0.44 0.57 0.61 0.50 0.31 0.40 0.57 0.61 0.05 -0.06 - ρ Final 0.75 0.75 0.72 0.60 0.57 0.64 0.63 0.62 0.41 0.62 -0.06 - 0.74 0.74 0.70 0.60 0.55 0.52 0.51 0.62 0.41 0.62 0.00 - Table 6: Pearson (r), Spearman (ρ) and official (Final) results of participating systems on the five monolingual word similarity datasets (subtask 1). across languages NASARI relies on the lexicalizations provided by BabelNet (Navigli and Ponzetto, 2012) for the concepts and entities in each language. Then, the final score was computed through the conventional closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006), using cosine similarity as the comparison measure. 3.2 Results We present the results of subtask 1 in Section 3.2.1 and subtask 2 in Section 3.2.2. 3.2.1 System Score Official Rank Luminoso run2 Luminoso run1 HCCL run1∗ NASARI (baseline) RUFINO run1∗ SEW run2 (a.d.) SEW run1 RUFINO run2∗ hjpwhuer run1 0.743 0.740 0.658 0.598 0.555 0.552 0.506 0.369 0.018 1 2 3 4 5 6 7 Table 7: Global results of participating systems on subtask 1 (multilingual word similarity). Subtask 1 Table 6 lists the results on all monolingual datasets.10 The systems which made use of the shared Wikipedia corpus are mark"
S17-2002,S17-2034,0,0.0204818,"inds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obta"
S17-2002,W16-2508,1,0.276959,"reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of system"
S17-2002,D16-1235,0,0.0758158,"Missing"
S17-2002,E17-2036,1,0.804232,"Missing"
S17-2002,P15-2001,1,0.838039,"s 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approach"
S17-2002,I05-1067,0,0.020408,"t contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from various issues: 1. The similarity scale used for the annotation of WordSim-353 and MEN (Bruni et al., 2014) does not distinguish between similarity and relatedness, and hence conflates these two. As a result, the datasets contain pairs that are judged to be highly similar even if they are not of similar type or nature. F"
S17-2002,2015.mtsummit-papers.27,0,0.0742157,"Missing"
S17-2002,D09-1124,0,0.0561085,"nce Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-Salvador et al., 2016). However, there have been very few reliable datasets for evaluating cross-lingual systems. Similarly to the case of multilingual datasets, these cross-lingual datasets have been constructed on the basis of conventional English word similarity datasets: MC-30 and WordSim-353 (Hassan and Mihalcea, 2009), and RG-65 (Camacho-Collados et al., 2015). As a result, they inherit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets,"
S17-2002,S17-2041,0,0.0352303,"Missing"
S17-2002,S17-2033,0,0.0191197,"arks for evaluation. All kinds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have"
S17-2002,N15-1184,0,0.0720481,"Missing"
S17-2002,J15-4004,0,0.047794,". Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-"
S17-2002,S17-2032,0,0.0235773,"ic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair com"
S17-2002,S17-2037,0,0.0372334,"Missing"
S17-2002,C12-1109,0,0.034733,"24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages ha"
S17-2002,S14-2003,1,0.88311,"Missing"
S17-2002,P11-1076,0,0.00934146,"lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked wit"
S17-2002,P16-2074,0,0.0185427,"ex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from"
S17-2002,S17-2035,0,0.0441697,"Missing"
S17-2002,D14-1162,0,0.118611,"c Word Similarity Jose Camacho-Collados*1 , Mohammad Taher Pilehvar*2 , Nigel Collier2 and Roberto Navigli1 1 2 Department of Computer Science, Sapienza University of Rome Department of Theoretical and Applied Linguistics, University of Cambridge 1 {collados,navigli}@di.uniroma1.it 2 {mp792,nhc30}@cam.ac.uk Abstract word representation, a research field that has recently received massive research attention mainly as a result of the advancements in the use of neural networks for learning dense low-dimensional semantic representations, often referred to as word embeddings (Mikolov et al., 2013; Pennington et al., 2014). Almost any application in NLP that deals with semantics can benefit from efficient semantic representation of words (Turney and Pantel, 2010). However, research in semantic representation has in the main focused on the English language only. This is partly due to the limited availability of word similarity benchmarks in languages other than English. Given the central role of similarity datasets in lexical semantics, and given the importance of moving beyond the barriers of the English language and developing languageindependent and multilingual techniques, we felt that this was an appropriat"
S17-2002,S17-2036,0,0.0222596,"ncouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3."
S17-2002,P15-2004,0,0.014103,"oned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarit"
S17-2002,S17-2038,0,0.0206795,"iety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results"
S17-2002,S17-2039,0,0.044791,"Missing"
S17-2002,P14-1044,1,0.822702,"task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations ("
S17-2002,S17-2040,0,0.0437279,"Missing"
S17-2002,S17-2042,0,0.0464227,"Missing"
S17-2002,D13-1141,0,0.039439,"such as RG-65, MC30 (Miller and Charles, 1991), and WS-Sim (Agirre et al., 2009) (the similarity portion of WordSim-353) are relatively small, containing 65, 30, and 200 word pairs, respectively. Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming"
S17-2002,K15-1026,0,0.0981438,"erit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets, constructed according to the procedure of Camacho-Collados et al. (2015), in a semi-automatic manner exploiting the monolingual datasets of subtask 1. These datasets constitute a reliable evaluation framework across five languages. 2 Table 1: The set of thirty-four domains. wide range of domains (Section 2.1.1), (2) through translation of these pairs, we obtained word pairs for the other four languages (Section 2.1.2) and, (3) all word pairs of each dataset were manually scored by multiple annotators (Section 2.1.3). 2.1.1 English dataset creation Seed set selection. The dataset creation started with the selection of 500 English words. One of the main objectives o"
S17-2002,S17-2008,0,0.123538,"distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results of the concept and entity embeddings of"
S17-2002,D14-1034,0,\N,Missing
S17-2002,P16-1157,0,\N,Missing
S17-2002,W16-2502,0,\N,Missing
W16-2902,D10-1115,0,0.0965553,"Missing"
W16-2902,D14-1113,0,0.0128517,"urces (Yu and Dredze, 2014; Bian et al., 2014; Faruqui et al., 2015) or by including arbitrary contexts in the training process (Levy and Goldberg, 2014). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our approach can provide a more"
W16-2902,P15-1072,1,0.912999,"ese techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our approach can provide a more reliable representation of domain-specific items based on significantly more statistical knowledge. Our experiments show that the proposed model can provide a considerable improv"
W16-2902,D14-1162,0,0.0802104,"rophthalmia. 3.2 3.3 Baselines As baseline, we benchmark our improved representations against Word2vec. We use the 300dimensional vectors trained on the Google News corpus (about 100B tokens). We also report results for the Word2vec vectors when retrofitted using the approach of Faruqui et al. (2015) to the Paraphrase Database (Ganitkevitch et al., 2013, PPDB) and SNOMED-CT1 . The latter is a comprehensive clinical terminology from which we extracted 108K synonymous sets, each comprising an average of 2.7 synonyms. We also compare our representations against the 300-dimensional GloVe vectors (Pennington et al., 2014) trained on the Wikipedia 2014 + Gigaword 5 corpus (6B tokens). We were also interested in verifying how Word2vec and GloVe would perform if trained on Tasks Based on the ontological structure of HPO, we propose two tasks in the framework of semantic similarity measurement. Synonym identification. Let P be the set of all phenotypes in the HPO ontology. Let P ∗ = {p1 , ..., pk } (⊂ P) be the subset of k phenotypes for which at least one synonymous phenotype is provided in HPO and Spi = {s1pi , ..., slpi } be the set of l synonymous phenotypes for phenotype pi . Given a spi , the task here is si"
W16-2902,D14-1110,0,0.0473768,"Missing"
W16-2902,N10-1013,0,0.0359127,"th the help of knowledge derived from other resources (Yu and Dredze, 2014; Bian et al., 2014; Faruqui et al., 2015) or by including arbitrary contexts in the training process (Levy and Goldberg, 2014). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more"
W16-2902,N15-1184,0,0.192577,"Missing"
W16-2902,P15-1173,0,0.0236663,"Missing"
W16-2902,N13-1092,0,0.0799235,"Missing"
W16-2902,P12-1092,0,0.0274672,"ived from other resources (Yu and Dredze, 2014; Bian et al., 2014; Faruqui et al., 2015) or by including arbitrary contexts in the training process (Levy and Goldberg, 2014). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our"
W16-2902,P14-2089,0,0.21571,"Missing"
W16-2902,P15-1010,1,0.786101,"14). However, most of these techniques still suffer from another deficiency of word embeddings that they inherit from their countbased ancestors: they conflate the different meanings of a word into a single vector representation. Attempts have been made to tackle the meaning conflation issue of word-level representations. A series of approaches cluster the context of a word prior to representation (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014) whereas others exploit lexical knowledge bases for sense-specific information (Rothe and Sch¨utze, 2015; Chen et al., 2014; Iacobacci et al., 2015; Camacho-Collados et al., 2015). We propose a model that addresses both these issues through a mapping of a lexical item to a sorted list of representative words that brings about two advantages. Firstly, it pinpoints with an inherent disambiguation the meaning of the given lexical item at a deeper semantic level. Secondly, by casting the representation of the item as that of a set of potentially more frequent words, our approach can provide a more reliable representation of domain-specific items based on significantly more statistical knowledge. Our experiments show that the proposed model c"
W16-2902,Q14-1019,0,\N,Missing
W16-2902,P14-2050,0,\N,Missing
W18-5406,N15-1011,0,0.0200302,"t al., 2016; Kuznetsov and Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on sta"
W18-5406,P14-1062,0,0.027683,"boxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducing sparsity and vocabulary size, lowercasing may negatively impact system’s performance by increasing ambiguity. For instance, the Apple company in our example and the apple fruit would be considered as identical entities. (Kalchbrenner et al., 2014; Kim, 2014; Dos Santos and Gatti, 2014; Yin et al., 2017), which are the tasks considered in this work. The goal of our evaluation study is to find answers to the following two questions: 1. Are neural network architectures (in particular CNNs) affected by seemingly small preprocessing decisions in the input text? 2. Does the preprocessing of the embeddings’ underlying training corpus have an impact on the final performance of a state-of-the-art neural network text classifier? 2.2 The process of lemmatizing consists of replacing a given token with its corresponding lemma: According to our exp"
W18-5406,E17-1104,0,0.0210703,"extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analys"
W18-5406,D14-1181,0,0.22698,"lehvar, 2018). However, while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus (Ebert et al., 2016; Kuznetsov and Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (parti"
W18-5406,J15-2004,0,0.013275,"these multiwords, or directly include multiwords along with single words in their pretrained embedding spaces (Mikolov et al., 2013b). Lowercasing This is the simplest preprocessing technique which consists of lowercasing each single token of the input text: apple is asking its manufacturers to move macbook air production to the united states . 41 We considered two tasks for our experiments: topic categorization, i.e. assigning a topic to a given document from a pre-defined set of topics, and polarity detection, i.e. detecting if the sentiment of a given piece of text is positive or negative (Dong et al., 2015). Two different settings were studied: (1) word embedding’s training corpus and the evaluation dataset were preprocessed in a similar manner (Section 3.2); and (2) the two were preprocessed differently (Section 3.3). In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews P"
W18-5406,C18-1020,0,0.0125999,"languages such as Chinese, Japanese and Korean. As opposed to our work, their analysis was focused on UTF-8 bytes, characters, words, romanized characters and romanized words as encoding levels, rather than the preprocessing techniques analyzed in this paper. Additionally, word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems (Goldberg, 2016; Camacho-Collados and Pilehvar, 2018). However, while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus (Ebert et al., 2016; Kuznetsov and Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015"
W18-5406,C14-1008,0,0.0184049,"Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducing sparsity and vocabulary size, lowercasing may negatively impact system’s performance by increasing ambiguity. For instance, the Apple company in our example and the apple fruit would be considered as identical entities. (Kalchbrenner et al., 2014; Kim, 2014; Dos Santos and Gatti, 2014; Yin et al., 2017), which are the tasks considered in this work. The goal of our evaluation study is to find answers to the following two questions: 1. Are neural network architectures (in particular CNNs) affected by seemingly small preprocessing decisions in the input text? 2. Does the preprocessing of the embeddings’ underlying training corpus have an impact on the final performance of a state-of-the-art neural network text classifier? 2.2 The process of lemmatizing consists of replacing a given token with its corresponding lemma: According to our experiments in topic categorization and po"
W18-5406,D16-1071,0,0.0488515,"Missing"
W18-5406,N15-1184,0,0.0665178,"Missing"
W18-5406,P16-1191,0,0.0131783,"sh.1 The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words. However, in practise, other preprocessing techniques can be (and are) further used together with tokenization. These include lemmatization, lowercasing and multiword grouping, among others. Although these preprocessing decisions have 1 Note that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural N"
W18-5406,Q15-1016,0,0.0506403,"that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducin"
W18-5406,D15-1200,0,0.0264423,"guages, including English.1 The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words. However, in practise, other preprocessing techniques can be (and are) further used together with tokenization. These include lemmatization, lowercasing and multiword grouping, among others. Although these preprocessing decisions have 1 Note that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzi"
W18-5406,P11-1015,0,0.0752179,"common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the second model, we add a recurrent layer (specifically an LSTM"
W18-5406,S13-1005,0,0.0156076,"directly to the fully connected softmax layer.3 The inclusion of this LSTM layer has been shown to be able to effectively replace multiple layers of convolution and be beneficial particularly for large inputs (Xiao and Cho, 2016). These models were used for both topic categorization and polarity detection tasks, with slight hyperparameter variations given their different natures (mainly in their text size) which were fixed across all datasets. The embedding layer was initialized using 300-dimensional CBOW Word2vec embeddings (Mikolov et al., 2013a) trained on the 3B-word UMBC WebBase corpus (Han et al., 2013) with standard hyperparameters4 . Preprocessing. Four different techniques (see Section 2) were used to preprocess the datasets as well as the corpus which was used to train word embeddings (i.e. UMBC). For tokenization and lemmatization we relied on Stanford CoreNLP (Manning et al., 2014). As for multiwords, we used the phrases from the pre-trained Google News Word2vec vectors, which were obtained using a simple statistical approach (Mikolov et al., 2013b).12 3.2 Experiment 1: Preprocessing effect Table 2 shows the accuracy13 of the classification models using our four preprocessing technique"
W18-5406,P14-5010,0,0.00325792,"nd polarity detection tasks, with slight hyperparameter variations given their different natures (mainly in their text size) which were fixed across all datasets. The embedding layer was initialized using 300-dimensional CBOW Word2vec embeddings (Mikolov et al., 2013a) trained on the 3B-word UMBC WebBase corpus (Han et al., 2013) with standard hyperparameters4 . Preprocessing. Four different techniques (see Section 2) were used to preprocess the datasets as well as the corpus which was used to train word embeddings (i.e. UMBC). For tokenization and lemmatization we relied on Stanford CoreNLP (Manning et al., 2014). As for multiwords, we used the phrases from the pre-trained Google News Word2vec vectors, which were obtained using a simple statistical approach (Mikolov et al., 2013b).12 3.2 Experiment 1: Preprocessing effect Table 2 shows the accuracy13 of the classification models using our four preprocessing techniques. We observe a certain variability of results depending on the preprocessing techniques used (averEvaluation datasets. For the topic categorization task we used the BBC news dataset5 (Greene and Cunningham, 2006), 20News (Lang, 1995), Reuters6 (Lewis et al., 2004) and Ohsumed7 . 8 Both PL"
W18-5406,W04-3253,0,0.25722,"gorization and polarity detection, these decisions are important in certain cases. Moreover, we shed some light on the motivations of each preprocessing decision and provide some hints on how to normalize the input corpus to better suit each setting. The accompanying materials of this submission can be downloaded at the following repository: https://github.com/pedrada88/ preproc-textclassification. 2 Apple be ask its manufacturer to move MacBook Air production to the United States . Lemmatization has been traditionally a standard preprocessing technique for linear text classification systems (Mullen and Collier, 2004; Toman et al., 2006; Hassan et al., 2007). However, it is rarely used as a preprocessing stage in neuralbased systems. The main idea behind lemmatization is to reduce sparsity, as different inflected forms of the same lemma may occur infrequently (or not at all) during training. However, this may come at the cost of neglecting important syntactic nuances. Text Preprocessing Given an input text, words are gathered as input units of classification models through tokenization. We refer to the corpus which is only tokenized as vanilla. For example, given the sentence “Apple is asking its manufact"
W18-5406,P14-3006,0,0.0606468,"Missing"
W18-5406,P04-1035,0,0.00895054,"sed differently (Section 3.3). In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the"
W18-5406,P05-1015,0,0.129875,"). In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the second model, we add a recu"
W18-5406,D14-1162,0,0.079463,"he corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducing sparsity and vocabulary size, lowercasing may negatively impact system’s performance by increasing ambiguity. For instance, the Apple company in our example and the apple fruit would be considered as identical entities. (Kalchbrenner et al., 2014; Kim, 2014; Dos Santos and Gatti, 2014; Yin et al., 2017), which are the tasks considered in this work. The goal of our evaluation study is to find answers to the following two questions: 1. Are neural network architectures (in particular CNNs) affected by seemingly small preprocessin"
W18-5406,P17-1170,1,0.823591,"NLP pipeline is a tokenizer which transforms texts to sequences of words. However, in practise, other preprocessing techniques can be (and are) further used together with tokenization. These include lemmatization, lowercasing and multiword grouping, among others. Although these preprocessing decisions have 1 Note that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 4"
W18-5406,E17-2081,0,0.0216112,"Missing"
W18-5406,P13-1045,0,0.00897806,"preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the second model, we add a recurrent layer (specifically an LSTM (Hochreiter and Schmidhuber, 1997)) before passing the po"
W18-5406,D15-1167,0,0.0275623,"d Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks fr"
W18-5507,aker-etal-2017-simple,0,0.0147379,"cle by attaching a new paragraph with the last updates at the beginning of it. Moreover, putting most newsworthy facts at the beginning of an article allows the impatient readers to quickly decide on their level of interest in the report. After manual analysis of excerpts of the F NC -1 corpus, we concluded that most articles were actuRumor Stance Detection on Tweets. The most commonly used datasets for rumor stance detection, the RumorEval (Derczynski et al., 2017) and the PHEME (Zubiaga et al., 2016b) corpora, collect Tweets. State-of-the-art results on the PHEME corpus has been obtained by Aker et al. (2017), who used a very rich set of problemspecific features. Their model beat the previous state-of-the-art system by Zubiaga et al. (2016a), 42 HEADLINE Sentence 1 w4 Sentence 1 w3 ARTICLE Backward LSTM conditioned on the previous sentence Forward LSTM conditioned on the previous sentence w1 w2 Input features (word embeddings, ...) w3 .... Sentence 2 w2 Sentence 2 ARTICLE w1 LSTM conditioned on the headline LSTM conditioned on the previous sentence Sentence n .... w1 w2 w3 w4 Figure 4: Detail of the forward component of the double-conditional encoding architecture (best seen in color). Dotted arro"
W18-5507,D16-1084,0,0.0356546,"Missing"
W18-5507,N16-1138,0,0.0393041,"peline can be naturally adapted to FND. In recent years, several efforts have been made by the research community toward the automatization of some of these stages, in order to provide effective tools to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). Concerning FND, Pomerleau and Rao (2017) recently released a dataset for the Stance Detection step in the framework of the Fake News Challenge1 (F NC -1). The core of the corpus is constituted by a collection of articles discussing 566 claims, 300 of which come from the E MERGENT dataset (Ferreira and Vlachos, 2016). Each article is summarized in a headline and labeled as agreeing (AGR), disagreeing (DSG) or discussing (DSC) the claim. Additionally, unrelated (UNR) samples were created by pairing headlines with random articles. The goal of the challenge was to classify the pairs constituted by a headline and an article as AGR, DSG , DSC or UNR. Following the pipeline discussed above, it is clear that the F NC -1 actually covers two of the four steps, namely: (1) The tracking step, consisting in filtering out the irrelevant UNR samples; (2) The actual stance detection step, consisting in the classificatio"
W18-5507,P05-1045,0,0.0334323,"standing, without counting on such possibly accidental correlations. In order to avoid the systems to rely on chance correlations, which would not generalize on the test set, we modified the input sequences by substituting all input tokens labeled as <PERSON&gt;, <ORGANIZATION&gt; and <LOCATION&gt; by the Stanford Named Entity Recognizer with the corresponding NE tags. Additional Experiments Using additional Input Channels To investigate the impact of features other than word embeddings, we consider two further input channels: • Named Entities (NE) - NEs were obtained using the Stanford NE Recognizer (Finkel et al., 2005), resulting in a tagset of 13 labels. • Characters - Each input word was split into characters. Only characters occurring more than 100 times in the training set were considered, obtaining a final vocabulary of 149 characters. As in Lample et al. (2016), in we concatenate the output of a BiLSTM run over the character sequence. 5.2.3 Results Results of experiments concatenating the previously mentioned features to the word embedding input to both architectures are reported in Table 4 (even lines). In general, using NE embeddings alone with word embeddings was not beneficial for both models. Con"
W18-5507,C18-1283,0,0.0118432,"stage, where posts concerning the identified rumor are collected; after determining the orientation expressed in each post with respect to the rumor (stance detection), the final truth value of the rumor is obtained by aggregating those single stance judgments (veracity classification). As shown in Figure 1, this pipeline can be naturally adapted to FND. In recent years, several efforts have been made by the research community toward the automatization of some of these stages, in order to provide effective tools to enhance the performance of human journalists in rumor and fake news debunking (Thorne and Vlachos, 2018). Concerning FND, Pomerleau and Rao (2017) recently released a dataset for the Stance Detection step in the framework of the Fake News Challenge1 (F NC -1). The core of the corpus is constituted by a collection of articles discussing 566 claims, 300 of which come from the E MERGENT dataset (Ferreira and Vlachos, 2016). Each article is summarized in a headline and labeled as agreeing (AGR), disagreeing (DSG) or discussing (DSC) the claim. Additionally, unrelated (UNR) samples were created by pairing headlines with random articles. The goal of the challenge was to classify the pairs constituted"
W18-5507,C18-1158,0,0.351489,"ure 1: The rumor verification (RV) pipeline proposed by Zubiaga et al. (2018). The first row describes the corresponding step whereas the second row shows the outputs of each step for both the RV and the fake news detection (FND) tasks. The red rectangle indicates steps covered by the FNC-1 corpus. Figure adapted from Zubiaga et al. (2018). claim, leveraging only word embeddings as input. Note that the amount of semantic understanding needed for the second task is much higher than for the first. In fact, even humans struggle in the related sample classification, as empirically demonstrated by Hanselowski et al. (2018): the interannotator agreement of five human judges drops from Fleiss’ κ of .686 to .218, after filtering out the UNR samples. For this reason, we concentrate on the stance detection step, and we make the following contributions: 2 2.1 Related Work Stance Detection Stance Detection (SD) has been defined as the task of determining the attitude expressed in a short piece of text with respect to a target, usually expressed with one or few words (as Feminism or Climate Change, Mohammad et al. (2016)). In fact, most of the available corpora for SD consider very short samples, as Tweets. SD became v"
W18-5507,P18-2118,0,0.132447,"time, reduce the number of parameters. Double-conditional Encoding. As a first method, we modeled the relationship between the headline and the article using conditional encoding. First, the headline is encoded using a bidirectional LSTM. Then, we separately process each sentence of the article with BiLSTMH , a BiLSTM conditioned on the last states of the BiLSTM which processed the headline. We finally stack BiLSTMA on top of BiLSTMH . In this way, we obtain a matrix H Si ∈ Rl×si for each sentence Si . 43 the final vector representation of the ith sentence Si is obtained as follows: Following Wang et al. (2018), we notate this as: HSi = Bi-LSTMH (ESi ) ∀i ∈ {1, ..., n} (1) uit = tanh(Ws hit + bs ) H Si = Bi-LSTMA (HSi ) ∀i ∈ {1, ..., n} (2) si = H Hi = HH Ai H Si = ReLU(Ws 3.3 (8) Decoding Following the inverted pyramid principles, according to which the most relevant information is concentrated at the beginning of the article, we aggregate the sentence vector representations {s1 , ..., sn } using a backward LSTM. The final prediction yˆ is finally obtained with a softmax operation over the tagset. 4 Experimental Setup 4.1 Data and Preprocessing We downloaded the F NC -1 corpus from the challenge we"
W18-5507,S14-2003,1,0.716238,"taset collecting long documents, we briefly mention some of the most relevant works on SD using Twitter data. 1. We identify asymmetry in length between headlines and articles as a key characteristic of the FNC-1 corpus: on average, an article contains more than 30 times the number of words contained in its associated headline. This is peculiar with respect to most of the commonly used datasets for stance detection (Mohammad et al., 2017) and require the development of architectures specifically tailored to this considerable asymmetry. Following on the terminology introduced by Jurgens et al. (2014) for Semantic Similarity, we propose to handle the problem as a CrossLevel Stance Detection task. To our knowledge, it is the first time that this task is investigated in isolation. 2. Inspired by theoretical principles in the field of Journalism Studies, we propose two simple neural architectures to model the argumentative structure of an article, and its complex interplay with a headline. We demonstrate that our systems can beat a strong feature-based baseline, based on one of the F NC -1 winning architectures, and that they can successfully model the internal structure of a news article and"
W18-5507,P17-2067,0,0.0356218,"hreshold of 4 considered sentences, simulations using forward encoding perform always consistently worse than using backwards encoding (the red violins in Figure 5). Reasonably, below this threshold, we do not observe a considerable difference in performance between backward and forward models. 5.2 5.2.1 5.2.2 Anonymizing the input After manual analysis of the predictions, we suspected that some models could have spotted some correlations between certain Named Entities and a specific stance in the training set. Some of those correlations are well known and can be useful in veracity detection (Wang, 2017). In this paper, however, we wanted to train a model for stance detection only based on its language understanding, without counting on such possibly accidental correlations. In order to avoid the systems to rely on chance correlations, which would not generalize on the test set, we modified the input sequences by substituting all input tokens labeled as <PERSON&gt;, <ORGANIZATION&gt; and <LOCATION&gt; by the Stanford Named Entity Recognizer with the corresponding NE tags. Additional Experiments Using additional Input Channels To investigate the impact of features other than word embeddings, we conside"
W18-5507,C18-1288,0,0.0444741,"Missing"
W18-5507,N16-1174,0,0.0297861,"Missing"
W18-5507,N16-1030,0,0.0214577,"ERSON&gt;, <ORGANIZATION&gt; and <LOCATION&gt; by the Stanford Named Entity Recognizer with the corresponding NE tags. Additional Experiments Using additional Input Channels To investigate the impact of features other than word embeddings, we consider two further input channels: • Named Entities (NE) - NEs were obtained using the Stanford NE Recognizer (Finkel et al., 2005), resulting in a tagset of 13 labels. • Characters - Each input word was split into characters. Only characters occurring more than 100 times in the training set were considered, obtaining a final vocabulary of 149 characters. As in Lample et al. (2016), in we concatenate the output of a BiLSTM run over the character sequence. 5.2.3 Results Results of experiments concatenating the previously mentioned features to the word embedding input to both architectures are reported in Table 4 (even lines). In general, using NE embeddings alone with word embeddings was not beneficial for both models. Considering the architecture based on double-conditional encoding, using both 47 Acknowledgments characters and NE features actually lead to (sometimes small) improvements in almost all considered evaluation metrics. Moving to the architecture using co-mat"
W18-5507,C16-1230,0,0.0152684,"This style is particularly suited for rapidly evolving breaking news event, where a journalist can update an article by attaching a new paragraph with the last updates at the beginning of it. Moreover, putting most newsworthy facts at the beginning of an article allows the impatient readers to quickly decide on their level of interest in the report. After manual analysis of excerpts of the F NC -1 corpus, we concluded that most articles were actuRumor Stance Detection on Tweets. The most commonly used datasets for rumor stance detection, the RumorEval (Derczynski et al., 2017) and the PHEME (Zubiaga et al., 2016b) corpora, collect Tweets. State-of-the-art results on the PHEME corpus has been obtained by Aker et al. (2017), who used a very rich set of problemspecific features. Their model beat the previous state-of-the-art system by Zubiaga et al. (2016a), 42 HEADLINE Sentence 1 w4 Sentence 1 w3 ARTICLE Backward LSTM conditioned on the previous sentence Forward LSTM conditioned on the previous sentence w1 w2 Input features (word embeddings, ...) w3 .... Sentence 2 w2 Sentence 2 ARTICLE w1 LSTM conditioned on the headline LSTM conditioned on the previous sentence Sentence n .... w1 w2 w3 w4 Figure 4: D"
W18-5507,S16-1003,0,0.0899858,"Missing"
