2020.acl-main.644,D16-1203,0,0.181898,"of referring expression datasets, including our proposed Refer360° dataset. Refer360° poses a more challenging scenario where the system observes only a partial and dynamic FoV. Refer360° also has includes explicit alignments between intermediate instruction steps and human follower actions which can be used as an auxiliary evaluation metric or source of supervision. Second, unlike other datasets, the target locations in Refer360° are randomly distributed and thus may occur anywhere – not just on predetermined objects. As a result, target locations are less prone to bias (Devlin et al., 2015; Agrawal et al., 2016; Jabri et al., 2016; Goyal et al., 2016; Cirik et al., 2018b). These random locations lead to more linguistically complex instructions, as shown in our analyses – when instead annotators choose the target location, they are likely to be biased towards locations that are more easily described (e.g. on top of a named object). Table 1 shows a comparison of similar datasets. In the following section, we motivate Refer360° dataset in more detail. 2 Motivation The vision behind Refer360° is to build systems that perform localization of any point in 3D space, bringing us closer to human-like reasoni"
2020.acl-main.644,N18-2123,1,0.725918,"ation from a bathroom. ‘First, face the sink, then find the second drawer in the cabinet to your left. The pills should be inside that drawer behind the toothbrush.” Interpreting instruction sequences in order to locate targets in novel environments is challenging for AI systems (e.g. personal robots and self-driving cars). First, the system needs to ground the instructions into visual perception (Anderson et al., 2018b; Hu et al., 2019). This often requires identification of the mentioned object (Plummer et al., 2015) through physical relationships with surrounding objects (Hu et al., 2017b; Cirik et al., 2018a). Second, since human visual perception has limited field-of-view, instructions are often sequential: First, the correct FoV should be identified before searching for the final target. In many situations, the target location is not visually unique (e.g. in the middle of a plain wall), and several intermediate instructions are required. To study these challenges, we introduce a novel dataset, named Refer360°1 , for the task of localizing a target in 360° scenes given a sequence of instructions. Figure 1 presents an example scenario 1 The annotations, learning simulator, and annotation setup a"
2020.acl-main.644,P19-1655,0,0.0191985,"align with what the instruction describes. Please see Figure 2a to see the correct location of Waldo. Introduction Imagine a scenario in which you are asked to retrieve medication from a bathroom. ‘First, face the sink, then find the second drawer in the cabinet to your left. The pills should be inside that drawer behind the toothbrush.” Interpreting instruction sequences in order to locate targets in novel environments is challenging for AI systems (e.g. personal robots and self-driving cars). First, the system needs to ground the instructions into visual perception (Anderson et al., 2018b; Hu et al., 2019). This often requires identification of the mentioned object (Plummer et al., 2015) through physical relationships with surrounding objects (Hu et al., 2017b; Cirik et al., 2018a). Second, since human visual perception has limited field-of-view, instructions are often sequential: First, the correct FoV should be identified before searching for the final target. In many situations, the target location is not visually unique (e.g. in the middle of a plain wall), and several intermediate instructions are required. To study these challenges, we introduce a novel dataset, named Refer360°1 , for the"
2020.acl-main.644,D14-1086,0,0.0797505,"nging since it is harder to describe a location when we cannot readily refer to it with the name of an object. Refer360° consists of 17,137 instruction sequences with ground-truth actions to complete these instructions in 360° scenes. Refer360° has some unique characteristics which differentiate it from prior work. First, Refer360° allows the scene to be viewed through a partial FoV that can be dynamically changed as instructions are followed. This is in contrast with existing 360° scene-based datasets such as Touchdown-SDR (Chen et al., 2018) and 2D image-based referring expression datasets (Kazemzadeh et al., 2014; Hu et al., 2016; Mao et al., 2016), where the visual input is either fixed, corresponding to a holistic, oracle-like view, or consists of fixed, cardinal FoVs. The partial and dynamic FoV in Refer360° poses new challenges for language grounding (see Figure 2a, 2b, and 2c for an illustrative comparison). For instance, the mentioned objects may not be visible in the current FoV, and language may refer to the FoV itself. Further, since our annotators generate instructions while observing a partial and dynamic FoV, and do so for a follower whose first FoV will be initially located at random, the"
2020.acl-main.644,D18-1287,0,0.044938,"Missing"
2020.acl-main.737,W14-1604,0,0.0233103,"romanized Russian (translit) and Arabic (Arabizi). Our unsupervised WFST outperforms the unsupervised neural baseline on both languages. 2 Related work Prior work on informal transliteration uses supervised approaches with character substitution rules either manually defined or learned from automatically extracted character alignments (Darwish, 2014; Chalamandaris et al., 2004). Typically, such approaches are pipelined: they produce candidate transliterations and rerank them using modules encoding knowledge of the source language, such as morphological analyzers or wordlevel language models (Al-Badrashiny et al., 2014; Eskander et al., 2014). Supervised finite-state approaches have also been explored (Wolf-Sonkin et al., 2019; Hellsten et al., 2017); these WFST cascade models are similar to the one we propose, but they encode a different set of assumptions about the transliteration process due to being designed for abugida scripts (using consonant-vowel syllables as units) rather than alphabets. To our knowledge, there is no prior unsupervised work on this problem. Named entity transliteration, a task closely related to ours, is better explored, but there is little unsupervised work on this task as well. I"
2020.acl-main.737,P03-1006,0,0.0333261,"s the prior distribution on the emission model parameters through which we introduce human knowledge into the model. Our goal is to learn the parameters θ of the emission distribution with the transition parameters γ being fixed. We parameterize the emission and transition distributions as weighted finite-state transducers (WFSTs): Transition WFSA The n-gram weighted finitestate acceptor (WFSA) T represents a characterlevel n-gram language model of the language in the native script, producing the native alphabet character sequence o with the probability p(o; γ). We use the parameterization of Allauzen et al. (2003), with the states encoding conditioning history, arcs weighted by n-gram probabilities, and failure transitions representing backoffs. The role of T is to inform the model of what well-formed text in the original orthography looks like; its parameters γ are learned from a separate corpus and kept fixed during the rest of the training. Emission WFST The emission WFST S transduces the original script sequence o to a Latin sequence l with the probability p(l|o; θ). Since there can be multiple paths through S that correspond to the input-output pair (o, l), this probability is summed over all such"
2020.acl-main.737,W14-3612,0,0.104505,"construct the machine T ◦ S ◦ A(l) in the tropical semiring and run the shortest path algorithm to obtain the most probable path eˆ; the source sequence oˆ is read off the obtained path. 4 Datasets Here we discuss the data used to train the unsupervised model. Unlike Arabizi, which has been explored in prior work due to its popularity in the modern online community, a dataset of informally romanized Russian was not available, so we collect and partially annotate our own dataset from the Russian social network vk.com. Arabic We use the Arabizi portion of the LDC BOLT Phase 2 SMS/Chat dataset (Bies et al., 2014; Song et al., 2014), a collection of written informal conversations in romanized Egyptian Arabic annotated with their Arabic script representation. To prevent the annotators from introducing orthographic variation inherent to dialectal Arabic, compliance with the Conventional orthography for dialectal Arabic (CODA; Habash et al., 2012) is ensured. However, the effects of some of the normalization choices (e.g. expanding frequent abbreviations) would pose difficulties to our model. To obtain a subset of the data better suited for our task, we discard any instances which are not originally roma"
2020.acl-main.737,P02-1001,0,0.0431268,"vely increase the language model order as learning progresses. Once most of the emission WFST arcs have been pruned, we can afford to compose it with a larger language model WFST without the size of the resulting lattice rendering the computation impractical. The two steps of the EM algorithm are performed as follows: E-step At the E-step we compute the sufficient statistics for updating θ, which in our case would be the expected number of traversals of each of the emission WFST arcs. For ease of bookkeeping, we compute those expectations using finitestate methods in the expectation semiring (Eisner, 2002). Summing over all paths in the lattice is usually performed via shortest distance computation in log semiring; in the expectation semiring, we augment the weight of each arc with a basis vector, where the only non-zero element corresponds to the index of the emission edit operation associated with the arc (i.e. the input-output label pair). This way the shortest distance algorithm yields not only the marginal likelihood but also the vector of the sufficient statistics for the input sequence. To speed up the shortest distance computation, we shrink the lattice by limiting delay of all paths th"
2020.acl-main.737,W14-3901,0,0.0191207,") and Arabic (Arabizi). Our unsupervised WFST outperforms the unsupervised neural baseline on both languages. 2 Related work Prior work on informal transliteration uses supervised approaches with character substitution rules either manually defined or learned from automatically extracted character alignments (Darwish, 2014; Chalamandaris et al., 2004). Typically, such approaches are pipelined: they produce candidate transliterations and rerank them using modules encoding knowledge of the source language, such as morphological analyzers or wordlevel language models (Al-Badrashiny et al., 2014; Eskander et al., 2014). Supervised finite-state approaches have also been explored (Wolf-Sonkin et al., 2019; Hellsten et al., 2017); these WFST cascade models are similar to the one we propose, but they encode a different set of assumptions about the transliteration process due to being designed for abugida scripts (using consonant-vowel syllables as units) rather than alphabets. To our knowledge, there is no prior unsupervised work on this problem. Named entity transliteration, a task closely related to ours, is better explored, but there is little unsupervised work on this task as well. In particular, Ravi and K"
2020.acl-main.737,habash-etal-2012-conventional,0,0.0921315,"Missing"
2020.acl-main.737,W17-4002,0,0.0187438,"2 Related work Prior work on informal transliteration uses supervised approaches with character substitution rules either manually defined or learned from automatically extracted character alignments (Darwish, 2014; Chalamandaris et al., 2004). Typically, such approaches are pipelined: they produce candidate transliterations and rerank them using modules encoding knowledge of the source language, such as morphological analyzers or wordlevel language models (Al-Badrashiny et al., 2014; Eskander et al., 2014). Supervised finite-state approaches have also been explored (Wolf-Sonkin et al., 2019; Hellsten et al., 2017); these WFST cascade models are similar to the one we propose, but they encode a different set of assumptions about the transliteration process due to being designed for abugida scripts (using consonant-vowel syllables as units) rather than alphabets. To our knowledge, there is no prior unsupervised work on this problem. Named entity transliteration, a task closely related to ours, is better explored, but there is little unsupervised work on this task as well. In particular, Ravi and Knight (2009) propose a fully unsupervised version of the WFST approach introduced by Knight and Graehl (1998),"
2020.acl-main.737,chalamandaris-etal-2004-bypassing,0,0.149213,"also constructed from naturally occurring resources—improves the decoding accuracy on both languages. We compare the proposed unsupervised WFST model with a supervised WFST, an unsupervised neural architecture, and commercial systems for decoding romanized Russian (translit) and Arabic (Arabizi). Our unsupervised WFST outperforms the unsupervised neural baseline on both languages. 2 Related work Prior work on informal transliteration uses supervised approaches with character substitution rules either manually defined or learned from automatically extracted character alignments (Darwish, 2014; Chalamandaris et al., 2004). Typically, such approaches are pipelined: they produce candidate transliterations and rerank them using modules encoding knowledge of the source language, such as morphological analyzers or wordlevel language models (Al-Badrashiny et al., 2014; Eskander et al., 2014). Supervised finite-state approaches have also been explored (Wolf-Sonkin et al., 2019; Hellsten et al., 2017); these WFST cascade models are similar to the one we propose, but they encode a different set of assumptions about the transliteration process due to being designed for abugida scripts (using consonant-vowel syllables as"
2020.acl-main.737,W14-3629,0,0.167586,"tion are not included in the dataset and are presented for illustration only. Since informal transliteration is not standardized, converting romanized text back to its original orthography requires reasoning about the specific user’s transliteration preferences and handling many-to-one (Figure 2) and one-to-many (Figure 1) character mappings, which is beyond traditional rule-based converters. Although user behaviors vary, there are two dominant patterns in informal romanization that have been observed independently across different languages, such as Russian (Paulsen, 2014), dialectal Arabic (Darwish, 2014) or Greek (Chalamandaris et al., 2006): Phonetic similarity: Users represent source characters with Latin characters or digraphs associated with similar phonemes (e.g. m /m/ → m, l /l/ → l in Figure 2). This substitution method requires implicitly tying the Latin characters to a phonetic system of an intermediate language (typically, English). Visual similarity: Users replace source characters &gt; with similar-looking symbols (e.g. q /tSj / → 4, u /u/ → y in Figure 2). Visual similarity choices often involve numerals, especially when the corresponding source language phoneme has no English equiv"
2020.acl-main.737,N09-1069,0,0.0578616,"rs instead: (, lo) → (, l), (, o). 8311 ⇤o : ⇤l ⇤o : ⇤l ⇤o : ✏ 2 ⇤o : ⇤l ⇤o : ✏ 1 ✏ : ⇤l ⇤o : ⇤l ⇤o : ✏ 0 ✏ : ⇤l ⇤o : ⇤l ⇤o : ✏ 1 ✏ : ⇤l 2 ✏ : ⇤l summing over the weights of all paths through a lattice obtained by composing T ◦ S ◦ A(l). Here A(l) is an unweighted acceptor of l, which, when composed with a lattice, constrains all paths through the lattice to produce l as the output sequence. The expectation–maximization (EM) algorithm is commonly used to maximize marginal likelihood; however, the size of the lattice would make the computation prohibitively slow. We combine online learning (Liang and Klein, 2009) and curriculum learning (Bengio et al., 2009) to achieve faster convergence, as described in §3.3.1. 3.3.1 Unsupervised learning We use a version of the stepwise EM algorithm described by Liang and Klein (2009), reminiscent of the stochastic gradient descent in the space of the sufficient statistics. Training data is split into mini-batches, and after processing each minibatch we update the overall vector of the sufficient statistics µ and re-estimate the parameters based on the updated vector. The update is performed by interpolating between the current value of the overall vector and the ve"
2020.acl-main.737,W19-3114,0,0.0150612,"seline on both languages. 2 Related work Prior work on informal transliteration uses supervised approaches with character substitution rules either manually defined or learned from automatically extracted character alignments (Darwish, 2014; Chalamandaris et al., 2004). Typically, such approaches are pipelined: they produce candidate transliterations and rerank them using modules encoding knowledge of the source language, such as morphological analyzers or wordlevel language models (Al-Badrashiny et al., 2014; Eskander et al., 2014). Supervised finite-state approaches have also been explored (Wolf-Sonkin et al., 2019; Hellsten et al., 2017); these WFST cascade models are similar to the one we propose, but they encode a different set of assumptions about the transliteration process due to being designed for abugida scripts (using consonant-vowel syllables as units) rather than alphabets. To our knowledge, there is no prior unsupervised work on this problem. Named entity transliteration, a task closely related to ours, is better explored, but there is little unsupervised work on this task as well. In particular, Ravi and Knight (2009) propose a fully unsupervised version of the WFST approach introduced by K"
2020.acl-main.737,D17-1266,0,0.0202279,"c substitutions), many of the informally romanized sequences would still not conform to its pronunciation rules: the transliteration process is characterlevel rather than phoneme-level and does not take possible TL digraphs into account (e.g. Russian sh /sx/ → sh), and it often involves eclectic visual substitution choices such as numerals or punctua8309 tion (e.g. Arabic  [tHt, ‘under’]3 → ta7t, Russian dl [dlja, ‘for’] → dl9 |). Finally, another relevant task is translating between closely related languages, possibly written in different scripts. An approach similar to ours is proposed by Pourdamghani and Knight (2017). They also take an unsupervised decipherment approach: the cipher model, parameterized as a WFST, is trained to encode the source language character sequences into the target language alphabet as part of a character-level noisy-channel model, and at decoding time it is composed with a word-level language model of the source language. Recently, the unsupervised neural architectures (Lample et al., 2018, 2019) have also been used for related language translation and similar decipherment tasks (He et al., 2020), and we extend one of these neural models to our characterlevel setup to serve as a b"
2020.acl-main.737,N09-1005,0,0.0990535,"Missing"
2020.acl-main.737,P12-3011,0,0.0260711,"Missing"
2020.acl-main.737,song-etal-2014-collecting,0,0.0616714,"Missing"
2020.acl-main.737,chalamandaris-etal-2006-greek,0,\N,Missing
2020.acl-main.737,J98-4003,0,\N,Missing
2020.emnlp-main.122,S14-2010,0,0.29394,"Missing"
2020.emnlp-main.122,S16-1081,0,0.28624,"Missing"
2020.emnlp-main.122,S13-1004,0,0.0548868,"Missing"
2020.emnlp-main.122,S12-1051,0,0.579735,"Conneau et al., 2017). More recently, 1 Code and data to replicate results available at https: //www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b;"
2020.emnlp-main.122,D15-1075,0,0.0500476,"ding. The first way is to concatenate the hidden states for the CLS token in the last four layers. The second way is to concatenate the hidden states of all word tokens in the last four layers and mean pool these representations. Both methods result in a 4096 dimension embedding. We also compare to the newly released model, Sentence2 Note that in all experiments using BERT, including Sentence-BERT, the large, uncased version is used. 1584 Bert (Reimers and Gurevych, 2019). This model is similar to Infersent (Conneau et al., 2017) in that it is trained on natural language inference data, SNLI (Bowman et al., 2015). However, instead of using pretrained word embeddings, they fine-tune BERT in a way to induce sentence embeddings.3 Models from the Literature (Trained on Our Data) These models are amenable to being trained in the exact same setting as our own models as they only require parallel text. These include the sentence piece averaging model, SP, from Wieting et al. (2019b), which is among the best of the averaging models (i.e. compared to averaging only words or character n-grams) as well the LSTM model, B I LSTM, from Wieting and Gimpel (2017). These models use a contrastive loss with a margin. Fo"
2020.emnlp-main.122,S17-2001,0,0.0499638,"Missing"
2020.emnlp-main.122,D18-2029,0,0.193693,"for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially comb"
2020.emnlp-main.122,N19-1254,0,0.12069,"ord embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, and to the best of our knowledge, this has not been explored in prior work. Specifically, we propose a deep generati"
2020.emnlp-main.122,W19-4330,0,0.0141189,"ing techniques use word embedding averaging (Wieting et al., 2016b), character n-grams (Wieting et al., 2016a), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, an"
2020.emnlp-main.122,L18-1269,0,0.120763,"ilarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), where the goal is to accurately predict the degree to which two sentences have the same meaning as measured by human judges. The evaluation metric Results We define symmetric word error rate for sentences s1 and s2 as 12 W ER(s1 , s2 ) + 12 W ER(s2 , s1 ), since word error rate (WER) is an asymmetric measure. 8 STS scores are between 0 and 5. 9 We selected examples for the negation split where one sentence contained not or ’t and the other did not. 10 We obtained values for STS 2012-2016 from prior works using SentEval (Conneau and Kiela, 2018). Note that we include all datasets for the 2013 competition, including SMT, which is not included in SentEval. 1586 Model BERT (CLS) BERT (Mean) Infersent GenSen USE LASER Sentence-BERT SP B I LSTM E NGLISH AE E NGLISH VAE E NGLISH T RANS B ILINGUALT RANS BGT W / O L ANG VARS BGT W / O P RIOR BGT Model LASER B ILINGUALT RANS BGT W / O L ANG VARS BGT W / O P RIOR BGT 2012 33.2 48.8 61.1 60.7 61.4 63.1 66.9 68.4 67.9 60.2 59.5 66.5 67.1 68.3 67.6 68.9 2013 29.6 46.5 51.4 50.8 59.0 47.0 63.2 60.3 56.4 52.7 54.0 60.7 61.0 61.3 59.8 62.2 es-es 79.7 83.4 81.7 84.5 85.7 Semantic Textual Similarity ("
2020.emnlp-main.122,D17-1070,0,0.362755,"experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.1 1 Introduction Learning useful representations of language has been a source of recent success in natural language processing (NLP). Much work has been done on learning representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Kiros et al., 2015; Conneau et al., 2017). More recently, 1 Code and data to replicate results available at https: //www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)),"
2020.emnlp-main.122,P18-1198,0,0.0503594,"Missing"
2020.emnlp-main.122,C04-1051,0,0.222341,"//www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data a"
2020.emnlp-main.122,N13-1092,0,0.121991,"Missing"
2020.emnlp-main.122,N18-1202,0,0.0490474,"ins on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.1 1 Introduction Learning useful representations of language has been a source of recent success in natural language processing (NLP). Much work has been done on learning representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Kiros et al., 2015; Conneau et al., 2017). More recently, 1 Code and data to replicate results available at https: //www.cs.cmu.edu/˜jwieting. deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wiet"
2020.emnlp-main.122,N16-1162,0,0.0635097,"Missing"
2020.emnlp-main.122,D18-2012,0,0.0128595,"S, but it includes a prior over the embedding space and therefore a KL loss term. This model differs from BGT since it does not have any language-specific variables. • BGT W / O P RIOR: Follows the same architecture as BGT, but without the priors and KL loss term. 4.2 Experimental Settings The training data for our models is a mixture of OpenSubtitles 20184 en-fr data and en-fr Gigaword5 data. To create our dataset, we combined the complete corpora of each dataset and then randomly selected 1,000,000 sentence pairs to be used for training with 10,000 used for validation. We use sentencepiece (Kudo and Richardson, 2018) with a vocabulary size of 20,000 to segment the sentences, and we chose sentence pairs whose sentences are between 5 and 100 tokens each. In designing the model architectures for the encoders and decoders, we experimented with Transformers and LSTMs. Due to better performance, we use a 5 layer Transformer for each of the encoders and a single layer decoder for each of the decoders. This design decision was empirically motivated as we found using a larger decoder was slower and worsened performance, but conversely, adding more encoder layers improved performance. More discussion of these trade"
2020.emnlp-main.122,D19-1437,1,0.826884,"we expect that in a well-trained model this variable will encode semantic, syntactic, or stylistic information shared across both sentences, while zf r and zen will handle any language-specific peculiarities or specific stylistic decisions that are less central to the sentence meaning and thus do not translate across sentences. In the following section, we further discuss how this is explicitly encouraged by the learning process. Decoder Architecture. Many latent variable models for text use LSTMs (Hochreiter and Schmidhuber, 1997) as their decoders (Yang et al., 2017; Ziegler and Rush, 2019; Ma et al., 2019). However, state-of-the-art models in neural machine translation have seen increased performance and speed using deep Transformer architectures. We also found in our experiments (see Appendix C for details) that Transformers led to increased performance in our setting, so they are used in our main model. We use two decoders in our model, one for modelling p(xf r |zsem , zf r ; θ) and one for modeling p(xen |zsem , zen ; θ) (see right side of Figure 2). Each decoder takes in a language variable and a semantic variable, which are concatenated and used by the decoder for reconstruction. We explor"
2020.emnlp-main.122,D14-1162,0,0.0977629,"Missing"
2020.emnlp-main.122,D19-1410,0,0.306073,"as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially combined with additional tasks li"
2020.emnlp-main.122,P18-2037,0,0.338404,"rchitecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially combined with additional tasks like constituency parsing (Subramanian et al., 2018). Surprisingly, despite ample testing of more powerful architectures, the best performing models for many sentence embedding tasks related to semantic similarity often use simple architectures that are mostly agnostic to the interactions between 1581 Proceedings of the 2020 Conference on Empirical Methods in Natur"
2020.emnlp-main.122,W17-2619,0,0.129934,"ata for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previous"
2020.emnlp-main.122,D16-1157,1,0.849436,"; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018; Artetxe and Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) potentially combined with additional tasks like constituency parsing (Subramanian et al.,"
2020.emnlp-main.122,P19-1427,1,0.884171,"2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et"
2020.emnlp-main.122,P17-1190,1,0.843564,"hat it is trained on natural language inference data, SNLI (Bowman et al., 2015). However, instead of using pretrained word embeddings, they fine-tune BERT in a way to induce sentence embeddings.3 Models from the Literature (Trained on Our Data) These models are amenable to being trained in the exact same setting as our own models as they only require parallel text. These include the sentence piece averaging model, SP, from Wieting et al. (2019b), which is among the best of the averaging models (i.e. compared to averaging only words or character n-grams) as well the LSTM model, B I LSTM, from Wieting and Gimpel (2017). These models use a contrastive loss with a margin. Following their settings, we fix the margin to 0.4 and tune the number of batches to pool for selecting negative examples from {40, 60, 80, 100}. For both models, we set the dimension of the embeddings to 1024. For B I LSTM, we train a single layer bidirectional LSTM with hidden states of 512 dimensions. To create the sentence embedding, the forward and backward hidden states are concatenated and mean-pooled. Following Wieting and Gimpel (2017), we shuffle the inputs with probability p, tuning p from {0.3, 0.5}. We also implicitly compare to"
2020.emnlp-main.122,P18-1042,1,0.882807,"mputational Linguistics words. For instance, some of the top performing techniques use word embedding averaging (Wieting et al., 2016b), character n-grams (Wieting et al., 2016a), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly,"
2020.emnlp-main.122,P19-1453,1,0.895546,"2018; Devlin et al., 2018) enabling state-of-theart results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful out-of-the-box for problems such as measurement of Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers and Gurevych, 2019), and averaging models (Wieting et al., 2016b; Arora et al., 2017) are capable of learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et"
2020.emnlp-main.122,2020.acl-demos.12,0,0.0347346,"bedding averaging (Wieting et al., 2016b), character n-grams (Wieting et al., 2016a), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting and Gimpel, 2018) or bilingual data (Wieting et al., 2019b; Chidambaram et al., 2019; Yang et al., 2020). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or languagespecific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, and to the best of our"
2020.emnlp-main.385,D19-1609,0,0.216749,"Missing"
2020.emnlp-main.385,P19-1635,0,0.276393,"ic variants: (1) masked number prediction (MNM), in which the goal is to predict the value of a masked number token in a sentence, and (2) numerical anomaly detection (NAD), with the goal of deciding whether a specific numeric value in a sentence is errorful or anomalous. In contrast with more standard MLM training setups, here we specifically care about the accuracy of the trained masked conditional distributions rather than the contextualized representations they induce. While successful models for these tasks are themselves useful in applications like typo correction and forgery detection (Chen et al., 2019), better models of numeracy are essential for further improving downstream tasks like question answering, numerical information extraction (Mirza et al., 2017; Saha et al., 2017) or numerical fact checking (Thorne and Vlachos, 2017), as well as for processing number-heavy domains like financial news, technical specifications, and scientific articles. Further, systems that detect anomalous numbers in text have applications in practical domains – for example, medicine (Thimbleby and Cairns, 2010) – where identification of numerical entry errors is critical. Our modeling approach to contextualize"
2020.emnlp-main.385,N19-1423,0,0.174933,"them with both recurrent and transformer-based encoder architectures. We evaluate these models on two numeric datasets in the financial and scientific domain. Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical prediction and anomaly detection. We also show that our models effectively utilize textual context and benefit from general-purpose unsupervised pretraining.1 1 Introduction Pretraining large neural architectures (e.g. transformers (Devlin et al., 2019; Raffel et al., 2019)) on vast amounts of unlabeled data has lead to great improvements on a variety of NLP tasks. Typically, such models are trained using a masked language modeling (MLM) objective and the resulting contextualized representations are finetuned for a particular downstream task like question answering or sentence classification (Devlin et al., 2019; Lan et al., 2020). In this paper, we focus on a 1 Code available at: https://github.com/dspoka/mnm Taylor Berg-Kirkpatrick UC San Diego tberg@ucsd.edu related modeling paradigm, but a different task. Specifically, we investigate co"
2020.emnlp-main.385,N19-1246,0,0.0941517,"Missing"
2020.emnlp-main.385,2020.acl-main.89,0,0.352218,"Missing"
2020.emnlp-main.385,2021.ccl-1.108,0,0.101177,"Missing"
2020.emnlp-main.385,2020.acl-main.447,0,0.0218834,"nce FinNews has many occurrences of dates and years, we also evaluate on a subset corpus, FinNews-$ , to measure effectiveness at modeling only dollar quantities in text. FinNews-$ is constructed exactly as FinNews , with the added requirement that the number is preceded by a dollar sign token ($). For all training and testing on FinNews-$ , we only predict dollar values. Academic papers Academic papers have diverse semantic quantities and measurements that make them an interesting challenge numeracy modeling. For this reason, we also use S2ORC, a newly constructed dataset of academic papers (Lo et al., 2020). We use the first 24,000 full text articles, randomly splitting into [20000, 2000, 2000] [train, valid, test] splits. 5 We refer to this dataset as Sci. All three datasets follow the same preprocessing discussed below and summary statistics are provided in Table 1. 3.1 Preprocessing Financial news, academic papers, and Wikipedia articles all have different style-guides that dictate how many digits of precision to use or whether certain quantities should be written out as words. While such stylistic queues might aid models in better predicting masked number strings, we are 4 www.kaggle.com/jee"
2020.emnlp-main.385,P17-2055,0,0.0161283,"etection (NAD), with the goal of deciding whether a specific numeric value in a sentence is errorful or anomalous. In contrast with more standard MLM training setups, here we specifically care about the accuracy of the trained masked conditional distributions rather than the contextualized representations they induce. While successful models for these tasks are themselves useful in applications like typo correction and forgery detection (Chen et al., 2019), better models of numeracy are essential for further improving downstream tasks like question answering, numerical information extraction (Mirza et al., 2017; Saha et al., 2017) or numerical fact checking (Thorne and Vlachos, 2017), as well as for processing number-heavy domains like financial news, technical specifications, and scientific articles. Further, systems that detect anomalous numbers in text have applications in practical domains – for example, medicine (Thimbleby and Cairns, 2010) – where identification of numerical entry errors is critical. Our modeling approach to contextualized number prediction combines two lines of past work. First, following Chen et al. (2019), we treat number prediction as a sentence-level MLM problem where onl"
2020.emnlp-main.385,P19-1329,0,0.118656,"Missing"
2020.emnlp-main.385,P17-2050,0,0.022852,"the goal of deciding whether a specific numeric value in a sentence is errorful or anomalous. In contrast with more standard MLM training setups, here we specifically care about the accuracy of the trained masked conditional distributions rather than the contextualized representations they induce. While successful models for these tasks are themselves useful in applications like typo correction and forgery detection (Chen et al., 2019), better models of numeracy are essential for further improving downstream tasks like question answering, numerical information extraction (Mirza et al., 2017; Saha et al., 2017) or numerical fact checking (Thorne and Vlachos, 2017), as well as for processing number-heavy domains like financial news, technical specifications, and scientific articles. Further, systems that detect anomalous numbers in text have applications in practical domains – for example, medicine (Thimbleby and Cairns, 2010) – where identification of numerical entry errors is critical. Our modeling approach to contextualized number prediction combines two lines of past work. First, following Chen et al. (2019), we treat number prediction as a sentence-level MLM problem where only numerical quantiti"
2020.emnlp-main.385,P18-1196,0,0.161774,"Missing"
2020.emnlp-main.385,P17-1015,0,0.0323734,"Missing"
2020.emnlp-main.385,E17-3010,0,0.021508,"c value in a sentence is errorful or anomalous. In contrast with more standard MLM training setups, here we specifically care about the accuracy of the trained masked conditional distributions rather than the contextualized representations they induce. While successful models for these tasks are themselves useful in applications like typo correction and forgery detection (Chen et al., 2019), better models of numeracy are essential for further improving downstream tasks like question answering, numerical information extraction (Mirza et al., 2017; Saha et al., 2017) or numerical fact checking (Thorne and Vlachos, 2017), as well as for processing number-heavy domains like financial news, technical specifications, and scientific articles. Further, systems that detect anomalous numbers in text have applications in practical domains – for example, medicine (Thimbleby and Cairns, 2010) – where identification of numerical entry errors is critical. Our modeling approach to contextualized number prediction combines two lines of past work. First, following Chen et al. (2019), we treat number prediction as a sentence-level MLM problem where only numerical quantities are masked. However, Chen et al. (2019) focused on"
2020.emnlp-main.385,D19-1534,0,0.150826,"Missing"
2020.emnlp-main.739,P19-1535,0,0.010743,". Uses Improper Persona 1. I make a million dollars a year. 2. I’m married and have three kids. 3. I’m a baseball player. I find it hard to support my family working at a bar. What about you? Null persona (∅) I enjoy my life. Table 8: Examples showing correct and incorrect persona choices in various dialog contexts by C OMPAC model. It shows that C OMPAC is capable of choosing a correct persona sentence (original or expanded) but sometimes the prior network fails to sample an appropriate one (third case). persona-grounded dialog generation performance (Wolf et al., 2019; Mazar´e et al., 2018; Bao et al., 2019) as well as persona consistency in generated dialog (Welleck et al., 2019; Li et al., 2019; Song et al., 2019a). Bao et al. (2019) proposed a reinforcement-learning-based framework that promoting informativeness and persona-consistency via personal knowledge exchange. Xu et al. (2020) focused on using plausible topical keywords related to the available persona facts using a neural topic model to explore beyond the given knowledge, possibly closest to our work. We rather focus on obtaining commonsense implications of the given persona in the form of text snippets that are more expressive than t"
2020.emnlp-main.739,K16-1002,0,0.0673639,"dom variable z is unobserved in the training data, we must marginalize over z to compute the desired likelihood p(x|H; θ, φ): log p(x|H; θ, φ) = log Ez∼pθ (z|H) [pφ (x|z, H)]; where we drop C from the conditionals for simplicity. 1992) to train the inference network parameters α. However, the REINFORCE estimator often suffers from high variance. To reduce the variance, we found it useful to (1) use a moving average baseline (Zhao et al., 2011); and (2) regularize the prior network by penalizing the entropy of the output categorical distribution. To avoid KL mode collapse, we use KL-annealing (Bowman et al., 2016) where we linearly increase the weight of the KL term beginning from 0 to 1 as training progresses. Decoding At decoding time, we first sample k from the prior pθ (z|H, C), and then Ck is fed to the generator network. Following previous work (Wolf et al., 2019), we use nucleus sampling (Holtzman et al., 2020) (with p = 0.95) to decode the final response from the probabilities produced by the generator. We also found that high-temperature sampling from the prior often leads to more diverse generation. 5 Inference Network Note that the number of persona expansions is typically in the range 15025"
2020.emnlp-main.739,2020.acl-main.711,0,0.0144372,"a contextual knowledge selection that precedes the grounded dialog generation has been successfully applied in prior works (Parthasarathi and Pineau, 2018). Specifically, Zhao et al. (2017) and later Song et al. (2019b) proposed a conditionalVAE framework to learn latent context given the dialog history to guide knowledge selection. Finally, few recent works focused on augmenting grounding with commonsense knowledge with successful applications in open-domain topical dialog generation (Ghazvininejad et al., 2018; Moon et al., 2019), story generation (Mao et al., 2019) and sarcasm generation (Chakrabarty et al., 2020). In this work, we extend this effort into persona-grounded dialog generation via augmenting grounding persona with commonsense knowledge. 7 Conclusion In this work, we showed that expanding persona sentences with commonsense helps a dialog model to generate high-quality and diverse personagrounded responses. Moreover, we found that finegrained persona grounding is crucial to effectively condition on a large pool of commonsense persona expansions, which further provided additional controllability in conditional generation. While our expansions are limited by the performance of COMET or paraphr"
2020.emnlp-main.739,N16-1014,0,0.0736687,"OMPAC-revised and C OM PAC -paraphrase. By default, C OMPAC indicates it is trained with COMET expansions. 5.2 Comparison of Dialog Quality We measure perplexity for language modeling performance, and BLEU-1 (Papineni et al., 2002) and BLEU-2 (Vedantam et al., 2015) scores between generated and gold utterances to measure the fidelity of the generated responses. Given our goal of generating engaging responses with novel information, we deem it important to consider the diversity in the generated responses which we measure using D-1 and D-2 (percentage of distinct uniand bi-grams respectively) (Li et al., 2016). Table 2 shows that C OMPAC outperforms three competitive baselines when trained on the original persona in all quality metrics indicating the efficacy of our architecture. Moreover, when combined with persona expansions, we observe a modest 3-8 point decrease in perplexity and a large improvement in both BLEU and diversity scores which confirms that C OMPAC successfully leverages the persona expansions to improve dialog quality. C OM PAC trained with COMET expansions achieves the best performance both in terms of fidelity and diversity which shows that COMET expansions help the model to resp"
2020.emnlp-main.739,D16-1230,0,0.0481216,"provide marginal performance gains, mirroring the observation from (Zhang et al., 2018). Finally we observe gradual degradation in performance when we trivially finetune GPT2 with paraphrase and COMET expansions. Note that GPT-2 could have implicitly learned to focus on a single persona attribute. However, the proposed C OM - 9199 PAC model performs better suggesting that finegrained persona grounding acts as a useful inductive bias in effectively utilizing larger expansion sets. 5.3 Human Evaluation for Dialog Generation Automatic evaluation of dialog systems is still notoriously unreliable (Liu et al., 2016; Novikova et al., 2017) and such systems should be evaluated by human users. Hence, we perform pairwise comparisons between responses generated our best system, C OMPAC trained on COMET expansions, and responses generated by four strong baselines: GPT2, GPT2-COMET, C OMPAC-original, C OM PAC -paraphrase (the best C OMPAC model with paraphrase expansions). We also consider the gold responses for comparison. We conduct a human evaluation with 100 test examples on three aspects critical for practical use: (1) Fluency measures whether the generated output is fluent (in English); (2) Engagement me"
2020.emnlp-main.739,2021.ccl-1.108,0,0.141105,"Missing"
2020.emnlp-main.739,D19-5503,0,0.0168945,"sentence. 3.2 Paraphrasing To explore alternative sources for generating commonsense expansions beyond COMET, we consider paraphrasing persona sentences. Paraphrases of a sentence convey almost the same meaning to a listener as the original. Often paraphrases use synonymous phrases or manipulate word-syntax of the original sentence, which implicitly involves both context comprehension and world knowledge (Zeng et al., 2019). We obtain these in two ways: Paraphrase Network To generate paraphrases at scale, we use an off-the-shelf paraphrasing system based on back-translation (Xie et al., 2019; Federmann et al., 2019) with pre-trained language translation models. We make use of En-Fr and Fr-En pre-trained translation models as the components for back-translation.2 While we tried other language pairs, the En-Fr pair proved the most satisfactory based on qualitative analysis on 500 samples. We generate 5 paraphrases per persona sentence, which readily provides more lexical and syntactic variants as shown in Figure 2b. Manual Paraphrasing To compare with other expansions, we reuse manually written revised versions of persona sentences provided with P ERSONA -C HAT (Zhang et al., 2018) though these are limited"
2020.emnlp-main.739,D19-1615,1,0.806946,"et al., 2018), etc. For dialog generation, a contextual knowledge selection that precedes the grounded dialog generation has been successfully applied in prior works (Parthasarathi and Pineau, 2018). Specifically, Zhao et al. (2017) and later Song et al. (2019b) proposed a conditionalVAE framework to learn latent context given the dialog history to guide knowledge selection. Finally, few recent works focused on augmenting grounding with commonsense knowledge with successful applications in open-domain topical dialog generation (Ghazvininejad et al., 2018; Moon et al., 2019), story generation (Mao et al., 2019) and sarcasm generation (Chakrabarty et al., 2020). In this work, we extend this effort into persona-grounded dialog generation via augmenting grounding persona with commonsense knowledge. 7 Conclusion In this work, we showed that expanding persona sentences with commonsense helps a dialog model to generate high-quality and diverse personagrounded responses. Moreover, we found that finegrained persona grounding is crucial to effectively condition on a large pool of commonsense persona expansions, which further provided additional controllability in conditional generation. While our expansions"
2020.emnlp-main.739,D18-1443,0,0.0236164,"Missing"
2020.emnlp-main.739,D18-1298,0,0.0376038,"Missing"
2020.emnlp-main.739,P19-1081,0,0.0201122,"t al., 2018), summarization (Gehrmann et al., 2018), etc. For dialog generation, a contextual knowledge selection that precedes the grounded dialog generation has been successfully applied in prior works (Parthasarathi and Pineau, 2018). Specifically, Zhao et al. (2017) and later Song et al. (2019b) proposed a conditionalVAE framework to learn latent context given the dialog history to guide knowledge selection. Finally, few recent works focused on augmenting grounding with commonsense knowledge with successful applications in open-domain topical dialog generation (Ghazvininejad et al., 2018; Moon et al., 2019), story generation (Mao et al., 2019) and sarcasm generation (Chakrabarty et al., 2020). In this work, we extend this effort into persona-grounded dialog generation via augmenting grounding persona with commonsense knowledge. 7 Conclusion In this work, we showed that expanding persona sentences with commonsense helps a dialog model to generate high-quality and diverse personagrounded responses. Moreover, we found that finegrained persona grounding is crucial to effectively condition on a large pool of commonsense persona expansions, which further provided additional controllability in conditio"
2020.emnlp-main.739,D17-1238,0,0.014124,"erformance gains, mirroring the observation from (Zhang et al., 2018). Finally we observe gradual degradation in performance when we trivially finetune GPT2 with paraphrase and COMET expansions. Note that GPT-2 could have implicitly learned to focus on a single persona attribute. However, the proposed C OM - 9199 PAC model performs better suggesting that finegrained persona grounding acts as a useful inductive bias in effectively utilizing larger expansion sets. 5.3 Human Evaluation for Dialog Generation Automatic evaluation of dialog systems is still notoriously unreliable (Liu et al., 2016; Novikova et al., 2017) and such systems should be evaluated by human users. Hence, we perform pairwise comparisons between responses generated our best system, C OMPAC trained on COMET expansions, and responses generated by four strong baselines: GPT2, GPT2-COMET, C OMPAC-original, C OM PAC -paraphrase (the best C OMPAC model with paraphrase expansions). We also consider the gold responses for comparison. We conduct a human evaluation with 100 test examples on three aspects critical for practical use: (1) Fluency measures whether the generated output is fluent (in English); (2) Engagement measures whether the gener"
2020.emnlp-main.739,D18-1436,1,0.8838,"Missing"
2020.emnlp-main.739,P02-1040,0,0.106663,"hoice of fine-grained persona grounding for an effective utilization of persona expansions, we also consider baseline versions of GPT2 trained with each of the expansion strategies: GPT2-revised, GPT2-paraphase, and GPT2-COMET. To show that C OMPAC can work with persona expansions derived from various sources, we compare with versions of C OMPAC trained with paraphrasebased expansions: C OMPAC-revised and C OM PAC -paraphrase. By default, C OMPAC indicates it is trained with COMET expansions. 5.2 Comparison of Dialog Quality We measure perplexity for language modeling performance, and BLEU-1 (Papineni et al., 2002) and BLEU-2 (Vedantam et al., 2015) scores between generated and gold utterances to measure the fidelity of the generated responses. Given our goal of generating engaging responses with novel information, we deem it important to consider the diversity in the generated responses which we measure using D-1 and D-2 (percentage of distinct uniand bi-grams respectively) (Li et al., 2016). Table 2 shows that C OMPAC outperforms three competitive baselines when trained on the original persona in all quality metrics indicating the efficacy of our architecture. Moreover, when combined with persona expa"
2020.emnlp-main.739,D18-1073,0,0.0642979,"Missing"
2020.emnlp-main.739,P17-1061,0,0.116931,"Missing"
2020.emnlp-main.739,N18-1198,0,0.0201818,"rsona-grounded dialog generation is a special case of knowledge-grounded dialog generation. Knowledge grounding in dialog has many realworld applications that are well-studied in recent literature (Zhou et al., 2018; Ghazvininejad et al., 2018; Dinan et al., 2019b; Lewis et al., 2019). In this work we use fine-grained grounding/selection on persona which performed better than encoding the entire persona for each response. Such fine-grained selection has been found useful in prior works on text generation such as dialog (Lian et al., 2019), image captioning (Jhamtani and BergKirkpatrick, 2018; Wang et al., 2018), summarization (Gehrmann et al., 2018), etc. For dialog generation, a contextual knowledge selection that precedes the grounded dialog generation has been successfully applied in prior works (Parthasarathi and Pineau, 2018). Specifically, Zhao et al. (2017) and later Song et al. (2019b) proposed a conditionalVAE framework to learn latent context given the dialog history to guide knowledge selection. Finally, few recent works focused on augmenting grounding with commonsense knowledge with successful applications in open-domain topical dialog generation (Ghazvininejad et al., 2018; Moon et al.,"
2020.emnlp-main.739,P19-1363,0,0.0364199,"ried and have three kids. 3. I’m a baseball player. I find it hard to support my family working at a bar. What about you? Null persona (∅) I enjoy my life. Table 8: Examples showing correct and incorrect persona choices in various dialog contexts by C OMPAC model. It shows that C OMPAC is capable of choosing a correct persona sentence (original or expanded) but sometimes the prior network fails to sample an appropriate one (third case). persona-grounded dialog generation performance (Wolf et al., 2019; Mazar´e et al., 2018; Bao et al., 2019) as well as persona consistency in generated dialog (Welleck et al., 2019; Li et al., 2019; Song et al., 2019a). Bao et al. (2019) proposed a reinforcement-learning-based framework that promoting informativeness and persona-consistency via personal knowledge exchange. Xu et al. (2020) focused on using plausible topical keywords related to the available persona facts using a neural topic model to explore beyond the given knowledge, possibly closest to our work. We rather focus on obtaining commonsense implications of the given persona in the form of text snippets that are more expressive than topical keywords. Persona-grounded dialog generation is a special case of"
2020.emnlp-main.739,P18-1205,0,0.256689,"t using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the P ERSONA C HAT dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation. 1 Figure 1: State-of-the-art models struggle to respond a user’s query, where generating an engaging response depends on commonsense reasoning. Introduction Persona-grounded dialog generation is a ‘chit-chat’ dialog setup where a dialog agent is expected to communicate based on a given profile (Zhang et al., 2018). Many recent works have focused on a popular benchmark dataset for this task: P ERSONA C HAT (Zhang et al., 2018) that provides personas as a set of sentences along with each dialog (example in Figure 1). However, a careful analysis of state-of-the-art (SOTA) models reveals that they often struggle to respond to contexts that do not closely match given persona sentences, even when the implications might be obvious to a human. For example, in Figure 1, the user asks an indirect question to the bot related to one of its persona sentences: I am an animal activist. SOTA1, which concatenates all p"
2020.findings-emnlp.325,W18-2706,0,0.0620801,"Missing"
2020.findings-emnlp.325,P19-1254,0,0.0845435,"discrete plan. 1 Figure 1: Our aim is to generate a story given a title. We propose models which first generate a high level story plan realized via a sequence of anchor words. Introduction Maintaining long-term narrative flow and consistency are important concerns when aiming to generate a plausible story (Porteous and Cavazza, 2009; Hou et al., 2019). Prior work on narrative text generation has focused on generating consistent stories via story outlines using keywords or key phrases (Xu et al., 2018; Yao et al., 2019), event-based representations (Riedl and Young, 2010; Martin et al., 2018; Fan et al., 2019), plot graphs (Li et al., 2013) or a sentence representing theme (Chen et al., 2019). Yao et al. (2019) note that compared to specific event based representations, using keywords to form the outline is more generalizable and widely applicable. In this work, we consider a sequence of anchor words as a means to model story outlines. For example, in Figure 1, given a story title ‘Winning the Race’, our model first predicts a sequence of anchor words which represents a high level story plan. Thereafter, a decoder conditions on the title and generated sequence of anchor words to generate the final"
2020.findings-emnlp.325,2020.acl-main.22,0,0.0247878,"d for amortized inference in prior work on summarization (Miao and Blunsom, 2016), though in a semi-supervised context. Non-monotonic sequence generation has been explored in past for tasks such as machine translation (Welleck et al., 2019). In the proposed model, the generation plan can be used to control the story via the anchor words. Hard and soft constraints for incorporating keywords into generation have been explored in Kiddon et al. (2016); Miao et al. (2019). Controllable text generation has been explored in other tasks as well, such as summarization (Fan et al., 2018), paraphrasing (Goyal and Durrett, 2020), style transfer (Keskar et al., 2019), and data-to-text generation (Shen et al., 2019). 6 Conclusion In this work we have proposed a deep latent variable model which induces a discrete sequence of anchor words as a high-level plan to guide story generation.2 We train the models though variational learning using a constrained inference network, and compare constrained and unconstrained versions of the decoder. The proposed model performs similarly or better than baselines on various automated and human evaluations. Related approaches might be used more broadly for a variety of language generat"
2020.findings-emnlp.325,N16-1147,0,0.321146,"ompared to the RAKE algorithm. Inference Network The latent plan model with no constraint on the inference network, LAP-U INF -U DEC, suffers from severe mode collapse and essentially ignores the plan. This demonstrates that constraining the inference network was useful in mitigating the posterior collapse issue. In preliminary experiments, we also observed that using a bag-of-words inference network instead of the BiLSTM leads to worse performance on perplexity, diversity and control, 4.8 Visual Storytelling Dataset We also conduct experiments with the text portion of a visual story dataset (Huang et al., 2016). The dataset consists of 40155, 4990, and 5055 stories in train, dev, and test splits. Compared to the ROC data, there are no titles associated with stories, and we learn unconditional anchor word sequence p(z). We train the best model configuration LAP-C INF U DEC (with constrained inference network and 3644 Model dev PPL↓ test DIV↑ plan story No Plan V IZ S TORY DATA N O P LAN -LM NA 38.5 NA 40.0 NA NA 8.9 6.3 With Plan S UPERV P LAN LAP-C INF -U DEC ≤41.5 ≤39.9 ≤42.2 ≤40.8 6.5 8.0 6.5 6.6 Table 7: Experiments with a second story dataset. We experiment with the text portion of the Visual St"
2020.findings-emnlp.325,D16-1031,0,0.180214,"ty, coherency, and controlFigure 2: Model Overview: We consider multisentence text generation via a latent generation plan realized through a sequence of anchor words with one word per sentence. [We show sequence models with first-order Markov assumption for simplicity, even though all sequence models in our approach are autoregressive with full context.] lable story generation as per various automatic and human evaluations. Finally, we note that our modelling approach for story generation has an interesting connection with work that treats text as a latent variable in deep generative models (Miao and Blunsom, 2016; Wen et al., 2017). We treat a latent sequence of anchor words as a form of hierarchical control over generated outputs, while related work treats the latent sequence itself as sequential text that is the output of the model. 2 Model Our goal is to generate a story x, consisting of multiple sentences x1 , x2 , ..xK , given a title t. Our model’s generative process is depicted in Figure 2 and operates as follows: First, a sequence of anchor words representing a generation plan is sampled from an auto-regressive prior conditioned on the title. Next, for each anchor word, a sentence is generated"
2020.findings-emnlp.325,N16-1098,0,0.0208316,"generated story, and highlight comparisons between various choices of inference networks and decoders. 4.1 Yao et al. (2019). Yao et al. (2019) had chosen a subset of the original ROC corpus in order to select only those stories which are accompanied by a title. The train, validation and test splits consist of 78529, 9816, and 9816 stories respectively. Most of the data consist of five sentence stories. Additionally, we experiment with the visual story dataset (only the text portion), which we discuss in more detail in Section 4.8. Dataset We use a subset of the ROC-stories corpus (ROCDATA ) (Mostafazadeh et al., 2016) used earlier by Methods N O P LAN -LM: This baseline does not consider any story generation plan and conditions only on the title. We use the same 3-layer LSTM as in the proposed model. S UPERV P LAN: This baseline is based on the work of (Yao et al., 2019) which utilizes RAKE-tagged keywords as observed anchor words. The model is trained to predict the the observed anchor words and the story given the title. We can view this baseline as a latent variable model that was trained using RAKE keywords as the output of a deterministic inference network. LAP: (1) We will refer to our model with the"
2020.findings-emnlp.325,2020.acl-main.646,0,0.0648516,"Missing"
2020.findings-emnlp.325,D19-1054,0,0.0271303,"a semi-supervised context. Non-monotonic sequence generation has been explored in past for tasks such as machine translation (Welleck et al., 2019). In the proposed model, the generation plan can be used to control the story via the anchor words. Hard and soft constraints for incorporating keywords into generation have been explored in Kiddon et al. (2016); Miao et al. (2019). Controllable text generation has been explored in other tasks as well, such as summarization (Fan et al., 2018), paraphrasing (Goyal and Durrett, 2020), style transfer (Keskar et al., 2019), and data-to-text generation (Shen et al., 2019). 6 Conclusion In this work we have proposed a deep latent variable model which induces a discrete sequence of anchor words as a high-level plan to guide story generation.2 We train the models though variational learning using a constrained inference network, and compare constrained and unconstrained versions of the decoder. The proposed model performs similarly or better than baselines on various automated and human evaluations. Related approaches might be used more broadly for a variety of language generation tasks, or even for related domains like music generation. Other modeling extensions"
2020.findings-emnlp.325,N18-1156,0,0.0276667,"tations are performed with 200 generated samples. We provide further example generations from various methods in the Appendix. 5 Related Work Prior work on story generation has largely focused on plot outline via keywords or key phrases (Yao et al., 2019; Xu et al., 2018), event-based representations (Martin et al., 2018; Fan et al., 2019), or a sentence theme (Chen et al., 2019). Liu et al. (2020) propose a method to generate a story conditioned on a character description. Prior work on narrative text generation with plans has mostly relied on external resources or tools to extract outlines (Zhou et al., 2018; Fan et al., 2019), and then training in a supervised manner. For example, using VADER (Hutto and Gilbert, 2014) to tag sentiment polarity (Luo et al.). Much prior work has used manually defined objectives to encourage coherence in generated text. In this context, reinforcement learning has been used to encourage stories to follow certain manually defined goals such as being locally coherent (Tambwekar et al., 2018; Xu et al., 2018). Prior work on visual story generation aim to learn topically coherent visual story generation (Huang et al., 2019; Wang et al., 2019). Compared to topics, keywor"
2020.findings-emnlp.325,2020.coling-main.204,0,0.0707045,"Missing"
2020.findings-emnlp.325,W19-3620,0,0.0616531,"Missing"
2020.findings-emnlp.325,D18-1462,0,0.154612,", the proposed model gets favorable scores when evaluated on perplexity, diversity, and control of story via discrete plan. 1 Figure 1: Our aim is to generate a story given a title. We propose models which first generate a high level story plan realized via a sequence of anchor words. Introduction Maintaining long-term narrative flow and consistency are important concerns when aiming to generate a plausible story (Porteous and Cavazza, 2009; Hou et al., 2019). Prior work on narrative text generation has focused on generating consistent stories via story outlines using keywords or key phrases (Xu et al., 2018; Yao et al., 2019), event-based representations (Riedl and Young, 2010; Martin et al., 2018; Fan et al., 2019), plot graphs (Li et al., 2013) or a sentence representing theme (Chen et al., 2019). Yao et al. (2019) note that compared to specific event based representations, using keywords to form the outline is more generalizable and widely applicable. In this work, we consider a sequence of anchor words as a means to model story outlines. For example, in Figure 1, given a story title ‘Winning the Race’, our model first predicts a sequence of anchor words which represents a high level story pl"
2020.nlp4musa-1.1,N18-2074,0,0.151583,"the next token (represented by a question mark) by attending to related subsequences appear previously. (1) and (2) show two potential alignments. The model assigns a larger weight (matching score) to key sequence (1) over (2) since key sequence (1) has strong relations (tonal sequence) with the query sequence and can help to predict the next token (E4 in this case). an explicit inductive bias for direct sequence-tosequence matching. Second, a multi-layer attention setting is required: the model needs to collect the sequential information using the positional embedding (Vaswani et al., 2017; Shaw et al., 2018) on the first layer, and then compare the sequential information on the subsequent layers. These problems make the model hard to train and require additional parameters, which may also harm the model’s generalization ability. In this paper, we propose the sequential attention module, a new attention module that explicitly models sequence-level music relations. In this module, we measure the similarity of two sequences by a token-wise comparison instead of the dynamic time warping approach (Walder and C4 D4 E4 C4 G4 Known Condition Known Samples Cond. Sample Known Condition A3 B3 C4 A3 ? Known"
2020.scil-1.43,C18-1135,0,0.0408748,"Missing"
2020.scil-1.43,D17-1118,0,0.0177716,"t applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shift (tracking changes in word sense or emergence of new senses) by comparing word embedding spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced earlier from the linguistic perspective, formalized in terms of distributional semantics. Hypothesis 1 Neologisms are more likely to emerge in sparser areas of the semantic space. This corresponds to the supply-driven neology hypothesis: we assume that areas of the space that contain fewer semantically related wor"
2020.scil-1.43,W16-2506,1,0.922812,"ord embeddings into the HISTORICAL space, we multiply them by the obtained rotation matrix R. 4.4 Control set selection To test our hypotheses, we collect an alternative set of words and analyze how certain statistical properties of their neighbors differ from those of neighbors of neologisms. At this stage it is important to control for non-semantic confounding factors that might affect the word distribution in the semantic space. One such factor is word frequency: it has been shown that embeddings of words of similar frequency tend to be closer in the embedding space (Schnabel et al., 2015; Faruqui et al., 2016), which results in very dense clusters, or hubs, of words with high cosine similarity (Radovanovi´c et al., 2010; Dinu et al., 2014). We choose to also restrict our control set to only include words that did not substantially grow or decline in frequency over the HISTORICAL period in order to prevent selecting counterparts that only share similar frequency in the MODERN subcorpus (e.g., due to recent topical relevance), but exhibit significant fluctuation prior to that period. In particular, we refrain from selecting words that emerged in language right before our HISTORI CAL - MODERN split. W"
2020.scil-1.43,P16-1141,0,0.143841,"s rather than modeling their lifecycle. We model language-external processes indirectly through their reflection in language, thereby capturing phenomena evident of our hypotheses through linguistic analysis. Distributional semantics and language change Word embeddings have been successfully used for different applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shift (tracking changes in word sense or emergence of new senses) by comparing word embedding spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced e"
2020.scil-1.43,P18-2032,0,0.0218419,"spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced earlier from the linguistic perspective, formalized in terms of distributional semantics. Hypothesis 1 Neologisms are more likely to emerge in sparser areas of the semantic space. This corresponds to the supply-driven neology hypothesis: we assume that areas of the space that contain fewer semantically related words are likely to give birth to new ones so as to fill in the ‘semantic gaps’. Word embeddings give us a natural way of formalizing this: since semantically related words have been shown to populate the same regions"
2020.scil-1.43,C18-1117,0,0.0134403,"their lifecycle. We model language-external processes indirectly through their reflection in language, thereby capturing phenomena evident of our hypotheses through linguistic analysis. Distributional semantics and language change Word embeddings have been successfully used for different applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shift (tracking changes in word sense or emergence of new senses) by comparing word embedding spaces across time periods (Kulkarni et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016; Kutuzov et al., 2018). Typically, embeddings are learned for discrete time periods and then aligned (but see Bamler and Mandt, 2017). There has also been work on revising the existing methodology, specifically accounting for frequency effects in embeddings when modeling semantic shift (Dubossarsky et al., 2017). Other related questions where distributional semantics proved useful were exploring the evolution of bias (Garg et al., 2018) and the degradation of age- and gender-predictive language models (Jaidka et al., 2018). 3 Hypotheses This section outlines the two hypotheses we introduced earlier from the linguis"
2020.scil-1.43,reese-etal-2010-wikicorpus,0,0.0316468,"Missing"
2020.scil-1.43,D15-1036,0,0.0235291,"r). To project MODERN word embeddings into the HISTORICAL space, we multiply them by the obtained rotation matrix R. 4.4 Control set selection To test our hypotheses, we collect an alternative set of words and analyze how certain statistical properties of their neighbors differ from those of neighbors of neologisms. At this stage it is important to control for non-semantic confounding factors that might affect the word distribution in the semantic space. One such factor is word frequency: it has been shown that embeddings of words of similar frequency tend to be closer in the embedding space (Schnabel et al., 2015; Faruqui et al., 2016), which results in very dense clusters, or hubs, of words with high cosine similarity (Radovanovi´c et al., 2010; Dinu et al., 2014). We choose to also restrict our control set to only include words that did not substantially grow or decline in frequency over the HISTORICAL period in order to prevent selecting counterparts that only share similar frequency in the MODERN subcorpus (e.g., due to recent topical relevance), but exhibit significant fluctuation prior to that period. In particular, we refrain from selecting words that emerged in language right before our HISTOR"
2020.scil-1.43,D18-1467,0,0.104121,", there are factors that predict where new words emerge other than the availability of semantic space. Demand, we hypothesize, plays a role as well as supply. Most existing computational research on the mechanisms of neology focuses on discovering sociolinguistic factors that predict acceptance of emerging words into the mainstream language and growth of their usage, typically in online social communities (Del Tredici and Fern´andez, 2018). The sociolinguistic factors can include geography (Eisenstein, 2017), user demographics (Eisenstein et al., 2012, 2014), diversity of linguistic contexts (Stewart and Eisenstein, 2018) or word form (Kershaw et al., 2016). To the best of our knowledge, there is no prior work focused on discovering factors predictive of the emergence of new words rather than modeling their lifecycle. We model language-external processes indirectly through their reflection in language, thereby capturing phenomena evident of our hypotheses through linguistic analysis. Distributional semantics and language change Word embeddings have been successfully used for different applications of the diachronic analysis of language (Tahmasebi et al., 2018). The closest task to ours is analyzing meaning shi"
2021.acl-short.74,P18-1154,1,0.526648,"I have a house there but it gets hot in summer. PABST (λd = 1): Yesterday while I was working, I jumped off a building and I got hurt. I had to be taken to the hospital. Table 3: Generations from different models. More examples are in Appendix §C. prior work (Li et al., 2016), we report the percentage of distinct uni-grams and bi-grams (D-1 and D2 respectively). Note that these values do not capture the actual frequency distribution of different word types. Therefore, we also report the geometric mean of entropy values of empirical frequency distributions of n-grams of words (n ∈ {1, 2, 3}) (Jhamtani et al., 2018), denoted by ENTR. We observe that methods that use story data show much higher diversity compared to methods that do not (Table 1). Among methods using story data, gradient-based decoding (PABST) performs better than D ISC C HOICE trained with P SEUDO or M ULTI TASK. Note that just using R ETRIEVAL outputs as-is leads to even more diverse outputs than PABST. However, they are much less sensible with the context, as shown in human evaluations. 3.2 Human Evaluation Since we do not have ground truth story-like responses in the dialog dataset, we perform human evaluation with 150 test examples to"
2021.acl-short.74,J82-2005,0,0.586393,"Missing"
2021.acl-short.74,D17-1018,0,0.0242989,"a persona attribute c, we aim to identify relevant stories from a story corpus. Toward this goal, we rank the stories using the F1 component of BERTscore (Zhang et al., 2020) based retrieval using the persona attribute c as the query and the highest scoring story is chosen. Note that many of the stories are written in the third person. For use as background stories, we must first transform them to first–person. Following prior work (Brahman and Chaturvedi, 2020), we identify the protagonist of such stories as the most frequently occurring character. Thereafter, we use co-reference resolution (Lee et al., 2017) to identify all words or phrases that refer to the protagonist. Finally, all words or phrases so identified are replaced with suitable first person pronouns (e.g. ‘his books’ to ‘my books’). 2.2 Given dialog history h and persona C consisting of several (typically 3-5, example shown in Figure 1) attributes, our goal is to construct a dialog response x. Our underlying model is based on the discrete persona attribute choice model from Majumder et al. (2020). To generate a dialog utterance x, we first sample a persona attribute c ∼ p(c|h) conditioned on the dialog history h. x is then generated"
2021.acl-short.74,N16-1014,0,0.0363918,"TI TASK: I started a new role. I got hurt. PABST (λd = 5): I went on a trip. I went to Florida. I have a house there. We do not have air condition. It was hot. PABST (λd = 5): I was working as a stunt double. I jumped off a building. I got hurt. I had to be rushed to the hospital. PABST (λd = 1): That’s great. I frequently go to Florida. I have a house there but it gets hot in summer. PABST (λd = 1): Yesterday while I was working, I jumped off a building and I got hurt. I had to be taken to the hospital. Table 3: Generations from different models. More examples are in Appendix §C. prior work (Li et al., 2016), we report the percentage of distinct uni-grams and bi-grams (D-1 and D2 respectively). Note that these values do not capture the actual frequency distribution of different word types. Therefore, we also report the geometric mean of entropy values of empirical frequency distributions of n-grams of words (n ∈ {1, 2, 3}) (Jhamtani et al., 2018), denoted by ENTR. We observe that methods that use story data show much higher diversity compared to methods that do not (Table 1). Among methods using story data, gradient-based decoding (PABST) performs better than D ISC C HOICE trained with P SEUDO or"
2021.acl-short.74,2020.acl-main.428,0,0.0313363,"sed on generating response which are consistent to given persona. Differently, we use a gradientbased decoding to generate a dialog response while honoring constraints such as consistency to persona and similarity to retrieved story. 5 Related Work A desired impact of the proposed approach is increase in diversity of the generated responses. To tackle the issue of diversity in dialog model outputs, prior work has focused on decoding strategies such as diversity-promoting sampling (Holtzman et al., 2020); training strategies such as discouraging undesirable responses via unlikelihood training (Li et al., 2020); model changes such as using stochastic variables (Serban et al., 2017); and using external data such as forum data (Su et al., 2020) or external knowledge bases (Majumder et al., 2020). In contrast to these, our proposed method generates responses with background stories using a gradientbased decoding approach. One of the steps in our proposed approach is to retrieve relevant stories from an external corpus. Prior work has explored using retrieval of similar dialog instances as an initial step in improving response diversity and other human-like desiderata in dialog (Roller et al., 2020; Wes"
2021.acl-short.74,2020.emnlp-main.739,1,0.914404,"existing corpus. We propose a gradient-based technique which encourages the generated response to be fluent with the dialog history, minimally different from the retrieved story, and consistent with the persona. The proposed approach leads to more specific and interesting responses. Introduction Humans often rely on specific incidents and experiences while conversing in social contexts (Dunbar et al., 1997). Responses from existing chitchat dialog agents often lack such specific details. To mitigate this, some prior work has looked into assigning personas to dialog agents (Zhang et al., 2018; Majumder et al., 2020). However, persona descriptions are often shallow and limited in scope, and while they lead to improvements response specificity, they still lack the level of detail with which humans share experiences. In this work, we propose methods to enrich dialog personas with relevant background events using fictional narratives from existing story datasets such as ROCStories (Mostafazadeh et al., 2016). For example, for a persona attribute ‘I have two children and a dog,’ we are able to identify a relevant narrative from a story corpus (Figure 1). However, such stories may not directly fit fluently in"
2021.acl-short.74,N16-1098,0,0.0776988,"Missing"
2021.acl-short.74,2020.emnlp-main.58,0,0.0633138,"Missing"
2021.acl-short.74,2020.acl-main.634,0,0.243212,"on the dialog history and persona. To incorporate the retrieved story in the response, we perform gradient-based inference (Section 2.2), that only assumes a left-toright language model trained on dialog context and responses, and the story is handled at decoding time in an unsupervised fashion. We refer to the proposed method as PABST (Unsupervised PersonA enrichment with Background STories). the remaining two constraints are incorporated via iterative updates to the decoder output distributions at inference time. Our inference-time decoding method is different from the only recent effort by Su et al. (2020) that leverages non-dialog data (forum comments, book snippets) as distant labels to train dialog systems with supervision. Our contributions can be summarized as follows: • We propose a novel approach to enrich dialog agent personas with relevant backstories, relying only on existing story datasets. • We propose to use an unsupervised backpropagation based decoding procedure1 to adapt the relevant stories such that the resulting response is fluent with the dialog history and consistent with the dialog agent persona. Our method works with a model trained just with dialog data i.e. without acce"
2021.acl-short.74,P19-1363,0,0.0227922,"e want to encourage x to be minimally different from the story s. Following prior work (Qin et al., 2020), we compute a cross entropy loss (denoted by cross-entr henceforth) with story s = {s1 , . . . , sT } tokens as labels and W o1 , . . . , W oT as the logits. (2) Consistency to persona: We want x to be consistent with persona attribute c. Consider a classifier qφ (o, c) which predicts the probability of x (or rather the soft representation o of x) entailing c. The classifier qφ (o, c) is a bag-of-words classification head on decoder hidden states o, fine-tuned on the Dialogue-NLI dataset (Welleck et al., 2019) to predict whether pairs of persona attributes and responses are entailed or not. The objective to maximize can be written as: L(c, s; o) = λc log qφ (o, c) − λd cross-entr(s, W o) where λc and λd are hyper-parameters. We update o through back-propagation by computing the gradient ∇o L(c, s; o), while keeping the model parameters constant. Let the resulting o after the gradient-based updates be denoted by ob . Forward Pass to Encourage Fluency Next we perform a forward pass of the underlying dialog model, with the goal of regularizing the hidden states towards the unmodified language model va"
2021.acl-short.74,W18-5713,0,0.0213273,"20); model changes such as using stochastic variables (Serban et al., 2017); and using external data such as forum data (Su et al., 2020) or external knowledge bases (Majumder et al., 2020). In contrast to these, our proposed method generates responses with background stories using a gradientbased decoding approach. One of the steps in our proposed approach is to retrieve relevant stories from an external corpus. Prior work has explored using retrieval of similar dialog instances as an initial step in improving response diversity and other human-like desiderata in dialog (Roller et al., 2020; Weston et al., 2018). Distant supervision by using retrieved text snippets as pseudo responses has been explored in prior work (Su et al., 2020; Roller et al., 2020). We use an external data source to improve dialog responses, a theme shared with some efforts in other tasks such as machine translation (Khandelwal et al.). The use of narrative text in dialog has been explored in prior work, mostly as a ‘script’ or template for conversation (Xu et al., 2020; Zhu et al., 2020). Conclusion We propose a method to enrich persona-grounded dialog with background stories at the inference time only using an existing corpus"
2021.acl-short.74,P18-1205,0,0.108566,"nd stories’ from an existing corpus. We propose a gradient-based technique which encourages the generated response to be fluent with the dialog history, minimally different from the retrieved story, and consistent with the persona. The proposed approach leads to more specific and interesting responses. Introduction Humans often rely on specific incidents and experiences while conversing in social contexts (Dunbar et al., 1997). Responses from existing chitchat dialog agents often lack such specific details. To mitigate this, some prior work has looked into assigning personas to dialog agents (Zhang et al., 2018; Majumder et al., 2020). However, persona descriptions are often shallow and limited in scope, and while they lead to improvements response specificity, they still lack the level of detail with which humans share experiences. In this work, we propose methods to enrich dialog personas with relevant background events using fictional narratives from existing story datasets such as ROCStories (Mostafazadeh et al., 2016). For example, for a persona attribute ‘I have two children and a dog,’ we are able to identify a relevant narrative from a story corpus (Figure 1). However, such stories may not d"
2021.emnlp-main.152,2021.acl-long.151,0,0.0856083,"Missing"
2021.emnlp-main.152,D19-1662,0,0.0547779,"Missing"
2021.emnlp-main.152,P19-1104,0,0.0277199,"comes (Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2017; Han et al., 2021; Mireshghallah et al., 2021b). While effective in many cases, such approaches do nothing to mitigate human bias in decisions based on text. Fundamentally, our framework is concerned with stylistic features of human-generated text. Thus, a large body of prior work on methods for unsupervised style transfer are related to our approach (Santos et al., 2018; Yang et al., 2018; Luo et al., 2019; He et al., 2020). There is also a vast body of work on style obfuscation (Emmery et al., 2018; Reddy and Knight, 2016; Bevendorff et al., 2019; Shetty et al., 2018). Our work is most closely related to Shetty et al. (2018) and Xu et al. (2019). A4NT (Shetty et al., 2018) attempts to obfuscate text style through automatic re-writing. However, their approach attempts to hide protected attributes simply by mapping the style of one protected category to that of another. In contrast, we seek not to map the author’s text to another author’s style, but to a central obfuscated style. Xu et al. propose Privacy Aware Text Re-writing (PATR), which takes a similar adversarial learning 5 Related Work translation based approach to address this pr"
2021.emnlp-main.152,D16-1120,0,0.0508808,"Missing"
2021.emnlp-main.152,2021.acl-long.81,0,0.0455717,"Missing"
2021.emnlp-main.152,K16-1002,0,0.0600094,"Missing"
2021.emnlp-main.152,D18-1002,0,0.123574,"ot, be biased by this information. sensitive attributes to text, in effect, computing A large body of prior work has attempted to a union of styles. Our style-obfuscation address algorithmic bias by modifying different framework can be used for multiple purposes, stages of the natural language processing (NLP) however, we demonstrate its effectiveness in pipeline. For example, Ravfogel et al. (2020) atimproving the fairness of downstream classitempt to de-bias word embeddings used by NLP sysfiers. We also conduct a comprehensive study on style pooling’s effect on fluency, semantic tems, while Elazar and Goldberg (2018) address the consistency, and attribute removal from text, in bias in learned model representations and encodings. two and three domain style obfuscation.1 While effective in many cases, such approaches do 1 Introduction nothing to mitigate bias in decisions made by humans based on text. We propose a fundamentally Machine learning (ML) algorithms are used in different approach. Rather than mitigating bias in a wide range of tasks, including high-stakes learning algorithms that make decisions based on applications like determining credit ratings, setting text, we propose a framework that obfusc"
2021.emnlp-main.152,C18-1084,0,0.0212451,"inference procedures to produce more fair outcomes (Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2017; Han et al., 2021; Mireshghallah et al., 2021b). While effective in many cases, such approaches do nothing to mitigate human bias in decisions based on text. Fundamentally, our framework is concerned with stylistic features of human-generated text. Thus, a large body of prior work on methods for unsupervised style transfer are related to our approach (Santos et al., 2018; Yang et al., 2018; Luo et al., 2019; He et al., 2020). There is also a vast body of work on style obfuscation (Emmery et al., 2018; Reddy and Knight, 2016; Bevendorff et al., 2019; Shetty et al., 2018). Our work is most closely related to Shetty et al. (2018) and Xu et al. (2019). A4NT (Shetty et al., 2018) attempts to obfuscate text style through automatic re-writing. However, their approach attempts to hide protected attributes simply by mapping the style of one protected category to that of another. In contrast, we seek not to map the author’s text to another author’s style, but to a central obfuscated style. Xu et al. propose Privacy Aware Text Re-writing (PATR), which takes a similar adversarial learning 5 Related W"
2021.emnlp-main.152,2020.emnlp-main.44,0,0.0579224,"Missing"
2021.emnlp-main.152,2021.findings-acl.41,0,0.0334953,"5 0.05 (2021a) and Sheng et al. (2019) propose and analyze benchmarks for evaluating fairness in different applications. Ravfogel et al. (2020), Kaneko and Bollegala (2019), Shin et al. (2020) and Kaneko and Bollegala (2021) attempt to de-bias word embeddings used by NLP systems, while Elazar and Goldberg (2018); Barrett et al. (2019); Wang et al. (2021) attempt to de-bias model representations and encodings. There is also a large body of work on modifying learning algorithms and inference procedures to produce more fair outcomes (Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2017; Han et al., 2021; Mireshghallah et al., 2021b). While effective in many cases, such approaches do nothing to mitigate human bias in decisions based on text. Fundamentally, our framework is concerned with stylistic features of human-generated text. Thus, a large body of prior work on methods for unsupervised style transfer are related to our approach (Santos et al., 2018; Yang et al., 2018; Luo et al., 2019; He et al., 2020). There is also a vast body of work on style obfuscation (Emmery et al., 2018; Reddy and Knight, 2016; Bevendorff et al., 2019; Shetty et al., 2018). Our work is most closely related to She"
2021.emnlp-main.152,P19-1160,0,0.0551331,"Missing"
2021.emnlp-main.152,2021.eacl-main.107,0,0.0683912,"Missing"
2021.emnlp-main.152,D16-1140,0,0.0226989,"s, which means we need to approximate their gradients. To address this, we use the Gumbel-softmax (Jang et al., 2017) straightthrough estimator to backpropagate gradients from both the KL and reconstruction loss terms. Length Control. During the training of the model, we observed that it tends to repeat the same word when it is trying to generate obfuscated text, yi . To mitigate this, we append two floating point length tokens to the input of the inference networks decoder at each step t, one of these tokens tells the model which step it is on, and the other tells it how many steps are left (Kikuchi et al., 2016; Hu et al., 2017). We also experimented with positional embeddings instead of floating point tokens, but we observed that they yield worse convergence. Another measure we take to encourage shorter sentences was to hard stop the decoding during training once the re-written sentence had the same length as the original sentence. To further stabilize training we share parameters between the inference network and the transduction models, appending an embedding to the input to indicate the output domain. 2.4 Style De-boosting To better encourage the removal of identifying stylistic features, we int"
2021.emnlp-main.152,2020.emnlp-main.602,0,0.0728107,"Missing"
2021.emnlp-main.152,2021.naacl-main.298,1,0.700419,"Sheng et al. (2019) propose and analyze benchmarks for evaluating fairness in different applications. Ravfogel et al. (2020), Kaneko and Bollegala (2019), Shin et al. (2020) and Kaneko and Bollegala (2021) attempt to de-bias word embeddings used by NLP systems, while Elazar and Goldberg (2018); Barrett et al. (2019); Wang et al. (2021) attempt to de-bias model representations and encodings. There is also a large body of work on modifying learning algorithms and inference procedures to produce more fair outcomes (Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2017; Han et al., 2021; Mireshghallah et al., 2021b). While effective in many cases, such approaches do nothing to mitigate human bias in decisions based on text. Fundamentally, our framework is concerned with stylistic features of human-generated text. Thus, a large body of prior work on methods for unsupervised style transfer are related to our approach (Santos et al., 2018; Yang et al., 2018; Luo et al., 2019; He et al., 2020). There is also a vast body of work on style obfuscation (Emmery et al., 2018; Reddy and Knight, 2016; Bevendorff et al., 2019; Shetty et al., 2018). Our work is most closely related to Shetty et al. (2018) and Xu et"
2021.emnlp-main.152,2020.acl-main.647,0,0.0197854,"thors effectively intersects the various styles seen in based on stylistic features of text, and (2) whether training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all consciously or not, be biased by this information. sensitive attributes to text, in effect, computing A large body of prior work has attempted to a union of styles. Our style-obfuscation address algorithmic bias by modifying different framework can be used for multiple purposes, stages of the natural language processing (NLP) however, we demonstrate its effectiveness in pipeline. For example, Ravfogel et al. (2020) atimproving the fairness of downstream classitempt to de-bias word embeddings used by NLP sysfiers. We also conduct a comprehensive study on style pooling’s effect on fluency, semantic tems, while Elazar and Goldberg (2018) address the consistency, and attribute removal from text, in bias in learned model representations and encodings. two and three domain style obfuscation.1 While effective in many cases, such approaches do 1 Introduction nothing to mitigate bias in decisions made by humans based on text. We propose a fundamentally Machine learning (ML) algorithms are used in different appro"
2021.emnlp-main.152,W16-5603,0,0.0295055,"to produce more fair outcomes (Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2017; Han et al., 2021; Mireshghallah et al., 2021b). While effective in many cases, such approaches do nothing to mitigate human bias in decisions based on text. Fundamentally, our framework is concerned with stylistic features of human-generated text. Thus, a large body of prior work on methods for unsupervised style transfer are related to our approach (Santos et al., 2018; Yang et al., 2018; Luo et al., 2019; He et al., 2020). There is also a vast body of work on style obfuscation (Emmery et al., 2018; Reddy and Knight, 2016; Bevendorff et al., 2019; Shetty et al., 2018). Our work is most closely related to Shetty et al. (2018) and Xu et al. (2019). A4NT (Shetty et al., 2018) attempts to obfuscate text style through automatic re-writing. However, their approach attempts to hide protected attributes simply by mapping the style of one protected category to that of another. In contrast, we seek not to map the author’s text to another author’s style, but to a central obfuscated style. Xu et al. propose Privacy Aware Text Re-writing (PATR), which takes a similar adversarial learning 5 Related Work translation based ap"
2021.emnlp-main.152,D15-1044,0,0.026451,"lihood: D1 DM p(X D1 ,...,X DM ,Y ;θy→x ,...,θy→x ) = N Y d(i) p xi |yi ;θy→x )pprior yi  (1) i=1 The log marginal likelihood of the observed data, which we approximate during training, can be written as: D1 DM log p(X D1 ,...,X DM ;θy→x ,...,θy→x ) X D1 DM D1 DM =log p(X ,...,X ;θy→x ,...,θy→x ) (2) Y Neural Architectures. We select a parameterization for our transduction distributions that makes no independence assumptions. We use an encoderdecoder architecture based on the standard attentional Seq2Seq model which has been shown to be successful across various tasks (Bahdanau et al., 2015; Rush et al., 2015). Our prior distributions for each domain are built using recurrent language models which also make no independence assumptions. of observed utterances in the training data. The intersection prior, pInter (y), is computed by taking the sum of the likelihoods of an entire utterance across the language models from all M domains (and then re-normalizing to ensure that resulting prior is a valid distribution). This utterance-level average pooling approach incentivizes a “majority-voting” effect, in which the model is pressured to remove any words and stylistic features that are characteristic of o"
2021.emnlp-main.152,P18-2031,0,0.0280787,"21) attempt to de-bias model representations and encodings. There is also a large body of work on modifying learning algorithms and inference procedures to produce more fair outcomes (Agarwal et al., 2018; Madras et al., 2018; Zafar et al., 2017; Han et al., 2021; Mireshghallah et al., 2021b). While effective in many cases, such approaches do nothing to mitigate human bias in decisions based on text. Fundamentally, our framework is concerned with stylistic features of human-generated text. Thus, a large body of prior work on methods for unsupervised style transfer are related to our approach (Santos et al., 2018; Yang et al., 2018; Luo et al., 2019; He et al., 2020). There is also a vast body of work on style obfuscation (Emmery et al., 2018; Reddy and Knight, 2016; Bevendorff et al., 2019; Shetty et al., 2018). Our work is most closely related to Shetty et al. (2018) and Xu et al. (2019). A4NT (Shetty et al., 2018) attempts to obfuscate text style through automatic re-writing. However, their approach attempts to hide protected attributes simply by mapping the style of one protected category to that of another. In contrast, we seek not to map the author’s text to another author’s style, but to a cent"
2021.emnlp-main.152,D19-1339,0,0.0635308,"Missing"
2021.emnlp-main.152,2021.acl-long.330,0,0.0131371,"gorithms that make decisions based on applications like determining credit ratings, setting text, we propose a framework that obfuscates stylisinsurance policy rates, making hiring decisions, and tic features of human-generated text by automatiperforming facial recognition. It has been shown cally re-writing the text itself. By obfuscating stylisthat such algorithms can produce outcomes that are biased towards a certain gender or race (Buo- tic features, readers (human or algorithms) will be less able to infer protected attributes that enable bias. lamwini and Gebru, 2018; Silva et al., 2021; Sheng et al., 2021). We introduce a novel framework that enables Ideally, high-stakes decisions made by either ‘style pooling’: the automatic transduction of humans or ML algorithms, should not be influenced user-generated text to a central, obfuscated style. by irrelevant, protected attributes like nationality, Notions of ‘centrality’ can themselves introduce 1 bias – for example, a system might learn to Code, models, and data is available at https: //github.com/mireshghallah/style-pooling obfuscate by mapping all text to the dominant style 2009 Proceedings of the 2021 Conference on Empirical Methods in Natural"
2021.emnlp-main.152,1983.tc-1.13,0,0.564894,"Missing"
2021.emnlp-main.152,2020.findings-emnlp.280,0,0.0546272,"Missing"
2021.emnlp-main.152,2021.naacl-main.189,0,0.0119557,"h-stakes learning algorithms that make decisions based on applications like determining credit ratings, setting text, we propose a framework that obfuscates stylisinsurance policy rates, making hiring decisions, and tic features of human-generated text by automatiperforming facial recognition. It has been shown cally re-writing the text itself. By obfuscating stylisthat such algorithms can produce outcomes that are biased towards a certain gender or race (Buo- tic features, readers (human or algorithms) will be less able to infer protected attributes that enable bias. lamwini and Gebru, 2018; Silva et al., 2021; Sheng et al., 2021). We introduce a novel framework that enables Ideally, high-stakes decisions made by either ‘style pooling’: the automatic transduction of humans or ML algorithms, should not be influenced user-generated text to a central, obfuscated style. by irrelevant, protected attributes like nationality, Notions of ‘centrality’ can themselves introduce 1 bias – for example, a system might learn to Code, models, and data is available at https: //github.com/mireshghallah/style-pooling obfuscate by mapping all text to the dominant style 2009 Proceedings of the 2021 Conference on Empiric"
2021.emnlp-main.152,2021.naacl-main.293,0,0.0694358,"Missing"
2021.emnlp-main.152,W19-8633,0,0.258536,"ed (from LinkedIn) age and gender of 436 Twitter users, along with up to 1000 tweets from each user. We use this data for the purpose of sensitive attribute (age) removal comparison with Elazar and Goldberg (2018) in Section A.7, and have therefore used the exact same preprocessing and handling of the data as done by them. DIAL dataset (Blodgett et al., 2016). This is a Twitter dataset which has binary dialect annotations of African American English (AAE) and Standard American English (SAE)4 , setting “author’s race” as the sensitive attribute. We use this dataset for comparison with the work Xu et al. (2019). 3.3 Baselines One language model prior (One-LM). This model is an instance of our framework which uses the output distribution of a single language model as the prior. For the Yelp Synthetic data this single LM is trained on the original data which does not have our modifications and would provide the ideal “intersection”, since the original data itself does not have misspellings from any of our synthetic domains and can be considered as central. In the case of the Blogs data where we don’t have any ideal central data which is void of style, we train an age classifier and then choose the sen"
2021.emnlp-main.152,2021.eacl-main.274,0,0.0859506,"Missing"
2021.emnlp-main.244,D19-1225,1,0.192848,"racters never seen during training. Our model treats font reconstruction as a matrix factorization problem, where we view our corpus as a matrix with rows corresponding to character type, and columns corresponding to fonts. Each row and column is assigned a latent variable that determines its structure or style respectively. A decoder network consisting of transposed convolutional layers parameterizes the model’s distribution on each cell in that matrix, i.e. an image of a glyph, conditioned on the corresponding row and column embeddings. This approach can be thought of as a generalization of Srivatsan et al. (2019), who used a similar factorization framework, but with only one manifold over font style. structure representations as latent variables (2) Propose a new adaptive loss function for synthesizing glyphs, and demonstrate its improvements over more common losses (3) Put forward two datasets that emphasize few-shot reconstruction, along with a preprocessing technique to remove near-duplicate fonts resulting in more challenging train/test splits. We evaluate on the task of few-shot font reconstruction, reporting the structural similarity (SSIM) – a popular metric for image synthesis better correlate"
2021.emnlp-main.55,D19-1609,0,0.0221905,"Time series data in the form of charts has been utilized in some prior work in figure question answering (Kahou et al., 2018; Chen et al., 2019). Past work has explored ways to handle numerical data in a variety of input data domains using neural networks. Trask et al. (2018) propose neural logic 8 Conclusion unit for tasks such as counting objects in images. Prior work has investigated handling of numeracy We present a truth-conditional neural model for in question answering datasets (Dua et al., 2019; time series captioning. Our model composes learned operations/modules to identify patterns Andor et al., 2019; Gupta et al., 2020), typically which hold true for a given input. Outputs from the using a predefined set of executable operations or proposed model demonstrate higher precision and using specific distributions for number prediction diversity compared to various baselines. Further, (Spokoyny and Berg-Kirkpatrick, 2020; Thawani the proposed model (and some of the baselines) et al., 2021). successfully generalize, to some extent, to multiNeuro-Symbolic Methods: Andreas et al. (2016b) ple input sizes. We release two new datasets (in proposed to use neural modular networks for visual English) fo"
2021.emnlp-main.55,W14-1607,0,0.0748846,"Missing"
2021.emnlp-main.55,N16-1181,0,0.23359,"ing trend at 1 (2016a,b), which condition on a question to generData and code can be found at https://github. ate a program, which then operates on an image or com/harsh19/TRUCE. 720 the beginning of a time  series x can be represented trough the logic z: ∃i s.t. INCREASE(xi ) AND BEGIN(i) Thereafter, if the program returns true on the input, one can condition on only the logical program z to generate output text that describes this pattern via a decoder, p(y|z). However, this still requires learning or defining modules for patterns and temporal location. Inspired by neural module networks (Andreas et al., 2016a,b), we propose to use functions parameterized by neural networks (Figure 2) as modules, incorporating inductive bias through architecture design. However, unlike past work, we condition only on an encoding of sampled programs that return true to generate output text. prove quite effective in experiments, but are actually relatively simple, being composed of only three module types. Our framework is extensible, however, and future work might consider larger program spaces. We refer to our proposed method as TRUCE (TRUth Conditional gEneration). 2.2 Programs and Modules As previously mentioned"
2021.emnlp-main.55,W05-0909,0,0.139107,"Missing"
2021.emnlp-main.55,2020.acl-main.708,0,0.0685622,"ce of module types. Figure 1: We propose a neural truth conditional model for high precision and diverse time series caption generation. are able to select entries from tabular or equivalent data during generation by using neural attention mechanisms. In many naturally occurring descriptions of tabular data, humans often refer to higherlevel patterns, for example in the description of stock index pricing over the week in Fig. 1, the speaker refers to how the stock price peaks towards the ending. Some recent work has looked into setups which require non-trivial inference (Wiseman et al., 2017; Chen et al., 2020). However, they typically don’t involve inference about numerical patterns in time series data. Moreover, much recent prior work on identifying more complex patterns in data for captioning has relied on deep neural networks, often employing neural encoders and attention mechanisms. However, such approaches often fail to generate faithful responses and lack interpretability (Tian et al., 2019; Dhingra et al., 2019; Parikh et al., 2020). We present a novel neural truth-conditional model for time series captioning, which learns to identify patterns which hold true for the input time series (Figur"
2021.emnlp-main.55,clairet-2017-dish,0,0.0212087,"been large interest in generating auto- with probability proportional to each program’s matic text description (McKeown, 1992) of tabu- truth-value, a language decoder generates a caption. lar data – for example, prior work has sought to Thus, programs that yield low truth values, do not generate biographies from tables of biographical produce captions. Critically, the decoder takes an information (Lebret et al., 2016), and generating encoding of the program itself, rather than the time descriptions from structured meaning representa- series, in order to determine output text. Overall, tions (Clairet, 2017). However, in many of these this approach allows for both: (a) precision in gentasks the main focus is on designing systems that erated output through explicit truth conditioning, 719 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 719–733 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 3: A program z = (zP , zL ) operates on an input time series x to given final output score sz (x). The module instances are learned from scratch during training. Figure 2: Method Overview: We present a truth conditional model for time se"
2021.emnlp-main.55,P19-1483,0,0.0360393,"Missing"
2021.emnlp-main.55,N19-1246,0,0.0378696,"Missing"
2021.emnlp-main.55,D18-1443,0,0.0318636,"Missing"
2021.emnlp-main.55,D18-1436,1,0.855708,"data by aligning sub-trees in sentence parses to segments of time series. Murakami et al. (2017) generate stock data commentary using encoders such as convolutional and recurrent neural networks, similar to the baselines used in our experiments. Sowdaboina et al. (2014) focus on the task of describing wind speed and direction. Time series data in the form of charts has been utilized in some prior work in figure question answering (Kahou et al., 2018; Chen et al., 2019). Past work has explored ways to handle numerical data in a variety of input data domains using neural networks. Trask et al. (2018) propose neural logic 8 Conclusion unit for tasks such as counting objects in images. Prior work has investigated handling of numeracy We present a truth-conditional neural model for in question answering datasets (Dua et al., 2019; time series captioning. Our model composes learned operations/modules to identify patterns Andor et al., 2019; Gupta et al., 2020), typically which hold true for a given input. Outputs from the using a predefined set of executable operations or proposed model demonstrate higher precision and using specific distributions for number prediction diversity compared to v"
2021.emnlp-main.55,P18-1154,1,0.853921,"ed to prior work on neural discrete representation learning (van den Oord et al., 2017; Zhao et al., 2018), though none of these past works explore utilizing such techniques for data to text problems. Our proposed model abstracts the numerical pattern detection from text generation. Related ideas have been explored in the past in other domains and tasks (Gehrmann et al., 2018; Jhamtani and Berg-Kirkpatrick, 2018; Amizadeh et al., 2020). Data to Text: Tabular or structured data to text generation has been explored in prior work (Lebret et al., 2016; Novikova et al., 2017; Wiseman et al., 2017; Jhamtani et al., 2018; Gehrmann et al., 2021). The Rotowire dataset (Wiseman et al., 2017) is comprised of sports summaries for tabular game data which may require modeling of numerical operations and trends. However, much of the past work has relied on neural models with attention mechanisms, without explicit and interpretable notions of numerical operations. Fidelity to the input in the context of neural text generation has received a lot of attention lately (Cao et al., 2018). Prior work has approached the aspect of fidelity to input through changes in model training and/or decoding methods (Tian et al., 2019;"
2021.emnlp-main.55,W17-5525,0,0.0203691,"tion. In this respect, our work is also related to prior work on neural discrete representation learning (van den Oord et al., 2017; Zhao et al., 2018), though none of these past works explore utilizing such techniques for data to text problems. Our proposed model abstracts the numerical pattern detection from text generation. Related ideas have been explored in the past in other domains and tasks (Gehrmann et al., 2018; Jhamtani and Berg-Kirkpatrick, 2018; Amizadeh et al., 2020). Data to Text: Tabular or structured data to text generation has been explored in prior work (Lebret et al., 2016; Novikova et al., 2017; Wiseman et al., 2017; Jhamtani et al., 2018; Gehrmann et al., 2021). The Rotowire dataset (Wiseman et al., 2017) is comprised of sports summaries for tabular game data which may require modeling of numerical operations and trends. However, much of the past work has relied on neural models with attention mechanisms, without explicit and interpretable notions of numerical operations. Fidelity to the input in the context of neural text generation has received a lot of attention lately (Cao et al., 2018). Prior work has approached the aspect of fidelity to input through changes in model training"
2021.emnlp-main.55,2020.acl-main.66,0,0.0134742,"; Gehrmann et al., 2021). The Rotowire dataset (Wiseman et al., 2017) is comprised of sports summaries for tabular game data which may require modeling of numerical operations and trends. However, much of the past work has relied on neural models with attention mechanisms, without explicit and interpretable notions of numerical operations. Fidelity to the input in the context of neural text generation has received a lot of attention lately (Cao et al., 2018). Prior work has approached the aspect of fidelity to input through changes in model training and/or decoding methods (Tian et al., 2019; Kang and Hashimoto, 2020; Majumder et al., 2021; Goyal and Durrett, 2021; Liu et al., 2021). We explore a different approach that increases fidelity through conditional independence structure and model parameterization. Time-Series Numerical Data and Natural Language Andreas and Klein (2014) worked on grounding news headlines to stock time series data by aligning sub-trees in sentence parses to segments of time series. Murakami et al. (2017) generate stock data commentary using encoders such as convolutional and recurrent neural networks, similar to the baselines used in our experiments. Sowdaboina et al. (2014) focu"
2021.emnlp-main.55,P02-1040,0,0.108835,"Missing"
2021.emnlp-main.55,2021.naacl-main.99,0,0.0379408,"successfully generalize, to some extent, to multiNeuro-Symbolic Methods: Andreas et al. (2016b) ple input sizes. We release two new datasets (in proposed to use neural modular networks for visual English) for the task of time series captioning. Fuquestion answering. Since then, similar approaches ture work might expand to a broader set of module have been used for several other tasks such as refertypes to cover more numerical patterns. ring expression comprehension (Cirik et al., 2018), image captioning (Yang et al., 2019), and text quesAcknowledgements tion answering (Andreas et al., 2016a; Khot et al., 2021). Compared to such past efforts, we induce We thank anonymous EMNLP reviewers for inthe latent numerical and temporal detection opera- sightful comments and feedback. We thank Nikita tions, pick a high scoring program, and condition Duseja for useful discussions. 727 Ethics Statement Second AAAI Conference on Artificial Intelligence, (AAAI-18). We collect natural language annotations from a crowd-sourcing platform. We do not collect or store any person identifiable information. We did not observe any toxic or hateful language in our dataset – though researchers working on the dataset in future"
2021.emnlp-main.55,2020.emnlp-main.89,0,0.0558841,"speaker refers to how the stock price peaks towards the ending. Some recent work has looked into setups which require non-trivial inference (Wiseman et al., 2017; Chen et al., 2020). However, they typically don’t involve inference about numerical patterns in time series data. Moreover, much recent prior work on identifying more complex patterns in data for captioning has relied on deep neural networks, often employing neural encoders and attention mechanisms. However, such approaches often fail to generate faithful responses and lack interpretability (Tian et al., 2019; Dhingra et al., 2019; Parikh et al., 2020). We present a novel neural truth-conditional model for time series captioning, which learns to identify patterns which hold true for the input time series (Figure 2). We first sample a latent program from the space of learned neural operators. 1 Introduction Each program produces a soft truth-value. Then, There has been large interest in generating auto- with probability proportional to each program’s matic text description (McKeown, 1992) of tabu- truth-value, a language decoder generates a caption. lar data – for example, prior work has sought to Thus, programs that yield low truth values,"
2021.emnlp-main.55,D16-1128,0,0.0660383,"Missing"
2021.emnlp-main.55,2020.emnlp-main.385,1,0.81721,"Missing"
2021.emnlp-main.55,2021.naacl-main.53,0,0.0189594,"Missing"
2021.emnlp-main.55,2021.acl-short.74,1,0.654356,"The Rotowire dataset (Wiseman et al., 2017) is comprised of sports summaries for tabular game data which may require modeling of numerical operations and trends. However, much of the past work has relied on neural models with attention mechanisms, without explicit and interpretable notions of numerical operations. Fidelity to the input in the context of neural text generation has received a lot of attention lately (Cao et al., 2018). Prior work has approached the aspect of fidelity to input through changes in model training and/or decoding methods (Tian et al., 2019; Kang and Hashimoto, 2020; Majumder et al., 2021; Goyal and Durrett, 2021; Liu et al., 2021). We explore a different approach that increases fidelity through conditional independence structure and model parameterization. Time-Series Numerical Data and Natural Language Andreas and Klein (2014) worked on grounding news headlines to stock time series data by aligning sub-trees in sentence parses to segments of time series. Murakami et al. (2017) generate stock data commentary using encoders such as convolutional and recurrent neural networks, similar to the baselines used in our experiments. Sowdaboina et al. (2014) focus on the task of descri"
2021.emnlp-main.55,D17-1239,0,0.0605507,"Missing"
2021.emnlp-main.55,P18-1101,0,0.0174379,"nalyzing words frequently present in captions when the argmax program prediction from inference network comprises of a give module-id. tion from the inference network. Next, we identify the most frequently occurring tokens present for each module-id/module-instance. We observe that the inference network seems to be associating semantically similar words to the same module instance (Table 5). 7 Related Work only on a program encoding to generate the output description. In this respect, our work is also related to prior work on neural discrete representation learning (van den Oord et al., 2017; Zhao et al., 2018), though none of these past works explore utilizing such techniques for data to text problems. Our proposed model abstracts the numerical pattern detection from text generation. Related ideas have been explored in the past in other domains and tasks (Gehrmann et al., 2018; Jhamtani and Berg-Kirkpatrick, 2018; Amizadeh et al., 2020). Data to Text: Tabular or structured data to text generation has been explored in prior work (Lebret et al., 2016; Novikova et al., 2017; Wiseman et al., 2017; Jhamtani et al., 2018; Gehrmann et al., 2021). The Rotowire dataset (Wiseman et al., 2017) is comprised of"
2021.emnlp-main.592,2021.findings-acl.357,1,0.804028,"Missing"
2021.emnlp-main.592,D18-1060,0,0.0182661,"ruct literal translations for two popular make a more direct comparison, we will next com- figurative constructs: metaphors and idioms. Thus, pare results when using figurative contexts versus the proposed mitigation approach does not require their literal counterparts. To perform this experi- any retraining of dialog models. ment, we utilize the human written literal versions Metaphor Detection Through Classifier: We in DD-Fig, and experiment with the GPT2 model train a metaphor detection classifier based on the (which is the best performing model as per human VUA corpus (Steen et al., 2010; Gao et al., 2018)2 . rating on the overall dataset). We report results To better generalize to external data via recent conunder two setups: (1) when figurative language is Gao et al. present in the last utterance of the dialog history, textual models, we skip using model by met by fine(2018), and instead learn a classifier C bert and (2) when figurative language is present anytuning the bert-base-uncased (Devlin et al., 2018) where in the dialog history. Human ratings are met checkpoint from Wolf et al. (2019). On VUA, Cbert collected using the same procedure as described gets a test F1 of 0.724, which is clo"
2021.emnlp-main.592,W15-4640,0,0.0767342,"Missing"
2021.emnlp-main.592,2020.emnlp-main.739,1,0.846699,"Missing"
2021.emnlp-main.592,P18-1113,0,0.0493907,"Missing"
2021.emnlp-main.592,S16-2003,0,0.019653,"sources (Bosselut et al., 2019) which have been separately shown to be useful in dialog domain (Majumder et al., 2020) and in generating figurative language (Chakrabarty et al., 2020). 5 (2020) analyze how well dialog models respond to utterances from infrequent sentence function types (e.g Negative Declarative utterances like I feel bad today.). Louis et al. (2020) propose to identify the categorical mapping of an indirect response with respect a polar question in a task oriented dialog setup. Challenges in handling metaphors and idioms has been explored in prior work on machine translation (Mohammad et al., 2016; Kordoni, 2018; Mao et al., 2018). Mao et al. (2018) propose a method to identify metaphors in English text, and paraphrase them into their literal counterparts before translating to Chinese. Our work on analyzing dialog models against figurative language contexts is along similar direction, though the task setup and scope of figurative language involved are different. Figurative language generation has received reasonable attention such as simile generation (Chakrabarty et al., 2020) and idiom generation (Zhou et al., 2021). Compared to them, our focus is on analyzing capability of popular c"
2021.emnlp-main.592,zesch-etal-2008-extracting,0,0.0274865,"Missing"
2021.emnlp-main.592,P18-1205,0,0.140587,"the best extent possible. Figure 1: An example illustrating how model responses are affected by figurative constructs in dialog context. Here, the model conflates the metaphorical use of build on the sand with its literal meaning, leading to an inappropriate, atopical response. 1), and the response seems to rely on the unintended literal sense of ‘sand’. In this work, we investigate the performance of existing dialog models when faced with inputs containing figurative language use. (1) First, we identify the subsets in existing datasets (such as DailyDialog (Li et al., 2017) and PersonaChat (Zhang et al., 2018)) which have figurative language use such as metaphors and similes. We observe that the performance of all the dialog models under consideration is lower on such subsets containing 1 Introduction figurative language use compared to the dataset Human frequently employ figurative language such as a whole. (2) Second, we gather manually writas metaphors (Carbonell, 1982) and idioms (Jack- ten literal/non-figurative equivalents of the dialog endoff, 1995) for effective and/or stylistic com- utterances in DailyDialog and PersonaChat which munication. Thus, dialog models interacting with exhibit fig"
2021.emnlp-main.592,P17-1061,0,0.0720658,"Missing"
2021.emnlp-main.592,2020.nlp4convai-1.15,0,0.0753144,"Missing"
2021.emnlp-main.592,P19-1004,0,0.0216582,"language detection. Our work is limited to a couple of open domain dialog datasets in English language. Similar analyses could be conducted on goal oriented dialog setups and datasets in other languages. Related Work Acknowledgements Past work has explored fine-grained analysis and We thank anonymous EMNLP reviewers for inunderstanding of the performance of dialog models sightful comments and feedback. (Roller et al., 2020). Saleh et al. (2020) analyze open domain dialog systems for skills such as inEthics Statement ferring contradictions and determining the topic of conversation inter alia. Sankar et al. (2019) analyze Our human preference/appropriateness ratings the change in perplexity when applying certain per- are collected over source content either directly turbations in dialog history. Past work has analyzed sourced from, or based on typical, off-the-shelf dialog models from the point of view of safety from models trained on already existing, publicly availtoxic language (Xu et al., 2020; Dinan et al., 2019), able and widely used dialog datasets - namely, Daiand gender biases (Dinan et al., 2020). Gao et al. lyDialog (Li et al., 2017) and Personachat (Zhang 7480 et al., 2018) as well as the m"
2021.findings-acl.357,P15-2073,0,0.166414,"xts occurring in some of the training dialogues, given a sufficient number of them. Using retrieval, we can identify such contexts and use their responses as pseudo-references for the test-time response. Specifically, for retrieval, we use the BM25 function Sbm25 (x, y) (Robertson et al., 1995) to compare each element {dpast , dresp , dft uture } of t t the turn under evaluation dt (the query) with those resp f uture of the candidate turn xt0 , {xpast }. t0 , xt0 , xt0 past f uture Here, dt and dt are the windows of turn sequences before and after response dresp . t Our approach is related to Galley et al. (2015), who propose ∆-BLEU measure which uses retrieval to produce pseudo-references. However, unlike here, they require annotator quality scores to weigh them during evaluation. Moreover, though we utilize retrieval for evaluation, methods of this kind have found success in many generation setups (Li et al., 2018; Peng et al., 2019; Khandelwal et al., 2019). Besides being automatic, our method differs from the above ones in that it explores the added utility of future information for retrieval. For instance, for the dialog shown in Figure 1, besides matching “Great!” in the response, our retrieval"
2021.findings-acl.357,W19-5944,0,0.259337,"ion methods which correlate with human evaluations. Automated metrics such as BLEU (Papineni et al., 2002) work well for tasks such as machine translation, but often correlate poorly with human ratings in tasks such as open domain dialog which admit a wide variety of ∗ VG and HJ contributed equally for this paper. Order decided by coin flip. 1 Code and data are available at https://github.com/harsh19/ Diverse-Reference-Augmentation/ valid response for given context, often due to small number of human written references (Zhao et al., 2017; Sai et al., 2020b). Prior work (Sugiyama et al., 2019; Gupta et al., 2019) has demonstrated that having multiple valid references for the same context leads to automated metrics being better correlated to human judgements for appropriateness. However, collecting human written responses is difficult to scale, can be costly, and may find it difficult to cover a large variety of correct responses (Celikyilmaz et al., 2020). In this work, we automatically extract a large number of diverse references to be used with such reference-based metrics, without resorting to expensive crowd-sourcing. Intuitively, since opendomain dialog pertains to everyday life, its utterance te"
2021.findings-acl.357,N18-1169,0,0.0227905,"Missing"
2021.findings-acl.357,I17-1099,0,0.147212,"ly on broader data sources compared to dialog models. For example, we use future context and human written reference for retrieval, while a dialog model cannot. Our contributions are as follows: (1) We propose a method for automated reference set augmentation for automated dialog evaluation. Compared to collecting more human-written responses, our approach is inexpensive and scalable, and fetches a diverse set of references. (2) We observe high correlations of various automated metrics with human ratings when proposed reference augmentation is applied to the test split of DailyDialog dataset (Li et al., 2017). We additionally observe that paraphrasing, a popular data augmentation technique, performs much worse. (3) We employ novel use of commonsense knowledge and dialog corpus instances, and unsupervised techniques for adapting retrieved references into more fluent forms. 2 Method Figure 1 shows an overview of our proposed methodology. We first fetch plausible candidates from two types of knowledge sources. Thereafter, the retrieved candidate references are adapted so that they are fluent in the target context. We refer to our proposed method as S CARCE ( SCalable Automated Reference Construction"
2021.findings-acl.357,P16-1009,0,0.0439003,"t al., 2020). We compare the correlations across following setups: S INGLE (Li et al., 2017): Original DailyDialog dataset which had one reference per context; S CARCE -S INGLE: Proposed method along with S INGLE reference; M ULTI (Gupta et al., 2019): 4 human written references. S CARCE -M ULTI: Reference responses from the proposed method along with M ULTI references. Additionally, we report the results when using PARAPHRASE instead of S CARCE: PARAPHRASE-S INGLE and PARAPHRASE-M ULTI. Paraphrasing is a popular approach for automated data augmentation. Paraphrasing via backtranslation (BT) (Sennrich et al., 2016) is known to be an effective, domain-independent way to generate good quality paraphrases (Wieting and Gimpel, 2017). We use the BT model from (Xie et al., 2020) with its default hyperparameters to sample 5 paraphrases per human written reference Results: We observe that most of the metrics show large improvements in correlations to human ratings for appropriateness when used along with S INGLE or M ULTI (Table 1). In fact, rank correlations across most of the metrics are better for S CARCE -S INGLE compared to M ULTI, even though former uses only single human written reference while latter us"
2021.findings-acl.357,P17-1061,0,0.357034,"ive and time consuming. Much focus has therefore been on automated evaluation methods which correlate with human evaluations. Automated metrics such as BLEU (Papineni et al., 2002) work well for tasks such as machine translation, but often correlate poorly with human ratings in tasks such as open domain dialog which admit a wide variety of ∗ VG and HJ contributed equally for this paper. Order decided by coin flip. 1 Code and data are available at https://github.com/harsh19/ Diverse-Reference-Augmentation/ valid response for given context, often due to small number of human written references (Zhao et al., 2017; Sai et al., 2020b). Prior work (Sugiyama et al., 2019; Gupta et al., 2019) has demonstrated that having multiple valid references for the same context leads to automated metrics being better correlated to human judgements for appropriateness. However, collecting human written responses is difficult to scale, can be costly, and may find it difficult to cover a large variety of correct responses (Celikyilmaz et al., 2020). In this work, we automatically extract a large number of diverse references to be used with such reference-based metrics, without resorting to expensive crowd-sourcing. Intu"
2021.findings-acl.357,D19-1670,0,0.0138095,"le. 5 Related Work Prior work explores many ways to improve over single-reference evaluation without collecting multiple ones. Fomicheva et al. (2020) obviate need for multiple references in MT by generating many “althypotheses&quot; via test-time dropout from the same model. Sai et al. (2020a) and Gupta et al. (2019) collect additional manually annotated responses for dialog contexts. Compare to them, our method of automatically collecting additional references automatically is more scalable. Automatic data augmentation in NLP has largely been used for increasing training data (Feng et al., 2020; Wei and Zou, 2019; Feng et al., 2021). In this work, we use retrieved dialog instances and commonsense knowledge base to augment reference set for a given dialog context. ∆-Bleu (Galley et al., 2015) and uBLEU (Yuma et al., 2020) also use retrieval to produce pseudo-references for dialog response evaluation. Compared to ∆-Bleu and uBLEU, our work is different since we utilize commonsense knowledge base and perform contextual adaptation. Prior work in dialog response generation has explored the use of commonsense knowledge base (Majumder et al., 2020) as well as retrieval (Song et al., 2016; Majumder et al., 20"
2021.findings-acl.357,2020.acl-srw.27,0,0.0326577,"y “althypotheses&quot; via test-time dropout from the same model. Sai et al. (2020a) and Gupta et al. (2019) collect additional manually annotated responses for dialog contexts. Compare to them, our method of automatically collecting additional references automatically is more scalable. Automatic data augmentation in NLP has largely been used for increasing training data (Feng et al., 2020; Wei and Zou, 2019; Feng et al., 2021). In this work, we use retrieved dialog instances and commonsense knowledge base to augment reference set for a given dialog context. ∆-Bleu (Galley et al., 2015) and uBLEU (Yuma et al., 2020) also use retrieval to produce pseudo-references for dialog response evaluation. Compared to ∆-Bleu and uBLEU, our work is different since we utilize commonsense knowledge base and perform contextual adaptation. Prior work in dialog response generation has explored the use of commonsense knowledge base (Majumder et al., 2020) as well as retrieval (Song et al., 2016; Majumder et al., 2021) – in contrast, our focus is on augmenting reference set for improving evaluation. Automatic model-based metrics like ADEM (Lowe et al., 2017) and RUBER (Tao et al., 2017), which incorporate context while scor"
2021.sigmorphon-1.22,P17-1183,0,0.014285,"stance algorithm (Mohri, 2009). For the neural models trained using conventional methods, decoding strategies that optimize for the output likelihood (e.g. beam search with a large beam size) have been shown to be susceptible to favoring empty outputs (Stahlberg and Byrne, 2019) and generating repetitions (Holtzman et al., 2020). Prior work on leveraging the strength of the two approaches proposes complex joint parameterizations, such as neural weighting of WFST arcs or paths (Rastogi et al., 2016; Lin et al., 2019) or encoding alignment constraints into the attention layer of seq2seq models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen,"
2021.sigmorphon-1.22,b-etal-2010-resource,0,0.0163283,"ll, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization systems such as pinyin, this type of transliteration is not standardized: character substitution choices vary between users and are based on the specific user’s perception of how similar characters in different scripts are. Although the substitutions are primarily phonetic (e.g. Russian n /n/ → n), i.e. based on the pronunciation of a specific character in or out of context, users"
2021.sigmorphon-1.22,W14-3612,0,0.0136932,"ied to all datasets are described in §3.4.2 3.1 Informal romanization Source: Filtered: de el menu:) de el menu<...> Target: Gloss: <...> éJÖ Ï @ ø X ‘This is the menu’ Figure 2: A parallel example from the LDC BOLT Arabizi dataset, written in Latin script (source) and converted to Arabic (target) semi-manually. Some source-side segments (in red) are removed by annotators; we use the version without such segments (filtered) for our task. The annotators also standardize spacing on the target side, which results in difference with the source (in blue). Arabic We use the LDC BOLT Phase 2 corpus (Bies et al., 2014; Song et al., 2014) for training and testing the Arabic transliteration models (Figure 2). The corpus consists of short SMS and chat in Egyptian Arabic represented using Latin script (Arabizi). The corpus is fully parallel: each message is automatically converted into the standardized dialectal Arabic orthography (CODA; Habash et al., 2012) and then manually corrected by human annotators. We split and preprocess the data according to Ryskina et al. (2020), discarding the target (native script) and source (romanized) parallel sentences to create the source and target monolingual training split"
2021.sigmorphon-1.22,P05-1022,0,0.0134739,"019) and generating repetitions (Holtzman et al., 2020). Prior work on leveraging the strength of the two approaches proposes complex joint parameterizations, such as neural weighting of WFST arcs or paths (Rastogi et al., 2016; Lin et al., 2019) or encoding alignment constraints into the attention layer of seq2seq models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization system"
2021.sigmorphon-1.22,W14-3629,0,0.0194065,"models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization systems such as pinyin, this type of transliteration is not standardized: character substitution choices vary between users and are based on the specific user’s perception of how similar characters in different scripts are. Although the substitutions are primarily phonetic (e.g. Russian n /n/ → n), i.e. based"
2021.sigmorphon-1.22,W16-2409,0,0.037721,"Missing"
2021.sigmorphon-1.22,habash-etal-2012-conventional,0,0.039387,"(in red) are removed by annotators; we use the version without such segments (filtered) for our task. The annotators also standardize spacing on the target side, which results in difference with the source (in blue). Arabic We use the LDC BOLT Phase 2 corpus (Bies et al., 2014; Song et al., 2014) for training and testing the Arabic transliteration models (Figure 2). The corpus consists of short SMS and chat in Egyptian Arabic represented using Latin script (Arabizi). The corpus is fully parallel: each message is automatically converted into the standardized dialectal Arabic orthography (CODA; Habash et al., 2012) and then manually corrected by human annotators. We split and preprocess the data according to Ryskina et al. (2020), discarding the target (native script) and source (romanized) parallel sentences to create the source and target monolingual training splits respectively. Russian We use the romanized Russian dataset collected by Ryskina et al. (2020), augmented with the monolingual Cyrillic data from the Taiga corpus of Shavrina and Shapovalova (2017) (Figure 3). The romanized data is split into training, validation, and test portions, and all validation and test sentences are converted to Cyr"
2021.sigmorphon-1.22,C14-1218,0,0.0266832,"ds and abugidas, where graphemes correspond to consonants or consonant-vowel syllables, increasingly use many-to-one alignment in their romanization (Figure 1, top right), which makes learning the latent alignments, and therefore decoding, more challenging. In this work, we experiment with three languages spanning over three major types of writing systems—Russian (alphabetic), Arabic (abjad), and Kannada (abugida)—and compare how well-suited character-level models are for learning these varying alignment patterns. 2.2 Related language translation As shown by Pourdamghani and Knight (2017) and Hauer et al. (2014), character-level models can be used effectively to translate between languages that are closely enough related to have only small lexical and grammatical differences, such as Serbian and Bosnian (Ljubeši´c and Klubiˇcka, 2014). We focus on this specific language pair and tie the languages to specific orthographies (Cyrillic for Serbian and Latin for Bosnian), approaching the task as an unsupervised orthography conversion problem. However, the transliteration framing of the translation problem is inherently limited since the task is not truly character-level in nature, as shown by the alignmen"
2021.sigmorphon-1.22,N15-1123,0,0.0120977,"ons (Holtzman et al., 2020). Prior work on leveraging the strength of the two approaches proposes complex joint parameterizations, such as neural weighting of WFST arcs or paths (Rastogi et al., 2016; Lin et al., 2019) or encoding alignment constraints into the attention layer of seq2seq models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization systems such as pinyin, this type"
2021.sigmorphon-1.22,2021.eacl-demos.3,0,0.0327922,"Missing"
2021.sigmorphon-1.22,P06-2065,0,0.108451,"ntroduction and prior work Many natural language sequence transduction tasks, such as transliteration or grapheme-to-phoneme conversion, call for a character-level parameterization that reflects the linguistic knowledge of the underlying generative process. Character-level transduction approaches have even been shown to perform well for tasks that are not entirely characterlevel in nature, such as translating between related languages (Pourdamghani and Knight, 2017). Weighted finite-state transducers (WFSTs) have traditionally been used for such character-level tasks (Knight and Graehl, 1998; Knight et al., 2006). Their structured formalization makes it easier to encode additional constraints, imposed either 1 Code will be published at https://github.com/ ryskina/error-analysis-sigmorphon2021 Figure 1: Parallel examples from our test sets for two character-level transduction tasks: converting informally romanized text to its original script (top; examples in Russian and Kannada) and translating between closely related languages (bottom; Bosnian–Serbian). Informal romanization is idiosyncratic and relies on both visual (q → 4) and phonetic (t → t) character similarity, while translation is more standar"
2021.sigmorphon-1.22,P07-2045,0,0.0130082,"his observation. As can be seen from Figure 7, the seq2seq model is likely to either predict the word correctly (CER of 0) or entirely wrong (CER of 1), while the the WFST more often predicts the word partially correctly—examples in Table 4 illustrate this as well. We also see this in the Kannada outputs: WFST typically gets all the consonants right but makes mistakes in the vowels, while the seq2seq tends to replace the entire word. Figure 7: Character error rate per word for the WFST (left) and seq2seq (right) bos→srp translation outputs. The predictions are segmented using Moses tokenizer (Koehn et al., 2007) and aligned to ground truth with word-level edit distance. The increased frequency of CER=1 for the seq2seq model as compared to the WFST indicates that it replaces entire words more often. 6 Conclusion We perform comparative error analysis in finitestate and seq2seq models and their combinations for two unsupervised character-level tasks, informal romanization decipherment and related language translation. We find that the two model types tend towards different errors: seq2seq models are more prone to word-level errors caused by distributional shifts while WFSTs produce more characterlevel n"
2021.sigmorphon-1.22,song-etal-2014-collecting,0,0.0109605,"are described in §3.4.2 3.1 Informal romanization Source: Filtered: de el menu:) de el menu<...> Target: Gloss: <...> éJÖ Ï @ ø X ‘This is the menu’ Figure 2: A parallel example from the LDC BOLT Arabizi dataset, written in Latin script (source) and converted to Arabic (target) semi-manually. Some source-side segments (in red) are removed by annotators; we use the version without such segments (filtered) for our task. The annotators also standardize spacing on the target side, which results in difference with the source (in blue). Arabic We use the LDC BOLT Phase 2 corpus (Bies et al., 2014; Song et al., 2014) for training and testing the Arabic transliteration models (Figure 2). The corpus consists of short SMS and chat in Egyptian Arabic represented using Latin script (Arabizi). The corpus is fully parallel: each message is automatically converted into the standardized dialectal Arabic orthography (CODA; Habash et al., 2012) and then manually corrected by human annotators. We split and preprocess the data according to Ryskina et al. (2020), discarding the target (native script) and source (romanized) parallel sentences to create the source and target monolingual training splits respectively. Russ"
D11-1029,D07-1093,1,0.897144,"Missing"
D11-1029,N09-1008,1,0.895478,"Missing"
D11-1029,P08-1088,1,0.95558,"identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information. 1 Introduction 2011), and bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008). We consider a common element, which is a model wherein there are character-level correspondences and word-level correspondences, with the word matching parameterized by the character one. This approach subsumes a range of past tasks, though of course past work has specialized in interesting ways. Past work has emphasized the modeling aspect, where here we use a parametrically simplistic model, but instead emphasize inference. 2 Decipherment as Two-Level Optimization Our method represents two matchings, one at the alphabet level and one at the lexicon level. A vector of variables x specifies"
D11-1029,P10-1105,1,0.948406,"ges, we attempt to induce the correspondence between alphabets and identify the cognates pairs present in the lexicons. The system we propose accomplishes this by defining a simple combinatorial optimization problem that is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics free. Now, the objective P P that we will minimize can be stated simply: u v yuv · E DIT D IST(u, v; x), the sum of the edit distances between th"
D11-1029,W99-0906,0,0.448585,"lexicons. The system we propose accomplishes this by defining a simple combinatorial optimization problem that is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics free. Now, the objective P P that we will minimize can be stated simply: u v yuv · E DIT D IST(u, v; x), the sum of the edit distances between the matched words, where the edit distance function is parameterized by the alphabet matching. Without restriction"
D11-1029,P06-2065,0,0.413488,"Missing"
D11-1029,W02-0902,0,0.132245,"herment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information. 1 Introduction 2011), and bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008). We consider a common element, which is a model wherein there are character-level correspondences and word-level correspondences, with the word matching parameterized by the character one. This approach subsumes a range of past tasks, though of course past work has specialized in interesting ways. Past work has emphasized the modeling aspect, where here we use a parametrically simplistic model, but instead emphasize inference. 2 Decipherment as Two-Level Optimization Our method represents two matchings, one at the alphabet level and one at the lexicon level. A vector o"
D11-1029,2005.mtsummit-papers.11,0,0.0586318,"Missing"
D11-1029,N01-1014,0,0.0282772,"spondence between the alphabets of the two languages exists, but is unknown. Given only two lists of words, the lexicons of both languages, we attempt to induce the correspondence between alphabets and identify the cognates pairs present in the lexicons. The system we propose accomplishes this by defining a simple combinatorial optimization problem that is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics fr"
D11-1029,P11-1025,0,0.0913072,"Missing"
D11-1029,P10-1107,0,0.818257,"t is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics free. Now, the objective P P that we will minimize can be stated simply: u v yuv · E DIT D IST(u, v; x), the sum of the edit distances between the matched words, where the edit distance function is parameterized by the alphabet matching. Without restrictions on the matchings x and y this objective can always be driven to zero by either mapping all characters t"
D12-1091,W10-1703,0,0.0145689,"ll systems that have been evaluated in published work. For each pair of these systems we could run a comparison and compute both δ(x) and p-value(x). While obtaining such data is not generally feasible, for several tasks there are public competitions to which systems are submitted by many researchers. Some of these competitions make system outputs publicly available. We obtained system outputs from the TAC 2008 workshop on automatic summarization (Dang and Owczarzak, 2008), the CoNLL 2007 shared task on dependency parsing (Nivre et al., 2007), and the WMT 2010 workshop on machine translation (Callison-Burch et al., 2010). For cases where the metric linearly decomposes over sentences, the mean of δ(x(i) ) is δ(x). By the central limit theorem, the distribution will be symmetric for large test sets; for small test sets it may not. 3 Note that the bootstrap procedure given only approximates the true significance level, with multiple sources of approximation error. One is the error introduced from using a finite number of bootstrap samples. Another comes from the assumption that the bootstrap samples reflect the underlying population distribution. A third is the assumption that the mean bootstrap gain is the test"
D12-1091,1993.eamt-1.1,0,0.348588,"a constant, the metric gain we actually observed. Traditionally, if p(δ(X) &gt; δ(x)|H0 ) < 0.05, we say that the observed value of δ(x) is sufficiently unlikely that we should reject H0 (i.e. accept that A’s victory was real and not just a random fluke). We refer to p(δ(X) &gt; δ(x)|H0 ) as p-value(x). In most cases p-value(x) is not easily computable and must be approximated. The type of approximation depends on the particular hypothesis testing method. Various methods have been used in the NLP community (Gillick and Cox, 1989; Yeh, 2000; Riezler and Maxwell, 2005). We use the paired bootstrap1 (Efron and Tibshirani, 1993) because it is one 1 Riezler and Maxwell (2005) argue the benefits of approximate randomization testing, introduced by Noreen (1989). However, this method is ill-suited to the type of hypothesis we are testing. Our null hypothesis does not condition on the test data, and therefore the bootstrap is a better choice. 996 Draw b bootstrap samples x(i) of size n by sampling with replacement from x. 2. Initialize s = 0. 3. For each x(i) increment s if δ(x(i) ) &gt; 2δ(x). 4. Estimate p-value(x) ≈ sb 1. Figure 1: The bootstrap procedure. In all of our experiments we use b = 106 , which is more than suff"
D12-1091,P09-1104,1,0.559336,"rated using the same base model type and comparisons between systems generated using different base model types are shown separately. of the same model type the computed p-value < 0.05 threshold is 0.28 BLEU. For comparisons between systems of different model types the threshold is 0.37 BLEU. 3.2.4 Word Alignment Now that we have validated our simple model of system variation on two tasks, we go on to generate plots for tasks that do not have competitions with publicly available system outputs. The first task is English-French word alignment, where we use three base models: the ITG aligner of Haghighi et al. (2009), the joint HMM aligner of Liang et al. (2006), and GIZA++ (Och and Ney, 2003). The last two aligners are unsupervised, while the first is supervised. We train the unsupervised word aligners using the 1.1M sentence pair Hansard training corpus, resampling 20 training sets of the same size.6 Following Haghighi et al. (2009), we train the supervised ITG aligner using the first 337 sentence pairs of the hand-aligned Hansard test set; again, we resample 20 training sets of the same size as the original data. We test on the remaining 100 hand-aligned sentence pairs from the Hansard test set. Unlike"
D12-1091,P03-1054,1,0.0361194,"erall, the spread of this plot is larger than previous ones. This may be due to the small size of the test set, or possibly some additional variance introduced by unsupervised training. For comparisons between systems of the same model type the p-value < 0.05 threshold is 0.50 AER. For comparisons between systems of different model types the threshold is 1.12 AER. 3.2.5 Constituency Parsing Finally, before we move on to further types of analysis, we run an experiment for the task of constituency parsing. We use three base models: the Berkeley parser (Petrov et al., 2006), the Stanford parser (Klein and Manning, 2003), and Dan Bikel’s implementation (Bikel, 2004) of the Collins parser (Collins, 1999). We use sections 2-21 of the WSJ corpus (Marcus et al., 1993), which consists of 38K sentences and parses, as a training set. We resample 10 training sets of size 38K, 10 of size 19K, and 10 of size 9K, and use these to train systems. We test on section 23. The results are shown in Figure 8. For comparisons between systems of the same model type, the p-value < 0.05 threshold is 0.47 F1. For comparisons between systems of different model types the threshold is 0.57 F1. 4.1 Varying the Size Figure 9 plots compar"
D12-1091,P07-2045,0,0.0217932,"shold is 1.51 unlabeled dependency accuracy. These results indicate that the similarity of the systems being compared is an important factor. As mentioned, rulesof-thumb derived from such thresholds cannot be applied blindly, but, in special cases where two systems are known to be correlated, the former threshold should be preferred over the latter. For example, during development most comparisons are made between incremental variants of the same system. If adding a feature to a supervised parser increases un1000 3.2.3 Machine Translation Our two base models for machine translation are Moses (Koehn et al., 2007) and Joshua (Li et al., 2009). We use 1.4M sentence pairs from the German-English portion of the WMT-provided Europarl (Koehn, 2005) and news commentary corpora as the original training set. We resample 75 training sets, 20 of 1.4M sentence pairs, 29 of 350K sentence pairs, and 26 of 88K sentence pairs. This yields a total of 150 system outputs on the system combination portion of the German-English WMT 2010 news test set. The results of the pairwise comparisons of all 150 system outputs are shown in Figure 6, along with the results of the WMT 2010 workshop system comparisons from Figure 4. Th"
D12-1091,W04-3250,0,0.704346,"ls are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. 1 Introduction It is, or at least should be, nearly universal that NLP evaluations include statistical significance tests to validate metric gains. As important as significance testing is, relatively few papers have empirically investigated its practical properties. Those that do focus on single tasks (Koehn, 2004; Zhang et al., 2004) or on the comparison of alternative hypothesis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani and Ney, 2004; Riezler and Maxwell, 2005). In this paper, we investigate two aspects of the empirical behavior of paired significance tests for NLP systems. For example, all else equal, larger metric gains will tend to be more significant. However, what does this relationship look like and how reliable is it? What should be made of the conventional wisdom that often springs up that a certain metric gain is roughly the point of significance for a given task (e.g. 0.4 F1 in parsin"
D12-1091,2005.mtsummit-papers.11,0,0.0263925,"r. As mentioned, rulesof-thumb derived from such thresholds cannot be applied blindly, but, in special cases where two systems are known to be correlated, the former threshold should be preferred over the latter. For example, during development most comparisons are made between incremental variants of the same system. If adding a feature to a supervised parser increases un1000 3.2.3 Machine Translation Our two base models for machine translation are Moses (Koehn et al., 2007) and Joshua (Li et al., 2009). We use 1.4M sentence pairs from the German-English portion of the WMT-provided Europarl (Koehn, 2005) and news commentary corpora as the original training set. We resample 75 training sets, 20 of 1.4M sentence pairs, 29 of 350K sentence pairs, and 26 of 88K sentence pairs. This yields a total of 150 system outputs on the system combination portion of the German-English WMT 2010 news test set. The results of the pairwise comparisons of all 150 system outputs are shown in Figure 6, along with the results of the WMT 2010 workshop system comparisons from Figure 4. The natural comparisons from the WMT 2010 workshop align well with the comparisons between synthetically varied models. Again, the dif"
D12-1091,W09-0424,0,0.0421198,"cy accuracy. These results indicate that the similarity of the systems being compared is an important factor. As mentioned, rulesof-thumb derived from such thresholds cannot be applied blindly, but, in special cases where two systems are known to be correlated, the former threshold should be preferred over the latter. For example, during development most comparisons are made between incremental variants of the same system. If adding a feature to a supervised parser increases un1000 3.2.3 Machine Translation Our two base models for machine translation are Moses (Koehn et al., 2007) and Joshua (Li et al., 2009). We use 1.4M sentence pairs from the German-English portion of the WMT-provided Europarl (Koehn, 2005) and news commentary corpora as the original training set. We resample 75 training sets, 20 of 1.4M sentence pairs, 29 of 350K sentence pairs, and 26 of 88K sentence pairs. This yields a total of 150 system outputs on the system combination portion of the German-English WMT 2010 news test set. The results of the pairwise comparisons of all 150 system outputs are shown in Figure 6, along with the results of the WMT 2010 workshop system comparisons from Figure 4. The natural comparisons from th"
D12-1091,N06-1014,1,0.395006,"ons between systems generated using different base model types are shown separately. of the same model type the computed p-value < 0.05 threshold is 0.28 BLEU. For comparisons between systems of different model types the threshold is 0.37 BLEU. 3.2.4 Word Alignment Now that we have validated our simple model of system variation on two tasks, we go on to generate plots for tasks that do not have competitions with publicly available system outputs. The first task is English-French word alignment, where we use three base models: the ITG aligner of Haghighi et al. (2009), the joint HMM aligner of Liang et al. (2006), and GIZA++ (Och and Ney, 2003). The last two aligners are unsupervised, while the first is supervised. We train the unsupervised word aligners using the 1.1M sentence pair Hansard training corpus, resampling 20 training sets of the same size.6 Following Haghighi et al. (2009), we train the supervised ITG aligner using the first 337 sentence pairs of the hand-aligned Hansard test set; again, we resample 20 training sets of the same size as the original data. We test on the remaining 100 hand-aligned sentence pairs from the Hansard test set. Unlike previous plots, the points corresponding to c"
D12-1091,W04-1013,0,0.00888053,"n in Figure 1. 2 Note that many authors have used a variant where the event tallied on the x(i) is whether δ(x(i) ) < 0, rather than δ(x(i) ) &gt; 2δ(x). If the mean of δ(x(i) ) is δ(x), and if the distribution of δ(x(i) ) is symmetric, then these two versions will be equivalent. 3 1 0.9 1 - p-value As mentioned, a major benefit of the bootstrap is that any evaluation metric can be used to compute δ(x).3 We run the bootstrap using several metrics: F1-measure for constituency parsing, unlabeled dependency accuracy for dependency parsing, alignment error rate (AER) for word alignment, ROUGE score (Lin, 2004) for summarization, and BLEU score for machine translation.4 We report all metrics as percentages. 0.8 Different research groups Same research group 0.7 0.6 Experiments 0.5 Our first goal is to explore the relationship between metric gain, δ(x), and statistical significance, p-value(x), for a range of NLP tasks. In order to say anything meaningful, we will need to see both δ(x) and p-value(x) for many pairs of systems. 3.1 Natural Comparisons Ideally, for a given task and test set we could obtain outputs from all systems that have been evaluated in published work. For each pair of these system"
D12-1091,J93-2004,0,0.0433698,"ntroduced by unsupervised training. For comparisons between systems of the same model type the p-value < 0.05 threshold is 0.50 AER. For comparisons between systems of different model types the threshold is 1.12 AER. 3.2.5 Constituency Parsing Finally, before we move on to further types of analysis, we run an experiment for the task of constituency parsing. We use three base models: the Berkeley parser (Petrov et al., 2006), the Stanford parser (Klein and Manning, 2003), and Dan Bikel’s implementation (Bikel, 2004) of the Collins parser (Collins, 1999). We use sections 2-21 of the WSJ corpus (Marcus et al., 1993), which consists of 38K sentences and parses, as a training set. We resample 10 training sets of size 38K, 10 of size 19K, and 10 of size 9K, and use these to train systems. We test on section 23. The results are shown in Figure 8. For comparisons between systems of the same model type, the p-value < 0.05 threshold is 0.47 F1. For comparisons between systems of different model types the threshold is 0.57 F1. 4.1 Varying the Size Figure 9 plots comparisons for machine translation on variously sized initial segments of the WMT 2010 news test set. Similarly, Figure 10 plots comparisons for consti"
D12-1091,H05-1066,0,0.0213104,"Missing"
D12-1091,nivre-etal-2006-maltparser,0,0.00996693,"from natural comparisons. We expect that each new system will be different, but that systems originating from the same base model will be highly correlated. This provides a useful division of comparisons: those between systems built with the same model, and those between systems built with different models. The first class can be used to approximate comparisons of systems that are expected to be specially correlated, and the latter for comparisons of systems that are not. 3.2.2 Dependency Parsing We use three base models for dependency parsing: MST parser (McDonald et al., 2005), Maltparser (Nivre et al., 2006), and the ensemble parser of Surdeanu and Manning (2010). We use the CoNLL 2007 Chinese training set, which consists of 57K sentences. We resample 5 training sets of 57K sentences, 10 training sets of 28K sentences, and 10 training sets of 14K sentences. Together, this yields a total of 75 system outputs on the CoNLL 2007 Chinese test set, 25 systems for each base model type. The score ranges of all the base models overlap. This ensures that for each pair of model types we will be able to see comparisons where the metric gains are small. The results of the pairwise comparisons of all 75 system"
D12-1091,J03-1002,0,0.015359,"ng different base model types are shown separately. of the same model type the computed p-value < 0.05 threshold is 0.28 BLEU. For comparisons between systems of different model types the threshold is 0.37 BLEU. 3.2.4 Word Alignment Now that we have validated our simple model of system variation on two tasks, we go on to generate plots for tasks that do not have competitions with publicly available system outputs. The first task is English-French word alignment, where we use three base models: the ITG aligner of Haghighi et al. (2009), the joint HMM aligner of Liang et al. (2006), and GIZA++ (Och and Ney, 2003). The last two aligners are unsupervised, while the first is supervised. We train the unsupervised word aligners using the 1.1M sentence pair Hansard training corpus, resampling 20 training sets of the same size.6 Following Haghighi et al. (2009), we train the supervised ITG aligner using the first 337 sentence pairs of the hand-aligned Hansard test set; again, we resample 20 training sets of the same size as the original data. We test on the remaining 100 hand-aligned sentence pairs from the Hansard test set. Unlike previous plots, the points corresponding to comparisons between systems with"
D12-1091,P03-1021,0,0.0221932,"mization testing, introduced by Noreen (1989). However, this method is ill-suited to the type of hypothesis we are testing. Our null hypothesis does not condition on the test data, and therefore the bootstrap is a better choice. 996 Draw b bootstrap samples x(i) of size n by sampling with replacement from x. 2. Initialize s = 0. 3. For each x(i) increment s if δ(x(i) ) &gt; 2δ(x). 4. Estimate p-value(x) ≈ sb 1. Figure 1: The bootstrap procedure. In all of our experiments we use b = 106 , which is more than sufficient for the bootstrap estimate of p-value(x) to stabilize. of the most widely used (Och, 2003; Bisani and Ney, 2004; Zhang et al., 2004; Koehn, 2004), and because it can be easily applied to any performance metric, even complex metrics like F1-measure or BLEU (Papineni et al., 2002). Note that we could perform the experiments described in this paper using another method, such as the paired Student’s ttest. To the extent that the assumptions of the t-test are met, it is likely that the results would be very similar to those we present here. 2.2 The Bootstrap The bootstrap estimates p-value(x) though a combination of simulation and approximation, drawing many simulated test sets x(i) an"
D12-1091,P02-1040,0,0.0880721,"ta, and therefore the bootstrap is a better choice. 996 Draw b bootstrap samples x(i) of size n by sampling with replacement from x. 2. Initialize s = 0. 3. For each x(i) increment s if δ(x(i) ) &gt; 2δ(x). 4. Estimate p-value(x) ≈ sb 1. Figure 1: The bootstrap procedure. In all of our experiments we use b = 106 , which is more than sufficient for the bootstrap estimate of p-value(x) to stabilize. of the most widely used (Och, 2003; Bisani and Ney, 2004; Zhang et al., 2004; Koehn, 2004), and because it can be easily applied to any performance metric, even complex metrics like F1-measure or BLEU (Papineni et al., 2002). Note that we could perform the experiments described in this paper using another method, such as the paired Student’s ttest. To the extent that the assumptions of the t-test are met, it is likely that the results would be very similar to those we present here. 2.2 The Bootstrap The bootstrap estimates p-value(x) though a combination of simulation and approximation, drawing many simulated test sets x(i) and counting how often A sees an accidental advantage of δ(x) or greater. How can we get sample test sets x(i) ? We lack the ability to actually draw new test sets from the underlying populati"
D12-1091,P06-1055,1,0.328321,"two models are particularly correlated. Overall, the spread of this plot is larger than previous ones. This may be due to the small size of the test set, or possibly some additional variance introduced by unsupervised training. For comparisons between systems of the same model type the p-value < 0.05 threshold is 0.50 AER. For comparisons between systems of different model types the threshold is 1.12 AER. 3.2.5 Constituency Parsing Finally, before we move on to further types of analysis, we run an experiment for the task of constituency parsing. We use three base models: the Berkeley parser (Petrov et al., 2006), the Stanford parser (Klein and Manning, 2003), and Dan Bikel’s implementation (Bikel, 2004) of the Collins parser (Collins, 1999). We use sections 2-21 of the WSJ corpus (Marcus et al., 1993), which consists of 38K sentences and parses, as a training set. We resample 10 training sets of size 38K, 10 of size 19K, and 10 of size 9K, and use these to train systems. We test on section 23. The results are shown in Figure 8. For comparisons between systems of the same model type, the p-value < 0.05 threshold is 0.47 F1. For comparisons between systems of different model types the threshold is 0.57"
D12-1091,W05-0908,0,0.17337,"independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. 1 Introduction It is, or at least should be, nearly universal that NLP evaluations include statistical significance tests to validate metric gains. As important as significance testing is, relatively few papers have empirically investigated its practical properties. Those that do focus on single tasks (Koehn, 2004; Zhang et al., 2004) or on the comparison of alternative hypothesis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani and Ney, 2004; Riezler and Maxwell, 2005). In this paper, we investigate two aspects of the empirical behavior of paired significance tests for NLP systems. For example, all else equal, larger metric gains will tend to be more significant. However, what does this relationship look like and how reliable is it? What should be made of the conventional wisdom that often springs up that a certain metric gain is roughly the point of significance for a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU In order for our results to be meaningful, we must have access to the outputs of many of NLP systems. Public competitions, such as the well-know"
D12-1091,N10-1091,0,0.0128219,"ew system will be different, but that systems originating from the same base model will be highly correlated. This provides a useful division of comparisons: those between systems built with the same model, and those between systems built with different models. The first class can be used to approximate comparisons of systems that are expected to be specially correlated, and the latter for comparisons of systems that are not. 3.2.2 Dependency Parsing We use three base models for dependency parsing: MST parser (McDonald et al., 2005), Maltparser (Nivre et al., 2006), and the ensemble parser of Surdeanu and Manning (2010). We use the CoNLL 2007 Chinese training set, which consists of 57K sentences. We resample 5 training sets of 57K sentences, 10 training sets of 28K sentences, and 10 training sets of 14K sentences. Together, this yields a total of 75 system outputs on the CoNLL 2007 Chinese test set, 25 systems for each base model type. The score ranges of all the base models overlap. This ensures that for each pair of model types we will be able to see comparisons where the metric gains are small. The results of the pairwise comparisons of all 75 system outputs are shown in Figure 5, along with the results o"
D12-1091,C00-2137,0,0.528095,"future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. 1 Introduction It is, or at least should be, nearly universal that NLP evaluations include statistical significance tests to validate metric gains. As important as significance testing is, relatively few papers have empirically investigated its practical properties. Those that do focus on single tasks (Koehn, 2004; Zhang et al., 2004) or on the comparison of alternative hypothesis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani and Ney, 2004; Riezler and Maxwell, 2005). In this paper, we investigate two aspects of the empirical behavior of paired significance tests for NLP systems. For example, all else equal, larger metric gains will tend to be more significant. However, what does this relationship look like and how reliable is it? What should be made of the conventional wisdom that often springs up that a certain metric gain is roughly the point of significance for a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU In order for our results to be meaningful, we must have access to the outputs of many of NLP s"
D12-1091,J04-4004,0,\N,Missing
D12-1091,J03-4003,0,\N,Missing
D12-1091,zhang-etal-2004-interpreting,0,\N,Missing
D12-1091,D07-1096,0,\N,Missing
D13-1087,D11-1029,1,0.884174,"homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 2 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder e"
D13-1087,P06-2065,0,0.439962,"random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 2 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP c"
D13-1087,D08-1085,0,0.154595,"Missing"
D13-1087,P11-1025,0,0.318369,"particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 2 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP community with great suc"
D13-1087,P10-1107,0,0.223343,"n, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder et al., 2010; Ravi and Knight, 2011). Many of these approaches learn an encryption key by maximizing the score of the decrypted message under a language model. We focus on homophonic substitution ciphers, where the encryption key is a 1-to-many mapping from a plaintext alphabet to a cipher alphabet. We use a simple method introduced by Knight et al. (2006): the EM algorithm (Dempster et al., 1977) is used to learn the emission parameters of an HMM that has a character trigram language model as a backbone and the ciphertext as the observed sequence of emissions. This means that we learn a multinomial over"
D15-1032,D13-1203,1,0.83022,"ng dev set performance over five training iterations, except for constituency parsing, where we took five measurements, 4k instances apart. For the cutting plane methods we cached constraints in memory to save time, but the memory cost was too great to run batch cutting plane on constituency parsing (over 60 Gb), and so is not included in the results. Coreference Resolution This gives an example of training when there are multiple gold outputs for each instance. The system we consider uses latent links between mentions in the same cluster, marginalizing over the possibilities during learning (Durrett and Klein, 2013). Since the model decomposes across mentions, we train by treating them as independent predictions with multiple gold outputs, comparing the inferred link with the gold link that is scored highest under the current model. We use the system’s weighted loss function, and the same data as for NER. 4 Observations From the results in Figure 1 and during tuning, we can make several observations about these optimization methods’ performance on these tasks. Constituency Parsing We considered two different systems. The first uses only sparse indicator features (Hall et al., 2014), while the second is p"
D15-1032,Q14-1037,1,0.270541,"Inference involves solving an integer linear program, the loss function is bigram recall, and the data is from the TAC shared tasks (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009). Tasks and Systems We considered tasks covering a range of structured output spaces, from sequences to non-projective trees. Most of the corresponding systems use models designed for likelihood-based structured prediction. Some use sparse indicator features, while others use dense continuous-valued features. Named Entity Recognition This task provides a case of sequence prediction. We used the NER component of Durrett and Klein (2014)’s entity stack, training it independently of the other components. We define the loss as the number of incorrectly labelled words, and train on the CoNLL 2012 division of OntoNotes (Pradhan et al., 2007). 3.1 Tuning For each method we tuned hyperparameters by considering a grid of values and measuring dev set performance over five training iterations, except for constituency parsing, where we took five measurements, 4k instances apart. For the cutting plane methods we cached constraints in memory to save time, but the memory cost was too great to run batch cutting plane on constituency parsin"
D15-1032,P11-1049,1,0.722984,"standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with features considering counts of bigrams in the input document collection. The system forms the output summary by selecting a subset of the sentences in the input collection that does not exceed a fixed word-length limit (Berg-Kirkpatrick et al., 2011). Inference involves solving an integer linear program, the loss function is bigram recall, and the data is from the TAC shared tasks (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009). Tasks and Systems We considered tasks covering a range of structured output spaces, from sequences to non-projective trees. Most of the corresponding systems use models designed for likelihood-based structured prediction. Some use sparse indicator features, while others use dense continuous-valued features. Named Entity Recognition This task provides a case of sequence prediction. We used the NER component of"
D15-1032,P15-1030,1,0.0373143,"Missing"
D15-1032,Q13-1017,0,0.0198867,"touch the weights corresponding to the (usually sparse) nonzero directions in the current batch’s subgradient. Algorithm 1 gives pseudocode for our implementation, which was based on Dyer (2013). Margin Cutting Plane (Tsochantaridis et al., 2004) Solves a sequence of quadratic programs (QP), each of which is an approximation to the dual formulation of the margin-based learning problem. At each iteration, the current QP is refined by adding additional active constraints. We solve each approximate QP using Sequential Minimal Optimization (Platt, 1999; Taskar et al., 2004). Online Cutting Plane (Chang and Yih, 2013) A modified form of cutting plane that only partially solves the QP on each iteration, operating in the dual space and optimizing a single dual variable on each iteration. We use a variant of Chang and Yih (2013) for the L1 loss margin objective. 2.2 Likelihood Stochastic Gradient Descent The built-in training method for many of the systems was softmax-margin likelihood optimization (Gimpel and Smith, 2010) via subgradient descent with either AdaGrad or AdaDelta (Duchi et al., 2011; Zeiler, 2012). We include results with each system’s default settings as a point of comparison. Online Primal Su"
D15-1032,P05-1022,0,0.0439671,"perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives. 1 Introduction Structured discriminative models have proven effective across a range of tasks in NLP including tagging (Lafferty et al., 2001; Collins, 2002), reranking parses (Charniak and Johnson, 2005), and many more (Taskar, 2004; Smith, 2011). Common approaches to training such models include margin methods, likelihood methods, and mistake-driven procedures like the averaged perceptron algorithm. In this paper, we primarily consider the relative empirical behavior of several online optimization methods for margin-based objectives, with secondary attention to other approaches for calibration. It is increasingly common to train structured models using a max-margin objective that incorporates a loss function that decomposes in the 2 Learning Algorithms We implemented a range of optimization"
D15-1032,C96-1058,0,0.0684883,"dified form of the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a l"
D15-1032,N10-1112,0,0.0573708,"Missing"
D15-1032,P14-1022,1,0.5352,"Missing"
D15-1032,W02-1001,0,0.6832,"outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives. 1 Introduction Structured discriminative models have proven effective across a range of tasks in NLP including tagging (Lafferty et al., 2001; Collins, 2002), reranking parses (Charniak and Johnson, 2005), and many more (Taskar, 2004; Smith, 2011). Common approaches to training such models include margin methods, likelihood methods, and mistake-driven procedures like the averaged perceptron algorithm. In this paper, we primarily consider the relative empirical behavior of several online optimization methods for margin-based objectives, with secondary attention to other approaches for calibration. It is increasingly common to train structured models using a max-margin objective that incorporates a loss function that decomposes in the 2 Learning Alg"
D15-1032,J93-2004,0,0.0552861,"hese instance-wise subgradients to optimize the global objective using AdaGrad (Duchi et al., 2011) with either L1 or L2 regularization. The simplest implementation of AdaGrad touches every weight when doing the update 2.3 Mistake Driven Averaged Perceptron (Freund and Schapire, 1999; Collins, 2002) On a mistake, weights for features on the system output are decremented and weights for features on the gold output are incre274 mented. Weights are averaged over the course of training, and decoding is not loss-augmented. of incorrect rule productions, and use the standard Penn Treebank division (Marcus et al., 1993). Margin Infused Relaxed Algorithm (Crammer and Singer, 2003) A modified form of the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order"
D15-1032,P05-1012,0,0.0382023,"the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with featur"
D15-1032,H05-1066,0,0.0798595,"the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with featur"
D15-1032,W04-3201,1,0.893268,"able loss-augmented decoding, and trained these models with six different methods. We have released our learning code as a Java library.1 Our results provide support for the conventional wisdom that margin-based optimization is broadly effective, frequently outperforming likelihood optimization and the perceptron algorithm. We also found that directly optimizing the primal structured margin objective based on subgradients calculated from single training instances is surprisingly effective, performing consistently well across all tasks. Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple o"
D17-1275,P13-1035,0,0.049848,"Missing"
D17-1275,D14-1082,0,0.0202088,"evel Evaluation Another axis of variation in metrics comes from whether we consider token-level or phrase-level outputs. As noted in the previous section, we did not annotate noun phrases, but we may actually be interested in identifying them. In Figure 1, for example, extracting Backconnect bot is more useful than extracting bot in isolation, since bot is a less specific characterization of the product. We can convert our token-level annotations to phrase-level annotations by projecting our annotations to the noun phrase level based on the output of an automatic parser. We used the parser of Chen and Manning (2014) to parse all sentences of each post. For each annotated token that was given a nominal tag (N*), we projected that token to the largest NP containing it of length less than or equal to 7; most product NPs are shorter than this, and when the parser predicts a longer NP, our analysis found that it typically reflects a mistake. In Figure 1, the entire noun phrase Backconnect bot would be labeled as a product. For products realized as verbs (e.g., hack), we leave the annotation as the single token. Throughout the rest of this work, we will evaluate sometimes at the token-level and sometimes at 6"
D17-1275,P07-1033,0,0.259721,"Missing"
D17-1275,P05-1045,0,0.0405631,"ervised methods (Section 5). Baselines One approach takes the most frequent noun or verb in a post and classifies all occurrences of that word type as products. A more sophisticated lexical baseline is based on a product dictionary extracted from our training data: we tag the most frequent noun or verb in a post that also appears in this dictionary. This method fails primarily in that it prefers to extract common words like account and website even when they do not occur as products. The most relevant off-theshelf system is an NER tagging model; we retrain the Stanford NER system on our data (Finkel et al., 2005). Finally, we can tag the first noun phrase of the post as a product, which will often capture the product if it is mentioned in the title of the post.8 We also include human performance results. We averaged the results for annotators compared with the consensus annotations. For the phrase level evaluation, we apply the projection method described in Section 3.1. Binary classifier/CRF One learning-based approach to this task is to employ a binary SVM classifier for each token in isolation. We also experimented with a token-level CRF with a binary tagset, and found identical performance, so we"
D17-1275,N13-1014,0,0.134481,"on extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of produ"
D17-1275,P13-1057,0,0.123805,"ing annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of products being traded: • Dark"
D17-1275,D15-1157,0,0.0604442,"Missing"
D17-1275,W10-2923,0,0.0239519,"oth slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for e"
D17-1275,D15-1032,1,0.800204,"es a useful form of prior knowledge, namely that each post has exactly one product in almost all cases. Our post-level system is formulated as an instance of a latent SVM (Yu and Joachims, 2009). The output space is the set of all tokens (or noun phrases, in the NP case) in the post. The latent variable is the choice of token/NP to select, since there may be multiple correct choices of product tokens. The features used on each token/NP are the same as in the token classifier. We trained all of the learned models by subgradient descent on the primal form of the objective (Ratliff et al., 2007; Kummerfeld et al., 2015). We use AdaGrad (Duchi et al., 2011) to speed convergence in the presence of a large weight vector with heterogeneous feature types. All product extractors in this section are trained for 5 iterations with `1 -regularization tuned on the development set. 2602 Freq Dict NER Binary Post Human∗ Freq Dict First NER Binary Post Human∗ P 41.9 57.9 59.7 62.4 82.4 86.9 P 61.8 57.9 73.1 63.6 67.0 87.6 87.6 Token Prediction Tokens Products R F1 P R F1 42.5 42.2 48.4 33.5 39.6 51.1 54.3 65.6 44.0 52.7 62.2 60.9 60.8 62.6 61.7 76.0 68.5 58.1 77.6 66.4 36.1 50.3 83.5 56.6 67.5 80.4 83.5 87.7 77.6 82.2 NP"
D17-1275,U10-1009,0,0.0262503,"k has similarities to both slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to a"
D17-1275,P14-5010,0,0.00355307,"ed every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit (Manning et al., 2014). Note that many sentences in the data are already delimited by line breaks, making the sentence-splitting task much easier. We performed annotation on the tokenized data so that annotations would be consistent with surrounding punctuation and hyphenated words. Our full annotation guide is available with our data release.3 Our basic annotation principle is 2 The table does not include additional posts that were labeled by all annotators in order to check agreement. 3 https://evidencebasedsecurity.org/ forums/annotation-guide.pdf to annotate tokens when they are either the product that will be"
D17-1275,P13-1108,0,0.0727509,"Missing"
D17-1275,P10-1040,0,0.780109,"uct extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of products being traded: • Darkode: Cybercriminal wares, including exploit kits, spam services, ransomware programs, and steal"
D17-1275,D11-1002,0,0.025689,"nformation extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that"
D17-1275,Q17-1036,0,0.0298178,"to train models available at https://evidencebasedsecurity.org/forums/ 2013). One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits (Krebs, 2013a,b). However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models (McClosky et al., 2010) and deep models (Zhang et al., 2017). Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging. In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. 2598 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2598–2607 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Words Products Annotated Annotators Inter-annotator agreement Forum Posts per post per pos"
D17-1275,N10-1004,0,0.0237298,"013; O’Connor et al., 1 Dataset and code to train models available at https://evidencebasedsecurity.org/forums/ 2013). One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits (Krebs, 2013a,b). However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models (McClosky et al., 2010) and deep models (Zhang et al., 2017). Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging. In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. 2598 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2598–2607 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Words Products Annotated Annotators Inter-annotator a"
D18-1160,P14-2131,0,0.202435,"within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mikolov et al., 2013) with small context window size are found to capture the syntactic properties of language well (Bansal et al., 2014; Lin et al., 2015). However, if our goal is to separate syntactic categories, this embedding space is not ideal – POS categories correspond to overlap1292 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics zi ⇠ Syntax Model z3 z2 z1 &lt;latexit sha1_base64=&quot;ZNKuRe23lzCJMNesh6cRgweKXJI=&quot;&gt;AAACCHicZVDLSgMxFM3UV62vqks3g0VwUcpMEdRd0Y3Lio4W2qFk0kwbmkyG5I5Qh36BuNXvcCVu/Qs/wz8wM52FbS+Ee3LuueHkBDFnGhznxyqtrK6tb5Q3K1vbO7t71f2DBy0TRahHJJeqE2BNOYuoBww47cSKYh"
D18-1160,N10-1083,1,0.89622,"Missing"
D18-1160,P11-1087,0,0.15266,"nd latent embeddings learned by our approach with a Markov-structured prior. Each node represents a word and is colored according to the most likely gold POS tag from the Penn Treebank (best seen in color). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word"
D18-1160,Q17-1010,0,0.0633438,"with random initialization. This is consistent with previous work that has noted the importance of careful initialization for DMV-based models such as the commonly used harmonic initializer (Klein and Manning, 2004). However, it is not straightforward to apply the harmonic initializer for DMV directly in our model without using some kind of pre-training since we do not observe gold POS. Impact of Observed Embeddings. We investigate the effect of the choice of pre-trained embedding on performance while using our approach. To this end, we additionally include results using fastText embeddings (Bojanowski et al., 2017) – which, in contrast with skip-gram embeddings, include character-level information. We set the context windows size to 1 and the dimension size to 100 as in the skip-gram training, while keeping other parameters set to their defaults. These results are summarized in Table 4 and Table 5. While fastText embeddings lead to reduced performance with our model, our approach still yields an improvement over the Gaussian baseline with the new observed embeddings space. 4.6 Qualitative Analysis of Embeddings We perform qualitative analysis to understand how the latent embeddings help induce syntactic"
D18-1160,D17-1171,0,0.279698,"we do the one-to-one mapping between gold POS tags and induced clusters and plot the normalized confusion matrix of noun subcategories in Figure 4. The Gaussian HMM fails to identify “NN” and “NNS” correctly for most cases, and it often recognizes “NNPS” as “NNP”. In contrast, our approach corrects these errors well. 1297 4.4 Unsupervised Dependency Parsing without gold POS tags 6 10 w/o gold POS tags DMV (Klein and Manning, 2004) 49.6 E-DMV (Headden III et al., 2009) 52.1 UR-A E-DMV (Tu and Honavar, 2012) 58.9 CS∗ (Spitkovsky et al., 2013) 72.0∗ Neural E-DMV (Jiang et al., 2016) 55.3 CRFAE (Cai et al., 2017) 37.2 Gaussian DMV 55.4 (1.3) Ours (4 layers) 58.4 (1.9) Ours (8 layers) 60.2 (1.3) Ours (16 layers) 54.1 (8.5) 35.8 38.2 46.1 64.4∗ 42.7 29.5 43.1 (1.2) 46.2 (2.3) 47.9 (1.2) 43.9 (5.7) w/ gold POS tags (for reference only) DMV (Klein and Manning, 2004) 55.1 UR-A E-DMV (Tu and Honavar, 2012) 71.4 MaxEnc (Le and Zuidema, 2015) 73.2 Neural E-DMV (Jiang et al., 2016) 72.5 CRFAE (Cai et al., 2017) 71.7 L-NDMV (Big training data) (Han et al., 2017) 77.2 39.7 57.0 65.8 57.6 55.7 63.2 System For the task of unsupervised dependency parse induction, we employ the Dependency Model with Valence (DMV) (K"
D18-1160,D10-1056,0,0.130718,"Missing"
D18-1160,D11-1005,0,0.154037,"lly because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies without using gold POS tags or other extra linguistic information. Setup. Like previous work we use sections 0221 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 6 10, “headpercolation” rules (Collins, 1999) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended DMV (E-DMV) (Headden III et al., 2009) and Gaussian DMV (which treats POS tag as unknown latent variables and gener"
D18-1160,N09-1009,0,0.0601511,"ank (best seen in color). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated cat"
D18-1160,N15-1140,0,0.0473601,"Missing"
D18-1160,D17-1176,0,0.575784,"erence and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate on two tasks: part-o"
D18-1160,P17-1044,0,0.0229364,"ortant. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mi"
D18-1160,N09-1012,0,0.246964,"Missing"
D18-1160,D16-1073,0,0.602307,"tractable exact inference and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate o"
D18-1160,D07-1031,0,0.052933,"and initialize it with the empirical variance of the word vectors. Following Lin et al. (2015), the covariance matrix is fixed during training. The multinomial probabilities are initialized as θkv ∝ exp(ukv ), where ukv ∼ U [0, 1]. For our approach, we initialize the syntax model and Gaussian parameters with the pre-trained Gaussian HMM. The weights of layers in the rectified network are initialized from a uniform distribution p with mean zero and a standard deviation of 1/nin , where nin is the input dimension.4 We evaluate the performance of POS tagging with both Many-to-One (M-1) accuracy (Johnson, 2007) and V-Measure (VM) (Rosenberg and Hirschberg, 2007). Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsupervised criterion to select the trained model over 10 random restarts after training 50 epochs. We repeat this process 5 times and report the mean and standard deviation of performance. 0.01 0.00 0.32 0.30 0.37 Others NNPS 4.3 System 0.01 0.20 0.01 0.47 0.31 0.02 0.00 0.00 0.00 0.98 NN NNS NNP NNPS Others (b) Our approach Figure 4: Normalized Confusion matrix for POS tagging experiments, ro"
D18-1160,P04-1061,0,0.921002,"gs to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate on two tasks: part-of-speech (POS) induction and unsupervised dependency parsing without gold POS tags. Experimental results on the Penn Treebank (Marcus et al., 1993) demonstrate that our approach improves the basic HMM and DMV by a large margin, leading to the state-of-the-art results on PO"
D18-1160,N15-1067,0,0.233499,"Missing"
D18-1160,P14-2050,0,0.0366216,"eighbors, based on skip-gram embeddings and our learned latent embeddings with Markov-structured syntax model. process dreams error 5 agenda plans payments timetable parents furriers smokers aides issuers folks (subj) (subj)aide owner (obj) resident attorney in the overlapping region are typically objects. This demonstrates that the latent embeddings are focusing on aspects of language that are specifically important for modeling dependency without ever having seen examples of dependency parses. Some previous work has deliberately created embeddings to capture different notions of similarity (Levy and Goldberg, 2014; Cotterell and Sch¨utze, 2015), while they use extra morphology or dependency annotations to guide the embedding learning, our approach provides a potential alternative to create new embeddings that are guided by structured syntax model, only using unlabeled text corpora. actress singer Figure 5: Visualization (t-SNE) of learned latent embeddings with DMV-structured syntax model. Each node represents a word and is colored according to the most likely gold POS tag in the Penn Treebank (best seen in color). For our Markov-structured model, we have displayed the embedding space in Figure 1(b), w"
D18-1160,N15-1144,0,0.122378,"models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mikolov et al., 2013) with small context window size are found to capture the syntactic properties of language well (Bansal et al., 2014; Lin et al., 2015). However, if our goal is to separate syntactic categories, this embedding space is not ideal – POS categories correspond to overlap1292 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics zi ⇠ Syntax Model z3 z2 z1 &lt;latexit sha1_base64=&quot;ZNKuRe23lzCJMNesh6cRgweKXJI=&quot;&gt;AAACCHicZVDLSgMxFM3UV62vqks3g0VwUcpMEdRd0Y3Lio4W2qFk0kwbmkyG5I5Qh36BuNXvcCVu/Qs/wz8wM52FbS+Ee3LuueHkBDFnGhznxyqtrK6tb5Q3K1vbO7t71f2DBy0TRahHJJeqE2BNOYuoBww47cSKYhFw+hiMr7P54xNVmsnoH"
D18-1160,J93-2004,0,0.0606684,"ddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV. We evaluate on two tasks: part-of-speech (POS) induction and unsupervised dependency parsing without gold POS tags. Experimental results on the Penn Treebank (Marcus et al., 1993) demonstrate that our approach improves the basic HMM and DMV by a large margin, leading to the state-of-the-art results on POS induction, and state-of-the-art results on unsupervised dependency parsing in the difficult training scenario where neither gold POS annotation nor punctuation-based constraints are available. 2 Model As an illustrative example, we first present a baseline model for Markov syntactic structure (POS induction) that treats a sequence of pre-trained word embeddings as observations. Then, we propose our novel approach, again using Markov structure, that introduces latent w"
D18-1160,D16-1031,0,0.0305231,"rast, the nouns Related Work Our approach is related to flow-based generative models, which are first described in NICE (Dinh et al., 2014) and have recently received more attention (Dinh et al., 2016; Jacobsen et al., 2018; Kingma and Dhariwal, 2018). This relevant work mostly adopts simple (e.g. Gaussian) and fixed priors and does not attempt to learn interpretable latent structures. Another related generative model class is variational auto-encoders (VAEs) (Kingma and Welling, 2013) that optimize a lower bound on the marginal data likelihood, and can be extended to learn latent structures (Miao and Blunsom, 2016; Yin et al., 2018). Against the flow-based models, VAEs remove the invertibility constraint but sacrifice the merits of exact inference and exact log likelihood computation, which potentially results in optimization challenges (Kingma et al., 2016). Our approach can also be viewed in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014) that is a likelihood-free framework to learn implicit generative models. However, it is nontrivial for a gradient-based method like GANs to propagate gradients through discrete structures. 6 Conclusion In this work, we define a novel"
D18-1160,Q13-1006,0,0.0629485,"ot fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies without using gold POS tags or other extra linguistic information. Setup. Like previous work we use sections 0221 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 6 10, “headpercolation” rules (Collins, 1999) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended D"
D18-1160,C16-1003,0,0.470026,"). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the spe"
D18-1160,N18-1202,0,0.0130934,"related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive unlabeled corpora offer a compact way of injecting a prior notion of word similarity into models that would otherwise treat words as discrete, isolated categories. However, the specific properties of language captured by any particular embedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram embeddings (Mikolov et al., 2013) wi"
D18-1160,Q16-1018,0,0.179395,"rned by our approach with a Markov-structured prior. Each node represents a word and is colored according to the most likely gold POS tag from the Penn Treebank (best seen in color). Introduction Data annotation is a major bottleneck for the application of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syntactic analysis, discrete generative models have dominated in recent years – for example, for both part-of-speech (POS) induction (Blunsom and Cohn, 2011; Stratos et al., 2016) and unsupervised dependency parsing (Klein and Manning, 1 Code is available at https://github.com/jxhe/structlearning-with-flow. 2004; Cohen and Smith, 2009; Pate and Johnson, 2016). While similar models have had success on a range of unsupervised tasks, they have mostly ignored the apparent utility of continuous word representations evident from supervised NLP applications (He et al., 2017; Peters et al., 2018). In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massiv"
D18-1160,W16-5907,0,0.500835,"are able to derive tractable exact inference and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to prevent information loss. Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches (Lin et al., 2015; Tran et al., 2016; Jiang et al., 2016; Han et al., 2017). Lin et al. (2015) build an HMM with Gaussian emissions on observed word embeddings, but they do not attempt to learn new embeddings. Tran et al. (2016), Jiang et al. (2016), and Han et al. (2017) extend HMM or dependency model with valence (DMV) (Klein and Manning, 2004) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables. In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, th"
D18-1160,D12-1121,0,0.536256,".0 points on VM. Confusion Matrix. We found that most tagging errors happen in noun subcategories. Therefore, we do the one-to-one mapping between gold POS tags and induced clusters and plot the normalized confusion matrix of noun subcategories in Figure 4. The Gaussian HMM fails to identify “NN” and “NNS” correctly for most cases, and it often recognizes “NNPS” as “NNP”. In contrast, our approach corrects these errors well. 1297 4.4 Unsupervised Dependency Parsing without gold POS tags 6 10 w/o gold POS tags DMV (Klein and Manning, 2004) 49.6 E-DMV (Headden III et al., 2009) 52.1 UR-A E-DMV (Tu and Honavar, 2012) 58.9 CS∗ (Spitkovsky et al., 2013) 72.0∗ Neural E-DMV (Jiang et al., 2016) 55.3 CRFAE (Cai et al., 2017) 37.2 Gaussian DMV 55.4 (1.3) Ours (4 layers) 58.4 (1.9) Ours (8 layers) 60.2 (1.3) Ours (16 layers) 54.1 (8.5) 35.8 38.2 46.1 64.4∗ 42.7 29.5 43.1 (1.2) 46.2 (2.3) 47.9 (1.2) 43.9 (5.7) w/ gold POS tags (for reference only) DMV (Klein and Manning, 2004) 55.1 UR-A E-DMV (Tu and Honavar, 2012) 71.4 MaxEnc (Le and Zuidema, 2015) 73.2 Neural E-DMV (Jiang et al., 2016) 72.5 CRFAE (Cai et al., 2017) 71.7 L-NDMV (Big training data) (Han et al., 2017) 77.2 39.7 57.0 65.8 57.6 55.7 63.2 System For"
D18-1160,D12-1086,0,0.131675,"re our approach with basic HMM, Gaussian HMM, and several stateof-the-art systems, including sophisticated HMM variants and clustering techniques with handengineered features. The results are presented in Table 1. Through the introduced latent embeddings and additional neural projection, our approach improves over the Gaussian HMM by 5.4 points in M-1 and 5.6 points in VM. Neural HMM 4 This is the default parameter initialization in PyTorch. w/ hand-engineered features Feature HMM (Berg-Kirkpatrick et al., 2010) 75.5 Brown (+ proto) (Christodoulopoulos et al., 2010) 76.1 Cluster (word-based) (Yatbaz et al., 2012) 80.2 Cluster (token-based) (Yatbaz et al., 2014) 79.5 VM 53.8 69.8 54.2 66.1 71.7 68.5 (0.5) 73.0 (0.7) 74.1 (0.7) 70.5 (2.1) – 68.8 72.1 69.1 Table 1: Unsupervised POS tagging results on entire WSJ, 0.01 0.18 0.50 0.19 0.12 0.01 0.00 0.00 0.00 0.98 NN NNS NNP NNPS Others (a) Gaussian HMM NN 0.01 0.00 0.35 0.06 0.59 NNS NNS 0.13 0.48 0.02 0.00 0.37 0.78 0.00 0.00 0.02 0.21 0.03 0.89 0.00 0.02 0.06 NNP NN 0.33 0.00 0.02 0.00 0.65 NNP compared with other baselines and state-of-the-art systems. Standard deviation is given in parentheses when available. Others NNPS Setup. Following existing liter"
D18-1160,C14-1217,0,0.0312024,"Missing"
D18-1160,P18-1070,1,0.760408,"Work Our approach is related to flow-based generative models, which are first described in NICE (Dinh et al., 2014) and have recently received more attention (Dinh et al., 2016; Jacobsen et al., 2018; Kingma and Dhariwal, 2018). This relevant work mostly adopts simple (e.g. Gaussian) and fixed priors and does not attempt to learn interpretable latent structures. Another related generative model class is variational auto-encoders (VAEs) (Kingma and Welling, 2013) that optimize a lower bound on the marginal data likelihood, and can be extended to learn latent structures (Miao and Blunsom, 2016; Yin et al., 2018). Against the flow-based models, VAEs remove the invertibility constraint but sacrifice the merits of exact inference and exact log likelihood computation, which potentially results in optimization challenges (Kingma et al., 2016). Our approach can also be viewed in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014) that is a likelihood-free framework to learn implicit generative models. However, it is nontrivial for a gradient-based method like GANs to propagate gradients through discrete structures. 6 Conclusion In this work, we define a novel generative approac"
D18-1160,D07-1043,0,0.13523,"l variance of the word vectors. Following Lin et al. (2015), the covariance matrix is fixed during training. The multinomial probabilities are initialized as θkv ∝ exp(ukv ), where ukv ∼ U [0, 1]. For our approach, we initialize the syntax model and Gaussian parameters with the pre-trained Gaussian HMM. The weights of layers in the rectified network are initialized from a uniform distribution p with mean zero and a standard deviation of 1/nin , where nin is the input dimension.4 We evaluate the performance of POS tagging with both Many-to-One (M-1) accuracy (Johnson, 2007) and V-Measure (VM) (Rosenberg and Hirschberg, 2007). Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsupervised criterion to select the trained model over 10 random restarts after training 50 epochs. We repeat this process 5 times and report the mean and standard deviation of performance. 0.01 0.00 0.32 0.30 0.37 Others NNPS 4.3 System 0.01 0.20 0.01 0.47 0.31 0.02 0.00 0.00 0.00 0.98 NN NNS NNP NNPS Others (b) Our approach Figure 4: Normalized Confusion matrix for POS tagging experiments, row label represents the gold tag. (NHMM) (Tran et al."
D18-1160,D11-1118,0,0.485595,"l that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntactic categories, in our approach, we treat each tag as a latent variable, as described in §2.3. Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies"
D18-1160,W11-0303,0,0.304323,"l that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntactic categories, in our approach, we treat each tag as a latent variable, as described in §2.3. Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories (Spitkovsky et al., 2011a). However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) (Spitkovsky et al., 2011a,b, 2012, 2013), acoustic cues (Pate and Goldwater, 2013), additional training data (Pate and Johnson, 2016), or annotated data from related languages (Cohen et al., 2011). Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies"
D18-1160,W12-1903,0,0.0451891,"Missing"
D18-1160,D13-1204,0,0.323605,"Missing"
D18-1160,W10-2902,0,0.048632,"a linguistic information. Setup. Like previous work we use sections 0221 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 6 10, “headpercolation” rules (Collins, 1999) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended DMV (E-DMV) (Headden III et al., 2009) and Gaussian DMV (which treats POS tag as unknown latent variables and generates observed word embeddings directly conditioned on them following Gaussian distribution) as baselines. Basic DMV and E-DMV are trained with Viterbi EM (Spitkovsky et al., 2010) on unsupervised POS tags induced from our Markov-structured model described in §4.3. Multinomial parameters of the syntax model in both Gaussian DMV and our model are initialized with the pre-trained DMV baseline. Other all Table 2: Directed dependency accuracy on section 23 of WSJ, evaluating on sentences of length 6 10 and all lengths. Starred entries (∗) denote that the system benefits from additional punctuation-based constraints. Standard deviation is given in parentheses when available. parameters are initialized in the same way as in the POS tagging experiment. The directed dependency"
D18-1436,D16-1125,0,0.0977218,"y ensure consistency in the latent variables for different sentences of a given data point i.e the model does not make explicit use of the fact that sentences report non-overlapping visual differences. Enforcing this knowledge while retaining the feasibility of training is a potential future direction of work. 6 Related Work Modeling pragmatics: The dataset presents an opportunity to test methods which can model pragmatics and reason about semantic, spatial and visual similarity to generate a textual description of what has changed from one image to another. Some prior work in this direction (Andreas and Klein, 2016; Vedantam et al., 2017) contrastively describe a target scene in presence of a distractor. In another related task – referring expression comprehension (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017) – the model has to identify which object in the image is being referred to by the given sentence. However, our proposed task comes with a pragmatic goal related to summarization: the goal is to identify and describe all the differences. Since the goal is well defined, it may be used to constrain models that attempt to learn how humans describe visual difference. 4031 Natural language"
D18-1436,J93-2003,0,0.0624758,"erm P (zi |X; w) represents the prior over the latent variable zi and is parameterized in a way that lets the model learn which types of clusters are visually salient. The term P (Si |zi , X; θ) represents the likelihood of sentence Si given the input image pair and alignment zi . We employ masking and attention mechanisms to encourage this decoder to focus on the cluster chosen by zi . Each of these components conditions on visual features produced by a pretrained image encoder. The alignment variable zi for each sentence is chosen independently, and thus our model is similar to IBM Model 1 (Brown et al., 1993) in terms of its factorization structure. This will allow tractable learning and inference as described in Section 3.3. We refer to our approach as DDLA (Difference Description with Latent Alignment). Alignment prior: We define a learnable prior over alignment variable zi . In particular, we let the multinomial distribution on zi be parameterized in a log-linear fashion using feature function g(zi ). Specifically, we consider the following four features: the length, width, and area of the smallest rectangular region enclosing cluster zi , and the number of active elements in mask Czi . Specifi"
D18-1436,W14-3348,0,0.0156848,"using greedy decoding. Since typically there are more clusters than sentences, we condition on the ground truth number of sentences and choose the corresponding number of clusters. We rank clusters by decreasing likelihood under the alignment prior and then choose the top T . 4 Experiments We split videos used to create the dataset into train, test, and validation in the ratio 80:10:10. This is done to ensure that all data points using images from the same video are entirely in one split. We report quantitative metrics like CIDEr (Vedantam et al., 2015), BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and ROUGE-L, as is often reported by works in image captioning. We report these measures for both sentence level setting and multi-sentence generation settings. Thereafter, we also discuss some qualitative examples. We implement our models in PyTorch (Paszke et al., 2017). We use mini-batches of size 8 and use Adam optimizer1 . We use CIDEr scores on validation set as a criteria for early stopping. Baseline models: We consider following baseline models: C APT model considers soft attention over the input pair of images (This atten1 Our data set can be obtained through https:// github.com/har"
D18-1436,P18-1154,1,0.853255,"be more complex rather than having a single unchanged class. Though image diff detection is part of our pipeline, our end task is to generate natural language descriptors. Moreover, we observe that simple clustering seems to work well for our dataset. Other relevant works: Maji (2012) aim to construct a lexicon of parts and attributes by formulating an annotation task where annotators are asked to describe differences between two images. Some other related works model phrases describing change in color (Winn and Muresan, 2018), move-by-move game commentary for describing change in game state (Jhamtani et al., 2018), and code commit message summarizing changes in code-base from one commit to another (Jiang et al., 2017). There exist some prior works on fine grained image classification and captioning (Wah et al., 2014; Nilsback and Zisserman, 2006; Khosla et al., 2011). The premise of such works is that it is difficult for machine to find discriminative features between similar objects e.g. birds of different species. Such works are relevant for us as the type of data we deal with are usually of same object or scene taken at a different time or conditions. 7 Conclusion In this paper, we proposed the new"
D18-1436,D14-1086,0,0.105935,"ping visual differences. Enforcing this knowledge while retaining the feasibility of training is a potential future direction of work. 6 Related Work Modeling pragmatics: The dataset presents an opportunity to test methods which can model pragmatics and reason about semantic, spatial and visual similarity to generate a textual description of what has changed from one image to another. Some prior work in this direction (Andreas and Klein, 2016; Vedantam et al., 2017) contrastively describe a target scene in presence of a distractor. In another related task – referring expression comprehension (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017) – the model has to identify which object in the image is being referred to by the given sentence. However, our proposed task comes with a pragmatic goal related to summarization: the goal is to identify and describe all the differences. Since the goal is well defined, it may be used to constrain models that attempt to learn how humans describe visual difference. 4031 Natural language generation: Natural language generation (NLG) has a rich history of previous work, including, for example, recent works on biography generation (Lebret et al., 2016), weather r"
D18-1436,D16-1032,0,0.014036,"tify which object in the image is being referred to by the given sentence. However, our proposed task comes with a pragmatic goal related to summarization: the goal is to identify and describe all the differences. Since the goal is well defined, it may be used to constrain models that attempt to learn how humans describe visual difference. 4031 Natural language generation: Natural language generation (NLG) has a rich history of previous work, including, for example, recent works on biography generation (Lebret et al., 2016), weather report generation (Mei et al., 2016), and recipe generation (Kiddon et al., 2016). Our task can viewed as a potential benchmark for coherent multi-sentence text generation since it involves assembling multiple sentences to succinctly cover a set of differences. Visual grounding: Our dataset may also provide a useful benchmark for training unsupervised and semi-supervised models that learn to align vision and language. Plummer et al. (2015) collected annotation for phrase-region alignment in an image captioning dataset, and follow up work has attempted to predict these alignments (Wang et al., 2016; Plummer et al., 2017; Rohrbach et al., 2016). Our proposed dataset poses a"
D18-1436,D16-1128,0,0.0698893,"Missing"
D18-1436,N16-1086,0,0.027048,"; Hu et al., 2017) – the model has to identify which object in the image is being referred to by the given sentence. However, our proposed task comes with a pragmatic goal related to summarization: the goal is to identify and describe all the differences. Since the goal is well defined, it may be used to constrain models that attempt to learn how humans describe visual difference. 4031 Natural language generation: Natural language generation (NLG) has a rich history of previous work, including, for example, recent works on biography generation (Lebret et al., 2016), weather report generation (Mei et al., 2016), and recipe generation (Kiddon et al., 2016). Our task can viewed as a potential benchmark for coherent multi-sentence text generation since it involves assembling multiple sentences to succinctly cover a set of differences. Visual grounding: Our dataset may also provide a useful benchmark for training unsupervised and semi-supervised models that learn to align vision and language. Plummer et al. (2015) collected annotation for phrase-region alignment in an image captioning dataset, and follow up work has attempted to predict these alignments (Wang et al., 2016; Plummer et al., 2017; Rohrbach"
D18-1436,P02-1040,0,0.10304,"single sentence for each cluster using greedy decoding. Since typically there are more clusters than sentences, we condition on the ground truth number of sentences and choose the corresponding number of clusters. We rank clusters by decreasing likelihood under the alignment prior and then choose the top T . 4 Experiments We split videos used to create the dataset into train, test, and validation in the ratio 80:10:10. This is done to ensure that all data points using images from the same video are entirely in one split. We report quantitative metrics like CIDEr (Vedantam et al., 2015), BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and ROUGE-L, as is often reported by works in image captioning. We report these measures for both sentence level setting and multi-sentence generation settings. Thereafter, we also discuss some qualitative examples. We implement our models in PyTorch (Paszke et al., 2017). We use mini-batches of size 8 and use Adam optimizer1 . We use CIDEr scores on validation set as a criteria for early stopping. Baseline models: We consider following baseline models: C APT model considers soft attention over the input pair of images (This atten1 Our data set can be obta"
D18-1436,D15-1044,0,0.0444025,"o assist humans in various tasks. For example, image captioning systems (Vinyals et al., 2015b; Xu et al., 2015; Rennie et al., 2017; Zhang et al., 2017) and visual question answering systems (Antol et al., 2015; Lu et al., 2016; Xu and Saenko, 2016) can help visually impaired people in interacting with the world. Another way in which machines can assist humans is by identifying meaningful patterns in data, selecting and combining salient patterns, and generating concise and fluent ‘humanconsumable’ descriptions. For instance, text summarization (Mani and Maybury, 1999; Gupta and Lehal, 2010; Rush et al., 2015) has been a long standing problem in natural language processing aimed at providing a concise text summary of a collection of documents. In this paper, we propose a new task and accompanying dataset that combines elements of image captioning and summarization: the goal of ‘spotthe-diff’ is to generate a succinct text description of all the salient differences between a pair of similar images. Apart from being a fun puzzle, solutions to this task may have applications in assisted surveillance, as well as computer assisted tracking of changes in media assets. We collect and release a novel datas"
D18-1436,P18-2125,0,0.0316814,"ensing images. Zanetti and Bruzzone (2016) propose a method that allows unchanged class to be more complex rather than having a single unchanged class. Though image diff detection is part of our pipeline, our end task is to generate natural language descriptors. Moreover, we observe that simple clustering seems to work well for our dataset. Other relevant works: Maji (2012) aim to construct a lexicon of parts and attributes by formulating an annotation task where annotators are asked to describe differences between two images. Some other related works model phrases describing change in color (Winn and Muresan, 2018), move-by-move game commentary for describing change in game state (Jhamtani et al., 2018), and code commit message summarizing changes in code-base from one commit to another (Jiang et al., 2017). There exist some prior works on fine grained image classification and captioning (Wah et al., 2014; Nilsback and Zisserman, 2006; Khosla et al., 2011). The premise of such works is that it is difficult for machine to find discriminative features between similar objects e.g. birds of different species. Such works are relevant for us as the type of data we deal with are usually of same object or scene"
D18-1496,W15-1526,0,0.0207021,"t 1 2 3 4 5 btc gameplay tutori cyclist dev currenc kitti bitcoin rpg crypto url_youtu url_leagueoflegends url_businessinsider url_twitter url_redd url_snopes comey pede macron pg13 maga globalist ucf committe cuck distributor maduro venezuelan ballot puerto catalonia rican quak skateboard venezuela quebec nra scotus opioid cheney nevada metallica marijuana vermont colorado xanax Table 3: Words with the highest emission weight for various comment-level and thread-level bits. to do so. The gap between comment-level RS and LDA is also consistent with LDA’s known issues dealing with sparse data (Sridhar, 2015), and lends credence to our theory that distributed topic representations are better suited to such domains. 6 Qualitative Analysis of Topics We now offer qualitative analysis of the topic embeddings learned by our model. Note that since we use distributed embeddings, our bits are more akin to filters than complete distributions over words, and we typically observe as many as half of them active for a single comment. In a sense, we have an exponential number of topics, whose parameterization simply factors over the bits. Therefore, it can be difficult to interpret them as one would interpret t"
D19-1225,P13-1021,1,0.814248,"he structure of each underlying form in order to be human readable – as shown in Figure 1. Modeling these stylistic attributes and how they compose with underlying character structure could aid typographic analysis and even allow for automatic generation of novel fonts. Further, the variability of these stylistic features presents a challenge for optical character recognition systems, which typically presume a library of known fonts. In the case of historical document recognition, for example, this problem is more pronounced due to the wide range of lost, ancestral fonts present in such data (Berg-Kirkpatrick et al., 2013; BergKirkpatrick and Klein, 2014). Models that capture this wide stylistic variation of glyph images may eventually be useful for improving optical character recognition on unknown fonts. In this work we present a probabilistic latent variable model capable of disentangling stylistic features of fonts from the underlying structure of each character. Our model represents the style of each font as a vector-valued latent variable, and parameterizes the structure of each character as a learned embedding. Critically, each style latent variable is shared by all characters within a font, while chara"
D19-1225,P14-2020,1,0.872487,"Missing"
D19-1370,K16-1002,0,0.17065,"(Yang et al., 2017; Xu and Durrett, 2018). 3 Pelsmaeker and Aziz (2019) thoroughly investigate using more complicated priors/posteriors (Rezende and Mohamed, 2015) but find only marginal improvements. PPL with ELBO directly since the gap between ELBO and log marginal likelihood might be large, particularly when the posterior does not collapse. Reconstruction loss (Recon). Reconstruction loss is equivalent to the negative reconstruction term in ELBO: −Ez∼qφ (z|x) [log pθ (x|z)]. It characterizes how well the latent code can be used to recover the input. Number of active units (AU, Burda et al. (2016)). Active units correspond to the dimensions of z that covary with observations after the model is trained. More active units usually indicates richer latent representations (Burda et al., 2016). Specifically, a dimension is “active” when it is sensitive to the change in observations x. Here we follow (Burda et al., 2016) and classify a latent dimension z as active if Cov(x, Ez∼q(z|x) [z])) &gt; 0.01. In addition to the metrics above, we include KL between prior and posterior approximation, as well as the negative ELBO, for reference – though we find that these quantities are only partially descr"
D19-1370,D17-1066,0,0.0330264,"17; Xu and Durrett, 2018; Kim et al., 2018; Dieng et al., 2018; He et al., 2019; Pelsmaeker and Aziz, 2019). When a strong decoder (e.g. the LSTM (Hochreiter and Schmidhuber, 1997)) is employed, training often falls into a trivial local optimum where the decoder learns to ignore the latent variable and the encoder fails to encode any information. This phenomenon is referred to as “posterior collapse” (Bowman et al., 2016). Existing efforts tackling this problem include re-weighting the KL loss (Bowman et al., 2016; Kingma et al., 2016; Liu et al., 2019), changing the model (Yang et al., 2017; Semeniuta et al., 2017; Xu and Durrett, 2018), and modifying the training procedure (He et al., 2019). After conducting an empirical examination of the state-of-the-art methods (Section 2), we find that they have difficulty striking a good balance between language modeling and representation learning. In this paper, we present a practically effective combination of two simple heuristic techniques for improving VAE learning: (1) pretraining the inference network using an autoencoder objective and (2) thresholding the KL term in the ELBO objective (also known as “free bits” (Kingma et al., 2016)). The former techniqu"
D19-1370,D18-1480,0,0.088437,"ariable models using neural networks. The generative model of VAEs first samples a latent vector z from a prior p(z), then applies a neural decoder p(x|z) to produce x conditioned on the latent code z. VAEs are trained ∗ Equal contribution. Code is available at https://github.com/ bohanli/vae-pretraining-encoder. 1 where qφ (z|x) represents an approximate posterior distribution (i.e. the encoder or inference network) and pθ (x|z) is the generative distribution (i.e. the decoder). However, modeling text with VAEs has proven to be challenging, and is an open research problem (Yang et al., 2017; Xu and Durrett, 2018; Kim et al., 2018; Dieng et al., 2018; He et al., 2019; Pelsmaeker and Aziz, 2019). When a strong decoder (e.g. the LSTM (Hochreiter and Schmidhuber, 1997)) is employed, training often falls into a trivial local optimum where the decoder learns to ignore the latent variable and the encoder fails to encode any information. This phenomenon is referred to as “posterior collapse” (Bowman et al., 2016). Existing efforts tackling this problem include re-weighting the KL loss (Bowman et al., 2016; Kingma et al., 2016; Liu et al., 2019), changing the model (Yang et al., 2017; Semeniuta et al., 2017;"
D19-1621,N18-1016,0,0.0465217,"Missing"
D19-1621,N18-2011,0,0.571679,"018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candidate outputs to encourage desired behavior such as presence of explicitly-defined rhyming patterns. We propose an adversarial approach to poetry generation that, by adding structure and inductive bias into the discriminator, is able to learn rhyming constraints directly from data without prior knowledge. The role of the discriminator is to try to distinguish between generated and real poems during training. We propose to add inductive bias via the choice of discriminato"
D19-1621,D16-1126,0,0.213792,"Missing"
D19-1621,P17-4008,0,0.16672,"Missing"
D19-1621,P18-1152,0,0.0251601,"using the discriminator to compare poems based only on a learned similarity matrix of pairs of line ending words, the proposed approach is able to successfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information. 1 Introduction Many existing approaches to text generation rely on recurrent neural networks trained using likelihood on sequences of words or characters. However, such models often fail to capture overall structure and coherency in multi-sentence or longform text (Bosselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018)"
D19-1621,P17-1016,0,0.1272,"Missing"
D19-1621,P18-1181,0,0.251365,"criminator architecture: We require the discriminator to reason about poems through pairwise comparisons between line ending words. These learned word comparisons form a similarity matrix for the poem within the discriminator’s architecture. Finally, the discriminator evaluates the poem through a 2D convolutional classifier applied directly to this matrix. This final convolution is naturally biased to identify spatial patterns across word comparisons, which, in turn, biases learned word comparisons to pick up rhyming since rhymes are typically the most salient spatial patterns. Recent work by Lau et al. (2018) proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns. They train the classifier along with a language model in a multi-task setup. Further, at generation time, they heavily rely on rejection sampling to produce quatrains which satisfy any valid rhyming pattern. In contrast, we find that generators trained us"
D19-1621,D16-1127,0,0.0434765,"ch is able to successfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information. 1 Introduction Many existing approaches to text generation rely on recurrent neural networks trained using likelihood on sequences of words or characters. However, such models often fail to capture overall structure and coherency in multi-sentence or longform text (Bosselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candi"
D19-1621,P19-1192,0,0.0283185,"Missing"
D19-1621,W17-3502,0,0.0565797,"sselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candidate outputs to encourage desired behavior such as presence of explicitly-defined rhyming patterns. We propose an adversarial approach to poetry generation that, by adding structure and inductive bias into the discriminator, is able to learn rhyming constraints directly from data without prior knowledge. The role of the discriminator is to try to distinguish between generated and real poems during training. We propose to add inductive bias v"
D19-1621,N19-1013,0,0.0656129,"Missing"
D19-1621,P11-2014,0,0.529802,"d representations g(.) to check if rhyming words are close-by in the learned manifold. We consider all pairs of words among the ending words in a quatrain/limerick, and label each pair to be rhyming or non-rhyming based on previously stated definition of rhyming. If the cosine similarity score between the representations of pairs of words is above a certain threshold, we predict that word pair as rhyming, else it is predicted as non-rhyming. We report F1 scores for the binary classification setup of predicting word-pairs to be rhyming or not. We consider some additional baselines: R HYM - EM (Reddy and Knight, 2011) uses latent variables to model rhyming schemes, and train parameters using EM. GRAPHEME - K baselines predict a word pair as rhyming only if the last K = {1, 2, 3} characters of the two words are same. For S ONNET data, we observe that R HYME GAN obtains a F1 score of 0.90 (Table 3) on the test split (threshold chosen to maximize f1 on dev split). We repeat the above analysis on the L IMERICK dataset and observe an F1 of 0.92 for R HYME - GAN. D EEP - SPEARE model reports F1 6028 Model GRAPHEME -1 GRAPHEME -2 GRAPHEME -3 R HYM - EM D EEP - SPEARE/M AX -M ARGIN R HYME - GAN - NS R HYME - GAN S"
D19-1621,N19-5001,0,0.0580344,"Missing"
D19-1621,P18-1083,0,0.0259984,"p words to satisfy poetry constraints. Ghazvininejad et al. (2018) worked on poetry translation using an unconstrained machine translation model and separately learned Finite State Automata for enforcing rhythm and rhyme. Similar to rhyming and rhythm patterns in poetry, certain types of musical compositions showcase rhythm and repetition patterns, and some prior works model such patterns in music generation (Walder and Kim, 2018; Jhamtani and BergKirkpatrick, 2019). Generative adversarial learning (Goodfellow et al., 2014) for text generation has been used in prior works (Fedus et al., 2018; Wang et al., 2018, 2019; Rao and Daum´e III, 2019), though has not been explored with regard to the similarity structure proposed in this paper. 5 Conclusions In this paper we have proposed a novel structured discriminator to learn a poem generator. The generator learned utilizing the structured adversary is able to identify rhyming structure patterns present in data, as demonstrated through the improved sampling efficiency. Through the rhyming classification probe, we demonstrate that the proposed discriminator is better at learning the notion of rhyming compared to baselines. Acknowledgements We are thankful"
D19-1621,D14-1074,0,0.0615192,"Missing"
D19-1621,D17-1062,0,0.0209209,"cessfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information. 1 Introduction Many existing approaches to text generation rely on recurrent neural networks trained using likelihood on sequences of words or characters. However, such models often fail to capture overall structure and coherency in multi-sentence or longform text (Bosselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candidate outputs to encourag"
N10-1083,H05-1009,0,0.0250886,": D ICTIONARY: S TEM: P REFIX: C HARACTER: (e = ·, y = ·) (dist(y, e) = ·) ((y, e) ∈ D) for dictionary D. (stem(e) = ·, y = ·) for Porter stemmer. (prefix(e) = ·, y = ·) for prefixes of length 4. (e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, whil"
N10-1083,N09-1009,0,0.61572,"noulli STOP probabilities θd,c,STOP capture the valence of a particular head. For this type, the decision d is whether or not to stop generating arguments, and the context c contains the current head h, direction δ and adjacency adj. If a head’s stop probability is high, it will be encouraged to accept few arguments. The ATTACH multinomial probability distributions θd,c,ATTACH capture attachment preferences of heads. For this type, a decision d is an argument token a, and the context c consists of a head h and a direction δ. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold POS tags in place of words. 3 Haghighi and Klein (2006) achieve higher accuracies by making use of labeled prototypes. We do not use any external information. 586 5.2 Grammar Induction Features One way to inject knowledge into a dependency model is to encode the similarity between the various morphological variants of nouns and verbs. We encode this similarity by incorporating features into both the STOP and the ATTACH probabilities. The attachment features appear below; the stop feature templates are similar and are therefore omitted. (a = ·, h = ·, δ = ·) Generalize the morphol"
N10-1083,P07-1003,1,0.716969,"ages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and us588 For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presented in Liang and Klein (2009) by adding phonological features. Unsupervised word segmentation is the task of identifying"
N10-1083,A94-1009,0,0.149142,"h task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z,"
N10-1083,P07-1094,0,0.456947,"stically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z, EMIT)i ′ y ′ exp hw, f (y , z, EMIT )i θy,z,EMIT (w) = P"
N10-1083,P06-1085,0,0.0182129,"ution: θl,LENGTH = exp(−l1.6 ) when 1 ≤ l ≤ 10. Their model is deficient since it is possible to generate 6 The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P (Y = y, Z = z) = θSTOP |z| Y   (1 − θSTOP ) θzi ,SEGMENT exp(−|zi |1.6 ) i=1 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word segmentation models for 300 iterations with 10 random initializations and report the mean and standard deviation of F1 in Table 1. 7.3 Segmentation Features The SEGMENT multinomial is the important distribution in this model. We use the following features: BASIC: L ENGTH: N UMBER -VOWELS: P HONO -C LASS -P REF: P HONO -C LASS -P REF: (z = ·) (length(z) = ·) (numVowels(z) = ·) (prefix(coarsePhone"
N10-1083,P09-1104,1,0.664262,". For this type, there is no context and the decision is the particular string generated. In order to avoid the degenerate MLE that assigns mass only to single segment sentences it is helpful to independently generate a length for each segment from a fixed distribution. Liang and Klein (2009) constrain individual segments to have maximum length 10 and generate lengths from the following distribution: θl,LENGTH = exp(−l1.6 ) when 1 ≤ l ≤ 10. Their model is deficient since it is possible to generate 6 The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P (Y = y, Z = z) = θSTOP |z| Y   (1 − θSTOP ) θzi ,SEGMENT exp(−|zi |1.6 ) i=1 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word"
N10-1083,N09-1036,0,0.361345,"lish sentence e. The likelihood of both models takes the form: Y p(zj = i|zj−1 ) · θyj ,ei ,ALIGN P (y, z|e) = j 4 Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. 587 WSJ WSJ10 – 1.0 5.0 Grammar Induction Basic-DMV Feature-DMV Inference EM LBFGS EM LBFGS EM EM LBFGS (Cohen and Smith, 2009) Basic-DMV EM Feature-DMV EM LBFGS (Cohen and Smith, 2009) Word Alignment Basic-Model 1 EM Feature-Model 1 EM Basic-HMM EM Feature-HMM EM Word Segmentation Basic-Unigram EM Feature-Unigram EM LBFGS (Johnson and Goldwater, 2009) BR NIST ChEn 5.4 Grammar Induction Results We are able to outperform Cohen and Smith’s (2009) best system, which requires a more complicated variational inference method, on both English and Chinese data sets. Their system achieves an accuracy of 61.3 for English and an accuracy of 51.9 for Chinese.4 Our feature-enhanced model, trained using the direct gradient approach, achieves an accuracy of 63.0 for English, and an accuracy of 53.6 for Chinese. To our knowledge, our method for featurebased dependency parse induction outperforms all existing methods that make the same set of conditional in"
N10-1083,D07-1031,0,0.900802,"lates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z, EMIT)i ′ y ′ exp hw, f (y , z, EMIT )i θy,z,EMIT (w) = P This feature-ba"
N10-1083,P04-1061,1,0.924351,"margin of 12.4. These results show that the direct gradient approach can offer additional boosts in performance when used with a feature-enhanced model. We also outperform the globally normalized MRF, which uses the same set of features and which we train using a direct gradient approach. To the best of our knowledge, our system achieves the best performance to date on the WSJ corpus for totally unsupervised POS tagging.3 5 Grammar Induction We next apply our technique to a grammar induction task: the unsupervised learning of dependency parse trees via the dependency model with valence (DMV) (Klein and Manning, 2004). A dependency parse is a directed tree over tokens in a sentence. Each edge of the tree specifies a directed dependency from a head token to a dependent, or argument token. Thus, the number of dependencies in a parse is exactly the number of tokens in the sentence, not counting the artificial root token. 5.1 Dependency Model with Valence The DMV defines a probability distribution over dependency parse trees. In this head-outward attachment model, a parse and the word tokens are derived together through a recursive generative process. For each token generated so far, starting with the root, a"
N10-1083,N09-1069,1,0.872403,"models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and us588 For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presented in Liang and Klein (2009) by adding phonological features. Unsupervised word segmentation is the task of identifying word boundaries in sentences where spaces have been removed. For a sequence of characters y = (y1 , ..., yn ), a segmentation is a sequence of segments z = (z1 , ..., z|z |) such that z is a partition of y and each zi is a contiguous subsequence of y. Unsupervised models for this task infer word boundaries from corpora of sentences of characters without ever seeing examples of well-formed words. 7.1 Unigram Double-Exponential Model Liang and Klein’s (2009) unigram doubleexponential model corresponds to"
N10-1083,N06-1014,1,0.244933,"each optimization procedure for 100 iterations. The results are reported in Table 1. κ – – – – κ – 0.2 0.2 Eval Many-1 63.1 (1.3) 59.6 (6.9) 68.1 (1.7) 75.5 (1.1) Dir 47.8 48.3 63.0 61.3 42.5 49.9 53.6 51.9 AER 38.0 35.6 33.8 30.0 F1 76.9 (0.1) 84.5 (0.5) 88.0 (0.1) 87 Table 1: Locally normalized feature-based models outperform all proposed baselines for all four tasks. LBFGS outperformed EM in all cases where the algorithm was sufficiently fast to run. Details of each experiment appear in the main text. The distortion term p(zj = i|zj−1 ) is uniform in Model 1, and Markovian in the HMM. See Liang et al. (2006) for details on the specific variant of the distortion model of the HMM that we used. We use these standard distortion models in both the baseline and feature-enhanced word alignment systems. The bilexical emission model θy,e,ALIGN differentiates our feature-enhanced system from the baseline system. In the former, the emission model is a standard conditional multinomial that represents the probability that decision word y is generated from context word e, while in our system, the emission model is re-parameterized as a logistic regression model and feature-enhanced. Many supervised feature-bas"
N10-1083,J93-2004,0,0.0463736,"ate for words with certain orthographic properties. We use only the BASIC features for transitions. For an emission with word y and tag z, we use the following feature templates: (y = ·, z = ·) Check if y contains digit and conjoin with z: (containsDigit(y) = ·, z = ·) C ONTAINS -H YPHEN: (containsHyphen(x) = ·, z = ·) I NITIAL -C AP: Check if the first letter of y is capitalized: (isCap(y) = ·, z = ·) N-G RAM: Indicator functions for character ngrams of up to length 3 present in y. BASIC: C ONTAINS -D IGIT: 4.2 POS Induction Data and Evaluation We train and test on the entire WSJ tag corpus (Marcus et al., 1993). We attempt the most difficult version of this task where the only information our system can make use of is the unlabeled text itself. In particular, we do not make use of a tagging dictionary. We use 45 tag clusters, the number of POS tags that appear in the WSJ corpus. There is an identifiability issue when evaluating inferred tags. In order to measure accuracy on the hand-labeled corpus, we map each cluster to the tag that gives the highest accuracy, the many-1 evaluation approach (Johnson, 2007). We run all POS induction models for 1000 iterations, with 10 random initializations. The mea"
N10-1083,J94-2001,0,0.262389,"entation. In each task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions:"
N10-1083,C96-2141,0,0.109148,"or our Chinese experiments, we use the same corpus and training/test split as Cohen and Smith 6 Word Alignment Word alignment is a core machine learning component of statistical machine translation systems, and one of the few NLP tasks that is dominantly solved using unsupervised techniques. The purpose of word alignment models is to induce a correspondence between the words of a sentence and the words of its translation. 6.1 Word Alignment Models We consider two classic generative alignment models that are both used heavily today, IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Ney and Vogel, 1996). These models generate a hidden alignment vector z and an observed foreign sentence y, all conditioned on an observed English sentence e. The likelihood of both models takes the form: Y p(zj = i|zj−1 ) · θyj ,ei ,ALIGN P (y, z|e) = j 4 Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. 587 WSJ WSJ10 – 1.0 5.0 Grammar Induction Basic-DMV Feature-DMV Inference EM LBFGS EM LBFGS EM EM LBFGS (Cohen and Smith, 2009) Basic-DMV EM Feature-DMV EM LBFGS (Cohen and Smith, 2009) Word Alignment Bas"
N10-1083,P00-1056,0,0.0467888,"er. (prefix(e) = ·, y = ·) for prefixes of length 4. (e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the basel"
N10-1083,P05-1044,0,0.794494,"in the form of conditional independence structure, which means that injecting it is both tricky (because the connection between independence and knowledge is subtle) and timeconsuming (because new structure often necessitates new inference algorithms). In this paper, we present a range of experiments wherein we improve existing unsupervised models by declaratively adding richer features. In particular, we parameterize the local multinomials of existThe idea of using features in unsupervised learning is neither new nor even controversial. Many top unsupervised results use feature-based models (Smith and Eisner, 2005; Haghighi and Klein, 2006). However, such approaches have presented their own barriers, from challenging normalization problems, to neighborhood design, to the need for complex optimization procedures. As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models. The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks. We c"
N10-1083,J07-4003,0,0.0056588,"malized models. For models parameterized by standard multinomials, EM optimizes L(θ) = log Pθ (Y = y) (Dempster et al., 1977). The E-step computes expected counts for each tuple of decision d, context c, and multinomial type t: ed,c,t ← Eθ 2 &quot; X i∈I # (Xi = d, Xπ(i) = c, t) Y = y (2) The locally normalized model class is actually equivalent to its globally normalized counterpart when the former meets the following three conditions: (1) The graphical model is a directed tree. (2) The BASIC features are included in f . (3) We do not include regularization in the model (κ = 0). This follows from Smith and Johnson (2007). 584 ed,c,t d′ ed′ ,c,t θd,c,t ← P Normalizing expected counts in this way maximizes the expected complete log likelihood with respect to the current model parameters. EM can likewise optimize L(w) for our locally normalized models with logistic parameterizations. The E-step first precomputes multinomial parameters from w for each decision, context, and type: exphw, f (d, c, t)i ′ d′ exphw, f (d , c, t)i θd,c,t (w) ← P Then, expected counts e are computed according to Equation 2. In the case of POS induction, expected counts are computed with the forwardbackward algorithm in both the standard"
N10-1083,C02-1145,0,0.0134758,"inese data sets. Their system achieves an accuracy of 61.3 for English and an accuracy of 51.9 for Chinese.4 Our feature-enhanced model, trained using the direct gradient approach, achieves an accuracy of 63.0 for English, and an accuracy of 53.6 for Chinese. To our knowledge, our method for featurebased dependency parse induction outperforms all existing methods that make the same set of conditional independence assumptions as the DMV. Reg κ – 0.1 1.0 1.0 κ – 0.05 10.0 CTB10 Model POS Induction Basic-HMM Feature-MRF Feature-HMM (2009). We train on sections 1-270 of the Penn Chinese Treebank (Xue et al., 2002), similarly reduced (CTB10). We test on sections 271-300 of CTB10, and use sections 400-454 as a development set. The DMV is known to be sensitive to initialization. We use the deterministic harmonic initializer from Klein and Manning (2004). We ran each optimization procedure for 100 iterations. The results are reported in Table 1. κ – – – – κ – 0.2 0.2 Eval Many-1 63.1 (1.3) 59.6 (6.9) 68.1 (1.7) 75.5 (1.1) Dir 47.8 48.3 63.0 61.3 42.5 49.9 53.6 51.9 AER 38.0 35.6 33.8 30.0 F1 76.9 (0.1) 84.5 (0.5) 88.0 (0.1) 87 Table 1: Locally normalized feature-based models outperform all proposed baselin"
N10-1083,J93-2003,0,\N,Missing
N10-1083,N06-1041,1,\N,Missing
N10-1083,P01-1027,0,\N,Missing
N10-1083,P06-1111,1,\N,Missing
N13-1132,W10-0701,0,0.033793,", both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy fo"
N13-1132,W10-1703,0,0.0107417,"ments over standard baselines, both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equa"
N13-1132,W02-0102,0,0.0301667,"i=1 P (Ti ) · M Y P (Sij ; θj ) · P (Aij |Sij , Ti ; ξj ) i j=1 where A is the matrix of annotations, S is the matrix of competence indicators, and T is the vector of true labels. We maximize the marginal data likelihood using Expectation Maximization (EM) (Dempster et al., 1977), which has successfully been applied to similar problems (Dawid and Skene, 1979). We initialize EM randomly and run for 50 iterations. We perform 100 random restarts, and keep the model with the best marginal data likelihood. We smooth the M-step by adding a fixed value δ to the fractional counts before normalizing (Eisner, 2002). We find that smoothing improves accuracy, but, overall, learning is robust to varying δ, and set δ = num0.1 labels . 1122 3.1 Natural Data In order to evaluate our model, we use the datasets from (Snow et al., 2008) that use discrete label values (some tasks used continuous values, which we currently do not model). Since they compared AMT annotations to experts, gold annotations exist for these sets. We can thus evaluate the accuracy of the model as well as the proficiency of each annotator. We show results for word sense disambiguation (WSD: 177 items, 34 annotators), recognizing textual en"
N13-1132,W10-0702,0,0.17971,"cy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy for majority voting"
N13-1132,D07-1031,0,0.00707372,"Variational-Bayes (VB) training with symmetric Beta priors on θj and symmetric Dirichlet priors on the strategy parameters, ξj . Setting the shape parameters of the Beta distribution to 0.5 favors the extremes of the distribution, i.e., either an annotator tried to get the right answer, or simply did not care, but (almost) nobody tried “a little”. With VB training, we observe improved correlations over all test sets with no loss in accuracy. The hyper-parameters of the Dirichlet distribution on ξj were clamped to 10.0 for all our experiments with VB training. Our implementation is similar to Johnson (2007), which the reader can refer to for details. 3 Experiments We evaluate our method on existing annotated datasets from various AMT tasks. However, we also want to ensure that our model can handle adversarial conditions. Since we have no control over the factors in existing datasets, we create synthetic data for this purpose. P (A; θ, ξ) = N XhY T,S i=1 P (Ti ) · M Y P (Sij ; θj ) · P (Aij |Sij , Ti ; ξj ) i j=1 where A is the matrix of annotations, S is the matrix of competence indicators, and T is the vector of true labels. We maximize the marginal data likelihood using Expectation Maximizatio"
N13-1132,D08-1027,0,0.767307,"Missing"
N13-1132,P10-1070,1,0.275755,"78 0.70 0.76 0.87 0.91 Temporal 0.73 0.80 0.73 0.88 0.90 WSD 0.81 0.13 0.81 0.44 0.90 Table 1: Correlation with annotator proficiency: Pearson ρ of different methods for various data sets. MACE-VB’s trustworthiness parameter (trained with Variational Bayes with α = β = 0.5) correlates best with true annotator proficiency. It is natural to apply some form of weighting. One approach is to assume that reliable annotators agree more with others than random annotators. Inter-annotator agreement is thus a good candidate to weigh the answers. There are various measures for inter-annotator agreement. Tratz and Hovy (2010) compute the average agreement of each annotator and use it as a weight to identify reliable ones. Raw agreement can be directly computed from the data. It is related to majority voting, since it will produce high scores for all members of the majority class. Raw agreement is thus a very simple measure. In contrast, Cohen’s κ corrects the agreement between two annotators for chance agreement. It is widely used for inter-annotator agreement in annotation tasks. We also compute the κ values for each pair of annotators, and average them for each annotator (similar to the approach in Tratz and Hov"
N13-1132,P11-1122,0,0.0091563,"ness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy for majority voting to go wrong. A common and simple s"
N13-1132,P10-5004,1,\N,Missing
N15-1109,P14-2020,1,0.788188,"Missing"
N15-1109,P13-1021,1,0.598859,"imilar to data used to train the LM. and where indigenous-language orthographies were first being developed (Baddeley and Voeste, 2013). The 349 digital facsimiles in the Primeros Libros collection are characteristic of this trend. Produced during the first century of Spanish colonization, they represent the introduction of printing technology into the Americas, and reflect the (sometimes conflicted) priorities of the nascent colony, from religious orthodoxy to conversion and education. For our experiments, we focus on multilingual documents in three languages: Spanish, Latin, and Nahuatl. As Berg-Kirkpatrick et al. (2013) show, a language model built on contemporaneous data will perform better than modern data. For this reason, we collected 15–17th century texts from Project Gutenberg,1 producing Spanish and Latin corpora of more than one million characters each. Due to its relative scarcity, we augmented the Nahuatl corpus with a private collection of transcribed colonial documents. 3 Baseline System The starting point for our work is the Ocular system described by Berg-Kirkpatrick et al. (2013). The fonts used in historical documents are usually unknown and can vary drastically from document to document. Ocu"
N15-1109,W14-3907,0,0.0441324,"eir modern forms. Fortunately, this can be done in our approach by running the variability rewrite rules “backward” as a post-processing step. Further technical improvements may be made by having the system automatically attempt to bootstrap the identification of spelling variants, a process that could complement our approach through an active learning setup. Additionally, since even our relatively simple unsupervised code-switch language modeling approach yielded improvements to OCR performance, it may be justified to attempt the adaptation of more complex code-switch recognition techniques (Solorio et al., 2014). The automatic transcription of the Primeros Libros collection has significant implications for scholars of the humanities interested in the role that inscription and transmission play in colonial history. For example, there are parallels between the way that the Spanish transformed indigenous languages into Latin-like writing systems (removing “noise” like phonemes that do not exist in Latin), and the way that the OCR tool transforms historical printed documents into unicode (removing “noise” like artifacts of the printing process and physical changes to the pages); in both instances, arguab"
N18-2123,D16-1203,0,0.103231,"y a human (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2016; Rohrbach et al., 2016; Yu et al., 2016; Nagaraja et al., 2016; Hu et al., 2017). Recent work on RER has sought to make progress by introducing models that are better capable of reasoning about linguistic structure (Hu et al., 2017; Nagaraja et al., 2016) – however, since most of the state-of-the-arts systems involve complex neural parameterizations, what these models actually learn has been difficult to interpret. This is concerning because several post-hoc analyses of related tasks (Zhou et al., 2015; Devlin et al., 2015; Agrawal et al., 2016; Jabri et al., 2016; Goyal et al., 2016) have revealed that some positive results are actually driven by superficial biases in datasets or shallow correlations without deeper visual or linguistic understanding. Evidently, it is hard to be completely sure if a model is performing well for the right reasons. To increase our understanding of how RER systems function, we present several analyses inspired by approaches that probe systems with perturbed inputs (Jia and Liang, 2017) and employ simple models to exploit and reveal biases in datasets (Chen et al., 2016a). First, we investigate whether"
N18-2123,P16-1223,0,0.069549,"Missing"
N18-2123,N18-2017,0,0.0221213,"network (Andreas et al., 2016) where the computation graph is defined in terms of a constituency parse of the input referring expression. Previous studies on other tasks have found that state-of-the-art systems may be successful for reasons different than originally assumed. For example, Chen et al. (2016b) show that a simple logistic regression baseline with carefully defined features can achieve competitive results for reading comprehension on CNN/Daily Mail datasets (Hermann et al., 2015), indicating that more sophisticated models may be learning realtively simple correlations. Similarly, Gururangan et al. (2018) reveal bias in a dataset for semantic inference by demonstrating a simple model that achieves competitive results without looking at the premise. 3 Analysis Methodology To perform our analysis, we take two state-of-theart systems CNN+LSTM-MIL (Nagaraja et al., 2016) and CMN (Hu et al., 2017) and train them from scratch with perturbed referring expressions. We note that the perturbation experiments explained in next subsections are performed on all train and test instances. All experiments are done on the standard train/test splits for the Google-Ref dataset (Mao et al., 2016). Systems are eva"
N18-2123,D17-1215,0,0.0298631,"erpret. This is concerning because several post-hoc analyses of related tasks (Zhou et al., 2015; Devlin et al., 2015; Agrawal et al., 2016; Jabri et al., 2016; Goyal et al., 2016) have revealed that some positive results are actually driven by superficial biases in datasets or shallow correlations without deeper visual or linguistic understanding. Evidently, it is hard to be completely sure if a model is performing well for the right reasons. To increase our understanding of how RER systems function, we present several analyses inspired by approaches that probe systems with perturbed inputs (Jia and Liang, 2017) and employ simple models to exploit and reveal biases in datasets (Chen et al., 2016a). First, we investigate whether systems that were designed to incorporate linguistic structure actually require it and make use of it. To test this, we perform perturbation experiments on the input referring expressions. Surprisingly, we find that models are robust to shuffling the word order and limiting the word categories to nouns and adjectives. Second, we attempt to reveal shallower correlations that systems might instead be leveraging to do well on this task. We build two simple systems called Neural S"
N18-2123,D14-1086,0,0.346215,"xpressions where various aspects of linguistic structure are obscured. We perform three types of analyses: the first one studying syntactic structure (Section 3.2), the second focusing on the importance of word categories (Section 3.3), and the final one analyzing potential biases in the dataset (Section 3.4). 2 3.1 Related Work Referring expression recognition and generation is a well studied problem in intelligent user interfaces (Chai et al., 2004), human-robot interaction (Fang et al., 2012; Chai et al., 2014; Williams et al., 2016), and situated dialogue (Kennington and Schlangen, 2017). Kazemzadeh et al. (2014) and Mao et al. (2016) introduce two benchmark datasets for referring expression recognition. Several models that leverage linguistic structure have been proposed. Nagaraja et al. (2016) propose a model where target and supporting objects (i.e. objects that are mentioned in order to disambiguate the target object) are identified and scored jointly. The resulting model is able to localize supporting objects without direct supervision. Hu et al. (2017) introduce a compositional approach for the RER task. They assume that the referring expression can be decomposed into a triplet consisting of the"
N18-2123,N16-1030,0,0.0435666,"for which the target object is contained in the model’s top-k predictions. We provide further details of our experimental methodology in Section 4.1. 3.2 Syntactic Analysis by Permuting Word Order In English, word order is important for correctly understanding the syntactic structure of a sentence. Both models we analyze use Recurrent Neural Networks (RNN) (Elman, 1990) with Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997). Previous studies have shown that reccurrent architectures can perform well on tasks where word order and syntax are important: for example, tagging (Lample et al., 2016), parsing (Sutskever et al., 2014), and machine translation (Bahdanau et al., 2014). We seek to determine whether recurrent models for RER depend on syntactic structure. Premise 1: Randomly permuting the word order of an English referring expression will obscure its syntactic structure. We train CMN and CNN+LSTM-MIL with shuffled referring expressions as input and evaluate their performance. Analysis by Perturbation In this section, we would like to analyze how the state-of-the-art referring expression recognition systems utilize linguistic structure. We conduct 782 Model CMN LSTM+CNN-MIL No P"
N18-2123,D14-1162,0,0.080959,"onsists of around 26K images with 104K annotations. We use their Ground-Truth evaluation setup where the ground truth bounding box annotations from MSCOCO (Lin et al., 2014) are provided to the system as a part of the input. We used the split provided by Nagaraja et al. (2016) where splits have disjoint sets of images. We use precision@k for evaluating the performance of models. Implementation Details. To train our models, we used stochastic gradient descent for 6 epochs with an initial learning rate of 0.01 and multiplied by 0.4 after each epoch. Word embeddings were initialized using GloVe (Pennington et al., 2014) and finetuned during training. We extracted features for bounding boxes using the fc7 layer output of Faster-RCNN VGG-16 network (Ren et al., 2015) pre-trained on MSCOCO dataset (Lin et al., 2014). Hyperparameters such as hidden layer size of LSTM networks were picked based on the best Baseline Models. We compare Neural Sieves to the state-of-the-art models from the literature. LSTM + CNN - MIL Nagaraja et al. (2016) score target object-context object pairs using LSTMs for processing the referring expression and CNN features for bounding boxes. The pair with the highest score is predicted as"
N19-1171,P16-1231,0,0.540019,"d to those with 1724 Proceedings of NAACL-HLT 2019, pages 1724–1733 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics the more common locally normalized parametrization. We posit that this difference is due to label bias (Bottou, 1991) arising from the interaction of approximate search and search-aware optimization in locally normalized models. A commonly understood source of label bias in locally normalized sequence models is an effect of conditioning only on partial input (for example, only the history of the input) at each step during decoding (Andor et al., 2016; Lafferty et al., 2001; Wiseman and Rush, 2016). We discus another potential source of label bias arising from approximate search with locally normalized models that may be present even with access to the full input at each step. To this end, we train search-aware globally and locally normalized models in an end-to-end (sub)-differentiable manner using a continuous relaxation to the discontinuous beam search procedure introduced by Goyal et al. (2017b). This approach requires initialization with a suitable globally normalized model to work in practice. Hence, we also propose an initialization"
N19-1171,N15-1027,0,0.02122,"model it is: &quot; n # X log s(x, y1:i−1 , yi ) − log ZG (x) i=1 3.2.1 Self Normalization One way to find a locally normalized model that is parametrized like a globally normalized model is to ensure that the local normalizer at each step, log ZL,i (x, y1:i−1 ), is 0. With the local normalizer being zero it is straightforward to see that the log probability of a sequence under a locally normalized model can easily be interpreted as log probability of the sequence under a globally normalized model with the global log-normalizer, log ZG (x) = 0. This training technique is called self-normalization (Andreas and Klein, 2015) because the resulting models’ unnormalized score at each step lies on a probability simplex. A common technique for training self-normalized models is L2-regularization of local log normalizer which encourages learning a model with log Z = 0 and was found to be effective for learning a language model by Devlin et al. (2014)2 . The L2regularized cross entropy objective is given by: min θ X x,y∗ ∈D − n X i=1 log p(yi∗ |x, y1:i−1 ) +λ · (log ZL,i (x, y1:i−1 ))2 In Table 1, we report the mean and variance of the local log normalizer on the two different tasks using L2-regularization (L2) based se"
N19-1171,P84-1044,0,0.311862,"Missing"
N19-1171,hockenmaier-steedman-2002-acquiring,0,0.034186,"entropy trained models and self-normalized models to study the effects of search-aware optimization and global normalization. We follow Goyal et al. (2017b) and use the decomposable Hamming loss approximation with search-aware optimization for both the tasks and decode via soft beam search decoding method which involves continuous beam search with soft backpointers for the LSTM Beam search dynamics as described in Section 3, but using identifiable backpointers and labels (using MAP estimates of soft backpointers and labels) to decode. CCG supertagging We used the standard splits of CCG bank (Hockenmaier and Steedman, 2002) for training, development, and testing. The label space of supertags is 1,284 and the labels are correlated with each other based on their syntactic relations. The distribution of supertag labels in the training data exhibits a long tail distribution. This task is sensitive to the long range sequential decisions because it encodes rich syntactic information about the sentence. Hence, this task is ideal to analyze the effects of label bias and search effects. We perform minor preprocessing on the data similar to the preprocessing in Vaswani et al. (2016). For experiments related to search awar"
N19-1171,2014.iwslt-evaluation.1,0,0.10811,"Missing"
N19-1171,P04-1015,0,0.488966,"zation Src sent-length → pretrain-beam locally-normalized globally-normalized 0-20 29.36 32.35 33.21 20-30 25.73 26.95 28.08 30-40 24.71 25.39 26.75 40+ 24.50 25.2 26.41 Table 6: BLEU scores with different length inputs on dev set Reported on Self-normalized initialization. The header specifies the range of length of the input sentences that globally normalized models perform better on all the length ranges but especially so on long sentences. 5 Related Work Much of the existing work on search-aware training of globally normalized neural sequence models uses some mechanism like early updates (Collins and Roark, 2004) that relies on explicitly tracking if the gold sequence falls off the beam and is not end-to-end continuous. Andor et al. (2016) describe a method for training globally normalized neural feedforward models, which involves optimizing a CRF-based likelihood where the normalizer is approximated by the sum of the scores of the final beam elements. They describe label bias arising out of conditioning on partial input and hence focused on the scenario in which locally normalized models can be less expressive than globally normalized models, whereas we also consider another source of label bias whic"
N19-1171,P14-1129,0,0.111054,"Missing"
N19-1171,D15-1044,0,0.0477999,"arm-starting with pre-trained models, we also propose a novel initialization strategy based on self-normalization for pretraining globally normalized models. We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global normalization under different conditions while using search-aware training. 1 Introduction Neural encoder-decoder models have been tremendously successful at a variety of NLP tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Dyer et al., 2016, 2015), summarization (Rush et al., 2015), dialog generation (Serban et al., 2015), and image captioning (Xu et al., 2015). With these models, the target sequence is generated in a left-to-right step-wise manner with the predictions at every step being conditioned on the input sequence and the whole prediction history. This long-distance memory precludes exact search for the maximally scoring sequence according to the model and therefore, approximate algorithms like greedy search or beam search are necessary in practice during decoding. In this scenario, it is natural to resort to search-aware learning techniques for these models whi"
N19-1171,P15-1033,1,0.853784,"Missing"
N19-1171,J07-4003,0,0.261515,"ware locally normalized sequence models that involve projecting the scores of items in the vocabulary onto a probability simplex at each step and globally normalized/unnormalized sequence models that involve scoring sequences without explicit normalization at each step. When conditioned on the the full input sequence and the entire prediction history, both locally normalized and globally normalized conditional models should have same expressive power under a highcapacity neural parametrization in theory, as they can both model same set of distributions over all finite length output sequences (Smith and Johnson, 2007). However, locally normalized models are constrained in how they respond to search errors during training since the scores at each decoding step must sum to one. To let a search-aware training setup have the most flexibility, abandoning this constraint may be useful for easier optimization. In this paper, we demonstrate that the interaction between approximate inference and nonconvex parameter optimization results in more robust training and better performance for models with global normalization compared to those with 1724 Proceedings of NAACL-HLT 2019, pages 1724–1733 c Minneapolis, Minnesot"
N19-1171,N16-1024,1,0.818841,"e our training approach is sensitive to warm-starting with pre-trained models, we also propose a novel initialization strategy based on self-normalization for pretraining globally normalized models. We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global normalization under different conditions while using search-aware training. 1 Introduction Neural encoder-decoder models have been tremendously successful at a variety of NLP tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Dyer et al., 2016, 2015), summarization (Rush et al., 2015), dialog generation (Serban et al., 2015), and image captioning (Xu et al., 2015). With these models, the target sequence is generated in a left-to-right step-wise manner with the predictions at every step being conditioned on the input sequence and the whole prediction history. This long-distance memory precludes exact search for the maximally scoring sequence according to the model and therefore, approximate algorithms like greedy search or beam search are necessary in practice during decoding. In this scenario, it is natural to resort to search-awar"
N19-1171,P17-2058,1,0.870083,"Missing"
N19-1171,N16-1027,0,0.0944075,"standard splits of CCG bank (Hockenmaier and Steedman, 2002) for training, development, and testing. The label space of supertags is 1,284 and the labels are correlated with each other based on their syntactic relations. The distribution of supertag labels in the training data exhibits a long tail distribution. This task is sensitive to the long range sequential decisions because it encodes rich syntactic information about the sentence. Hence, this task is ideal to analyze the effects of label bias and search effects. We perform minor preprocessing on the data similar to the preprocessing in Vaswani et al. (2016). For experiments related to search aware optimization, we report results with beam size of 5.3 4.1.1 Tagging model for ablation study We changed the standard sequence-to-sequence model to be more suitable for the tagging task. This change also lets us perform controlled experiments pertaining to the amount of input sequence information available to the decoder at each time step. In a standard encoder-decoder model with attention, the initial hidden state of the decoder is often some function of the final encoder state so that the decoder’s predictions can be conditioned on the full input. For"
N19-1171,D16-1137,0,0.661753,"LT 2019, pages 1724–1733 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics the more common locally normalized parametrization. We posit that this difference is due to label bias (Bottou, 1991) arising from the interaction of approximate search and search-aware optimization in locally normalized models. A commonly understood source of label bias in locally normalized sequence models is an effect of conditioning only on partial input (for example, only the history of the input) at each step during decoding (Andor et al., 2016; Lafferty et al., 2001; Wiseman and Rush, 2016). We discus another potential source of label bias arising from approximate search with locally normalized models that may be present even with access to the full input at each step. To this end, we train search-aware globally and locally normalized models in an end-to-end (sub)-differentiable manner using a continuous relaxation to the discontinuous beam search procedure introduced by Goyal et al. (2017b). This approach requires initialization with a suitable globally normalized model to work in practice. Hence, we also propose an initialization strategy based upon self-normalization for pre-"
P08-1088,N04-4038,0,0.00685771,"ogously. The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. 774 Lexicon Note that the although the corpora here are derived from a parallel corpus, there are no parallel sentences. 8 LDC catalog # 2002E18. 9 LDC catalog # 2004E13. 10 These corpora contain no parallel sentences. 11 We use the Tree Tagger (Schmid, 1994) for all POS tagging except for Arabic, where we use the tagger described in Diab et al. (2004). Setting p0.1 p0.25 p0.33 p0.50 Best-F1 E DIT D IST O RTHO C ONTEXT MCCA 58.6 76.0 91.1 87.2 62.6 81.3 81.3 89.7 61.1 80.1 80.2 89.0 —52.3 65.3 89.7 47.4 55.0 58.0 72.0 1 EN-ES-P EN-ES-W 0.95 Precision 0.9 0.85 0.8 0.75 Table 1: Performance of E DIT D IST and our model with various features sets on EN-ES-W. See section 5. 0.7 0.65 0.6 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Figure 3: Example precision/recall curve of our system on EN-ES-P and EN-ES-W settings. See section 6.1. all languages pairs except English-Arabic, we extract evaluation lexicons from the Wiktionary online dictionary. As"
P08-1088,W95-0114,0,0.842841,"s. We present the highest confidence system predictions, where the only editing done is to ignore predictions which consist of identical source and target words. D and EN-FR-D, presumably due in part to the lack of orthographic features. However, MCCA still achieved surprising precision at lower recall levels. For instance, at p0.1 , MCCA yielded 60.1 and 70.0 on Chinese and Arabic, respectively. Figure 3 shows the highest-confidence outputs in several languages. 6.4 Comparison To Previous Work There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here. Due to unavailability of data and specificity in experimental conditions and evaluations, it is not possible to perform exact comparisons. How(a) Example Non-Cognate Pairs health traceability youth report advantages salud rastreabilidad juventud informe ventajas (b) Interesting Incorrect Pairs liberal Kirkhope action Albanians a.m. Netherlands partido Gorsel reacci´ on Bosnia horas Breta˜ na Table 4: System analysis on EN-ES-W: (a) non-cognate pairs proposed by our system, (b) hand-selected represe"
P08-1088,P06-1121,0,0.0217612,"Missing"
P08-1088,W02-0902,0,0.871975,"ihood weights with a simple proxy, the distances between the words’ mean latent concepts: wi,j = A − ||zi∗ − zj∗ ||2 , • EN-ES-P: 1st 100k sentences of text from the parallel English and Spanish Europarl corpus (Koehn, 2005). • EN-CH-D: English: 1st 50k sentences of Xinhua parallel news corpora;8 Chinese: 2nd 50k sentences. • EN-AR-D: English: 1st 50k sentences of 1994 proceedings of UN parallel corpora;9 Arabic: 2nd 50k sentences. • EN-ES-G: English: 100k sentences of English Gigaword; Spanish: 100k sentences of Spanish Gigaword.10 Each experiment requires a lexicon for evaluation. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. We consider a word type to be a noun if its most common tag is a noun in our monolingual corpus.11 For 7 (5) where A is a thresholding constant, zi∗ = E(zi,j |fS (si )) = P 1/2 US> fS (si ), and zj∗ is defined analogously. The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. 774 Lexicon"
P08-1088,koen-2004-pharaoh,0,0.0112554,"Missing"
P08-1088,2005.mtsummit-papers.11,0,0.0928857,"plicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments 6 Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, the distances between the words’ mean latent concepts: wi,j = A − ||zi∗ − zj∗ ||2 , • EN-ES-P: 1st 100k sentences of text from the parallel English and Spanish Europarl corpus (Koehn, 2005). • EN-CH-D: English: 1st 50k sentences of Xinhua parallel news corpora;8 Chinese: 2nd 50k sentences. • EN-AR-D: English: 1st 50k sentences of 1994 proceedings of UN parallel corpora;9 Arabic: 2nd 50k sentences. • EN-ES-G: English: 100k sentences of English Gigaword; Spanish: 100k sentences of Spanish Gigaword.10 Each experiment requires a lexicon for evaluation. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. We consider a word type to be a noun if its most common tag is a noun in our monolingual co"
P08-1088,P95-1050,0,0.779727,"inese systems. We present the highest confidence system predictions, where the only editing done is to ignore predictions which consist of identical source and target words. D and EN-FR-D, presumably due in part to the lack of orthographic features. However, MCCA still achieved surprising precision at lower recall levels. For instance, at p0.1 , MCCA yielded 60.1 and 70.0 on Chinese and Arabic, respectively. Figure 3 shows the highest-confidence outputs in several languages. 6.4 Comparison To Previous Work There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here. Due to unavailability of data and specificity in experimental conditions and evaluations, it is not possible to perform exact comparisons. How(a) Example Non-Cognate Pairs health traceability youth report advantages salud rastreabilidad juventud informe ventajas (b) Interesting Incorrect Pairs liberal Kirkhope action Albanians a.m. Netherlands partido Gorsel reacci´ on Bosnia horas Breta˜ na Table 4: System analysis on EN-ES-W: (a) non-cognate pairs proposed by our system, (b) hand-sele"
P08-1088,P06-1072,0,0.0065989,"rd types. If too few word types are matched, learning will not progress quickly; if too many are matched, the model will be swamped with noise. We found that it was helpful to explicitly control the number of edges. Thus, we adopt a bootstrapping-style approach that only permits high confidence edges at first, and then slowly permits more over time. In particular, we compute the optimal full matching, but only retain the highest weighted edges. As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). • EN-ES(FR)-D: English: 1st 50k sentences of Europarl; Spanish (French): 2nd 50k sentences of Europarl.7 Note that even when corpora are derived from parallel sources, no explicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental exp"
P08-1088,P95-1026,0,0.225036,"Thus, we adopt a bootstrapping-style approach that only permits high confidence edges at first, and then slowly permits more over time. In particular, we compute the optimal full matching, but only retain the highest weighted edges. As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). • EN-ES(FR)-D: English: 1st 50k sentences of Europarl; Spanish (French): 2nd 50k sentences of Europarl.7 Note that even when corpora are derived from parallel sources, no explicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments 6 Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, th"
P10-1131,N10-1083,1,0.230173,"Missing"
P10-1131,W00-1201,0,0.0104417,"eech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of their linguistic classifications. English and Dutch are part of the West Ger1291 share a common parent node in the prior, meaning that global regularities that are consistent across all languages can be captured. We refer to this structure as G LOBAL. While the globa"
P10-1131,D07-1093,1,0.714797,"Missing"
P10-1131,D08-1092,1,0.430261,"Missing"
P10-1131,N09-1009,0,0.612839,"nly available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter level, using a phylogeny-structured prior to tie together the various individual languages’ learning problems. Our joint, hierarchical prior couples model parameters for different languages in a way that respects knowledge about how the languages evolved. Aspects of this work are closely related to Cohen and Smith (2009) and Bouchard-Cˆot´e et al. (2007). Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. In their work, structurally constrained covariance in a logistic normal prior is used to couple parameters between the two languages. Our work, though also different in technical approach, differs most centrally in the extension to multiple languages and the use of a phylogeny. Bouchard-Cˆot´e et al. (2007) considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure a"
P10-1131,J93-2004,0,0.0355398,"utch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. For all languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been remove"
P10-1131,D09-1086,0,0.0711954,"Missing"
P10-1131,P09-1009,0,0.28893,"Missing"
P10-1131,N09-1010,0,0.227813,"Missing"
P10-1131,C02-1145,0,0.00871453,"ll languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of t"
P10-1131,P07-1033,0,0.140506,"Missing"
P10-1131,P02-1001,0,0.0338299,"Germanic English Dutch North Germanic IberoRomance Danish Swedish Spanish Portuguese Slavic Slovene Chinese Figure 1: An example of a linguistically-plausible phylogenetic tree over the languages in our training data. Leaves correspond to (observed) modern languages, while internal nodes represent (unobserved) ancestral languages. actually word classes. 2.1.1 Log-Linear Parameterization The DMV’s local conditional distributions were originally given as simple multinomial distributions with one parameter per outcome. However, they can be re-parameterized to give the following log-linear form (Eisner, 2002; Bouchard-Cˆot´e et al., 2007; Berg-Kirkpatrick et al., 2010): P CONTINUE (c|h, dir, adj; θℓ ) = ˆ ˜ exp θℓ T f CONTINUE (c, h, dir, adj) ˆ T ˜ P ′ c′ exp θℓ f CONTINUE (c , h, dir, adj) P ATTACH (a|h, dir; θℓ ) = ˆ ˜ exp θℓ T f ATTACH (a, h, dir) ˆ T ˜ P ′ a′ exp θℓ f ATTACH (a , h, dir) The parameters are weights θℓ with one weight vector per language. In the case where the vector of feature functions f has an indicator for each possible conjunction of outcome and conditions, the original multinomial distributions are recovered. We refer to these full indicator features as the set of S PECI"
P10-1131,N09-1068,0,0.0206512,"retation, but which provides very similar capacity for parameter coupling. 3.2.1 Phylogenetic Models The first phylogenetic model uses the shallow phylogeny shown in Figure 2(a), in which only languages within the same family have a shared parent node. We refer to this structure as FAMILIES. Under this prior, the learning task decouples into independent subtasks for each family, but no regularities across families can be captured. The family-level model misses the constraints between distant languages. Figure 2(b) shows another simple configuration, wherein all languages Daum´e III (2007) and Finkel and Manning (2009) consider a formally similar Gaussian hierarchy for domain adaptation. As pointed out in Finkel and Manning (2009), there is a simple equivalence between hierarchical regularization as described here and the addition of new tied features in a “flat” model with zero-meaned Gaussian regularization on all parameters. In particular, instead of parameterizing the objective in Section 2.4 in terms of multiple sets of weights, one at each node in the phylogeny (the hierarchical parameterization, described in Section 2.4), it is equivalent to parameterize this same objective in terms of a single set o"
P10-1131,P04-1061,1,0.564388,"le languages and the use of a phylogeny. Bouchard-Cˆot´e et al. (2007) considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure as well as the use of log-linear parameterization of local model components. Our work differs from theirs primarily in the task (syntax vs. phonology) and the variables governed by the phylogeny: in our model it is the grammar parameters that drift (in the prior) rather than individual word forms (in the likelihood model). Specifically, we consider dependency induction in the DMV model of Klein and Manning (2004). Our data is a collection of standard dependency data sets in eight languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. Our focus is not the DMV model itself, which is well-studied, but rather the prior which couples the various languages’ parameters. While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. In our experiments, joint multili"
P10-1131,P04-1060,0,0.379192,"Missing"
P10-1131,P09-1042,0,0.135264,"Missing"
P10-1131,W06-2920,0,\N,Missing
P10-1131,J03-4003,0,\N,Missing
P11-1049,D08-1024,0,0.00885439,"T: 1(docCount(b) = ·) where docCount(b) is the number of documents containing b. S TOP: 1(isStop(b1 ) = ·, isStop(b2 ) = ·) where isStop(w) indicates a stop word. P OSITION: 1(docPosition(b) = ·) where docPosition(b) is the earliest position in a document of any sentence containing b, buckets earliest positions ≥ 4. C ONJ: All two- and three-way conjunctions of C OUNT, S TOP, and P OSITION features. B IAS: Bias feature, active on all bigrams. Table 1: Bigram features: component feature functions in g(b, x) that we use to characterize the bigram b in both the extractive and compressive models. Chiang et al., 2008). Previous work has referred to the lack of extracted, compressed data sets as an obstacle to joint learning for summarizaiton (Daum´e III, 2006; Martins and Smith, 2009). We collected joint data via a Mechanical Turk task. To make the joint annotation task more feasible, we adopted an approximate approach that closely matches our fast approximate prediction procedure. Annotators were shown a 150-word maximum bigram recall extractions from the full document set and instructed to form a compressed summary by deleting words until 100 or fewer words remained. Each task was performed by two annota"
P11-1049,W09-1802,1,0.726218,"s certainly simple and does guarantee some minimal readability, Lin (2003) showed that sentence compression (Knight and Marcu, 2001; McDonald, 2006; Clarke and Lapata, 2008) has the potential to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using"
P11-1049,W10-0722,1,0.343109,"four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. It is designed to help annotators distinguish information content from linguistic quality. Two annotators performed the entire evaluation without overlap by splitting the set of problems in half. To evaluate linguistic quality, we sent all the summaries to Mechanical Turk (with two times redun488 dancy), using the template and instructions designed by Gillick and Liu (2010). They report that Turkers can faithfully reproduce experts’ rankings of average system linguistic quality (though their judgements of content are poorer). The table shows average linguistic quality. All the content-based metrics show substantial improvement for learned systems over unlearned ones, and we see an extremely large improvement for the learned joint extractive and compressive system over the previous state-of-the-art E XTRACTIVE BASELINE. The ROUGE scores for the learned joint system, L EARNED C OMPRESSIVE, are, to our knowledge, the highest reported on this task. We cannot compare"
P11-1049,P06-1096,1,0.262422,"Missing"
P11-1049,W03-1101,0,0.0232796,"2007) learn predictors of individual words’ appearance in the references, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contribution of the current paper is the direct optimization of summary quality in a single model; we find that our learned systems substantially outperform unlearned counterparts on both automatic and manual metrics. While pure extraction is certainly simple and does guarantee some minimal readability, Lin (2003) showed that sentence compression (Knight and Marcu, 2001; McDonald, 2006; Clarke and Lapata, 2008) has the potential to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form o"
P11-1049,W04-1013,0,0.357697,"ial to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19"
P11-1049,W09-1801,0,0.871248,"ying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics learns parameters for compression and extraction jointly using an approximate training procedure, but his results are not competitive with state-of-the-art extractive systems, and he does not report improvements on manual content or quali"
P11-1049,E06-1038,0,0.132293,"ces, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contribution of the current paper is the direct optimization of summary quality in a single model; we find that our learned systems substantially outperform unlearned counterparts on both automatic and manual metrics. While pure extraction is certainly simple and does guarantee some minimal readability, Lin (2003) showed that sentence compression (Knight and Marcu, 2001; McDonald, 2006; Clarke and Lapata, 2008) has the potential to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in"
P11-1049,N04-1019,0,0.614718,"er, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics learns"
P11-1049,P08-2052,0,0.0136493,"out a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set. 1 Introduction Applications of machine learning to automatic summarization have met with limited success, and, as a result, many top-performing systems remain largely ad-hoc. One reason learning may have provided limited gains is that typical models do not learn to optimize end summary quality directly, but rather learn intermediate quantities in isolation. For example, many models learn to score each input sentence independently (Teufel and Moens, 1997; Shen et al., 2007; Schilder and Kondadadi, 2008), and then assemble extractive summaries from the top-ranked sentences in a way not incorporated into the learning process. This extraction is often done in the 481 presence of a heuristic that limits redundancy. As another example, Yih et al. (2007) learn predictors of individual words’ appearance in the references, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contribution of the current paper is the direct optimization"
P11-1049,W04-3201,1,0.160476,"nly O( N ) constraints are added before constraint induction finds a C-optimal solution. Loss-augmented prediction is not always tractable. Luckily, our choice of loss function, bigram recall, factors over bigrams. Thus, we can easily perform loss-augmented prediction using the same procedure we use to perform Viterbi prediction (described in Section 4). We simply modify each bigram value vb to include bigram b’s contribution to the total loss. We solve the intermediate partially-constrained max-margin problems using the factored sequential minimal optimization (SMO) algorithm (Platt, 1999; Taskar et al., 2004). In practice, for  = 10−4 , the cutting-plane algorithm converges after only three passes through the training set when applied to our summarization task. 3.3 Loss function In the simplest case, 0-1 loss, the system only receives credit for exactly identifying the label summary. Since there are many reasonable summaries we are less interested in exactly matching any specific training instance, and more interested in the degree to which a predicted summary deviates from a label. The standard method for automatically evaluating a summary against a reference is ROUGE, which we simplify slightly"
P11-1049,W97-0710,0,0.209424,"only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set. 1 Introduction Applications of machine learning to automatic summarization have met with limited success, and, as a result, many top-performing systems remain largely ad-hoc. One reason learning may have provided limited gains is that typical models do not learn to optimize end summary quality directly, but rather learn intermediate quantities in isolation. For example, many models learn to score each input sentence independently (Teufel and Moens, 1997; Shen et al., 2007; Schilder and Kondadadi, 2008), and then assemble extractive summaries from the top-ranked sentences in a way not incorporated into the learning process. This extraction is often done in the 481 presence of a heuristic that limits redundancy. As another example, Yih et al. (2007) learn predictors of individual words’ appearance in the references, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contributi"
P11-1049,P10-1058,0,0.14535,"essions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics learns parameters for compression and extraction jointly using an approximate training procedure, but his results are not competitive with state-of-the-art extractive systems, and he does not report improvements on manual content or quality metrics. In our approach, we"
P13-1021,koen-2004-pharaoh,0,0.0151054,"to each result. Bernoulli parameter for this pixel, we can write:  [XiGLYPH ]jk ∼ Bernoulli θ PIXEL (j, k, gi ; φei ) The interpolation process for a single row is depicted in Figure 5. We define a constant interpolation vector µ(gi , k) that is specific to the glyph box width gi and glyph box column k. Each µ(gi , k) is shaped according to a Gaussian centered at the relative column position in φei . The glyph pixel Bernoulli parameters are defined as follows: 4.2 The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. θ PIXEL (j, k,gi ; φei ) = w h X i logistic µ(gi , k)k0 · [φei ]jk0 k0 =1 The fact that the parameterization is log-linear will ensure tha"
P13-1021,D11-1029,1,0.848221,"fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. P (E, T, R, X) = [Language model] P (E) [Ty"
P13-1021,N03-1018,0,0.0344936,"Missing"
P13-1021,N10-1083,1,0.467591,"Missing"
P13-1021,J04-4002,0,0.02125,"Missing"
P13-1021,D08-1012,1,0.899245,"Missing"
P13-1021,D08-1085,0,0.0125897,"rical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typicall"
P13-1021,P11-1025,0,0.01315,"d approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. P (E, T,"
P13-1021,P10-1107,0,0.0205837,"e so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with dis"
P13-1021,P08-1025,0,0.0542564,"a Gaussian centered at the relative column position in φei . The glyph pixel Bernoulli parameters are defined as follows: 4.2 The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. θ PIXEL (j, k,gi ; φei ) = w h X i logistic µ(gi , k)k0 · [φei ]jk0 k0 =1 The fact that the parameterization is log-linear will ensure that, during the unsupervised learning process, updating the shape parameters φc is simple and feasible. By varying the magnitude of µ we can change the level of smoothing in the logistic model and cause it to permit areas that are over-inked. This is the effect that di controls. By offsetting the rows of φc that we interpolate weights from, we change the v"
P14-1020,D13-1195,1,0.152689,"ley.edu Abstract classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine. Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on. Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (Petrov and Klein, 2007) (which is particularly amenable to GPU processing), “compiling” the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing detai"
P14-1020,N06-1022,0,0.0738887,"Missing"
P14-1020,N13-1033,0,0.188699,"Missing"
P14-1020,U11-1006,0,0.0944752,"Missing"
P14-1020,J93-2004,0,0.0455682,"U implementation, resulting in overall speeds of up to 404 sentences per second. For comparison, the publicly available CPU implementation of Petrov and Klein (2007) parses approximately 7 sentences per second per core on a modern CPU. A further drawback of the dense approach in Canny et al. (2013) is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein (2007) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (Marcus et al., 1993). To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass. We then implement minimumBayes-risk parsing via the max recall algorithm of Goodman (1996). Without the coarse pass, the dense marginal computation is not efficient on a GPU, processing only 32 sentences per second. However, our approach allows us to process over 190 sentences per second, almost a 6x speedup. 2 use two NVIDIA GeForce GTX 690s—each of which is essentially a repackaging of two 680"
P14-1020,P05-1010,0,0.0375439,"ecause parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, ear"
P14-1020,N07-1051,1,0.909398,"CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on. Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (Petrov and Klein, 2007) (which is particularly amenable to GPU processing), “compiling” the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below). In this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-tofine pruning to a GPU setting. On a CPU, pruning methods can give speedups of up to 100x. Such extreme speedups over a den"
P14-1020,W03-3021,0,0.0193841,"per second. Because parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for laten"
P14-1020,W06-1666,0,0.0263958,"e grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1. Here, we"
P14-1020,W11-2921,0,0.287847,"Missing"
P14-1020,P96-1024,0,\N,Missing
P14-2020,P13-1021,1,0.803406,"that generates vi conditioned on the previous vertical offset vi−1 (labeled Slow-vary) and an extension that generates a sequence of font styles fi (labeled Italic). with a forward-cost-augmented beaming scheme. Our method is over 25x faster on a typical document, yet actually yields improved transcriptions. 2 system they are independent. Finally, the pixels in the ith glyph bounding box XiGLYPH are generated conditioned on the character ei , width gi , and vertical offset vi , and the pixels in the ith pad bounding box XiPAD are generated conditioned on the width pi . We refer the reader to Berg-Kirkpatrick et al. (2013) for the details of the pixel generation process. We have omitted the token-level inking random variables for the purpose of brevity. These can be treated as part of the pixel generation process. Let X denote the matrix of pixels for the entire line, V = (v1 , . . . , vn ), P = (p1 , . . . , pn ), and G = (g1 , . . . , gn ). The joint distribution is written: Model We first describe the generative model used by the ‘Ocular’ historical OCR system of BergKirkpatrick et al. (2013)1 and then describe our extensions. The graphical model corresponding to their basic generative process for a single l"
P14-2020,koen-2004-pharaoh,0,0.0761304,"pus (Graff et al., 2007).3 tive, but slow. For example, while transcribing a typical document consisting of 30 lines of text, their system spends 63 minutes computing expected sufficient statistics and decoding when run on a 4.5GHz 4-core CPU. We instead use hard counts of the sufficient statistics for learning (i.e. perform hard-EM). As a result, we are free to use inference procedures that are specialized for Viterbi computation. Specifically, we use beam-search with estimated forward costs. Because the model is semi-Markov, our beam-search procedure is very similar the one used by Pharaoh (Koehn, 2004) for phrasebased machine translation, only without a distortion model. We use a beam of size 20, and estimate forward costs using a character bigram language model. On the machine mentioned above, transcribing the same document, our simplified system that uses hard-EM and beam-search spends only 2.4 minutes computing sufficient statistics and decoding. This represents a 26x speedup. 4 4.1 Document Recognition Performance We evaluate predicted transcriptions using both character error rate (CER) and word error rate (WER). CER is the edit distance between the guessed transcription and the gold t"
P14-2020,D08-1012,1,0.893269,"Missing"
P14-2020,P08-1025,0,0.0314965,"that the if vi differs substantially from vi−1 , a large penalty is incurred. As a result, the model should prefer sequences of vi that vary slowly. In experiments, we set σ 2 = 0.05. 2.2 Streamlined Inference Inference in our extended typesetting models is costly because the state space is large; we propose an new inference procedure that is fast and simple. Berg-Kirkpatrick et al. (2013) used EM to learn the font parameters Φ, and therefore required expected sufficient statistics (indicators on (ei , gi , vi ) tuples), which they computed using coarse-tofine inference (Petrov et al., 2008; Zhang and Gildea, 2008) with a semi-Markov dynamic program (Levinson, 1986). This approach is effecItalic Font Styles Many of the documents in the Old Bailey corpus contain both italic and non-italic font styles (Shoemaker, 2005). The way that italic fonts are used depends on the year the document was printed, but generally italics are reserved for proper nouns, 120 Learned typesetting: Document image: Figure 3: This first line depicts the Viterbi typesetting layout predicted by the O CULAR -B EAM -IT model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities"
P16-1188,P13-1020,0,0.0223492,"its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We show a sentence broken into"
P16-1188,J08-1001,0,0.0195037,"ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extracti"
P16-1188,N04-1015,0,0.0350411,"bjective and are incorporated into the length constraint. Yoshida et al. (2014) and approaching the clarity of a sentence-extractive baseline—and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with e"
P16-1188,H01-1065,0,0.162869,"Missing"
P16-1188,P11-1049,1,0.934644,"damental units of extraction in our model. For a sentence-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in Figure 2 and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015). For our model, type-level n-gram scoring only arises when we compute our loss function in maxmargin training (see Section 3). In Section 2.1, we discuss grammaticality constraints, which take the form of introducing dependencies between textual units, as shown in Figure 2. If one textual unit requires another, it cannot be included unless its prerequisite is. We will show that different sets of requirements can capture both syntactic and discourse-based compression schemes. Furthermore, we introduce anaphora constraints (Section 2.2) via a new set of"
P16-1188,W01-1605,0,0.0722609,"Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We show a sentence broken into elemen2 The features in our model are actually rich enough to learn a sophisticated compression model, but the data we have (abstractive summaries) does not directly provide examples of correct compressions; past work has gotten around this with multi-task learning (Almeida and Martins, 2013), but we simply treat grammaticality as a constraint from upstream models. tary discourse units (EDUs) with RST relations between them. Units marked as S AME -U NIT must both be kept or both be deleted, but other nodes in the tree structure can be deleted as long as we do n"
P16-1188,N13-1136,0,0.0110846,"ate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UN"
P16-1188,J10-3005,0,0.0598346,"ll trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations. u1 This hasn’t been Kellogg’s year . it No replacement necessary u2 Kellogg p1 p3 p2 it year The oat-bran cr"
P16-1188,P02-1057,0,0.146913,"Missing"
P16-1188,E14-4040,0,0.0180116,"as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms—one syntactic and the other discursive. Additionally, we incorporate a model of anaphora resolution and give our system the ability rewrite pronominal mentions, further increasing expressivity. In order to guide the model, we incorporate (1) constraints from coreference ensuring that critical pronoun references are clear in the final summ"
P16-1188,D13-1203,1,0.744439,"is Kellogg. We explore two types of constraints for dealing with this: rewriting the pronoun explicitly, or constraining the summary to include the pronoun’s antecedent. 2.2.1 Pronoun Replacement One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we 4 We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013). 2001 can make an isolated textual unit meaningful even if it contains a pronoun. Figure 3 shows how this process works. We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. maxi pi &gt; α for a specified threshold α &gt; 12 ), we allow ourselves to replace the pronoun with the first mention of the entity corresponding to the pronoun’s most likely antecedent. In Figure 3, if the system correctly determines that Kellogg is the correct antecedent wi"
P16-1188,Q14-1037,1,0.910185,"ment One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we 4 We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013). 2001 can make an isolated textual unit meaningful even if it contains a pronoun. Figure 3 shows how this process works. We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. maxi pi &gt; α for a specified threshold α &gt; 12 ), we allow ourselves to replace the pronoun with the first mention of the entity corresponding to the pronoun’s most likely antecedent. In Figure 3, if the system correctly determines that Kellogg is the correct antecedent with high probability, we enable the first replacement shown there, which is used if u2 is included the summary without u1 .5 As shown in the ILP in Figure 1, we instantiate corresponding pronou"
P16-1188,W09-1802,0,0.312645,"n is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Time"
P16-1188,W10-0722,0,0.076163,"hat removing either syntactic or EDU-based compressions decreases ROUGE. ument prefix summary. One reason for this is that many of the articles are longer-form pieces that begin with a relatively content-free lede of several sentences, which should be identifiable with lexicosyntactic indicators as are used in our discriminative model. 4.3 New York Times Results We evaluate our system along two axes: first, on content selection, using ROUGE9 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon Mechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original document or the reference summary, and rate each summary in isolation. Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems according to expert judgments. To speed up preprocessing and training time 9 We use the ROUGE 1.5.5 script with the following command line arguments: -n 2 -"
P16-1188,D13-1158,0,0.0503894,"then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with"
P16-1188,E14-1075,0,0.0534042,"selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms—one syntactic and the other discursive. Additionally, we incorporate a model of anaphora resolution and give our system the ability rewrite pronominal mentions, further increasing expressivity. In order to guide the model, we incorporate (1) constraints from coreference ensuring that critical pronoun references are clear in the final summary and (2) constraints f"
P16-1188,P13-1048,0,0.015369,"Missing"
P16-1188,P14-2052,0,0.0168089,": Tcomb = (S ∪ Ssyn (kl) ∪ {(i, k), (l, j)}, πrst ∪ πsyn(kl) ∪ {(i, k) → (l, j), (l, j) → (i, k), (k, l) → (i, k)}) That is, we maintain the existing tree structure except for the EDU (i, j), which is broken into three parts: the outer two depend on each other (is a claims adjuster and . from Figure 2d) and the inner one depends on the others and preserves the tree structure from Tsyn . We augment Trst with all maximal subtrees of Tsyn , i.e. all trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use cros"
P16-1188,D15-1032,1,0.437982,"y that in general cannot be produced by our model. Specifically, we take: `(xNGRAM , y) = maxx∗ ∗ ROUGE -1(x , y) − ROUGE -1(xNGRAM , y) i.e. the gap between the hypothesis’s ROUGE score and the oracle ROUGE score achievable under the model (including constraints). Here xNGRAM are indicator variables that track, for each n-gram type in the reference summary, whether that n-gram is present in the system summary. These are the sufficient statistics for computing ROUGE. We train the model via stochastic subgradient descent on the primal form of the structured SVM objective (Ratliff et al., 2007; Kummerfeld et al., 2015). In order to compute the subgradient for a given training example, we need to find the most violated constraint on the given instance through a loss-augmented decode, which for a linear model takes the form arg maxx w&gt; f (x) + `(x, y). To do this decode at training time in the context of our model, we use an extended version of our ILP in Figure 1 that is augmented to explicitly track typelevel n-grams: &quot; Xh max xUNIT ,xREF ,xNGRAM UNIT xi i (w f (ui )) &gt; i  + Xh REF &gt; i xij (w f (rij )) − `(x NGRAM , y) (i,j) subject to all constraints from Figure 1, and xiNGRAM = 1 iff an included textual"
P16-1188,N15-1079,0,0.012553,"ce-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in Figure 2 and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015). For our model, type-level n-gram scoring only arises when we compute our loss function in maxmargin training (see Section 3). In Section 2.1, we discuss grammaticality constraints, which take the form of introducing dependencies between textual units, as shown in Figure 2. If one textual unit requires another, it cannot be included unless its prerequisite is. We will show that different sets of requirements can capture both syntactic and discourse-based compression schemes. Furthermore, we introduce anaphora constraints (Section 2.2) via a new set of variables that capture the process of rew"
P16-1188,P11-1052,0,0.0300998,"ion rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sa"
P16-1188,J95-2003,0,0.469362,"Missing"
P16-1188,N03-1020,0,0.193323,"ally significant gains compared to No Anaphoricity and Tree Knapsack (respectively) with p &lt; 0.05 according to a bootstrap resampling test. We also see that removing either syntactic or EDU-based compressions decreases ROUGE. ument prefix summary. One reason for this is that many of the articles are longer-form pieces that begin with a relatively content-free lede of several sentences, which should be identifiable with lexicosyntactic indicators as are used in our discriminative model. 4.3 New York Times Results We evaluate our system along two axes: first, on content selection, using ROUGE9 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon Mechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original document or the reference summary, and rate each summary in isolation. Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems"
P16-1188,P14-1020,1,0.819066,"(Sandhaus, 2008). We also investigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.8 Throughout this section, when we decode a document, we set the word budget for our summarizer to be the same as the number of words in the corresponding reference summary, following previous work (Hirao et al., 2013; Yoshida et al., 2014). 4.1 Preprocessing We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution System (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans. To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style of Hirao et al. (2013), since their parser is not publicly available. Specifically, 8 Tasks like DUC and TAC have focused on multidocument summari"
P16-1188,W03-1101,0,0.0302607,"explicit mentions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions fr"
P16-1188,D12-1106,0,0.016443,"thod. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing"
P16-1188,W10-4327,0,0.0273313,"Missing"
P16-1188,W98-1124,0,0.0373498,"Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning"
P16-1188,W09-1801,0,0.0266717,"entions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure T"
P16-1188,P05-1012,0,0.0434139,"Missing"
P16-1188,E06-1038,0,0.0215508,"d) Process of augmenting a textual unit with syntactic compressions. REF explicit mentions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (20"
P16-1188,R11-1066,0,0.0174782,"nstraint. Yoshida et al. (2014) and approaching the clarity of a sentence-extractive baseline—and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in"
P16-1188,D15-1182,0,0.0301045,"Missing"
P16-1188,P14-1084,0,0.0402219,"nsure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UNIT , . . . , xnUNIT , which are binary indicators of whether each unit is included. Textual units are contig"
P16-1188,N03-1030,0,0.042861,"Missing"
P16-1188,I13-1198,0,0.0176762,"fficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UNIT , . . . , xnUNIT , which are binary indicators of whether each unit is included. Te"
P16-1188,D12-1022,0,0.0100531,"ronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We"
P16-1188,D14-1196,0,0.0482556,"By training our full system endto-end on a large-scale dataset, we are able to learn a high-capacity structured model of the summarization process, contrasting with past approaches to the single-document task which have typically been heuristic in nature (Daum´e and Marcu, 2002; Hirao et al., 2013). We focus our evaluation on the New York Times Annotated corpus (Sandhaus, 2008). According to ROUGE, our system outperforms a document prefix baseline, a bigram coverage baseline adapted from a strong multi-document system (Gillick and Favre, 2009), and a discourse-informed method from prior work (Yoshida et al., 2014). Imposing discursive and referential constraints improves human judgments of linguistic clarity and referential structure—outperforming the method of 1998 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1998–2008, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics max unit ref x ,x subject to h X ⇥ Extraction Score xunit (w&gt; f (ui )) + i i Grammaticality Constraints (Section 2.1) Anaphora Score &gt; xref ij (w f (rij )) (i,j) i ⇤ Length adjustment for explicit mention Length Constraint X xunit  xunit if ui requires"
P16-1188,C14-1156,0,0.0799128,"reserves the tree structure from Tsyn . We augment Trst with all maximal subtrees of Tsyn , i.e. all trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations. u1 This h"
P16-1188,P08-1054,0,0.0219105,"r work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model"
P16-1188,P06-1055,1,0.129979,"n a roughly 3000-document evaluation set from the New York Times Annotated Corpus (Sandhaus, 2008). We also investigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.8 Throughout this section, when we decode a document, we set the word budget for our summarizer to be the same as the number of words in the corresponding reference summary, following previous work (Hirao et al., 2013; Yoshida et al., 2014). 4.1 Preprocessing We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution System (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans. To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style of Hirao et al. (2013), since their parser is not publicly availabl"
P16-1188,W01-0100,0,\N,Missing
P17-2058,P16-1231,0,0.0779065,"Missing"
P17-2058,D15-1044,0,0.0463382,"ding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation. 1 Taylor Berg-Kirkpatrick Carnegie Mellon University Pittsburgh, PA, USA tberg@cs.cmu.edu Introduction Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), dialogue generation (Serban et al., 2015), and image captioning (Xu et al., 2015). However, the standard cross-entropy training procedure for these models suffers from the well-known problem of exposure bias: because cross-entropy training always uses gold contexts, the states and contexts encountered during training do not match those encountered at test time. This issue has been addressed using several approaches that try to incorporate awareness of decoding choices into the training optimization. These include reinforcement learning (Ranzato et al., 2016; Bahdanau 366 Proceedings of the 5"
P17-2058,2014.iwslt-evaluation.1,0,0.400421,"Missing"
P17-2058,W03-0419,0,0.0826902,"Missing"
P17-2058,D16-1137,0,0.0405369,"Missing"
P17-2058,Q15-1035,0,0.0224919,"ty of selecting the ground truth token as an inverse sigmoid (Bengio et al., 2015) of epochs with a decay strength parameter k. We also tuned for different values of α and explore the effect of varying α exponentially (annealing) with the epochs. In table 1, we report results for the best performing configuration of decay parameter and the α parameter on the validation set. To account for variance across randomly started runs, we ran multiple random restarts (RR) for all the systems evaluated and always used the RR with the best validation set score to calculate test performance. Related Work Gormley et al. (2015)’s approximation-aware training is conceptually related, but focuses on variational decoding procedures. Hoang et al. (2017) also propose continuous relaxations of decoders, but are focused on developing better inference procedures. Grefenstette et al. (2015) successfully use a soft approximation to argmax in neural stack mechanisms. Finally, Ranzato et al. (2016) experiment with a similarly motivated objective that was not fully continuous, but found it performed worse than the standard training. 5 Comparison We report validation and test metrics for NER and MT tasks in Table 1, F1 and BLEU r"
P17-2065,N10-1083,1,0.855453,"Missing"
P17-2065,P13-1021,1,0.871968,"Missing"
P17-2065,D10-1056,0,0.015765,"best model likelihood. 4 Table 1: The experimental results for all setups of the model. In the experiments with BASIC model, we compare the short HINMAN word list with the automatically filtered AUTO word list. We show results for several variants of our full model, labeled as FEAT, both with and without spacing generation. A random baseline is included for comparison. with Blayney’s. We compute the one-to-one and many-to-one accuracy, mapping the recovered page groups to the gold compositors to maximize accuracy, as is standard for many unsupervised clustering tasks, e.g. POS induction (see Christodoulopoulos et al. (2010)). Experiments Data: To evaluate our model when it has access to perfectly transcribed historical text, we use the Bodleian diplomatic transcription of the First Folio.3 To test whether our approach can also work with untranscribed books, we ran the Ocular OCR system (Berg-Kirkpatrick et al., 2013) on the Bodleian facsimile images to create an automatic diplomatic transcription. In both cases, we used Ocular’s estimates of glyph bounding boxes on the complete First Folio images to extract spacing information. The modern text was taken from MIT Complete Works of Shakespeare4 and was aligned wit"
P18-1154,D16-1032,0,0.195865,"Missing"
P18-1154,D16-1128,0,0.139786,"Missing"
P18-1154,P09-1011,0,0.0572073,"we have introduced a new largescale data for game commentary generation. The commentaries cover a variety of aspects like move description, quality of move, and alternative moves. This leads to a content selection challenge, similar to that noted in Wiseman et al. (2017). Unlike Wiseman et al. (2017), our focus is on generating commentary for individual moves in a game, as opposed to game summaries from aggregate statistics as in their task. One of the first NLG datasets was the SUMTIME-METEO (Reiter et al., 2005) corpus with ≈ 500 record-text pairs for technical weather forecast generation. Liang et al (2009) worked on common weather forecast generation using the WEATHERGOV dataset, which has ≈ 10K record-text pairs. A criticism of WEATHERGOV dataset (Reiter, 2017) is that weather records themselves may have used templates and rules with optional human post-editing. There have been prior works on generating commentary for ROBOCUP matches (Chen and Mooney, 2008; Mei et al., 2015). The ROBOCUP dataset, however, is collected from 4 games and contains about 1K events in total. Our dataset is two orders of magnitude larger than the ROBOCUP dataset, and we hope that it provides a promising setting for f"
P18-1154,O90-1011,0,0.648433,"blocked by that move. Both descriptions are true, but the latter is most salient given the player’s goal. However, sometimes, none of the aspects may stand out as being most salient, and the most salient aspect may even change from commentator to commentator. Moreover, a human commentator may introduce variations in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary. This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language (Liu et al., 2016). Prior work has explored game commentary generation. Liao and Chang (1990); Sadikov et al. (2006) have explored chess commentary generation, but for lack of large-scale training data their methods have been mainly rule-based. Kameko et al. (2015) have explored commentary generation for the game of Shogi, proposing a twostep process where salient terms are generated from the game state and then composed in a language model. In contrast, given the larger amount of training data available to us, our proposed model uses an end-to-end trainable neural architecture to predict commentaries given the game state. Our model conditions on semantic and pragmatic information abo"
P18-1154,D16-1230,0,0.0252276,"imply that the pawn was moved, or one may comment on how the check was blocked by that move. Both descriptions are true, but the latter is most salient given the player’s goal. However, sometimes, none of the aspects may stand out as being most salient, and the most salient aspect may even change from commentator to commentator. Moreover, a human commentator may introduce variations in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary. This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language (Liu et al., 2016). Prior work has explored game commentary generation. Liao and Chang (1990); Sadikov et al. (2006) have explored chess commentary generation, but for lack of large-scale training data their methods have been mainly rule-based. Kameko et al. (2015) have explored commentary generation for the game of Shogi, proposing a twostep process where salient terms are generated from the game state and then composed in a language model. In contrast, given the larger amount of training data available to us, our proposed model uses an end-to-end trainable neural architecture to predict commentaries given the"
P18-1154,P02-1040,0,0.102226,"ross-entropy loss over the decoding outputs to train the model. 4 Experiments We split each of the data subsets in a 70:10:20 ratio into train, validation and test. All our models are implemented in Pytorch version 0.3.1 (Paszke et al., 2017). We use the ADAM optimizer (Kingma and Ba, 2014) with its default parameters and a mini-batch size of 32. Validation set perplexity is used for early-stopping. At test-time, we use greedy search to generate the model output. We observed that beam decoding does not lead to any significant improvement in terms of validation BLEU score. We observe the BLEU (Papineni et al., 2002) and BLEU-2 (Vedantam et al., 2015) scores to measure the performance of the models. Addi1665 Dataset Features TEMP NN (M+T+S) RAW MoveDesc GAC-sparse GAC (M+T) TEMP NN (M+T) RAW Quality GAC-sparse GAC(M+T+S) NN (M) RAW Comparative GAC-sparse GAC(M+T) tionally, we consider a measure to quantify the diversity in the generated outputs. Finally, we also conduct a human evaluation study. In the remainder of this section, we discuss baselines along with various experiments and results. 4.1 Baselines In this subsection we discuss the various baseline methods. Manually-defined template (TEMP) We devi"
P18-1154,J09-4008,0,0.159794,"Missing"
P18-1154,W12-1516,0,0.0199831,"and previous board (Ri ) information from the game. P (Si |Mi , Gi ) = P (Si |Mi , Ci , Ri ). We model this using an end-to-end trainable neural model, which models conjunctions of features using feature encoders. Our model employs a selection mechanism to select the salient features for a given chess move. Finally a LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997) is used to generate the commentary text based on selected features from encoder. 3.1 Incorporating Domain Knowledge Past work shows that acquiring domain knowledge is critical for NLG systems (Reiter et al., 2003b; Mahamood and Reiter, 2012). Commentary texts cover a range of perspectives, including criticism or goodness of current move, possible alternate moves, quality of alternate moves, etc. To be able to make such comments, the model must learn about the quality of moves, as well as the set of valid moves for a given chess board state. We consider the following features to provide our model with necessary information to generate commentary texts (Figure 3): Move features fmove (Mi , Ci , Ri ) encode the current move information such as which piece moved, the position of the moved piece before and after the move was made, the"
P19-1311,D14-1187,0,\N,Missing
P19-1311,D09-1086,0,\N,Missing
P19-1311,P12-1066,0,\N,Missing
P19-1311,W14-4203,0,\N,Missing
P19-1311,petrov-etal-2012-universal,0,\N,Missing
P19-1311,D11-1006,0,\N,Missing
P19-1311,N13-1126,0,\N,Missing
P19-1311,Q13-1001,0,\N,Missing
P19-1311,D15-1213,0,\N,Missing
P19-1311,P15-1119,0,\N,Missing
P19-1311,W15-2137,0,\N,Missing
P19-1311,N16-1156,0,\N,Missing
P19-1311,Q17-1010,0,\N,Missing
P19-1311,D16-1073,0,\N,Missing
P19-1311,C16-1012,0,\N,Missing
P19-1311,E17-2002,0,\N,Missing
P19-1311,D17-1302,0,\N,Missing
P19-1311,D18-1160,1,\N,Missing
P19-1311,D18-1163,0,\N,Missing
P19-1311,N19-1162,0,\N,Missing
P19-1311,Q18-1046,0,\N,Missing
P19-1311,D19-1077,0,\N,Missing
P19-1311,N19-1253,1,\N,Missing
P19-1311,P18-4013,0,\N,Missing
P19-1311,N19-1423,0,\N,Missing
P19-1427,S14-2010,0,0.0425502,"Missing"
P19-1427,S16-1081,0,0.075813,"Missing"
P19-1427,S13-1004,0,0.0543439,"Missing"
P19-1427,S12-1051,0,0.0491652,"entum of 0.99. Gradients are renormalized to norm 0.1 (Pascanu et al., 2012). We train the LTokLS objective for 200 epochs and the combined objective, LWeighted , for 10. Model selection is done by selecting the model with the lowest validation loss on the validation set. Then, depending on the evaluation being considered, we select models with the highest performance on the validation set. 4 Experiments 4.1 where u is a candidate hypothesis, U(x) is a set of candidate hypotheses, and t is the reference. 7 Evaluation is on the SemEval Semantic Textual Similarity (STS) datasets from 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). In the SemEval STS competitions, teams create models that need to work well on domains both represented in the training data and hidden domains revealed at test time. Our model and those of Wieting and Gimpel (2018), in contrast to the best performing STS systems, do not use any manually-labeled training examples nor any other linguistic resources beyond the ParaNMT corpus (Wieting and Gimpel, 2018). 8 Available at https://github.com/pytorch/ fairseq. Data Training models with minimum risk is expensive, but we wanted to evaluate in a difficult, realistic setting usin"
P19-1427,2014.iwslt-evaluation.4,0,0.0200873,"hat nearly all metrics performed similarly to one another. The 2015 and 2016 results showed more variation among metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when"
P19-1427,W11-2103,0,0.0721467,"Missing"
P19-1427,N10-1080,0,0.0160871,"scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In these tasks, participants submitted metrics to optimize during training or combinations of metrics and optimizers, given a fixed SMT system. The 2011 results showed that nearly all metrics p"
P19-1427,D17-1070,0,0.0180748,"the output of S IMI L E is continuous, it provides more informative gradients during the optimization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT training, we augment these basic models with two modifications: we a"
P19-1427,W14-3348,0,0.0229286,"s evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.1 1 Introduction In neural machine translation (NMT) and other natural language generation tasks, it is common practice to improve likelihood-trained models by further tuning their parameters to explicitly maximize an automatic metric of system accuracy – for example, BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2014). Directly optimizing accuracy metrics involves backpropagating through discrete decoding decisions, and thus is typically accomplished with structured prediction techniques like reinforcement learning (Ranzato et al., 2016), minimum risk training (Shen 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. et al., 2015), and other specialized methods (Wiseman and Rush, 2016). Generally, these methods work by repeatedly generating a translation under the current parameters (via decoding, sampling, or loss-augmented decoding), comparing the generated translation"
P19-1427,N18-1033,0,0.107196,"sentence is about 20 times faster than METEOR when code is executed on GPU (NVIDIA GeForce GTX 1080). 6 We used the segment level data from newstest2015 and newstest2016 available at http://statmt.org/ wmt18/metrics-task.html. The former contains 7 language pairs and the latter 5. 4346 close, but the semantic similarity correlations7 in Table 1 are not, suggest that the difference between METEOR and SIM largely lies in fluency. However, not capturing fluency is something that can be ameliorated by adding a down-weighted maximum-likelihood (MLE) loss to the minimum risk loss. This was done by Edunov et al. (2018) and we use this in our experiments as well. 3 Lang. cs-en de-en ru-en tr-en Objective Functions. Following (Edunov et al., 2018), we first train models with maximum-likelihood with label-smoothing (LTokLS ) (Szegedy et al., 2016; Pereyra et al., 2017). We set the confidence penalty of label smoothing to be 0.1. Next, we fine-tune the model with a weighted average of minimum risk training (LRisk ) (Shen et al., 2015) and (LTokLS ), where the expected risk is defined as: u∈U (x) Test 2,983 2,998 3,000 3,000 Therefore, our fine-tuning objective becomes: LWeighted = γLTokLS + (1 − γ)LRisk Archite"
P19-1427,N16-1162,0,0.0688546,"Missing"
P19-1427,D11-1125,0,0.0338379,"ain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In these tasks, participants submitted met"
P19-1427,W16-2303,0,0.0303766,"Missing"
P19-1427,C04-1072,0,0.0918735,"mber of sentence pairs in the training/validation/test sets for all four languages. Machine Translation Preliminaries X Train 218,384 284,286 235,159 207,678 p(u|x) cost(t, u) P 0 u0 ∈U (x) p(u |x) We tune γ from the set {0.2, 0.3} in our experiments. In minimum risk training, we aim to minimized the expected cost. In our case that is 1 − BLEU(t, h) or 1 − S IMI L E(t, h) where t is the target and h is the generated hypothesis. As is commonly done, we use a smoothed version of BLEU by adding 1 to all n-gram counts except unigram counts. This is to prevent BLEU scores from being overly sparse (Lin and Och, 2004). We generate candidates for minimum risk training from n-best lists with 8 hypotheses without and do not include the reference in the candidates. Optimization. We optimize our models using Nesterov’s accelerated gradient method (Sutskever et al., 2013) using a learning rate of 0.25 and momentum of 0.99. Gradients are renormalized to norm 0.1 (Pascanu et al., 2012). We train the LTokLS objective for 200 epochs and the combined objective, LWeighted , for 10. Model selection is done by selecting the model with the lowest validation loss on the validation set. Then, depending on the evaluation be"
P19-1427,D11-1035,0,0.0238807,"mong metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when evaluated under other metrics. These results differ from the usual results obtained for SMT systems, in"
P19-1427,P13-2067,0,0.0266082,"2011 results showed that nearly all metrics performed similarly to one another. The 2015 and 2016 results showed more variation among metrics, but also found that BLEU was a strong choice overall, echoing the results of Cer et al. (2010). We have shown that our metric stabilizes training for NMT more than BLEU, which is a promising result given the limited success of the broad spectrum of previous attempts to discover easily tunable metrics in the context of SMT. Some researchers have found success in terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the"
P19-1427,P16-1159,0,0.0403668,"n terms of improved human judgments when training to maximize metrics other than BLEU for SMT. Lo et al. (2013) and Beloucif et al. (2014) trained SMT systems to maximize variants of MEANT, a metric based on semantic roles. Liu et al. (2011) trained systems using TESLA, a family of metrics based on softly matching n-grams using lemmas, WordNet synsets, and part-of-speech tags. We have demonstrated that our metric similarly leads to gains in performance as assessed by human annotators, and our method has an auxiliary advantage of being much simpler than these previous hand-engineered measures. Shen et al. (2016) explored minimum risk training for NMT, finding that a sentence-level BLEU score led to the best performance even when evaluated under other metrics. These results differ from the usual results obtained for SMT systems, in which tuning to optimize a metric leads to the best performance on that metric (Och, 2003). Edunov et al. (2018) compared structured losses for NMT, also using sentence-level BLEU. They found risk to be an effective and robust choice, so we use risk as well in this paper. 9 Conclusion We have proposed S IMI L E, an alternative to BLEU for use as a reward in minimum risk tra"
P19-1427,N19-4007,1,0.804774,"0.50 0.55 0.45 0.15 0.08 0.03 0.33 0.24 0.27 0.34 1.48 2.50 0.66 0.13 0.25 0.34 0.63 0.65 Table 6: F1 score for various buckets of words. The values in the table are the difference between F1 for that specific language type and bucket between training using S IMI L E and BLEU (positive values means S IM I L E had a higher F1). The first part of the table shows F1 scores across bins defined by word frequency on the test set. So words appearing only 1 time are in the first row, between 2-5 times are in the second row, etc. The next part of the table buckets words by coarse POS tags. compare-mt (Neubig et al., 2019)12 to compute the F1 scores for target word types based on their frequency and their coarse part-of-speech-tag (as labeled by SpaCy13 ) and show the results in Table 6. From the table, we see that training with S IM I L E helps produce low frequency words more accurately, a fact that is consistent with the POS tag analysis in the second part of the table. Wieting and Gimpel (2017) noted that highly discriminative parts-of-speech, such as nouns, proper nouns, and numbers, made the most contribution to the sentence embeddings. Other works (Pham et al., 2015; Wieting et al., 2016) have also found"
P19-1427,P06-2101,0,0.06992,"do not know how to explain it - it is really unique. I don’t know how to explain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015"
P19-1427,P03-1021,0,0.454799,"he case and it’s even possible for less accurate translations to have higher scores than more accurate ones. The bottom half of the table shows examples where the difference in BLEU scores is large, but the difference in SIM scores is small. From thexe examples we can see that when BLEU scores are very different, the semantics of the sentence can still be preserved. However, we observe that often in these cases, the SIM scores of the sentences tend to be similar. 8 Related Work The seminal work on training machine translation systems to optimize particular evaluation measures was performed by Och (2003), who intro4351 Reference BLEU system SIM system ∆BLEU ∆SIM Reference BLEU system SIM system ∆BLEU ∆SIM Workers are beginning to clean up workers . Workers have begun to clean up in Rszke. In Rszke, workers are beginning to clean up. 3.2 -26.3 All that stuff sure does take a toll. None of this takes a toll . All of this is certain to take its toll . 7.1 -22.7 Reference BLEU system SIM system ∆BLEU ∆SIM Reference BLEU system SIM system ∆BLEU ∆SIM Another advantage is that they have fewer enemies. Another benefit: they have less enemies. Another advantage: they have fewer enemies. -33.8 -9.6 I d"
P19-1427,W15-3032,0,0.0355517,"Missing"
P19-1427,P02-1040,0,0.10718,"c results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.1 1 Introduction In neural machine translation (NMT) and other natural language generation tasks, it is common practice to improve likelihood-trained models by further tuning their parameters to explicitly maximize an automatic metric of system accuracy – for example, BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2014). Directly optimizing accuracy metrics involves backpropagating through discrete decoding decisions, and thus is typically accomplished with structured prediction techniques like reinforcement learning (Ranzato et al., 2016), minimum risk training (Shen 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. et al., 2015), and other specialized methods (Wiseman and Rush, 2016). Generally, these methods work by repeatedly generating a translation under the current parameters (via decoding, sampling, or loss-augmented decoding"
P19-1427,P15-1094,0,0.112474,"rrect but lexically different translations. Moreover, since the output of S IMI L E is continuous, it provides more informative gradients during the optimization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT train"
P19-1427,D07-1080,0,0.0565854,"lly unique. I don’t know how to explain - it is really unique. -39.1 -2.1 Table 8: The top two rows show examples where the generated sentences have similar BLEU scores but quite different SIM scores. The bottom two rows show the converse. Negative values indicate the SIM system had a higher score for that sentence. duced minimum error rate training (MERT) and used it to optimize several different metrics in statistical MT (SMT). This was followed by a large number of alternative methods for optimizing machine translation systems based on minimum risk (Smith and Eisner, 2006), maximum margin (Watanabe et al., 2007), or ranking (Hopkins and May, 2011), among many others. Within the context of SMT, there have also been studies on the stability of particular metrics for optimization. Cer et al. (2010) compared several metrics to optimize for SMT, finding BLEU to be robust as a training metric and finding that the most effective and most stable metrics for training are not necessarily the same as the best metrics for automatic evaluation. The WMT shared tasks included tunable metric tasks in 2011 (CallisonBurch et al., 2011) and again in 2015 (Stanojevi´c et al., 2015) and 2016 (Jawaid et al., 2016). In the"
P19-1427,P17-1190,1,0.867686,"s bins defined by word frequency on the test set. So words appearing only 1 time are in the first row, between 2-5 times are in the second row, etc. The next part of the table buckets words by coarse POS tags. compare-mt (Neubig et al., 2019)12 to compute the F1 scores for target word types based on their frequency and their coarse part-of-speech-tag (as labeled by SpaCy13 ) and show the results in Table 6. From the table, we see that training with S IM I L E helps produce low frequency words more accurately, a fact that is consistent with the POS tag analysis in the second part of the table. Wieting and Gimpel (2017) noted that highly discriminative parts-of-speech, such as nouns, proper nouns, and numbers, made the most contribution to the sentence embeddings. Other works (Pham et al., 2015; Wieting et al., 2016) have also found that when training semantic embeddings using an averaging function, embeddings that bear the most information regarding the meaning have larger norms. We also see that these same parts-of-speech (nouns, proper nouns, numbers) have the largest difference in F1 scores between S IMI L E and BLEU. Other parts-of-speech like SYM and INTJ have high F1 scores as well, and words belongin"
P19-1427,P18-1042,1,0.892666,"timization process by distinguishing between candidates that would be similarly scored under matching-based metrics like BLEU. Lastly, we show in our analysis that S IMI L E has an additional benefit over BLEU by translating words with heavier semantic content more accurately. To define an exact metric, we reference the burgeoning field of research aimed at measuring semantic textual similarity (STS) between two sentences (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Hill et al., 2016; Conneau et al., 2017; Pagliardini et al., 2017). Specifically, we start with the method of Wieting and Gimpel (2018), which learns paraphrastic sentence representations using a contrastive loss and a parallel corpus induced by backtranslating bitext. Wieting and Gimpel showed that simple models that average word or character trigram embeddings can be highly effective for semantic similarity. The strong performance, domain robustness, and computationally efficiency of these models make them highly attractive. For the purpose of discriminative NMT training, we augment these basic models with two modifications: we add a length penalty to avoid short translations, and compose the embeddings of subword units, ra"
P19-1427,P19-1453,1,0.81423,"s, s0 i and we use a margin-based loss: `(s, s0 ) = max(0, δ − cos(g(s), g(s0 )) + cos(g(s), g(t))) 2 In semantic textual similarity the goal is to produce scores that correlate with human judgments on the degree to which two sentences have the same semantics. In embedding based models, including the models used in this paper, the score is produced by the cosine of the two sentence embeddings. 3 We use SentencePiece which is available at https:// github.com/google/sentencepiece. 4 We use 16.77 million paraphrase pairs extracted from the ParaNMT corpus (Wieting and Gimpel, 2018). Recently, in (Wieting et al., 2019) it has been shown that strong performance on semantic similarity tasks can also be achieved using bitext directly without the need for backtranslation. 4345 Model SIM S IMI L E Wieting and Gimpel (2018) BLEU METEOR STS 1st Place STS 2nd Place STS 3rd Place 2012 69.3 70.1 67.8 39.2 53.4 64.8 63.4 64.1 2013 64.1 59.8 62.8 29.5 47.6 62.0 59.1 58.3 2014 77.2 74.7 76.9 42.8 63.7 74.3 74.2 74.3 2015 80.3 79.4 79.8 49.8 68.8 79.0 78.0 77.8 2016 78.6 77.8 76.8 47.4 61.8 77.7 75.7 75.7 tion). However, we found that this favored short sentences. We instead penalize a generated sentence if its length di"
P19-1427,D16-1137,0,0.0755019,"Missing"
P19-1453,S14-2010,0,0.192289,"Missing"
P19-1453,S16-1081,0,0.10707,"Missing"
P19-1453,S13-1004,0,0.309609,"Missing"
P19-1453,S12-1051,0,0.146732,"overlap share more parameters. We utilize several regularization methods (Wieting and Gimpel, 2017) including dropout (Srivastava et al., 2014) and shuffling the words in the sentence when training the LSTMSP. Additionally, we find that annealing the mega-batch size by increasing it during training improved performance by a significant margin for LSTM-SP. 3 Experiments Experiments are split into two groups. First, we compare training on parallel data to training on back-translated parallel data. We evaluate these models on the 2012-2016 SemEval Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), which predict the degree to which sentences have the same meaning as measured by human judges. The evaluation metric is Pearson’s r with the gold labels. We use the small STS English-English dataset from Cer et al. (2017) for model selection. Second, we compare our best model, SP, on two semantic crosslingual tasks: the 2017 SemEval STS task (Cer et al., 2017) which consists of monolingual and cross-lingual datasets and the 2018 Building and Using Parallel Corpora (BUCC) shared bitext mining task (Zweigenbaum et al., 2018). 3.1 Hyperparameters and Optimization Unless"
P19-1453,N06-1003,0,0.0530907,"semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at http"
P19-1453,S17-2001,0,0.187252,"e additional benefit of creating cross-lingual representations that are useful for tasks such as mining or filtering parallel data and cross-lingual retrieval. We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the timeconsuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006;"
P19-1453,D17-1070,0,0.039622,"ets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly be"
P19-1453,P11-2117,0,0.0379629,"he resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence"
P19-1453,C04-1051,0,0.257736,"data and cross-lingual retrieval. We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the timeconsuming intermediate step of creating paraphrase corpora. Further, we show that the resulting model can be applied to cross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018"
P19-1453,N13-1092,0,0.0967382,"Missing"
P19-1453,C18-1122,0,0.06052,"Missing"
P19-1453,W18-6317,0,0.237718,"In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these representations for mining or filtering large amounts of bitext. Our approach forms the simplest me"
P19-1453,N16-1162,0,0.095929,"Missing"
P19-1453,D18-2012,0,0.0362043,"current pair. However, in the bilingual case, negative examples are only selected from the sentences in the batch from the opposing language. To select difficult negative examples that aid training, we use the mega-batching procedure of Wieting and Gimpel (2018), which aggregates M mini-batches to create one mega-batch and selects negative examples therefrom. Once each pair in the megabatch has a negative example, the mega-batch is split back up into M mini-batches for training. Encoders. Our primary sentence encoder simply averages the embeddings of subword units generated by sentencepiece (Kudo and Richardson, 2018); we refer to it as SP. This means that the sentence piece embeddings themselves are the only learned parameters of this model. As baselines we explore averaging character trigrams (T RIGRAM) (Wieting et al., 2016a) and words (W ORD). SP provides a compromise between averaging words and character trigrams, combining the more distinct semantic units of words with the coverage of character trigrams. We also use a bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997), with LSTM parameters fully shared between languages , as well as LSTM-SP, which uses sentence pieces instead of words as t"
P19-1453,D17-1126,0,0.0250789,"ross-lingual tasks where it both outperforms and is orders of magnitude faster than more complex stateof-the-art baselines.1 1 Introduction Measuring sentence similarity is a core task in semantics (Cer et al., 2017), and prior work has achieved strong results by training similarity models on datasets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015"
P19-1453,L16-1147,0,0.0418628,"ell et al. (2017). We included correlations for all languages as well as those with low and high SP overlap with English. 0.50 0.45 0.1 0.2 0.3 0.4 SP Overlap 0.5 0.6 Figure 1: Plot of average performance on the 20122016 STS tasks compared to SP overlap and language distance as defined by Littell et al. (2017). 4.2 SP Ovl. 71.5 23.6 18.5 Does Language Choice Matter? We next investigate the impact of the non-English language in the bitext when training English paraphrastic sentence embeddings. We took all 46 languages with at least 100k parallel sentence pairs in the 2016 OpenSubtitles Corpus (Lison and Tiedemann, 2016) and made a plot of their average STS performance on the 2012-2016 English datasets compared to their SP overlap6 and language distance.7 We segmented the languages separately and trained the models for 10 epochs using the 2017 STS task for model selection. The plot, shown in Figure 1, shows that SentencePieces (SP) overlap is highly correlated with STS score. There are also two clusters in the plot, languages that have a similar alphabet to English and those that do not. In each cluster we find that performance is negatively correlated with language distance. Therefore, languages similar to E"
P19-1453,E17-2002,0,0.141406,"ng that SP is hundreds of times faster. 4605 Model All Lang. Lang. (SP Ovl. ≤ 0.3) Lang. (SP Ovl. &gt; 0.3) Language Similarity Vs. Performance 0.75 indmsa Avg. STS Pearson&apos;s r 0.64 0.63 0.62 0.61 0.60 0.59 tha kor zho 0.70 hun glgron deu swedan ita spa boscat vie turpolnldpor fra norhrv srp ces est slk fin slv sqi lit eus isl lav bul ell heb rus ukr ara fas sinjpn mkd mal kat 0.65 Language Distance 0.65 0.60 0.55 Lang. Distance -22.8 -63.8 -34.2 Table 5: Spearman’s ρ × 100 between average performance on the 2012-2016 STS tasks compared to SP overlap (SP Ovl.) and language distance as defined by Littell et al. (2017). We included correlations for all languages as well as those with low and high SP overlap with English. 0.50 0.45 0.1 0.2 0.3 0.4 SP Overlap 0.5 0.6 Figure 1: Plot of average performance on the 20122016 STS tasks compared to SP overlap and language distance as defined by Littell et al. (2017). 4.2 SP Ovl. 71.5 23.6 18.5 Does Language Choice Matter? We next investigate the impact of the non-English language in the bitext when training English paraphrastic sentence embeddings. We took all 46 languages with at least 100k parallel sentence pairs in the 2016 OpenSubtitles Corpus (Lison and Tiedema"
P19-1453,P15-1094,0,0.0479643,"Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature ha"
P19-1453,W17-2619,0,0.0335664,"and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when app"
P19-1453,P18-2035,0,0.017455,"s. 2 In fact, we show that for monolingual similarity, we can devise random encoders that outperform some of this work. 4602 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4602–4608 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Learning Sentence Embeddings We first describe our objective function and then describe our encoder, in addition to several baseline encoders. The methodology proposed here borrows much from past work (Wieting and Gimpel, 2018; Guo et al., 2018; Gr´egoire and Langlais, 2018; Singla et al., 2018), but this specific combination has not been explored and, as we show in experiments, is surprisingly effective. Training. The training data consists of a sequence of parallel sentence pairs (si , ti ) in source and target languages respectively. For each sentence pair, we randomly choose a negative target sentence t0i during training that is not a translation of si . Our objective is to have source and target sentences be more similar than source and negative target examples by a margin δ: i Xh min δ−fθ (si , ti ) + fθ (s, t0i )) . θsrc ,θtgt i + The similarity function is defined as:   fθ"
P19-1453,D16-1157,1,0.904892,"ese intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these representations for mining or filtering large amounts of bitext. Our approach forms the simplest method to date that is able to achieve state-of-the-art results on multiple monolingual and cross-lingual semantic textual similarity (STS) and parallel corpora mining tasks.2 Lastly, since bitext is available for so many language"
P19-1453,P17-1190,1,0.866482,"words and character trigrams, combining the more distinct semantic units of words with the coverage of character trigrams. We also use a bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997), with LSTM parameters fully shared between languages , as well as LSTM-SP, which uses sentence pieces instead of words as the input tokens. For all encoders, when the vocabularies of source and target languages overlap, the corresponding encoder embedding parameters are shared. As a result, languages pairs with more lexical overlap share more parameters. We utilize several regularization methods (Wieting and Gimpel, 2017) including dropout (Srivastava et al., 2014) and shuffling the words in the sentence when training the LSTMSP. Additionally, we find that annealing the mega-batch size by increasing it during training improved performance by a significant margin for LSTM-SP. 3 Experiments Experiments are split into two groups. First, we compare training on parallel data to training on back-translated parallel data. We evaluate these models on the 2012-2016 SemEval Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), which predict the degree to which sentences have the s"
P19-1453,P18-1042,1,0.918867,"irs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from n"
P19-1453,D17-1026,1,0.872931,"asets of paraphrase pairs (Dolan et al., 2004). However, such datasets are not produced naturally at scale and therefore must be created either through costly manual annotation or by leveraging natural annotation in specific domains, like Simple English Wikipedia (Coster and Kauchak, 2011) or Twitter (Lan et al., 2017). One of the most promising approaches for inducing paraphrase datasets is via manipulation of large bilingual corpora. Examples include bilingual pivoting over phrases (Callison-Burch et al., 2006; Ganitkevitch et al., 2013), and automatic translation of one side of the bitext (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). However, this is costly – Wieting and Gimpel (2018) report their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on model"
P19-1453,P18-2037,0,0.407958,"their large-scale database of sentential paraphrases required 10,000 GPU hours to generate. In this paper, we propose a method that trains highly performant sentence embeddings (Pham et al., 2015; Hill et al., 2016; Pagliardini et al., 2017; McCann et al., 2017; Conneau et al., 2017) directly on bitext, obviating these intermediate 1 Code and data to replicate results are available at https://www.cs.cmu.edu/˜jwieting. Most previous work for cross-lingual representations has focused on models based on encoders from neural machine translation (EspanaBonet et al., 2017; Schwenk and Douze, 2017; Schwenk, 2018) or deep architectures using a contrastive loss (Gr´egoire and Langlais, 2018; Guo et al., 2018; Chidambaram et al., 2018). However, the paraphrastic sentence embedding literature has observed that simple models such as pooling word embeddings generalize significantly better than complex architectures (Wieting et al., 2016b). Here, we find a similar effect in the bilingual setting. We propose a simple model that not only produces state-of-the-art monolingual and bilingual sentence representations, but also encode sentences hundreds of times faster – an important factor when applying these repr"
