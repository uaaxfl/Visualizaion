2020.starsem-1.8,D14-1125,0,0.0749085,"Missing"
2020.starsem-1.8,P12-3029,0,0.0191374,"Missing"
2020.starsem-1.8,D14-1162,0,0.0863466,"supervised classifiers for determining the moral values expressed by text. Works that try to overcome the issues related to the simple counts of lexicon hits embed the moral values in continuous spaces. Dehghani et al. (2016) and Kaur and Sasahara (2016) generate vectors for words based on a Latent Semantic Analysis (LSA) (Deerwester et al., 1990) methodology. Then, for each moral value, a vector in the same space is obtained by adding up all the vectors of the modeled lexicon words. Garten et al. (2016), Nokhiz and Li (2017) and Xie et al. (2019) use Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) models to embed the words of the lexicon, and then either aggregate these vectors to produce a vector representation of each moral value, or they keep each of these vectors separate and use different strategies for determining the moral values expressed by novel embedded texts (such as k-nearest neighbor). Other works that go into a direction more related to ours extend the MFD lexicon by using WordNet (Rezapour et al., 2019; Araque et al., 2020), and then manually curate the results with human annotators (Rezapour et al., 2019) or extend the annotations with values for valence and arousal (A"
2020.starsem-1.8,W19-1305,0,0.0182801,"adding up all the vectors of the modeled lexicon words. Garten et al. (2016), Nokhiz and Li (2017) and Xie et al. (2019) use Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) models to embed the words of the lexicon, and then either aggregate these vectors to produce a vector representation of each moral value, or they keep each of these vectors separate and use different strategies for determining the moral values expressed by novel embedded texts (such as k-nearest neighbor). Other works that go into a direction more related to ours extend the MFD lexicon by using WordNet (Rezapour et al., 2019; Araque et al., 2020), and then manually curate the results with human annotators (Rezapour et al., 2019) or extend the annotations with values for valence and arousal (Araque et al., 2020). However, the methods differ from our approach as the results of these works are still word-level lexicons, while our moral value relevance annotation is done at a synset rather than Moreover, it only contains uni-grams, and the entries are associated with either vice or virtue, without a score for the strength of the association. However, this lexicon has been widely exploited lately, and several approach"
2021.findings-acl.170,P19-1602,0,0.0189644,"on non-parallel datasets with two types of stylistic variation: diachronic language shift and newspaper titles versus scientific paper titles. In parallel work, John et al. (2019) proposed a disentanglement model that appends additional contentbased losses, where content is approximated by a bag-of-words representation of the text. Their approach was applied to sentiment transfer for Yelp and Amazon reviews. Other work has looked at disentangling syntax from the semantics of a text. Chen et al. (2019) proposed a VAE-based model that used parallel paraphrase corpora; this was also the focus of Bao et al. (2019) and Balasubramanian et al. (2020). All of these works are very similar in the base model architecture and the kinds of loss functions used to guide disentanglement. In the following sections, we consolidate and propose a broad categorization of these losses that we hope will guide future work in this area. We then evaluate these models on parallel style transfer datasets, with ablation studies on the PersonageNLG dataset. Note on unsupervised disentanglement: While unsupervised approaches such as the β-VAE have been very successful at disentangling factors of variation in visual data (Higgins"
2021.findings-acl.170,P19-1041,0,0.252058,"methods combine all of these different aspects of a text into a single vector embedding (Conneau et al., 2017; Reimers and Gurevych, 2019). This results in only a fuzzy measure of text similarity when it is calculated using methods such as the cosine distance between vector embeddings. Recently, some research in NLP has focused on learning disentangled representations for texts, which aim to capture the different dimensions of variation of a text in separate vector embeddings. These methods have been investigated for style transfer to obtain disentangled representations of content and style (John et al., 2019; Romanov et al., 2019; Cheng et al., 2020), and paraphrase generation for disentangling syntax and semantics (Chen et al., 2019; Balasubramanian et al., 2020). Inspired by parallel developments on style transfer and disentanglement in computer vision, many of them operate within the variational autoencoder framework, where the autoencoder is modified to now encode a text into two latent vectors: one capturing the style (the aspect of variation), and the other capturing the content. Style transfer is then achieved by combining the content vector of the input with a style vector of the target s"
2021.findings-acl.170,N19-1088,0,0.0508528,"Missing"
brooke-hirst-2012-measuring,P11-1093,0,\N,Missing
brooke-hirst-2012-measuring,D11-1148,0,\N,Missing
brooke-hirst-2012-measuring,P09-1080,0,\N,Missing
brooke-hirst-2012-measuring,C08-1118,0,\N,Missing
brooke-hirst-2012-measuring,P11-1019,0,\N,Missing
brooke-hirst-2012-measuring,W08-0336,0,\N,Missing
brooke-hirst-2012-measuring,D11-1010,0,\N,Missing
brooke-hirst-2012-measuring,W07-0602,0,\N,Missing
brooke-hirst-2012-measuring,P11-1132,0,\N,Missing
brooke-hirst-2012-measuring,U09-1008,0,\N,Missing
C10-1133,E93-1055,0,0.0487377,"o illustrate some basic ideas about near-synonymy. Cruse (1986) compared the notion of plesionymy to cognitive synonymy in terms of mutual entailment and semantic traits, which, to the best of our knowledge, is possibly the closest to a textbook account of near-synonymy. There has been a substantial amount of interest in characterizing the nuances between nearsynonyms for a computation-friendly representation of near-synonymy. DiMarco et al. (1993) discovered 38 dimensions for differentiating nearsynonyms from dictionary usage notes and categorized them into semantic and stylistic variations. Stede (1993) focused on the latter and further decomposed them into seven scalable subTable 1: Examples of near-synonyms and dimension of variations (Edmonds and Hirst, 2002). Types of variation Continuous, intermittent Emphasis Denotational, indirect Denotational, fuzzy Stylistic, formality Stylistic, force Expressed attitude Emotive Collocational Selectional Sub-categorization Examples seep:drip enemy:foe error:mistake woods:forest pissed:drunk:inebriated ruin:annihilate skinny:thin:slim:slender daddy:dad:father task:job pass away:die give:donate categories. By organizing near-synonym variations into a"
C10-1133,P97-1067,0,0.607542,"synonym variations into a tree structure, Inkpen and Hirst (2006) combined stylistic and attitudinal variation into one class parallel to denotational differences. They also incorporated this knowledge of near-synonyms into a knowledge base and demonstrated its application in an NLG system. 2.2 Lexical Choice Evaluation Due to their symbolic nature, many of the early studies were only able to provide “demo runs” in NLG systems rather than any empirical evaluation. The study of near-synonym lexical choice had remained largely qualitative until a “fill-inthe-blank” (FITB) task was introduced by Edmonds (1997). The task is based on sentences collected from the 1987 Wall Street Journal (WSJ) that contain any of a given set of near-synonyms. Each occurrence of the near-synonyms is removed from the sentence to create a “lexical gap”, and the goal is to guess which one of the near-synonyms is the missing word. Presuming that the 1987 WSJ authors have made high-quality lexical choices, the FITB test provides a fairly objective benchmark for empirical evaluation for near-synonym lexical choice. The same idea can be applied to virtually any corpus to provide a fair amount of gold-standard data at relative"
C10-1133,J06-2003,1,0.91641,"e notion of subtlety through its relation to semantic space dimensionality. Using this formalization and our learning models, several of our intuitions about subtlety, dimensionality, and context are quantified and empirically tested. 1 Introduction Lexical choice is the process of selecting content words in language generation. Consciously or not, people encounter the task of lexical choice on a daily basis — when speaking, writing, and perhaps even in inner monologues. Its application also extends to various domains of natural language processing, including Natural Language Generation (NLG, Inkpen and Hirst 2006), writers’ assistant systems (Inkpen, 2007), and second language (L2) teaching and learning (Ouyang et al., 2009). When the goal is to make plausible or even elegant lexical choices that best suit the context, the representation of that context becomes a key issue. We approach this problem in the latent semantic space, where transformed local cooccurrence data is capable of implicitly inducing global knowledge (Landauer and Dumais, 1997). A latent semantic space is constructed by reducing the dimensionality of co-occurring linguistic units — typically words and documents as in Latent Semantic"
C10-1133,J02-4007,0,\N,Missing
C10-1133,J02-2001,1,\N,Missing
C10-2011,J06-2003,1,0.795169,"(Biber, 1995). Heyligen and Dewaele (1998) provide a part-of-speech based quantification of textual contextuality (which they argue is fundamental to the notion of formality); their metric has been used, for instance, in a computational investigation of the formality of online encyclopedias (Emigh and Herring, 2005). In this kind of quantification, however, there is little, if any, focus on individual elements of the lexicon. In computational linguistics, formality has received attention in the context of text generation (Hovy, 1990); of particular note relevant to our research is the work of Inkpen and Hirst (2006), who derive boolean formality tags from Choose the Right Word (Hayakawa, 1994). Like us, their focus was improved word choice, though the approach was much broader, also including dimensions such as polarity. An intriguing example of formality relevant to text classification is the use of informal language (slang) to help distinguish true news from satire (Burfoot and Baldwin, 2009). Our approach to this task is inspired and informed by automatic lexical acquisition research within the field of sentiment analysis (Turney and Littman, 2003; Esuli and Sebastiani, 2006; Taboada and Voll, 2006; R"
C10-2011,P09-2041,0,0.0188157,", focus on individual elements of the lexicon. In computational linguistics, formality has received attention in the context of text generation (Hovy, 1990); of particular note relevant to our research is the work of Inkpen and Hirst (2006), who derive boolean formality tags from Choose the Right Word (Hayakawa, 1994). Like us, their focus was improved word choice, though the approach was much broader, also including dimensions such as polarity. An intriguing example of formality relevant to text classification is the use of informal language (slang) to help distinguish true news from satire (Burfoot and Baldwin, 2009). Our approach to this task is inspired and informed by automatic lexical acquisition research within the field of sentiment analysis (Turney and Littman, 2003; Esuli and Sebastiani, 2006; Taboada and Voll, 2006; Rao and Ravichandra, 2009). Turney and Littman (2003) apply latent semantic analysis (LSA) (Landauer and Dumais, 1997) and pointwise mutual information (PMI) to derive semantic orientation ratings for words using large corpora; like us, they found that LSA was a powerful technique for deriving this lexical information. The lexical database SentiWordNet (Esuli and Sebastiani, 2006) pro"
C10-2011,C94-2174,0,0.0525908,"okens in 86,000 documents. 3.2 3 These blogs were gathered by the University of Toronto Blogscope project (www.blogscope.net) over a week in May 2008. 4 We use an upper bound of 28 characters, which is the length of antidisestablishmentarianism, the prototypical longest word in English; this value of L provides an appropriate formality/informality threshold, between 5- and 6-letter words 4 4.1 Methods Simple Formality Measures The simplest kind of formality measure is based on word length, which is often used directly as an indicator of formality for applications such as genre classification (Karlgren and Cutting, 1994). Here, we use logarithmic scaling to derive a FS score based on word length. Given a maximum word length L4 and a word w of length l, the formality score function, FS(w), is given by: FS(w) = −1 + 2 Corpora Our corpora fall generally into three categories: formal (written) copora, informal (spoken) corpora, and mixed corpora. The Brown Corpus (Francis and Kuˇcera, 1982), our development corpus, is used here both as a formal and mixed cor92 log l log L 4.2 For hyphenated terms, the length of each component is averaged. Though this metric works relatively well for English, we note that it is pr"
C10-2011,P97-1005,0,0.303168,"Missing"
C10-2011,E09-1077,0,0.0419405,"), who derive boolean formality tags from Choose the Right Word (Hayakawa, 1994). Like us, their focus was improved word choice, though the approach was much broader, also including dimensions such as polarity. An intriguing example of formality relevant to text classification is the use of informal language (slang) to help distinguish true news from satire (Burfoot and Baldwin, 2009). Our approach to this task is inspired and informed by automatic lexical acquisition research within the field of sentiment analysis (Turney and Littman, 2003; Esuli and Sebastiani, 2006; Taboada and Voll, 2006; Rao and Ravichandra, 2009). Turney and Littman (2003) apply latent semantic analysis (LSA) (Landauer and Dumais, 1997) and pointwise mutual information (PMI) to derive semantic orientation ratings for words using large corpora; like us, they found that LSA was a powerful technique for deriving this lexical information. The lexical database SentiWordNet (Esuli and Sebastiani, 2006) provides 0–1 rankings for positive, negative, and neutral polarity, 3 Data and Resources 3.1 Word Lists All the word lists discussed here are publicly available.1 We begin with two, one formal and one informal, that we use both as seeds for o"
C10-2011,esuli-sebastiani-2006-sentiwordnet,0,0.0032504,"o our research is the work of Inkpen and Hirst (2006), who derive boolean formality tags from Choose the Right Word (Hayakawa, 1994). Like us, their focus was improved word choice, though the approach was much broader, also including dimensions such as polarity. An intriguing example of formality relevant to text classification is the use of informal language (slang) to help distinguish true news from satire (Burfoot and Baldwin, 2009). Our approach to this task is inspired and informed by automatic lexical acquisition research within the field of sentiment analysis (Turney and Littman, 2003; Esuli and Sebastiani, 2006; Taboada and Voll, 2006; Rao and Ravichandra, 2009). Turney and Littman (2003) apply latent semantic analysis (LSA) (Landauer and Dumais, 1997) and pointwise mutual information (PMI) to derive semantic orientation ratings for words using large corpora; like us, they found that LSA was a powerful technique for deriving this lexical information. The lexical database SentiWordNet (Esuli and Sebastiani, 2006) provides 0–1 rankings for positive, negative, and neutral polarity, 3 Data and Resources 3.1 Word Lists All the word lists discussed here are publicly available.1 We begin with two, one form"
C10-2011,taboada-etal-2006-methods,0,0.0252341,"f Inkpen and Hirst (2006), who derive boolean formality tags from Choose the Right Word (Hayakawa, 1994). Like us, their focus was improved word choice, though the approach was much broader, also including dimensions such as polarity. An intriguing example of formality relevant to text classification is the use of informal language (slang) to help distinguish true news from satire (Burfoot and Baldwin, 2009). Our approach to this task is inspired and informed by automatic lexical acquisition research within the field of sentiment analysis (Turney and Littman, 2003; Esuli and Sebastiani, 2006; Taboada and Voll, 2006; Rao and Ravichandra, 2009). Turney and Littman (2003) apply latent semantic analysis (LSA) (Landauer and Dumais, 1997) and pointwise mutual information (PMI) to derive semantic orientation ratings for words using large corpora; like us, they found that LSA was a powerful technique for deriving this lexical information. The lexical database SentiWordNet (Esuli and Sebastiani, 2006) provides 0–1 rankings for positive, negative, and neutral polarity, 3 Data and Resources 3.1 Word Lists All the word lists discussed here are publicly available.1 We begin with two, one formal and one informal, tha"
C12-1025,D11-1010,0,0.0231838,"e speakers of various different native language (L1) backgrounds are used to identify those backgrounds. One potential application is as a facet of author proﬁling, which can be used to identify those who misrepresent themselves online (Fette et al., 2007). Another is as a preprocessing step to language learner error correction (Leacock et al., 2010): for example, Rozovskaya and Roth (2011) use L1-speciﬁc information to improve their preposition-correction system, and recent work in collocation correction relies on the speciﬁc forms present in the writer’s native language (Chang et al., 2008; Dahlmeier and Ng, 2011). As a distinct task in computational linguistics, native language identiﬁcation has been reasonably well-addressed (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2009), and in fact there has been a ﬂurry of recent activity (Kochmar, 2011; Golcher and Reznicek, 2011; Wong and Dras, 2011; Brooke and Hirst, 2011; Wong et al., 2012). Though a wide range of feature types has been explored—with conﬂicting results—the evaluation of these feature sets has been fairly uniform: training and testing in one of several small corpora of learner essays (Granger et al., 2009; Yannakoudakis et"
C12-1025,de-marneffe-etal-2006-generating,0,0.00761986,"Missing"
C12-1025,P09-1080,0,0.0174858,"h, we segregated a portion of the ICLE by topic and found that all the core features used by Koppel et al. for L1-identiﬁcation showed signiﬁcant drops in performance when topic-segregated 2-fold cross-validation is compared to standard (randomized) 2-fold cross-validation. This was particularly true of character n-grams. 1 http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/Cambridge-InternationalCorpus-Cambridge-Learner-Corpus 393 Finally, we note that native language identiﬁcation has also been included as an element of larger author proﬁling studies (Estival et al., 2007; Garera and Yarowsky, 2009). A closely related task is the identiﬁcation of translated texts and/or their language of origin (Baroni and Bernardini, 2006; van Halteren, 2008; Koppel and Ordan, 2011), though the tasks are distinct because the learners included in native language identiﬁcation studies are usually at a level of linguistic proﬁciency below that of a professional translator (who in any case may be writing in his or her L1, rather than an L2) and are not operating under the requirement of faithfulness to some original text. Distinguishing whether or not a text is non-native (Tomokiyo and Jones, 2001) is also"
C12-1025,P03-1054,0,0.0362535,"est, p < 0.001). 5 5.1 Feature Analysis Features Our model includes the following feature types: Function words A common feature in stylistic analysis. Our list of 416 common English words comes from the LIWC (Pennebaker et al., 2001). Character n-grams (unigrams, bigrams, and trigrams) For bigrams and trigrams, the beginning and end of a word are treated as special characters. Word n-grams (unigrams and bigrams) tion words. Punctuation is included. Note that word n-grams are a superset of funcPOS n-grams (unigrams, bigrams, and trigrams) POS tagging is provided by the Stanford Parser V1.6.9 (Klein and Manning, 2003), also used by Wong and Dras (2011). POS/function mixture n-grams (bigrams and trigrams) Wong et al. (2012) report better results with POS n-grams that retain the identity of individual function words rather than using their part of speech. CFG productions Context-free grammar production rules, as provided by the Stanford parser. Lexical production rules are not included. Dependencies Dependencies consist of two lexical items and the syntactic relationship between them. Also produced by the Stanford parser (de Marneffe et al., 2006). 399 Features Chance baseline (1) Function words (2) Characte"
C12-1025,P11-1132,0,0.00636231,"pic-segregated 2-fold cross-validation is compared to standard (randomized) 2-fold cross-validation. This was particularly true of character n-grams. 1 http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/Cambridge-InternationalCorpus-Cambridge-Learner-Corpus 393 Finally, we note that native language identiﬁcation has also been included as an element of larger author proﬁling studies (Estival et al., 2007; Garera and Yarowsky, 2009). A closely related task is the identiﬁcation of translated texts and/or their language of origin (Baroni and Bernardini, 2006; van Halteren, 2008; Koppel and Ordan, 2011), though the tasks are distinct because the learners included in native language identiﬁcation studies are usually at a level of linguistic proﬁciency below that of a professional translator (who in any case may be writing in his or her L1, rather than an L2) and are not operating under the requirement of faithfulness to some original text. Distinguishing whether or not a text is non-native (Tomokiyo and Jones, 2001) is also a related task, but most work in the area of L1 identiﬁcation, including ours, assumes that we already know that a text was produced by a non-native speaker. 3 Corpora Our"
C12-1025,P11-1093,0,0.0246565,"LING 2012: Technical Papers, pages 391–408, COLING 2012, Mumbai, December 2012. 391 1 Introduction Native language identiﬁcation (Koppel et al., 2005) is a task in which features of the second language (L2) texts written by non-native speakers of various different native language (L1) backgrounds are used to identify those backgrounds. One potential application is as a facet of author proﬁling, which can be used to identify those who misrepresent themselves online (Fette et al., 2007). Another is as a preprocessing step to language learner error correction (Leacock et al., 2010): for example, Rozovskaya and Roth (2011) use L1-speciﬁc information to improve their preposition-correction system, and recent work in collocation correction relies on the speciﬁc forms present in the writer’s native language (Chang et al., 2008; Dahlmeier and Ng, 2011). As a distinct task in computational linguistics, native language identiﬁcation has been reasonably well-addressed (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2009), and in fact there has been a ﬂurry of recent activity (Kochmar, 2011; Golcher and Reznicek, 2011; Wong and Dras, 2011; Brooke and Hirst, 2011; Wong et al., 2012). Though a wide range o"
C12-1025,P12-2038,0,0.28841,"Missing"
C12-1025,N01-1031,0,0.0519051,", 2007; Garera and Yarowsky, 2009). A closely related task is the identiﬁcation of translated texts and/or their language of origin (Baroni and Bernardini, 2006; van Halteren, 2008; Koppel and Ordan, 2011), though the tasks are distinct because the learners included in native language identiﬁcation studies are usually at a level of linguistic proﬁciency below that of a professional translator (who in any case may be writing in his or her L1, rather than an L2) and are not operating under the requirement of faithfulness to some original text. Distinguishing whether or not a text is non-native (Tomokiyo and Jones, 2001) is also a related task, but most work in the area of L1 identiﬁcation, including ours, assumes that we already know that a text was produced by a non-native speaker. 3 Corpora Our training corpus is a set of 154,702 English journal entries collected from the Lang-8 language learner website.2 In all, 65 L1s are represented, but only 14 languages have more than 1000 entries, with Asian languages being overrepresented (the website is based in Japan). Users may write whatever they want in their journal, and there are a variety of text types (some learners post assignments or translation questions"
C12-1025,W07-0602,0,0.303896,"ng, which can be used to identify those who misrepresent themselves online (Fette et al., 2007). Another is as a preprocessing step to language learner error correction (Leacock et al., 2010): for example, Rozovskaya and Roth (2011) use L1-speciﬁc information to improve their preposition-correction system, and recent work in collocation correction relies on the speciﬁc forms present in the writer’s native language (Chang et al., 2008; Dahlmeier and Ng, 2011). As a distinct task in computational linguistics, native language identiﬁcation has been reasonably well-addressed (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2009), and in fact there has been a ﬂurry of recent activity (Kochmar, 2011; Golcher and Reznicek, 2011; Wong and Dras, 2011; Brooke and Hirst, 2011; Wong et al., 2012). Though a wide range of feature types has been explored—with conﬂicting results—the evaluation of these feature sets has been fairly uniform: training and testing in one of several small corpora of learner essays (Granger et al., 2009; Yannakoudakis et al., 2011; Lüdeling et al., 2008), which are unfortunately quite expensive to collect. A notable problem with these corpora with respect to native language ident"
C12-1025,C08-1118,0,0.0445016,"Missing"
C12-1025,U09-1008,0,0.0472079,"dentify those who misrepresent themselves online (Fette et al., 2007). Another is as a preprocessing step to language learner error correction (Leacock et al., 2010): for example, Rozovskaya and Roth (2011) use L1-speciﬁc information to improve their preposition-correction system, and recent work in collocation correction relies on the speciﬁc forms present in the writer’s native language (Chang et al., 2008; Dahlmeier and Ng, 2011). As a distinct task in computational linguistics, native language identiﬁcation has been reasonably well-addressed (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2009), and in fact there has been a ﬂurry of recent activity (Kochmar, 2011; Golcher and Reznicek, 2011; Wong and Dras, 2011; Brooke and Hirst, 2011; Wong et al., 2012). Though a wide range of feature types has been explored—with conﬂicting results—the evaluation of these feature sets has been fairly uniform: training and testing in one of several small corpora of learner essays (Granger et al., 2009; Yannakoudakis et al., 2011; Lüdeling et al., 2008), which are unfortunately quite expensive to collect. A notable problem with these corpora with respect to native language identiﬁcation, however, is"
C12-1025,D11-1148,0,0.495945,"arner error correction (Leacock et al., 2010): for example, Rozovskaya and Roth (2011) use L1-speciﬁc information to improve their preposition-correction system, and recent work in collocation correction relies on the speciﬁc forms present in the writer’s native language (Chang et al., 2008; Dahlmeier and Ng, 2011). As a distinct task in computational linguistics, native language identiﬁcation has been reasonably well-addressed (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2009), and in fact there has been a ﬂurry of recent activity (Kochmar, 2011; Golcher and Reznicek, 2011; Wong and Dras, 2011; Brooke and Hirst, 2011; Wong et al., 2012). Though a wide range of feature types has been explored—with conﬂicting results—the evaluation of these feature sets has been fairly uniform: training and testing in one of several small corpora of learner essays (Granger et al., 2009; Yannakoudakis et al., 2011; Lüdeling et al., 2008), which are unfortunately quite expensive to collect. A notable problem with these corpora with respect to native language identiﬁcation, however, is a clear interaction between native language and essay topic. Generally speaking, the solution in previous work has been"
C12-1025,D12-1064,0,0.560639,": for example, Rozovskaya and Roth (2011) use L1-speciﬁc information to improve their preposition-correction system, and recent work in collocation correction relies on the speciﬁc forms present in the writer’s native language (Chang et al., 2008; Dahlmeier and Ng, 2011). As a distinct task in computational linguistics, native language identiﬁcation has been reasonably well-addressed (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2009), and in fact there has been a ﬂurry of recent activity (Kochmar, 2011; Golcher and Reznicek, 2011; Wong and Dras, 2011; Brooke and Hirst, 2011; Wong et al., 2012). Though a wide range of feature types has been explored—with conﬂicting results—the evaluation of these feature sets has been fairly uniform: training and testing in one of several small corpora of learner essays (Granger et al., 2009; Yannakoudakis et al., 2011; Lüdeling et al., 2008), which are unfortunately quite expensive to collect. A notable problem with these corpora with respect to native language identiﬁcation, however, is a clear interaction between native language and essay topic. Generally speaking, the solution in previous work has been to avoid the use of lexical features that m"
C12-1025,P11-1019,0,0.124737,"er and Ng, 2011). As a distinct task in computational linguistics, native language identiﬁcation has been reasonably well-addressed (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2009), and in fact there has been a ﬂurry of recent activity (Kochmar, 2011; Golcher and Reznicek, 2011; Wong and Dras, 2011; Brooke and Hirst, 2011; Wong et al., 2012). Though a wide range of feature types has been explored—with conﬂicting results—the evaluation of these feature sets has been fairly uniform: training and testing in one of several small corpora of learner essays (Granger et al., 2009; Yannakoudakis et al., 2011; Lüdeling et al., 2008), which are unfortunately quite expensive to collect. A notable problem with these corpora with respect to native language identiﬁcation, however, is a clear interaction between native language and essay topic. Generally speaking, the solution in previous work has been to avoid the use of lexical features that might carry topical information, limiting feature sets to syntactic and phonological phenomena. There are two reasons to be critical of this approach. First, there are almost certainly kinds of language transfer (Odlin, 1989), i.e. transfer related to lexical choi"
C14-1071,J90-1003,0,0.386123,"into a multiword lexicon. Evaluating in four large, distinct corpora, we show that this method creates segments which correspond well to known multiword expressions; our model is particularly strong with regards to longer (3+ word) multiword units, which are often ignored or minimized in relevant work. 1 Introduction Identification of multiword units in language is an active but increasingly fragmented area of research, a problem which can limit the ability of others to make use of units beyond the level of the word as input to other applications. General research on word association metrics (Church and Hanks, 1990; Smadja, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010), though increasingly comprehensive in its scope, has mostly failed to identify a single best choice, leading some to argue that the variety of multiword phenomena must be tackled individually. For instance, there is a body of research focusing specifically on collocations that are (to some degree) non-compositional, i.e. multiword expressions (Sag et al., 2002; Baldwin and Kim, 2010), with individual projects often limited to a particular set of syntactic patterns, e.g. verb-noun combinations (Fazly et al., 2009). A major is"
C14-1071,N09-1062,0,0.0318708,"natives based on relevant association measures. This result is consistent across corpora, though we do particularly well with highly stereotyped language such as seen in the biomedical domain. Future work on improving the model will likely focus on extensions related to syntax, for instance bootstrapped POS filtering and discounting of predictability that can be attributed solely to syntactic patterns. Our method could also be adapted to decompose full syntactic trees rather than sequences of words, offering tractable alternatives to Bayesian approaches that identify recurring tree fragments (Cohn et al., 2009); this would allow us, for instance, to correctly identify constructions with long-distance dependencies or other kinds of variation where relying on the surface form is insufficient (Seretan, 2011). With regards to applications, we will be investigating how to help learners notice these chunks when reading and then use them appropriately in their own writing; this work will eventually intersect with the well-established areas of grammatical error correction (Leacock et al., 2014) and automated essay scoring (Shermis and Burstein, 2003). As part of this, we will be building distributional lexi"
C14-1071,I05-3017,0,0.0369613,"han theirs). Though the model is general, their focus is limited to term extraction, and for larger terms they compare only with the c-value approach of Frantzi et al. (2000). Other closely related work includes general tools available for creating multiword lexicons using association measures or otherwise exploring the collocational behavior of words (Kilgarriff and Tugwell, 2001; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Pedersen et al., 2011). Other related but distinct tasks include syntactic chunking (Abney, 1991) and word segmentation for Asian languages, in particular Chinese (Emerson, 2005). 754 3 3.1 Method Prediction-based segmentation Our full method consists of multiple independent steps, but it is based on one central and relatively simple idea that we will introduce first. Given a sequence of words, w1 . . . wn , and statistics (i.e. n-gram counts) about the use of these words in a corpus, we first define p(wi |w j,k ) as the conditional probability of some word wi appearing with some contextual subsequence w j . . . wi−1 , wi+1 . . . wk , 1 ≤ j ≤ i ≤ k ≤ n. In the case i = j = k, this is simply the marginal probability, p(wi ). We then define the word predictability of so"
C14-1071,J09-1005,0,0.286677,"metrics (Church and Hanks, 1990; Smadja, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010), though increasingly comprehensive in its scope, has mostly failed to identify a single best choice, leading some to argue that the variety of multiword phenomena must be tackled individually. For instance, there is a body of research focusing specifically on collocations that are (to some degree) non-compositional, i.e. multiword expressions (Sag et al., 2002; Baldwin and Kim, 2010), with individual projects often limited to a particular set of syntactic patterns, e.g. verb-noun combinations (Fazly et al., 2009). A major issue with approaches involving statistical association is that they rarely address expressions larger than 2 words (Heid, 2007); in corpus linguistics, larger sequences referred to as lexical bundles are extracted using an n-gram frequency cutoff (Biber et al., 2004), but the frequency threshold is typically set very high so that only a very limited set is extracted. Another drawback, common to almost all these methods, is that they rarely offer an explicit segmentation of a text into multiword units, which would be preferable for downstream uses such as probabilistic distributional"
C14-1071,W11-0818,0,0.0230072,"rations through the corpus (Newman et al. report using 5000), an approach which is simply not tractable for the large corpora that we address in this paper (which are roughly 1000 times larger than theirs). Though the model is general, their focus is limited to term extraction, and for larger terms they compare only with the c-value approach of Frantzi et al. (2000). Other closely related work includes general tools available for creating multiword lexicons using association measures or otherwise exploring the collocational behavior of words (Kilgarriff and Tugwell, 2001; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Pedersen et al., 2011). Other related but distinct tasks include syntactic chunking (Abney, 1991) and word segmentation for Asian languages, in particular Chinese (Emerson, 2005). 754 3 3.1 Method Prediction-based segmentation Our full method consists of multiple independent steps, but it is based on one central and relatively simple idea that we will introduce first. Given a sequence of words, w1 . . . wn , and statistics (i.e. n-gram counts) about the use of these words in a corpus, we first define p(wi |w j,k ) as the conditional probability of some word wi appearing with some contextual"
C14-1071,C12-1127,0,0.225758,"association is that they rarely address expressions larger than 2 words (Heid, 2007); in corpus linguistics, larger sequences referred to as lexical bundles are extracted using an n-gram frequency cutoff (Biber et al., 2004), but the frequency threshold is typically set very high so that only a very limited set is extracted. Another drawback, common to almost all these methods, is that they rarely offer an explicit segmentation of a text into multiword units, which would be preferable for downstream uses such as probabilistic distributional semantics. An exception is the Bayesian approach of Newman et al. (2012), but their method does not scale well (see Section 2). Our own long-term motivation is to identify a wide variety of multiword units for assisting language learning, since correct use of collocations is known to pose a particular challenge to learners (Chen and Baker, 2010). Here, we present a multiword unit segmenter1 with the following key features: • It is entirely unsupervised. • It offers both segmentation of the input corpus and a lexicon which can be used to segment new corpora. • It is scalable to very large corpora, and works for a variety of corpora. • It is language independent. Th"
C14-1071,W11-0821,0,0.0132998,"wman et al. report using 5000), an approach which is simply not tractable for the large corpora that we address in this paper (which are roughly 1000 times larger than theirs). Though the model is general, their focus is limited to term extraction, and for larger terms they compare only with the c-value approach of Frantzi et al. (2000). Other closely related work includes general tools available for creating multiword lexicons using association measures or otherwise exploring the collocational behavior of words (Kilgarriff and Tugwell, 2001; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Pedersen et al., 2011). Other related but distinct tasks include syntactic chunking (Abney, 1991) and word segmentation for Asian languages, in particular Chinese (Emerson, 2005). 754 3 3.1 Method Prediction-based segmentation Our full method consists of multiple independent steps, but it is based on one central and relatively simple idea that we will introduce first. Given a sequence of words, w1 . . . wn , and statistics (i.e. n-gram counts) about the use of these words in a corpus, we first define p(wi |w j,k ) as the conditional probability of some word wi appearing with some contextual subsequence w j . . . wi"
C14-1071,W01-0513,0,0.698372,"n four large, distinct corpora, we show that this method creates segments which correspond well to known multiword expressions; our model is particularly strong with regards to longer (3+ word) multiword units, which are often ignored or minimized in relevant work. 1 Introduction Identification of multiword units in language is an active but increasingly fragmented area of research, a problem which can limit the ability of others to make use of units beyond the level of the word as input to other applications. General research on word association metrics (Church and Hanks, 1990; Smadja, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010), though increasingly comprehensive in its scope, has mostly failed to identify a single best choice, leading some to argue that the variety of multiword phenomena must be tackled individually. For instance, there is a body of research focusing specifically on collocations that are (to some degree) non-compositional, i.e. multiword expressions (Sag et al., 2002; Baldwin and Kim, 2010), with individual projects often limited to a particular set of syntactic patterns, e.g. verb-noun combinations (Fazly et al., 2009). A major issue with approaches involving statistical"
C14-1071,J93-1007,0,0.310092,". Evaluating in four large, distinct corpora, we show that this method creates segments which correspond well to known multiword expressions; our model is particularly strong with regards to longer (3+ word) multiword units, which are often ignored or minimized in relevant work. 1 Introduction Identification of multiword units in language is an active but increasingly fragmented area of research, a problem which can limit the ability of others to make use of units beyond the level of the word as input to other applications. General research on word association metrics (Church and Hanks, 1990; Smadja, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010), though increasingly comprehensive in its scope, has mostly failed to identify a single best choice, leading some to argue that the variety of multiword phenomena must be tackled individually. For instance, there is a body of research focusing specifically on collocations that are (to some degree) non-compositional, i.e. multiword expressions (Sag et al., 2002; Baldwin and Kim, 2010), with individual projects often limited to a particular set of syntactic patterns, e.g. verb-noun combinations (Fazly et al., 2009). A major issue with appro"
C14-1071,W11-0822,0,\N,Missing
C14-1089,P05-1018,0,0.108424,"a more coherent text over a less coherent counterpart. Accuracy is therefore measured as the fraction of correct pairwise rankings as recognized by the ranker. In our experiments, we use the SVMlight package4 (Joachims, 1999) with the ranking configuration, and all parameters are set to their default values. 5.1 Sentence Ordering The task of sentence ordering, which has been extensively studied in previous work, attempts to simulate the situation where, given a predefined set of information-bearing items, we need to determine the best order in which the items should be presented. As argued by Barzilay and Lapata (2005), sentence ordering is an essential step in many content-generation components, such as multi-document summarization. In this task, we use a dataset consisting of a subset of the Wall Street Journal (WSJ) corpus, in which the minimum length of a text is 20 sentences, and the average length is 41 sentences. For each text, we create 20 random permutations by shuffling the original order of the sentences. In total, we have 735 source documents and 735 × 20 = 14, 700 permutations. Because the RST-style discourse parser we use is trained on a fraction of the WSJ corpus, we remove the training texts"
C14-1089,P10-1020,0,0.0223086,"puted as the number of occurrences of t in the entity grid of document d, divided by the total number of transitions of the same length. Moreover, entities are differentiated by their salience — an entity is deemed to be salient if it occurs at least l times in the text, and non-salient otherwise — and transitions are computed separately for salient and non-salient entities. 3.1 Extension: Lin et al.’s Discourse Role Matrix As mentioned previously, most extensions to B&L’s entity-based local coherence model focus on enriching the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and Hirst (2012a)’s extension from the perspective of improving the learning procedure. Among various extensions to B&L’s entity-based local coherence model, the one most related to ours is Lin et al. (2011)’s work on encoding a text as a set of entities with their associated discourse roles. Lin et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using such patterns to measure the coherence of a text can result in feature sparseness. To"
C14-1089,de-marneffe-etal-2006-generating,0,0.04626,"Missing"
C14-1089,P11-2022,0,0.382735,"ccurrences of t in the entity grid of document d, divided by the total number of transitions of the same length. Moreover, entities are differentiated by their salience — an entity is deemed to be salient if it occurs at least l times in the text, and non-salient otherwise — and transitions are computed separately for salient and non-salient entities. 3.1 Extension: Lin et al.’s Discourse Role Matrix As mentioned previously, most extensions to B&L’s entity-based local coherence model focus on enriching the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and Hirst (2012a)’s extension from the perspective of improving the learning procedure. Among various extensions to B&L’s entity-based local coherence model, the one most related to ours is Lin et al. (2011)’s work on encoding a text as a set of entities with their associated discourse roles. Lin et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem, they ex"
C14-1089,E12-1032,1,0.935301,"e length. Moreover, entities are differentiated by their salience — an entity is deemed to be salient if it occurs at least l times in the text, and non-salient otherwise — and transitions are computed separately for salient and non-salient entities. 3.1 Extension: Lin et al.’s Discourse Role Matrix As mentioned previously, most extensions to B&L’s entity-based local coherence model focus on enriching the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and Hirst (2012a)’s extension from the perspective of improving the learning procedure. Among various extensions to B&L’s entity-based local coherence model, the one most related to ours is Lin et al. (2011)’s work on encoding a text as a set of entities with their associated discourse roles. Lin et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem, they expand the relation sequence into a discourse role matrix, as shown in Table 2. correspond Columns"
C14-1089,P12-1007,1,0.922797,"e length. Moreover, entities are differentiated by their salience — an entity is deemed to be salient if it occurs at least l times in the text, and non-salient otherwise — and transitions are computed separately for salient and non-salient entities. 3.1 Extension: Lin et al.’s Discourse Role Matrix As mentioned previously, most extensions to B&L’s entity-based local coherence model focus on enriching the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and Hirst (2012a)’s extension from the perspective of improving the learning procedure. Among various extensions to B&L’s entity-based local coherence model, the one most related to ours is Lin et al. (2011)’s work on encoding a text as a set of entities with their associated discourse roles. Lin et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem, they expand the relation sequence into a discourse role matrix, as shown in Table 2. correspond Columns"
C14-1089,P14-1048,1,0.405897,"Missing"
C14-1089,W07-2321,0,0.472327,"fined length k. pt (d) is computed as the number of occurrences of t in the entity grid of document d, divided by the total number of transitions of the same length. Moreover, entities are differentiated by their salience — an entity is deemed to be salient if it occurs at least l times in the text, and non-salient otherwise — and transitions are computed separately for salient and non-salient entities. 3.1 Extension: Lin et al.’s Discourse Role Matrix As mentioned previously, most extensions to B&L’s entity-based local coherence model focus on enriching the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and Hirst (2012a)’s extension from the perspective of improving the learning procedure. Among various extensions to B&L’s entity-based local coherence model, the one most related to ours is Lin et al. (2011)’s work on encoding a text as a set of entities with their associated discourse roles. Lin et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using such patterns to measure the coherence of a text can result i"
C14-1089,D12-1083,0,0.0181804,"List.N Contrast.S List.N Contrast.S nil nil S2 List.N Condition.N Contrast.S S3 Contrast.N Background.N Cause.N nil nil Cause.S nil Cause.S concern market Table 3: A fragment of the full RST-style discourse role matrix for the example text with the first six entities across three sentences. First, we differentiate between intra- and multi-sentential discourse relations, which is motivated by a finding in the field of RST-style discourse parsing — distributions of various discourse relation types are quite distinct between intra-sentential and multi-sentential instances (Feng and Hirst, 2012b; Joty et al., 2012) — and we assume that a similar phenomenon exists for PDTB-style discourse relations. Therefore, we assign two sets of discourse roles to each entity: intra-sentential and multi-sentential roles, which are the roles that the entity plays in the corresponding intra- and multi-sentential relations. Second, instead of Level-1 PDTB discourse relations (6 in total), we use Level-2 relations (18 in total) in feature encoding, so that richer information can be captured in the model, resulting in 18 × 2 = 36 different discourse roles with argument attached. We then generate four separate set of featur"
C14-1089,P11-1100,1,0.710723,"d features. In contrast, we seek insights of better feature encoding from a more general problem: discourse parsing (to be introduced in Section 2). Discourse parsing aims to identify the discourse relations held among various discourse units in the text. Therefore, one can expect that discourse parsing provides useful information to the evaluation of text coherence, because, essentially, the existence and the distribution of discourse relations are the basis of the coherence in a text. In fact, there is already evidence showing that discourse relations can help better capture text coherence. Lin et al. (2011) use a PDTB-style discourse parser (to be introduced in Section 2.1) to identify discourse relations in the text, and they represent a text by entities and their associated discourse roles in each sentence. In their experiments, using discourse roles alone, their model performs very similar or even better than B&L’s model. Combining their discourse role features with B&L’s entity-based transition features further improves the performance. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. Licens"
C14-1089,D10-1023,0,0.094098,"encoded, including the RST-style feature encoding and its combination with the entity-base feature encoding. With respect to the performance, we observe a number of consistent patterns across both evaluation tasks. First, with no discourse structure encoded, the entity-based model (the first row) performs the worst among all models, suggesting that discourse structures are truly important and can capture coherence in a more sophisticated way than pure grammatical roles. Moreover, the performance gap is particularly large for essay scoring, which is probably due to the fact that, as argued by Persing et al. (2010), the organization score, which we use to approximate the degrees of coherence, is not equivalent to text coherence. Organization relates more to the logical development in the texts, while coherence is about lexical and semantic continuity; but discourse relations can capture the logical relations at least to some extent. Secondly, with deep discourse structures encoded, the RST-style model in the third section significantly outperforms (p &lt; .01) the models with shallow discourse structures, i.e., the PDTB-style and 947 shallow RST-style models in the middle section, confirming our intuition"
C14-1089,prasad-etal-2008-penn,0,0.0825747,"erent applications. Currently, the two main directions in the study of discourse parsing are PDTB-style and RST-style parsing. These two directions are based on distinct theoretical frameworks, and each can be potentially useful for particular kinds of downstream applications. As will be discussed shortly, the major difference between PDTB- and RST-style discourse parsing is the notion of deep hierarchical discourse structure, which, according to our hypothesis, can be very useful for recognizing text coherence. 2.1 PDTB-style Discourse Parsing The Penn Discourse Treebank (PDTB), developed by Prasad et al. (2008), is currently the largest discourse-annotated corpus, consisting of 2159 Wall Street Journal articles. The annotation in PDTB adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because) is treated as a predicate that takes two text spans as its arguments. The argument that the discourse connective structurally attaches to is called Arg2, and the other argument is called Arg1. In PDTB, relations are further categorized into explicit and implicit relations: a relation is explicit if there is an explicit discourse connective presented in the text; othe"
C14-1205,andreevskaia-bergler-2006-semantic,0,0.0230515,"appears to be fundamental to the success of a supervised approach to lexical spectra. 2 Related Work Viewed primarily as a categorical task, the creation or expansion of lexical resources for sentiment analysis is a commonly-addressed problem. In addition to SentiWordNet (Baccianella et al., 2010), which we will compare to directly to here, there are numerous mostly semi-supervised approaches based on exploiting the glosses and/or the graph structure of WordNet to determine whether a word is positive or negative (Kamps et al., 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Rao and Ravichandra, 2009; Hassan and Radev, 2010), or taking advantage of some other lexicographic resources (Mohammad et al., 2009; Klebanov et al., 2013). The earliest corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic information, i.e. conjunctions, to make connections between adjectives; other work that makes use of local patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006). Turney (2002) built a continuous polarity lexicon using PMI based on Internet hit counts as a useful measure of relatedness b"
C14-1205,baccianella-etal-2010-sentiwordnet,0,0.256466,"ibute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra such as those in the MRC database, e.g. abstractness (Turney et al., 2011), and other kinds of variation captured in, for instance, Osgood’s semantic differential (Osgood et al., 1957). The typical approa"
C14-1205,I13-1010,1,0.90543,"re, we follow our earlier work in using a much smaller k value (20) than is typical for topical uses of LSA, which we found was better for this dataset, since major stylistic differences seem to be mostly captured in the first few dimensions after dimensionality reduction, a result which is consistent with the work of Biber (1988) looking at differences across registers. Unlike for polarity, there is no resource available that offers a full scale of formality for a large number of words, and the set used in our initial work on formality has only extreme, handpicked words. In more recent work (Brooke and Hirst, 2013), we used a larger set of words (900) that included a variety of different styles that had been tagged by a group of 5 annotators. In that work, we did not use the term “formality,” but one of our styles, colloquial, corresponds to the informal end of the spectrum, and two other styles, objective and literary, can both be viewed as social-distancing language.5 The words tagged by annotators as belonging to neither of these categories will serve as the middle of this spectrum. Compared to our polarity lexicon, our training set therefore is much more coarse-grained (with only 3 rankings, as comp"
C14-1205,C10-2011,1,0.449503,"wise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra such as those in the MRC database, e.g. abstractness (Turney et al., 2011), and other kinds of variation captured in, for instance, Osgood’s semantic differential (Osgood et al., 1957). The typical approach to this problem involves semi-supervised methods using vector space and/or graph repre"
C14-1205,E99-1042,0,0.0732454,"level, Li and Yarowsky (2008) identify formal and informal synonyms in Chinese. Heylighen and Dewaele (2002) and Li et al. (2013) both offer text-level quantifications of formality; the former is based on POS frequency, while the latter is based on the Coh-Metrix textual metrics. Using these kinds of metrics, formality has been evaluated in social media (Mosquera and Moreda, 2012). A supervised text classification approach to formality is offered by Sheika and Inkpen (2012). Lexical formality is obviously related to lexicon-based readability (Kidwell et al., 2009) and lexical simplification (Carroll et al., 1999), and is 2173 also relevant to the recent interest in identifying social relationships (Peterson et al., 2011) and shows of politeness (Danescu-Niculescu-Mizil et al., 2013). 3 Method Our approach to lexicon acquisition falls into the general category of corpus-based techniques. For both attributes addressed in this paper, we use the same corpus, the 2009 ICWSM Spinn3r dataset (Burton et al., 2009), a publicly-available blog corpus which we also used in our earlier work on lexical formality (Brooke et al., 2010). Blogs are a good resource for broad lexical acquisition because they are very bro"
C14-1205,C12-1037,0,0.0177807,"Turney and Littman (2003) compared this approach with LSA, which uses general patterns of co-occurrence based on dimensionality reduction. Velikovich et al. (2010) combined web-scale corpora with a graph-based approach, assigning polarity scores to n-grams on the basis of the maximum weighed path from an n-gram to the seed terms, using a small (6-word) context around the word. Like us, Volkova et al. (2013) use social media, iteratively labeling tweets and words for subjectivity and polarity. Fully-supervised approaches to polarity lexicon acquisition are rare, but one example is the work of Chetviorkin and Loukachevitch (2012), who classify words as being sentiment-relevant in Russian using a small set of statistical features, including ratios across disparate corpora. Our interest in the continuous aspect of polarity overlaps with work on deriving the semantic intensity of lexical items from corpora (Sheinman and Tokunaga, 2009); in this task, small sets of synonyms are ranked according to their intensity, including (but not limited to) polarity. De Melo and Bansal (2013) use a Mixed Integer Linear Programing algorithm to combine information from multiple pairs into a single coherent ranking. As with some of the w"
C14-1205,P13-1025,0,0.0314978,"tifications of formality; the former is based on POS frequency, while the latter is based on the Coh-Metrix textual metrics. Using these kinds of metrics, formality has been evaluated in social media (Mosquera and Moreda, 2012). A supervised text classification approach to formality is offered by Sheika and Inkpen (2012). Lexical formality is obviously related to lexicon-based readability (Kidwell et al., 2009) and lexical simplification (Carroll et al., 1999), and is 2173 also relevant to the recent interest in identifying social relationships (Peterson et al., 2011) and shows of politeness (Danescu-Niculescu-Mizil et al., 2013). 3 Method Our approach to lexicon acquisition falls into the general category of corpus-based techniques. For both attributes addressed in this paper, we use the same corpus, the 2009 ICWSM Spinn3r dataset (Burton et al., 2009), a publicly-available blog corpus which we also used in our earlier work on lexical formality (Brooke et al., 2010). Blogs are a good resource for broad lexical acquisition because they are very broad in style and content, and are available in essentially unlimited amounts. We use the English Tier 1 (high-quality) blogs that have at least 100 word types, excluding dupl"
C14-1205,Q13-1023,0,0.0381633,"Missing"
C14-1205,P10-1041,0,0.0182475,"proach to lexical spectra. 2 Related Work Viewed primarily as a categorical task, the creation or expansion of lexical resources for sentiment analysis is a commonly-addressed problem. In addition to SentiWordNet (Baccianella et al., 2010), which we will compare to directly to here, there are numerous mostly semi-supervised approaches based on exploiting the glosses and/or the graph structure of WordNet to determine whether a word is positive or negative (Kamps et al., 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Rao and Ravichandra, 2009; Hassan and Radev, 2010), or taking advantage of some other lexicographic resources (Mohammad et al., 2009; Klebanov et al., 2013). The earliest corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic information, i.e. conjunctions, to make connections between adjectives; other work that makes use of local patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006). Turney (2002) built a continuous polarity lexicon using PMI based on Internet hit counts as a useful measure of relatedness between seeds, and Turney and Littman (2003) compared"
C14-1205,P97-1023,0,0.756019,"ntain fine-grained judgments of spectra—an example is the MRC psychological database (Coltheart, 1980)—these tend to be very low in coverage, reflecting the difficulty in collecting this information. Within computational linguistics, the continuous lexical attribute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and a"
C14-1205,D07-1115,0,0.114002,"RC psychological database (Coltheart, 1980)—these tend to be very low in coverage, reflecting the difficulty in collecting this information. Within computational linguistics, the continuous lexical attribute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra suc"
C14-1205,kamps-etal-2004-using,0,0.0611527,"Missing"
C14-1205,W06-1642,0,0.036584,"ine whether a word is positive or negative (Kamps et al., 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Rao and Ravichandra, 2009; Hassan and Radev, 2010), or taking advantage of some other lexicographic resources (Mohammad et al., 2009; Klebanov et al., 2013). The earliest corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic information, i.e. conjunctions, to make connections between adjectives; other work that makes use of local patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006). Turney (2002) built a continuous polarity lexicon using PMI based on Internet hit counts as a useful measure of relatedness between seeds, and Turney and Littman (2003) compared this approach with LSA, which uses general patterns of co-occurrence based on dimensionality reduction. Velikovich et al. (2010) combined web-scale corpora with a graph-based approach, assigning polarity scores to n-grams on the basis of the maximum weighed path from an n-gram to the seed terms, using a small (6-word) context around the word. Like us, Volkova et al. (2013) use social media, iteratively labeling tweet"
C14-1205,D09-1094,0,0.0165161,"arch that directly addresses formality. At lexical level, Li and Yarowsky (2008) identify formal and informal synonyms in Chinese. Heylighen and Dewaele (2002) and Li et al. (2013) both offer text-level quantifications of formality; the former is based on POS frequency, while the latter is based on the Coh-Metrix textual metrics. Using these kinds of metrics, formality has been evaluated in social media (Mosquera and Moreda, 2012). A supervised text classification approach to formality is offered by Sheika and Inkpen (2012). Lexical formality is obviously related to lexicon-based readability (Kidwell et al., 2009) and lexical simplification (Carroll et al., 1999), and is 2173 also relevant to the recent interest in identifying social relationships (Peterson et al., 2011) and shows of politeness (Danescu-Niculescu-Mizil et al., 2013). 3 Method Our approach to lexicon acquisition falls into the general category of corpus-based techniques. For both attributes addressed in this paper, we use the same corpus, the 2009 ICWSM Spinn3r dataset (Burton et al., 2009), a publicly-available blog corpus which we also used in our earlier work on lexical formality (Brooke et al., 2010). Blogs are a good resource for b"
C14-1205,C04-1200,0,0.0312333,"nking rather than regression in this space appears to be fundamental to the success of a supervised approach to lexical spectra. 2 Related Work Viewed primarily as a categorical task, the creation or expansion of lexical resources for sentiment analysis is a commonly-addressed problem. In addition to SentiWordNet (Baccianella et al., 2010), which we will compare to directly to here, there are numerous mostly semi-supervised approaches based on exploiting the glosses and/or the graph structure of WordNet to determine whether a word is positive or negative (Kamps et al., 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Rao and Ravichandra, 2009; Hassan and Radev, 2010), or taking advantage of some other lexicographic resources (Mohammad et al., 2009; Klebanov et al., 2013). The earliest corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic information, i.e. conjunctions, to make connections between adjectives; other work that makes use of local patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006). Turney (2002) built a continuous polarity lexicon using PMI based on I"
C14-1205,Q13-1009,0,0.0112384,"of lexical resources for sentiment analysis is a commonly-addressed problem. In addition to SentiWordNet (Baccianella et al., 2010), which we will compare to directly to here, there are numerous mostly semi-supervised approaches based on exploiting the glosses and/or the graph structure of WordNet to determine whether a word is positive or negative (Kamps et al., 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Rao and Ravichandra, 2009; Hassan and Radev, 2010), or taking advantage of some other lexicographic resources (Mohammad et al., 2009; Klebanov et al., 2013). The earliest corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic information, i.e. conjunctions, to make connections between adjectives; other work that makes use of local patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006). Turney (2002) built a continuous polarity lexicon using PMI based on Internet hit counts as a useful measure of relatedness between seeds, and Turney and Littman (2003) compared this approach with LSA, which uses general patterns of co-occurrence based on dimensionality reduction. V"
C14-1205,D08-1108,0,0.0230715,"anking. As with some of the work in polarity, the focus is on adjectives and local patterns which explicitly distinguish degrees of intensity e.g. not only x but also y, which limits its range of application; it would not, for instance, be useful for formality or other more pragmatic variations. Beyond the our work in LSA-based formality lexicon creation (Brooke et al., 2010) and the sentencelevel formality annotation of Lahiri et al. (2011), which we discuss later in more detail, there is a relatively small amount of computational research that directly addresses formality. At lexical level, Li and Yarowsky (2008) identify formal and informal synonyms in Chinese. Heylighen and Dewaele (2002) and Li et al. (2013) both offer text-level quantifications of formality; the former is based on POS frequency, while the latter is based on the Coh-Metrix textual metrics. Using these kinds of metrics, formality has been evaluated in social media (Mosquera and Moreda, 2012). A supervised text classification approach to formality is offered by Sheika and Inkpen (2012). Lexical formality is obviously related to lexicon-based readability (Kidwell et al., 2009) and lexical simplification (Carroll et al., 1999), and is"
C14-1205,D09-1063,0,0.303491,"to be very low in coverage, reflecting the difficulty in collecting this information. Within computational linguistics, the continuous lexical attribute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra such as those in the MRC database, e.g. abstractness"
C14-1205,S13-2053,0,0.0224531,"position, we use a binary term-document matrix with the same ICWSM texts as our supervised model, with k = 500 (a fairly standard choice). In the second step, which involves calculating the cosine similarity with a set of seed terms using the LSA vectors and then taking the difference, the positive and negative seeds are just the training instances for our supervised model (neutral terms are discarded). Our third comparison is the PMI approach of Turney (2002), which is still popular: for instance, PMI was used to built a Twitter sentiment lexicon in the winning entry in a recent shared task (Mohammad et al., 2013). Because they have access to the same corpus and even the same example words as our method, the LSA and PMI alternatives are most directly comparable to ours. The results for the word-level polarity experiments are shown in the left side of Table 1. In the SOCAL test set, the results are clear: our SVM ranking method is preferred over alternatives, across all the different categories of pairwise comparison. The relative difficulty of each pair type reflects the average distance between relevant pairs on the spectrum, as expected. Surprisingly, the correlation method, despite using the same fe"
C14-1205,W11-0711,0,0.336843,"and Li et al. (2013) both offer text-level quantifications of formality; the former is based on POS frequency, while the latter is based on the Coh-Metrix textual metrics. Using these kinds of metrics, formality has been evaluated in social media (Mosquera and Moreda, 2012). A supervised text classification approach to formality is offered by Sheika and Inkpen (2012). Lexical formality is obviously related to lexicon-based readability (Kidwell et al., 2009) and lexical simplification (Carroll et al., 1999), and is 2173 also relevant to the recent interest in identifying social relationships (Peterson et al., 2011) and shows of politeness (Danescu-Niculescu-Mizil et al., 2013). 3 Method Our approach to lexicon acquisition falls into the general category of corpus-based techniques. For both attributes addressed in this paper, we use the same corpus, the 2009 ICWSM Spinn3r dataset (Burton et al., 2009), a publicly-available blog corpus which we also used in our earlier work on lexical formality (Brooke et al., 2010). Blogs are a good resource for broad lexical acquisition because they are very broad in style and content, and are available in essentially unlimited amounts. We use the English Tier 1 (high-q"
C14-1205,E09-1077,0,0.131094,"oltheart, 1980)—these tend to be very low in coverage, reflecting the difficulty in collecting this information. Within computational linguistics, the continuous lexical attribute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra such as those in the MRC datab"
C14-1205,P13-1094,0,0.0442699,"Missing"
C14-1205,taboada-etal-2006-methods,0,0.0357795,"Missing"
C14-1205,J11-2001,1,0.389663,"get word w that we wish to profile, we sample a set of n texts from S which contain the target word (or all the documents where the word appears, if it appears fewer than n times), and count the document frequency of each profile word p in this subcorpus, Tw . We ignore the term frequencies within individual documents because a binary representation is known to be preferred for stylistic dimensions like formality (Brooke et al., 2010), and this seems to be also somewhat true in the domain of polarity, where better results can be obtained when multiple instances of a polar word are discounted (Taboada et al., 2011). To avoid overfitting our statistical model, we do not count a word as appearing with itself. Once we have sub-corpus document frequences df Tpw for each p, for each profile word p we define the element of our co-occurrence profile vector v p as vp = df Tpw ∑q∈P df Tqw That is, we normalize each count by the sum across all counts, such that the L1 norm of v is 1. For our applications here, the dimension of the co-occurrence profile vector is typically in the tens of thousands, but to illustrate the creation of this vector, suppose we choose an extremely narrow document frequency band min-df,"
C14-1205,P05-1017,0,0.248503,"tra—an example is the MRC psychological database (Coltheart, 1980)—these tend to be very low in coverage, reflecting the difficulty in collecting this information. Within computational linguistics, the continuous lexical attribute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily"
C14-1205,D11-1063,0,0.0155705,"Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra such as those in the MRC database, e.g. abstractness (Turney et al., 2011), and other kinds of variation captured in, for instance, Osgood’s semantic differential (Osgood et al., 1957). The typical approach to this problem involves semi-supervised methods using vector space and/or graph representations and a set of seed terms. Our method is novel in that it uses fully supervised SVM ranking of co-occurrence profiles, i.e. normalized counts of instances of binary text co-occurrence between the target word and a large set of profiling words, selected on the basis of their frequency, in a publicly-available blog corpus. The seed terms from earlier methods are now viewe"
C14-1205,P02-1053,0,0.180548,"linguistics, the continuous lexical attribute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra such as those in the MRC database, e.g. abstractness (Turney et al., 2011), and other kinds of variation captured in, for instance, Osgood’s semantic d"
C14-1205,N10-1119,0,0.0146608,"). The earliest corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic information, i.e. conjunctions, to make connections between adjectives; other work that makes use of local patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006). Turney (2002) built a continuous polarity lexicon using PMI based on Internet hit counts as a useful measure of relatedness between seeds, and Turney and Littman (2003) compared this approach with LSA, which uses general patterns of co-occurrence based on dimensionality reduction. Velikovich et al. (2010) combined web-scale corpora with a graph-based approach, assigning polarity scores to n-grams on the basis of the maximum weighed path from an n-gram to the seed terms, using a small (6-word) context around the word. Like us, Volkova et al. (2013) use social media, iteratively labeling tweets and words for subjectivity and polarity. Fully-supervised approaches to polarity lexicon acquisition are rare, but one example is the work of Chetviorkin and Loukachevitch (2012), who classify words as being sentiment-relevant in Russian using a small set of statistical features, including ratios across d"
C14-1205,P13-2090,0,0.0547606,"age, reflecting the difficulty in collecting this information. Within computational linguistics, the continuous lexical attribute that has received the most attention is undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity. Much of the work focused on acquisition of this attribute at the lexical level has involved simplification to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009; Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification (Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper, we will focus on these two spectra; the method presented, however, is intended to be general, and as such could be easily applied to other spectra such as those in the MRC database, e.g. abstractness (Turney et al., 2011),"
C14-1205,H05-1044,0,0.0614535,"st sets. The first test set is the rest of the SO-CAL dictionary, excluding words in the training set as well as those not given a rating by SentiWordNet (see below). Note that this set is not balanced across SO values, since there are many more weakly positive (SO 1 to 3) or weakly negative (SO −1 to −3) words than more-extreme or neutral words; we would argue, though, that this reflects the actual situation in subjective corpora such as product reviews. To test whether we might be overfitting to the product reviews domain, we also test using annotations from the MPQA (Subjectivity) lexicon (Wilson et al., 2005), which was built primarily from news texts.1 For this, we again include only words that are in SentiWordNet. The MPQA lexicon uses a very different tagging schema than the SO-CAL dictionary, with 3 polarity categories (positive, negative, and neutral) as well as two degrees of subjectivity, weak or strong. Strong or weak subjectivity is defined as how reliable an indicator of subjectivity the word is, which does not directly correspond to the rationale used for the SO-CAL dictionary (which is closer to the notion of force or intensity); the results in Taboada et al. (2011) and our own examina"
D07-1060,W02-2006,0,0.0324426,"Missing"
D07-1060,P06-1046,0,0.0283785,"ombining written text with a published thesaurus to measure distance between concepts (or word senses) using distributional measures, thereby eliminating sense-conflation and achieving results better than the simple word-distance measures and indeed also most of the WordNet-based semantic measures. We called these measures distributional measures of concept-distance. Concept-distance 2 LSA is especially expensive as singular value decomposition, a key component for dimensionality reduction, requires computationally intensive matrix operations; making it less scalable to large amounts of text (Gorman and Curran, 2006). 572 measures can be used to measure distance between a word pair by choosing the distance between their closest senses. Thus, even though ‘children’s recreation’ is the predominant sense of play, the ‘drama’ sense is much closer to actor and so their distance will be chosen. These distributional conceptdistance approaches need to create only V × C cooccurrence and C × C distance matrices, where C is the number of categories or senses (usually about 1000). It should also be noted that unlike the best WordNet-based measures, distributional measures (both word- and concept-distance ones) can be"
D07-1060,I05-1067,1,0.955571,"ke resources that these methods require do not exist for most of the 3000–6000 languages in existence today and they are costly to create. In this paper, we introduce cross-lingual distributional measures of concept-distance, or simply cross-lingual measures, that determine the distance between a word pair belonging to a resource-poor language using a knowledge source in a resourcerich language and a bilingual lexicon3 . We will use the cross-lingual measures to calculate distances between German words using an English thesaurus and a German corpus. Although German is not resourcepoor per se, Gurevych (2005) has observed that the German wordnet GermaNet (Kunze, 2004) (about 60,000 synsets) is less developed than the English WordNet (Fellbaum, 1998) (about 117,000 synsets) with respect to the coverage of lexical items and lexical semantic relations represented therein. On the other hand, substantial raw corpora are available for the German language. Crucially for our evaluation, the existence of GermaNet allows comparison of our cross-lingual approach with monolingual ones. 2 Monolingual Distributional Measures In order to set the context for cross-lingual conceptdistance measures (Section 3), we"
D07-1060,O97-1002,0,0.740037,"guistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an English thesaurus to create English– German distributional profiles of concepts, which in turn will be used to measure the semantic distance between German words. Two classes of methods have been used in determining semantic distance. Semantic measures of concept-distance, such as those of Jiang and Conrath (1997) and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it (see Budanitsky and Hirst (2006) for a survey). Distributional measures of word-distance1 , such as cosine and α-skew divergence (Lee, 2001), deem 1 Many distributional approaches represent the sets of contexts of the target words as points in multidimensional cooccurrence space or as co-occurrence distributions. A measure, such as cosine, that captures vector distance or a measure, such as α-skew divergence, that captures distance between distributions"
D07-1060,S07-1004,0,0.0788107,"Missing"
D07-1060,W04-2607,1,0.78437,"e synset and in synsets close to it in the network. 576 (2005) and Zesch et al. (2007) asked native German speakers to mark two different sets of German word pairs with distance values. Set 1 (Gur65) consists of a German translation of the English Rubenstein and Goodenough (1965) dataset. It has 65 noun– noun word pairs. Set 2 (Gur350) is a larger dataset containing 350 word pairs made up of nouns, verbs, and adjectives. The semantically close word pairs in Gur65 are mostly synonyms or hypernyms (hyponyms) of each other, whereas those in Gur350 have both classical and non-classical relations (Morris and Hirst, 2004) with each other. Details of these semantic distance benchmarks13 are summarized in Table 2. Inter-subject correlations are indicative of the degree of ease in annotating the datasets. 4.1.2 Results and Discussion Word-pair distances determined using different distance measures are compared in two ways with the two human-created benchmarks. The rank ordering of the pairs from closest to most distant is evaluated with Spearman’s rank order correlation ρ; the distance judgments themselves are evaluated with Pearson’s correlation coefficient r. The higher the correlation, the more accurate the me"
D07-1060,2003.mtsummit-papers.42,0,0.0345828,"n raw text and possibly some shallow syntactic processing. They do not require any other manually-created resource, and tend to have a higher coverage. However, by themselves they perform poorly when compared to semantic measures (Mohammad and Hirst, 2006b) because when given a target word pair we usually need the distance between their closest senses, but distributional measures of word-distance tend to conflate the distances between all possible sense pairs. Latent semantic analysis (LSA) (Landauer et al., 1998) has also been used to measure distributional distance with encouraging results (Rapp, 2003). However, it too measures the distance between words and not senses. Further, the dimensionality reduction inherent to LSA has the effect of making the predominant sense more dominant while de-emphasizing the other senses. Therefore, an LSA-based approach will also conflate information from the different senses, and even more emphasis will be placed on the predominant senses. Given the semantically close target nouns play and actor, for example, a distributional measure will give a score that is some sort of a dominance-based average of the distances between their senses. The noun play has th"
D07-1060,P98-2127,0,0.234235,"Missing"
D07-1060,P04-1036,0,0.211979,"cooccurs with any word. A statistic such as PMI can then give the strength of association between w and c. with each of its senses is summed. The sense that has the highest cumulative association is chosen as the intended sense. A new bootstrapped WCCM is created such that each cell mi j , corresponding to en word wen i and concept c j , is populated with the en number of times wi co-occurs with any word used in sense cen j . Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain nearupper-bound results in the task of determining word sense dominance. Unlike the McCarthy et al. (2004) dominance system, our approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sensedistributed text5 . In Mohammad and Hirst (2006a), the DPC-based monolingual distributional measures of concept-distance were used to rank word pairs by their semantic similarity and to correct realword spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the distributional concept-distance measures performed better than all WordNet-based measures as well, excep"
D07-1060,P06-1040,0,0.0290057,". Nachbildung (replica) b. Doppelkinn (double chin) d. Zweitschrift (copy) Our approach to evaluating distance measures fol14 In Table 3, all values are statistically significant at the 0.01 level (2-tailed), except for the one in italic (0.212), which is significant at the 0.05 level (2-tailed). 15 English translations are in parentheses. 577 lows that of Jarmasz and Szpakowicz (2003), who evaluated semantic similarity measures through their ability to solve synonym problems (80 TOEFL (Landauer and Dumais, 1997), 50 ESL (Turney, 2001), and 300 (English) Reader’s Digest Word Power questions). Turney (2006) used a similar approach to evaluate the identification of semantic relations, with 374 college-level multiple-choice word analogy questions. The Reader’s Digest Word Power (RDWP) benchmark for German consists of 1072 of these word-choice problems collected from the January 2001 to December 2005 issues of the Germanlanguage edition (Wallace and Wallace, 2005). We discarded 44 problems that had more than one correct answer, and 20 problems that used a phrase instead of a single term as the target. The remaining 1008 problems form our evaluation dataset, which is significantly larger than any of"
D07-1060,E06-1016,1,0.581452,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,W06-1605,1,0.605764,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,N07-2052,1,0.781373,"Missing"
D07-1060,C92-2070,0,\N,Missing
D07-1060,J06-1003,1,\N,Missing
D08-1103,C92-2082,0,0.0701406,"Methods in Natural Language Processing, pages 982–991, c Honolulu, October 2008. 2008 Association for Computational Linguistics Lexicons of pairs of words that native speakers consider antonyms have been created for certain languages, but their coverage has been limited. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that are commonly considered antonym pairs, and they remain unrecorded. Even though a number of computational approaches have been proposed for semantic closeness, and some for hypernymy–hyponymy (Hearst, 1992), measures of antonymy have been less successful. To some extent, this is because antonymy is not as well understood as other classical lexical-semantic relations. We first very briefly summarize insights and intuitions about this phenomenon, as proposed by linguists and lexicographers (Section 2). We discuss related work (Section 3). We describe the resources we use (Section 4) and present experiments that examine the manifestation of antonymy in text (Sections 5 and 6). We then propose a new empirical approach to determine the degree of antonymy between two words (Section 7). We compiled a d"
D08-1103,J91-1001,0,0.842435,"rgeon) are also semantically related, but terms that are semantically related may not always be semantically similar (plane–sky, surgeon–scalpel). Antonymy is unique among these relations because it simultaneously conveys both a sense of closeness and of distance (Cruse, 1986). Antonymous concepts are semantically related but not semantically similar. 3 Related work Charles and Miller (1989) proposed that antonyms occur together in a sentence more often than chance. This is known as the co-occurrence hypothesis. They also showed that this was empirically true for four adjective antonym pairs. Justeson and Katz (1991) demonstrated the co-occurrence hypothesis for 35 prototypical antonym pairs (from an original set of 39 antonym pairs compiled by Deese (1965)) and also for an additional 22 frequent antonym pairs. All of these pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb, adjective, and adverb pairs (noun–noun, noun–verb, noun–adjective, verb–adverb and so on) pertaining to 18 concepts (for example, lose(v)–gain(n) and loss(n)–gain(n), where lose(v) and loss(n) pertain to the concept of “failing to have/maintain”). However, non-antonymous semantically related words su"
D08-1103,P98-2127,0,0.292578,"c1  c2  ∑w  T  c1   T  c2  I c1  w  I c2  w   (2) ∑w  T  c1  I c1  w  ∑w  T  c2  I c2  w   Here T c  is the set of all words w that have positive pointwise mutual information with the thesaurus category c (I c  w  0). We adopt this method for use in our approach to determine word-pair antonymy. 5 The co-occurrence hypothesis of antonyms Co-occurrence statistics The distributional hypothesis of closeness states that words that occur in similar contexts tend to be semantically close (Firth, 1957). Distributional measures of distance, such as those proposed by Lin (1998), quantify how similar the two sets of contexts of a target word pair are. Equation 1 is a modified form of Lin’s measure that ignores syntactic dependencies and hence it estimates semantic relatedness rather than semantic similarity: Lin w1  w2  ∑w  T  w1   T  w2  I w1  w  I w2  w   (1) ∑w  T  w1  I w1  w  ∑w  T  w2  I w2  w   Here w1 and w2 are the target words; I x  y  is the pointwise mutual information between x and y; and T x  is the set of all words y that have positive pointwise mutual information with the word x (I x  y  0). Mohammad and Hirst (20"
D08-1103,P02-1047,0,0.084301,"ascend–descend, shout–whisper). In its broadest Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automatic creation of a thesaurus it is necessary to distinguish nearsynonyms from word pairs that are semantically contrasting. Measures of distributional similarity fail to do so. Detecting antonymous words is not sufficient to solve most of these problems, but it remains a crucial, and largely unsolved, component"
D08-1103,P08-1118,0,0.078719,"Missing"
D08-1103,H05-1067,0,0.0321323,"ining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automatic creation of a thesaurus it is necessary to distinguish nearsynonyms from word pairs that are semantically contrasting. Measures of distributional similarity fail to do so. Detecting antonymous words is not sufficient to solve most of these problems, but it remains a crucial, and largely unsolved, component. 982 Proceedings of the 2008 Conference on Empirical Methods in Natural La"
D08-1103,W06-1605,1,0.670245,"proposed by Lin (1998), quantify how similar the two sets of contexts of a target word pair are. Equation 1 is a modified form of Lin’s measure that ignores syntactic dependencies and hence it estimates semantic relatedness rather than semantic similarity: Lin w1  w2  ∑w  T  w1   T  w2  I w1  w  I w2  w   (1) ∑w  T  w1  I w1  w  ∑w  T  w2  I w2  w   Here w1 and w2 are the target words; I x  y  is the pointwise mutual information between x and y; and T x  is the set of all words y that have positive pointwise mutual information with the word x (I x  y  0). Mohammad and Hirst (2006) showed that these distributional word-distance measures perform poorly when compared with WordNet-based concept-distance measures. They argued that this is because the word-distance measures clump together the contexts of the different senses of the target words. They proposed a way to obtain distributional distance between word senses, using any of the distributional measures such as cosine or that proposed by Lin, and showed that this approach performed markedly better than the traditional worddistance approach. They used thesaurus categories 985 As a first step towards formulating our appr"
D08-1103,W04-2607,1,0.680847,"ter-thanchance co-occurrence of antonyms in sentences is because together they convey contrast well, which is rhetorically useful, and not really the reason why they are considered antonyms in the first place. 2.2 Are semantic closeness and antonymy opposites? Two words (more precisely, two lexical units) are considered to be close in meaning if there is a lexical-semantic relation between them. Lexicalsemantic relations are of two kinds: classical and non-classical. Examples of classical relations include synonymy, hyponymy, troponymy, and meronymy. Non-classical relations, as pointed out by Morris and Hirst (2004), are much more common and include concepts pertaining to another concept (kind, chivalrous, formal pertaining to gentlemanly), and commonly co-occurring words (for example, problem–solution pairs such as homeless, shelter). Semantic distance (or closeness) in this broad sense is known as semantic relatedness. Two words are considered to be semantically similar if they are associated via the synonymy, hyponymy– hypernymy, or the troponymy relation. So terms that are semantically similar (plane–glider, doctor– surgeon) are also semantically related, but terms that are semantically related may n"
D08-1103,W02-1011,0,0.0141073,"Missing"
D08-1103,C02-1061,0,0.311749,"proposed in this paper is completely unsupervised. Harabagiu et al. (2006) detected antonyms for the purpose of identifying contradictions by using WordNet chains—synsets connected by the hypernymy–hyponymy links and exactly one antonymy link. Lucerto et al. (2002) proposed detecting antonym pairs using the number of words 984 between two words in text and also cue words such as but, from, and and. Unfortunately, they evaluated their method on only 18 word pairs. Neither of these methods determines the degree of antonymy between words and they have not been shown to have substantial coverage. Schwab et al. (2002) create “antonymous vector” for a target word. The closer this vector is to the context vectors of the other target word, the more antonymous the two target words are. However, the antonymous vectors are manually created. Further, the approach is not evaluated beyond a handful of word pairs. Work in sentiment detection and opinion mining aims at determining the polarity of words. For example, Pang, Lee and Vaithyanathan (2002) detect that adjectives such as dazzling, brilliant, and gripping cast their qualifying nouns positively whereas adjectives such as bad, cliched, and boring portray the n"
D08-1103,C08-1114,0,0.0555083,"such as hypernyms, holonyms, meronyms, and nearsynonyms also tend to occur together more often than chance. Thus, separating antonyms from them has proven to be difficult. Lin et al. (2003) used patterns such as “from X to Y ” and “either X or Y ” to separate antonym word pairs from distributionally similar pairs. They evaluated their method on 80 pairs of antonyms and 80 pairs of synonyms taken from the Webster’s Collegiate Thesaurus (Kay, 1988). In this paper, we propose a method to determine the degree of antonymy between any word pair and not just those that are distributionally similar. Turney (2008) proposed a uniform method to solve word analogy problems that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. However, the Turney method is supervised whereas the method proposed in this paper is completely unsupervised. Harabagiu et al. (2006) detected antonyms for the purpose of identifying contradictions by using WordNet chains—synsets connected by the hypernymy–hyponymy links and exactly one antonymy link. Lucerto et al. (2002) proposed detecting antonym pairs using the number of words 984 between two words in text and also cue w"
D08-1103,P08-1008,0,0.0734216,"from each other in small and large respects. In its strictest sense, antonymy applies to gradable adjectives, such as hot–cold and tall–short, where the two words represent the two ends of a semantic dimension. In a broader sense, it includes other adjectives, nouns, and verbs as well (life–death, ascend–descend, shout–whisper). In its broadest Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automati"
D08-1103,C98-2122,0,\N,Missing
D11-1093,C96-1005,0,0.140517,". As for the relation between density and similarity, the intuition is that if the overall semantic mass for a given node is constant (Jiang and Conrath, 1997), then the more neighboring nodes there are in a locally connected subnetwork, the closer its members are to each other. For example, animal, person, and plant are more strongly connected with life form than aerobe and plankton because the first three words all have high density in their local network structures (Richardson and Smeaton, 1995). Note that the notion of density here is not to be confused with the conceptual density used by Agirre and Rigau (1996), which is essentially a semantic similarity measure by itself. In general, both observations on depth and density conform to intuition and are supported qualitatively by several existing studies. The main objective of this study is to empirically examine the validity of this assumption. 2.2 Semantic Similarity Measures Using Depth and/or Density One of the first examples of using depth and density in WordNet-based similarity measures is that of Sussna (1993). The weight on an edge between two nodes c1 and c2 with relation r in WordNet is given as: w(c1 , c2 ) = w(c1 →r c2 ) + w(c2 →r c1 ) 2d"
D11-1093,P85-1037,0,0.319041,"Missing"
D11-1093,P98-2127,0,0.011135,"improvement in degree of correlation with similarity. When used in WordNet-based semantic similarity measures, the new definitions consistently improve performance on a task of correlating with human judgment. 1 Introduction Semantic similarity measures are widely used in natural language processing for measuring distance between meanings of words. There are currently two mainstream approaches to deriving such measures, i.e., distributional and lexical resource-based approaches. The former usually explores the cooccurrence patterns of words in large collections of texts such as text corpora (Lin, 1998) or the Web (Turney, 2001). The latter takes advantage of mostly handcrafted information, such as dictionaries (Chodorow et al., 1985; Kozima and Ito, 1997) or thesauri (Jarmasz and Szpakowicz, 2003). 1003 Graeme Hirst Department of Computer Science University of Toronto gh@cs.toronto.edu Another important resource in the latter stream is semantic taxonomies such as WordNet (Fellbaum, 1998). Despite their high cost of compilation and limited availability across languages, semantic taxonomies have been widely used in similarity measures, and one of the main reasons behind this is that the often"
D11-1093,P94-1019,0,0.438413,"xonomies have been widely used in similarity measures, and one of the main reasons behind this is that the often complex notion of lexical semantic similarity can be approximated with ease by the distance between words (represented as nodes) in their hierarchical structures, and this approximation appeals much to our intuition. Even methods as simple as “hop counts” between nodes (e.g., that of Rada et al. 1989 on the English WordNet) can take us a long way. Meanwhile, taxonomy-based methods have been constantly refined by incorporating various structural features such as depth (Sussna, 1993; Wu and Palmer, 1994), density (Sussna, 1993), type of connection (Hirst and St-Onge, 1998; Sussna, 1993), word class (sense) frequency estimates (Resnik, 1999), or a combination these features (Jiang and Conrath, 1997). Most of these algorithms are fairly self-contained and easy to implement, with off-theshelf toolkits such as that of Pedersen et al. (2004). With the existing literature focusing on carefully weighting these features to construct a better semantic similarity measure, however, the rationale for adopting these features in calculating semantic similarity remains largely intuitive. To the best of our"
D11-1093,O97-1002,0,\N,Missing
D11-1093,N04-3012,0,\N,Missing
D11-1093,C98-2122,0,\N,Missing
D12-1115,I11-1012,0,0.0514323,"ker, 2007). But all these approaches focus only on the anaphors with nominal antecedents. By contrast, the area of abstract object anaphora remains relatively unexplored mainly because the standard anaphora resolution features such as agreement and apposition cannot be applied to abstract anaphora resolution. Asher (1993) built a theoretical framework to resolve abstract anaphora. He divided discourse abstract anaphora into three broad categories: event anaphora, proposition anaphora, and fact anaphora, and discussed how abstract entities can be resolved using discourse representation theory. Chen et al. (2011) focused on a subset of event anaphora and resolved event coreference chains in terms of the representative verbs of the events from the OntoNotes corpus. Our task differs from their work as follows. Chen et al. mainly focus on events and actions and use verbs as a proxy for the non-nominal antecedents. But this-issue antecedents cannot usually be represented by a verb. Our work is not restricted to a particular syntactic type of the antecedent; rather we provide the flexibility of marking arbitrary spans of text as antecedents. There are also some prominent approaches to abstract anaphora res"
D12-1115,D08-1069,0,0.105836,"d of the candidate is a preposition ISPPUNCT 1 iff the preceding word of the candidate is a punctuation ISFPUNCT 1 iff the following word of the candidate is a punctuation LENGTH (L) LEN length of the candidate in words Table 2: Feature sets for this-issue resolution. All features are extracted automatically. 1260 overlap features, and context features. 4.3 Candidate Ranking Model Given an anaphor ai and a set of candidate antecedents C = {C1 ,C2 , ...,Ck }, the problem of anaphora resolution is to choose the best candidate antecedent for ai . We follow the candidate-ranking model proposed by Denis and Baldridge (2008). The advantage of the candidate-ranking model over the mention-pair model is that it overcomes the strong independence assumption made in mentionpair models and evaluates how good a candidate is relative to all other candidates. We train our model as follows. If the anaphor is a this-issue anaphor, the set C is extracted using the candidate extraction algorithm from Section 4.1. Then a corresponding set of feature vectors, C f = {C f 1 ,C f 2 , ...,C f k }, is created using the features in Table 2. The training instances are created as described by Soon et al. (2001). Note that the instance c"
D12-1115,O04-1011,0,0.0347328,"esolution (e.g., resolving multiple ambiguous mentions of a single entity representing a person, a location, or an organization) mainly for two reasons. First, nominal anaphora is the most frequently occurring anaphora in most domains, and second, there is a substantial amount of annotated data available for this kind of anaphora. Besides pronominal anaphora, some work has been done on complement anaphora (Modjeska, 2003) (e.g., British and other European steelmakers). There is also some research on resolving sortal anaphora in the medical domain using domain knowledge (Casta˜no et al., 2002; Lin and Liang, 2004; Torii and Vijay-Shanker, 2007). But all these approaches focus only on the anaphors with nominal antecedents. By contrast, the area of abstract object anaphora remains relatively unexplored mainly because the standard anaphora resolution features such as agreement and apposition cannot be applied to abstract anaphora resolution. Asher (1993) built a theoretical framework to resolve abstract anaphora. He divided discourse abstract anaphora into three broad categories: event anaphora, proposition anaphora, and fact anaphora, and discussed how abstract entities can be resolved using discourse r"
D12-1115,W04-1013,0,0.0026451,"propose two metrics for abstract anaphora evaluation. The simplest metric is the percentage of antecedents on which the system and the annotated gold data agree. We denote this metric as EXACTM (Exact Match) and compute it as the ratio of number of correctly identified antecedents to the total number of marked antecedents. This metric is a good indicator of a system’s performance; however, it is a rather strict evaluation because, as we noted in section 1, issues generally have no precise boundaries in the text. So we propose another metric called RLL, which is similar to the ROUGE-L metric (Lin, 2004) used for the evaluation of automatic summarization. Let the marked antecedents of the gold corpus for k anaphor instances be G = hg1 , g2 , ..., gk i and the system-annotated antecedents be A = ha1 , a2 , ..., ak i. Let the number of words in G and A be m and n respectively. Let LCS(gi , ai ) be the the number of words in the longest common subsequence of gi and ai . Then the precision (PRLL ) and recall (RRLL ) over the whole data set are computed as shown in equations (2) and (3). PRLL is the total number of word overlaps between the gold and system-annotated antecedents normalized by the n"
D12-1115,P89-1007,0,0.293649,"ning the marked clause could also be thought to be the correct antecedent. Third, the actual referents are not always the precise textual antecedents. The actual referent in (2), the issue to be clarified, is whether oral carvedilol is more effective than oral metoprolol in the prevention of AF after on-pump CABG or not, a variant of the antecedent text. Generally, abstract anaphora, as distinguished from nominal anaphora, is signalled in English by pronouns this, that, and it (M¨uller, 2008). But in abstract anaphora, English prefers demonstratives to personal pronouns and definite articles (Passonneau, 1989; Navarretta, 2011).1 Demonstra1 This is not to say that personal pronouns and definite articles do not occur in abstract anaphora, but they are not common. 1255 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1255–1265, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics tives can be used in isolation (That in (1)) or with nouns (e.g., this issue in (2)). The latter follows the pattern demonstrative {modifier}* noun. The demonstrative acts as a determiner and the n"
D12-1115,poesio-artstein-2008-anaphoric,0,0.283982,"the antecedent; rather we provide the flexibility of marking arbitrary spans of text as antecedents. There are also some prominent approaches to abstract anaphora resolution in the spoken dialogue domain (Eckert and Strube, 2000; Byron, 2004; M¨uller, 2008). These approaches go beyond nominal antecedents; however, they are restricted to spoken dialogues in specific domains and need serious adaptation if one wants to apply them to arbitrary text. In addition to research on resolution, there is also some work on effective annotation of abstract anaphora (Strube and M¨uller, 2003; Botley, 2006; Poesio and Artstein, 2008; Dipper and Zinsmeister, 2011). However, to the best of our knowledge, there is currently no English corpus annotated for issue anaphora antecedents. 3 Data and Annotation To create an initial annotated dataset, we collected 188 this {modifier}* issue instances along with the surrounding context from Medline abstracts.3 Five instances were discarded as they had an unrelated (publication related) sense. Among the remaining 183 instances, 132 instances were independently annotated by two annotators, a domain expert and a non-expert, and the remaining 51 instances were annotated only by the doma"
D12-1115,J01-4004,0,0.224489,"model proposed by Denis and Baldridge (2008). The advantage of the candidate-ranking model over the mention-pair model is that it overcomes the strong independence assumption made in mentionpair models and evaluates how good a candidate is relative to all other candidates. We train our model as follows. If the anaphor is a this-issue anaphor, the set C is extracted using the candidate extraction algorithm from Section 4.1. Then a corresponding set of feature vectors, C f = {C f 1 ,C f 2 , ...,C f k }, is created using the features in Table 2. The training instances are created as described by Soon et al. (2001). Note that the instance creation is simpler than for general coreference resolution because of the absence of anaphoric chains in our data. For every anaphor ai and eligible candidates C f = {C f 1 ,C f 2 , ...,C f k }, we create training examples (ai ,C f i , label), ∀C f i ∈ C f . The label is 1 if Ci is the true antecedent of the anaphor ai , otherwise the label is −1. The examples with label 1 get the rank of 1, while other examples get the rank of 2. We use SVMrank (Joachims, 2002) for training the candidate-ranking model. During testing, the trained model is used to rank the candidates"
D12-1115,P03-1022,0,0.512056,"Missing"
D12-1115,P08-4003,0,\N,Missing
D12-1115,versley-etal-2008-bart-modular,0,\N,Missing
D12-1115,P02-1011,0,\N,Missing
D13-1030,D08-1069,0,0.0472133,"the sentence) for each shell noun. Then for all antecedent unigrams for a particular shell noun, we computed term goodness in terms of information gain (Yang and Pedersen, 1997) and considered the first 50 highly ranked unigrams as the lexical features for that noun. Note that, in contrast with the other features, these lexical features are tailored for each shell noun and are extracted a priori. 5.3.3 Candidate ranking models Now that we have the set of candidate antecedents and a set of features, we are ready to train CSN antecedent models. We follow the candidate-ranking models proposed by Denis and Baldridge (2008) because they allow us to evaluate how good an antecedent candidate is relative to all other candidates. For every shell noun, we gather automatically extracted antecedent data given by the extractor for all instances of that shell noun. Then for each instance in this data, we extract the set C as explained in Section 5.3.1. For each candidate Ci ∈ C, we extract a feature vector to create a corresponding set of feature vectors, C f = {C f 1 ,C f 2 , ...,C f k }. For every CSN ai and a set of feature vectors corresponding to its eligible candidates C f = {C f 1 ,C f 2 , ...,C f k }, we create t"
D13-1030,C92-2082,0,0.0700054,"Missing"
D13-1030,D12-1115,1,0.878603,"hora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1 Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. 301 There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxies for non-nominal antecedents. Recently, Kolhatkar and Hirst (2012) presented a machine-learning based resolution system for this issue anaphora, identifying full syntactic phrases as antecedents. Although they achieved promising results, their approach was limited in two respects. First, it focused on only one type of shell noun anaphora (issue anaphora). Second, their training data was restricted to MEDLINE abstracts in which this issue is used in a rather systematic way. Furthermore, their work is based on manually labelled ASN antecedents, whereas we use automatically identified CSN antecedents, which we interpret as explicitly expressed antecedents in co"
D13-1030,W13-2314,1,0.544069,"ce contains three paragraphs from the corresponding NYT article: the paragraph containing the ASN and two preceding paragraphs as context. After automatically removing duplicates and ASNs with a nonabstract sense (e.g., this issue with a publicationrelated sense), we were left with 2,323 instances. 6.2 we consider n preceding sentences, the sentence containing the anaphor, and one following sentence8 as sources for antecedents, then the average number of antecedent candidates will be 49.5 × (n + 2). This is large compared to the search space of ordinary nominal anaphora. In our previous work (Kolhatkar et al., 2013), we have developed methods that identify the sentence containing the antecedent of the ASN before identifying the precise antecedent. In brief, given a set of a fixed number of sentences around the sentence containing an ASN, these methods reliably identify the sentence containing the antecedent. In this paper, we treat these methods as a black box. Given the sentence containing the antecedent, we extract all syntactic constituents given by the Stanford parser from that sentence as potential antecedent candidates as for the training phase. In the training phase, the antecedent is always conta"
D13-1030,P02-1047,0,0.0472658,"st, it focused on only one type of shell noun anaphora (issue anaphora). Second, their training data was restricted to MEDLINE abstracts in which this issue is used in a rather systematic way. Furthermore, their work is based on manually labelled ASN antecedents, whereas we use automatically identified CSN antecedents, which we interpret as explicitly expressed antecedents in comparison to the more implicitly expressed ASN antecedents. Using explicitly expressed structure in the text to identify implicit structure is not new. The same idea has been applied before in computational linguistics. Marcu and Echihabi (2002) identified implicit discourse relations using explicit ones. Markert and Nissim (2005) used Hearst’s (1992) explicit patterns to learn lexical semantic relations for NPcoreference and other-anaphora resolution from the web. Although our work focuses on a different topic, the methodology is in the same vein. 3 Hypothesis of this work The hypothesis of this work is that CSN antecedents and ASN antecedents share some linguistic properties and hence linguistic knowledge encoded in CSN antecedents will help in interpreting ASNs. Accordingly, we examine which features present in CSN antecedents are"
D13-1030,J93-2004,0,0.0438709,". For the N-wh pattern, we exclude certain wh words for certain nouns. For example, we exclude the wh word which for question as the Penn Treebank tagset7 does not distinguish between which as a relative pronoun and as a question. We are interested in the latter but not the former. Other discarded wh words include which and when for fact; all wh words except when and why for reason, all wh words except how and whether for issue; which, whom, when, and why for decision; which and when for question; and all wh words for possibility. 7 The Stanford tagger we employ uses the Penn Treebank tagset (Marcus et al., 1993). 304 To address the second goal of allowing a wide variety of antecedent examples, we try to include as many patterns as possible for each shell noun, as shown in Table 3. For instance, the patterns question-to and question-be-to will have infinitive clauses as antecedents (marked as VP or S+VP by the parser), whereas for the examples following patterns question-wh and question-be-wh the antecedent will be in the wh clauses (marked as SBAR). For the pattern question-that, the antecedent will be in the predicate (similar to example (3)), which can be a prepositional phrase, a noun phrase or a"
D13-1030,P89-1007,0,0.867312,"issue is an important problem which requires a solution. In this work, we propose an approach to interpret ASNs that exploits unlabelled but easy-to-interpret CSN examples to extract characteristic features associated with the antecedent of different shell concepts. We evaluate our approach using crowdsourcing. Our results show that these unlabelled CSN examples provide useful linguistic properties that help in interpreting ASNs. 2 Related work The resolution of anaphors to non-nominal antecedents has been well analyzed taking discourse structure and semantic types into account (Webber, 1991; Passonneau, 1989; Asher, 1993). Most work in machine anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1 Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. 301 There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxie"
D13-1030,J01-4004,0,0.250944,"extract all syntactic constituents of this sentence, given by the Stanford parser. All the syntactic constituents, except the true antecedent, are considered as negative examples. With this candidate extraction method, we end up with many more negative examples than positive examples, but that is exactly what we expect with ASN antecedent candidates, i.e., the test data on which we will be applying our models. 5.3.2 Features Although our problem is similar to anaphora resolution, we cannot make use of the usual anaphora or coreference resolution features such as agreement or string matching (Soon et al., 2001) because of the nature of ASN and CSN antecedents. We came up with a set of features based on the properties that were common in both ASN and CSN antecedents, according to our judgement. Syntactic type of the candidate (S) We observed that each shell noun prefers specific CSN patterns and each pattern involves a particular syntactic type. For instance, decision prefers the pattern N-to and consequently realizes as its antecedents more verb phrases than, for example, noun phrases. We employ two versions of syntactic type: fine-grained syntactic type given by the Stanford parser (e.g., NP-TMP, R"
D13-1030,P03-1022,0,0.440646,"Missing"
D13-1030,P08-4003,0,\N,Missing
D13-1030,versley-etal-2008-bart-modular,0,\N,Missing
D13-1030,P02-1011,0,\N,Missing
D14-1056,P98-1013,0,0.278471,"Missing"
D14-1056,W13-2314,1,0.804629,"l anaphora resolution (see the survey by Poesio et al. (2011)), such as gender and number agreement, are not applicable in resolving shell nouns. That said, there is a line of literature on annotating and resolving personal and demonstrative pronouns, which typically refer to similar kinds of non-nominal abstract entities (Passonneau, 1989; Eckert and Strube, 2000; Byron, 2003; Müller, 2008; Hedberg et al., 2007; Poesio and Artstein, 2008; Navarretta, 2011, inter alia). Also, there have been attempts at annotating the shell content of anaphoric occurrences of shell nouns (e.g., Botley (2006), Kolhatkar et al. (2013a)). However, none of these approaches attempt to annotate and resolve cataphoric examples such (1) and (2). discourse structure. Accordingly, identifying shell content can be helpful in summarization, information retrieval, and ESL learning (Flowerdew, 2003; Hinkel, 2004). Despite their importance in discourse, understanding of shell nouns from a computational linguistics perspective is only in the preliminary stage. Recently, we proposed an approach to annotate and resolve anaphoric cases of six typical shell nouns: fact, reason, issue, decision, question, and possibility (Kolhatkar et al.,"
D14-1056,D13-1030,1,0.642144,"l anaphora resolution (see the survey by Poesio et al. (2011)), such as gender and number agreement, are not applicable in resolving shell nouns. That said, there is a line of literature on annotating and resolving personal and demonstrative pronouns, which typically refer to similar kinds of non-nominal abstract entities (Passonneau, 1989; Eckert and Strube, 2000; Byron, 2003; Müller, 2008; Hedberg et al., 2007; Poesio and Artstein, 2008; Navarretta, 2011, inter alia). Also, there have been attempts at annotating the shell content of anaphoric occurrences of shell nouns (e.g., Botley (2006), Kolhatkar et al. (2013a)). However, none of these approaches attempt to annotate and resolve cataphoric examples such (1) and (2). discourse structure. Accordingly, identifying shell content can be helpful in summarization, information retrieval, and ESL learning (Flowerdew, 2003; Hinkel, 2004). Despite their importance in discourse, understanding of shell nouns from a computational linguistics perspective is only in the preliminary stage. Recently, we proposed an approach to annotate and resolve anaphoric cases of six typical shell nouns: fact, reason, issue, decision, question, and possibility (Kolhatkar et al.,"
D14-1056,W04-2705,0,0.0435225,"as shown in example (4).4 Similarly, decision takes an agent making the decision and the shell content is represented as an action or a proposition, as shown in (5).5 Related work Shell-nounhood is a well-established concept in linguistics (Vendler, 1968; Ivanic, 1991; Asher, 1993; Francis, 1994; Schmid, 2000, inter alia). However, understanding of shell nouns from a computational linguistics perspective is only in the preliminary stage. Shell nouns take a number of semantic arguments. In this respect, they are similar to the general class of argument-taking nominals as given in the NomBank (Meyers et al., 2004). Similarly, there is a small body of literature that addresses nominal semantic role labelling (Gerber et al., 2009) and nominal subcategorization frames (Preiss et al., 2007). That said, the distinguishing property of shell nouns is that one of their seman(4) One reason [that 60 percent of New York City public-school children read below grade level]effect is [that many elementary schools don’t have libraries]cause . 4 Observe that the postnominal that clause in (4) is not a relative clause, and still it is not the shell content because it is not the cause argument of the shell noun reason. 5"
D14-1056,P89-1007,0,0.260782,"e in computational linguistics does not provide any method that is able to identify the shell content. The focus of our work is to rectify this. Shell content represents complex and abstract objects. So traditional linguistic and psycholinguistic principles used in pronominal anaphora resolution (see the survey by Poesio et al. (2011)), such as gender and number agreement, are not applicable in resolving shell nouns. That said, there is a line of literature on annotating and resolving personal and demonstrative pronouns, which typically refer to similar kinds of non-nominal abstract entities (Passonneau, 1989; Eckert and Strube, 2000; Byron, 2003; Müller, 2008; Hedberg et al., 2007; Poesio and Artstein, 2008; Navarretta, 2011, inter alia). Also, there have been attempts at annotating the shell content of anaphoric occurrences of shell nouns (e.g., Botley (2006), Kolhatkar et al. (2013a)). However, none of these approaches attempt to annotate and resolve cataphoric examples such (1) and (2). discourse structure. Accordingly, identifying shell content can be helpful in summarization, information retrieval, and ESL learning (Flowerdew, 2003; Hinkel, 2004). Despite their importance in discourse, under"
D14-1056,N09-1017,0,0.016584,"d as an action or a proposition, as shown in (5).5 Related work Shell-nounhood is a well-established concept in linguistics (Vendler, 1968; Ivanic, 1991; Asher, 1993; Francis, 1994; Schmid, 2000, inter alia). However, understanding of shell nouns from a computational linguistics perspective is only in the preliminary stage. Shell nouns take a number of semantic arguments. In this respect, they are similar to the general class of argument-taking nominals as given in the NomBank (Meyers et al., 2004). Similarly, there is a small body of literature that addresses nominal semantic role labelling (Gerber et al., 2009) and nominal subcategorization frames (Preiss et al., 2007). That said, the distinguishing property of shell nouns is that one of their seman(4) One reason [that 60 percent of New York City public-school children read below grade level]effect is [that many elementary schools don’t have libraries]cause . 4 Observe that the postnominal that clause in (4) is not a relative clause, and still it is not the shell content because it is not the cause argument of the shell noun reason. 5 Observe that this aspect of shell nouns of taking different numbers and kinds of complement clauses is similar to ve"
D14-1056,poesio-artstein-2008-anaphoric,0,0.363607,"l content. The focus of our work is to rectify this. Shell content represents complex and abstract objects. So traditional linguistic and psycholinguistic principles used in pronominal anaphora resolution (see the survey by Poesio et al. (2011)), such as gender and number agreement, are not applicable in resolving shell nouns. That said, there is a line of literature on annotating and resolving personal and demonstrative pronouns, which typically refer to similar kinds of non-nominal abstract entities (Passonneau, 1989; Eckert and Strube, 2000; Byron, 2003; Müller, 2008; Hedberg et al., 2007; Poesio and Artstein, 2008; Navarretta, 2011, inter alia). Also, there have been attempts at annotating the shell content of anaphoric occurrences of shell nouns (e.g., Botley (2006), Kolhatkar et al. (2013a)). However, none of these approaches attempt to annotate and resolve cataphoric examples such (1) and (2). discourse structure. Accordingly, identifying shell content can be helpful in summarization, information retrieval, and ESL learning (Flowerdew, 2003; Hinkel, 2004). Despite their importance in discourse, understanding of shell nouns from a computational linguistics perspective is only in the preliminary stage"
D14-1056,P07-1115,0,0.0217796,"work Shell-nounhood is a well-established concept in linguistics (Vendler, 1968; Ivanic, 1991; Asher, 1993; Francis, 1994; Schmid, 2000, inter alia). However, understanding of shell nouns from a computational linguistics perspective is only in the preliminary stage. Shell nouns take a number of semantic arguments. In this respect, they are similar to the general class of argument-taking nominals as given in the NomBank (Meyers et al., 2004). Similarly, there is a small body of literature that addresses nominal semantic role labelling (Gerber et al., 2009) and nominal subcategorization frames (Preiss et al., 2007). That said, the distinguishing property of shell nouns is that one of their seman(4) One reason [that 60 percent of New York City public-school children read below grade level]effect is [that many elementary schools don’t have libraries]cause . 4 Observe that the postnominal that clause in (4) is not a relative clause, and still it is not the shell content because it is not the cause argument of the shell noun reason. 5 Observe that this aspect of shell nouns of taking different numbers and kinds of complement clauses is similar to verbs having different subcategorization frames. 500 (5) I ap"
D14-1056,P02-1029,0,0.0499708,"s issue of Sports Illustrated). Our algorithm is able to deal with only a restricted number of shell noun usage constructions, but the shell content can be expressed in a variety of other constructions. A robust machine learning approach that incorporates context and deeper semantics of the sentence, along with Schmid’s cues, could mitigate this limitation. This work opens a number of new research directions. Our next planned task is clustering different shell nouns based on the kind of complements they take in different usages similar to verb clustering (Merlo and Stevenson, 2000; Schulte im Walde and Brew, 2002). (13) I was trying to address the problem of unreliable testimony by experts in capital cases. Similarly, the Plan family does not allow the Nof pattern. This cue works well for the shell noun decision from the same family because often the postnominal of clause is the agent for this shell noun and not the shell content. However, it hurts the performance of the shell noun policy, as Nof is a common pattern for this shell noun (e.g., . . . officials in Rwanda have established a policy of refusing to protect refugees. . . ). Other failures of the algorithm are due to parsing errors and lack of"
D14-1056,W09-3309,0,\N,Missing
D14-1056,J03-4003,0,\N,Missing
D14-1056,C98-1013,0,\N,Missing
D14-1056,J08-4004,0,\N,Missing
D14-1056,I11-1012,0,\N,Missing
D14-1056,P09-1068,0,\N,Missing
D14-1056,P02-1011,0,\N,Missing
D14-1056,J01-3003,0,\N,Missing
D19-1472,W19-2111,0,0.0289279,"modern day. We will describe systematic evaluations and applications of our framework that extend beyond these anecdotal cases of moral sentiment change. An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse (Garten et al., 2016; Johnson and Goldwasser, 2018; Lin et al., 2018), the relation between moralization in social media and violent protests (Mooijman et al., 2018), and bias toward refugees in talk radio shows (Gillani and Levy, 2019) have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic (Bloom, 2010). The general text-based framework that we propose consists of a parameter-free approach that facilitates the prediction of public moral sentiment toward individual concepts, automated retrieval of morally changing concepts, and broad-scale psycholinguistic analyses of historical rates of moral sentiment change. We provide a descr"
D19-1472,P16-1141,0,0.298069,"controls the sensitivity of the model to distance in embedding space. Table 1 specifies the formulation of each model. Note that we adopt a parsimonious design principle in our modelling: both Centroid and Na¨ıve Bayes are parameter-free models, kNN only depends on the choice of k, and KDE uses a single bandwidth parameter h. 4 Historical corpus data To apply our models diachronically, we require a word embedding space that captures the meanings of words at different points in time and reflects changes pertaining to a particular word as diachronic shifts in a common embedding space. Following Hamilton et al. (2016), we combine skip-gram word embeddings (Mikolov et al., 2013) trained on longitudinal corpora of English with rotational alignments of embedding spaces to obtain diachronic word embeddings that are aligned through time. We divide historical time into decade-long bins, and use two sets of embeddings provided by Hamilton et al. (2016), each trained on a different historical corpus of English: • Google N-grams (Lin et al., 2012): a corpus of 8.5 × 1011 tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999. • COHA (Davies, 2010): a smaller corpus of"
D19-1472,P18-1067,0,0.219687,"s now perceived as morally positive, as a mechanism for fairness; gay, which came to denote homosexuality only in the 1930s (Kay et al., 2019), is inferred to be morally irrelevant until the modern day. We will describe systematic evaluations and applications of our framework that extend beyond these anecdotal cases of moral sentiment change. An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse (Garten et al., 2016; Johnson and Goldwasser, 2018; Lin et al., 2018), the relation between moralization in social media and violent protests (Mooijman et al., 2018), and bias toward refugees in talk radio shows (Gillani and Levy, 2019) have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic (Bloom, 2010). The general text-based framework that we propose consists of a parameter-free approach that facilitates the prediction of public moral sentiment"
D19-1472,P12-3029,0,0.0170963,"es the meanings of words at different points in time and reflects changes pertaining to a particular word as diachronic shifts in a common embedding space. Following Hamilton et al. (2016), we combine skip-gram word embeddings (Mikolov et al., 2013) trained on longitudinal corpora of English with rotational alignments of embedding spaces to obtain diachronic word embeddings that are aligned through time. We divide historical time into decade-long bins, and use two sets of embeddings provided by Hamilton et al. (2016), each trained on a different historical corpus of English: • Google N-grams (Lin et al., 2012): a corpus of 8.5 × 1011 tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999. • COHA (Davies, 2010): a smaller corpus of 4.1 × 108 tokens from works selected so as to be genre-balanced and representative of American English in the period 1810–2009. 5 Model evaluations We evaluated our models in two ways: classification of moral seed words on all three tiers (moral relevance, polarity, and fine-grained categories), and correlation of model predictions with human judgments. 5.1 Moral sentiment inference of seed words In this evaluation, we assess"
E06-1016,J98-1006,0,0.337939,"Missing"
E06-1016,P98-2127,0,0.896817,"unsupervised algorithm that discriminates instances into different usages can use word sense dominance to assign senses to the different clusters generated. Sense dominance may be determined by simple counting in sense-tagged data. However, dominance varies with domain, and existing sensetagged data is largely insufficient. McCarthy et al. (2004) automatically determine domainspecific predominant senses of words, where the domain may be specified in the form of an untagged target text or simply by name (for example, financial domain). The system (Figure 1) automatically generates a thesaurus (Lin, 1998) using a measure of distributional similarity and an untagged corpus. The target text is used for this purpose, provided it is large enough to learn a thesaurus from. Otherwise a large corpus with sense distribution similar to the target text (text pertaining to the specified domain) must be used. The thesaurus has an entry for each word type, which lists a limited number of words (neighbors) that are distributionally most similar to it. Since Lin’s distributional measure overestimates the distributional similarity of more-frequent word pairs (Mohammad and Hirst, Submitted), the neighbors of a"
E06-1016,P04-1036,0,0.633615,"he sense to the total occurrences of the target word. The sense with the highest dominance in the target text is called the predominant sense of the target word. Determination of word sense dominance has many uses. An unsupervised system will benefit by backing off to the predominant sense in case of insufficient evidence. The dominance values may be used as prior probabilities for the different senses, obviating the need for labeled training data in a sense disambiguation task. Natural language systems can choose to ignore infrequent senses of words or consider only the most dominant senses (McCarthy et al., 2004). An unsupervised algorithm that discriminates instances into different usages can use word sense dominance to assign senses to the different clusters generated. Sense dominance may be determined by simple counting in sense-tagged data. However, dominance varies with domain, and existing sensetagged data is largely insufficient. McCarthy et al. (2004) automatically determine domainspecific predominant senses of words, where the domain may be specified in the form of an untagged target text or simply by name (for example, financial domain). The system (Figure 1) automatically generates a thesau"
E06-1016,P05-1016,0,0.0272457,"Missing"
E06-1016,C92-2070,0,0.297198,"are better indicators of a word’s characteristics than second-order co-occurrences (distributionally similar words). 2 Thesauri Published thesauri, such as Roget’s and Macquarie, divide the English vocabulary into around a thousand categories. Each category has a list of semantically related words, which we will call category terms or c-terms for short. Words with multiple meanings may be listed in more than one category. For every word type in the vocabulary of the thesaurus, the index lists the categories that include it as a c-term. Categories roughly correspond to coarse senses of a word (Yarowsky, 1992), and the two terms will be used interchangeably. For example, in the Macquarie Thesaurus, bark is a c-term in the categories ‘animal noises’ and ‘membrane’. These categories represent the coarse senses of bark. Note that published thesauri are structurally quite different from the “thesaurus” automatically generated by Lin (1998), wherein a word has exactly one entry, and its neighbors may be semantically related to it in any of its senses. All future mentions of thesaurus will refer to a published thesaurus. While other sense inventories such as WordNet exist, use of a published thesaurus ha"
E06-1016,C98-2122,0,\N,Missing
E06-1016,kilgarriff-yallop-2000-whats,0,\N,Missing
E12-1032,N04-4001,0,0.0237043,"Missing"
E12-1032,P05-1018,0,0.322461,"Missing"
E12-1032,J08-1001,0,0.0608129,"among the permutations themselves. We show that this can be done by assigning a suitable objective score for each permutation indicating its dissimilarity from the original one. We call this a multiple-rank model since we train our model on a multiple-rank basis, rather than taking the original pairwise ranking approach. This extension can also be easily combined with other extensions by incorporating their enriched feature sets. We show that our multiple-rank model outperforms B&L’s basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). In sentence ordering, we experiment with different approaches to assigning dissimilarity scores and ranks (Section 5.1.1). We also experiment with different entity extraction approaches 315 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315–324, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics 1 2 3 Manila Miles Island Quake Baco − S X − − X X O X X − X − − X Table 1: A fragment of an entity grid for five entities across three sentences. (Section 5.1.2) and different distributions of perm"
E12-1032,P06-1049,0,0.0168072,"judgments of coherence, since additional manual annotation is certainly undesirable. (2) It depends on the particular sentence ordering in a permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences. In our work we use three different metrics: Kendall’s τ distance, average continuity, and edit distance. Kendall’s τ distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1 . It measures the disagreement between two orderings σ and π in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s τ distance is defined as τ= 2m , N(N − 1) where m is the number of sentence inversions necessary to convert σ to π. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1 Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in a differe"
E12-1032,P10-1020,0,0.0915453,"urther clustered entities by semantic relatedness as computed by the WikiRelated! API (Strube and Ponzetto, 2006). Though the improvement was not significant, interestingly, a short subsection in 316 their paper described their approach to extending pairwise rankings to longer rankings, by supplying the learner with rankings of all renderings as computed by Kendall’s τ, which is one of our extensions considered in this paper. Although Filippova and Strube simply discarded this idea because it hurt accuracies when tested on their data, we found it a promising direction for further exploration. Cheung and Penn (2010) adapted the standard entity-based coherence model to the same German corpus, but replaced the original linguistic dimension used by Barzilay and Lapata (2008) — grammatical role — with topological field information, and showed that for German text, such a modification improves accuracy. For English text, two extensions have been proposed recently. Elsner and Charniak (2011) augmented the original features used in the standard entity-based coherence model with a large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document di"
E12-1032,de-marneffe-etal-2006-generating,0,0.0289068,"Missing"
E12-1032,P11-2022,0,0.430713,"which is one of our extensions considered in this paper. Although Filippova and Strube simply discarded this idea because it hurt accuracies when tested on their data, we found it a promising direction for further exploration. Cheung and Penn (2010) adapted the standard entity-based coherence model to the same German corpus, but replaced the original linguistic dimension used by Barzilay and Lapata (2008) — grammatical role — with topological field information, and showed that for German text, such a modification improves accuracy. For English text, two extensions have been proposed recently. Elsner and Charniak (2011) augmented the original features used in the standard entity-based coherence model with a large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document discrimination (another name for sentence ordering), and sentence insertion. Lin et al. (2011) adapted the entity grid representation in the standard model into a discourse role matrix, where additional discourse information about the document was encoded. Their extended model significantly improved ranking accuracies on the same two datasets used by Barzilay and Lapata (2008)"
E12-1032,W07-2321,0,0.279679,"ather than training and evaluating on a set of synthetic data, system-generated summaries and human-composed reference summaries from the Document Understanding Conference (DUC 2003) were used. Human annotators were asked to give a coherence score on a seven-point scale for each item. The pairwise ranking preferences between summaries generated from the same input document cluster (excluding the pairs consisting of two human-written summaries) are used by a support vector machine ranker to learn a discriminant function to rank each pair according to their coherence scores. 2.3 Extended Models Filippova and Strube (2007) applied Barzilay and Lapata’s model on a German corpus of newspaper articles with manual syntactic, morphological, and NP coreference annotations provided. They further clustered entities by semantic relatedness as computed by the WikiRelated! API (Strube and Ponzetto, 2006). Though the improvement was not significant, interestingly, a short subsection in 316 their paper described their approach to extending pairwise rankings to longer rankings, by supplying the learner with rankings of all renderings as computed by Kendall’s τ, which is one of our extensions considered in this paper. Althoug"
E12-1032,P03-1069,0,0.0662614,"ighly correlated with human judgments of coherence, since additional manual annotation is certainly undesirable. (2) It depends on the particular sentence ordering in a permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences. In our work we use three different metrics: Kendall’s τ distance, average continuity, and edit distance. Kendall’s τ distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1 . It measures the disagreement between two orderings σ and π in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s τ distance is defined as τ= 2m , N(N − 1) where m is the number of sentence inversions necessary to convert σ to π. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1 Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings;"
E12-1032,J06-4002,0,0.0530225,"ed with human judgments of coherence, since additional manual annotation is certainly undesirable. (2) It depends on the particular sentence ordering in a permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences. In our work we use three different metrics: Kendall’s τ distance, average continuity, and edit distance. Kendall’s τ distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1 . It measures the disagreement between two orderings σ and π in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s τ distance is defined as τ= 2m , N(N − 1) where m is the number of sentence inversions necessary to convert σ to π. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1 Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings; but they were"
E12-1032,P11-1100,0,0.354996,"Missing"
E12-1032,W07-2312,0,0.0336407,"Missing"
E12-1032,P02-1014,0,0.561805,"(p1 (d), p2 (d), . . . , pm (d)), where pt (d) is the probability of the transition t in the entity grid, and m is the number of transitions with length no more than a predefined optimal transition length k. pt (d) is computed as the number of occurrences of t in the entity grid of document d, divided by the total number of transitions of the same length in the entity grid. For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference+ and Coreference−. In Coreference+, entity coreference relations in the document were resolved by an automatic coreference resolution tool (Ng and Cardie, 2002), whereas in Coreference−, nouns are simply clustered by string matching. 2.2 Evaluation Tasks Two evaluation tasks for Barzilay and Lapata (2008)’s entity-based model are sentence ordering and summary coherence rating. In sentence ordering, a set of random permutations is created for each source document, and the learning procedure is conducted on this synthetic mixture of coherent and incoherent documents. Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents). A training d"
E12-1032,P11-3002,0,0.0134757,"In our work we use three different metrics: Kendall’s τ distance, average continuity, and edit distance. Kendall’s τ distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1 . It measures the disagreement between two orderings σ and π in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s τ distance is defined as τ= 2m , N(N − 1) where m is the number of sentence inversions necessary to convert σ to π. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1 Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in a different language and with manual annotations, so its effect on our datasets is worth trying nonetheless. by Bollegala et al. (2006). This metric estimates the quality of a particular sentence ordering by the number of correctly arranged continuous sentences, compared to the reference ordering. For example, if π = (. . . , 3, 4, 5, 7, . . . , oN ), then {3, 4, 5} is con"
E17-1017,N16-1165,1,0.380564,"sive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated"
E17-1017,W16-2808,0,0.057931,") Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment sy"
E17-1017,P16-2085,1,0.841124,"Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale, 2007). As a result, diverse quality dimensio"
E17-1017,W15-0514,0,0.0600702,"Missing"
E17-1017,W98-0303,0,0.311573,"nal appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argument"
E17-1017,P12-2041,0,0.119946,"Missing"
E17-1017,P11-1099,1,0.393064,"Missing"
E17-1017,C14-1089,1,0.513151,"tation Quality Assessment in Natural Language Henning Wachsmuth Bauhaus-Universität Weimar Weimar, Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audien"
E17-1017,P16-2089,0,0.0173404,"al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus or"
E17-1017,P16-1150,0,0.41004,"right side of Figure 1 show where the approaches surveyed in Section 2.2 are positioned in the taxonomy. Some dimensions have been tackled multiple times (e.g., clarity), others not at all (e.g., credibility). The taxonomy indicates what sub-dimensions will affect the same high-level dimension. 4 The Dagstuhl-15512 ArgQuality Corpus Finally, we present our new annotated Dagstuhl15512 ArgQuality Corpus for studying argumentation quality based on the developed taxonomy, and we report on a first corpus analysis.3 4.1 Data and Annotation Process Our corpus is based on the UKPConvArgRank dataset (Habernal and Gurevych, 2016), which contains rankings of 25 to 35 textual debate portal arguments for two stances on 16 issues, such as evolution vs. creation and ban plastic water bottles. All ranks were derived from crowdsourced convincingness labels. For every issue/stance pair, we took the five top-ranked texts and chose five further via stratified sampling. Thereby, we covered both high-quality arguments and different levels of lower quality. Two example texts follow below in Figure 2. Before annotating the 320 chosen texts, we carried out a full annotation study with seven authors of this paper on 20 argumentative"
E17-1017,W14-2104,0,0.0646839,". (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy d"
E17-1017,P11-1032,0,0.00976501,"ween the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the roles of practical approaches. It does not require a particular argumentation model, but it rests on the notion of the granularity levels from Section 1. 3.1 Overview of the Theory-based Taxonomy Our ob"
E17-1017,D15-1110,0,0.0252984,"riven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on death penalty. This example reveals three central challeng"
E17-1017,P13-1026,0,0.362263,"Missing"
E17-1017,P14-1144,0,0.0803542,"Missing"
E17-1017,P15-1053,0,0.125852,"Missing"
E17-1017,D10-1023,0,0.426521,"Missing"
E17-1017,D08-1020,0,0.0224306,"brio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of"
E17-1017,W15-0603,0,0.0749477,"ncy, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or rebuttal) turn out to be most effective. Organization is also analyzed by Rahimi et al. (2015) on the same dataset used for the evidence 179 Aspect Quality Dimension Granularity Text Genres Sources Evidence Level of support Sufficiency Argumentation Argument unit Argument Student essays Wikipedia articles Student essays Rahimi et al. (2014) Braunstain et al. (2016) Stab and Gurevych (2017) Rhetoric Argument strength Evaluability Global coherence Organization Persuasiveness Prompt adherence Thesis clarity Winning side Argumentation Argumentation Argumentation Argumentation Argument Argumentation Argumentation Debate Student essays Law comments Student essays Student essays Forum discuss"
E17-1017,D15-1050,0,0.330444,"s under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on de"
E17-1017,W14-2110,0,0.0505174,"al et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the role"
E17-1017,D14-1006,0,0.0178285,"essment. 1 benno.stein@uni-weimar.de Introduction What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the f"
E17-1017,W16-2813,0,0.364991,"Missing"
E17-1017,E17-1092,0,0.242722,"al models and argument-oriented features, they rank sentencelevel argument units according to the level of support they provide for an answer. Unlike classical essay scoring, Rahimi et al. (2014) score an essay’s evidence, a quality dimension of argumentation: it captures how sufficiently the given details support the essay’s thesis. On the dataset of Correnti et al. (2013) with 1569 student essays and scores from 1 to 4, they find that the concentration and specificity of words related to the essay prompt (i.e., the statement defining the discussed issue) impacts scoring accuracy. Similarly, Stab and Gurevych (2017) introduce an essay corpus with 1029 argument-level annotations of sufficiency, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or"
E17-1017,W15-4631,0,0.0408966,"ltužic´ and Šnajder (2015) Relevance Reasonableness le Sufficiency Local sufficiency Prominence ia Evidence Cabrio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also,"
E17-1017,D15-1072,1,0.846424,"Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale,"
E17-1017,C16-1158,1,0.644819,"Missing"
E17-1017,E17-1105,1,0.83682,"ion What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if em"
E17-1017,P16-2032,0,0.105377,"Missing"
E17-1017,N16-1017,0,0.0362025,"Missing"
E93-1033,P85-1007,0,0.0562554,"Missing"
E93-1033,P85-1026,0,0.074497,"Missing"
E93-1033,P92-1020,0,0.0596065,"Missing"
E93-1033,J80-3003,0,0.192287,"Missing"
I11-1160,C10-2011,1,0.925831,"ian Brooke Tong Wang Graeme Hirst Department of Computer Science University of Toronto {jbrooke,tong,gh}@cs.toronto.edu Abstract Compared to many near-synonyms, clipped forms have the important property that the differences between full and abbreviated forms are almost entirely connotational or stylistic, closely tied to the formality of the discourse.1 This fact allows us to pursue two distinct though related approaches to this task, comparing a supervised model of word choice (Wang and Hirst, 2010) with a mostly unsupervised system that leverages an automatically-built lexicon of formality (Brooke et al., 2010). Our findings indicate that the lexicon-based method is highly competitive with the supervised, task-specific method. Both models approach the human performance evidenced in an independent crowdsourced annotation. In this paper, we compare a resourcedriven approach with a task-specific classification model for a new near-synonym word choice sub-task, predicting whether a full or a clipped form of a word will be used (e.g. doctor or doc) in a given context. Our results indicate that the resourcedriven approach, the use of a formality lexicon, can provide competitive performance, with the param"
I11-1160,W09-2010,0,0.0311175,"resourcedriven approaches have general potential. Clipping is a type of word formation where the beginning and/or the end of a longer word is omitted (Kreidler, 1979). This phenomenon is attested in various languages; well-known examples in English include words such as hippo (hippopotamus) and blog (weblog). Clipping and related kinds of word formation have received attention in computational linguistics with respect to the task of identifying source words from abbreviated forms, which has been studied, for instance, in the biomedical and text messaging domains (Okazaki and Ananiadou, 2006; Cook and Stevenson, 2009). 2 2.1 Methods Latent Semantic Analysis Both approaches that we are investigating make use of Latent Semantic Analysis (LSA) as a dimensionality-reduction technique (Landauer and Dumais, 1997).2 In LSA, the first step is to create a matrix representing the association between words as determined by their co-occurrence in a corpus, and then apply singular value decomposition (SVD) to identify the first k most significant dimensions of variation. After this step, each word can be represented as a vector of length k, which can be compared or combined with the vectors of other words. The best k i"
I11-1160,J02-2001,1,0.736126,"lexicon, can provide competitive performance, with the parameters of the taskspecific model mirroring the parameters under which the lexicon was built. 1 Introduction Lexical resources, though the focus of much work in computational linguistics, often compare poorly to direct statistical methods when applied to problems such as sentiment classification (Kennedy and Inkpen, 2006). Nevertheless, such resources offer advantages in terms of human interpretability and portability to many different tasks (Argamon et al., 2007). In this paper, we introduce a new sub-task of near-synonym word choice (Edmonds and Hirst, 2002), prediction of word clipping, in order to provide some new evidence that resourcedriven approaches have general potential. Clipping is a type of word formation where the beginning and/or the end of a longer word is omitted (Kreidler, 1979). This phenomenon is attested in various languages; well-known examples in English include words such as hippo (hippopotamus) and blog (weblog). Clipping and related kinds of word formation have received attention in computational linguistics with respect to the task of identifying source words from abbreviated forms, which has been studied, for instance, in"
I11-1160,W10-0204,0,0.0818254,"Missing"
I11-1160,P06-2083,0,0.0139517,"rovide some new evidence that resourcedriven approaches have general potential. Clipping is a type of word formation where the beginning and/or the end of a longer word is omitted (Kreidler, 1979). This phenomenon is attested in various languages; well-known examples in English include words such as hippo (hippopotamus) and blog (weblog). Clipping and related kinds of word formation have received attention in computational linguistics with respect to the task of identifying source words from abbreviated forms, which has been studied, for instance, in the biomedical and text messaging domains (Okazaki and Ananiadou, 2006; Cook and Stevenson, 2009). 2 2.1 Methods Latent Semantic Analysis Both approaches that we are investigating make use of Latent Semantic Analysis (LSA) as a dimensionality-reduction technique (Landauer and Dumais, 1997).2 In LSA, the first step is to create a matrix representing the association between words as determined by their co-occurrence in a corpus, and then apply singular value decomposition (SVD) to identify the first k most significant dimensions of variation. After this step, each word can be represented as a vector of length k, which can be compared or combined with the vectors o"
I11-1160,C10-1133,1,0.851536,"nd thus a lexicon of formality could be a useful tool for this kind of word choice. Our results mostly bear this out: although the vector classification model has a slight advantage, the lexicon-based method, which has the advantage of compactness, interpretability, and portability, does reasonably well. Tellingly, the best vector-based model is very similar to the lexicon in terms of its parameters, including a preference for the use of the entire document as context window and low LSA k, rather than the local context and high LSA k that was preferred for a previous near-synonym choice task (Wang and Hirst, 2010). In comparison to that task, clipping prediction is clearly more difficult, a fact that is confirmed by the results of our crowdsourced annotation. The fact that the models do better on certain individual word pairs and more poorly on others indicates that the degree of formality difference between clipped and full forms is probably quite variable, and in some cases may be barely noticeable. Under those circumstances, the advantages of a vector classification model, which might base the classification on other kinds of relevant context (e.g. topic), are clear. We conclude by noting that for a"
I13-1010,P97-1005,0,0.324238,"Missing"
I13-1010,J08-4004,0,0.0176655,"Missing"
I13-1010,D09-1094,0,0.101149,"ne pole (the oral pole) reflecting a reliance on the context of the linguistic situation, and the other (the literate pole) reflecting a reliance on cultural knowledge. The more specific elements of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions. Computational linguistics research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability, and concreteness) (Brooke et al., 2010; Pan and Hsieh, 2010; Kidwell et al., 2009; Turney et al., 2011). Of particular methodological relevance is work on the induction of polarity lexicons based on cooccurrence in large corpora (Turney and Littman, 2003; Velikovich et al., 2010), or connections in WordNet (Rao and Ravichandra, 2009; Baccianella et al., 2010); semi-supervised vector space and graph methods are common, and several of the methods we apply here are taken directly from or inspired by work in this area. 3 Literary Words which you would expect to see primarily in literature; these words often feel oldfashioned or flowery. Concrete Words which refer to events, ob"
I13-1010,baccianella-etal-2010-sentiwordnet,0,0.0208452,"main cline, creating probabilistic relationships between related dimensions. Computational linguistics research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability, and concreteness) (Brooke et al., 2010; Pan and Hsieh, 2010; Kidwell et al., 2009; Turney et al., 2011). Of particular methodological relevance is work on the induction of polarity lexicons based on cooccurrence in large corpora (Turney and Littman, 2003; Velikovich et al., 2010), or connections in WordNet (Rao and Ravichandra, 2009; Baccianella et al., 2010); semi-supervised vector space and graph methods are common, and several of the methods we apply here are taken directly from or inspired by work in this area. 3 Literary Words which you would expect to see primarily in literature; these words often feel oldfashioned or flowery. Concrete Words which refer to events, objects, or properties of objects in the physical world that you would be able to see, hear, smell, or touch. Abstract Words which refer to something that requires major psychological or cultural knowledge to grasp; complex ideas which can’t purely be defined in physical terms. Sub"
I13-1010,C10-2108,0,0.0229424,"ne of register with one pole (the oral pole) reflecting a reliance on the context of the linguistic situation, and the other (the literate pole) reflecting a reliance on cultural knowledge. The more specific elements of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions. Computational linguistics research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability, and concreteness) (Brooke et al., 2010; Pan and Hsieh, 2010; Kidwell et al., 2009; Turney et al., 2011). Of particular methodological relevance is work on the induction of polarity lexicons based on cooccurrence in large corpora (Turney and Littman, 2003; Velikovich et al., 2010), or connections in WordNet (Rao and Ravichandra, 2009; Baccianella et al., 2010); semi-supervised vector space and graph methods are common, and several of the methods we apply here are taken directly from or inspired by work in this area. 3 Literary Words which you would expect to see primarily in literature; these words often feel oldfashioned or flowery. Concrete Words whi"
I13-1010,N13-1078,1,0.767607,"s by Turney et al. (2011). We begin by converting our corpus into a binary word-document matrix, and carry out latent semantic analysis (Landauer and Dumais, 1997), which includes a singular value decomposition of the matrix and dimensionality reduction to k dimensions. Assuming vw denotes the resulting k-dimensional vector for word w, we calculate ri j as: ri j = ∑ cos(θ (vw , vs )) j s∈Si 4.1 Corpus analysis Our third method, using latent Dirichlet allocation (Blei et al., 2003), is more novel for lexical acquisition, and we address the specifics of this method in more detail in other work (Brooke and Hirst, 2013). Briefly, LDA is a Bayesian topic model which assumes that texts are generated via a For all the methods in this section, we use the same corpus, the ICWSM Spinn3r 2009 dataset (Burton et al., 2009), which has been used successfully in earlier work (Brooke et al., 2010). Social media corpora are particularly appropriate for research 85 4.3 distribution of topics for each text (θ ), and a distribution of words for each topic (β ); given a corpus, appropriate values for θ and β are derived using inference, in this case variational Bayes inference using the original implementation provided by Bl"
I13-1010,W11-0711,0,0.0879394,"resources are useful for many NLP tasks, manual lexicon creation is often onerous, particularly for aspects of language for where full coverage requires hundred of thousands of annotations. This work deals with one such aspect which we refer to as stylistic variation. This should not be understood in a purely aesthetic sense, but as reflecting various high-level aspects of the text, including genre and social identity. Some tasks relevant to style so defined include genre classification (Kessler et al., 1997), author profiling (Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntac2 Related Work In English manuals of style and other prescriptivist texts (Strunk and White, 1979; Kane, 1983), writers are urged to pay attention to various aspects of lexical style, including elements such as familiarity, readability, formality, fanciness, colloquialness, specificity, concrete"
I13-1010,C10-2011,1,0.830873,"its a single main cline of register with one pole (the oral pole) reflecting a reliance on the context of the linguistic situation, and the other (the literate pole) reflecting a reliance on cultural knowledge. The more specific elements of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions. Computational linguistics research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability, and concreteness) (Brooke et al., 2010; Pan and Hsieh, 2010; Kidwell et al., 2009; Turney et al., 2011). Of particular methodological relevance is work on the induction of polarity lexicons based on cooccurrence in large corpora (Turney and Littman, 2003; Velikovich et al., 2010), or connections in WordNet (Rao and Ravichandra, 2009; Baccianella et al., 2010); semi-supervised vector space and graph methods are common, and several of the methods we apply here are taken directly from or inspired by work in this area. 3 Literary Words which you would expect to see primarily in literature; these words often feel oldfashioned or flower"
I13-1010,E09-1077,0,0.0189628,"trongly influenced by this main cline, creating probabilistic relationships between related dimensions. Computational linguistics research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability, and concreteness) (Brooke et al., 2010; Pan and Hsieh, 2010; Kidwell et al., 2009; Turney et al., 2011). Of particular methodological relevance is work on the induction of polarity lexicons based on cooccurrence in large corpora (Turney and Littman, 2003; Velikovich et al., 2010), or connections in WordNet (Rao and Ravichandra, 2009; Baccianella et al., 2010); semi-supervised vector space and graph methods are common, and several of the methods we apply here are taken directly from or inspired by work in this area. 3 Literary Words which you would expect to see primarily in literature; these words often feel oldfashioned or flowery. Concrete Words which refer to events, objects, or properties of objects in the physical world that you would be able to see, hear, smell, or touch. Abstract Words which refer to something that requires major psychological or cultural knowledge to grasp; complex ideas which can’t purely be def"
I13-1010,P11-1077,0,0.0223577,"ybrid approach is indeed warranted. 1 Introduction Though lexical resources are useful for many NLP tasks, manual lexicon creation is often onerous, particularly for aspects of language for where full coverage requires hundred of thousands of annotations. This work deals with one such aspect which we refer to as stylistic variation. This should not be understood in a purely aesthetic sense, but as reflecting various high-level aspects of the text, including genre and social identity. Some tasks relevant to style so defined include genre classification (Kessler et al., 1997), author profiling (Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntac2 Related Work In English manuals of style and other prescriptivist texts (Strunk and White, 1979; Kane, 1983), writers are urged to pay attention to various aspects of lexical style, including elements such as familiarity, readability,"
I13-1010,J90-1003,0,0.116567,"rly do not belong to the relevant style. These results are consistent with the 1 The annotations and our guidelines are available at http://cs.toronto.edu/∼jbrooke/style annotations.zip . 84 on style, since they contain a variety of registers. Here, we include all 2.46 million texts in the Tier 1 portion which contained at least 100 word types. Hapax legomena were excluded, since they could not possibly offer any co-occurrence information, but otherwise we did not filter or lemmatize words: our full vocabulary is 1.95 million words. Our simplest method uses pointwise mutual information (PMI) (Church and Hanks, 1990), a popular metric for measuring the association between words. Since standard PMI has a lower bound of −∞ when the joint probability is 0 (a common occurrence since many of our words are relatively rare), we actually use a normalized version, NPMI, which has an upper bound of 1 and a lower bound of −1. Table 2: Number of seeds, by style. Style Positive Negative Literary 132 660 Abstract 107 599 Objective 245 495 Colloquial 163 684 Concrete 190 572 Subjective 258 487 idea that disagreement is a rough indicator of degree, and that not all disagreement should be dismissed as noise or some other"
I13-1010,J11-2001,1,0.227863,"at style. Colloquial seeds consist of English slang terms and acronyms, e.g. cuz, gig, asshole, lol. The literary seeds were primarily drawn from web sites which explain difficult language in texts such as the Bible and Lord of the Rings; examples include behold, resplendent, amiss, and thine. The concrete seeds all denote physical objects and actions, e.g. shove and lamppost, while the abstract seeds all involve nontrivial concepts patriotism and nonchalant. For our subjective seeds, we used an edited list of strongly positive and negative terms from a manually-constructed sentiment lexicon (Taboada et al., 2011), e.g. gorgeous and depraved, and for our objective set we selected words from sets of near-synonyms where one was clearly an emotionally-distant, formal alternative, e.g. residence (for home) or occupied (for busy). We filtered initial lists to 150 of each type (900 in total), removing words which did not appear in the corpus or which occurred in multiple lists. Relying on a single annotator, however, is problematic, and a more serious issue with our original Word annotation In this study, we consider six styles—colloquial, literary, concrete, abstract, subjective, and objective—which are cle"
I13-1010,D11-1063,0,0.102791,") reflecting a reliance on the context of the linguistic situation, and the other (the literate pole) reflecting a reliance on cultural knowledge. The more specific elements of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions. Computational linguistics research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability, and concreteness) (Brooke et al., 2010; Pan and Hsieh, 2010; Kidwell et al., 2009; Turney et al., 2011). Of particular methodological relevance is work on the induction of polarity lexicons based on cooccurrence in large corpora (Turney and Littman, 2003; Velikovich et al., 2010), or connections in WordNet (Rao and Ravichandra, 2009; Baccianella et al., 2010); semi-supervised vector space and graph methods are common, and several of the methods we apply here are taken directly from or inspired by work in this area. 3 Literary Words which you would expect to see primarily in literature; these words often feel oldfashioned or flowery. Concrete Words which refer to events, objects, or properties o"
I13-1010,N10-1119,0,0.0599641,"of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions. Computational linguistics research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability, and concreteness) (Brooke et al., 2010; Pan and Hsieh, 2010; Kidwell et al., 2009; Turney et al., 2011). Of particular methodological relevance is work on the induction of polarity lexicons based on cooccurrence in large corpora (Turney and Littman, 2003; Velikovich et al., 2010), or connections in WordNet (Rao and Ravichandra, 2009; Baccianella et al., 2010); semi-supervised vector space and graph methods are common, and several of the methods we apply here are taken directly from or inspired by work in this area. 3 Literary Words which you would expect to see primarily in literature; these words often feel oldfashioned or flowery. Concrete Words which refer to events, objects, or properties of objects in the physical world that you would be able to see, hear, smell, or touch. Abstract Words which refer to something that requires major psychological or cultural knowl"
I13-1010,H05-1044,0,0.0176278,"ual lexicon creation is often onerous, particularly for aspects of language for where full coverage requires hundred of thousands of annotations. This work deals with one such aspect which we refer to as stylistic variation. This should not be understood in a purely aesthetic sense, but as reflecting various high-level aspects of the text, including genre and social identity. Some tasks relevant to style so defined include genre classification (Kessler et al., 1997), author profiling (Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntac2 Related Work In English manuals of style and other prescriptivist texts (Strunk and White, 1979; Kane, 1983), writers are urged to pay attention to various aspects of lexical style, including elements such as familiarity, readability, formality, fanciness, colloquialness, specificity, concreteness, and objectivity; these stylistic cat"
I13-1039,de-marneffe-etal-2006-generating,0,0.0503727,"Missing"
I13-1039,P11-1032,0,0.145864,"ons with facts about the product, or the writer might fail to mention those asWe propose using profile compatibility to differentiate genuine and fake product reviews. For each product, a collective profile is derived from a separate collection of reviews. Such a profile contains a number of aspects of the product, together with their descriptions. For a given unseen review about the same product, we build a test profile using the same approach. We then perform a bidirectional alignment between the test and the collective profile, to compute a list of aspect-wise compatible features. We adopt Ott et al. (2011)’s op spam v1.3 dataset for identifying truthful vs. deceptive reviews. We extend the recently proposed N - GRAM+SYN model of Feng et al. (2012a) by incorporating profile compatibility features, showing such an addition significantly improves upon their state-ofart classification performance. 1 Introduction With the rapid development of e-commerce and the increasing popularity of various product review websites, people are more and more used to making purchase decisions based on the reported experience of other customers. A product rated positively by its previous users is able to attract pote"
I13-1039,P12-2034,0,0.211234,"nd fake product reviews. For each product, a collective profile is derived from a separate collection of reviews. Such a profile contains a number of aspects of the product, together with their descriptions. For a given unseen review about the same product, we build a test profile using the same approach. We then perform a bidirectional alignment between the test and the collective profile, to compute a list of aspect-wise compatible features. We adopt Ott et al. (2011)’s op spam v1.3 dataset for identifying truthful vs. deceptive reviews. We extend the recently proposed N - GRAM+SYN model of Feng et al. (2012a) by incorporating profile compatibility features, showing such an addition significantly improves upon their state-ofart classification performance. 1 Introduction With the rapid development of e-commerce and the increasing popularity of various product review websites, people are more and more used to making purchase decisions based on the reported experience of other customers. A product rated positively by its previous users is able to attract potential new customers, while a poorly rated product is certainly not a good option for most new customers. Given this influential power of produc"
I13-1039,N13-1053,0,0.38358,"Missing"
I13-1039,W12-0404,0,0.0308316,"Missing"
I13-1039,N07-1051,0,0.0675438,"Missing"
I13-1039,W12-0415,0,0.0447473,"Missing"
I13-1039,P08-1036,0,0.0185188,"Missing"
I13-1039,P03-1054,0,0.0460076,"views Rh , we extract the aspects Ah = {a1,t1 ,h , . . . , aM,tM ,h } (e.g., room, service, and Michigan Ave. in Table 1) to be included into h’s profile Ph , where ti is the type, distinct or general, of each aspect. For the sake of notational clarity, from now on, when talking about a specific hotel h, we will omit the subscript h. We first extract all noun phrases present in R, and use these noun phrases as raw aspects. For each raw aspect extracted, we identify whether it is a distinct or general aspect: if the aspect is extracted as a proper noun phrase (as tagged by the Stanford parser (Klein and Manning, 2003)), then the aspect is labeled as distinct, and general otherwise. Noun phrase extraction is performed solely on Sorting aspects by occurrences After clustering distinct and general aspects separately, we keep track of the total occurrences of each aspect across the entire set R. Then, we let A be the set of the most common N aspects4 . N is a parameter identical for all hotels, indicating how many aspects are included in each profile. We expect that the choice of N is affected by the particular type of product, and we experiment with several values of N. 4 N is usually much smaller than M, the"
I17-1051,D16-1136,0,0.01991,"ta. While these models are more cost-efficient than parallel-only approaches, it remains expensive and sometimes prohibitive to obtain sentence alignments for many languages (a problem that we seek to avoid). A lower-cost alternative to these expensive Offline Alignment The simplest approach to achieving the crosslingual objective is to train each monolingual objective separately (create a model for each language), and then learn a transformation to enforce the second objective. This approach uses a dictionary of paired words in order to learn a trans507 jointly trained models was proposed by Duong et al. (2016) and later used to project multiple languages in the same vector space (Duong et al., 2017). The model involved creating and making use of translations produced using a bilingual dictionary during training. Using expectationmaximization–inspired training, sentence translations were produced by selecting translations of words based on context to deal with polysemy, and this approach demonstrated improvements on the simple linear transformation method. However, even this model uses a significantly larger amount of data than the methods used in this work, with its smallest dictionary being compos"
I17-1051,E17-1084,0,0.0212337,"pensive and sometimes prohibitive to obtain sentence alignments for many languages (a problem that we seek to avoid). A lower-cost alternative to these expensive Offline Alignment The simplest approach to achieving the crosslingual objective is to train each monolingual objective separately (create a model for each language), and then learn a transformation to enforce the second objective. This approach uses a dictionary of paired words in order to learn a trans507 jointly trained models was proposed by Duong et al. (2016) and later used to project multiple languages in the same vector space (Duong et al., 2017). The model involved creating and making use of translations produced using a bilingual dictionary during training. Using expectationmaximization–inspired training, sentence translations were produced by selecting translations of words based on context to deal with polysemy, and this approach demonstrated improvements on the simple linear transformation method. However, even this model uses a significantly larger amount of data than the methods used in this work, with its smallest dictionary being composed of 35,000 word pairs compared to our approach, which can use as little as 2000 words for"
I17-1051,E14-1049,0,0.0201554,"ade use of only 2000, 4500, or 8500 word pairs – a task easily accomplished by a human translator on a data-poor language. We experiment with differing amounts of data during training to show the robustness of our observation. We then apply the fine-grained sentiment regressor to the task of review classification as done by Chen et al. (2016), and show that our na¨ıve algorithm achieves results similar to their benchmark but at a much lower cost. formation or ‘alignment’ from the vector space of one language to that of another. First introduced by Mikolov et al. (2013a), and later extended by Faruqui and Dyer (2014), this offline alignment is fast and low cost, but does not achieve a high translation accuracy. A big drawback of these approaches is that using a dictionary ignores the polysemic nature of languages. It is also not clear or proven that a single transformation would be able to capture the relationship between all the words in a cross-lingual setting. We opt to use offline alignment to show that such a low-cost approach does, in fact, capture a significant part of the relationship between words of different languages when it comes to sentiment. That is, a single transformation (linear in the c"
I17-1051,N15-1078,0,0.0426187,"Missing"
I17-1051,D15-1016,0,0.0196021,"rial in the other languages. This is usually done through the use of either bilingual lexicons (Balamurali et al., 2012), machine translation (MT) systems (Salameh et al., 2015; Zhou et al., 2016), or more recently, through the use of bilingual vector space embeddings (Chen et al., 2016). Unfortunately, in some cases such data is still expensive to obtain. Many languages do not have good, or sometimes any, MT systems, and the cost of producing word alignments or sentence alignments for training bilingual word embeddings (BWE) (Zou et al., 2013; Bengio and Corrado, 2015) or similar techniques (Jain and Batra, 2015) is prohibitive for data-poor languages. Here we introduce a high-performance, lowcost approach to cross-lingual sentiment classification, which can be used to benchmark more expensive methods. We demonstrate the utility of this approach by highlighting how very limited training data suffices for effective cross-lingual sentiment analysis in various contexts (both at the word and sentence/document level). Our approach relies on the simple vector space translation matrix method (Mikolov et al., 2013a), which computes a matrix to convert from the vector space of one language to that of another."
I17-1051,C12-1089,0,0.0398753,"the embeddings of a single language (such as English, with a wealth of labeled data) for other languages as well. Below we discuss algorithms to achieve the crosslingual objective, their costs, performance, and the rationale underlying our algorithm design. 2.1.1 Parallel-Only 2.1.3 Jointly-Trained Model Combining the offline alignment and parallel-only algorithms is a third class of jointly-trained approaches. These approaches jointly optimize the monolingual objective at the same time as the cross-lingual objective, making use of both monolingual and parallel data. Approaches like those of Klementiev et al. (2012) and Zou et al. (2013) use word-aligned data in order to learn the finegrained cross-lingual features and tend to be quite slow. Other approaches (including that of Bengio and Corrado (2015)) rely on sentence-aligned data and are faster than those using word-aligned data. While these models are more cost-efficient than parallel-only approaches, it remains expensive and sometimes prohibitive to obtain sentence alignments for many languages (a problem that we seek to avoid). A lower-cost alternative to these expensive Offline Alignment The simplest approach to achieving the crosslingual objectiv"
I17-1051,W11-3713,0,0.0214184,"st, thus making sentiment and related analyses for many languages inexpensive. 1 Introduction Methods for sentiment analysis and classification have largely been limited to English, making use of large amounts of labeled data to produce sentiment classification. As a consequence, many developed approaches cannot be readily applied to other languages, which usually do not have the wealth of labeled data that is exclusive to English. Therefore many approaches which deal with other languages often: i) experiment with small datasets that are limited in domain or size of training and testing sets (Lee and Renganathan, 2011; Tan and Zhang, 2008), or ii) attempt to elucidate sentiment lexicons for their respective languages (Mohammad et al., 2016). A growing number of publications attempt to leverage labeled English data to compensate for 1 This work involves transfers in both directions between English and other languages. We will take the perspective of the translation matrix (section 3.2) and refer to English as the target language and the others as source languages. 506 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 506–515, c Taipei, Taiwan, November 27 – Dece"
I17-1051,Y15-2030,0,0.0304456,"https://github.com/first20hours/google-10000-english 7 https://translate.google.com/ 2 https://code.google.com/archive/p/word2vec/ 8 508 https://github.com/williamgunn/SciSentiment Low Stimulus relaxed (2.39) victim (2.69) death (1.61) High Stimulus infatuation (7.02) confident (7.68) beauty (7.82) Table 1: Examples of words on each end of the spectrum for each of the three dimensions of ANEW. Numeric stimulus value is shown in parentheses. (2015) with their sentiment ratings as labels ranging from 1 for very negative to 5 for very positive. Labeled Chinese Reviews Here we use a dataset from Lin et al. (2015). Their work provides hotel reviews, with labels ranging from 1 for very negative to 5 for very positive. In order to fairly compare our work with that of Chen et al. (2016), we use 10, 000 reviews for model selection, and another unseen 10, 000 as our test set. 3.1.3 3.2 Arousal Dominance Valence Fine-Grained Data For fine-grained sentiment regression, we used Affective Norms for English Words (ANEW) (Bradley and Lang, 1999). The creators of ANEW sought to provide emotional ratings for a large number of words in the English language. ANEW proposes that all human emotion can be organized in a"
I17-1051,P11-1015,0,0.0609745,"el with similar representations. Such approaches can be effective, but require extremely expensive data. Another drawback is that these approaches can be affected by the writing style of the parallel text (Bengio and Corrado, 2015). Cross-Lingual Word Embeddings Monolingual word embedding algorithms use large unlabeled datasets to learn useful features about the given language (Pennington et al., 2014; Mikolov et al., 2013b). These algorithms learn vector representations for the words of the language — an encoding that has proven utility in a variety of NLP tasks including sentiment analysis (Maas et al., 2011) and machine translation (Mikolov et al., 2013a). When working with more than one language, we seek to satisfy two objectives: i) monolingually, similar words of the same language have similar embeddings; and ii) cross-lingually, similar words across languages also have similar embeddings. Satisfying these two criteria would allow us to use algorithms trained for the embeddings of a single language (such as English, with a wealth of labeled data) for other languages as well. Below we discuss algorithms to achieve the crosslingual objective, their costs, performance, and the rationale underlyin"
I17-1051,P15-1042,0,0.0193857,"e text into tokens we used Jieba5 . Finally, to create the actual word embedding model, we used Genˇ uˇrek and Sojka, 2011) with the minimum sim (Reh˚ count set to 1, using continuous bag of words (CBOW), a window of 8, and vector dimension set to 300. Cross-Lingual Sentiment Analysis 3.1.2 Previous approaches to cross-lingual sentiment analysis can be classified into two main categories: i) those that rely on parallel corpora to train BWE’s (i.e., they use pre-trained embeddings) (Chen et al., 2016; Sarath Chandar et al., 2014; Tang and Wan, 2014), and ii) those that use translation systems (Zhou et al., 2015, 2016) in order to obtain aligned inputs to learn to extract features which work on both languages. Both approaches allow the sentiment portion of training and testing data to be in the same vector space. However, many languages have no MT system, and it is extremely expensive to create one on a language-by-language basis. Our proposed approach is simpler in that it requires only a small word-list to learn both the embedding and the sentiment classification, the duality of which cannot be claimed by previous approaches. 3 Translation Word List For the process of learning a translation matrix"
I17-1051,L16-1006,0,0.0226432,"classification have largely been limited to English, making use of large amounts of labeled data to produce sentiment classification. As a consequence, many developed approaches cannot be readily applied to other languages, which usually do not have the wealth of labeled data that is exclusive to English. Therefore many approaches which deal with other languages often: i) experiment with small datasets that are limited in domain or size of training and testing sets (Lee and Renganathan, 2011; Tan and Zhang, 2008), or ii) attempt to elucidate sentiment lexicons for their respective languages (Mohammad et al., 2016). A growing number of publications attempt to leverage labeled English data to compensate for 1 This work involves transfers in both directions between English and other languages. We will take the perspective of the translation matrix (section 3.2) and refer to English as the target language and the others as source languages. 506 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 506–515, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP We quantitatively evaluate our methods by training on English and testing on words from Spanish and C"
I17-1051,P16-1133,0,0.067564,"Missing"
I17-1051,D13-1141,0,0.204839,"Toronto Toronto, Canada Abstract the relative lack of training material in the other languages. This is usually done through the use of either bilingual lexicons (Balamurali et al., 2012), machine translation (MT) systems (Salameh et al., 2015; Zhou et al., 2016), or more recently, through the use of bilingual vector space embeddings (Chen et al., 2016). Unfortunately, in some cases such data is still expensive to obtain. Many languages do not have good, or sometimes any, MT systems, and the cost of producing word alignments or sentence alignments for training bilingual word embeddings (BWE) (Zou et al., 2013; Bengio and Corrado, 2015) or similar techniques (Jain and Batra, 2015) is prohibitive for data-poor languages. Here we introduce a high-performance, lowcost approach to cross-lingual sentiment classification, which can be used to benchmark more expensive methods. We demonstrate the utility of this approach by highlighting how very limited training data suffices for effective cross-lingual sentiment analysis in various contexts (both at the word and sentence/document level). Our approach relies on the simple vector space translation matrix method (Mikolov et al., 2013a), which computes a matr"
I17-1051,D14-1162,0,\N,Missing
I17-1051,C12-2008,0,\N,Missing
J02-2001,W98-1406,0,0.0115323,"raints. So lexical choice—genuine lexical choice—is making choices between options rather than merely finding the words for concepts, as was the case in many early text generation systems (for instance, BABEL [Goldman 1975], MUMBLE [McDonald 1983], and TEXT [McKeown 1985]). This kind of lexical choice is now thought to be the central task in text generation (or, at least, sentence generation), because it interacts with almost every other task involved. Indeed, many recent text generation systems, including MOOSE (Stede 1999), ADVISOR II (Elhadad, McKeown, and Robin 1997), and Hunter-Gatherer (Beale et al. 1998), among others (see Reiter and Dale’s [1997] survey), adopt this view, yet their lexical-choice components do not account for near-synonymy. Without loss of generality, we will look at fine-grained lexical choice in the context of one of these systems: Stede’s MOOSE (1999). The input to MOOSE is a “SitSpec,” that is, a specification of a situation represented on the conceptual–semantic level as a graph of instances of concepts linked 128 Edmonds and Hirst Near-Synonymy and Lexical Choice by relations. MOOSE outputs a complete well-formed “SemSpec,” or semantic specification on the syntactic–se"
J02-2001,J90-2002,0,0.0338651,"t link between synsets in different languages and has an explicit relation, EQ NEAR SYNONYM, for relating synsets that are not directly equivalent across languages. But, as in individual WordNets, there is no provision for representing differences between near-synonyms. In statistical MT, there would seem to be some promise for handling near-synonymy. In principle, a system could choose the near-synonym that is most probable given the source sentence and the target-language model. Near-synonymy seems to have been of little concern, however, in statistical MT research: The seminal researchers, Brown et al. (1990), viewed such variations as a matter of taste; in evaluating their system, two different translations of the same source that convey roughly the same meaning (perhaps with different words) are considered satisfactory translations. More recently, though, Foster, Isabelle, and Plamondon (1997) show how such a model can be used in interactive MT, and Langkilde and Knight (1998) in text generation. Such methods are unfortunately limited in practice, because it is too computationally expensive to go beyond a trigram model (only two words of context). Even if a statistical approach could account for"
J02-2001,P93-1022,0,0.0111992,"Missing"
J02-2001,J93-3002,1,0.651818,"actually a reference (i.e., a variable) to one of the concepts specified in the core denotation of peripheral concepts. The second part of Table 2 gives an example. 5.4.3 Stylistic Distinctions. Although we take a rather basic approach to representing stylistic distinctions, that does not imply that style is easy to capture. Style is one of the most difficult of lexical phenomena to account for, since it affects the text at a pragmatic level and is highly influenced by context. Since there is as yet no comprehensive theory of style, our approach is similar to past approaches, such as those of DiMarco and Hirst (1993), Stede (1993), and Hovy (1988). Unlike the denotational distinctions discussed above, stylistic features have a global or absolute quality to them. We can compare all words, whether or not they are near-synonyms, on various stylistic dimensions, such as formality and concreteness. Because style is a global aspect of text, a certain style can be (and should be) achieved by more than just lexical choice; structural choices are just as important (DiMarco and Hirst 1993). Hence, in defining a set of stylistic dimensions, we must look for global stylistic features that can be carried not only by w"
J02-2001,P97-1067,1,0.884056,"r-synonyms into single nodes (e.g., in WordNet). In any case, as we argued in Section 3, taxonomies are inappropriate for modeling near-synonyms. Second, as we noted in Section 2.2, standard dictionary definitions are not usually fine-grained enough (they define the core meaning but not all the nuances of a word) and can even be circular, defining each of several nearsynonyms in terms of the other near-synonyms. And third, although corpus-based methods (e.g., Lin’s [1998]) do compute different similarity values for different pairs of near-synonyms of the same cluster, Church et al. (1994) and Edmonds (1997) show that such methods are not yet capable of uncovering the more subtle differences in the use of near-synonyms for lexical choice. But one benefit of the clustered model of lexical knowledge is that it naturally lends itself to the computation of explicit differences or degrees of similarity between near-synonyms. Although a fully effective similarity measure for near-synonyms still eludes us, in this section we will characterize the problem and give a solution to one part of it: computing the similarity of individual lexical distinctions. 6.1 Computing the Similarity of Near-Synonyms In th"
J02-2001,J97-2001,0,0.0217099,"Missing"
J02-2001,1997.mtsummit-papers.1,0,0.0838808,"Missing"
J02-2001,1997.mtsummit-papers.19,0,0.00732598,"l level and use constraints and preferences, which gives flexibility and robustness to the lexical-choice process. Viegas (1998), on the other hand, describes a preliminary solution that accounts for semantic vagueness and underspecification in a generative framework. Although her model is intended to account for near-synonymy, she does not explicitly discuss it. Transfer-based MT systems use a bilingual lexicon to map words and expressions from one language to another. Lists, sometimes huge, of handcrafted language-pairspecific rules encode the knowledge to use the mapping (e.g., in SYSTRAN [Gerber and Yang 1997]). EuroWordNet (Vossen 1998) could be used in such a system. Its Inter-Lingual-Index provides a language-independent link between synsets in different languages and has an explicit relation, EQ NEAR SYNONYM, for relating synsets that are not directly equivalent across languages. But, as in individual WordNets, there is no provision for representing differences between near-synonyms. In statistical MT, there would seem to be some promise for handling near-synonymy. In principle, a system could choose the near-synonym that is most probable given the source sentence and the target-language model"
J02-2001,O97-1002,0,0.0290992,"n developing methods to compute the degree of semantic similarity between any two words, or, more precisely, between the simple or primitive concepts15 denoted by any two words. There are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these (Dagan, Marcus, and Markovitch 1993; Kozima and Furugori 1993; Pereira, Tishby, and Lee 1993; Church et al. 1994; Grefenstette 1994; Resnik 1995; McMahon and Smith 1996; Jiang and Conrath 1997; Schutze ¨ 1998; Lin 1998; Resnik and Diab 2000; Budanitsky 1999; Budanitsky and Hirst 2001, 2002). Unfortunately, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures lack the required precision. First, taxonomic hierarchies and semantic networks inherently treat near-synonyms as absolute synonyms in grouping near-synonyms into single nodes (e.g., in WordNet). In any case, as we argued in Section 3, taxonomies are inappropriate for modeling near-synonyms. Second, as we noted in Section 2.2, standard dictionary definitions are not usually fin"
J02-2001,P91-1025,0,0.0124262,"in the satisfaction of one of the preferences. Fib is chosen despite the fact that it is informal, because it is the only word that implies an insignificant lie. But the system compensates by choosing two 137 Computational Linguistics Volume 28, Number 2 other formal words: enjoin and inebriate. If we add a preference to this case to imply that John has official authority (case vi), then I-Saurus system chooses command instead of enjoin, further sacrificing high formality. 9. Related Work Most computational work on near-synonymy has been motivated by lexical mismatches in machine translation (Kameyama et al. 1991). In interlingual MT, an intermediate representational scheme, such as an ontology in knowledge-based machine translation (KBMT) (Nirenburg et al. 1992), or lexical-conceptual structures in UNITRAN (Dorr 1993) is used in encoding lexical meaning (and all other meaning). But as we showed in Section 3, such methods don’t work at the fine grain necessary for near-synonymy, despite their effectiveness at a coarse grain. To overcome these problems but retain the interlingual framework, Barnett, Mani, and Rich (1994) describe a method of generating natural-sounding text that is maximally close in me"
J02-2001,E93-1028,0,0.0745683,"ch (1978) then used as the basis for the prototype theory of meaning. Recent research in computational linguistics has focused more on developing methods to compute the degree of semantic similarity between any two words, or, more precisely, between the simple or primitive concepts15 denoted by any two words. There are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these (Dagan, Marcus, and Markovitch 1993; Kozima and Furugori 1993; Pereira, Tishby, and Lee 1993; Church et al. 1994; Grefenstette 1994; Resnik 1995; McMahon and Smith 1996; Jiang and Conrath 1997; Schutze ¨ 1998; Lin 1998; Resnik and Diab 2000; Budanitsky 1999; Budanitsky and Hirst 2001, 2002). Unfortunately, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures lack the required precision. First, taxonomic hierarchies and semantic networks inherently treat near-synonyms as absolute synonyms in grouping near-synonyms into single nodes (e.g., in WordNet). In any case, as we argued in Section 3, taxonomies ar"
J02-2001,W98-1426,0,0.00695037,"system could choose the near-synonym that is most probable given the source sentence and the target-language model. Near-synonymy seems to have been of little concern, however, in statistical MT research: The seminal researchers, Brown et al. (1990), viewed such variations as a matter of taste; in evaluating their system, two different translations of the same source that convey roughly the same meaning (perhaps with different words) are considered satisfactory translations. More recently, though, Foster, Isabelle, and Plamondon (1997) show how such a model can be used in interactive MT, and Langkilde and Knight (1998) in text generation. Such methods are unfortunately limited in practice, because it is too computationally expensive to go beyond a trigram model (only two words of context). Even if a statistical approach could account for near-synonymy, Edmonds (1997) showed that its strength is not in choosing the right word, but rather in determining which near-synonym is most typical or natural in a given context. So such an approach would not be so useful in goal-directed applications such as text generation, or even in sophisticated MT. 10. Conclusion Every natural language processing system needs some"
J02-2001,P98-2127,0,0.0464134,"ee of semantic similarity between any two words, or, more precisely, between the simple or primitive concepts15 denoted by any two words. There are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these (Dagan, Marcus, and Markovitch 1993; Kozima and Furugori 1993; Pereira, Tishby, and Lee 1993; Church et al. 1994; Grefenstette 1994; Resnik 1995; McMahon and Smith 1996; Jiang and Conrath 1997; Schutze ¨ 1998; Lin 1998; Resnik and Diab 2000; Budanitsky 1999; Budanitsky and Hirst 2001, 2002). Unfortunately, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures lack the required precision. First, taxonomic hierarchies and semantic networks inherently treat near-synonyms as absolute synonyms in grouping near-synonyms into single nodes (e.g., in WordNet). In any case, as we argued in Section 3, taxonomies are inappropriate for modeling near-synonyms. Second, as we noted in Section 2.2, standard dictionary definitions are not usually fine-grained enough (they def"
J02-2001,J96-2003,0,0.0087024,"stics has focused more on developing methods to compute the degree of semantic similarity between any two words, or, more precisely, between the simple or primitive concepts15 denoted by any two words. There are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these (Dagan, Marcus, and Markovitch 1993; Kozima and Furugori 1993; Pereira, Tishby, and Lee 1993; Church et al. 1994; Grefenstette 1994; Resnik 1995; McMahon and Smith 1996; Jiang and Conrath 1997; Schutze ¨ 1998; Lin 1998; Resnik and Diab 2000; Budanitsky 1999; Budanitsky and Hirst 2001, 2002). Unfortunately, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures lack the required precision. First, taxonomic hierarchies and semantic networks inherently treat near-synonyms as absolute synonyms in grouping near-synonyms into single nodes (e.g., in WordNet). In any case, as we argued in Section 3, taxonomies are inappropriate for modeling near-synonyms. Second, as we noted in Section 2.2, standard dictionary definit"
J02-2001,P93-1024,0,0.0881091,"Missing"
J02-2001,J98-1004,0,0.0396814,"Missing"
J02-2001,E93-1055,0,0.372659,"ance of meaning. At best, absolute synonymy is limited mostly to dialectal variation and technical terms (underwear (AmE) : pants (BrE); groundhog : woodchuck; distichous : two-ranked; plesionym : near-synonym), but even these words would change the style of an utterance when intersubstituted. Usually, words that are close in meaning are near-synonyms (or plesionyms)1 — almost synonyms, but not quite; very similar, but not identical, in meaning; not fully intersubstitutable, but instead varying in their shades of denotation, connotation, implicature, emphasis, or register (DiMarco, Hirst, and Stede 1993).2 Section 4 gives a more formal definition. Indeed, near-synonyms are pervasive in language; examples are easy to find. Lie, falsehood, untruth, fib, and misrepresentation, for instance, are near-synonyms of one another. All denote a statement that does not conform to the truth, but they differ from one another in fine aspects of their denotation. A lie is a deliberate attempt to deceive that is a flat contradiction of the truth, whereas a misrepresentation may be more indirect, as by misplacement of emphasis, an untruth might be told merely out of ignorance, and a fib is deliberate but relat"
J02-2001,P98-2215,0,0.0171644,"se problems but retain the interlingual framework, Barnett, Mani, and Rich (1994) describe a method of generating natural-sounding text that is maximally close in meaning to the input interlingual representation. Like us, they define the notion of semantic closeness, but whereas they rely purely on denotational representations and (approximate) logical inference in addition to lexical features for relative naturalness, we explicitly represent fine-grained aspects on a subconceptual level and use constraints and preferences, which gives flexibility and robustness to the lexical-choice process. Viegas (1998), on the other hand, describes a preliminary solution that accounts for semantic vagueness and underspecification in a generative framework. Although her model is intended to account for near-synonymy, she does not explicitly discuss it. Transfer-based MT systems use a bilingual lexicon to map words and expressions from one language to another. Lists, sometimes huge, of handcrafted language-pairspecific rules encode the knowledge to use the mapping (e.g., in SYSTRAN [Gerber and Yang 1997]). EuroWordNet (Vossen 1998) could be used in such a system. Its Inter-Lingual-Index provides a language-in"
J02-2001,W96-0401,0,0.0266443,"Missing"
J02-2001,W91-0202,0,\N,Missing
J02-2001,W91-0207,0,\N,Missing
J02-2001,C98-2122,0,\N,Missing
J02-2001,J03-2001,0,\N,Missing
J02-2001,C98-2210,0,\N,Missing
J06-1003,W05-1203,0,0.444412,"est (out of “various schemes”, which they do not list) for their task of learning coarse-grained semantic classes. In word-sense disambiguation, Patwardhan, Banerjee, and Pedersen (2003) found Jiang–Conrath to be clearly the best of the five measures evaluated here, albeit edged out by their own new “Lesk” measure based on gloss overlaps;21 and McCarthy et al. (2004) found that the Jiang–Conrath and Lesk measures gave the best accuracy in their task of finding predominant word senses, with the results of the two being “comparable” but Jiang–Conrath being far more efficient. On the other hand, Corley and Mihalcea (2005) found little difference between the measures when using them in an algorithm for computing text similarity. 6.2 Measures of Distributional Similarity as Proxies for Measures of Semantic Relatedness In Section 1.1, we mentioned that the lexical semantic relatedness or similarity that we have dealt with in this paper is a notion distinct from that of lexical distributional 21 Patwardhan et al.’s measure is based on the idea, originally due to Lesk (1986), of measuring the degree of relatedness of two words by the number of string overlaps in their dictionary definitions or glosses. Patwardhan e"
J06-1003,O97-1002,0,0.42208,"n refer to human performance. We have such an upper bound for the Miller and Charles word pairs (but not for the complete set of Rubenstein and Goodenough pairs): Resnik (1995) replicated Miller and 8 We have kept the original orderings of the pairs: from dissimilar to similar for the Rubenstein– Goodenough data and from similar to dissimilar for Miller–Charles. This explains why the two groups of graphs (Figures 2 and 3) as wholes have the opposite directions. Notice that because distJC measures distance, the Jiang–Conrath plot has a slope opposite to the rest of each group. 9 Resnik (1995), Jiang and Conrath (1997), and Lin (1998b) report the coefficients of correlation between their measures and the Miller–Charles ratings to be 0.7911, 0.8282, and 0.8339, respectively, which differ slightly from the corresponding figures in Table 3. These discrepancies can be explained by possible minor differences in implementation (e.g., the compound-word recognition mechanism used in collecting the frequency data), differences between the versions of WordNet used in the experiments (Resnik), and differences in the corpora used to obtain the frequency data (Jiang and Conrath, Lin). Also, the coefficients reported by"
J06-1003,P05-1005,0,0.0559872,"al results of this work (Budanitsky and Hirst 2001), Pedersen and his colleagues (Pedersen, Patwardhan, and Michelizzi 2004) have made available a Perl implementation of the five WordNet-based measures (plus Wu and Palmer’s and their own; see below) that has been used by a number of researchers in published work on other NLP applications. Generally, these results are consistent with our own. For example, Stevenson and Greenwood (2005) found Jiang–Conrath to be the best measure (out of “several”, which they do not list) for their task of pattern induction for information extraction. Similarly, Kohomban and Lee (2005) found Jiang–Conrath the best (out of “various schemes”, which they do not list) for their task of learning coarse-grained semantic classes. In word-sense disambiguation, Patwardhan, Banerjee, and Pedersen (2003) found Jiang–Conrath to be clearly the best of the five measures evaluated here, albeit edged out by their own new “Lesk” measure based on gloss overlaps;21 and McCarthy et al. (2004) found that the Jiang–Conrath and Lesk measures gave the best accuracy in their task of finding predominant word senses, with the results of the two being “comparable” but Jiang–Conrath being far more effi"
J06-1003,E93-1028,0,0.0557465,"will usually be denoted by c and w, with various subscripts. For the sake of uniformity of presentation, we have taken the liberty of altering the original notation accordingly in some other authors’ formulas. 2. Lexical Resource–based Approaches to Measuring Semantic Relatedness All approaches to measuring semantic relatedness that use a lexical resource construe the resource, in one way or another, as a network or directed graph, and then base the measure of relatedness on properties of paths in this graph. 14 Budanitsky and Hirst Lexical Semantic Relatedness 2.1 Dictionary-based Approaches Kozima and Furugori (1993) turned the Longman Dictionary of Contemporary English (LDOCE) (Procter 1978) into a network by creating a node for every headword and linking each node to the nodes for all the words used in its definition. The 2851word controlled defining vocabulary of LDOCE thus becomes the densest part of the network: the remaining nodes, which represent the headwords outside of the defining vocabulary, can be pictured as being situated at the fringe of the network, as they are linked only to defining-vocabulary nodes and not to each other. In this network, the similarity function simKF between words of th"
J06-1003,P99-1004,0,0.160561,"l similarity measure with his semantic relatedness measure, simL , which has been discussed in earlier sections of this paper; but observe that both are derived from the same theorem. 42 Budanitsky and Hirst Lexical Semantic Relatedness such as various kinds of ambiguity resolution in which it is words rather than senses that are available (see Weeds (2003) for an extensive list). Second, whereas semantic relatedness is symmetric, distributional similarity is a potentially asymmetrical relationship. If distributional similarity is conceived of as substitutability, as Weeds and Weir (2005) and Lee (1999) emphasize, then asymmetries arise when one word appears in a subset of the contexts in which the other appears; for example, the adjectives that typically modify apple are a subset of those that modify fruit, so fruit substitutes for apple better than apple substitutes for fruit. While some distributional similarity measures, such as cosine, are symmetric, many, such as α-skew divergence and the co-occurrence retrieval models developed by Weeds and Weir, are not. But this is simply not an adequate model of semantic relatedness, for which substitutability is far too strict a requirement: windo"
J06-1003,P98-2127,0,0.335249,"2 ) in (14)), because it has no parent in the set. Expanding the sum in the right-hand side of equation (14), plugging in the expression for parent–child distance from equation (13), and performing necessary eliminations results in the following final formula for the semantic distance between concepts c1 and c2 : distJC (c1 , c2 ) = IC(c1 ) + IC(c2 ) − 2 × IC(lso(c1 , c2 )) = 2 log p(lso(c1 , c2 )) − (log p(c1 ) + log p(c2 )) (15) (16) 2.6.3 Lin’s Universal Similarity Measure. Noticing that all of the similarity measures known to him were tied to a particular application, domain, or resource, Lin (1998b) attempted to define a measure of similarity that would be both universal (applicable to arbitrary objects and “not presuming any form of knowledge representation”) and theoretically justified (“derived from a set of assumptions”, instead of “directly by a formula”, so that “if the assumptions are deemed reasonable, the similarity measure necessarily follows”). He used the following three intuitions as a basis: 1. The similarity between arbitrary objects A and B is related to their commonality; the more commonality they share, the more similar they are. 2. The similarity between A and B is r"
J06-1003,P04-1036,0,0.311836,"Missing"
J06-1003,J91-1002,1,0.16915,"citly indicate how and why they are related. The user’s main access is through the index, which contains category numbers along with labels representative of those categories for each word. Polysemes are implicitly disambiguated, to a certain extent, by the other words in their cluster and in their index entry. Closely related concepts might or might not be physically close in the thesaurus: “Physical closeness has some importance . . . but words in the index of the thesaurus often have widely scattered categories, and each category often points to a widely scattered selection of categories” (Morris and Hirst 1991). Methods of semantic distance that are based on Rogetstructured thesauri therefore rely not only on the category structure but also on the index and on the pointers within categories that cross-reference other categories. In part as a consequence of this, typically no numerical value for semantic distance can be obtained: rather, algorithms using the thesaurus compute a distance implicitly and return a boolean value of ‘close’ or ‘not close’. Working with an abridged version of Roget’s Thesaurus, Morris and Hirst (1991) identified five types of semantic relations between words. In their appro"
J06-1003,W04-2607,1,0.156533,"the same authors: semantic relatedness, similarity, and semantic distance. Resnik (1995) attempts to demonstrate the distinction between the first two by way of example. “Cars and gasoline”, he writes, “would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar.” Similarity is thus a special case of semantic relatedness, and we adopt this perspective in this paper. Among other relationships that the notion of relatedness encompasses are the various kinds of meronymy, antonymy, functional association, and other “non-classical relations” (Morris and Hirst 2004). The term semantic distance may cause even more confusion, as it can be used when talking about either just similarity or relatedness in general. Two concepts are “close” to one another if their similarity or their relatedness is high, and otherwise they are “distant”. Most of the time, these two uses are consistent with one another, but not always; antonymous concepts are dissimilar and hence distant in one sense, and yet are strongly related semantically and hence close in the other sense. We would thus have very much preferred to be able to adhere to the view of semantic distance as the in"
J06-1003,P05-1047,0,0.15269,"and Resnik and increases only from 1 to 4 for Lin and from 1 to 12 for Jiang–Conrath. 6. Related Work 6.1 Other Applications of WordNet-based Measures Since the first publication of the initial results of this work (Budanitsky and Hirst 2001), Pedersen and his colleagues (Pedersen, Patwardhan, and Michelizzi 2004) have made available a Perl implementation of the five WordNet-based measures (plus Wu and Palmer’s and their own; see below) that has been used by a number of researchers in published work on other NLP applications. Generally, these results are consistent with our own. For example, Stevenson and Greenwood (2005) found Jiang–Conrath to be the best measure (out of “several”, which they do not list) for their task of pattern induction for information extraction. Similarly, Kohomban and Lee (2005) found Jiang–Conrath the best (out of “various schemes”, which they do not list) for their task of learning coarse-grained semantic classes. In word-sense disambiguation, Patwardhan, Banerjee, and Pedersen (2003) found Jiang–Conrath to be clearly the best of the five measures evaluated here, albeit edged out by their own new “Lesk” measure based on gloss overlaps;21 and McCarthy et al. (2004) found that the Jian"
J06-1003,J05-4002,0,0.310399,"aps be used to automatically create (first-draft) lexical resources (Grefenstette 1994). It is therefore natural to ask how distributional-similarity measures compare with the WordNet-based measures that we have looked at above. Formally, by distributional similarity (or co-occurrence similarity) of two words w1 and w2 , we mean that they tend to occur in similar contexts, for some definition of context; or that the set of words that w1 tends to co-occur with is similar to the set that w2 tends to co-occur with; or that if w1 is substituted for w2 in a context, its “plausibility” (Weeds 2003; Weeds and Weir 2005) is unchanged. The context considered may be a small or large window around the word, or an entire document; or it may be a syntactic relationship. For example, Weeds (2003; Weeds and Weir, 2005) (see below) took verbs as contexts for nouns in object position: so they regarded two nouns to be similar to the extent that they occur as direct objects of the same set of verbs. Lin (1998b, 1998a) considered other syntactic relationships as well, such as subject–verb and modifier– noun, and looked at both roles in the relationship. Given this framework, many different methods of measuring distributi"
J06-1003,P94-1019,0,0.126938,"c1 and c2 is then the average of the weights on each direction of the edge, scaled by the depth of the nodes: distS (c1 , c2 ) = wt(c1 →r ) + wt(c2 →r ) 2 × max{depth(c1 ), depth(c2 )} (4) where r is the relation that holds between c1 and c2 and r is its inverse (i.e., the relation that holds between c2 and c1 ). Finally, the semantic distance between two arbitrary nodes ci and cj is the sum of the distances between the pairs of adjacent nodes along the shortest path connecting them. 2.5.2 Wu and Palmer’s Conceptual Similarity. In a paper on translating English verbs into Mandarin Chinese, Wu and Palmer (1994) introduce a scaled metric for what they call conceptual similarity between a pair of concepts c1 and c2 in a hierarchy as simWP (c1 , c2 ) = 2 × depth(lso(c1 , c2 )) len(c1 , lso(c1 , c2 )) + len(c2 , lso(c1 , c2 )) + 2 × depth(lso(c1 , c2 )) (5) Note that depth(lso(c1 , c2 )) is the “global” depth in the hierarchy; its role as a scaling factor can be seen more clearly if we recast equation 5 from similarity into distance: distWP (c1 , c2 ) = 1 − simWP (c1 , c2 ) len(c1 , lso(c1 , c2 )) + len(c2 , lso(c1 , c2 )) = len(c1 , lso(c1 , c2 )) + len(c2 , lso(c1 , c2 )) + 2 × depth(lso(c1 , c2 )) (6"
J06-1003,C98-2122,0,\N,Missing
J06-2003,C96-1013,0,0.0603358,"Missing"
J06-2003,P99-1016,0,0.0167551,"Missing"
J06-2003,A00-2018,0,0.0424908,"different because of translation divergences between languages (Dorr 1993). For the sentences in our test data, a quick manual inspection shows that this happens very rarely or not at all. This simplification eliminates the need to parse the French sentence and the need to build a tool to extract its semantics. As depicted in Figure 20, the interlingual representation is produced with a preexisting input construction tool that was previously used by Langkilde-Geary (2002a) in her HALogen evaluation experiments. In order to use this tool, we parsed the English sentences with Charniak’s parser (Charniak 2000).22 The tool was designed to work on parse trees from the Penn TreeBank, which have some extra annotations; it worked on parse trees produced by Charniak’s parser, but it failed on some parse trees probably more often than it 21 The sentences were obtained from USC/ISI (http://www.isi.edu/natural-language/download/hansard/) (approximately one million pairs of sentences). Other sources of parallel text, such as parallel translations of the Bible (http://benjamin.umd.edu/parallel/) (Resnik 1999) and a collection of Web pages (Resnik, Olsen, and Diab 1999), happened to contain very few occurrence"
J06-2003,P96-1025,0,0.0190393,"Missing"
J06-2003,W99-0613,0,0.056278,"extraction phase, for each sentence (or fragment of a sentence) in CTRW the program will decide which leaf class is expressed, with what strength, and what frequency. We use a decision-list algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDE - STYLE DISTINCTIONS . These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of Yarowsky (1995) on word sense disambiguation. He classified the senses of a word on the basis of other words that the given word co-occurs with. Collins and Singer (1999) classified proper names 1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project. 228 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 4 Example of distinctions extracted from CTRW. as P ERSON, O RGANIZATION, or L OCATION using contextual rules (that rely on other words appearing in the context of the proper names) and spelling rules (that rely on words in the proper names). Starting with a few spelling rules (using some proper-name features) in the decision list, their algorithm learns new contextual rules; using these rules,"
J06-2003,J02-2001,1,0.51899,"orce, floridity, and familiarity (Hovy 1990). Only the first three of these occur in CTRW. A sentence in CTRW expressing stylistic distinctions is this: Assistant and helper are nearly identical except for the latter’s greater informality. Words that signal the degree of formality include formal, informal, formality, and slang. The degree of concreteness is signaled by words such as abstract, concrete, and concretely. Force can be signaled by words such as emphatic and intensification. 1.1.1 The Class Hierarchy of Distinctions. Following the analysis of the distinctions among near-synonyms of Edmonds and Hirst (2002), we derived the class hierarchy of 225 Computational Linguistics Volume 32, Number 2 distinctions presented in Figure 2. The top-level class DISTINCTIONS consists of DENO TATIONAL DISTINCTIONS , ATTITUDE , and STYLE . The last two are grouped together in a class ATTITUDE - STYLE DISTINCTIONS because they are expressed by similar syntactic constructions in the text of CTRW. Therefore the algorithm to be described in Section 2.2 will treat them together. The leaf classes of DENOTATIONAL DISTINCTIONS are S UGGESTION, I MPLICATION, and D ENOTATION; those of ATTITUDE are FAVORABLE, N EUTRAL, and P"
J06-2003,C92-2082,0,0.0306758,"lexical knowledge base from a machine-readable dictionary (MRD) because the information it contains may be incomplete, or it may contain circularities. It is possible to combine information from multiple MRDs or to enhance an existing LKB, they say, although human supervision may be needed. Automatically extracting world knowledge from MRDs was attempted by projects such as MindNet at Microsoft Research (Richardson, Dolan, and Vanderwende 1998), and Barri`erre and Popowich’s (1996) project, which learns from children’s dictionaries. IS-A hierarchies have been learned automatically from MRDs (Hearst 1992) and from corpora (Caraballo [1999] among others). 14 http://www.cl.cam.ac.uk/Research/NL/acquilex/acqhome.html 240 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Research on merging information from various lexical resources is related to the present work in the sense that the consistency issues to be resolved are similar. One example is the construction of Unified Medical Language System (UMLS)15 (Lindberg, Humphreys, and McCray 1993), in the medical domain. UMLS takes a wide range of lexical and ontological resources and brings them together as a single resource. Most"
J06-2003,W02-0909,1,0.41021,"nd familiarity (Hovy 1990). Only the first three of these occur in CTRW. A sentence in CTRW expressing stylistic distinctions is this: Assistant and helper are nearly identical except for the latter’s greater informality. Words that signal the degree of formality include formal, informal, formality, and slang. The degree of concreteness is signaled by words such as abstract, concrete, and concretely. Force can be signaled by words such as emphatic and intensification. 1.1.1 The Class Hierarchy of Distinctions. Following the analysis of the distinctions among near-synonyms of Edmonds and Hirst (2002), we derived the class hierarchy of 225 Computational Linguistics Volume 32, Number 2 distinctions presented in Figure 2. The top-level class DISTINCTIONS consists of DENO TATIONAL DISTINCTIONS , ATTITUDE , and STYLE . The last two are grouped together in a class ATTITUDE - STYLE DISTINCTIONS because they are expressed by similar syntactic constructions in the text of CTRW. Therefore the algorithm to be described in Section 2.2 will treat them together. The leaf classes of DENOTATIONAL DISTINCTIONS are S UGGESTION, I MPLICATION, and D ENOTATION; those of ATTITUDE are FAVORABLE, N EUTRAL, and P"
J06-2003,N01-1001,0,0.0194005,"Missing"
J06-2003,A00-2023,0,0.0574166,"ts our LKB of NS. To implement Xenon, 242 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 9 Lexical analysis and choice in machine translation; adapted from Edmonds and Hirst (2002). The solid lines show the flow of data: input, intermediate representations, and output; the dashed lines show the flow of knowledge from the knowledge sources to the analysis and the generation module. The rectangles denote the main processing modules; the rest of the boxes denote data or knowledge sources. we modified the lexical-choice component of a preexisting NLG system, HALogen (Langkilde 2000; Langkilde and Knight 1998), to handle knowledge about the nearsynonym differences. (Xenon will be described in detail in Section 7.) This required customization of the LKB to the Sensus ontology (Knight and Luk 1994) that HALogen uses as its representation. Customization of the core denotations for Xenon was straightforward. The core denotation of a cluster is a metaconcept representing the disjunction of all the Sensus concepts that could correspond to the near-synonyms in a cluster. The names of metaconcepts, which must be distinct, are formed by the prefix generic, followed by the name of"
J06-2003,W98-1426,0,0.176952,"nly one (or several) of the synonyms of the other word. For example, emotional baggage is a good collocation because baggage and luggage are in the same synset and ∗emotional luggage is not a collocation. Unlike Pearce, we use a combination of t-test and MI, not just frequency counts, to classify collocations. There are two typical approaches to collocations in previous NLG systems: the use of phrasal templates in the form of canned phrases, and the use of automatically extracted collocations for unification-based generation (McKeown and Radev 2000). Statistical NLG systems (such as Nitrogen [Langkilde and Knight 1998]) make good use of the most frequent words and their collocations, but such systems cannot choose a less-frequent synonym that may be more appropriate for conveying desired nuances of meaning if the synonym is not a frequent word. Turney (2001) used mutual information to choose the best answer to questions about near-synonyms in the Test of English as a Foreign Language (TOEFL) and English as a Second Language (ESL). Given a problem word (with or without context) and four alternative words, the question is to choose the alternative most similar in meaning to the problem word (the problem here"
J06-2003,W02-2103,0,0.0403331,"is a semantic representation and a set of preferences to be satisfied. The final output is a set of sentences and their scores. A concrete example of input and output is shown in Figure 13. Note that HALogen may generate some ungrammatical constructs, but they are (usually) assigned lower scores. The first sentence (the highest ranked) is considered to be the solution. 7.1 Metaconcepts The semantic representation input to Xenon is represented, like the input to HALogen, in an interlingua developed at University of Southern California/Information Sciences Institute (USC/ISI).18 As described by Langkilde-Geary (2002b), this language contains a specified set of 40 roles, whose fillers can be either words, concepts from Sensus (Knight and Luk 1994), or complex interlingual representations. The interlingual representations may be underspecified: If some information needed by HALogen is not present, it will use its corpus-derived statistical information to 16 We found that sometimes a rule would extract only a fragment of the expected configuration of concepts but still provided useful knowledge; however, such cases were not considered to be correct in this evaluation, which did not allow credit for partial"
J06-2003,2001.mtsummit-papers.68,0,0.0499307,"Missing"
J06-2003,J03-2001,0,0.00904644,"y Apresjan (2000). An entry includes the following types of differences: semantic, evaluative, associative and connotational, and differences in emphasis or logical stress. These differences are similar to the ones used in our work. Gao (2001) studied the distinctions between near-synonym verbs, more specifically Chinese physical action verbs such as verbs of cutting, putting, throwing, touching, and lying. Her dissertation presents an analysis of the types of semantic distinctions relevant to these verbs, and how they can be arranged into hierarchies on the basis of their semantic closeness. Ploux and Ji (2003) investigated the question of which words should be considered near-synonyms, without interest in their nuances of meaning. They merged clusters of near-synonyms from several dictionaries in English and French and represented them in a geometric space. In our work, the words that are considered near-synonyms are taken from CTRW; a different dictionary of synonyms may present slightly different views. For example, a cluster may contain some extra words, some missing words, or sometimes the clustering could be done in a different way. A different approach is to automatically acquire near-synonym"
J06-2003,P99-1068,0,0.0118209,"ion experiments. In order to use this tool, we parsed the English sentences with Charniak’s parser (Charniak 2000).22 The tool was designed to work on parse trees from the Penn TreeBank, which have some extra annotations; it worked on parse trees produced by Charniak’s parser, but it failed on some parse trees probably more often than it 21 The sentences were obtained from USC/ISI (http://www.isi.edu/natural-language/download/hansard/) (approximately one million pairs of sentences). Other sources of parallel text, such as parallel translations of the Bible (http://benjamin.umd.edu/parallel/) (Resnik 1999) and a collection of Web pages (Resnik, Olsen, and Diab 1999), happened to contain very few occurrences of the near-synonyms of interest. 22 ftp://ftp.cs.brown.edu/pub/nlparser/ 251 Computational Linguistics Volume 32, Number 2 Figure 19 Examples of parallel sentences used in Experiment 2. Figure 20 The architecture of Experiment 2 (French to English). did in HALogen’s evaluation experiments. We replaced each near-synonym with the metaconcept that is the core meaning of its cluster. The interlingual representation for the English sentence is semantically shallow; it does not reflect the meanin"
J06-2003,P98-2180,0,0.0747484,"Missing"
J06-2003,P95-1026,0,0.0787679,"is to learn a set of words and expressions from CTRW—that is, extraction patterns—that characterizes descriptions of the class. Then, during the extraction phase, for each sentence (or fragment of a sentence) in CTRW the program will decide which leaf class is expressed, with what strength, and what frequency. We use a decision-list algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDE - STYLE DISTINCTIONS . These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of Yarowsky (1995) on word sense disambiguation. He classified the senses of a word on the basis of other words that the given word co-occurs with. Collins and Singer (1999) classified proper names 1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project. 228 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 4 Example of distinctions extracted from CTRW. as P ERSON, O RGANIZATION, or L OCATION using contextual rules (that rely on other words appearing in the context of the proper names) and spelling rules (that rely on words in the proper names)."
J06-2003,1993.eamt-1.2,0,\N,Missing
J06-2003,J98-3003,0,\N,Missing
J06-2003,J90-1003,0,\N,Missing
J06-2003,J93-1007,0,\N,Missing
J06-2003,W02-0907,0,\N,Missing
J06-2003,C94-1049,0,\N,Missing
J06-2003,P02-1040,0,\N,Missing
J06-2003,P01-1025,0,\N,Missing
J06-2003,C98-2175,0,\N,Missing
J06-2003,P98-2181,0,\N,Missing
J06-2003,C98-2176,0,\N,Missing
J13-3004,J90-1003,0,0.118002,"onyms set. We will refer to the common terms (agitation in this example) as the focus words. Because we also wanted to compare occurrence statistics of the high-contrast set with the random pairs set, we created the control set of random pairs by taking each of the focus words and pairing them with another word in WordNet that has a frequency of occurrence in BNC closest to the term contrasting with the focus word. This is to ensure that members of the pairs across the high-contrast set and the control set have similar unigram frequencies. We calculated the pointwise mutual information (PMI) (Church and Hanks 1990) for each of the word pairs in the high-contrast set, the random pairs set, and the synonyms set using unigram and co-occurrence frequencies in the BNC. If two words occurred within a window of five adjacent words in a sentence, they were marked as co-occurring (same window as Church and Hanks [1990] used in their seminal work on word–word associations). Table 7 shows the average and standard deviation in each set. 7 If both members of a pair have WordNet synonyms, then one is chosen at random, and its synonym is taken. 8 WordNet lists synonyms in order of decreasing frequency in the SemCor co"
J13-3004,P08-1118,0,0.0607305,"Missing"
J13-3004,P97-1023,0,0.224847,"tors for each using dictionary definitions. The approach was evaluated on only a handful of word pairs. There is a large amount of work on sentiment analysis and opinion mining aimed at determining the polarity of words (Pang and Lee 2008). For example, Pang, Lee, and Vaithyanathan (2002) detected that adjectives such as dazzling, brilliant, and gripping cast their qualifying nouns positively whereas adjectives such as bad, cliched, and boring portray the noun negatively. Many of these gradable adjectives have opposites, but these approaches, with the exception of that of Hatzivassiloglou and McKeown (1997), did not attempt to determine pairs of positive and negative polarity words that are opposites. Hatzivassiloglou and McKeown proposed a supervised algorithm that uses word usage patterns to generate a graph with adjectives as nodes. An edge between two nodes indicates either that the two adjectives have the same or opposite polarity. A clustering algorithm then partitions the graph into two subgraphs such that the nodes in a subgraph have the same polarity. They used this method to create a lexicon of positive and negative words, and argued that the method could also be used to detect opposit"
J13-3004,C92-2082,0,0.0872931,"in GRE “most contrasting word” questions are not listed as antonyms in WordNet. We should not infer from this that WordNet or any other lexicographic resource is a poor source for detecting opposites, but rather that identifying the large number of contrasting word pairs requires further computation, possibly relying on other semantic relations stored in the lexicographic resource. Even though a number of computational approaches have been proposed for semantic closeness (Curran 2004; Budanitsky and Hirst 2006), and some for hypernymy– 556 Mohammad et al. Computing Lexical Contrast hyponymy (Hearst 1992), measures of lexical contrast have been less successful. To some extent, this is because lexical contrast is not as well understood as other classical lexical– semantic relations. Over the years, many definitions of semantic contrast and opposites have been proposed by linguists (Lehrer and Lehrer 1982; Cruse 1986), cognitive scientists (Kagan 1984), psycholinguists (Deese 1965), and lexicographers (Egan 1984), which differ from each other in various respects. Cruse (1986, page 197) observes that even though people have a robust intuition of opposites, “the overall class is not a well-defined"
J13-3004,J91-1001,0,0.858055,"entary pairs, but poorly on disjoint opposite pairs. Among different parts of speech, the method performs best on noun pairs, and relatively worse on verb pairs. All of the data created and compiled as part of this research are summarized in Table 18 (Section 8), and is available for download.3 2. Related Work Charles and Miller (1989) proposed that opposites occur together in a sentence more often than chance. This is known as the co-occurrence hypothesis. Paradis, Willners, and Jones (2009) describe further experiments to show how canonical opposites tend to have high textual co-occurrence. Justeson and Katz (1991) gave evidence in support of the hypothesis using 35 prototypical opposites (from an original set of 39 opposites compiled by Deese [1965]) and also with an additional 22 frequent opposites. They also showed that opposites tend to occur in parallel syntactic constructions. All of these pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb, adjective, and adverb pairs (noun–noun, noun–verb, noun–adjective, verb–adverb, etc.) pertaining to 18 concepts (for example, lose(v)–gain(n) and loss(n)–gain(n), where lose(v) and loss(n) pertain to the concept of “failing to"
J13-3004,P98-2127,0,0.762129,"Word Pairs in Text As pointed out earlier, there is work on a small set of opposites showing that opposites co-occur more often than chance (Charles and Miller 1989; Fellbaum 1995). Section 5.1 describes experiments on a larger scale to determine whether highly contrasting word pairs (including opposites) occur together more often than randomly chosen word pairs of similar frequency. The section also compares co-occurrence associations with synonyms. Research in distributional similarity has found that entries in distributional thesauri tend to also contain terms that are opposite in meaning (Lin 1998; Lin et al. 2003). Section 5.2 describes experiments to determine whether highly contrasting word pairs (including opposites) occur in similar contexts as often as randomly chosen pairs of words with similar frequencies, and whether highly contrasting words occur in similar contexts as often as synonyms. 5.1 Co-Occurrence In order to compare the tendencies of highly contrasting word pairs, synonyms, and random word pairs to co-occur in text, we created three sets of word pairs: the highcontrast set, the synonyms set, and the control set of random word pairs. The high-contrast set was created"
J13-3004,P02-1047,0,0.0671634,"Missing"
J13-3004,W11-2128,0,0.0852512,"Missing"
J13-3004,H05-1067,0,0.027602,"Missing"
J13-3004,D09-1063,1,0.520022,"Missing"
J13-3004,D07-1060,1,0.643146,"r, that as of February 2012, most of the Mechanical Turk participants are native speakers of English, certain Indian languages, and some European languages. Our future goals include porting this approach to a cross-lingual framework to determine lexical contrast in a resource-poor language by using a bilingual lexicon to connect the words in that language with words in another resource-rich language. We can then use the structure of the thesaurus from the resource-rich language as described in this article to detect contrasting categories of terms. This is similar to the approach described by Mohammad et al. (2007), who compute semantic distance in a resourcepoor language by using a bilingual lexicon and a sense disambiguation algorithm to connect text in the resource-poor language with a thesaurus in a different language. This enables automatic discovery of lexical contrast in a language even if it does not have a Roget-like thesaurus. The cross-lingual method still requires a bilingual lexicon to map words between the target language and the language with the thesaurus, however. 587 Computational Linguistics Volume 39, Number 3 Our method used only one Roget-like published thesaurus, but even more gai"
J13-3004,W02-1011,0,0.021522,"Missing"
J13-3004,C02-1061,0,0.33566,"Missing"
J13-3004,C08-1114,1,0.717504,"sites and synonyms? How does the proposed method perform when compared with other automatic methods? Experiments: We conduct three experiments (described in Sections 7.1, 7.2, and 7.3) involving three different data sets and two tasks to answer 2 Note that though linguists have classified opposites into different kinds, we know of no work doing so for contrasts more generally. Thus this particular analysis must be restricted to opposites alone. 559 Computational Linguistics Volume 39, Number 3 these questions. We compare performance of our method with methods proposed by Lin et al. (2003) and Turney (2008). We automatically generate a new set of 1,296 “most contrasting word” questions to evaluate performance of our method on five different kinds of opposites and across four parts of speech. (The evaluation described in Section 7.1 was first described in Mohammad, Dorr, and Hirst [2008].) Findings: We find that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods. Our method performs best on gradable pairs, antipodal pairs, and complementary pairs, but poorly on disjoint opposite pairs. Among different parts of speech, the method perf"
J13-3004,P08-1008,0,0.0266766,"Missing"
J13-3004,D08-1103,1,\N,Missing
J13-3004,J06-1003,1,\N,Missing
J13-3004,C98-2122,0,\N,Missing
J90-2007,C88-2142,0,0.0126975,"panding the scope of work in this way by no means requires any logical representation to be chosen as the main object of research rather than the words themselves. On the contrary, a crosslinguistic investigation of translation equivalences can, in my view, profit from an approach like Moeschler's. Part 1 of the book analyzes the logical content of connectors, Part 2 discusses their function as markers of pertinence and, indirectly, of coherence, and Part 3 suggests a tree-structured discourse representation. This representation might be seen as one of many (for an overview of text models see Papegaaij and Schubert 1988:13-14), if it were not for the specific view on connectors that underlies Moeschler's model. With the exception of a brief reference to scripts, the book at first sight does not seem to have any direct link to computational linguistics. Nevertheless, I am convinced that a study that approaches language at the grammatical and pragmatic level and leads toward a linguistically motivated formalization is worth the attention of computational linguists. I feel that materials of this kind are needed on a much larger scale than is currently available, to let computational linguists' skills in formali"
J90-2007,C88-2125,0,0.0212483,"Missing"
J91-1002,J81-4005,0,0.0695487,"Missing"
J91-1002,J86-3001,0,\N,Missing
J91-2004,J80-3003,0,0.146502,"Missing"
J91-2004,J86-4002,0,0.0197438,"prompt to the user, usually a character such as &apos;,&apos; or &apos;&gt;&apos;. Neither side can interrupt the other. The suggestion of this book is that this is not how things should be. Since elements like turn-taking cues and interruptions are so important in human conversation, if we want to build natural, flexible interfaces, these matters should also be an issue in HCI. Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in HCI on recovery from misunderstandings with that in CA. The HCI research emphasizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman 1986), while CA emphasizes the conversationalpatterns (e.g., Sacks et al. 1974) and the active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs 1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy. 2. The Intellectual Background of Conversation Analysis Conversation Analysis follows the tenets of ethnomethodology, a radical movement that developed in sociology in the e"
J91-2004,P83-1023,0,0.0648214,"as evidence in the debate in the first. 3.1 The Advice System The chapter that is perhaps most convincingly pro-CA is the one by David Frohlich and Paul Luff that describes a CA-inspired interface to an advice system. The advice system, which is named Advice System, gives information and assistance on British government welfare benefits, and is intended for use directly by clients. The interface is primarily mouse-driven, with the user clicking on &apos;buttons&apos; on the screen, or picking up phrases from menus to assemble natural language sentences, rather like the Texas Instruments NLMenu system (Tennant et al. 1983a, 1983b). Unfortunately, the authors are fuzzy about many of the details, and it&apos;s not clear whether there is also direct keyboard input. (Page 190 and Figure 9.1 suggest that there isn&apos;t, yet page 207 and Tables 9.5 and 9.7 suggest that the user might make spelling errors.) It&apos;s also unclear whether the input menus are dynamically generated from a true grammar and lexicon, as in NLMenu, or whether they are just a static, pre-determined selection of useful fragments. Likewise, the authors don&apos;t say whether the Advice System&apos;s responses are just pre-stored text or dynamically generated. (It&apos;s"
J93-3002,P92-1048,0,0.0200806,". 25 Another type of structure that seems to work against cohesion is a disruption in normal linear ordering, as the syntactic inversion at the end of the now-familiar text, And the rains descended, and the floods came, and the winds blew, and beat upon that house; and it fell: and great was the fall of it. However, we have not yet incorporated this feature into our formalization of style. 26 Manchester Guardian Weekly, 21 February 1988, p. 14. 27 Quirk et al. (1985, p. 1314). 28 We do not use here the full range of connectivity that we defined above; however, some extensions of the theory by Green (1992a, 1992b) do. 29 Quirk et al. (1985, p. 1295). 471 Computational Linguistics Volume 19, Number 3 Halliday distinguishes two types of subordination: embedding (or rank-shifting) and hypotaxis. These phenomena will be used to classify the hierarchic primitive elements in our stylistic grammar. Embedding is described as: the 'rank shift' by which a clause or phrase comes to function within the structure of a group, like who came to dinner in the man who came to dinner. (Halliday 1985, p. 219) There are various types of embeddings, including prepositional phrases, as in John might arrive as early"
J93-3002,J90-2001,0,0.0267728,"eneral, Kane's rules appear to be just an enumeration of stylistic tenets, not a deliberately constructed organization. Nevertheless, we have found it useful to draw upon these rules as justification for the syntactic coverage of our grammar. In computational stylistics, an application area of particular interest to us is machine translation (MT). In suggesting the applicability of computational stylistics to MT, Loffier-Laurian (1986) emphasized an important point: although corresponding group styles may exist across languages, the realization of the style can be different for each language. Tsutsumi (1990) presented a methodology for bridging stylistic gaps (stylistic differences) between the syntax of the source and target languages in MT. An important contribution of Tsutsumi's work is the recognition that computational stylistics is useful for the translation of pairs of languages that are not in the same language group. Hovy's (1988) PAULINE system was the first computational system that implemented goal-directed style. PAULINEwas able to generate text that conformed to various stylistic and pragmatic constraints that it was given. The system was goal-directed, able to correlate such stylis"
J95-3003,P91-1050,1,0.235483,"Missing"
J95-3003,P89-1009,0,0.0473219,"lization of referring actions. Although acts of reference have been incorporated into plan-based models, determining the content of referring expressions hasn&apos;t been. For instance, in Appelt&apos;s model, concept activations can be achieved by the action describe, which is a primitive, not further decomposed. Rather, this action has an associated procedure that determines a description that satisfies the preconditions of describe. Such special procedures have been the mainstay for accounting for the content of referring expressions, both in constructing and in understanding them, as exemplified by Dale (1989), who chose descriptors on the basis of their discriminatory power; Ehud Reiter (1990), who focused 1 For simplicity, we have not shown the change in speakers between refashionings and judgments. 354 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions on avoiding misleading conversational irnplicatures when generating descriptions; and Mellish (1985), who used a constraint satisfaction algorithm to identify referents. Our work follows the plan-based approach to language generation and understanding. We extend the earlier approaches of Cohen and Appelt by accounting for the"
J95-3003,C94-2182,0,0.0770634,"Missing"
J95-3003,P85-1026,0,0.120699,"Missing"
J95-3003,P91-1007,0,0.106986,"Missing"
J95-3003,J95-4001,1,0.88103,"Missing"
J95-3003,P90-1010,0,0.125514,"Missing"
J95-3003,P90-1013,0,\N,Missing
J95-4001,P79-1021,0,0.419871,"erstanding, the communication might mislead them into prematurely believing that their goals have been achieved. • The need for an alternative to the notion of mutual belief. Typically, models rely on mutual beliefs without accounting for how speakers achieve them or for why speakers should believe that they have achieved them. 2.1 Using social conventions to guide interpretation and repair Our account of interpretation avoids the extended inference required by plan-based models by reversing the standard dependency between an agent&apos;s expectations and task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Carberry 1990; Lambert and Carberry 1991) start by applying context-independent inference rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that extend a previously recognized plan. By contrast, our approach begins with an expectation, using it to premise both the analysis of utterance meaning and any inference 439 Computational Linguistics Volume 21, Number 4 about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expectations as meaningful; for example, if an utterance is inconsistent with expectations, then the reasoner will"
J95-4001,P88-1011,0,0.029258,"r lend money. A: No, I meant to offer you one. B: Oh. Thanks. A: Bye. Metaplan type Plan adoption Acceptance Challenge Repair Repair Closing Figure 3 Examples of different types of coherence strategies. terpretations of utterances fincluding recognizing misunderstanding) correspond to abductive inference over the theory. Definition 1 Given a theory T and a goal proposition G, we say that one can assumptions A from ~ if T U A ~ G and T U A is consistent. abduce a set of Abduction has been applied to the solution of local pragmatics problems (Hobbs et al. 1988, 1993) and to story understanding (Charniak and Goldman 1988). The model incorporates five strategies, or metaplans, for generating coherent utterances: plan adoption, acceptance, challenge, repair, and closing (the model treats opening as a kind of plan adoption). Figure 3 contains a conversation that includes an example for each of the five types. In plan adoption, speakers simply choose an action that can be expected to achieve a desired illocutionary goal, given social norms and the discourse context. (The goal itself must originate within the speaker&apos;s non-linguistic planning mechanism.) The first utterance in the figure is a plan adoption. The sec"
J95-4001,P85-1007,0,0.0475193,"Missing"
J95-4001,C94-2182,0,0.0391088,"Our colleagues Heeman and Edmonds have looked at the repair of non-understanding. The difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no w a y to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of th"
J95-4001,J86-1002,0,0.0312114,"the following types of responses: 1. A statement about background knowledge that might be needed. 2. A statement about the underlying purpose of A. 3. A statement about related task steps (i.e., subgoals of A, tasks that contain A as a step, or tasks that might follow A). 4. A statement about the accomplishment of A. These expectations are independent of the belief state of an agent and are specified down to the semantic (and sometimes even lexical) level. This information has long been used to discriminate between ambiguous interpretations and correct mistakes made by the speech recognizer (Fink and Biermann 1986; Smith 1992). Typically, an utterance will be interpreted according to the expectation that matches it most closely. By contrast, our approach and that of the plan-based accounts use &quot;expectation&quot; to refer to agents&apos; beliefs about how future utterances might relate to prior ones. These expectations are determined both by an agent&apos;s understanding of typical behavior and by his or her mental state. These two notions of expectation are complementary, and any dialogue model that uses speech as input must be able to represent and reason with both. 5.3 Approaches to misconception Misconceptions are"
J95-4001,P85-1026,0,0.164916,"entionally evoke a concept or relation. To prevent misconceptions from triggering a misunderstanding, agents can check for evidence of misconception and try to resolve apparent errors. The symptoms of misconception include references to entities that do not map to previously known objects or operations (Webber and Mays 1983) or requests for clarification (Moore 1989). Errors are corrected by replacing or deleting parts of the problematic utterance so that it makes sense. Several correction strategies have been suggested: • Generalize a description by selectively ignoring some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &quot;sibling&quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible interpretations could cue some type of repair mechanism, if they chose to incorporate one. 45 This is the same sense of &quot;expectation&quot; as used by Riesbeck (1974). 467 Computational Li"
J95-4001,J95-3003,1,0.897582,"ning framework: Prioritized Theorist The model has been formulated using the Prioritized Theorist framework (Poole, Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supports both default and abductive reasoning. Theorist typifies what is known as a &quot;proof12 Non-understanding, which entails non-acceptance (or deferred acceptance), is signaled by second-turn repair. This type of repair will not be considered here. 13 Other misunderstandings are possible; for example there can be disagreement about what object a speaker is trying to identify with a referring expression (cf. Heeman and Hirst 1995; Hirst et al. 1994). 14 This distinction is similar to the one made by Luperfoy (1992). 15 For present purposes, we also assume that the complete model is accessible to the hearer; one could better simulate the limitations of working memory by limiting access to only the most recent utterances. 443 Computational Linguistics Volume 21, Number 4 b a s e d a p p r o a c h &quot; to abduction because it relies on a t h e o r e m p r o v e r to collect the a s s u m p t i o n s that w o u l d be n e e d e d to p r o v e a given set of observations a n d to verify their consistency. O u r reasoning algo"
J95-4001,P88-1012,0,0.00867287,"6 Example A: Do you have a quarter? B: No. B: I never lend money. A: No, I meant to offer you one. B: Oh. Thanks. A: Bye. Metaplan type Plan adoption Acceptance Challenge Repair Repair Closing Figure 3 Examples of different types of coherence strategies. terpretations of utterances fincluding recognizing misunderstanding) correspond to abductive inference over the theory. Definition 1 Given a theory T and a goal proposition G, we say that one can assumptions A from ~ if T U A ~ G and T U A is consistent. abduce a set of Abduction has been applied to the solution of local pragmatics problems (Hobbs et al. 1988, 1993) and to story understanding (Charniak and Goldman 1988). The model incorporates five strategies, or metaplans, for generating coherent utterances: plan adoption, acceptance, challenge, repair, and closing (the model treats opening as a kind of plan adoption). Figure 3 contains a conversation that includes an example for each of the five types. In plan adoption, speakers simply choose an action that can be expected to achieve a desired illocutionary goal, given social norms and the discourse context. (The goal itself must originate within the speaker&apos;s non-linguistic planning mechanism.)"
J95-4001,P91-1007,0,0.155033,"prematurely believing that their goals have been achieved. • The need for an alternative to the notion of mutual belief. Typically, models rely on mutual beliefs without accounting for how speakers achieve them or for why speakers should believe that they have achieved them. 2.1 Using social conventions to guide interpretation and repair Our account of interpretation avoids the extended inference required by plan-based models by reversing the standard dependency between an agent&apos;s expectations and task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Carberry 1990; Lambert and Carberry 1991) start by applying context-independent inference rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that extend a previously recognized plan. By contrast, our approach begins with an expectation, using it to premise both the analysis of utterance meaning and any inference 439 Computational Linguistics Volume 21, Number 4 about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expectations as meaningful; for example, if an utterance is inconsistent with expectations, then the reasoner will try to explain the inconsistency. The model focuses on"
J95-4001,P86-1033,0,0.207383,"tion might mislead them into prematurely believing that their goals have been achieved. • The need for an alternative to the notion of mutual belief. Typically, models rely on mutual beliefs without accounting for how speakers achieve them or for why speakers should believe that they have achieved them. 2.1 Using social conventions to guide interpretation and repair Our account of interpretation avoids the extended inference required by plan-based models by reversing the standard dependency between an agent&apos;s expectations and task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Carberry 1990; Lambert and Carberry 1991) start by applying context-independent inference rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that extend a previously recognized plan. By contrast, our approach begins with an expectation, using it to premise both the analysis of utterance meaning and any inference 439 Computational Linguistics Volume 21, Number 4 about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expectations as meaningful; for example, if an utterance is inconsistent with expectations, then the reasoner will try to expla"
J95-4001,P92-1004,0,0.0187161,"rist framework (Poole, Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supports both default and abductive reasoning. Theorist typifies what is known as a &quot;proof12 Non-understanding, which entails non-acceptance (or deferred acceptance), is signaled by second-turn repair. This type of repair will not be considered here. 13 Other misunderstandings are possible; for example there can be disagreement about what object a speaker is trying to identify with a referring expression (cf. Heeman and Hirst 1995; Hirst et al. 1994). 14 This distinction is similar to the one made by Luperfoy (1992). 15 For present purposes, we also assume that the complete model is accessible to the hearer; one could better simulate the limitations of working memory by limiting access to only the most recent utterances. 443 Computational Linguistics Volume 21, Number 4 b a s e d a p p r o a c h &quot; to abduction because it relies on a t h e o r e m p r o v e r to collect the a s s u m p t i o n s that w o u l d be n e e d e d to p r o v e a given set of observations a n d to verify their consistency. O u r reasoning algorithm is b a s e d on Poole&apos;s i m p l e m e n t a t i o n of Theorist, which w e extend"
J95-4001,P86-1016,0,0.0800005,"Missing"
J95-4001,J88-3005,0,0.0615144,"Missing"
J95-4001,J80-3003,0,0.305006,"ir compatibility by verifying that each of the conjuncts of each supposition is compatible. (In the system, this is implemented as a special predicate, inconsistentLI). There is a danger in treating compatibility as a default in that one might miss some intuitively incompatible cases and hence some misunderstandings might not be detectable. An alternative would be to base compatibility on the notion of consistency in the underlying logic, if a complete logic has been defined. TM 3.2.2 Speech acts. For simplicity, we represent utterances as surface-level speech acts in the manner first used by Perrault and Allen (1980). 19Following Cohen and Levesque (1985), we limit the surface language to the acts surface-request, surface-inform, surface-informref, and surface-informif. Example 3 shows the representation of the literal form of Example 2, the fourth-turn repair example. (We abbreviate &quot;m&quot; for &quot;Mother&quot;, &quot;r&quot; for &quot;Russ&quot;, and &quot;whoIsGoing&quot; for &quot;who&apos;s going&quot;.) Example 3 T1 m: surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))) T2 r: surface-request(r, m, informref(m, r, whoIsGoing)) T3 m: surface-inform(m, r, not knowref(m, whoIsGoing)) T4 r: surface-informref(r, m, whoIsGoing) We assume that such form"
J95-4001,P86-1032,0,0.404878,"fference between misunderstanding and misconception The notions of misunderstanding and misconception are easily confounded, so we shall begin by explicating the distinction. Misconceptions are errors in the prior knowledge of a participant; for example, believing that Canada is one of the United States. • Departmentof Electrical Engineeringand ComputerScience,Milwaukee,WI 53201, mcroy@cs.uwm.edu t Departmentof ComputerScience,Toronto,Canada M5S 1A4,gh@cs.toronto.edu @ 1995 Associationfor ComputationalLinguistics Computational Linguistics Volume 21, Number 4 McCoy (1989), Calistri-Yeh (1991), Pollack (1986b), Pollack (1990), and others have studied the problem of how one participant can determine the misconceptions of another during a conversation (see Section 5.3 below). Typically such errors can be recognized immediately when an expression is not interpretable with respect to the computer&apos;s (presumedly perfect!) knowledge of the world. By contrast, a participant is not aware, at least initially, when misunderstanding has occurred. In misunderstanding, a participant obtains an interpretation that she believes is complete and correct, but which is, however, not the one that the other participan"
J95-4001,P95-1016,0,0.0698401,"it at 6pm might create an expectation that dinner will be served, it does not express an intention to serve it. 7 In the figure, we have used the symbol intend to n a m e both the intention to achieve a situation in which a property holds and the intention to do action. 8 Schegloff actually argues against representing such sequences as speech acts; however, as in the computational work cited above, we have used the notion of &quot;discourse-level speech act&quot; to represent the functional relationship between the surface form of an utterance, the context, and the attitudes expressed by the speaker. 9 Reithinger and Maier (1995) have used n-gram dialogue act probabilities to induce the adjacency pairs from a corpus of dialogues for appointment scheduling. 10 Communication can occur despite such differences because speakers with similar linguistic experiences presumably will develop similar expectations about how discourse works. Differences in expectations might very well be one thing that new acquaintances m u s t resolve in order to avoid social conflict. 440 McRoy and Hirst Act type informative inquisitive requestive The Repair of Speech Act Misunderstandings Speech act name assert(S, H, P) assertref(S, H, P) asse"
J95-4001,J88-3003,0,\N,Missing
J95-4001,P94-1001,0,\N,Missing
J95-4001,C92-1054,0,\N,Missing
J95-4001,P92-1025,0,\N,Missing
N13-1078,C10-2011,1,0.959506,"s and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used fo"
N13-1078,1996.amta-1.36,0,0.460965,"ons of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 2.1 Model Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fix"
N13-1078,P09-1080,0,0.0546568,"sions. 1 Introduction In language, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classifi"
N13-1078,J06-2003,1,0.847436,"style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on c"
N13-1078,D07-1115,0,0.0165703,"used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 2.1 Model Ling"
N13-1078,P97-1005,0,0.675574,"Missing"
N13-1078,D09-1094,0,0.050573,"syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we"
N13-1078,W11-0711,0,0.119565,"factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of"
N13-1078,J11-2004,0,0.0157794,"ationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style"
N13-1078,P11-1077,0,0.0495338,"nguage, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounde"
N13-1078,J11-2001,1,0.277706,"rary seeds were primarily drawn from web sites which explain difficult language in texts such as the Bible and Lord of the Rings; examples include behold, resplendent, amiss, and thine. The concrete seeds all denote objects and actions strongly rooted in the physical world, e.g. shove and lamppost, while the abstract seeds all involve concepts which require significant human psychological or cultural knowledge to grasp, for instance patriotism and nonchalant. For our subjective seeds, we used an edited list of strongly positive and negative terms from a manually-constructed sentiment lexicon (Taboada et al., 2011), e.g. gorgeous and depraved, and for our objective set we selected words from sets of nearsynonyms where one was clearly an emotionallydistant alternative, e.g. residence (for home), jocular (for funny) and communicable (for contagious). We filtered initial lists to 150 of each type, removing words which did not appear in the corpus or which occurred in multiple lists. For evaluation we 675 used stratified 3-fold crossvalidation, averaged over 5 different (3-way) splits of the seeds, with the same splits used for all evaluated conditions. Given two sets of opposing seeds, we follow our earlie"
N13-1078,P02-1053,0,0.00462185,"l., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particu"
N13-1078,N10-1119,0,0.0809289,"for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 2.1 Model Linguistic foundations In Engl"
N13-1078,H05-1044,0,0.0225289,"lationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models t"
N15-1087,W13-3909,1,0.81437,"2011), frontotemporal lobar degeneration (Pakhomov et al., 2010b), primary progressive aphasia (Fraser et al., 2014), or Alzheimer’s disease (Orimaye et al., 2014; Thomas et al., 2005). These studies used manually transcribed samples of patient speech; however, it is clear that for such systems to be practical in the real world they must use automatic speech recognition (ASR). One issue that arises with ASR is the introduction of word recognition errors: insertions, deletions, and substitutions. This problem as it relates to impaired speech has been considered elsewhere (Jarrold et al., 2014; Fraser et al., 2013; Rudzicz et al., 2014), although more work is needed. Another issue, which we address here, is how ASR transcripts are divided into sentences. The raw output from an ASR system is generally a stream of words, as shown in Figure 1. With some effort, it can be transformed into a format which is more readable by both humans and machines. Many algorithms exist for the segmentation of the raw text stream into sentences. However, there has been no previous work on how those algorithms might be applied to impaired speech. This problem must be addressed for two reasons: first, sentence boundaries are"
N15-1087,A97-1003,0,0.248549,"Missing"
N15-1087,W14-3204,0,0.0297553,"irment (Roark et al., 2011), frontotemporal lobar degeneration (Pakhomov et al., 2010b), primary progressive aphasia (Fraser et al., 2014), or Alzheimer’s disease (Orimaye et al., 2014; Thomas et al., 2005). These studies used manually transcribed samples of patient speech; however, it is clear that for such systems to be practical in the real world they must use automatic speech recognition (ASR). One issue that arises with ASR is the introduction of word recognition errors: insertions, deletions, and substitutions. This problem as it relates to impaired speech has been considered elsewhere (Jarrold et al., 2014; Fraser et al., 2013; Rudzicz et al., 2014), although more work is needed. Another issue, which we address here, is how ASR transcripts are divided into sentences. The raw output from an ASR system is generally a stream of words, as shown in Figure 1. With some effort, it can be transformed into a format which is more readable by both humans and machines. Many algorithms exist for the segmentation of the raw text stream into sentences. However, there has been no previous work on how those algorithms might be applied to impaired speech. This problem must be addressed for two reasons: first, se"
N15-1087,P03-1054,0,0.0159771,"Missing"
N15-1087,P05-1056,0,0.0240959,"cally segmented text, and compare the feature values with measurements taken on manually segmented text. We assess which features are most robust to the noisy segmentation, and thus could be appropriate features for future work on automatic diagnostic interfaces. 2 2.1 Background Automatic sentence segmentation Many approaches to the problem of segmenting recognized speech have been proposed. One popular way of framing the problem is to treat it as a sequence tagging problem, where each interword boundary must be labelled as either a sentence boundary (B) or not (NB) (Liu and Shriberg, 2007). Liu et al. (2005) showed that using a conditional random field (CRF) classifier for this problem resulted in a lower error rate than using a hidden Markov model or maximum entropy classifier. They stated that the CRF approach combined the benefits of these two other popular approaches, since it is discriminative, can handle correlated features, and uses a globally optimal sequence decoding. The features used to train such classifiers fall broadly into two categories: word features and 863 prosodic features. Word features can include word or part-of-speech n-grams, keyword identification, and filled pauses (Ste"
N15-1087,W14-3210,0,0.100481,"rs will get a live, unfiltered view of them together. Figure 1: ASR text before and after processing. Introduction The automatic analysis of speech samples is a promising direction for the screening and diagnosis of cognitive impairments. For example, recent studies have shown that machine learning classifiers trained on speech and language features can detect, with reasonably high accuracy, whether a speaker has mild cognitive impairment (Roark et al., 2011), frontotemporal lobar degeneration (Pakhomov et al., 2010b), primary progressive aphasia (Fraser et al., 2014), or Alzheimer’s disease (Orimaye et al., 2014; Thomas et al., 2005). These studies used manually transcribed samples of patient speech; however, it is clear that for such systems to be practical in the real world they must use automatic speech recognition (ASR). One issue that arises with ASR is the introduction of word recognition errors: insertions, deletions, and substitutions. This problem as it relates to impaired speech has been considered elsewhere (Jarrold et al., 2014; Fraser et al., 2013; Rudzicz et al., 2014), although more work is needed. Another issue, which we address here, is how ASR transcripts are divided into sentences."
N15-1087,W14-1904,0,0.0181175,"lobar degeneration (Pakhomov et al., 2010b), primary progressive aphasia (Fraser et al., 2014), or Alzheimer’s disease (Orimaye et al., 2014; Thomas et al., 2005). These studies used manually transcribed samples of patient speech; however, it is clear that for such systems to be practical in the real world they must use automatic speech recognition (ASR). One issue that arises with ASR is the introduction of word recognition errors: insertions, deletions, and substitutions. This problem as it relates to impaired speech has been considered elsewhere (Jarrold et al., 2014; Fraser et al., 2013; Rudzicz et al., 2014), although more work is needed. Another issue, which we address here, is how ASR transcripts are divided into sentences. The raw output from an ASR system is generally a stream of words, as shown in Figure 1. With some effort, it can be transformed into a format which is more readable by both humans and machines. Many algorithms exist for the segmentation of the raw text stream into sentences. However, there has been no previous work on how those algorithms might be applied to impaired speech. This problem must be addressed for two reasons: first, sentence boundaries are important when analyzi"
N15-1087,A00-1012,0,0.0966366,"05) showed that using a conditional random field (CRF) classifier for this problem resulted in a lower error rate than using a hidden Markov model or maximum entropy classifier. They stated that the CRF approach combined the benefits of these two other popular approaches, since it is discriminative, can handle correlated features, and uses a globally optimal sequence decoding. The features used to train such classifiers fall broadly into two categories: word features and 863 prosodic features. Word features can include word or part-of-speech n-grams, keyword identification, and filled pauses (Stevenson and Gaizauskas, 2000; Stolcke and Shriberg, 1996; Gavalda et al., 1997). Prosodic features include measures of pitch, energy, and duration of phonemes around the boundary, as well as the length of the silent pause between words (Shriberg et al., 2000; Wang et al., 2003). The features which are most discriminative to the segmentation task can change depending on the nature of the speech. One important factor can be whether the speech is prepared or spontaneous. Cuendet et al. (2007) explored three different genres of speech: broadcast news, broadcast conversations, and meetings. They analyzed the effectiveness of"
N15-1115,J08-1001,0,0.310993,"parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents. 1 Introduction In a well-written document, sentences are organized and presented in a logical and coherent form, which makes the text fluent and easily understood. Therefore, coherence is a fundamental aspect of high text quality, and the evaluation of coherence is a crucial component of many NLP applications, such as essay scoring (Miltsakaki and Kukich, 2004), story generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗"
N15-1115,P08-2011,0,0.0781101,"Missing"
N15-1115,P11-2022,0,0.66887,"kaki and Kukich, 2004), story generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common e"
N15-1115,D11-1142,0,0.0124592,"trieve all world knowledge related to d. There are two major issues for this process: (1) knowledge sources: where can we obtain this knowledge?, and (2) knowledge selection: how do we pinpoint the most relevant ones? Knowledge sources There are two main kinds of knowledge sources: (1) manually edited knowledge 1089 bases, such as YAGO (Hoffart et al., 2013), which consists of about 4 million human-edited instances from on-line encyclopedias such as WikiPedia (Denoyer and Gallinari, 2007) and FreeBase (Bollacker et al., 2008), and (2) automatically constructed knowledge bases, such as Reverb (Fader et al., 2011), which covers about 20 million instances extracted from raw texts. Generally speaking, manually edited knowledge bases have better accuracy but lower coverage, while automatically extracted knowledge bases are the opposite. To seek a good balance, we use both YAGO and Reverb as our knowledge sources. In addition, the automatically constructed knowledge bases can be extracted from raw texts of any domain, which makes our method adaptable. Both sources are presented in triples, argument1 -predicate-argument2 , (e.g., Gates-create-Microsoft), where the two arguments are usually entities and the"
N15-1115,C14-1089,1,0.939925,"ntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common entity in s2 and s3 . However, the tran"
N15-1115,W07-2321,0,0.0327156,"(d)), where pt (d) is the normalized frequency of the transition t in the entity grid, and m is the number of predefined transitions. pt (d) is computed as the number of occurrences of transition t among all entities in the entity grid, divided by the total number of transitions of the same length. Using this feature encoding, the model is then trained as a preference ranking problem between documents of different degrees of coherence. 2.2 Graph-based local coherence modeling As mentioned previously, most extensions to the entity-based local coherence model focus on enriching the feature set (Filippova and Strube, 2007; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), all of which follow a supervised learning framework. To the best of our knowledge, the only exception is the unsupervised method proposed by s1 s2 s3 G&S, which transforms the entity grid into a sentence graph and measures text coherence by computing the average out-degree of the graph. For a document d, its entity grid is constructed first, following the method described in Section 2.1. Then, a bipartite graph G = (V s , Ve , L, W) is constructed, where V s is the set of nodes representing sentences in the text; Ve is the set"
N15-1115,P13-1010,0,0.839476,"gh}@cs.toronto.edu Abstract Previous work on text coherence was primarily based on matching multiple mentions of the same entity in different parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents. 1 Introduction In a well-written document, sentences are organized and presented in a logical and coherent form, which makes the text fluent and easily understood. Therefore, coherence is a fundamental aspect of high text quality, and the evaluation of coherence is a crucial component of many NLP applications, such as essay"
N15-1115,P11-1100,0,0.511546,"ry generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common entity in s2 and s3"
N15-1115,de-marneffe-etal-2006-generating,0,0.00708196,"train a ranker to prefer the(b)more coherent Sentence graph text over the less coherent one. Performance is therefore measured as the fraction of correct pairwise rankings as recognized by the ranker. We use SVMlight (Joachims, 2002) with the ranking configuration to train and evaluate our models, with all parameters set to default values. On both tasks, across two frameworks, supervised and unsupervised, we directly compare our modified model against the original one, i.e., B&L in the supervised framework and G&S in the unsupervised framework. In our experiments, we use the Stanford parser (Marneffe et al., 2006) to automatically extract the grammatical role for each entity mention. 5.1 Sentence ordering The task of sentence ordering attempts to simulate the situation where, given a predefined set of information-bearing items, we need to determine the best order to arrange those items. In this paper, we follow G&S and introduce CoNLL 20122 (Pradhan et al., 2012) as our dataset, which is composed of documents from multiple news sources. For each text, we randomly shuffle its sentences to generate 20 permutations with incorrect sentence order. For a fair comparison, we also evaluate our model on a filte"
N15-1115,P10-1158,0,0.026107,"Missing"
N15-1115,W12-4501,0,0.0383312,"across two frameworks, supervised and unsupervised, we directly compare our modified model against the original one, i.e., B&L in the supervised framework and G&S in the unsupervised framework. In our experiments, we use the Stanford parser (Marneffe et al., 2006) to automatically extract the grammatical role for each entity mention. 5.1 Sentence ordering The task of sentence ordering attempts to simulate the situation where, given a predefined set of information-bearing items, we need to determine the best order to arrange those items. In this paper, we follow G&S and introduce CoNLL 20122 (Pradhan et al., 2012) as our dataset, which is composed of documents from multiple news sources. For each text, we randomly shuffle its sentences to generate 20 permutations with incorrect sentence order. For a fair comparison, we also evaluate our model on a filtered subset of documents with an average length of 31.8 sentences. Therefore, our dataset contains 72 documents and 72 × 20 = 1440 permutations, among which the shortest one contains 25 sentences. For our enhanced graph-based model (introduced in Section 4.1), which is purely unsupervised, we evaluate our model over the entire dataset. For our enhanced en"
N15-1115,C14-1087,1,0.829171,"s extracted from raw texts. Generally speaking, manually edited knowledge bases have better accuracy but lower coverage, while automatically extracted knowledge bases are the opposite. To seek a good balance, we use both YAGO and Reverb as our knowledge sources. In addition, the automatically constructed knowledge bases can be extracted from raw texts of any domain, which makes our method adaptable. Both sources are presented in triples, argument1 -predicate-argument2 , (e.g., Gates-create-Microsoft), where the two arguments are usually entities and the predicate is the relation between them (Zhang et al., 2014). Knowledge selection For each document d, we then select the subset of relevant knowledge instances, in the sense that they represent relations between the entities in d. In particular, we extract all entities in d, and query the knowledge bases to obtain all the knowledge instances in which both of the two arguments, argument1 and argument2 , match some of the entities in d. One issue in knowledge selection is whether to retrieve knowledge instances using exact or partial matching. For a given pair of entities in the text, the chance is rather low to find instances in the knowledge bases whe"
naderi-hirst-2017-recognizing,H05-1044,0,\N,Missing
naderi-hirst-2017-recognizing,P03-1054,0,\N,Missing
naderi-hirst-2017-recognizing,P11-1099,1,\N,Missing
naderi-hirst-2017-recognizing,D09-1036,0,\N,Missing
naderi-hirst-2017-recognizing,P12-1007,1,\N,Missing
naderi-hirst-2017-recognizing,W15-4625,0,\N,Missing
naderi-hirst-2017-recognizing,P16-1107,0,\N,Missing
naderi-hirst-2017-recognizing,P13-1066,0,\N,Missing
P11-1099,J87-1002,0,0.413852,"Missing"
P11-1099,W03-1210,0,0.0404548,"on and the first premise proposition. lenRat: the ratio of the length (in token or sentence) of the premise(s) to that of the conclusion. numPrem: the number of explicit premise propositions (PROP nodes) in the argument. type: type of argumentation structure, i.e., linked or convergent. Argument from example 8 keywords and phrases including for example, such as, for instance, etc.; 3 punctuation cues: “:”, “;”, and “—”. Argument from cause to effect 22 keywords and simple cue phrases including result, related to, lead to, etc.; 10 causal and noncausal relation patterns extracted from WordNet (Girju, 2003). Practical reasoning 28 keywords and phrases including want, aim, objective, etc.; 4 modal verbs: should, could, must, and need; 4 patterns including imperatives and infinitives indicating the goal of the speaker. Argument from consequences The counts of positive and negative propositions in the conclusion and premises, calculated from the General Inquirer2 . Table 2: List of general features. the features for any particular scheme are used only when it is the subject of a particular task. For example, when we classify argument from example in a one-against-others setup, we use the schemespec"
P11-1099,O97-1002,0,0.0392956,"ted as 0 if cpk is not found in A. For argument from consequences, since the arguer has an obvious preference for some particular consequence, sentiment orientation can be a good indicator for this scheme, which is quantified by the counts of positive and negative propositions in the conclusion and premise. For argument from verbal classification, there exists a hypernymy-like relation between some pair of propositions (entities, concepts, or actions) located in the conclusion and the premise respectively. The existence of such a relation is quantified by the maximal Jiang-Conrath Similarity (Jiang and Conrath, 1997) between the “central word” pairs extracted from the conclusion and the premise. We parse each sentence of the argument with the Stanford dependency parser, and a word or phrase is considered to be a central word if it is the dependent or governor of several particular dependency relations, which basically represents the attribute or the action of an entity in a sentence, or the entity itself. For example, if a word or phrase is the dependent of the dependency relation agent, it is therefore considered as a “central word”. In addition, an arguer tends to use several particular syntactic struct"
P11-1099,P09-1075,0,\N,Missing
P12-1007,W05-0613,0,0.730657,"Missing"
P12-1007,W01-1605,0,0.733359,"dings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 60–68, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics [Catching up with commercial competitors in retail banking 3 Related work and financial services,]e1 [they argue,]e2 [will be difficult,]e3 [particularly if market conditions turn sour.]e4 (e1-e4) condition (e1-e3) (e4) same-unit (e1-e2) (e3) attribution (e1) (e2) Figure 1: An example text fragment (wsj 0616) composed of four EDUs, and its RST discourse tree representation. The RST Discourse Treebank (RST-DT) (Carlson et al., 2001), is a corpus annotated in the framework of RST. It consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. In RST-DT, the original 24 discourse relations defined by Mann and Thompson (1988) are further divided into a set of 18 relation classes with 78 finergrained rhetorical relations in total, which provides a high level of expressivity. 2.2 The Penn Discourse Treebank The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is another annotated discourse corpus. Its text is a superset of that of RST-DT (2159 Wall Street Journal articles). Unlike RST-DT,"
P12-1007,P09-1075,0,0.388469,"Missing"
P12-1007,D10-1039,0,0.0642328,"etation requires understanding the unit’s relation with the context. Research in discourse parsing aims to unmask such relations in text, which is helpful for many downstream applications such as summarization, information retrieval, and question answering. However, most existing discourse parsers operate on individual sentences alone, whereas discourse parsing is more powerful for text-level analysis. Therefore, in this work, we aim to develop a textlevel discourse parser. We follow the framework of Rhetorical Structure Theory (Mann and Thompson, 1988) and we take the HILDA discourse parser (Hernault et al., 2010b) as the basis of our work, because it is the first fully implemented text-level discourse parser with state-of-the-art performance. We significantly improve the performance of HILDA’s treebuilding step (introduced in Section 5.1 below) by incorporating rich linguistic features (Section 5.3). In our experiments (Section 6), we also analyze the Discourse-annotated corpora The RST Discourse Treebank Rhetorical Structure Theory (Mann and Thompson, 1988) is one of the most widely accepted frameworks for discourse analysis. In the framework of RST, a coherent text can be represented as a discourse"
P12-1007,D09-1036,0,0.288933,"Missing"
P12-1007,P02-1047,0,0.547587,", SR ), we extract the following seven types of features. HILDA’s features: We incorporate the original features used in the HILDA discourse parser with slight modification, which include the following four types of features occurring in SL , SR , or both: (1) N-gram prefixes and suffixes; (2) syntactic tag prefixes and suffixes; (3) lexical heads in the constituent parse tree; and (4) POS tag of the dominating nodes. Lin et al.’s features: Following Lin et al. (2009), we extract the following three types of features: (1) pairs of words, one from SL and one from SR , as originally proposed by Marcu and Echihabi (2002); (2) dependency parse features in SL , SR , or both; and (3) syntactic production rules in SL , SR , or both. Contextual features: For a globally coherent text, there exist particular sequential patterns in the local usage of different discourse relations. Given (SL , SR ), the pair of text spans of interest, contextual features attempt to encode the discourse relations assigned to the preceding and the following text span pairs. Lin et al. (2009) also incorporated contextual features in their feature set. However, their work was based on PDTB, which has a very different annotation framework"
P12-1007,P97-1013,0,0.0443124,"Missing"
P12-1007,prasad-etal-2008-penn,0,0.0779322,"le text fragment (wsj 0616) composed of four EDUs, and its RST discourse tree representation. The RST Discourse Treebank (RST-DT) (Carlson et al., 2001), is a corpus annotated in the framework of RST. It consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. In RST-DT, the original 24 discourse relations defined by Mann and Thompson (1988) are further divided into a set of 18 relation classes with 78 finergrained rhetorical relations in total, which provides a high level of expressivity. 2.2 The Penn Discourse Treebank The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is another annotated discourse corpus. Its text is a superset of that of RST-DT (2159 Wall Street Journal articles). Unlike RST-DT, PDTB does not follow the framework of RST; rather, it follows a lexically grounded, predicate-argument approach with a different set of predefined discourse relations, as proposed by Webber (2004). In this framework, a discourse connective (e.g., because) is considered to be a predicate that takes two text spans as its arguments. The argument that the discourse connective structurally attaches to is called Arg2, and the other argument is called Arg1 — unlike in R"
P12-1007,W09-3813,0,0.243943,"Missing"
P12-1007,N03-1030,0,0.912075,"Missing"
P12-1007,N09-1064,0,0.769042,"Missing"
P12-1007,C04-1048,0,\N,Missing
P14-1048,W05-0613,0,0.24131,"Missing"
P14-1048,P08-1099,0,0.0182462,"Missing"
P14-1048,J08-1001,0,0.0139049,"t number of chains on higher levels. We have thus showed that the time complexity Contextual features: The feature vector of the previous and the next constituent in the chain. Substructure features: The root node of the left and right discourse subtrees of each unit. Syntactic features: whether each unit corresponds to a single syntactic subtree, and if so, the top PoS tag of the subtree; the distance of each unit to their lowest common ancestor in the syntax tree (intra-sentential only). Entity transition features: The type and the number of entity transitions across the two units. We adopt Barzilay and Lapata (2008)’s entitybased local coherence model to represent a document by an entity grid, and extract local transitions among entities in continuous discourse constituents. We use bigram and trigram transitions with syntactic roles attached to each entity. 4 We implicitly made an assumption that the parsing time is dominated by the time to perform inference on CRF chains. However, for complex features, the time required for feature computation might be dominant. Nevertheless, a careful caching strategy can accelerate feature computation, since a large number of multi-sentential chains overlap with each"
P14-1048,W01-1605,0,0.339473,"(Klein and Manning, 2003; de Marneffe et al., 2006; Recasens et al., 2013) to syntactically parse the texts and extract coreference relations, and we use Penn2Malt5 to lexicalize syntactic trees to extract dominance features. For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013), we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. Table 1: Performance of different models using gold-standard EDU segmentation, evaluated using the constit"
P14-1048,de-marneffe-etal-2006-generating,0,0.0601327,"Missing"
P14-1048,P12-1007,1,0.774312,"hows a text fragment consisting of two sentences with four EDUs in total (e1 -e4 ). Its discourse tree representation is 511 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 511–521, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics [On Aug. 1, the state tore up its controls,]e1 [and food prices leaped]e2 [Without buffer stocks,]e3 [inflation exploded.]e4 strategy is its efficiency in practice. Also, the employment of SVM classifiers allows the incorporation of rich features for better data representation (Feng and Hirst, 2012). However, HILDA’s approach also has obvious weakness: the greedy algorithm may lead to poor performance due to local optima, and more importantly, the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account. wsj 1146 e1:4 sequence e1:2 e3:4 consequence e1 e2 circumstance e3 2.2 e4 Joty et al. (2013) approach the problem of textlevel discourse parsing using a model trained by Conditional Random Fields (CRF). Their model has two distinct features. First, they decomposed the problem of textlevel discourse parsing into two stages: i"
P14-1048,C12-1115,0,0.0889751,"Missing"
P14-1048,N13-1071,0,0.0305146,"Missing"
P14-1048,D12-1083,0,0.0298465,"Missing"
P14-1048,P13-1048,0,0.301408,"and food prices leaped]e2 [Without buffer stocks,]e3 [inflation exploded.]e4 strategy is its efficiency in practice. Also, the employment of SVM classifiers allows the incorporation of rich features for better data representation (Feng and Hirst, 2012). However, HILDA’s approach also has obvious weakness: the greedy algorithm may lead to poor performance due to local optima, and more importantly, the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account. wsj 1146 e1:4 sequence e1:2 e3:4 consequence e1 e2 circumstance e3 2.2 e4 Joty et al. (2013) approach the problem of textlevel discourse parsing using a model trained by Conditional Random Fields (CRF). Their model has two distinct features. First, they decomposed the problem of textlevel discourse parsing into two stages: intrasentential parsing to produce a discourse tree for each sentence, followed by multi-sentential parsing to combine the sentence-level discourse trees and produce the text-level discourse tree. Specifically, they employed two separate models for intra- and multi-sentential parsing. Their choice of two-stage parsing is well motivated for two reasons: (1) it has b"
P14-1048,N09-1064,0,0.0398687,"Missing"
P14-1048,P03-1054,0,0.0292109,"-sentential chains overlap with each other. 517 Cue phrase features: Whether a cue phrase occurs in the first or last EDU of each unit. The cue phrase list is based on the connectives collected by Knott and Dale (1994) Post-editing features: the initial tree. 8 Model jCRF gSVMFH gCRF gCRFPE The depth of each unit in Relation 82.5 82.8 84.9∗ 85.7∗† 68.4 67.1 69.9∗ 71.0∗† 55.7 52.0 57.2∗ 58.2∗† MAFS N/A 27.4/23.3 35.3/31.3 36.2/32.3 Human 88.7 77.7 65.8 N/A FH ∗: significantly better than gSVM (p &lt; .01) †: significantly better than gCRF (p &lt; .01) For pre-processing, we use the Stanford CoreNLP (Klein and Manning, 2003; de Marneffe et al., 2006; Recasens et al., 2013) to syntactically parse the texts and extract coreference relations, and we use Penn2Malt5 to lexicalize syntactic trees to extract dominance features. For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), wh"
P14-1048,W09-3515,0,0.019458,"ond the scope of this paper. 513 S1 ... Si Mintra p S1 T TD Pintra TS i Pmulti TSpi ... ... Mintra Mmulti ... ... Sn T S1 ... ... D Pintra Mintra TDp Pintra TS n TSpn Figure 3: The work flow of our proposed discourse parser. In the figure, Mintra and Mmulti stand for the intra- and multi-sentential bottom-up tree-building models, and Pintra and Pmulti stand for the intra- and multi-sentential post-editing models. chain CRFs to model the structure and the relation separately. Although joint modeling has shown to be effective in various NLP and computer vision applications (Sutton et al., 2007; Yang et al., 2009; Wojek and Schiele, 2008), our choice of using two separate models is for the following reasons: First, it is not entirely appropriate to model the structure and the relation at the same time. For example, with respect to Figure 2, it is unclear how the relation node R j is represented for a training instance whose structure node S j = 0, i.e., the units U j−1 and U j are disjoint. Assume a special relation NO-REL is assigned for R j . Then, in the tree-building process, we will have to deal with the situations where the joint model yields conflicting predictions: it is possible that the mode"
P14-2087,C04-1162,0,0.0378102,"significant improvement over the Lesk algorithm on word sense disambiguation tasks. With additional lexical knowledge from WordNet, performance is further improved to surpass the state-of-the-art results. 1 2 Related Work In the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM. In particular, we opt for the “simplified Lesk” (Kilgarriff and Rosenzweig, 2000), where inventory senses are assessed by glosscontext overlap rather than gloss-gloss overlap. This particular variant prevents proliferation of gloss comparison on larger contexts (Mihalcea et al., 2004) and is shown to outperform the original Lesk algorithm (Vasilescu et al., 2004). To the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD — that is, in contrast to their use as a similarity measure in this study. Gale et al. (1992) used NB classifier resembling an information retrieval system: a WSD instance is regarded as a document d, and candidate senses are scored in terms of “relevance” to d. When evaluated on a WSD benchmark (Vasilescu et al., 2004), the algorithm compared favourably to Lesk variants (as expected for a supervised method). Pedersen (2000) p"
P14-2087,N09-1004,0,0.11183,"g, 2000; Vasilescu et al., 2004). Among the popular explanations is a key limitation of the algorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Banerjee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et al., 2009) and vector space models (Abdalgader and Skabar, 2012; Raviv et al., 2012; Patwardhan and Pedersen, 2006). To address this limitation, a 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531–537, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics higher scores to multi-word overlaps. 3 Model and Task Descriptions Breaking away from string matching, Wilks et al. (1990) measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order"
P14-2087,W06-2501,0,0.042125,"lgorithm, that “Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results.” (Navigli, 2009). Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (Banerjee and Pedersen, 2002), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching (Chen et al., 2009) and vector space models (Abdalgader and Skabar, 2012; Raviv et al., 2012; Patwardhan and Pedersen, 2006). To address this limitation, a 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531–537, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics higher scores to multi-word overlaps. 3 Model and Task Descriptions Breaking away from string matching, Wilks et al. (1990) measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order cooccurrence information in glosses. An extension by Patwardhan and Pedersen (2006) differentiated contex"
P14-2087,J90-1003,0,0.232457,"for example. Selected hypernyms, hyponyms, and synonyms pertaining to its two senses factory and life form are listed in Table 1. Hypernyms can be overly general terms (e.g., being). Although conceptually helpful for humans in coarse-grained WSD, this generality is 3.4 Probability Estimation A most open-ended question is how to estimate the probabilities in Equation (1). In WSD in particular, the estimation concerns the marginal and conditional probabilities of and between word tokens. Many options are available to this end in statistical machine learning (MLE, MAP, etc.), information theory (Church and Hanks, 1990; Turney, 2001), as well as the rich body of research in lexical semantic similarity Resnik, 1995; Jiang and Conrath, 1997; Budanitsky and Hirst, 2006). Here we choose maximum likelihood — not only for its simplicity, but also to demonstrate model strength with a relatively crude probability estimation. To avoid underflow, Equation (1) is estimated as the following log probability: ∑ log i c( f j ) c(ei , f j ) c(ei ) + ∑ log − |f |∑ log c(·) ∑ c( f ) c(·) j i j j =(1 − |e|) ∑ log c( f j ) − |f |∑ log c(ei ) i j + ∑ ∑ log c(ei , f j ) + |f|(|e |− 1) log c(·), i j where c(x) is the count of wor"
P14-2087,S01-1018,0,0.03423,"h semantic and distributional), we postulate that their probability in the NBM is uniformly inflated among many sense candidates, and hence they decrease in distinguishability. Synonyms might help with regard to semantic specification, though their limited quantity also limits their benefits. These patterns on the LK types are consistent in all three experiments. When including all four LK sources, our model outperforms the state-of-the-art systems with statistical significance in both coarse-grained tasks. For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. 4.3 Conclusions and Future Work For future work, we plan to apply the model in other shared tasks, including open-text WSD, so as to compare with more recent Lesk variants. We would also like to explore how to incorporate syntactic features and employ alternative statistical methods (e.g., parametric models) to improve probability estimation and inference. Other NLP problems involving composition"
P14-2087,S07-1016,0,0.0152833,"ion on both datasets, and tokenization on the Senseval-2 instances.6 Stanford CoreNLP7 is used for lemmatization and tokenization. Identical procedures are applied to all corpora used for probability estimation. Binomial test is used for significance testing, and with one exception explicitly noted in Section 4.3, all differences presented are statistically highly significant (p &lt; 0.001). Evaluation Data, Scoring, and Pre-processing Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval-2 (Edmonds and Cotton, 2001) and SemEval-2007 (Pradhan et al., 2007). Training sections are used as development data and test sections held out for final testing. Model performance is evaluated in terms of WSD accuracy using Equation (2) as the scoring function. Accuracy is defined as the number of correct responses over the number of instances. Because it is a rare event for the NBM to produce identical scores,4 the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. Multiword expressions (MWEs) in the Senseval-2 sense inventory are not explicitly marked in the contexts. Several of the top-rankin"
P14-2087,W14-0111,0,0.0227836,"quality. Specifically in WSD, a source corpus is defined as the source of the majority of the WSD instances in a given dataset, and a baseline corpus of a smaller size and less resemblance to the instances is used for all datasets. The assumption is that a source corpus offers better estimates for the model than the baseline corpus, and difference in model performance is expected when using probability estimation of different quality. 4 4.1 from their corresponding WordNet synset (or in the coarse-grained case, a concatenation of tokens from all synsets in a sense group). The MIT-JWI library (Finlayson, 2014) is used for accessing WordNet. Usage examples in glosses (included by the library by default) are removed in our experiments.5 Basic pre-processing is performed on the contexts and the glosses, including lower-casing, stopword removal, lemmatization on both datasets, and tokenization on the Senseval-2 instances.6 Stanford CoreNLP7 is used for lemmatization and tokenization. Identical procedures are applied to all corpora used for probability estimation. Binomial test is used for significance testing, and with one exception explicitly noted in Section 4.3, all differences presented are statist"
P14-2087,O97-1002,0,0.396389,"Table 1. Hypernyms can be overly general terms (e.g., being). Although conceptually helpful for humans in coarse-grained WSD, this generality is 3.4 Probability Estimation A most open-ended question is how to estimate the probabilities in Equation (1). In WSD in particular, the estimation concerns the marginal and conditional probabilities of and between word tokens. Many options are available to this end in statistical machine learning (MLE, MAP, etc.), information theory (Church and Hanks, 1990; Turney, 2001), as well as the rich body of research in lexical semantic similarity Resnik, 1995; Jiang and Conrath, 1997; Budanitsky and Hirst, 2006). Here we choose maximum likelihood — not only for its simplicity, but also to demonstrate model strength with a relatively crude probability estimation. To avoid underflow, Equation (1) is estimated as the following log probability: ∑ log i c( f j ) c(ei , f j ) c(ei ) + ∑ log − |f |∑ log c(·) ∑ c( f ) c(·) j i j j =(1 − |e|) ∑ log c( f j ) − |f |∑ log c(ei ) i j + ∑ ∑ log c(ei , f j ) + |f|(|e |− 1) log c(·), i j where c(x) is the count of word x, c(·) is the corpus 2 We do, however, refer curious readers to the work of Raviv et al. (2012) for a novel treatment o"
P14-2087,vasilescu-etal-2004-evaluating,0,0.843953,"Missing"
P14-2087,P95-1026,0,0.115162,"ostulate that their probability in the NBM is uniformly inflated among many sense candidates, and hence they decrease in distinguishability. Synonyms might help with regard to semantic specification, though their limited quantity also limits their benefits. These patterns on the LK types are consistent in all three experiments. When including all four LK sources, our model outperforms the state-of-the-art systems with statistical significance in both coarse-grained tasks. For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. 4.3 Conclusions and Future Work For future work, we plan to apply the model in other shared tasks, including open-text WSD, so as to compare with more recent Lesk variants. We would also like to explore how to incorporate syntactic features and employ alternative statistical methods (e.g., parametric models) to improve probability estimation and inference. Other NLP problems involving compositionality in general might also benefit from the"
P14-2087,W02-0807,0,\N,Missing
P14-2087,S01-1037,0,\N,Missing
P14-2087,J06-1003,1,\N,Missing
P14-2087,S01-1001,0,\N,Missing
P14-2087,C12-1109,0,\N,Missing
P14-2087,A00-2009,0,\N,Missing
P15-1051,C10-3004,1,0.77884,"Missing"
P15-1051,D07-1074,0,0.205382,"Missing"
P15-1051,P13-2006,0,0.0290674,"Missing"
P15-1051,D11-1072,0,0.076523,"Missing"
P15-1051,C14-1087,1,0.243274,"ghdad hasChild CapitalQusay Hussein k2: SaddamkHussein Saddam Hussein hasChild Qusay Hussein k(b)2: Two omitted relevant pieces of background knowledge Figure 1: An example of document enrichment: A source document about a U.S. air strike omitting two important pieces of background knowledge which are acquired by our framework. triples from multiple sources, we also get better coverage. Therefore, one can expect that this representation is helpful for better document enrichment by incorporating both accuracy and coverage. In fact, there is already evidence that this representation is helpful. Zhang et al. (2014) proposed a triple-based document enrichment framework which uses triples of SPO as background knowledge. They first proposed a search engine– based method to evaluate the relatedness between every pair of triples, and then an iterative propagation algorithm was introduced to select the most relevant triples to a given source document (see Section 2), which achieved a good performance. However, to evaluate the semantic relatedness between two triples, Zhang et al. (2014) primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in t"
P15-1051,J00-3005,0,0.0121463,"Missing"
P15-1051,nastase-etal-2010-wikinet,0,0.0788196,"Missing"
P15-1051,N10-1072,0,0.0604509,"Missing"
P15-1051,P11-1009,0,0.0688529,"Missing"
P15-1051,P09-1077,0,0.0248825,"Missing"
P15-2075,J87-3001,0,0.299356,"pendency pair, and dˆ∗ correspond to random samples from the dependent vocabulary (drawn by unigram frequency). 3.2 Incorporating Lexicographic Knowledge Semantic information used in existing studies (Section 2) either relies on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining relation (also known as the Olney Concordance Index, Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic The Proposed Models Factorizing Dependency Relations One strong limitation of the existing dependencybased models is that no distinctions are made"
P15-2075,P85-1037,0,0.105034,"r the dependent of a dependency pair, and dˆ∗ correspond to random samples from the dependent vocabulary (drawn by unigram frequency). 3.2 Incorporating Lexicographic Knowledge Semantic information used in existing studies (Section 2) either relies on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining relation (also known as the Olney Concordance Index, Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic The Proposed Models Factorizing Dependency Relations One strong limitation of the existing dependencybased models is that no disti"
P15-2075,N15-1184,0,0.12178,"Missing"
P15-2075,W12-3018,0,0.0333345,"Missing"
P15-2075,J15-4004,0,0.091001,"Missing"
P15-2075,D14-1162,0,0.103107,"ming notably better than several popular embedding models in similarity tasks. 1 Graeme Hirst University of Toronto gh@cs.toronto.edu Lexical Embeddings and Relatedness Lexical embeddings are essentially real-valued distributed representations of words. As a vectorspace model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks. In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pennington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014). To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Euclidean similarity between the embeddings of related words. Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality"
P15-2075,P12-1092,0,0.12178,"e said to “understand” the meaning of apple by possessing semantic knowledge about its certain attributes. By extension, similar models can be trained to learn the meaning of the governors in other dependency relations (e.g., adjectival governors in the inverse relation amod−1 , etc.). The basic model uses an objective function similar to that of Mikolov et al. (2013a): negative sampling (similar to that of Collobert and Weston 2008) or hierarchical softmax (similar to that of Mnih and Hinton 2007). To address the limitation of contextual locality in many language models (including word2vec), Huang et al. (2012) added a “global context score” to the local n-gram score (Collobert and Weston, 2008). The concatenation of word vectors and a “document vector” (centroid of the composing word vectors weighted by idf ) was used as model input. Pennington et al. (2014) proposed to explicitly factorize the global co-occurrence matrix between words, and the resulting log bilinear model achieved state-of-the-art performance in lexical similarity, analogy, and named entity recognition. Several later studies addressed the limitations of window-based co-occurrence by extending the word2vec model to predict words th"
P15-2075,P14-2050,0,0.0610623,"Missing"
P15-2075,P10-1040,0,0.0344938,"izing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models. In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts. Al2 Related Work Lexical embeddings have traditionally been used in language modelling as distributed representations of words (Bengio et al., 2003; Mnih and Hinton, 2009) and have only recently been used in other NLP tasks. Turian et al. (2010), for example, used embeddings from existing language models (Collobert and Weston, 2008; Mnih and Hinton, 2007) as unsupervised lexical features to improve named entity recognition and chunking. Embedding models gained further popularity thanks to the simplicity and effectiveness of the word2vec model (Mikolov et al., 2013a), which implicitly factorizes the point-wise mutual information matrix shifted by biases consisting of marginal counts of individual words (Levy and Goldberg, 2014b). Efficiency is greatly improved by approximating the computationally costly softmax function with 458 Proce"
P15-2075,N13-1090,0,0.0914383,"Missing"
P15-2075,P14-2131,0,\N,Missing
P17-2039,W16-2808,1,0.847172,"cy, but arguments are judged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. B"
P17-2039,P12-2041,0,0.0279224,"ying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors al"
P17-2039,D16-1129,1,0.101498,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,P16-1150,1,0.106561,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,N13-1132,0,0.0340534,"y dimensions with scores from 1 to 3 (or choose “cannot judge”). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total). Given the crowd ratings, we then performed two comparisons as detailed in the following. 4.2 Agreement of the Crowd with Experts First, we checked to what extent lay annotators and experts agree in terms of Krippendorff’s α. On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of Wachsmuth et al. (2017a). On the other hand, we estimated a reliable rating from the crowd ratings using MACE (Hovy et al., 2013) and compared it to the experts. 253 (a) Crowd / Expert Quality Dimension Cog LA LR LS Eff Cre Emo Cla App Arr Rea GA GR GS OQ Cogency Local acceptability Local relevance Local sufficiency Effectiveness Credibility Emotional appeal Clarity Appropriateness Arrangement Reasonableness Global acceptability Global relevance Global sufficiency Overall quality (b) Crowd 1 / 2 / Expert (c) Crowd 1 / Expert (d) Crowd 2 / Expert Mean MACE Mean MACE Mean MACE Mean MACE .27 .49 .42 .18 .13 .41 .45 .42 .54 .53 .33 .54 .44 –.17 .43 .38 .35 .39 .31 .31 .27 .23 .28 .26 .30 .40 .40 .31 .19 .43 .24 .37 .33 .21"
P17-2039,P15-1053,0,0.080264,"dged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that r"
P17-2039,D15-1050,0,0.125974,"ng theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argument has acceptable premises that are relevant to its conclusion and sufficient to draw the conclusion (Johnson and Blair, 2006). Practitioners object that such quality dimensions are hard to assess for real-life arguments (Habernal and Gurevych, 2016b). Moreover, the normative nature of theory suggests absolute quality ratings, but in practice it seems much easier to state which argument is more convinc"
P17-2039,E17-1017,1,0.313544,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,E17-1105,1,0.205942,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,P16-2032,0,0.0789411,"o cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a foll"
P19-1166,Q16-1028,0,0.0458632,"~y) (3) The normalized difference between the mean values of s(w, X,Y ) across the two target word sets is called the effect size. For the sake of simplicity, we consider the case where both attribute word sets contain a single word (i.e., X = {x},Y = {y}). Proposition 1 Let X = {x},Y = {y}, and w be unbiased with respect to {(x, y)} by Definition 1. According to WEAT, an SGNS vector ~w is equally associated with X and Y under perfect reconstruction iff p(x) = p(y). Both theoretical and empirical work have found the squared word embedding norm to be linear in the log probability of the word. (Arora et al., 2016; Ethayarajh et al., 2018). Where α1 , α2 ∈ R, w is then equally associated with X and Y if 0 = cos(~w,~x) − cos(~w,~y)   h~w,~xi h~w,~yi 1 = − k~wk2 k~xk2 k~yk2 h~w,~yi h~w,~xi −p =p α1 log p(x) + α2 α1 log p(y) + α2 (4) By the Debiasing Theorem, w is unbiased with respect to the set {(x, y)} iff h~w,~xi = h~w,~yi. Therefore (4) holds iff p(x) = p(y). Thus for w to be equally associated with both sets of attribute words, not only must w be unbiased with respect to {(x, y)} by Definition 1, but words x and y must also occur with equal frequency in the corpus. Despite this being implicitly re"
P19-1166,W19-3621,0,0.161759,"’, ‘woman’)}. If we define a bias subspace using S and use it to debias ~w, we can only say definitively that ~w is unbiased with respect to S. We cannot claim, for example, that ~w is also unbiased with respect to {(‘policeman’, ‘policewoman’)}, because ~ ~ it is possible that policewoman − policeman 6= woman ~ − man. ~ Debiasing ~w with respect to a nonexhaustive set of gender-defining word pairs is not equivalent to erasing all vestiges of gender from ~w. This may explain why it is still possible to cluster words by gender after debiasing them using a handful of gender-defining word pairs (Gonen and Goldberg, 2019). 4 The Flaws of WEAT Given attribute word sets X and Y (e.g., {‘male’, ‘man’} vs. {‘female’, ‘woman’}), WEAT uses a cosine similarity-based measurement to capture whether two target word sets have the same relative association to both sets of attribute words. At the heart of WEAT is the statistic s(w, X,Y ), which &quot;measures the association of [a word] w with the attribute&quot; (Caliskan et al., 2017): s(w, X,Y ) = EX cos(~w,~x) − EY cos(~w,~y) (3) The normalized difference between the mean values of s(w, X,Y ) across the two target word sets is called the effect size. For the sake of simplicity,"
P19-1166,Q15-1016,0,0.0358783,"an’, ‘woman’) to define the gender relation vector, for example, would have a negligible impact. In contrast, as shown in section 4, the lack of robustness of WEAT to the choice of attribute sets is one reason it is so unreliable. We can also interpret β (~w;~b) for other embedding models, not just SGNS. Where Xx,y denotes the frequency of a word pair (x, y) and zx , zy denote the learned bias terms for GloVe:   p(x, w) βGloVe (~w;~b) = C log − zx + zy p(y, w) √ (6) 1/ λ where C = p −csPMI(x, y) + α Because the terms zx , zy are learned, β (~w;~b) is not as interpretable for GloVe. However, Levy et al. (2015) have conjectured that, in practice, zx , zy may be equivalent to the log counts of x and y respectively, in which case βGloVe = βSGNS . 5.2 Statistical Significance Unlike with WEAT, there is no notion of statistical significance attached to RIPA. There is a simple reason for this. Whether a word vector ~w is spuriously or non-spuriously associated with respect to a relation vector (~x −~y)/k~x −~yk depends on how frequently (w, x) and (w, y) co-occur in the training corpus; the more co-occurrences there are, the less likely the association is spurious. As shown in experiments by Ethayarajh e"
P19-1166,D17-1308,0,0.0318077,"aughter’, ‘son’), (‘gal’, ‘guy’), (‘female’, ‘male’), (‘her’, ‘his’), (‘herself’, ‘himself’), (‘mary’, ‘john’)}. 1702 Figure 1: Before debiasing words using subspace projection, one needs to identify which words are genderappropriate – to avoid debiasing them. The Bolukbasi et al. (2016) method of identifying these words is ineffective: it ends up precluding most gender-appropriate analogies (dotted line, left) while preserving most gender-biased analogies (dotted line, right). Our unsupervised method (dashed line) does much better in both respects. λ ≈ 1 in practice (Ethayarajh et al., 2018; Mimno and Thompson, 2017). Similarly, α ← −1 because it minimizes the difference between k~x −~yk and its information theoretic interpretation over the gender-defining word pairs in S, though this is an estimate and may differ from the true value of α. In Table 2, we list the gender association in the training corpus (g(w)), the gender association in embedding space (g(w)), ˆ and the absolute change (∆g (w)) for each group of words. On average, the SGNS embedding model does not make gender-neutral words any more gendered than they are in the training corpus. Given that much of the vocabulary falls into this category,"
P19-1166,D14-1162,0,0.118139,"Mikolov et al., 2013; Collobert and Weston, 2008); (b) low-rank approximations of word-context matrices containing a co-occurrence statistic (Landauer and Dumais, 1997; Levy and Goldberg, 2014). The objective of SGNS is to maximize the probability of observed word-context pairs and to minimize the probability of k randomly sampled negative examples. Though no co-occurrence statistics are explicitly calculated, Levy and Goldberg (2014) proved that SGNS is implicitly factorizing a word-context PMI matrix shifted by − log k. Similarly, GloVe implicitly factorizes a log cooccurrence count matrix (Pennington et al., 2014). Measuring Associations Caliskan et al. (2017) proposed what is now the most commonly used association test for word embeddings. The word embedding association test (WEAT) uses cosine similarity to measure how associated two given sets of target words are with respect to two sets of attribute words (e.g., ‘male’ vs. ‘female’). For example, Caliskan et al. (2017) claimed that sciencerelated words are more associated with ‘male’ than ‘female’ attributes compared to art-related words, and that this was statistically significant. However, aside from some intuitive results (e.g., that female names"
P19-1166,D18-1521,0,0.058693,"m principal components for ten gender relation vectors (e.g., man ~ − woman). ~ Each debiased word vector was thus orthogonal to the gender bias subspace and its projection on the subspace was zero. While this subspace projection method precluded gender-biased analogies from holding in the embedding space, Bolukbasi et al. (2016) did not provide any theoretical guarantee that the vectors were unbiased (i.e., equivalent to vectors that would be obtained from training on a gender-agnostic corpus with no reconstruction error). Other work has tried to learn gender-neutral embeddings from scratch (Zhao et al., 2018), despite this approach requiring custom changes to the objective of each embedding model. 3 Provably Debiasing Embeddings Experiments by Bolukbasi et al. (2016) found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding. However, as we noted earlier, despite this method being intuitive, there is no theoretical guarantee that the debiased vectors are perfectly unbiased or that the debiasing method works for embedding models other than SGNS. In this section, we prove that for any embedding model that does implicit matrix factorizatio"
P19-1315,Q16-1028,0,0.598291,"he closest vector to king ~ + woman, ~ which should be queen. ~ It is unclear why arithmetic operators can effectively compose embeddings generated by non-linear models such as skip-gram with negative sampling (SGNS). There have been two attempts to rigorously explain this phenomenon, but both have made strong assumptions about either the embedding space or the word distribution. The paraphrase model (Gittens et al., 2017) hinges on words having a uniform distribution rather than the typical Zipf distribution, which the authors themselves acknowledge is unrealistic. The latent variable model (Arora et al., 2016) assumes that word vectors are known a priori and generated by randomly scaling vectors sampled from the unit sphere. In this paper, we explain why – and under what conditions – word analogies can be solved with vector arithmetic, without making the strong assumptions past work has. We focus on GloVe and SGNS because they implicitly factorize a wordcontext matrix containing a co-occurrence statistic (Levy and Goldberg, 2014), which allows us to interpret the inner product of a word and context vector. We begin by formalizing word analogies as functions that transform one word vector into anoth"
P19-1315,D14-1162,0,0.144522,"an be perfectly reconstructed), a linear analogy holds over a set of ordered word pairs iff csPMI(x, y) is the same for every word pair, csPMI(x1 , x2 ) = csPMI(y1 , y2 ) 3253 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3253–3262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics for any two word pairs, and the row vectors of x1 , x2 , y1 , y2 in the factorized matrix are coplanar. By then framing vector addition as a kind of word analogy, we offer several new insights: 1. Past work has often cited the Pennington et al. (2014) conjecture as an intuitive explanation of why vector arithmetic works for analogy solving. The conjecture is that an analogy of the form a is to b as x is to y holds iff p(w|a)/p(w|b) ≈ p(w|x)/p(w|y) for every word w in the vocabulary. While this is sensible, it is not based on any theoretical derivation or empirical support. We provide a formal proof that this is indeed true. 2. Consider two words x, y and their sum ~z = ~x +~y in an SGNS embedding space with no reconstruction error. If z were in the vocabulary, the similarity between z and x (as measured by the csPMI) would be the log proba"
P19-1315,J90-1003,0,0.609254,"embedding spaces with no reconstruction error, we also explain why, in practice, linear word analogies hold in embedding spaces with some noise. We conduct experiments that support the few assumptions we make and show that the transformations represented by various word analogies correspond to different csPMI values. Without making the strong assumptions of past theories, we thus offer a formal explanation of why, and when, word analogies can be solved with vector arithmetic. 2 Related Work PMI Pointwise mutual information (PMI) captures how much more frequently x, y co-occur than by chance (Church and Hanks, 1990): PMI(x, y) = log p(x, y) p(x)p(y) (1) Word Embeddings Word embeddings are distributed representations in a low-dimensional continuous space. Also called word vectors, they capture semantic and syntactic properties of words, even allowing relationships to be expressed arithmetically (Mikolov et al., 2013b). Word vectors are generally obtained in two ways: (a) from neural networks that learn representations by predicting co-occurrence patterns in the training corpus (Bengio et al., 2003; Mikolov et al., 2013b; Collobert and Weston, 2008); (b) from low-rank approximations of word-context matrice"
P19-1315,W18-3012,1,0.872917,"> p(y). By (12): p(x) > p(y) ⇐⇒ log p(x) + δ > log p(y) + δ ⇐⇒ csPMI(z, y) > csPMI(z, x) (13) Therefore addition automatically down-weights the more frequent word. For example, if the vectors for x = ‘the’ and y = ‘apple’ were added to create a vector for z = ‘the apple’, we would expect csPMI(‘the apple’, ‘apple’) > csPMI(‘the apple’, ‘the’); being a stopword, ‘the’ would on average be heavily down-weighted. While the rarer word is not always the more informative one, weighting schemes like inverse document frequency (IDF) (Robertson, 2004) and unsupervised smoothed inverse frequency (uSIF) (Ethayarajh, 2018) are all based on the principle that more frequent words should be down-weighted because they are typically less informative. The fact that addition automatically down-weights the more frequent word thus provides novel justification for using addition to compose words. 4.3 Interpreting Euclidean Distance Corollary 3 ∃ λ ∈ R+ , α ∈ R− such that for any two words x and y in an SGNS or GloVe embedding space with no reconstruction error, λ k~x − ~yk22 = −csPMI(x, y) + α. From (9), we know that for some λ , α, γ 0 ∈ R, csPMI(x, y) = λ γ 0 + α, where γ 0 = −k~x −~yk22 . Rearranging this identity, we"
P19-1315,P17-1007,0,0.744211,"ty of word vectors learned via neural networks is that word analogies can often be solved with vector arithmetic. For example, ‘king is to ? as man is to woman’ can be solved by ~ − man finding the closest vector to king ~ + woman, ~ which should be queen. ~ It is unclear why arithmetic operators can effectively compose embeddings generated by non-linear models such as skip-gram with negative sampling (SGNS). There have been two attempts to rigorously explain this phenomenon, but both have made strong assumptions about either the embedding space or the word distribution. The paraphrase model (Gittens et al., 2017) hinges on words having a uniform distribution rather than the typical Zipf distribution, which the authors themselves acknowledge is unrealistic. The latent variable model (Arora et al., 2016) assumes that word vectors are known a priori and generated by randomly scaling vectors sampled from the unit sphere. In this paper, we explain why – and under what conditions – word analogies can be solved with vector arithmetic, without making the strong assumptions past work has. We focus on GloVe and SGNS because they implicitly factorize a wordcontext matrix containing a co-occurrence statistic (Lev"
P83-1010,C82-1049,0,0.0332134,"Missing"
P83-1010,J82-1003,0,0.0699894,"a b o u t is t h e c o r r e c t o n e in c o n t e x t . 16polaroid is a 69 trademark of the Polaroid C o r p o r a t i o n . TABLE is used to disambiguate case roles as well. Polaroid Words are described more fully in Hirst and Charniak 5. ABSITY E X A M P L E (CONTINUED) 1982 and Hirst 1983. SUBJ N a d i a (agent,= (the ?x (thlng ?x (propername=""Nadla"")))) 7. Comparison with other work OSJ the book (patlenl;=(the ?y (book Our approach to semantic interpretation m a y usefully be compared with other recent work with similar goals ?y))) to ours. O n e such project is that of Jones and Warren (1982), w h o attempt a conciliation between Montague semantics and a conceptual dependency representation in t h e m a l l (loca~lon:C1;he ?~ (mall ?w))) a store in the mall (a ?z (s~core ?z (loca~ion=C~he ?w (mall ?w))))) (Schank 1975). Their approach is to modify Montague&apos;s translation from English to intensional logic so that the resulting expressions have a canonical interpretation in conceptual dependency. They do not address such issues as extending Montague&apos;s syntax, nor from a store in the mall (source=Ca ?z (s~ore ?z (locatlon=(the ?w (mall ?W)))))) whether their approach can be extended t"
P83-1010,P81-1028,0,\N,Missing
P83-1010,P82-1014,0,\N,Missing
P83-1010,P82-1001,0,\N,Missing
P86-1030,J81-3002,0,0.0696008,"Missing"
P86-1030,P81-1028,0,0.0480459,"Missing"
P86-1030,E85-1007,0,\N,Missing
P95-1020,P94-1009,0,0.020705,"erances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of 144 the phenomenon"
P95-1020,P90-1012,0,0.031289,"clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other"
P95-1020,P92-1009,0,0.280275,"uences of utterances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first obse"
P95-1020,W91-0201,0,0.0165244,"them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of 144 the phenomenon preferred to keep defeasibility outside the mathematical world. For Frege (1892), Russell (1905), and Quine (1949) &quot;everything exists&quot;; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994). We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts). 1. Most linguistic approaches account for the defeasibility of pragmatic inferences by analyzing them in a context that consists of all or some of the previous utterances, including the current one. Context (Karttunen, 1974; Kay, 1992), procedural rules (Gazdar, 1979; Karttunen and Peters, 1979), lexical and syntactic structure (Weischedel, 1979), intentions (Hir"
P95-1020,E93-1033,1,0.753967,"A &quot;-,married&quot;(x)) (VUtx)(-4bachelorU(=) --~ marriedi(x)) (vUtx )(-~bachelor&quot;( x ) --~ adulta( x )) (vu'x)(--,bachelorU(x) .-, maled(=)) Chris is not a bachelor presupposes that Chris is a male adult; Chris regrets that Mary came to the party presupposes that Mary came to the party. There is no contradiction between these two presuppositions, 148 3.3 P r a g m a t i c i n f e r e n c e s in s e q u e n c e s o f utterances We have already mentioned that speech repairs constitute a good benchmark for studying the generation and cancellation of pragmatic inferences along sequences of utterances (McRoy and Hirst, 1993). Suppose, for example, that Jane has two friends - John Smith and John Pevler - - and that her roommate Mary has met only John Smith, a married fellow. Assume now that Jane has a conversation with Mary in which Jane mentions only the name John because she is not aware that Mary does not know about the other John, who is a five-year-old boy. In this context, it is natural for Mary to become confused and to come to wrong conclusions. For example, Mary may reply that John is not a bachelor. Although this is true for both Johns, it is more appropriate for the married fellow than for the five-year"
R09-1084,W05-0604,0,0.0355319,"Missing"
R09-1084,C92-2082,0,0.338675,"y Definitions Tong Wang and Graeme Hirst Department of Computer Science University of Toronto Toronto, ON, M5S 3G4, Canada tong,gh@cs.toronto.edu Abstract the source language (Wu and Zhou, 2003), but the mapping on the lexical level between languages is far from bijective, which in turn leads to many exceptions to the hypothesis. The difficulties in finding features for strict synonymy partly come from the syntactic and stylistic diversity in free-texts — this is the motivation behind using dictionary definitions as a resource for synonym identification. Unlike the extraction strategy used by Hearst (1992), where hyponyms would necessarily follow the phrase such as: We investigate the problem of extracting synonyms from dictionary definitions. Our premise for using definition texts in dictionaries is that, in contrast to freetexts, their composition usually exhibits more regularities in terms of syntax and style and thus, will provide a better controlled environment for synonym extraction. We propose three extraction methods: two rule-based ones and one using the maximum entropy model; each method is evaluated on three experiments — by solving TOEFL synonym questions, by comparing extraction re"
R09-1084,W06-3811,0,0.0182012,"ndel and Senellart (2002) employed an algorithm similar to PageRank (Brin et al., 1998), and similarities between words can be computed using their adjacency matrix. Alternatively, the problem can be viewed from an information theory perspective and formalized to propagate information content instead of endorsement (Ho 1 Unfortunately, we have not been able to access the original work of Reichert et al. (1969); we resort to the description of their method by Amsler (1980) instead. 471 International Conference RANLP 2009 - Borovets, Bulgaria, pages 471–477 and C´edrick, 2004). Another example (Muller et al., 2006) is to simulate a random walk on the graph by building and iteratively updating a Markovian matrix, and the distance between words corresponds to the average number of steps the random walk takes from one node to the other. Despite all these interesting variations, the original method of inverted indexing has been left unevaluated for decades, and one of the objectives of this paper is to bring the evaluation of this method into the modern paradigm of NLP. Together with this algorithm, two other extraction methods will be discussed in Section 2; evaluations of these methods are conducted in th"
R09-1084,W03-1610,0,0.0234424,"to deal with such false positives. Taking POS for example, one can specify the POS of a given target word and only extract words that are of the same POS. Constraints can also be applied on where a target word is allowed to appear in order to be considered a synonym of the corresponding definiendum. Apart from all these, a more pertinent issue, as it turned out in later experiments, is actually the low coverage for lowfrequency target words, which do not appear often (or even not at all) in other words’ definitions. In fact, this conforms with the claim made by several previous authors (e.g., Wu and Zhou 2003) that coverage is a key issue for dictionarybased methods. 2.2 position. In PbE, instead of relating a definiendum to every word in its definition (as in IIE), we focus more on those definientia that follow particular patterns synonyms tend to follow in definition texts. Consequently, one of the objectives of PbE is to discover such patterns. 4 In case they do not, we can either start off with a group of known synonyms instead of one target word t, or even with another word that does lead to more appropriate situations in terms of Wi+1 , because at this stage, we aim at bootstrapping for patte"
R09-1084,J87-3001,0,\N,Missing
R09-1084,D08-1103,1,\N,Missing
R09-1084,J02-2001,1,\N,Missing
R09-1084,P08-2037,0,\N,Missing
R09-1084,P85-1037,0,\N,Missing
R09-1084,P08-3001,0,\N,Missing
R09-1084,P91-1019,0,\N,Missing
R09-1084,J06-2003,1,\N,Missing
R09-1084,P06-2111,0,\N,Missing
R09-1084,C90-3025,0,\N,Missing
R09-1084,P98-2127,0,\N,Missing
R09-1084,C98-2122,0,\N,Missing
R15-1057,P02-1047,0,0.240813,"437 2.2 Corpus RST-DT-PT CSTNews Rhetalho Summ-it CorpusTCC RST-DT Semi-supervised Discourse Parsing All the above cited approaches to DP use annotated data to extract discursive knowledge and are limited to the availability of this resource, which is expensive to obtain. Specially, it is important to note that, to obtain good performance in the task more data is necessary. Semi-supervised learning (SSL) is employed in scenarios in which there is some labeled data and large availability of unlabeled data, and manual annotation is an expensive task (Zhu, 2008). Related to the use of SSL in DP, Marcu and Echihabi (2002) used naive Bayes to train binary classifiers to distinguish between some types of relations, as Elaboration vs. Cause-ExplanationEvidence. For example, for this binary classifier, applying SSL, the accuracy increased from approximately 0.6 to 0.95 after the use of millions of new instances. Chiarcos (2012) used SSL to develop a probabilistic model mapping the occurrence of discourse markers and verbs to rhetorical relations. For Italian, Soria and Ferrari (1998) conducted work in the same direction. Sporleder and Lascarides (2005) performed similar work to Marcu and Echihabi, with similar res"
R15-1057,W01-1605,0,0.147156,"Missing"
R15-1057,P12-2042,0,0.0164249,"btain good performance in the task more data is necessary. Semi-supervised learning (SSL) is employed in scenarios in which there is some labeled data and large availability of unlabeled data, and manual annotation is an expensive task (Zhu, 2008). Related to the use of SSL in DP, Marcu and Echihabi (2002) used naive Bayes to train binary classifiers to distinguish between some types of relations, as Elaboration vs. Cause-ExplanationEvidence. For example, for this binary classifier, applying SSL, the accuracy increased from approximately 0.6 to 0.95 after the use of millions of new instances. Chiarcos (2012) used SSL to develop a probabilistic model mapping the occurrence of discourse markers and verbs to rhetorical relations. For Italian, Soria and Ferrari (1998) conducted work in the same direction. Sporleder and Lascarides (2005) performed similar work to Marcu and Echihabi, with similar results for a different set of relations and a more sophisticated classifier. Building on this, there is an interesting idea, known as never-ending learning (NEL) by Carlson et al. (2010), in which they apply SSL with infinite unlabeled data. The needed data is widely and freely available on the web. Their arc"
R15-1057,P09-1075,0,0.0180718,"according to RST. From Soricut and Marcu (2003). the EDUs, explicating the intentions of the author. For example, consider the sentence in Figure 1. It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer’s purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models (Soricut and Marcu, 2003), SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012). To obtain acceptable results, these approaches need plenty of labeled data. But even more than other levels of linguistic information, such as morphology or syntax, the annotation of discourse is an expensive task. Given this fact, what can we do when there is not enough data to perform effective learning of DP, as in languages with little annotated data? This paper describes a methodology to overcome the problem of insufficient labeled data in the task of identifying rhetorical relations b"
R15-1057,P12-1007,1,0.855186,"EDUs, explicating the intentions of the author. For example, consider the sentence in Figure 1. It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer’s purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models (Soricut and Marcu, 2003), SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012). To obtain acceptable results, these approaches need plenty of labeled data. But even more than other levels of linguistic information, such as morphology or syntax, the annotation of discourse is an expensive task. Given this fact, what can we do when there is not enough data to perform effective learning of DP, as in languages with little annotated data? This paper describes a methodology to overcome the problem of insufficient labeled data in the task of identifying rhetorical relations between Introduction A text is composed of cohe"
R15-1057,W98-0306,0,0.178079,"and large availability of unlabeled data, and manual annotation is an expensive task (Zhu, 2008). Related to the use of SSL in DP, Marcu and Echihabi (2002) used naive Bayes to train binary classifiers to distinguish between some types of relations, as Elaboration vs. Cause-ExplanationEvidence. For example, for this binary classifier, applying SSL, the accuracy increased from approximately 0.6 to 0.95 after the use of millions of new instances. Chiarcos (2012) used SSL to develop a probabilistic model mapping the occurrence of discourse markers and verbs to rhetorical relations. For Italian, Soria and Ferrari (1998) conducted work in the same direction. Sporleder and Lascarides (2005) performed similar work to Marcu and Echihabi, with similar results for a different set of relations and a more sophisticated classifier. Building on this, there is an interesting idea, known as never-ending learning (NEL) by Carlson et al. (2010), in which they apply SSL with infinite unlabeled data. The needed data is widely and freely available on the web. Their architecture runs 24 hours per day, forever, obtaining new information and performing a learning task. With the aim of surpassing the limitation of labeled RST in"
R15-1057,N03-1030,0,0.509887,"enough labeled data to obtain good discourse parsing, specially in the relation identification step, and the additional use of unlabeled data is a plausible solution. A workflow is presented that uses a semi-supervised learning approach. Instead of only a predefined additional set of unlabeled data, texts obtained from the web are continuously added. This obtains near human perfomance (0.79) in intra sentential rhetorical relation identification. An experiment for English also shows improvement using a similar workflow. 1 Figure 1: An example of sentence-level structure according to RST. From Soricut and Marcu (2003). the EDUs, explicating the intentions of the author. For example, consider the sentence in Figure 1. It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer’s purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models (Soricut and Marcu, 2003), SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al.,"
R15-1057,D12-1083,0,0.0262114,"Missing"
R15-1057,D09-1036,0,0.0319408,"greement). The authors, then, use the probabilistic model with manual segmentation and syntactic trees to see the impact of this information in the parsing and the model achieves 0.75. Hernault et al. (2010) use support vector machine (SVM) classifiers to perform DP. This discourse parser is named HILDA (HIgh-Level Discourse Analyser). This work used a set of 41 rhetorical relations and achieves a F-measure of 0.48 in the step of relation identification, both intra-sentential and inter-sentential. Feng and Hirst (2012) improve HILDA by incorporating new proposed features and some adapted from Lin et al. (2009). Another important decision was the specification of features for intra-sentential and inter-sentential relationships and the use of contextual features in the building of the rhetorical tree. Considering the approach to intra-sentential relation identification, with 18 RST relations this work achieves a macro average F-measure of 0.49 and weighted average Fmeasure of 0.77 in relation identification. Joty et al. (2012) use a joint modelling approach to identify the structure and the relations at the sentence-level using DCRFs (dynamic conditional random fields) and a non-greedy bottom-up meth"
R15-1057,P95-1037,0,0.0236275,"n discourse and, to avoid overfitting of a learning model on the lessfrequent relations, no balancing was made. The relation Summary, for example, occurs only 2 times, and Elaboration occurs 1491 times, making very difficult the identification of the Summary relation. 4 Adapted Models Syntactic information is crucial in SPADE (Soricut and Marcu, 2003) and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010)). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser. In this adaptation, only pairs of adjacent segments at sentence-level were considered, and nuclearity was not considered, in order to avoid sparseness in the data. Training the adapted model (here called SPADE-PT) using the RST-DT-PT achieved F-measure of 0.30. The precision was 0.69, but the recall was only 0.19. The same features used by HILDA (Hernault et al., 2010) were extracted from the pairs of adjacent segments at sentence-level and many machine learning algorithms were tested, besides the SVM, which was used in the or"
S07-1071,N06-2015,0,0.0357647,"or manually-annotated data. ∑i m i j ∑i, j mi j mi j P(wi |c j ) = ∑i m i j P(c j ) = (2) (3) 329 c pmi = argmax c j ∈C ∑ PMI(wi , c j ) (4) (5) (6) (7) wi ∈W Note that this PMI-based classifier does not capitalize on prior probabilities of the different senses. 4 Data 4.1 English Lexical Sample Task The English Lexical Sample Task training and test data (Pradhan et al., 2007) have 22281 and 4851 instances respectively for 100 target words (50 nouns and 50 verbs). WordNet 2.1 is used as the sense inventory for most of the target words, but certain words have one or more senses from OntoNotes (Hovy et al., 2006). Many of the finegrained senses are grouped into coarser senses. Our approach relies on representing a sense with a number of near-synonymous words, for which a thesaurus is a natural source. Even though the approach can be ported to WordNet4 , there was no easy 4 The synonyms within a synset, along with its one-hop neighbors and all its hyponyms, can represent that sense. W ORDS all nouns only verbs only BASELINE 27.8 25.6 29.2 T RAINING DATA PMI- BASED NA¨I VE BAYES 41.4 50.8 43.4 53.6 38.4 44.5 P RIOR 37.4 18.1 58.9 T EST DATA L IKELIHOOD NA¨I VE BAYES 49.4 52.1 49.6 49.7 49.1 54.7 Table 1"
S07-1071,O97-1002,0,0.0672605,"to correct real-word spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the 1 The McCarthy et al. (2004) system needs to first generate a distributional thesaurus from the target text (if it is large enough—a few million words) or from another large text with a distribution of senses similar to the target text. ... .. . 327 Figure 1: The cross-lingual candidate senses of Chiand . nese words distributional concept-distance measures performed better than all WordNet-based measures as well, except for the Jiang and Conrath (1997) measure. 2.2 Cross-lingual word–category co-occurrence matrix Given a Chinese word wch in context, we use a Chinese–English bilingual lexicon to determine its different possible English translations. Each English translation wen may have one or more possible coarse senses, as listed in an English thesaurus. These English thesaurus concepts (cen ) will be referred to as cross-lingual candidate senses of the Chinese word wch .2 Figure 1 depicts examples. We create a cross-lingual word–category cooccurrence matrix (CL-WCCM) with Chinese word types wch as one dimension and English thesaurus conce"
S07-1071,S07-1004,0,0.323552,"e a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical Sample Task (Jin et al., 2007) is to select from a given list a suitable English translation of a Chinese target word in context. Mohammad et al. (2007) proposed a way to create cross-lingual distributional profiles of a concepts (CL-DPCs)— the strengths of association between the concepts of one language and words of another. For this task, we mapped the list of English translations to appropriate thesaurus categories and used an implementation of a CL-DPC–based unsupervised na¨ıve Bayes classifier to identify the intended senses (and thereby the English translations) of target Chinese words. 2 Distributional profiles of"
S07-1071,S07-1009,0,0.189256,"the target in each of its senses. However, only limited amounts of senseannotated data exist and it is expensive to create. In our previous work (Mohammad and Hirst, 2006a), The English Lexical Sample Task (Pradhan et al., 2007) is a traditional word sense disambiguation task wherein the intended (WordNet) sense of a target word is to be determined from its context. We manually mapped the WordNet senses to the categories in a thesaurus and the DPC-based na¨ıve Bayes classifier was used to identify the intended sense (category) of the target words. The object of the Lexical Substitution Task (McCarthy and Navigli, 2007) is to replace a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical"
S07-1071,P04-1036,0,0.0308951,"he words in it. For each word, the strength of association of each of the words in its context (±5 words) with each of its senses is summed. The sense that has the highest cumulative association is chosen as the intended sense. A new bootstrapped WCCM is created such that each cell mi j , corresponding to word wen i and concept cen , is populated with the number of times wen j i en co-occurs with any word used in sense c j . Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain near-upper-bound results in the task of determining word sense dominance. Unlike the McCarthy et al. (2004) dominance system, this approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sense-distributed text1 . Mohammad and Hirst (2006b) used the DPC-based monolingual distributional measures of concept-distance to rank word pairs by their semantic similarity and to correct real-word spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the 1 The McCarthy et al. (2004) system needs to first generate a distributional thesaurus from the target text (i"
S07-1071,E06-1016,1,0.928947,"stributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words—the distributional profile of a concept (DPC)—without the use of manually annotated data. We implemented an unsupervised na¨ıve Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese– English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its perfo"
S07-1071,W06-1605,1,0.921315,"stributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words—the distributional profile of a concept (DPC)—without the use of manually annotated data. We implemented an unsupervised na¨ıve Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese– English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its perfo"
S07-1071,D07-1060,1,0.850175,"sible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical Sample Task (Jin et al., 2007) is to select from a given list a suitable English translation of a Chinese target word in context. Mohammad et al. (2007) proposed a way to create cross-lingual distributional profiles of a concepts (CL-DPCs)— the strengths of association between the concepts of one language and words of another. For this task, we mapped the list of English translations to appropriate thesaurus categories and used an implementation of a CL-DPC–based unsupervised na¨ıve Bayes classifier to identify the intended senses (and thereby the English translations) of target Chinese words. 2 Distributional profiles of concepts In order to determine the strength of association between a sense of the target word and its co-occurring words,"
S07-1071,S07-1016,0,0.332879,"poor. 1 Introduction Determining the intended sense of a word is potentially useful in many natural language tasks including machine translation and information retrieval. The best approaches for word sense disambiguation are supervised and they use words that co-occur with the target as features. These systems rely on senseannotated data to identify words that are indicative of the use of the target in each of its senses. However, only limited amounts of senseannotated data exist and it is expensive to create. In our previous work (Mohammad and Hirst, 2006a), The English Lexical Sample Task (Pradhan et al., 2007) is a traditional word sense disambiguation task wherein the intended (WordNet) sense of a target word is to be determined from its context. We manually mapped the WordNet senses to the categories in a thesaurus and the DPC-based na¨ıve Bayes classifier was used to identify the intended sense (category) of the target words. The object of the Lexical Substitution Task (McCarthy and Navigli, 2007) is to replace a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its i"
S07-1071,C92-2070,0,0.588737,"Toronto Toronto, ON M5S 3G4 Canada smm@cs.toronto.edu Philip Resnik Graeme Hirst Dept. of Computer Science Dept. of Linguistics and UMIACS University of Maryland University of Toronto College Park, MD 20742 Toronto, ON M5S 3G4 USA Canada gh@cs.toronto.edu resnik@umiacs.umd.edu Abstract we proposed an unsupervised approach to determine the strength of association between a sense or concept and its co-occurring words—the distributional profile of a concept (DPC)—relying simply on raw text and a published thesaurus. The categories in a published thesaurus were used as coarse senses or concepts (Yarowsky, 1992). We now show how distributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word"
W02-0909,J93-1003,0,0.178753,"Missing"
W02-0909,J02-2001,1,0.822493,"lexical choice between nearsynonyms. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web). We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry. 1 Introduction Edmonds and Hirst (2002 to appear) developed a lexical choice process for natural language generation (NLG) or machine translation (MT) that can decide which near-synonyms are most appropriate in a particular situation. The lexical choice process has to choose between clusters of near-synonyms (to convey the basic meaning), and then to choose between the near-synonyms in each cluster. To group near-synonyms in clusters we trust lexicographers’ judgment in dictionaries of synonym differences. For example task, job, duty, assignment, chore, stint, hitch all refer to a one-time piece of work, but which one to choose de"
W02-0909,P01-1025,0,0.0389636,"As in our work, three types of collocations are distinguished: words that collocate well; words that tend to not occur together, but if they do the reading is acceptable; and words that must not be used together because the reading will be unnatural (anti-collocations). In a similar manner with (Pearce, 2001), in section 3, we don’t record collocations in our lexical knowledge-base if they don’t help discriminate between near-synonyms. A difference is that we use more than frequency counts to classify collocations (we use a combination of t-test and MI). Our evaluation was partly inspired by Evert and Krenn (2001). They collect collocations of the form noun-adjective and verb-prepositional phrase. They build a solution using two human judges, and use the solution to decide what is the best threshold for taking the N highest-ranked pairs as true collocations. In their experiment MI behaves worse that other measures (LL, t-test), but in our experiment MI on the Web achieves good results. 8 Conclusions and Future Work We presented an unsupervised method to acquire knowledge about the collocational behaviour of near-synonyms. Our future work includes improving the way we combine the five measures for ranki"
W02-0909,W98-1426,0,0.011249,"e the Web as a corpus for this task (and a modified form of mutual information), and we distinguish three types of collocations (preferred, less-preferred, and anticollocations). We are concerned with extracting collocations for use in lexical choice. There is a lot of work on using collocations in NLG (but not in the lexical choice sub-component). There are two typical approaches: the use of phrasal templates in the form of canned phrases, and the use of automatically extracted collocations for unification-based generation (McKeown and Radev, 2000). Statistical NLG systems (such as Nitrogen (Langkilde and Knight, 1998)) make good use of the most frequent words and their collocations. But such a system cannot choose a less-frequent synonym that may be more appropriate for conveying desired nuances of meaning, if the synonym is not a frequent word. Finally, there is work related to ours from the point of view of the synonymy relation. Turney (2001) used mutual information to detect the best answer to questions about synonyms from Test of English as a Foreign Language (TOEFL) and English as a Second Language (ESL). Given a problem word (with or without context), and four alternative words, the question is to c"
W02-0909,J93-1007,0,0.485363,"ur purposes, such as proper names (tagged NP0). If we keep the proper names, they are likely to be among the highest-ranked collocations. There are many statistical methods that can be used to identify collocations. Four general methods are presented by Manning and Sch¨utze (1999). The first one, based on frequency of co-occurrence, 3 http://www.hcu.ox.ac.uk/BNC/ does not consider the length of the corpus. Part-ofspeech filtering is needed to obtain useful collocations. The second method considers the means and variance of the distance between two words, and can compute flexible collocations (Smadja, 1993). The third method is hypothesis testing, which uses statistical tests to decide if the words occur together with probability higher than chance (it tests whether we can reject the null hypothesis that the two words occurred together by chance). The fourth method is (pointwise) mutual information, an informationtheoretical measure. We use Ted Pedersen’s Bigram Statistics Package4 . BSP is a suite of programs to aid in analyzing bigrams in a corpus (newer versions allow Ngrams). The package can compute bigram frequencies and various statistics to measure the degree of association between two wo"
W02-0909,J90-1003,0,\N,Missing
W03-1310,P01-1037,0,0.0177342,"processing of yes–no questions is an important task in MQA. 3.2 Question–answer matching The matching of question and answer is the process that most GQA systems put great effort into. Different methods are applied according to different views of the problem. The approaches can be classified into two categories: knowledge-intensive and data-intensive. Knowledge-intensive approaches try to find the correct match between a question and the answer by using effective natural language processing techniques that combine linguistic and realworld knowledge. Typical systems include those of Pas¸ca and Harabagiu (2001) and Hovy, Hermjakob, and Lin (2001). Data-intensive approaches explore information embedded in the data sources to extract the evidence that supports a good answer. They can be further divided into information extraction–based (Soubbotin, 2001), redundancy-based (Clarke et al., 2001; Dumais et al., 2002), and statistical QA (Ittycheriah et al., 2001). Many systems contain elements of both approaches. Although there have been many technologies developed for matching the answer with the question, they are not applicable to the medical area directly for the following reasons. Knowledge taxonomy."
W03-1310,P00-1071,0,0.018717,"te, e.g., 15 May 1932. However, in the medical area, when questions are usually answered by relative time, e.g., two hours after the onset of chest pain. Sometimes the answers are not even a time; instead, they are a clinical condition, e.g., in response to When should antibiotics be applied? Some problems of MQA are not addressed at all by current QA technologies: Question focus. Sometimes, the answer type is not enough to determine what a question is about. Other information contained in the question is needed to understand its goal. This information is defined as the focus of the question (Moldovan and Harabagiu, 2000). Although different systems use different names for the idea of question focus, it is regarded to be very important in question processing. However, there is still no special technique to tackle this problem. Yes–no questions. As mentioned, most current QA systems focus on wh- questions; yes–no questions are still left untouched. However, we have found that they are very common in our collection of clinical questions that arose in patient treatment. Efficient processing of yes–no questions is an important task in MQA. 3.2 Question–answer matching The matching of question and answer is the pro"
W03-2502,P01-1005,0,\N,Missing
W04-0509,J02-3001,0,0.0221587,"se. In medical text, the appearance of some words is found often to be a signal of the occurrence of an outcome, and usually several words signal the occurrence of one single outcome. The combination approach that we applied for identifying outcomes is based on this observation. Our approach does not extract the whole outcome at once. Instead, it tries to identify the different parts of an outcome that may be scattered in the sentence, and then combines them to form the complete outcome. 2.2.1 Related work Rule-based methods and machine-learning approaches have been used for similar problems. Gildea and Jurafsky (2002) used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme, target, goal, and the boundaries of the roles (Baker et al., 2003). A set of features were learned from a large training set, and then applied to the unseen data to detect the roles. The performance of the system was quite good. However, it requires a large training set for related roles, which is not available in many tasks, including tasks in the medical area. Rule-based methods are explored in information extraction (IE) to identify roles to fill slots in some pre-defined"
W04-0509,W03-1305,0,0.0272049,"ne; is this medication applied to this disease? These are the kind of relations that we are interested in. In this work, we use a cue-word–based approach to identify semantic classes in the treatment scenario and analyze the relations between them. We also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions. 2 Identifying Semantic Classes in Medical Text 2.1 Diseases and Medications The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003), Shen et al. (2003). However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001). UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns prope"
W04-0509,W03-1310,1,0.779311,"(Sackett et al., 2000). In this format, a clinical question is represented by a set of four fields that correspond to the basic elements of the question: P: a description of the patient (or the problem); I: an intervention; C: a comparison or control intervention (may be omitted); O: the clinical outcome. For example, the question shown above can be represented in PICO format as follows: P: I: C: O: myocardial infarction thrombolysis — mortality Our work in the project is to extend the keyword retrieval to a system that can answer questions expressed in natural language. In our earlier work (Niu et al., 2003), we showed that current technologies for factoid question answering (QA) are not adequate for clinical questions, whose answers must often be obtained by synthesizing relevant context. To adapt to this new characteristic of QA in the medical domain, we exploit semantic classes and relations between them in medical text. Semantic classes are important for our task because the information contained in them is often a good candidate for answering clinical questions. In the example above, PICO elements correspond to three semantic classes: DISEASE (medical problem of the patient), INTERVENTION (m"
W04-0509,W02-1011,0,0.0196787,"r observation is that outcomes often involve the change in a clinical value. For example, after a medication was applied to a disease, something was increased (enhanced, more, . . . ) or decreased (reduced, less, . . . ). Thus the polarity of an outcome is often determined by how change happens: if a bad thing (e.g., mortality) is reduced then it is a positive outcome; if the bad thing is increased, then the outcome is negative. We try to capture this observation by adding context features to the feature set. The way they were added is similar to incorporating the negation effect described by Pang et al. (2002). But instead of just finding a “negation word” (not, isn’t, didn’t, etc.), we need to find two groups of words: those indicating more and those indicating less. In the training text, we found 9 words in the first group and 7 words in the second group. When pre-processing text for classification, following the method of Pang et al., we attached the tag MORE to all words between the more-words and the following punctuation mark, and the tag LESS to the words after the less-words. Table 5: Results of outcome polarity classification Features Baseline Original unigrams Unigrams with disease Unigra"
W04-0509,W03-1307,0,0.0133455,"ion applied to this disease? These are the kind of relations that we are interested in. In this work, we use a cue-word–based approach to identify semantic classes in the treatment scenario and analyze the relations between them. We also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions. 2 Identifying Semantic Classes in Medical Text 2.1 Diseases and Medications The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003), Shen et al. (2003). However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001). UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categorie"
W04-2607,2003.mtsummit-papers.26,0,0.0212783,"the structure of LIS thesauri and Roget’s Thesaurus are similar. They are both hierarchically organized — Roget’s by Roget’s own principles of domain and topic division and LIS thesauri by a broad-term / narrow-term structure — but they also both have a non-hierarchical, non-classified “structure” (or at least mechanism) for representing non-classical relations. But while both, unlike WordNet, give access to non-classically related word pairs, they don’t give any indication of what the actual relation between the words is. Other recent computational work such as that of Ji, Ploux, and Wehrli (2003) suffers from the same problem, in that groups of related words are created (in this case through automatic processing of text corpora), but the actual relations that hold between the members of the groups are not determined. 2.2 Non-classical lexical semantic relations Lakoff (1987) gives the name “classical” to categories whose members are related by shared properties. We will extend Lakoff’s terminology and refer to relations that depend on the sharing of properties of classical categories as classical relations. Hence we will use the term non-classical for relations that do not depend on t"
W04-2607,W97-0713,0,0.119097,"Missing"
W04-2607,J91-1002,1,0.489319,"s contribute to their understanding of it. Related word pairs may join together to form larger groups of related words that can extend freely over sentence boundaries. These larger word groups contribute to the meaning of text through “the cohesive effect achieved by the continuity of lexical meaning” (Halliday and Hasan, 1976, p. 320, emphasis added). Lexical semantic relations are the building blocks of lexical cohesion, and so a clear understanding of their nature and behavior is crucial. Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). In recent lexical cohesion research in linguistics (Hasan, 1984; Halliday and Hasan, 1989; Martin, 1992) non-classical relations are largely ignored, and the same is true in implementations of lexical cohesion in computational linguistics (Barzilay and Elhadad, 1999; Silber and McCoy, 2002), as the lexical resource used is WordNet. It is notable, however, that the original view of lexical semantic relations in the lexical cohesion work of Halliday and Hasan (1976) was very broad and general; the only criterion was that there had t"
W04-2607,J02-4004,0,0.011781,"Lexical semantic relations are the building blocks of lexical cohesion, and so a clear understanding of their nature and behavior is crucial. Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). In recent lexical cohesion research in linguistics (Hasan, 1984; Halliday and Hasan, 1989; Martin, 1992) non-classical relations are largely ignored, and the same is true in implementations of lexical cohesion in computational linguistics (Barzilay and Elhadad, 1999; Silber and McCoy, 2002), as the lexical resource used is WordNet. It is notable, however, that the original view of lexical semantic relations in the lexical cohesion work of Halliday and Hasan (1976) was very broad and general; the only criterion was that there had to be a recognizable relation between two words. Most research on lexical semantic relations in linguistics (Cruse, 1986) and psychology has also ignored nonclassical relations (with the exception of Chaffin and Herrmann, 1984); however there have been recent calls to broaden the focus and include non-classical relations as well (McRae and Boisvert, 1998"
W04-2607,W97-0703,0,\N,Missing
W06-1605,E06-1016,1,0.904743,"concepts or senses (Yarowsky, 1992). Words with more than one sense are listed in more than one category. A published thesaurus thus provides us with a very coarse human-developed set or inventory of word senses or concepts2 that are more intuitive and discernible than the “concepts” generated by dimensionality-reduction methods such as latent semantic analysis. Using coarse senses from a known inventory means that the senses can be represented unambiguously by a large number of possibly ambiguous words (conveniently available in the thesaurus)—a feature that we exploited in our earlier work (Mohammad and Hirst, 2006) to determine useful estimates of the strength of association between a concept and co-occurring words. In this paper, we go one step further and use the idea of a very coarse sense inventory to develop a framework for distributional measures of concepts that can more naturally and more accurately be used in place of semantic measures of word senses. We use the Macquarie Thesaurus (Bernard, 1986) as a sense inventory and repository of words pertaining to each sense. It has 812 categories with around 176,000 word tokens and 98,000 word types. This allows us to have much smaller concept–concept"
W06-1605,P05-1016,0,0.010195,"cosine of the an41 gle between their aggregate vectors. However, as we pointed out in Mohammad and Hirst (2005), such aggregate co-occurrence vectors are expected to be noisy because they are created from data that is not sense-annotated. Therefore, we employed simple word sense disambiguation and bootstrapping techniques on our base WCCM to create more-accurate co-occurrence vectors, which gave markedly higher accuracies in the task of determining word sense dominance. In the experiments described in this paper, we used these bootstrapped co-occurrence vectors to determine concept-distance. Pantel (2005) also provides a way to create co-occurrence vectors for WordNet senses. The lexical co-occurrence vectors of words in a leaf node are propagated up the WordNet hierarchy. A parent node inherits those co-occurrences that are shared by its children. Lastly, co-occurrences not pertaining to the leaf nodes are removed from its vector. Even though the methodology attempts at associating a WordNet node or sense with only those co-occurrences that pertain to it, no attempt is made at correcting the frequency counts. After all, word1–word2 co-occurrence frequency (or association) is likely not the sa"
W06-1605,W06-2501,0,0.0499162,"Moreover, if we consider correction ratio to be the bottom-line statistic, then the Distribconcept measures outperform all WNetconcept measures except the Jiang–Conrath measure. If we consider correction performance to be the bottom-line statistic, then again we see that the distributional concept-distance measures outperform the worddistance measures, except in the case of Linpmi , which gives slightly poorer results with conceptdistance. Also, in contrast to correction ratio values, using the Leacock–Chodorow measure results in relatively higher correction performance values 6 Related Work Patwardhan and Pedersen (2006) create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss. The distance between two senses is then determined by the cosine of the an41 gle between their aggregate vectors. However, as we pointed out in Mohammad and Hirst (2005), such aggregate co-occurrence vectors are expected to be noisy because they are created from data that is not sense-annotated. Therefore, we employed simple word sense disambiguation and bootstrapping techniques on our base WCCM to create more-accurate co-occurrence vectors, which gave markedly hig"
W06-1605,J90-1003,0,0.271448,"a semantic network, and hence they measure the distance between the concepts or word-senses that the nodes of the resource represent. Examples include the measure for MeSH proposed by Rada et al. (1989) and those for WordNet proposed by Leacock and Chodorow (1998) and Jiang and Conrath (1997). (Some of the more successful measures, such as Jiang–Conrath, also use information content derived from word frequency.) Typically, these measures rely on an extensive hierarchy of hyponymy relationships for nouns. Therefore, these measures 1 In our experiments, we set negative PMI values to 0, because Church and Hanks (1990), in their seminal paper on word association ratio, show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus. 35 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 35–43, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 The distributional hypothesis and its limitations Table 1: Measures of DP distance and measures of strength of association. DP distance α-skew divergence cosine Jensen–Shannon divergence Lin The distributional hypothesis (Firth, 1"
W06-1605,J05-2004,0,0.0234816,":: .. . A cell mi j , corresponding to word wi and category c j , contains the number of times wi co-occurs (in a window of 5 words in the corpus) with any of the words listed under category c j in the thesaurus. Intuitively, the cell mi j captures the number of times c j and wi co-occur. A contingency table for a single word and single category can be created by simply collapsing all other rows and columns into one and summing their frequencies. Applying a suitable statistic, such as odds 2 We use the terms senses and concepts interchangeably. This is in contrast to studies, such as that of Cooper (2005), that attempt to make a principled distinction between them. 37 BNC BNC Thesaurus word–word co-occurrence counting word–category co-occurrence counting word–word co-occurrence matrix word–category co-occurrence matrix bootstrapping and sense disambiguation distributional measures distributional measures distributional relatedness of words distributional relatedness of concepts Figure 1: Distributional word-distance. Figure 2: Distributional concept-distance. ratio, on the contingency table gives the strength of association between a concept (category) and co-occurring word. Therefore, the WCC"
W06-1605,C92-2070,0,0.766966,"Missing"
W06-1605,O97-1002,0,0.434151,"nt–theme) relation with the target word. We will refer to the former kind of DPs as relation-free. Usually in 1 Semantic and distributional measures Measures of distance of meaning are of two kinds. The first kind, which we will refer to as semantic measures, rely on the structure of a resource such as WordNet or, in some cases, a semantic network, and hence they measure the distance between the concepts or word-senses that the nodes of the resource represent. Examples include the measure for MeSH proposed by Rada et al. (1989) and those for WordNet proposed by Leacock and Chodorow (1998) and Jiang and Conrath (1997). (Some of the more successful measures, such as Jiang–Conrath, also use information content derived from word frequency.) Typically, these measures rely on an extensive hierarchy of hyponymy relationships for nouns. Therefore, these measures 1 In our experiments, we set negative PMI values to 0, because Church and Hanks (1990), in their seminal paper on word association ratio, show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus. 35 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processin"
W06-1605,J06-1003,1,\N,Missing
W06-1605,P98-2127,0,\N,Missing
W06-1605,C98-2122,0,\N,Missing
W12-2205,C10-2011,1,0.94464,"ommon use of words across different age groups. With respect to our goal of lowering reliance on fine-grained annotation, the work of Tanaka-Ishii et al. (2010) is also relevant; they create a readability system that requires only two general classes of text (easy and difficult), other texts are ranked relative to these two classes using regression. Other lexical acquisition work has also informed our approach here. For instance, our co-occurrence method is an adaption of a technique applied in sentiment analysis (Turney and Littman, 2003), which has recently been shown to work for formality (Brooke et al., 2010), a dimension of stylistic variation that seems closely related to readability. Taboada et al. (2011) validate their sentiment lexicon using crowdsourced judgments of the relative polarity of pairs of words, and in fact crowd sourcing has been applied directly to the creation of emotion lexicons (Mohammad and Turney, 2010). 3 Resources Our primary resource is an existing lexicon, previously built under the supervision of the one of authors. This resource, which we will refer to as the Difficulty lexicon, consists of 15,308 words and expressions classified into three difficulty categories: begi"
W12-2205,N07-4002,0,0.0315729,"ity metric (Dale and Chall, 1995) and a number of machine learning approaches to readability rely on lexical features (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009; Tanaka-Ishii et al., 2010), the readability of individual lexical items is not addressed directly in these approaches. Nevertheless, information about the difficulty of individual lexical items, in addition to being useful for text readability classification (Kidwell et al., 2009), can be applied to other tasks, for instance lexical simplification (Carroll et al., 1999; Burstein et al., 2007). Our interest is in providing students with educational software that is sensitive to the difficulty of particular English expressions, providing proactive support for those which are likely to be outside a reader’s vocabulary. However, our existing lexical resource is coarse-grained and lacks coverage. In this paper, we explore the extent to which an automatic approach could be used to fill in the gaps of our lexicon. Prior approaches have generally depended on some kind of age-graded corpus (Kidwell et al., 2009; Li and Feng, 2011), but this kind of resource is unlikely to provide the cover"
W12-2205,E99-1042,0,0.327289,"t one popular readability metric (Dale and Chall, 1995) and a number of machine learning approaches to readability rely on lexical features (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009; Tanaka-Ishii et al., 2010), the readability of individual lexical items is not addressed directly in these approaches. Nevertheless, information about the difficulty of individual lexical items, in addition to being useful for text readability classification (Kidwell et al., 2009), can be applied to other tasks, for instance lexical simplification (Carroll et al., 1999; Burstein et al., 2007). Our interest is in providing students with educational software that is sensitive to the difficulty of particular English expressions, providing proactive support for those which are likely to be outside a reader’s vocabulary. However, our existing lexical resource is coarse-grained and lacks coverage. In this paper, we explore the extent to which an automatic approach could be used to fill in the gaps of our lexicon. Prior approaches have generally depended on some kind of age-graded corpus (Kidwell et al., 2009; Li and Feng, 2011), but this kind of resource is unlik"
W12-2205,E09-1027,0,0.0205881,"s are fairly interchangeable (van Oosten et al., 2010). In machine-learning classification of texts by grade level, unigrams have been found to be reasonably effective for this task, outperforming readability metrics (Si and Callan, 2001; Collins-Thompson and Callan, 2005). Var33 NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 33–39, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics ious other features have been explored, including parse (Petersen and Ostendorf, 2009) and coherence features (Feng et al., 2009), but the consensus seems to be that lexical features are the most consistently useful for automatic readability classification, even when considering non-native readers (Heilman et al., 2007). In the field of readability, the work of Kidwell et al. (2009) is perhaps closest to ours. Like the above, their goal is text readability classification, but they proceed by first deriving an age of acquisition for each word based on its statistical distribution in ageannotated texts. Also similar is the work of Li and Feng (2011), who are critical of raw frequency as an indicator and instead identify c"
W12-2205,N07-1058,0,0.135621,"ese various features. We show the efficacy of this approach by comparing our lexicon with an existing coarse-grained, low-coverage resource and a new crowdsourced annotation. 1 Fraser Shein*† Introduction With its goal of identifying documents appropriate to readers of various proficiencies, automatic analysis of readability is typically approached as a textlevel classification task. Although at least one popular readability metric (Dale and Chall, 1995) and a number of machine learning approaches to readability rely on lexical features (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009; Tanaka-Ishii et al., 2010), the readability of individual lexical items is not addressed directly in these approaches. Nevertheless, information about the difficulty of individual lexical items, in addition to being useful for text readability classification (Kidwell et al., 2009), can be applied to other tasks, for instance lexical simplification (Carroll et al., 1999; Burstein et al., 2007). Our interest is in providing students with educational software that is sensitive to the difficulty of particular English expressions, providing proactive support for thos"
W12-2205,D09-1094,0,0.494659,"of readability is typically approached as a textlevel classification task. Although at least one popular readability metric (Dale and Chall, 1995) and a number of machine learning approaches to readability rely on lexical features (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009; Tanaka-Ishii et al., 2010), the readability of individual lexical items is not addressed directly in these approaches. Nevertheless, information about the difficulty of individual lexical items, in addition to being useful for text readability classification (Kidwell et al., 2009), can be applied to other tasks, for instance lexical simplification (Carroll et al., 1999; Burstein et al., 2007). Our interest is in providing students with educational software that is sensitive to the difficulty of particular English expressions, providing proactive support for those which are likely to be outside a reader’s vocabulary. However, our existing lexical resource is coarse-grained and lacks coverage. In this paper, we explore the extent to which an automatic approach could be used to fill in the gaps of our lexicon. Prior approaches have generally depended on some kind of age-g"
W12-2205,W10-0204,0,0.0272307,"ese two classes using regression. Other lexical acquisition work has also informed our approach here. For instance, our co-occurrence method is an adaption of a technique applied in sentiment analysis (Turney and Littman, 2003), which has recently been shown to work for formality (Brooke et al., 2010), a dimension of stylistic variation that seems closely related to readability. Taboada et al. (2011) validate their sentiment lexicon using crowdsourced judgments of the relative polarity of pairs of words, and in fact crowd sourcing has been applied directly to the creation of emotion lexicons (Mohammad and Turney, 2010). 3 Resources Our primary resource is an existing lexicon, previously built under the supervision of the one of authors. This resource, which we will refer to as the Difficulty lexicon, consists of 15,308 words and expressions classified into three difficulty categories: beginner, intermediate, and advanced. Beginner, which was intended to capture the vocabulary of early elementary school, is an amalgamation of various smaller sources, including the Dolch list (Dolch, 1948). The intermediate words, which include words learned in late elementary and middle 34 Table 1: Examples from the Difficul"
W12-2205,J11-2001,1,0.469453,"e-grained annotation, the work of Tanaka-Ishii et al. (2010) is also relevant; they create a readability system that requires only two general classes of text (easy and difficult), other texts are ranked relative to these two classes using regression. Other lexical acquisition work has also informed our approach here. For instance, our co-occurrence method is an adaption of a technique applied in sentiment analysis (Turney and Littman, 2003), which has recently been shown to work for formality (Brooke et al., 2010), a dimension of stylistic variation that seems closely related to readability. Taboada et al. (2011) validate their sentiment lexicon using crowdsourced judgments of the relative polarity of pairs of words, and in fact crowd sourcing has been applied directly to the creation of emotion lexicons (Mohammad and Turney, 2010). 3 Resources Our primary resource is an existing lexicon, previously built under the supervision of the one of authors. This resource, which we will refer to as the Difficulty lexicon, consists of 15,308 words and expressions classified into three difficulty categories: beginner, intermediate, and advanced. Beginner, which was intended to capture the vocabulary of early ele"
W12-2205,J10-2002,0,0.0976038,"pproach by comparing our lexicon with an existing coarse-grained, low-coverage resource and a new crowdsourced annotation. 1 Fraser Shein*† Introduction With its goal of identifying documents appropriate to readers of various proficiencies, automatic analysis of readability is typically approached as a textlevel classification task. Although at least one popular readability metric (Dale and Chall, 1995) and a number of machine learning approaches to readability rely on lexical features (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009; Tanaka-Ishii et al., 2010), the readability of individual lexical items is not addressed directly in these approaches. Nevertheless, information about the difficulty of individual lexical items, in addition to being useful for text readability classification (Kidwell et al., 2009), can be applied to other tasks, for instance lexical simplification (Carroll et al., 1999; Burstein et al., 2007). Our interest is in providing students with educational software that is sensitive to the difficulty of particular English expressions, providing proactive support for those which are likely to be outside a reader’s vocabulary. Ho"
W12-2205,van-oosten-etal-2010-towards,0,0.0544303,"Missing"
W12-2205,N12-1063,0,\N,Missing
W12-2205,W10-1001,0,\N,Missing
W12-2205,P03-1054,0,\N,Missing
W12-2205,C10-2032,0,\N,Missing
W12-2504,baccianella-etal-2010-sentiwordnet,0,0.0157748,"Chall, 1995). Average unigram count in 1T Corpus Another metric of whether a word is commonly used. We use the unigram counts in the 1T 5-gram Corpus (Brants and Franz, 2006). Here and below, if a word is not included it is given a zero. Sentiment polarity The positive or negative stance of a span could be viewed as a stylistic variable. We test two lexicons, a hand-built lexicon for the SO-CAL sentiment analysis system which has shown superior performance in lexicon-based sentiment analysis (Taboada et al., 2011), and SentiWordNet (SWN), a high-coverage automatic lexicon built from WordNet (Baccianella et al., 2010). The polarity of each word over the span is averaged. Sentiment extremity Both lexicons provide a measure of the degree to which a word is positive or negative. Instead of summing the sentiment scores, we sum their absolute values, to get a measure of how extreme (subjective) the span is. Formality Average formality score, using a lexicon of formality (Brooke et al., 2010) built using latent semantic analysis (LSA) (Landauer and Dumais, 1997). Dynamic General Inquirer The General Inquirer dictionary (Stone et al., 1966), which was used for stylistic inconsistency detection by Guthrie (2008),"
W12-2504,W97-0304,0,0.0833448,"ng to a separate feature. We test with vectors derived from the word-document matrix of the ICWSM 2009 blog dataset (Burton et al., 2009) which includes 1.3 billion tokens, and also from the BNC (Burnard, 2000), which is 100 million tokens. The length of the vector depends greatly on the frequency of the word; since this is being accounted for elsewhere, we normalize each vector to the unit circle. 5 5.1 Evaluation method Metrics To evaluate our method we apply standard topic segmentation metrics, comparing the segmentation boundaries to a gold standard reference. The measure Pk , proposed by Beeferman et al. (1997), uses a probe window equal to half the average length of a segment; the window slides over the text, and counts the number of instances where a unit (in our case, a token) at one edge of the window was predicted to be in the same segment (according to the reference) as a unit at the other edge, but in fact is not; or was predicted not to be in the same segment, but in fact is. This count is normalized by the total number of tests to get a score between 0 and 1, with 0 being a perfect score (the lower, the better). Pevzner and Hearst (2002) criticize this metric because it penalizes false posi"
W12-2504,C10-2011,1,0.790001,"on for the SO-CAL sentiment analysis system which has shown superior performance in lexicon-based sentiment analysis (Taboada et al., 2011), and SentiWordNet (SWN), a high-coverage automatic lexicon built from WordNet (Baccianella et al., 2010). The polarity of each word over the span is averaged. Sentiment extremity Both lexicons provide a measure of the degree to which a word is positive or negative. Instead of summing the sentiment scores, we sum their absolute values, to get a measure of how extreme (subjective) the span is. Formality Average formality score, using a lexicon of formality (Brooke et al., 2010) built using latent semantic analysis (LSA) (Landauer and Dumais, 1997). Dynamic General Inquirer The General Inquirer dictionary (Stone et al., 1966), which was used for stylistic inconsistency detection by Guthrie (2008), includes 182 content analysis tags, many of which are relevant to style; we remove the two polarity tags already part of the SO-CAL dictionary, and select others dynamically using our tf-cl metric. LSA vector features Brooke et al. (2010) have posited that, in highly diverse register/genre corpora, the lowest dimensions of word vectors derived using LSA (or other dimensiona"
W12-2504,D08-1035,0,0.223905,"proposed. One popular unsupervised approach is to identify the points in the text where a metric of lexical coherence is at a (local) minimum (Hearst, 1994; Galley et al., 2003). Malioutov and Barzilay (2006) also used a lexical coherence metric, but applied a graphical model where segmentations are graph cuts chosen to maximize coherence of sentences within a segment, and minimize coherence among sentences in different segments. Another class of approaches is based on a generative model of text, for instance HMMs (Blei and Moreno, 2001) and Bayesian topic modeling (Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008); in such approaches, the goal is to choose segment breaks that maximize the probability of generating the text, under the assumption that each segment has a different language model. 3 Stylistic change curves Many popular text segmentation methods depend crucially on a reliable textual unit (often a sentence) which can be reliably classified or compared to others. But, for our purposes here, a sentence is both too small a unit — our stylistic metrics will be more accurate over larger spans — and not small enough 28 — we do not want to limit our breaks to sentence boundaries. Generative models"
W12-2504,P03-1071,0,0.0322292,"indow that compares, using a special distance function, a character trigram feature vector at various steps throughout the text, creating a style change function whose maxima indicate points of interest (potential plagarism). Topic segmentation is a similar problem that has been quite well-explored. A common thread in this work is the importance of lexical cohesion, though a large number of competing models based on this concept have been proposed. One popular unsupervised approach is to identify the points in the text where a metric of lexical coherence is at a (local) minimum (Hearst, 1994; Galley et al., 2003). Malioutov and Barzilay (2006) also used a lexical coherence metric, but applied a graphical model where segmentations are graph cuts chosen to maximize coherence of sentences within a segment, and minimize coherence among sentences in different segments. Another class of approaches is based on a generative model of text, for instance HMMs (Blei and Moreno, 2001) and Bayesian topic modeling (Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008); in such approaches, the goal is to choose segment breaks that maximize the probability of generating the text, under the assumption that each seg"
W12-2504,P94-1002,0,0.86002,"in the poem, such as the formal and traditionally poetic voice of a narrator that recurs many times in the poem: Above the antique mantel was displayed As though a window gave upon the sylvan scene The change of Philomel [97–99] While the stylistic contrasts between these and other voices are apparent to many readers, Eliot does not explicitly mark the transitions between them. The goal of the present work is to investigate whether computational stylistic analysis can identify the transition between one voice and the next. Our unsupervised approach, informed by research in topic segmentation (Hearst, 1994) and intrinsic plagiarism detection (Stamatatos, 2009), is based on deriving a curve representing stylistic change, where the local maxima represent likely transition points. Notably, our curve represents an amalgamation of different stylistic metrics, including those that incorporate external (extrinsic) knowledge, e.g. vector representations based on larger corpus cooccurrence, which we show to be extremely useful. For development and initial testing we follow other work on stylistic inconsistency by using artificial (mixed) poems, but the our main evaluation is on The Waste Land itself. We"
W12-2504,P06-1004,0,0.0753393,"sing a special distance function, a character trigram feature vector at various steps throughout the text, creating a style change function whose maxima indicate points of interest (potential plagarism). Topic segmentation is a similar problem that has been quite well-explored. A common thread in this work is the importance of lexical cohesion, though a large number of competing models based on this concept have been proposed. One popular unsupervised approach is to identify the points in the text where a metric of lexical coherence is at a (local) minimum (Hearst, 1994; Galley et al., 2003). Malioutov and Barzilay (2006) also used a lexical coherence metric, but applied a graphical model where segmentations are graph cuts chosen to maximize coherence of sentences within a segment, and minimize coherence among sentences in different segments. Another class of approaches is based on a generative model of text, for instance HMMs (Blei and Moreno, 2001) and Bayesian topic modeling (Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008); in such approaches, the goal is to choose segment breaks that maximize the probability of generating the text, under the assumption that each segment has a different language m"
W12-2504,J02-1002,0,0.400735,"a gold standard reference. The measure Pk , proposed by Beeferman et al. (1997), uses a probe window equal to half the average length of a segment; the window slides over the text, and counts the number of instances where a unit (in our case, a token) at one edge of the window was predicted to be in the same segment (according to the reference) as a unit at the other edge, but in fact is not; or was predicted not to be in the same segment, but in fact is. This count is normalized by the total number of tests to get a score between 0 and 1, with 0 being a perfect score (the lower, the better). Pevzner and Hearst (2002) criticize this metric because it penalizes false positives and false negatives differently and sometimes fails to penalize false positives altogether; their metric, WindowDiff (WD), solves these problems by counting an error whenever there is a difference between the number of segments in the prediction as compared to the reference. Recent work in topic segmentation (Eisenstein and Barzilay, 2008) continues to use both metrics, so we also present both here. During initial testing, we noted a fairly serious shortcoming with both these metrics: all else being equal, they will usually prefer a s"
W12-2504,J11-2001,1,0.17629,"hall Word List A list of 3000 basic words that is used in the Dale-Chall Readability metric (Dale and Chall, 1995). Average unigram count in 1T Corpus Another metric of whether a word is commonly used. We use the unigram counts in the 1T 5-gram Corpus (Brants and Franz, 2006). Here and below, if a word is not included it is given a zero. Sentiment polarity The positive or negative stance of a span could be viewed as a stylistic variable. We test two lexicons, a hand-built lexicon for the SO-CAL sentiment analysis system which has shown superior performance in lexicon-based sentiment analysis (Taboada et al., 2011), and SentiWordNet (SWN), a high-coverage automatic lexicon built from WordNet (Baccianella et al., 2010). The polarity of each word over the span is averaged. Sentiment extremity Both lexicons provide a measure of the degree to which a word is positive or negative. Instead of summing the sentiment scores, we sum their absolute values, to get a measure of how extreme (subjective) the span is. Formality Average formality score, using a lexicon of formality (Brooke et al., 2010) built using latent semantic analysis (LSA) (Landauer and Dumais, 1997). Dynamic General Inquirer The General Inquirer"
W12-2504,P01-1064,0,0.0551555,"on this concept have been proposed. One popular unsupervised approach is to identify the points in the text where a metric of lexical coherence is at a (local) minimum (Hearst, 1994; Galley et al., 2003). Malioutov and Barzilay (2006) also used a lexical coherence metric, but applied a graphical model where segmentations are graph cuts chosen to maximize coherence of sentences within a segment, and minimize coherence among sentences in different segments. Another class of approaches is based on a generative model of text, for instance HMMs (Blei and Moreno, 2001) and Bayesian topic modeling (Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008); in such approaches, the goal is to choose segment breaks that maximize the probability of generating the text, under the assumption that each segment has a different language model. 3 Stylistic change curves Many popular text segmentation methods depend crucially on a reliable textual unit (often a sentence) which can be reliably classified or compared to others. But, for our purposes here, a sentence is both too small a unit — our stylistic metrics will be more accurate over larger spans — and not small enough 28 — we do not want to limit our breaks to senten"
W12-2504,P11-1136,0,\N,Missing
W13-1008,J09-1005,1,0.826814,"o identify many fixed and semifixed clich´es. Nevertheless, an appropriate clich´e lexicon would be required for this approach. Moreover, because of the relationship between clich´es and culture, to be applicable to historical texts, such as for the literary analysis of interest to us, a lexicon for the appropriate time period would be required. Techniques for MWE extraction could potentially be used to (semi-) automatically build a clich´e lexicon. Much work in this area has again focused on specific types of MWEs — e.g., verb–particle constructions (Baldwin, 2005) or verb–noun combinations (Fazly et al., 2009) — but once more the heterogeneity of clich´es limits the applicability of such approaches for extracting them. Methods based on strength of association — applied to n-grams or words co-occurring through some other relation such as syntactic dependency (see Evert, 2008, for an overview) — could be applied to extract a wider range of MWEs, although here most research has focused on two-word co-occurrences, with considerably less attention paid to longer MWEs. Even if general-purpose MWE extraction were a solved problem, methods would still be required to distinguish between MWEs that are and ar"
W13-1008,U12-1012,0,0.0400271,"be strongly context dependent. This paper identifies clich´es as an under-studied problem closely related to many issues of interest to the MWE community. We propose a preliminary method for assessing the degree to which a text is clich´ed, and then show how such a method can contribute to literary analysis. Specifically, we apply this approach to James Joyce’s novel Ulysses to offer insight into the ongoing literary debate about the use of clich´es in this work. 2 Related work Little research in computational linguistics has specifically addressed clich´es. The most relevant work is that of Smith et al. (2012) who propose a method for identifying clich´es in song lyrics, and determining the extent to which a song is clich´ed. Their method combines information about rhymes and the df-idf of trigrams (tf-idf, but using document frequency instead of term frequency) in song 53 lyrics. However, this method isn’t applicable for our goal of determining how clich´ed an arbitrary text is with a focus on literary analysis, because in this case rhyming is not a typical feature of the texts. Moreover, repetition in song lyrics motivated their df-idf score, but this is not a salient feature of the texts we cons"
W13-1401,P11-2019,0,0.0170385,"uistics, by contrast, ambiguity is almost uniformly treated as a problem to be solved; the focus is on disambiguation, with the assumption that one true, correct interpretation exists. In the sphere of annotation, for instance, there is an expectation that agreement between annotators, as measured by statistics such as kappa (Di Eugenio and Glass, 2004), reach levels (generally 0.67 or higher) where disagreements can be reasonably dismissed as noise; the implicit assumption here is that subjectivity is something to be minimized. The challenge of dealing with subjectivity in CL has been noted (Alm, 2011), and indeed there are rare examples in the field where multiple interpretations have been considered during evaluations—for instance, work in lexical cohesion (Morris and Hirst, 2005) and in using annotator disagreements as an indicator that two words are of similar orientation (Taboada et al., 2011)—but they are the exception. Work in CL focused on literary texts tends towards aspects of the texts which readers would not find particularly ambiguous, for example identifying major narrative threads (Wallace, 2012) or distinguishing author gender (Luyckx et al., 2006). 3 A Collaborative Researc"
W13-1401,C10-2011,1,0.860383,"ectivity in literature— FID is included in the narrative annotation schema of Mani (2013), but it is not given any particular attention within that framework—there are obvious connections with sentence-level subjectivity analysis (Wilson et al., 2005) and various other stylistic tasks, including authorship profiling (Argamon et al., 2007). Since the subjective nature of these passages is often expressed through specific lexical choice, it would be interesting to see if sentiment dictionaries (Taboada et al., 2011) or other stylistic lexical resources such as dictionaries of lexical formality (Brooke et al., 2010) could be useful. 4.2 Our Approach Our project is proceeding in four stages: an initial round of student annotation, a second round of student annotation, computational analysis of these annotations, and the development of a project website. In the first stage, we had 160 students mark up a passage of between 100–150 words in accordance with TEI guidelines. Students were instructed to use the TEI said element to enclose any instance of character speech, to identify the character whose speech is being introduced, and to classify each of these instances as either direct, indirect, or free indire"
W13-1401,W12-2504,1,0.819,"to in the title of this paper. 3 promised to bring added rigor as well as a degree of objectivity to this question, which humanities methods had proven unable to resolve in almost a century of debate. Both because poetry is dense in signification, and because the multiple voices in The Waste Land are a deliberate effect achieved by a single author rather than a disguised piecing together of the works of multiple authors, the question provided a meaningful challenge to the computational approach, an unsupervised vector-space model which first segments by identifying points of stylistic change (Brooke et al., 2012) and then clusters the resulting segments together into voices (Brooke et al., 2013). This research project was tightly integrated into the curriculum of “The Digital Text”. Students were instructed in the use of the Text Encoding Initiative (TEI) XML guidelines,3 and each of the students provided one annotation related to voice as part of a marked assignment. Students also participated in an online poll in which they indicated every instance in which they perceived a vocal switch in the poem, and their responses were used in the construction of a gold standard for the evaluation of our comput"
W13-1401,W13-1406,1,0.816535,"f objectivity to this question, which humanities methods had proven unable to resolve in almost a century of debate. Both because poetry is dense in signification, and because the multiple voices in The Waste Land are a deliberate effect achieved by a single author rather than a disguised piecing together of the works of multiple authors, the question provided a meaningful challenge to the computational approach, an unsupervised vector-space model which first segments by identifying points of stylistic change (Brooke et al., 2012) and then clusters the resulting segments together into voices (Brooke et al., 2013). This research project was tightly integrated into the curriculum of “The Digital Text”. Students were instructed in the use of the Text Encoding Initiative (TEI) XML guidelines,3 and each of the students provided one annotation related to voice as part of a marked assignment. Students also participated in an online poll in which they indicated every instance in which they perceived a vocal switch in the poem, and their responses were used in the construction of a gold standard for the evaluation of our computational approach. Once they were complete, we developed our results into a publicly"
W13-1401,J11-2001,1,0.377483,"easured by statistics such as kappa (Di Eugenio and Glass, 2004), reach levels (generally 0.67 or higher) where disagreements can be reasonably dismissed as noise; the implicit assumption here is that subjectivity is something to be minimized. The challenge of dealing with subjectivity in CL has been noted (Alm, 2011), and indeed there are rare examples in the field where multiple interpretations have been considered during evaluations—for instance, work in lexical cohesion (Morris and Hirst, 2005) and in using annotator disagreements as an indicator that two words are of similar orientation (Taboada et al., 2011)—but they are the exception. Work in CL focused on literary texts tends towards aspects of the texts which readers would not find particularly ambiguous, for example identifying major narrative threads (Wallace, 2012) or distinguishing author gender (Luyckx et al., 2006). 3 A Collaborative Research Agenda The obvious solution to the problem of the “two cultures”—and one that has often been proposed (Friedlander, 2009)—is interdisciplinary collaboration. But while there are many computational linguists working in literary topics such as genre, and many literary scholars performing computational"
W13-1401,N12-1001,0,0.0721752,"ng to be minimized. The challenge of dealing with subjectivity in CL has been noted (Alm, 2011), and indeed there are rare examples in the field where multiple interpretations have been considered during evaluations—for instance, work in lexical cohesion (Morris and Hirst, 2005) and in using annotator disagreements as an indicator that two words are of similar orientation (Taboada et al., 2011)—but they are the exception. Work in CL focused on literary texts tends towards aspects of the texts which readers would not find particularly ambiguous, for example identifying major narrative threads (Wallace, 2012) or distinguishing author gender (Luyckx et al., 2006). 3 A Collaborative Research Agenda The obvious solution to the problem of the “two cultures”—and one that has often been proposed (Friedlander, 2009)—is interdisciplinary collaboration. But while there are many computational linguists working in literary topics such as genre, and many literary scholars performing computational analysis of literature, genuine collaboration between the disciplines remains quite rare. Over the past two years, we have undertaken two collaborative projects—one mostly complete, one ongoing— which aim at such gen"
W13-1401,H05-1044,0,0.00553481,"iterary Studies, Toolan (2008) was perhaps the first to discuss the possibility of automatic recognition of FID, but his work was limited to a very small, very informal experiment using a few a priori features, with no implementation or quantitative analysis of the results. Though we are not aware of work in Computational Linguistics that deals with this kind of subjectivity in literature— FID is included in the narrative annotation schema of Mani (2013), but it is not given any particular attention within that framework—there are obvious connections with sentence-level subjectivity analysis (Wilson et al., 2005) and various other stylistic tasks, including authorship profiling (Argamon et al., 2007). Since the subjective nature of these passages is often expressed through specific lexical choice, it would be interesting to see if sentiment dictionaries (Taboada et al., 2011) or other stylistic lexical resources such as dictionaries of lexical formality (Brooke et al., 2010) could be useful. 4.2 Our Approach Our project is proceeding in four stages: an initial round of student annotation, a second round of student annotation, computational analysis of these annotations, and the development of a projec"
W13-1406,baccianella-etal-2010-sentiwordnet,0,0.00613171,"we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large 42 external corpus (Brants and Franz, 2006), lexiconbased sentiment metrics using SentiWordNet (Baccianella et al., 2010), formality score (Brooke et al., 2010), and, perhaps most notably, the centroid of 20dimensional distributional vectors built using latent semantic analysis (Landauer and Dumais, 1997), reflecting the use of words in a large web corpus (Burton et al., 2009); in previous work (Brooke et al., 2010), we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature se"
W13-1406,P98-1012,0,0.0303295,"e with other work in stylistic inconsistency detection (Guthrie, 2008). Though it would be interesting to see if a good k could be estimated independently, for our purposes here we set k to be the known number of speakers in our gold standard. 4 Evaluation We evaluate our clusters by comparing them to a gold standard annotation. There are various metrics for extrinsic cluster evaluation; Amig´o et al. 1 Occasionally, there was no convergence, at which point we halted the process arbitrarily after 100 iterations. (2009) review various options and select the BCubed precision and recall metrics (Bagga and Baldwin, 1998) as having all of a set of key desirable properties. BCubed precision is a calculation of the fraction of item pairs in the same cluster which are also in the same category, whereas BCubed recall is the fraction of item pairs in the same category which are also in the same cluster. The harmonic mean of these two metrics is BCubed F-score. Typically, the ‘items’ are exactly what has been clustered, but this is problematic in our case, because we wish to compare methods which have different segmentations and thus the vectors that are being clustered are not directly comparable. Instead, we calcu"
W13-1406,C10-2011,1,0.807278,"clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large 42 external corpus (Brants and Franz, 2006), lexiconbased sentiment metrics using SentiWordNet (Baccianella et al., 2010), formality score (Brooke et al., 2010), and, perhaps most notably, the centroid of 20dimensional distributional vectors built using latent semantic analysis (Landauer and Dumais, 1997), reflecting the use of words in a large web corpus (Burton et al., 2009); in previous work (Brooke et al., 2010), we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature set, see Brooke et al. (2012). All the fe"
W13-1406,W12-2504,1,0.676833,"each appear multiple times throughout the text. Our interest is distinguishing these voices automatically. One of the poem’s most distinctive voices is that of the woman who speaks at the end of its second section: I can’t help it, she said, pulling a long face, It’s them pills I took, to bring it off, she said [158–159] Although the stylistic contrasts between these and other voices are clear to many readers, Eliot does not explicitly mark the transitions, nor is it obvious when a voice has reappeared. Our previous work focused on only the segmentation part of the voice identification task (Brooke et al., 2012). Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice. Of particular interest is the influence of the initial segmentation on the success of this downstream task. 2 Related Work There is a small body of work applying quantitative methods to poetry: Simonton (1990) looked at lexical and semantic diversity in Shakespearean sonnets and correlated this with aesthetic success, whereas Dugan (1973) developed statistics of formulaic style and applied them to the Chanson de Roland to determin"
W13-1406,W13-1401,1,0.818929,"sults suggest that we are a long way from something that would be a considered a possible human interpretation. Nevertheless, applying quantitative methods to these kinds of texts can, for literary scholars, bridge the gab between abstract interpretations and the details of form and function (McKenna and Antonia, 2001). In our own case, this computational work is just one aspect of a larger project in literary analysis where the ultimate goal is not to mimic human behavior per se, but rather to better understand literary phenomena by annotation and modelling of these phenomena (Hammond, 2013; Hammond et al., 2013). With respect to future enhancements, improving segmentation is obviously important; the best automated efforts so far provide only a small boost over a baseline approach to segmentation. However, independently of this, our experiments with goldstandard seeding suggest that refining our approach to clustering, e.g. a method that identifies good initial points for our centroids, may also pay dividends in the long run. A more radical idea for future work would be to remove the somewhat artificial delim2 These passages are the original examples from our earlier work (Brooke et al., 2012), select"
W13-1406,W12-2502,0,0.0270558,"and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice. Of particular interest is the influence of the initial segmentation on the success of this downstream task. 2 Related Work There is a small body of work applying quantitative methods to poetry: Simonton (1990) looked at lexical and semantic diversity in Shakespearean sonnets and correlated this with aesthetic success, whereas Dugan (1973) developed statistics of formulaic style and applied them to the Chanson de Roland to determine whether it represents an oral or written style. Kao and Jurafsky (2012) quantify various aspects of poety, including style and sentiment, and use these features to distinguish professional and amateur writers of contemporary poetry. 41 Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 41–46, c Atlanta, Georgia, June 14, 2013. 2013 Association for Computational Linguistics With respect to novels, the work of McKenna and Antonia (2001) is very relevant; they used principal components analysis of lexical frequency to discriminate different voices and narrative styles in sections of Ulysses by James Joyce. Clustering techniques hav"
W13-1406,J02-1002,0,0.143644,"(Stamatatos, 2009) may include many more authors, though some form of supervision (i.e. training data) is usually assumed. Our work here is built on our earlier work (Brooke et al., 2012). Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches. Performance on The Waste Land was far from perfect, but evaluation using standard text segmentation metrics (Pevzner and Hearst, 2002) indicated that it was well above various baselines. 3 Method Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans (Brooke et al., 2012). Given a segmentation of the text, we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics ("
W13-1406,C98-1012,0,\N,Missing
W13-1406,P11-1136,0,\N,Missing
W13-1725,brooke-hirst-2012-measuring,1,0.817744,"training task 1), particularly when some form of domain adaptation is also applied. This method can also be used to boost performance even when training data from the same corpus is available (i.e. open-training task 2). However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work. 1 Introduction Our participation in the 2013 NLI shared task (Tetreault et al., 2013) follows on our recent work exploring cross-corpus evaluation, i.e. using distinct corpora for training and testing (Brooke and Hirst, 2011; Brooke and Hirst, 2012a; Brooke and Hirst, 2012b), an approach that is now becoming fairly standard alternative in relevant work (Bykh and Meurers, 2012; Tetreault et al., 2012; Swanson and Charniak, 2013). Our promotion of crosscorpus evaluation in NLI was partially motivated by serious issues with the most popular corpus for native language identification work up to now, the International Corpus of Learner English (Granger et al., 2009). The new TOEFL-11 (Blanchard et al., 2013) used for this NLI shared task addresses some of the problems with the ICLE (most glaringly, the fact that some topics in the ICLE appear"
W13-1725,C12-1025,1,0.881759,"training task 1), particularly when some form of domain adaptation is also applied. This method can also be used to boost performance even when training data from the same corpus is available (i.e. open-training task 2). However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work. 1 Introduction Our participation in the 2013 NLI shared task (Tetreault et al., 2013) follows on our recent work exploring cross-corpus evaluation, i.e. using distinct corpora for training and testing (Brooke and Hirst, 2011; Brooke and Hirst, 2012a; Brooke and Hirst, 2012b), an approach that is now becoming fairly standard alternative in relevant work (Bykh and Meurers, 2012; Tetreault et al., 2012; Swanson and Charniak, 2013). Our promotion of crosscorpus evaluation in NLI was partially motivated by serious issues with the most popular corpus for native language identification work up to now, the International Corpus of Learner English (Granger et al., 2009). The new TOEFL-11 (Blanchard et al., 2013) used for this NLI shared task addresses some of the problems with the ICLE (most glaringly, the fact that some topics in the ICLE appear"
W13-1725,C12-1027,0,0.223938,"nce even when training data from the same corpus is available (i.e. open-training task 2). However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work. 1 Introduction Our participation in the 2013 NLI shared task (Tetreault et al., 2013) follows on our recent work exploring cross-corpus evaluation, i.e. using distinct corpora for training and testing (Brooke and Hirst, 2011; Brooke and Hirst, 2012a; Brooke and Hirst, 2012b), an approach that is now becoming fairly standard alternative in relevant work (Bykh and Meurers, 2012; Tetreault et al., 2012; Swanson and Charniak, 2013). Our promotion of crosscorpus evaluation in NLI was partially motivated by serious issues with the most popular corpus for native language identification work up to now, the International Corpus of Learner English (Granger et al., 2009). The new TOEFL-11 (Blanchard et al., 2013) used for this NLI shared task addresses some of the problems with the ICLE (most glaringly, the fact that some topics in the ICLE appeared only in some L1 backgrounds), but, from the perspective of The models built for all of three of the tasks are extensions of the"
W13-1725,C12-1064,0,0.0164073,"good substitute for Hindi; we overlooked this possibility during our preparation for the task, unfortunately. Table 5: Number of tokens (in thousands) in Indian corpora, by expected L1. L1 Hindi Telugu Indian Corpus News Twitter Blog 996k 146k 2089k 998k 133k 76k training task. We explored a few methods to get data to fill this gap. First, we downloaded two collections of English language Indian news articles, one from a Hindi newspaper, the Hindustan Times, and one from a Telugu newspaper, the Andhra Jyothy.4 Second, we extracted a collection of English tweets from the WORLD twitter corpus (Han et al., 2012) that were geolocated in the Hindi and Telugu speaking areas; as with the Lang-8, these were combined to create texts of at least 250 tokens.5 Our third Indian corpus consists of translations (by Google Translate) of Hindi and Telugu blogs from the ICWSM 2009 Spinn3r Dataset (Burton et al., 2009), which we used in other work on using L1 text for NLI (Brooke and Hirst, 2012a). The number of tokens in each of these corpora are given in Table 5. 5 Open-training Task 2 Our approach to open-training task 2 is based on the assumption that in many ways it is a direct extension of the closed-training"
W13-1725,P03-1054,0,0.0327837,"Missing"
W13-1725,P12-2038,0,0.0402666,"to our system for this task. Inspired by Bykh and Meurers (2012), we first considered n-grams (up to trigrams) where at least one lexical word is abstracted to its POS, and at least one isn’t (partial abstraction). Since dependencies were found to be a positive feature, we tried adding dependency chains, which combine two dependencies, i.e. three lexical words linked by two grammatical relations. We tested productions with wild cards, e.g. S → NP VP * matches any sentence production which starts with NP VP. Tree Substitution grammar fragments have been shown to be superior to CFG productions (Swanson and Charniak, 2012); we used raw Tree Substitution Grammar (TSG) fragments for the TOEFL-111 and tested a subset of those fragments which involved at least two levels of the grammar (i.e. those not already covered by n-grams or CFG productions). Our final feature option requires slightly more explanation. Crossley and McNamara (2012) report that metrics associated with word concreteness, imagability, meaningfulness, and familiarity are useful for NLI; the metrics they use are derived from the MRC Psycholinguistic database (Coltheart, 1980), 1 We Table 3: Feature testing for closed-training task, new features; be"
W13-1725,N13-1009,0,0.125753,"is available (i.e. open-training task 2). However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work. 1 Introduction Our participation in the 2013 NLI shared task (Tetreault et al., 2013) follows on our recent work exploring cross-corpus evaluation, i.e. using distinct corpora for training and testing (Brooke and Hirst, 2011; Brooke and Hirst, 2012a; Brooke and Hirst, 2012b), an approach that is now becoming fairly standard alternative in relevant work (Bykh and Meurers, 2012; Tetreault et al., 2012; Swanson and Charniak, 2013). Our promotion of crosscorpus evaluation in NLI was partially motivated by serious issues with the most popular corpus for native language identification work up to now, the International Corpus of Learner English (Granger et al., 2009). The new TOEFL-11 (Blanchard et al., 2013) used for this NLI shared task addresses some of the problems with the ICLE (most glaringly, the fact that some topics in the ICLE appeared only in some L1 backgrounds), but, from the perspective of The models built for all of three of the tasks are extensions of the model used in our recent work (Brooke and Hirst, 201"
W13-1725,C12-1158,0,0.309649,"ata from the same corpus is available (i.e. open-training task 2). However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work. 1 Introduction Our participation in the 2013 NLI shared task (Tetreault et al., 2013) follows on our recent work exploring cross-corpus evaluation, i.e. using distinct corpora for training and testing (Brooke and Hirst, 2011; Brooke and Hirst, 2012a; Brooke and Hirst, 2012b), an approach that is now becoming fairly standard alternative in relevant work (Bykh and Meurers, 2012; Tetreault et al., 2012; Swanson and Charniak, 2013). Our promotion of crosscorpus evaluation in NLI was partially motivated by serious issues with the most popular corpus for native language identification work up to now, the International Corpus of Learner English (Granger et al., 2009). The new TOEFL-11 (Blanchard et al., 2013) used for this NLI shared task addresses some of the problems with the ICLE (most glaringly, the fact that some topics in the ICLE appeared only in some L1 backgrounds), but, from the perspective of The models built for all of three of the tasks are extensions of the model used in our recen"
W13-1725,W13-1706,0,0.290362,"n the potential benefits of external corpora. We show that including training data from multiple corpora is highly effective at robust, cross-corpus NLI (i.e. open-training task 1), particularly when some form of domain adaptation is also applied. This method can also be used to boost performance even when training data from the same corpus is available (i.e. open-training task 2). However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work. 1 Introduction Our participation in the 2013 NLI shared task (Tetreault et al., 2013) follows on our recent work exploring cross-corpus evaluation, i.e. using distinct corpora for training and testing (Brooke and Hirst, 2011; Brooke and Hirst, 2012a; Brooke and Hirst, 2012b), an approach that is now becoming fairly standard alternative in relevant work (Bykh and Meurers, 2012; Tetreault et al., 2012; Swanson and Charniak, 2013). Our promotion of crosscorpus evaluation in NLI was partially motivated by serious issues with the most popular corpus for native language identification work up to now, the International Corpus of Learner English (Granger et al., 2009). The new TOEFL-1"
W13-1725,D11-1148,0,0.0446693,"re selection methods (e.g. use of information gain) were ineffective in our previous work, so we did not explore them in detail here. With regards to the features themselves, our earlier work tested a fairly standard collection of distributional features, including function words, word ngrams (up to bigram), POS n-grams (up to trigram), character n-grams (up to trigram), dependencies, context-free productions, and ‘mixed’ POS/function n-grams (up to trigram), i.e. n-grams with all lexical words replaced with part of speech. Most of these had appeared in previous NLI work (Koppel et al., 2005; Wong and Dras, 2011; Wong et al., 2012), though until recently word n-grams had been avoided because of ICLE topic bias. Our best model used only two of these features, word n-grams and the mixed POS/function n-grams. This was our starting point for the present work. The Stanford parser (Klein and Manning, 2003) was used for POS tagging and parsing. Obviously, the training set used varies throughout the paper, and other differences in specific models built for each task will be mentioned as they become relevant. For evaluation here, we primarily use the test set for NLI shared task, though we 189 Table 1: Featur"
W13-1725,D12-1064,0,0.0233866,"(e.g. use of information gain) were ineffective in our previous work, so we did not explore them in detail here. With regards to the features themselves, our earlier work tested a fairly standard collection of distributional features, including function words, word ngrams (up to bigram), POS n-grams (up to trigram), character n-grams (up to trigram), dependencies, context-free productions, and ‘mixed’ POS/function n-grams (up to trigram), i.e. n-grams with all lexical words replaced with part of speech. Most of these had appeared in previous NLI work (Koppel et al., 2005; Wong and Dras, 2011; Wong et al., 2012), though until recently word n-grams had been avoided because of ICLE topic bias. Our best model used only two of these features, word n-grams and the mixed POS/function n-grams. This was our starting point for the present work. The Stanford parser (Klein and Manning, 2003) was used for POS tagging and parsing. Obviously, the training set used varies throughout the paper, and other differences in specific models built for each task will be mentioned as they become relevant. For evaluation here, we primarily use the test set for NLI shared task, though we 189 Table 1: Feature testing for closed"
W13-1725,P11-1019,0,0.0267268,"ssues that might cause problems (Brooke and Hirst, 2011), it is probably the closest match in terms of genre and writer proficiency to the TOEFL-11. FCE What we call the FCE corpus is a small sample of the First Certificate in English portion of the Cambridge Learner Corpus, which was re2 We do not have permission to distribute the corpus directly; however, we can offer a list of URLs together with software which can be used to recreate the corpus. 191 ICCI 232k 243k 0k 0k 49k 0k 91k 0k 0k 0k 0k ICNALE 199k 366k 151k 0k 0k 0k 0k 0k 0k 0k 0k leased for the purposes of essay scoring evaluation (Yannakoudakis et al., 2011); 16 different L1 backgrounds are represented, 9 of which overlap with the TOEFL-11. Each of the texts consists of two short answers in the form of a letter, a report, an article, or a short story. Relative to the other corpora, the actual amount of text in the FCE is small. ICCI Like the ICLE and TOEFL-11, the International Corpus of Crosslinguistic Interlanguage (Tono et al., 2012) is also an essay corpus, though in contrast with other corpora it is focused on young learners, i.e. those in grade school. It includes both descriptive and argumentative essays on a number of topics. Only 4 of it"
W13-2314,D12-1115,1,0.780092,"eceding paragraph. ASNs play an important role in organizing a discourse. First, they are used metadiscursively to 112 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 112–121, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics 2 Related work There exist only few annotated corpora of anaphora with non-nominal antecedents (Dipper and Zinsmeister, 2011). The largest one of these, the ARRAU corpus (Poesio and Artstein, 2008), contains 455 anaphors pointing to non-nominal antecedents, but only a few instances are ASNs. Kolhatkar and Hirst (2012) annotated antecedents of the same type as we do, but restricted their efforts to the ASN this issue.1 In addition, there are corpora annotated with event anaphora in which verbal instances are identified as proxies for nonnominal antecedents (Pradhan et al., 2007; Chen et al., 2011; Lee et al., 2012). For the task of identifying non-nominal antecedents as free spans of text, there is no standard way of reporting inter-annotator agreement. Some studies report only observed percentage agreement with results in the range of about 0.40– 0.55 (Vieira et al., 2002; Dipper and Zinsmeister, 2011). Th"
W13-2314,J08-4004,0,0.133677,"Missing"
W13-2314,D12-1045,0,0.0427458,"lated work There exist only few annotated corpora of anaphora with non-nominal antecedents (Dipper and Zinsmeister, 2011). The largest one of these, the ARRAU corpus (Poesio and Artstein, 2008), contains 455 anaphors pointing to non-nominal antecedents, but only a few instances are ASNs. Kolhatkar and Hirst (2012) annotated antecedents of the same type as we do, but restricted their efforts to the ASN this issue.1 In addition, there are corpora annotated with event anaphora in which verbal instances are identified as proxies for nonnominal antecedents (Pradhan et al., 2007; Chen et al., 2011; Lee et al., 2012). For the task of identifying non-nominal antecedents as free spans of text, there is no standard way of reporting inter-annotator agreement. Some studies report only observed percentage agreement with results in the range of about 0.40– 0.55 (Vieira et al., 2002; Dipper and Zinsmeister, 2011). The studies differed with respect to number of annotators, types of anaphors, and language of the corpora. Artstein and Poesio (2006) discuss Krippendorff’s alpha for chance-corrected agreement. They considered antecedent strings as bags of words and computed the degree of difference between them by dif"
W13-2314,W09-3309,0,0.0858361,"res (e.g. Jaccard, Dice). The bag-of-words approach is rather optimistic in the sense that even two nonoverlapping strings are very likely to share at least a few words. Kolhatkar and Hirst (2012) followed a different approach by using Krippendorff’s unitizing alpha (u α) which considers the longest common subsequence of different antecedent options (Krippendorff, 2013). They reported high chancecorrected u α of 0.86 for two annotators but in a very restricted domain. There has been some prior effort to annotate anaphora and coreference using Games with a Purpose as a method of crowdsourcing (Chamberlain et al., 2009; Hladk´a et al., 2009). Another, less time-consuming approach of crowdsourcing is using platforms such as Amazon Mechanical Turk2 . It has been shown that crowdsourced data can successfully be used as training data for NLP tasks (Hsueh et al., 2009). Class Description Examples factual linguistic mental modal eventive circumstantial states of affairs linguistic acts thoughts and ideas subjective judgements events situations fact, reason question, report issue, decision possibility, truth act, reaction situation, way Table 1: Schmids categorization of shell nouns. The nouns in boldface are used"
W13-2314,W10-0730,0,0.0298789,"s of English, we limited the allowed demographic region to English-speaking countries. Finally, CrowdFlower also provides detailed annotation results including demographic information and trustworthiness of each annotator. Design of the annotation tasks With the help of well-designed gold examples, CrowdFlower can get rid of spammers and ensures that only reliable annotators perform the annotation task. But the annotation task must be well-designed in the first place to get a good quality annotation. Following the claim in the literature that with crowdsourcing platforms simple tasks do best (Madnani et al., 2010; Wang et al., 2012), we split our annotation task into two relatively simple sequential annotation tasks. First, identifying the broad region of the antecedent, i.e., not the precise antecedent but the region where the antecedent lies, and second, identifying the precise antecedent, given the broad region of the antecedent. Now we will discuss each of our annotation tasks in detail. 5.1 5.2 CrowdFlower experiment 2 This annotation task was about pinpointing the exact antecedent text of the ASN instances. We designed a CrowdFlower experiment, where we presented to the annotators ASN instances"
W13-2314,P89-1007,0,0.681846,"Missing"
W13-2314,I11-1012,0,0.0334064,"al Linguistics 2 Related work There exist only few annotated corpora of anaphora with non-nominal antecedents (Dipper and Zinsmeister, 2011). The largest one of these, the ARRAU corpus (Poesio and Artstein, 2008), contains 455 anaphors pointing to non-nominal antecedents, but only a few instances are ASNs. Kolhatkar and Hirst (2012) annotated antecedents of the same type as we do, but restricted their efforts to the ASN this issue.1 In addition, there are corpora annotated with event anaphora in which verbal instances are identified as proxies for nonnominal antecedents (Pradhan et al., 2007; Chen et al., 2011; Lee et al., 2012). For the task of identifying non-nominal antecedents as free spans of text, there is no standard way of reporting inter-annotator agreement. Some studies report only observed percentage agreement with results in the range of about 0.40– 0.55 (Vieira et al., 2002; Dipper and Zinsmeister, 2011). The studies differed with respect to number of annotators, types of anaphors, and language of the corpora. Artstein and Poesio (2006) discuss Krippendorff’s alpha for chance-corrected agreement. They considered antecedent strings as bags of words and computed the degree of difference"
W13-2314,poesio-artstein-2008-anaphoric,0,0.169878,"ot been formally trained to try it when needed. Here, the ASN this fact encapsulates the clause marked in bold from the preceding paragraph. ASNs play an important role in organizing a discourse. First, they are used metadiscursively to 112 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 112–121, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics 2 Related work There exist only few annotated corpora of anaphora with non-nominal antecedents (Dipper and Zinsmeister, 2011). The largest one of these, the ARRAU corpus (Poesio and Artstein, 2008), contains 455 anaphors pointing to non-nominal antecedents, but only a few instances are ASNs. Kolhatkar and Hirst (2012) annotated antecedents of the same type as we do, but restricted their efforts to the ASN this issue.1 In addition, there are corpora annotated with event anaphora in which verbal instances are identified as proxies for nonnominal antecedents (Pradhan et al., 2007; Chen et al., 2011; Lee et al., 2012). For the task of identifying non-nominal antecedents as free spans of text, there is no standard way of reporting inter-annotator agreement. Some studies report only observed"
W13-2314,C12-1055,0,0.0640774,"ASN annotation discussed above, there were two main challenges involved in the annotation process: first, to find annotators who can annotate data reliably with minimal guidelines, and second, to design simple annotation tasks that will elicit data useful for our purposes. Now we discuss how we dealt with these challenges. ASN Annotation Challenges ASN antecedent annotation is a complex task, as it involves deeply understanding the discourse and interpreting it. Here we point out two main challenges associated with the task. What to annotate? The question of ‘what to annotate’ as mentioned by Fort et al. (2012) is not straightforward for ASN antecedents, as the notion of markables is complex compared to ordinary nominal anaphora: the units on which the annotation work should focus are heterogeneous.5 Moreover, due to this heterogeneous nature of annotation units, there is a huge number of markables (e.g., all syntactic constituents given by a syntactic parse tree). So there are many options to choose from, while only a few units are actually to be annotated. Moreover, there is no oneto-one correspondence between the syntactic type of an antecedent and the semantic type of its referent (Webber, 1991)"
W13-2314,P09-2053,0,0.0413405,"Missing"
W13-2314,D08-1027,0,0.12234,"Missing"
W13-2314,W09-1904,0,0.0278596,"izing alpha (u α) which considers the longest common subsequence of different antecedent options (Krippendorff, 2013). They reported high chancecorrected u α of 0.86 for two annotators but in a very restricted domain. There has been some prior effort to annotate anaphora and coreference using Games with a Purpose as a method of crowdsourcing (Chamberlain et al., 2009; Hladk´a et al., 2009). Another, less time-consuming approach of crowdsourcing is using platforms such as Amazon Mechanical Turk2 . It has been shown that crowdsourced data can successfully be used as training data for NLP tasks (Hsueh et al., 2009). Class Description Examples factual linguistic mental modal eventive circumstantial states of affairs linguistic acts thoughts and ideas subjective judgements events situations fact, reason question, report issue, decision possibility, truth act, reaction situation, way Table 1: Schmids categorization of shell nouns. The nouns in boldface are used in this research. 3 The Anaphoric Shell Noun Corpus Our goal is to obtain annotated data for ASN antecedents that could be used to train a supervised machine learning system to resolve ASNs. For that, we created the Anaphoric Shell Noun (ASN) corpus"
W14-3203,P03-1054,0,0.0215749,"n result in the omission of grammatical morphemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the first time such features have been applied to speech from participants with dementia. In Tab"
W14-3203,E09-1017,0,0.0278164,".2 Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the first time such features have been applied to speech from participants with dementia. In Table 4 we list a few examples of our 134 CFG features, as well as the three phrase-level features (calculated for noun phrases, verb phrases, and prepositional phrases). Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can"
W14-3203,N03-1033,0,0.0192033,"circumlocutory phrases. In contrast, individuals with PNFA may have more difficulty with verbs and may even demonstrate agrammatism, which can result in the omission of grammatical morphemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translati"
W14-3203,U10-1011,0,0.0214454,"t, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the first time such features have been applied to speech from participants with dementia. In Table 4 we list a few examples of our 134 CFG features, as well as the three phrase-level features (calculated for noun phrases, verb phrases, and prepositional phrases). Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The"
W14-3420,W11-0610,0,0.0361093,"Missing"
W14-3420,C12-1025,1,0.855923,"ics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1 http://talkbank.org/AphasiaBank/ 135 Male/Female Age (years) Education (years) Agrammatic (N = 24) 15/9 58.1 (10.6) 16.3 (2.5) Control (N = 15) 8/7 63.3 (6.4) 16.4 (2.4) 4 4.1 (2009) calculated severa"
W14-3420,E09-1017,0,0.0273431,"the set of non-lexical productions. The total number of types of productions is large, many of them occurring very infrequently, so we compile a list of the 50 most frequently occurring productions in each of the two groups (agrammatic and controls) and use the combined set as the set of features. The feature values can be binary (does a particular production rule appear in the narrative or not?) or integer (how many times does a rule occur?). The CFG non-terminal symbols follow the Penn Treebank naming conventions. For our phrase-level statistics, we use a subset of the features described by Chae and Nenkova (2009), which are related to the incidence of different phrase types. We consider three different phrase types: noun phrases, verb phrases, and prepositional phrases. These features are defined as follows: Table 1: Demographic information. Numbers are given in the form: mean (standard deviation). 3 Methods • Phrase type proportion: Length of each phrase type (including embedded phrases), divided by total narrative length. • Average phrase length: Total number of words in a phrase type, divided by number of phrases of that type. • Phrase type rate: Number of phrases of a given type, divided by total"
W14-3420,A00-2018,0,0.153217,"ants This was a retrospective analysis of data collected by the the Aphasia and Neurolinguistics Research Laboratory at Northwestern University. All agrammatic participants had experienced a stroke at least 1 year prior to the narrative sample collection. Demographic information for the participants is given in Table 1. There is no significant (p &lt; 0.05) difference between the patient and control groups on age or level of education. 3.2 Parser Features We consider two types of features: CFG production rules and phrase-level statistics. For the CFG production rules, we use the Charniak parser (Charniak, 2000) trained on Wall Street Journal data to parse each utterance in the transcript and then extract the set of non-lexical productions. The total number of types of productions is large, many of them occurring very infrequently, so we compile a list of the 50 most frequently occurring productions in each of the two groups (agrammatic and controls) and use the combined set as the set of features. The feature values can be binary (does a particular production rule appear in the narrative or not?) or integer (how many times does a rule occur?). The CFG non-terminal symbols follow the Penn Treebank na"
W14-3420,P05-1025,0,0.0323505,"chness and frequency counts of various parts-of-speech (e.g. nouns, verbs); however they also measured “clause-like semantic unit rate”. This feature was intended to measure the speaker’s ability to cluster words together, although it is not clear what the criteria for segmenting clause-like units were or whether it was done 2.2 Using parse features to assess grammaticality Syntactic complexity metrics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentence"
W14-3420,W13-3909,1,0.876641,", we also use the “raw” dataset, with no dysfluencies removed (i.e. including everything inside the parentheses), and an “auto-cleaned” dataset, in which filled pauses are automatically removed from the raw transcripts. We also use a simple algorithm to remove “stutters” and false starts, by 136 removing non-word tokens of length one or two (e.g. C- C- Cinderella would become simply Cinderella). This provides a more realistic view of the performance of our system on real data. We also hypothesize that there may be important information to be found in the dysfluent speech segments. al. (2011), Fraser et al. (2013b)). We hypothesize that the parse features will capture more information about the specific impairments seen in agrammatic aphasia; however, using the general measures of syntactic complexity may be sufficient for the classifiers to distinguish between the groups. 4.2 4.4 Feature weighting and selection To test whether the features can effectively distinguish between the agrammatic group and controls, we use them to train and test a machine learning classifier. We test three different classification algorithms: naive Bayes (NB), support vector machine (SVM), and random forests (RF). We use a"
W14-3420,P12-2038,0,0.0228515,"rees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1 http://talkbank.org/AphasiaBank/ 135 Male/Female Age (years) Education (years) Agrammatic (N = 24) 15/9 58.1 (10.6) 16.3 (2.5) Control (N = 15) 8/7 63.3 (6.4) 16.4 (2.4) 4 4.1 (2009) calculated several surface features based on t"
W14-3420,U10-1011,0,0.0181812,"l., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1 http://talkbank.org/AphasiaBank/ 135 Male/Female Age (years) Education (years) Agrammatic (N = 24) 15/9 58.1 (10.6) 16.3 (2.5) Control (N = 15) 8/7 63.3 (6.4) 16.4 (2.4) 4 4.1 (2009) calculated several surface features based on the output of a parser, such as the length and relative proportion of different phrase types. They used these features to distinguish between human and machine translations, and to determine which of a"
W14-3420,D11-1148,0,0.0219163,"actic complexity metrics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1 http://talkbank.org/AphasiaBank/ 135 Male/Female Age (years) Education (years) Agrammatic (N = 24) 15/9 58.1 (10.6) 16.3 (2.5) Control (N = 15) 8/7 63.3 (6.4) 16.4 (2.4) 4 4.1"
W15-0705,P14-1035,0,0.0363967,"The emerging field of digital literary studies has embraced not only statistical analysis of literary texts in the corpus linguistics tradition, but even more complex methods such as principal components analysis (Burrows, 1987), clustering (Rybicki, 2006), and topic modeling (Goldstone and Underwood, 2012; Jockers, 2013). At the same time, there is sustained interest in computational linguistics in tackling problems that are specific to literature, as evidenced by an annual dedicated workshop as well as various papers at major conferences (Elson et al., 2010; Wallace, 2012; He et al., 2013; Bamman et al., 2014). Though some work in the shared ground between these two fields is explicitly crossdisciplinary, this is still fairly atypical, reflecting a deep cultural barrier (Hammond et al., 2013): in most cases, digital humanists are using off-the-shelf statistical tools with little or no interaction with computer scientists, and computational linguists are developing literature-specific techniques which are unavailable or unknown to the digital humanist community. The high-level goal of the project proposed here is to create an on-going two-way flow of resources between these groups, allowing computat"
W15-0705,I13-1010,1,0.746173,"ently provided by the NLTK part-of-speech tagger. GutenTag also supports simple named entity recognition to identify the major characters in a literary work, by looking at repeated proper names which appear in contexts which indicate personhood. Any collection of words or phrases can be grouped under a single tag using user-defined lexicons, which can be nominal or numerical; as examples of this, the GutenTag includes word properties from the MRC psycholinguistic database (Coltheart, 1980), the General Inquirer Dictionary (Stone et al., 1966) and high-coverage stylistic and polarity lexicons (Brooke and Hirst, 2013; Brooke and Hirst, 2014) which were built automatically using the variation within the PG corpus itself. Tags above the word level include, most prominently, structural elements such as chapters identified in the corpus-reader step. Another tag supported in GutenTag is the TEI “said” tag which is used to identify quoted speech and assign it to a specific character. The current version of “said” identification first detects the quotation convention being used in the text (i.e. single or double quotes), matches right and left quotes to create quote spans, and then looks in the immediate vicinit"
W15-0705,C14-1205,1,0.740399,"TK part-of-speech tagger. GutenTag also supports simple named entity recognition to identify the major characters in a literary work, by looking at repeated proper names which appear in contexts which indicate personhood. Any collection of words or phrases can be grouped under a single tag using user-defined lexicons, which can be nominal or numerical; as examples of this, the GutenTag includes word properties from the MRC psycholinguistic database (Coltheart, 1980), the General Inquirer Dictionary (Stone et al., 1966) and high-coverage stylistic and polarity lexicons (Brooke and Hirst, 2013; Brooke and Hirst, 2014) which were built automatically using the variation within the PG corpus itself. Tags above the word level include, most prominently, structural elements such as chapters identified in the corpus-reader step. Another tag supported in GutenTag is the TEI “said” tag which is used to identify quoted speech and assign it to a specific character. The current version of “said” identification first detects the quotation convention being used in the text (i.e. single or double quotes), matches right and left quotes to create quote spans, and then looks in the immediate vicinity around the quotes to id"
W15-0705,P10-1015,0,0.0247928,"communities, to the benefit of both. 1 Introduction The emerging field of digital literary studies has embraced not only statistical analysis of literary texts in the corpus linguistics tradition, but even more complex methods such as principal components analysis (Burrows, 1987), clustering (Rybicki, 2006), and topic modeling (Goldstone and Underwood, 2012; Jockers, 2013). At the same time, there is sustained interest in computational linguistics in tackling problems that are specific to literature, as evidenced by an annual dedicated workshop as well as various papers at major conferences (Elson et al., 2010; Wallace, 2012; He et al., 2013; Bamman et al., 2014). Though some work in the shared ground between these two fields is explicitly crossdisciplinary, this is still fairly atypical, reflecting a deep cultural barrier (Hammond et al., 2013): in most cases, digital humanists are using off-the-shelf statistical tools with little or no interaction with computer scientists, and computational linguists are developing literature-specific techniques which are unavailable or unknown to the digital humanist community. The high-level goal of the project proposed here is to create an on-going two-way flo"
W15-0705,P13-1129,0,0.147087,"h. 1 Introduction The emerging field of digital literary studies has embraced not only statistical analysis of literary texts in the corpus linguistics tradition, but even more complex methods such as principal components analysis (Burrows, 1987), clustering (Rybicki, 2006), and topic modeling (Goldstone and Underwood, 2012; Jockers, 2013). At the same time, there is sustained interest in computational linguistics in tackling problems that are specific to literature, as evidenced by an annual dedicated workshop as well as various papers at major conferences (Elson et al., 2010; Wallace, 2012; He et al., 2013; Bamman et al., 2014). Though some work in the shared ground between these two fields is explicitly crossdisciplinary, this is still fairly atypical, reflecting a deep cultural barrier (Hammond et al., 2013): in most cases, digital humanists are using off-the-shelf statistical tools with little or no interaction with computer scientists, and computational linguists are developing literature-specific techniques which are unavailable or unknown to the digital humanist community. The high-level goal of the project proposed here is to create an on-going two-way flow of resources between these gro"
W15-0705,C14-1005,0,0.0176841,"ent of direct connections between alluding and alluded works with the PG corpus, which we could then employ to derive metrics of influence and 45 canonicity within the corpus. We are also interested, where appropriate, in identifying features relevant to narratives: when analyzing a novel, for example, it would be interesting to be able to tag entire scenes with a physical location, a time of day, and a list of participants; for an entire narrative, it would be useful to identify particular points in the plot structure such as climax and dénouement, and other kinds of variation such as topic (Kazantseva and Szpakowicz, 2014) and narrator viewpoint (Wiebe, 1994). 6 Interfaces GutenTag is intended for users with no programming background. The potential options are sufficiently complex that a run of GutenTag is defined within a single configuration file, including any number of defined subcorpora, the desired tag sets (including various built-in tagging options and user-defined lexicons), and options for output. We also offer a web interface for small-scale, limited analysis for those who do not want to download the entire corpus. Given our interest in serving the digital humanities community, it is important that t"
W15-0705,P14-1024,0,0.0135392,"ing that might be useful from a digital humanities perspective as well as interesting for computational linguists. Some have been addressed already, and some have not. The following is intended not as an exhaustive list but rather as a starting point for further discussion. At the simpler end of the spectrum, we can imagine taggers which identify some of the classic poetic elements such as rhyme scheme, meter, anaphora, alliteration, onomatopoeia, and the use of foreign languages (along with identification of the specific language being used). Metaphor detection is of growing interest in NLP (Tsvetkov et al., 2014), and would undoubtedly be useful for literary analysis (as would simile detection, a somewhat simpler task). Another challenging but important task is the identification of literary allusions: we envision not only the identification of allusions, but also the establishment of direct connections between alluding and alluded works with the PG corpus, which we could then employ to derive metrics of influence and 45 canonicity within the corpus. We are also interested, where appropriate, in identifying features relevant to narratives: when analyzing a novel, for example, it would be interesting t"
W15-0705,N12-1001,0,0.0122882,"benefit of both. 1 Introduction The emerging field of digital literary studies has embraced not only statistical analysis of literary texts in the corpus linguistics tradition, but even more complex methods such as principal components analysis (Burrows, 1987), clustering (Rybicki, 2006), and topic modeling (Goldstone and Underwood, 2012; Jockers, 2013). At the same time, there is sustained interest in computational linguistics in tackling problems that are specific to literature, as evidenced by an annual dedicated workshop as well as various papers at major conferences (Elson et al., 2010; Wallace, 2012; He et al., 2013; Bamman et al., 2014). Though some work in the shared ground between these two fields is explicitly crossdisciplinary, this is still fairly atypical, reflecting a deep cultural barrier (Hammond et al., 2013): in most cases, digital humanists are using off-the-shelf statistical tools with little or no interaction with computer scientists, and computational linguists are developing literature-specific techniques which are unavailable or unknown to the digital humanist community. The high-level goal of the project proposed here is to create an on-going two-way flow of resources"
W15-0705,J94-2004,0,0.197488,"rks with the PG corpus, which we could then employ to derive metrics of influence and 45 canonicity within the corpus. We are also interested, where appropriate, in identifying features relevant to narratives: when analyzing a novel, for example, it would be interesting to be able to tag entire scenes with a physical location, a time of day, and a list of participants; for an entire narrative, it would be useful to identify particular points in the plot structure such as climax and dénouement, and other kinds of variation such as topic (Kazantseva and Szpakowicz, 2014) and narrator viewpoint (Wiebe, 1994). 6 Interfaces GutenTag is intended for users with no programming background. The potential options are sufficiently complex that a run of GutenTag is defined within a single configuration file, including any number of defined subcorpora, the desired tag sets (including various built-in tagging options and user-defined lexicons), and options for output. We also offer a web interface for small-scale, limited analysis for those who do not want to download the entire corpus. Given our interest in serving the digital humanities community, it is important that the output options reflect their needs"
W15-0705,W13-1401,1,\N,Missing
W15-0915,C14-1071,1,0.462676,"those that are noncompositional (idiomatic) or that are otherwise useful for information retrieval applications. With our goal of helping advanced learners produce more fluent language, we are more interested in sequences that underpin the structure of sentences and not just terms that reflect its topic. As much as possible, we do not want to limit the syntactic composition, size, or frequency of our lexical items, and we want methods that allow us to build distinct, high-coverage lexicons for varying genres. Working on top of an existing pipeline for unsupervised multiword unit segmentation (Brooke et al., 2014), the current work presents two key improvements on that initial model that allow us to build high-coverage lexicons of formulaic language. With respect to improving the quality of the sequences, we present a new measure for distinguishing true (lexicalized) affinity from background syntactic effects, the lexical predictability ratio, and integrate it into the model to improve the quality of the out96 Proceedings of NAACL-HLT 2015, pages 96–104, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics put lexicon. The second major advance expands the coverage o"
W15-0915,J90-1003,0,0.372344,"f our lexicon to contextual recognition of multiword expressions, using a recently released dataset. In both cases, our method outperforms a variety of alternatives, including the original segmentation approach that was our starting point; like that original approach, our lexicon creation method is highly scalable and deterministic, and has only one key parameter (minimum frequency in the corpus). 2 Related Work There is a long-standing area of research in computational linguistics focusing on lexical association measures, often, though not exclusively, for the creation of multiword lexicons (Church and Hanks, 1990; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010): for two-word sequences there are, in fact, far too many to list in this context, though most of the research has centered upon popular options such as the t-test, log-likelihood, and pointwise mutual information (PMI). When these methods are used to build a lexicon, particular syntactic patterns and thresholds for the metrics are typically chosen. Critics note that many of the statistical metrics do not generalize at all beyond two words, but PMI (Church and Hanks, 1990), the log ratio of the joint probability to the product of the margi"
W15-0915,W11-0823,0,0.0166607,"aluate using a novel method that allows us to calculate an estimate of recall without a reference lexicon, showing that good performance in the second enhancement depends crucially on the first, and that our lexicon conforms much more with human judgment of formulaic language than alternatives. 1 Introduction A significant portion of a speaker’s lexical knowledge consists not of atomic lexical entries, i.e. words, but rather sequences built from their combination; in fact, the working multiword lexicon of the average native speaker is almost certainly much larger than the single-word lexicon (Church, 2011). Language learners, due to lack of exposure to the new language and interference from their native language, often fail to use these larger sequences proficiently, a fact which has been demonstrated via corpus analysis using high frequency n-grams (Chen and Baker, 2010; Granger and Bestgen, 2014). Although high frequency n-grams, known in corpus linguistics as lexical bundles, are useful for certain kinds of analysis, they are inappropriate for a fullyfeatured multiword learning system, which would ideally involve an electronic lexicon corresponding roughly to the internal lexicon of native s"
W15-0915,J09-1005,0,0.0411605,"n-grams that appear in frequent larger ones, and mutual expectation (Dias et al., 1999), which produces a normalized statistic 97 that reflects how much a candidate phrase resists the omission of any particular word. Overlapping with this area is the research on multiword expressions (Baldwin and Kim, 2010), which is generally (though not exclusively) understood to refer to idiomatic, non-compositional multiword units; even so restricted, there is a huge variety of distinct types, and research in the area has tended to be rather focused, looking at, for instance, just verb/noun combinations (Fazly et al., 2009). The recent work of Schneider et al. (2014a) is a rare example of a comprehensive MWE identification model which distinguishes a full range of MWE sequences, including those involving gaps, using a supervised sequence tagging model; like other models in this space, Schneider et al. make use of existing manual lexical resources and they note that an (unsupervised) automatic lexical resource could be useful addition to the model. Otherwise, gaps in MWEs have generally addressed by using full syntactic representations (Seretan, 2011). Beyond association metrics, other unsupervised approaches to"
W15-0915,W11-2165,0,0.0302465,"sing a supervised sequence tagging model; like other models in this space, Schneider et al. make use of existing manual lexical resources and they note that an (unsupervised) automatic lexical resource could be useful addition to the model. Otherwise, gaps in MWEs have generally addressed by using full syntactic representations (Seretan, 2011). Beyond association metrics, other unsupervised approaches to the multiword problem include that of Newman et al. (2012), who used a generative Dirichlet Process model which jointly creates a linear segmentation of the corpus and a multiword vocabulary. Gimpel and Smith (2011) focus specifically on deriving word sequences with gaps using a generative model, with the intent of improving machine translation. The drawback to these generative methods, relative to association metrics, is scalability and a certain degree of randomness, since these methods generally involve Gibbs sampling with many iterations through the corpus to reach an acceptable model. The approach presented here is based on that of Brooke et al. (2014), which was developed explicitly to work well for larger corpora, in the order of a billion words or more; we will leave further discussion of that wo"
W15-0915,C12-1127,0,0.32165,"igh frequency n-grams, known in corpus linguistics as lexical bundles, are useful for certain kinds of analysis, they are inappropriate for a fullyfeatured multiword learning system, which would ideally involve an electronic lexicon corresponding roughly to the internal lexicon of native speakers. In this work, we adopt the creation of such a lexicon as our goal. Though much work has been done and many resources created which focus on specific aspects of the multiword vocabulary, most notably in fields such as multiword expressions (MWEs) (Baldwin and Kim, 2010) and keyphrase/term extraction (Newman et al., 2012), our pedagogical perspective leads us towards a somewhat broader theoretical foundation, the formulaic sequence theory of Wray (2002; 2008). We are interested in any multiword sequence that could plausibly be lexicalized, not simply those that are noncompositional (idiomatic) or that are otherwise useful for information retrieval applications. With our goal of helping advanced learners produce more fluent language, we are more interested in sequences that underpin the structure of sentences and not just terms that reflect its topic. As much as possible, we do not want to limit the syntactic c"
W15-0915,Q14-1016,0,0.310649,"ones, and mutual expectation (Dias et al., 1999), which produces a normalized statistic 97 that reflects how much a candidate phrase resists the omission of any particular word. Overlapping with this area is the research on multiword expressions (Baldwin and Kim, 2010), which is generally (though not exclusively) understood to refer to idiomatic, non-compositional multiword units; even so restricted, there is a huge variety of distinct types, and research in the area has tended to be rather focused, looking at, for instance, just verb/noun combinations (Fazly et al., 2009). The recent work of Schneider et al. (2014a) is a rare example of a comprehensive MWE identification model which distinguishes a full range of MWE sequences, including those involving gaps, using a supervised sequence tagging model; like other models in this space, Schneider et al. make use of existing manual lexical resources and they note that an (unsupervised) automatic lexical resource could be useful addition to the model. Otherwise, gaps in MWEs have generally addressed by using full syntactic representations (Seretan, 2011). Beyond association metrics, other unsupervised approaches to the multiword problem include that of Newma"
W15-0915,schneider-etal-2014-comprehensive,0,0.215966,"ones, and mutual expectation (Dias et al., 1999), which produces a normalized statistic 97 that reflects how much a candidate phrase resists the omission of any particular word. Overlapping with this area is the research on multiword expressions (Baldwin and Kim, 2010), which is generally (though not exclusively) understood to refer to idiomatic, non-compositional multiword units; even so restricted, there is a huge variety of distinct types, and research in the area has tended to be rather focused, looking at, for instance, just verb/noun combinations (Fazly et al., 2009). The recent work of Schneider et al. (2014a) is a rare example of a comprehensive MWE identification model which distinguishes a full range of MWE sequences, including those involving gaps, using a supervised sequence tagging model; like other models in this space, Schneider et al. make use of existing manual lexical resources and they note that an (unsupervised) automatic lexical resource could be useful addition to the model. Otherwise, gaps in MWEs have generally addressed by using full syntactic representations (Seretan, 2011). Beyond association metrics, other unsupervised approaches to the multiword problem include that of Newma"
W15-0915,W01-0513,0,0.101501,"ual recognition of multiword expressions, using a recently released dataset. In both cases, our method outperforms a variety of alternatives, including the original segmentation approach that was our starting point; like that original approach, our lexicon creation method is highly scalable and deterministic, and has only one key parameter (minimum frequency in the corpus). 2 Related Work There is a long-standing area of research in computational linguistics focusing on lexical association measures, often, though not exclusively, for the creation of multiword lexicons (Church and Hanks, 1990; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010): for two-word sequences there are, in fact, far too many to list in this context, though most of the research has centered upon popular options such as the t-test, log-likelihood, and pointwise mutual information (PMI). When these methods are used to build a lexicon, particular syntactic patterns and thresholds for the metrics are typically chosen. Critics note that many of the statistical metrics do not generalize at all beyond two words, but PMI (Church and Hanks, 1990), the log ratio of the joint probability to the product of the marginal probabilities, is a pro"
W16-0301,E09-1017,0,0.0283564,"Missing"
W16-0301,W15-1204,0,0.0306285,"Missing"
W16-0301,W14-3202,0,0.143221,"Missing"
W16-0301,W14-3204,0,0.0703373,"Missing"
W16-0301,W14-3210,0,0.51461,"Missing"
W16-0301,W15-1207,0,0.0376156,"Missing"
W16-0417,N04-1015,0,0.0806853,"of our knowledge, three unsupervised techniques have been previously proposed for categorization of posts in Web forums. Out of these, Deepak and Visweswariah (2014) identified only answer posts, and Cong et al. (2008) additionally extracted question posts. The more difficult task of identifying multiple categories was tackled only by Joty et al. (2011). They used a combination of HMMs and Gaussian Mixture Models (GMMs) in order to classify forum posts into 12 dialogue act categories. This model is similar to the content and conversation models used for other tasks by Ritter et al. (2010) and Barzilay and Lee (2004) respectively. In addition to the probability distribution of word n-grams, they use some structural features such as the chronological position of a post in the thread, the number of tokens in the post, and au101 thor identity. The motivation for this approach is that HMMs can model the sequential nature of dialogue acts well. For example, the fact that a solution is more likely to follow a question, as opposed to any other category, can be implicitly encoded in the HMMs. Our approach is inspired by the same idea. One major drawback of unsupervised methods is that they often generate clusters"
W16-0417,C12-2018,0,0.133014,"g but want to maintain some street manners. Solution User JcArnold: I’ve got 37” trails and they are not noisy. I don’t know about mud but they are great tires in the rocks and snow. Solution User 15JKU: Thanks guys! Truly appreciate it. Feedback Table 1: Example forum thread manually tagged with each post’s purpose in the conversation. (Adapted from: http://www.jeepforum.com/forum/f15/tire-recommendations-3455674/) into one or more categories such as question, solution, clarification, feedback, command, request, etc. Most previous work has concentrated on supervised machine learning methods (Catherine et al., 2012; Bhatia et al., 2012; Wang et al., 2010; Kim et al., 2010; Sondhi et al., 2010) using manually annotated data in order to predict the annotations of unseen data. Apart from being constrained by the requirement of manually annotated data for training, these methods are also limited in applicability to the domains they are trained on. In contrast, unsupervised methods overcome these drawbacks by identifying unlabeled clusters of data, each of which could potentially be mapped to a target category that one wants to identify. To the best of our knowledge, three unsupervised techniques have been p"
W16-0417,I13-1001,0,0.0146061,"relevant when the purpose is to find clusters of post categories such as question, answer, feedback, etc. Moreover, because the clusters are unlabeled, post-processing is necessary to map the clusters to the target categories. Semi-supervised methods can overcome the drawbacks of both unsupervised and supervised methods by using a minimal amount of labeled data (which is costly to obtain) and a large amount of unlabeled data (which is easily available). To our knowledge, there exist only two semi-supervised methods for categorization of posts in Web forums and they identify only answer posts. Catherine et al. (2013) employed the co-training framework, whereas Jeong et al. (2009) used domain adaptation from labeled spoken dialogue datasets by means of a sub-tree pattern mining algorithm. In experiments, we show that our methods outperform the former work; however, the unavailability of code and data prevents empirical comparison with the latter. 3 3.1 Proposed Methods Conversation Model As discussed previously, our models derive inspiration from the work of Joty et al. (2011). Our underlying model is the same but differs in several important details. Our conversation model is a Hidden Markov Model (HMM),"
W16-0417,P14-1015,0,0.021031,"010) using manually annotated data in order to predict the annotations of unseen data. Apart from being constrained by the requirement of manually annotated data for training, these methods are also limited in applicability to the domains they are trained on. In contrast, unsupervised methods overcome these drawbacks by identifying unlabeled clusters of data, each of which could potentially be mapped to a target category that one wants to identify. To the best of our knowledge, three unsupervised techniques have been previously proposed for categorization of posts in Web forums. Out of these, Deepak and Visweswariah (2014) identified only answer posts, and Cong et al. (2008) additionally extracted question posts. The more difficult task of identifying multiple categories was tackled only by Joty et al. (2011). They used a combination of HMMs and Gaussian Mixture Models (GMMs) in order to classify forum posts into 12 dialogue act categories. This model is similar to the content and conversation models used for other tasks by Ritter et al. (2010) and Barzilay and Lee (2004) respectively. In addition to the probability distribution of word n-grams, they use some structural features such as the chronological positi"
W16-0417,D09-1130,0,0.014502,"h as question, answer, feedback, etc. Moreover, because the clusters are unlabeled, post-processing is necessary to map the clusters to the target categories. Semi-supervised methods can overcome the drawbacks of both unsupervised and supervised methods by using a minimal amount of labeled data (which is costly to obtain) and a large amount of unlabeled data (which is easily available). To our knowledge, there exist only two semi-supervised methods for categorization of posts in Web forums and they identify only answer posts. Catherine et al. (2013) employed the co-training framework, whereas Jeong et al. (2009) used domain adaptation from labeled spoken dialogue datasets by means of a sub-tree pattern mining algorithm. In experiments, we show that our methods outperform the former work; however, the unavailability of code and data prevents empirical comparison with the latter. 3 3.1 Proposed Methods Conversation Model As discussed previously, our models derive inspiration from the work of Joty et al. (2011). Our underlying model is the same but differs in several important details. Our conversation model is a Hidden Markov Model (HMM), in which hidden (unobserved) states correspond to post categorie"
W16-0417,W10-2923,0,0.0255775,"ld: I’ve got 37” trails and they are not noisy. I don’t know about mud but they are great tires in the rocks and snow. Solution User 15JKU: Thanks guys! Truly appreciate it. Feedback Table 1: Example forum thread manually tagged with each post’s purpose in the conversation. (Adapted from: http://www.jeepforum.com/forum/f15/tire-recommendations-3455674/) into one or more categories such as question, solution, clarification, feedback, command, request, etc. Most previous work has concentrated on supervised machine learning methods (Catherine et al., 2012; Bhatia et al., 2012; Wang et al., 2010; Kim et al., 2010; Sondhi et al., 2010) using manually annotated data in order to predict the annotations of unseen data. Apart from being constrained by the requirement of manually annotated data for training, these methods are also limited in applicability to the domains they are trained on. In contrast, unsupervised methods overcome these drawbacks by identifying unlabeled clusters of data, each of which could potentially be mapped to a target category that one wants to identify. To the best of our knowledge, three unsupervised techniques have been previously proposed for categorization of posts in Web foru"
W16-0417,P14-5010,0,0.00318074,"1: 42: |Tx |−1 trans countsi, j := ∑Tx ∑ a=1 1Sx,a = i, Sx,a+1 = j end for end for for i = 1 → numStates do for j = 1 → numStates do φi, j := (trans countsi, j + δ2 )/(Σk,l (trans countsk,l ) + δ2 × numStates2 ) sitioning from state i to state j end for end for S := Viterbi algorithm(π, φ , L) if sum of observation probabilities converged then break end if end for CL := S 104 // φi, j is the probability of tran4.2 Preprocessing and Configuration Parameters Initially, all forum posts were tokenized by sentence and word, followed by POS tagging and stemming — all using Stanford CoreNLP Toolkit (Manning et al., 2014). Stopword removal was found to degrade performance; hence, it was not used. Forum conversations often consist of informal English language text, along with the use of domain-specific abbreviations and non-standard special characters such as ellipses and emoticons. Hence, some errors are introduced in all the previous steps. However, no effort was made to overcome them, and this is accepted as a limitation of the current work. All methods require the conversion of posts to vectors of n-grams. For this purpose, we experimented with both unigrams and bigrams, and the former was found to produce"
W16-0417,J93-2004,0,0.053178,"the posts’ purpose or intention. To overcome this limitation, we enhance the plain conversation model by modeling HMM emissions partially from partof-speech (POS) tags of words. This idea is based on the assumption that posts belonging to the same category are likely to be syntactically similar. For example, question posts are very likely to contain POS tags such as WDT, WP, WP$, and WRB2 . Our model uses POS n-gram language models in addition to word n-gram language models, and calculates the HMM emission probability of a post given its 2 These tags can be seen in the Penn Treebank project (Marcus et al., 1993). state using a linear combination of both. Here, the probability of a post Pi , given a state Sk , is calculated as shown in Equation 2.   ∏ j λ × p(Wi, j |Lk ) + (1 − λ ) × p(POSi, j |PLk ) Z 0≤λ ≤1 &quot; #   Z = ∑ ∏ λ × p(Wi, j |Lk ) + (1 − λ ) × p(POSi, j |PLk ) Ubuntu (Bhatia et al., 2012) Domain: Computer technical Tagset: Question, Repeat Question, Clarification, Solution,Further Details, Positive Feedback, Negative Feedback, Spam Number of threads: 100 p(Pi |Sk ) = i,k j where POSi, j is the jth (in no particular order) POS n-gram in post Pi , PLk is the POS n-gram language model for s"
W16-0417,N10-1020,0,0.118514,"to identify. To the best of our knowledge, three unsupervised techniques have been previously proposed for categorization of posts in Web forums. Out of these, Deepak and Visweswariah (2014) identified only answer posts, and Cong et al. (2008) additionally extracted question posts. The more difficult task of identifying multiple categories was tackled only by Joty et al. (2011). They used a combination of HMMs and Gaussian Mixture Models (GMMs) in order to classify forum posts into 12 dialogue act categories. This model is similar to the content and conversation models used for other tasks by Ritter et al. (2010) and Barzilay and Lee (2004) respectively. In addition to the probability distribution of word n-grams, they use some structural features such as the chronological position of a post in the thread, the number of tokens in the post, and au101 thor identity. The motivation for this approach is that HMMs can model the sequential nature of dialogue acts well. For example, the fact that a solution is more likely to follow a question, as opposed to any other category, can be implicitly encoded in the HMMs. Our approach is inspired by the same idea. One major drawback of unsupervised methods is that"
W16-0417,C10-2133,0,0.0307316,"rails and they are not noisy. I don’t know about mud but they are great tires in the rocks and snow. Solution User 15JKU: Thanks guys! Truly appreciate it. Feedback Table 1: Example forum thread manually tagged with each post’s purpose in the conversation. (Adapted from: http://www.jeepforum.com/forum/f15/tire-recommendations-3455674/) into one or more categories such as question, solution, clarification, feedback, command, request, etc. Most previous work has concentrated on supervised machine learning methods (Catherine et al., 2012; Bhatia et al., 2012; Wang et al., 2010; Kim et al., 2010; Sondhi et al., 2010) using manually annotated data in order to predict the annotations of unseen data. Apart from being constrained by the requirement of manually annotated data for training, these methods are also limited in applicability to the domains they are trained on. In contrast, unsupervised methods overcome these drawbacks by identifying unlabeled clusters of data, each of which could potentially be mapped to a target category that one wants to identify. To the best of our knowledge, three unsupervised techniques have been previously proposed for categorization of posts in Web forums. Out of these, Deep"
W16-0417,J00-3003,0,0.254215,"Missing"
W16-0417,U10-1006,0,0.0292727,"olution User JcArnold: I’ve got 37” trails and they are not noisy. I don’t know about mud but they are great tires in the rocks and snow. Solution User 15JKU: Thanks guys! Truly appreciate it. Feedback Table 1: Example forum thread manually tagged with each post’s purpose in the conversation. (Adapted from: http://www.jeepforum.com/forum/f15/tire-recommendations-3455674/) into one or more categories such as question, solution, clarification, feedback, command, request, etc. Most previous work has concentrated on supervised machine learning methods (Catherine et al., 2012; Bhatia et al., 2012; Wang et al., 2010; Kim et al., 2010; Sondhi et al., 2010) using manually annotated data in order to predict the annotations of unseen data. Apart from being constrained by the requirement of manually annotated data for training, these methods are also limited in applicability to the domains they are trained on. In contrast, unsupervised methods overcome these drawbacks by identifying unlabeled clusters of data, each of which could potentially be mapped to a target category that one wants to identify. To the best of our knowledge, three unsupervised techniques have been previously proposed for categorization of"
W18-5214,P82-1020,0,0.649674,"Missing"
W18-5214,D14-1181,0,0.0111237,"reats, whereas positive emotions are used more in nonthreats. These features are motivated by theories, such as Brown and Levinson (1987) and Partington (2003) that recognize varying degrees of politeness in threatening or saving the addressee’s face. Achievements are used more in non-threats and cognitive processes are used more in reputation threats. This is consistent with theories (Mulholland, 2003) that recognize mentioning the consequences of the fault as one mode of accusation. Data 6 Approach Convolutional Neural Networks (CNN) have been shown to be effective for classification tasks (Kim, 2014). Here, we used a CNN model to represent the question and answer pairs for binary classifications of face-saving language. We first represented each word in the question and the answer with its associated pre-trained embedding. We then applied a convolution operation to each possible window of x words from the question and the answer to produce a feature map, similar to the apReputation threat analysis A principled analysis of the language of facethreats or accusations themselves falls outside the scope of this work, but here we characterize the 6 LInked Opposition 11,090 11,504 Table 1: Corpu"
W18-5214,P13-2013,0,0.0135461,"ther models, and for brevity we do not report the results here. We further trained an SVM classifier (using the scikit-learn package (Pedregosa et al., 2011)) with all possible combinations of words extracted from the cross-product of questions and answers to cap7 Using ture the interaction between reputation threat and reputation defence. The features are tuples of word pairs from question and answer pairs. We removed word pairs that occurred fewer than 80 times in the datasets. Our use of this set of features is inspired by the effectiveness of word pairs in classifying discourse relations (Biran and McKeown, 2013; Pitler et al., 2009) regardless of their sparsity issue. 7 Evaluation and results We approach the recognition of the face-saving language as a binary supervised classification task. Our baselines are majority class (which is always answers given to the opposition questions), an SVM model trained with answer unigram vectors (weighted using tf–idf, represented with the notation ‘-Answers’ in the result tables), and one layer of GRU to model answer sequences. Since reputation defence is expressed in response to the reputation threat, we further considered the question as the context of the repu"
W18-5214,D18-1387,0,0.0142845,"ourcing. We compute the total association scores of the lexicon words in the answer for each class of emotions and sentiments. We further examined the NRC VAD Lexicon12 for our analysis. This lexicon provides valence (positiveness–negativeness / pleasure / displeasure), arousal (active–passive), and dominance (dominant–submissive) scores for 20K English words (Mohammad, 2018). These dimensions have been used for analysis of human interaction (Burgoon and Hale, 1984). We use the total score of each dimension in the answer as a feature. We also consider vagueness cue words (Bhatia et al., 2016; Lebanoff and Liu, 2018). This set of features (40 cue words) is represented by the frequency of the vagueness cues in the answer. The use of these features is motivated by theories such as that of Fraser (2012) that suggest that hedge words can be used to avoid face-threatening acts. We also use bigrams as additional features. We performed the classification using SVM. The results of the binary classification of face-saving language on the balanced data of the cross-parliament setting is presented in Table 5. The only emotion that contributed to the classification was anger. The positive impact of anger Non-defence"
W18-5214,P18-1017,0,0.0155887,"for basic emotions including anger, fear, joy, sadness, disgust, anticipation, trust, surprise, and sentiments (positive and negative) (Mohammad and Turney, 2013). It consists of 14,182 unigrams that are manually annotated through crowdsourcing. We compute the total association scores of the lexicon words in the answer for each class of emotions and sentiments. We further examined the NRC VAD Lexicon12 for our analysis. This lexicon provides valence (positiveness–negativeness / pleasure / displeasure), arousal (active–passive), and dominance (dominant–submissive) scores for 20K English words (Mohammad, 2018). These dimensions have been used for analysis of human interaction (Burgoon and Hale, 1984). We use the total score of each dimension in the answer as a feature. We also consider vagueness cue words (Bhatia et al., 2016; Lebanoff and Liu, 2018). This set of features (40 cue words) is represented by the frequency of the vagueness cues in the answer. The use of these features is motivated by theories such as that of Fraser (2012) that suggest that hedge words can be used to avoid face-threatening acts. We also use bigrams as additional features. We performed the classification using SVM. The re"
W18-5214,naderi-hirst-2017-recognizing,1,0.832038,"tual language used in the defence of reputation. Here, we examine political speeches and investigate whether we can detect the language of reputation defence. We created a corpus of reputa2 Related work Reputation defense is more broadly related to Aristotelian ethos (Aristotle, 2007) or one’s credibility that is reflected through the use of language. Previous studies on face-saving and reputation management focused on identifying various persuasive strategies and their effectiveness (Benoit, 1995; Coombs and Holladay, 2008; Burns and Bruner, 2000; Sheldon and Sallot, 2008). In the NLP field, Naderi and Hirst (2017) performed a manual annotation analysis on reputation defence strategies in Parliament and proposed a computational model to identify strategies of denial, excuse, justification, and concession. Naderi and Hirst (2018) further proposed two approaches to 1 The data is freely available at http://www.cs. toronto.edu/˜nona/data/data.html 2 https://www.ourcommons.ca/ About/Compendium/Questions/c_d_ principlesguidelinesoralquestions-e.htm 111 Proceedings of the 5th Workshop on Argument Mining, pages 111–120 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics automat"
W18-5214,D14-1162,0,0.0878481,"0 for five-fold crossvalidation and cross-parliament experiments, respectively. Recurrent neural networks have been used effectively in NLP for sequence modeling. Here, we further used two long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks7 with 128 units to represent questions and answers, separately. The LSTM layers were then passed to a dropout layer (Hinton et al., 2012) with a rate of 0.6. We then merged the two representations. For all our Neural Network models, we initialized our word representations using the publicly available GloVe pre-trained word embeddings (Pennington et al., 2014)8 (300-dimensional vectors trained on Common Crawl data), and restricted the vocabulary to the 5,000 most-frequent words. The models were trained with binary crossentropy with the Adam optimizer (Kingma and Ba, 2014) for 10 and 5 epochs for five-fold crossvalidation and cross-parliament experiments, respectively. We also tried encoding the questions and answers using a layer of Gated Recurrent Units (GRU) (Cho et al., 2014) with shared parameters, but this model performed worse than the other models, and for brevity we do not report the results here. We further trained an SVM classifier (using"
W18-5214,P09-1077,0,0.0108969,"ity we do not report the results here. We further trained an SVM classifier (using the scikit-learn package (Pedregosa et al., 2011)) with all possible combinations of words extracted from the cross-product of questions and answers to cap7 Using ture the interaction between reputation threat and reputation defence. The features are tuples of word pairs from question and answer pairs. We removed word pairs that occurred fewer than 80 times in the datasets. Our use of this set of features is inspired by the effectiveness of word pairs in classifying discourse relations (Biran and McKeown, 2013; Pitler et al., 2009) regardless of their sparsity issue. 7 Evaluation and results We approach the recognition of the face-saving language as a binary supervised classification task. Our baselines are majority class (which is always answers given to the opposition questions), an SVM model trained with answer unigram vectors (weighted using tf–idf, represented with the notation ‘-Answers’ in the result tables), and one layer of GRU to model answer sequences. Since reputation defence is expressed in response to the reputation threat, we further considered the question as the context of the reputation defence and tra"
W18-5509,W03-1014,0,0.22456,"n Toronto Star annotations. DQ: GRU model is trained with only direct quotes from the PolitiFact data and tested on Toronto Star annotations. While Rashkin et al. (2017), found that LIWC features were effective for predicting the truthfulness of the statements in PolitiFact, we did not observe any improvements in the performance of the classifier in our classification task on Canadian Parliamentary data. Furthermore, we did not observe any improvements in the classification tasks using sentiment and subjectivity features extracted using OpinionFinder (Wilson et al., 2005; Riloff et al., 2003; Riloff and Wiebe, 2003). 6 F1 63 40 50 Majority GRU (All) GRU (DQ) F1 81 73 72 True False 84 88 29 8 Table 6: 2-point scale comparison of the PolitiFact data and Toronto Star annotations. All: GRU model is trained with all PolitiFact data and tested on Toronto Star annotations. DQ: GRU model is trained with only direct quotes from the PolitiFact data and tested on Toronto Star annotations. Comparison with PolitiFact dataset In this section, we perform a direct analysis with the PolitiFact dataset. We first train a GRU model (used a sequence length of 200, other hyperparameters the same as those of the experiment des"
W18-5509,W14-4012,0,0.059938,"Missing"
W18-5509,W03-0404,0,0.0802669,"Missing"
W18-5509,N18-5006,0,0.0656814,"Missing"
W18-5509,N18-1074,0,0.0375208,"Missing"
W18-5509,W14-2508,0,0.0625778,"Missing"
W18-5509,P17-1066,0,0.0162714,"Pants-on-fire false 867 of the annotated data (not including the justificaTotal 10,483 tions): Table 2: Distribution of labels in the PolitiFact dataset Example 3.1 Q. [Michelle Rempel] Mr. Speaker, [social programs across Canada are under severe strain due to tens of thousands of unplanned immigrants illegally crossing into Canada from the United States.]False [Forty per cent in Toronto’s homeless shelters are recent asylum claimants.]True [This, food bank usage, and unemployment rates show that many new asylum claimants are not having successful integration experiences.]False fication task. Ma et al. (2017) used a kernel-based model to detect rumors in tweets. Wang (2017) used the statements from PolitiFact and the 6point scale of truthfulness; he compared the performance of multiple classifiers and reported some improvement by using metadata related to the person making the statements. Rashkin et al. (2017) examined the effectiveness of LIWC (Linguistic Inquiry and Word Count) and stylistic lexicon features in determining the reliability of the news corpus and truthfulness of the PolitiFact dataset. The only reliability measurement reported on the PolitiFact dataset is by Wang (2017), who manua"
W18-5509,P17-2067,0,0.0239492,"icaTotal 10,483 tions): Table 2: Distribution of labels in the PolitiFact dataset Example 3.1 Q. [Michelle Rempel] Mr. Speaker, [social programs across Canada are under severe strain due to tens of thousands of unplanned immigrants illegally crossing into Canada from the United States.]False [Forty per cent in Toronto’s homeless shelters are recent asylum claimants.]True [This, food bank usage, and unemployment rates show that many new asylum claimants are not having successful integration experiences.]False fication task. Ma et al. (2017) used a kernel-based model to detect rumors in tweets. Wang (2017) used the statements from PolitiFact and the 6point scale of truthfulness; he compared the performance of multiple classifiers and reported some improvement by using metadata related to the person making the statements. Rashkin et al. (2017) examined the effectiveness of LIWC (Linguistic Inquiry and Word Count) and stylistic lexicon features in determining the reliability of the news corpus and truthfulness of the PolitiFact dataset. The only reliability measurement reported on the PolitiFact dataset is by Wang (2017), who manually analyzed 200 statements from PolitiFact and reached an agreeme"
W18-5509,H05-1044,0,0.0477727,"ined with all PolitiFact data and tested on Toronto Star annotations. DQ: GRU model is trained with only direct quotes from the PolitiFact data and tested on Toronto Star annotations. While Rashkin et al. (2017), found that LIWC features were effective for predicting the truthfulness of the statements in PolitiFact, we did not observe any improvements in the performance of the classifier in our classification task on Canadian Parliamentary data. Furthermore, we did not observe any improvements in the classification tasks using sentiment and subjectivity features extracted using OpinionFinder (Wilson et al., 2005; Riloff et al., 2003; Riloff and Wiebe, 2003). 6 F1 63 40 50 Majority GRU (All) GRU (DQ) F1 81 73 72 True False 84 88 29 8 Table 6: 2-point scale comparison of the PolitiFact data and Toronto Star annotations. All: GRU model is trained with all PolitiFact data and tested on Toronto Star annotations. DQ: GRU model is trained with only direct quotes from the PolitiFact data and tested on Toronto Star annotations. Comparison with PolitiFact dataset In this section, we perform a direct analysis with the PolitiFact dataset. We first train a GRU model (used a sequence length of 200, other hyperpara"
W18-5509,D14-1162,0,0.0830538,"Missing"
W18-5509,D17-1317,0,0.143732,"illegally crossing into Canada from the United States.]False [Forty per cent in Toronto’s homeless shelters are recent asylum claimants.]True [This, food bank usage, and unemployment rates show that many new asylum claimants are not having successful integration experiences.]False fication task. Ma et al. (2017) used a kernel-based model to detect rumors in tweets. Wang (2017) used the statements from PolitiFact and the 6point scale of truthfulness; he compared the performance of multiple classifiers and reported some improvement by using metadata related to the person making the statements. Rashkin et al. (2017) examined the effectiveness of LIWC (Linguistic Inquiry and Word Count) and stylistic lexicon features in determining the reliability of the news corpus and truthfulness of the PolitiFact dataset. The only reliability measurement reported on the PolitiFact dataset is by Wang (2017), who manually analyzed 200 statements from PolitiFact and reached an agreement of 0.82 using Cohen’s kappa measurement with the journalists’ labels. Jaradat et al. (2018) used a set of linguistic features to rank checkworthy claims. Throne et al. (2018) created a dataset for claim verification. This dataset consists"
W18-5620,S17-2093,0,0.0376166,"n for clinical text relies on human-labeled spans and relations. The input to these models is usually pairs of events (or events and times) that a human has identified as being related, and all the model has to do is decide the type of relation. However, given an unlabeled dataset the task is much more difficult – the system must first identify the events and time phrases, decide which pairs are related, and then determine the type of each relation. Derczynski (2017) covers the general topic of temporal ordering of events in text. For the medical domain, the Clinical TempEval task at SemEval (Bethard et al., 2017) has multiple tasks that involve identification of events, time expressions, and attributes in clinical notes, as well as relation classification. SemEval 2015 also had a task on cross-document event ordering, although the data was in the news domain (Minard et al., 2015). Additionally, most recent work has focused on small relation sets, such as narrative container relations (CONTAINS, NO - RELATION), which were originally introduced by Pustejovsky and Stubbs (2011), or simple relations (BEFORE, AFTER, OVERLAP , NONE ), although some work has attempted to classify with Allen’s complete set of"
W18-5620,E17-2118,0,0.013856,"ification of events, time expressions, and attributes in clinical notes, as well as relation classification. SemEval 2015 also had a task on cross-document event ordering, although the data was in the news domain (Minard et al., 2015). Additionally, most recent work has focused on small relation sets, such as narrative container relations (CONTAINS, NO - RELATION), which were originally introduced by Pustejovsky and Stubbs (2011), or simple relations (BEFORE, AFTER, OVERLAP , NONE ), although some work has attempted to classify with Allen’s complete set of 13 temporal relations (Allen, 1984). Dligach et al. (2017) and Lin et al. (2017) achieved state-of-the-art performance on identifying container relations in the THYME corpus (Styler et al., 2014); however, they considered only relations in which both entities appear in the same sentence. This is a limitation in many temporal relation systems. Since clinical notes are often long and may refer to distant entities such as the admission or discharge date, crosssentence relations should not be ignored. Tourille et al. (2017) identified cross-sentence container relations in the THYME corpus, in addition to intrasentence relations, using a bi-directional LS"
W18-5620,W17-2341,0,0.0119317,"expressions, and attributes in clinical notes, as well as relation classification. SemEval 2015 also had a task on cross-document event ordering, although the data was in the news domain (Minard et al., 2015). Additionally, most recent work has focused on small relation sets, such as narrative container relations (CONTAINS, NO - RELATION), which were originally introduced by Pustejovsky and Stubbs (2011), or simple relations (BEFORE, AFTER, OVERLAP , NONE ), although some work has attempted to classify with Allen’s complete set of 13 temporal relations (Allen, 1984). Dligach et al. (2017) and Lin et al. (2017) achieved state-of-the-art performance on identifying container relations in the THYME corpus (Styler et al., 2014); however, they considered only relations in which both entities appear in the same sentence. This is a limitation in many temporal relation systems. Since clinical notes are often long and may refer to distant entities such as the admission or discharge date, crosssentence relations should not be ignored. Tourille et al. (2017) identified cross-sentence container relations in the THYME corpus, in addition to intrasentence relations, using a bi-directional LSTM. They used word and"
W18-5620,W11-0419,0,0.0272631,"i (2017) covers the general topic of temporal ordering of events in text. For the medical domain, the Clinical TempEval task at SemEval (Bethard et al., 2017) has multiple tasks that involve identification of events, time expressions, and attributes in clinical notes, as well as relation classification. SemEval 2015 also had a task on cross-document event ordering, although the data was in the news domain (Minard et al., 2015). Additionally, most recent work has focused on small relation sets, such as narrative container relations (CONTAINS, NO - RELATION), which were originally introduced by Pustejovsky and Stubbs (2011), or simple relations (BEFORE, AFTER, OVERLAP , NONE ), although some work has attempted to classify with Allen’s complete set of 13 temporal relations (Allen, 1984). Dligach et al. (2017) and Lin et al. (2017) achieved state-of-the-art performance on identifying container relations in the THYME corpus (Styler et al., 2014); however, they considered only relations in which both entities appear in the same sentence. This is a limitation in many temporal relation systems. Since clinical notes are often long and may refer to distant entities such as the admission or discharge date, crosssentence"
W18-5620,Q14-1012,0,0.0572906,"Missing"
W18-5620,P17-2035,0,0.0372832,"Missing"
W19-2504,W15-0705,1,0.785975,"tant features. 3 Corpus Our corpus consists of plays published in the late 19th and early 20th centuries by George Bernard Shaw, Oscar Wilde, Cale Young Rice, Sydney Grundy, Somerset Maugham, Arthur Wing Pinero, and Hermann Sudermann (whose plays are translated from German) — giving a total of 63 plays. We would ideally have examined character dialogue in novels, Bakhtin’s preferred genre, but the problem of sufficiently reliable quote attribution for novels remains unsolved. However, in plays, each utterance is explicitly labeled with the name of the character who speaks it. We use GutenTag (Brooke et al., 2015) to extract all plays from the specified authors, restricting the year of publication to 1880–1920 to roughly capture the literary period from which Bakhtin developed his theory of dialogism. 4 Methodology Our primary method of measuring the distinguishability of character voices is classification. Our task is to build a classifier able to correctly discriminate between the speech of different characters. We perform experiments using several feature sets, in order to capture stylistic aspects that are syntactic as well as lexical. These include surface, syntactic, and generative topic-modeling"
W19-2504,N13-1078,1,0.810731,"guistics between precision and recall. We experiment with both support vector machine (SVM) and logistic regression classifiers. In addition, we experiment with vector representations of words as features. We use distributed word vectors trained on the Wikipedia corpus using the word2vec algorithm (Mikolov et al., 2013). Each dialogue is represented as a weighted average of the individual word vectors, where the weights are TF-IDF weights, or obtained from the SAGE algorithm. We also look at representations obtained from lexicons that score words across a discrete set of stylistic dimensions. Brooke and Hirst (2013) pick three dimensions to rate words along, the opposing polarities of which give us six styles: colloquial vs. literary, concrete vs. abstract, and subjective vs. objective. We also use the NRC Emotion Intensity Lexicon (EmoLex) (Mohammad, 2018b) and the NRC Valence, Arousal, and Dominance Lexicon (VAD Lexicon) (Mohammad, 2018a). The former provides real-valued intensity scores for four basic emotions — anger, fear, sadness, and joy, and the latter for the three primary dimensions of word meaning — valence, arousal, and dominance. The scores along each dimension are normalized to give us a se"
W19-2504,P18-1017,0,0.0175702,"ikipedia corpus using the word2vec algorithm (Mikolov et al., 2013). Each dialogue is represented as a weighted average of the individual word vectors, where the weights are TF-IDF weights, or obtained from the SAGE algorithm. We also look at representations obtained from lexicons that score words across a discrete set of stylistic dimensions. Brooke and Hirst (2013) pick three dimensions to rate words along, the opposing polarities of which give us six styles: colloquial vs. literary, concrete vs. abstract, and subjective vs. objective. We also use the NRC Emotion Intensity Lexicon (EmoLex) (Mohammad, 2018b) and the NRC Valence, Arousal, and Dominance Lexicon (VAD Lexicon) (Mohammad, 2018a). The former provides real-valued intensity scores for four basic emotions — anger, fear, sadness, and joy, and the latter for the three primary dimensions of word meaning — valence, arousal, and dominance. The scores along each dimension are normalized to give us a set of values ranging from 0 to 1. Principal component analysis (PCA) of these vectors gives us an insight into which authors are the most successful at creating characters whose style is highly mutually distinguishable. We repeat these experiment"
W19-2504,E17-2106,0,0.0268149,"that are sensitive to rare and infrequent words, and whose results allow us to distinguish between stylistic and topical phenomena. Recently, machine learning methods have been applied in computational stylometry for authorship attribution tasks, and also in the context of style transfer for texts. Bagnall (2015) uses a recurrent neural network (RNN) based model for the author identification task. Since neural architectures massively overfit the training set unless used with large datasets, the authors propose a shared recurrrent layer, with only the final softmax layer being author-specific. Shrestha et al. (2017) use convolutional neural networks (CNNs) over character n-grams for authorship attribution, which proves to be more interpretable than the former in identifying important features. 3 Corpus Our corpus consists of plays published in the late 19th and early 20th centuries by George Bernard Shaw, Oscar Wilde, Cale Young Rice, Sydney Grundy, Somerset Maugham, Arthur Wing Pinero, and Hermann Sudermann (whose plays are translated from German) — giving a total of 63 plays. We would ideally have examined character dialogue in novels, Bakhtin’s preferred genre, but the problem of sufficiently reliable"
W19-5025,D14-1162,0,0.0827649,"tions to the classification model to see whether it will improve the results. We also compare this model to the wordonly model. 3.3 Character–based model For the character-based model we use publicly available pre-trained character embeddings3 de1 http://ice-corpora.net/ice/avail.htm 2 http://www.medhelp.org 3 https://github.com/minimaxir/ char-embeddings 235 Figure 1: Embedding concatenation model architecture. d1 is the dimensionality of the word embedding (100), and d2 is the dimensionality of the character embedding (24). Figure 2: Model combination architecture. rived from GloVe vectors (Pennington et al., 2014) trained on Common Crawl. The dimensionality of the character embeddings is reduced from 300 to 24 with principal component analysis (PCA). We also tried learning the embeddings directly as a first layer in the model, but the model was unable to learn useful embeddings, likely because our training set is too small. The character-based classification model is also a CNN, with a maximum of 1000 characters for each narrative. We also remove punctuation for the character-based model. 3.4 for the diagram of the model architecture4 . This model allows us to combine the full information from both the"
W19-5025,N18-1202,0,0.105827,"Missing"
W19-5025,P13-2121,0,0.0546227,"Missing"
W19-5025,W18-2302,1,0.786588,"anada Toronto, Ontario, Canada sjeblee@cs.toronto.edu gh@cs.toronto.edu zhaodong.yan@mail.utoronto.ca Abstract processing problems, they allow for more detail and explanation than the structured data alone. Only a few methods have used the full text of the narrative for CoD classification. Danso et al. (2013) used term frequency and TF-IDF (term frequency–inverse document frequency) features to classify CoD from VA narratives of neonatal deaths. The Tariff method (Serina et al., 2015) uses a small set of word occurrence features from the narrative, but both of these methods ignore word order. Jeblee et al. (2018) used VA narrative text to jointly predict CoD and a list of keywords for each record using a neural network model with word embeddings. In our work, we therefore focus on the narrative text. However, the models that have been developed to date for VA classification using the narrative, including SVMs (Danso et al., 2013) and neural networks (Jeblee et al., 2018), have used only word-level information. However, recent research has shown that character-level information can improve text classification models, especially in cases where there are many spelling errors and variations, which is the"
W93-0205,J93-3002,1,0.84749,"2 A multi-level grammar of rhetoric ()ur g r a m m a r of rhetoric allows us to recognize, at the top level, how the overall structure of a text works to achieve a certain communicative goal, and, at the lower level, how the individual pieces of text fit, together to produce subtle stylistic effects. Thus, through multiple levels of abstraction, we tie together rhetorical forms characteristic of high-level intentions and syntactic relationships associated with stylistically significant effects. Thus, we have four levels, the pr, gm~ttir, the rht&apos;loric~d, the stylistic, and the syntttetic. In DiMarco and Hirst (1993), DiMarco et al (1992), Green (1992), and tloyt (forthcoming), we describe the construction of a syntactic stylistic g r a m m a r that relates stylistic goals to abstract stylistic properties, and then relates these abstract properties to low-level syntax. The foundations of the g r a m m a r draw on the work of Halliday (1985) and Halliday and Hasan (1976); we consider the primitive stylistic effects of a sentence to be correlated with its underlying cohesive and hierarchical syntactic structure. We assign each type of sentence component a primitive-element cl;L~sitication on the basis of th"
W93-0205,J92-4007,0,0.0146406,"cts of rhetoric, but to lexical and semantic aspects as well, and have begun to apply our approach to studying ]tow lexical choices realize particular intentional goals (DiMarco, Hirst, and Sl,c(lc 1993). 17 Eventually, we see lexis, syntax, and semantics being represented by separate primitive-level r(q)resentations that act together to determine the realization of communicative goals at the stylistic, rhetorical, and pragmatic levels; our formalism is therefore both stratified and branching. Thus, the ways in which intentional relations interact with ideational, or informational, relations (Moore and Pollack 1992) can be accounted for nicely by our model of rhetoric, which integrates the ell~cl,s of lexis, syntax, and semantics on rhetorical strncture within a single cohesive framework. 5 Conclusion &apos;lb construct a fidl computational theory of rhetoric, we will need to first develop complete formalizations of the lexical and semantic aspects of style, and then integrate these representations with our current syntactic grammar. The syntactic theory formalizes some significant aspects of style and rhetoric. While it has limitations, it does explain several aspects of the writing process and how intention"
W98-1413,P92-1016,0,0.0783094,"Missing"
W98-1413,W94-0307,0,0.0672995,"Missing"
W98-1413,P89-1025,0,0.0797203,"Missing"
W98-1413,A92-1009,0,0.0213589,"ave largely ignored by instead incorporating the knowledge •of the task at their top level, i.e., the basic content of the• instructions is assumed to already exist and does not need to be planned for. In our approach, all the knowledge necessary for the planning stage of a system i s contained (possibly in a more abstract form) in the knowledge of the artifact together with the world knowledge. The kinds of knowledge that Should .be sufficient for this planning are device knowledge •(topological, kinematic, electrical, thermodynamic, and electronic) and world knowledge. T h e IDAS project of Reiter et al. (1992; 1995) served as a key motivation for this work. One of the primary goals of the IDAS project was to automatically generate technical documentation IAddresscorrespondenceto the second author. E-maili gh@cs.toront0.edu ..118 I !1 I I .I i m ! I i I I I I I I I I I I I I I I | from a domain knowledge base containing design information (such as that produced by an advanced computer-aided design tool) using NLG techniques. IDAS turned outto be successful in demonstrating the usefulness, from a cost and benefits perspective, of applying NLG technology to partially automate the generation of docume"
Y11-1028,W11-2508,0,0.476424,"I gave up. [text = ‘text message’] Furthermore, new word senses are not necessarily frequent. For example, the above usages are taken from the enTenTen corpus,1 a very large corpus containing a wide variety of text types, but the corresponding senses of these words appear to be rare in this corpus. The identification of new word senses is an important task in lexicography, and is necessary to keep dictionaries up-to-date. But lexical semantic change has only recently been studied from a computational perspective, and only to a limited extent (e.g., Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Furthermore, despite the low frequency of many novel word senses, no work to date has specifically considered the identification of words with novel infrequent senses. In contrast, in this paper we focus specifically on this issue The manual identification of novel senses is becoming increasingly difficult nowadays due to the vast quantities of text being produced (e.g., through online social media) that must be searched. ⋆ 1 We thank Afsaneh Fazly, Diana McCarthy, Suzanne Stevenson, and the members of the University of Melbourne Language Technology Group for their feedback on earlier versio"
Y11-1028,P98-2127,0,0.815347,"ither nouns or verbs—that are hypothesized by the method to have different senses in one of the corpora compared to the other. We consider a statistical model similar to one that Peirsman et al. (2010) used for automatically identifying lectal markers. This approach assumes that usages of different senses of a word will occur in different contexts, and that the aggregated contexts of a word in two corpora will differ if the senses of that word differ in those corpora. The model is based on a distributional representation of meaning that draws on work on automatically clustering similar words (Lin, 1998) that has been incorporated into tools used by lexicographers to identify word senses (Kilgarriff and Tugwell, 2002). Specifically, this method measures the similarity of two lexico-syntactic representations of the aggregated contexts of a target word; these two representations would typically come from different corpora representing, for example, different time periods. The lexico-syntactic representations capture the association of a target word with dependency triples, and the similarity between two target word representations is determined with a number of metrics. We propose some variatio"
Y11-1028,J07-4005,0,0.0604068,"vely—and exploit properties of these phenomena in their methods for identifying them. Gulordava and Baroni (2011) consider the identification of diachronic changes in meaning from an n-gram database, but in contrast to Sagi et al. and Cook and Stevenson, do not focus on specific types of semantic change. Others have studied differences in meaning between dialects and domains, instead of over time. Peirsman et al. (2010) consider the identification of lectal markers—words typical of one dialect versus another, either because of their marked frequency or sense—in Belgian and Netherlandic Dutch. McCarthy et al. (2007) consider the identification of predominant word senses in corpora, focusing on differences between domains. This method can be applied to not only identify the words that differ in predominant sense in two corpora, but also the specific predominant senses of those words. Nevertheless, none of these studies has specifically considered the identification of words with novel infrequent senses, the focus of this study. Given the challenges of evaluating methods for identifying semantic change—namely a lack of suitable resources—we propose the use of synthetic examples of semantic change for evalu"
Y11-1028,W04-0807,0,0.0913346,"Missing"
Y11-1028,N03-2023,0,0.0247257,"fication of words with novel infrequent senses, the focus of this study. Given the challenges of evaluating methods for identifying semantic change—namely a lack of suitable resources—we propose the use of synthetic examples of semantic change for evaluation. Gaustad (2001) showed that evaluations using pseudowords (artificially-ambiguous words) can over-estimate the accuracy of a word sense disambiguation system on real data. Gaustad suggests this is because word senses are typically related (i.e., words are polysemous) whereas pseudowords are usually created from words with distinct senses. Nakov and Hearst (2003) and Otrusina and Smrz (2010) propose methods for constructing more-realistic pseudowords by taking into account information about lexical categories, and lexical or distributional information, respectively. In this work we propose a new use for pseudowords—evaluating methods for identifying semantic change—and attempt to address concerns about the use of pseudowords, such as those raised by Gaustad, by creating our pseudowords (discussed in Sections 4 and 5) from real word senses and words with related senses. 266 3 Model The input to our method is two corpora which represent different text v"
Y11-1028,otrusina-smrz-2010-new,0,0.0302482,"infrequent senses, the focus of this study. Given the challenges of evaluating methods for identifying semantic change—namely a lack of suitable resources—we propose the use of synthetic examples of semantic change for evaluation. Gaustad (2001) showed that evaluations using pseudowords (artificially-ambiguous words) can over-estimate the accuracy of a word sense disambiguation system on real data. Gaustad suggests this is because word senses are typically related (i.e., words are polysemous) whereas pseudowords are usually created from words with distinct senses. Nakov and Hearst (2003) and Otrusina and Smrz (2010) propose methods for constructing more-realistic pseudowords by taking into account information about lexical categories, and lexical or distributional information, respectively. In this work we propose a new use for pseudowords—evaluating methods for identifying semantic change—and attempt to address concerns about the use of pseudowords, such as those raised by Gaustad, by creating our pseudowords (discussed in Sections 4 and 5) from real word senses and words with related senses. 266 3 Model The input to our method is two corpora which represent different text varieties, e.g., different tim"
Y11-1028,W09-0214,0,0.434278,"ignored the first few texts and phone calls, I gave up. [text = ‘text message’] Furthermore, new word senses are not necessarily frequent. For example, the above usages are taken from the enTenTen corpus,1 a very large corpus containing a wide variety of text types, but the corresponding senses of these words appear to be rare in this corpus. The identification of new word senses is an important task in lexicography, and is necessary to keep dictionaries up-to-date. But lexical semantic change has only recently been studied from a computational perspective, and only to a limited extent (e.g., Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Furthermore, despite the low frequency of many novel word senses, no work to date has specifically considered the identification of words with novel infrequent senses. In contrast, in this paper we focus specifically on this issue The manual identification of novel senses is becoming increasingly difficult nowadays due to the vast quantities of text being produced (e.g., through online social media) that must be searched. ⋆ 1 We thank Afsaneh Fazly, Diana McCarthy, Suzanne Stevenson, and the members of the University of Melbourne Languag"
Y11-1028,cook-stevenson-2010-automatically,1,\N,Missing
Y11-1028,J98-1004,0,\N,Missing
Y11-1028,C98-2122,0,\N,Missing
