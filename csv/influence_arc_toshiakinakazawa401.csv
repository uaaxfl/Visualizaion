2005.iwslt-1.27,W99-0604,0,0.0401063,"common and something different. The important common feature is to use bilingual corpus, or translation examples, for the translation of new inputs. Both methods exploit translation knowledge implicitly embedded in translation examples, and make MT system maintenance and improvement much easier compared with Rule-Based Machine Translation. The difference is that SMT supposes bilingual corpus is the only available resource (but not a bilingual lexicon and parsers); EBMT does not consider such a constraint. SMT basically combines words or phrases (relatively small pieces) with high probability [2]; EBMT tries to use larger translation examples. When EBMT tries to use larger examples, it had better handle examples which are discontinuous as a word-string, but continuous structurally. Accordingly, though it is not inevitable, EBMT naturally seeks syntactic information. The difference in the problem setting is important. SMT is a natural approach when linguistic resources such as parsers and a bilingual lexicon are not available. On the other hand, in case of such linguistic resources are available, it is also natural to see how accurate MT can be achieved using all the available resource"
2005.iwslt-1.27,J94-4001,1,0.547121,"Missing"
2005.iwslt-1.27,A00-2018,0,0.0468239,"Missing"
2005.iwslt-1.27,J03-1002,0,0.00380044,"other remaining nodes are merged into correspondences of their parent (or ancestor) nodes. In the case of Figure 1, “あの (that)” is merged into the correspondence “ 車 (car) ↔ the car”, since it is within an NP. Then, “突然 (suddenly)”, “at me” and “from the side” are merged into their parent correspondence, “ 飛び出して来 たのです (rush out) ↔ came”. We call the correspondences constructed so far as basic correspondences. 2.5. Comparison with EM based Alignment Here, let us compare our alignment method with an EM based alignment. We tested an EM based tool, giza++ for the alignment of 20,000 training data [6]. We found many inappropriate word alignments in the giza++ results, and concluded that this size of training data might be too small for EM based alignment. On the other hand, our method using a 0.9M-entry bilingual dictionary and a transliteration module could find correspondences quite accurately. For the given training set, we could conclude that our proposed method is superior to the EM based method. However, the correspondence statistics in the whole training data must be an important information, and it is our future target to use a flat bilingual dictionary and the statistical informat"
2005.iwslt-1.27,2005.mtsummit-papers.29,1,0.795144,"Missing"
2005.iwslt-1.27,P93-1004,0,0.0606899,"that parsing might cause sideeffects and lower translation performance. As we mentioned above, parsing errors are not a principal cause of translation errors, but these are not a few. One of the possible countermeasures is to reconsider the learning process of an English parser. The English parser used here is learned from Penn Treebank, and seems to be vulnerable to conversational sentences in travel domain. Furthermore, it is quite possible to improve parsing accuracies of both languages complementarily by taking advantage of the difference of syntactic ambiguities between the two languages [9]. This approach may not substantially improve the parsing accuracy of the travel domain sentences, because of their short length, but is promising for translating longer general sentences. 7. Conclusion As we stated in Introduction, we not only aim at the development of machine translation through some evaluation measure, but also tackle this task from the comprehensive viewpoint including the development of structural NLP. The examination of translation errors revealed the problems, such as problems in parsing and inflexible matching of a Japanese input and Japanese translation examples. Reso"
2006.iwslt-evaluation.9,P93-1004,0,0.0526775,"parsing might cause sideeffects and lower translation performance. As we mentioned above, parsing errors are not a principal cause of translation errors, but these are not a few. One of the possible countermeasures is to reconsider the learning process of an English parser. The English parser used here is learned from the penn Treebank, and seems to be vulnerable to conversational sentences in travel domain. Furthermore, it is quite possible to improve parsing accuracies of both languages complementarily by taking advantage of the difference of syntactic ambiguities between the two languages [8]. This approach may not substantially improve the parsing accuracy of the travel domain sentences, because of their short length, but is promising for translating longer general sentences. Other main points are as follows: ࡎ࠹࡞1. 0 ߦ &lt;hotel> ߦ 0.99 ৻⇟ (best) 1. 0 &lt;most>0.99 &lt;nearest> ㄭ (near ) 1. 0 0.99 㚞 (station) 1. 0 ߪ ߤߎ (where) 1. 0 ߢߔ ߆ Figure 3: Example of SYNGRAPH. Table 1: JP to EN Evaluation results. Dev 1 Dev 2 Dev 3 Dev 4 Dev 4 ASR Test Test ASR BLEU 0.5087 0.4881 0.4468 0.1921 0.1590 0.1655 0.1418 NIST 9.6803 9.4918 9.1883 5.7880 5.0107 5.4325 4.8804 • Punctuation marks are remove"
2006.iwslt-evaluation.9,E06-1031,0,0.0225558,"des are basic nodes and the other nodes are SYN nodes 2 . Then, if the expression conjoining two or more nodes corresponds to one synonymous group, a SYN node will be added there. For example, in Figure 3,  nearest  is such a SYN node. Furthermore, if one SYN node has a hyper synonymous group in the synonymy database, the SYN node with the hyper SYNID is also added. In this SYNGRAPH data structure, each node has a score, NS (Node Score), which reflects how much the expression of the node is shifted from the original expression. • Automatic evaluation methods are a little advantageous to SMT [9],[10]. • The soundness of dictionaries heavily affects on the accuracy of alignment. • The extension rules of remaining nodes should be revised. • The constraint of selecting translation examples should be more robust. It is currently impossible to use ’almost equal’ exanples to the input sentence, such as those that differ perhaps only with respect to whether or not it contains a negation adverb such as ’not’. 5. Results and Discussion 6. Related Work Our Japanese-English translation system tried two tasks: manual manuscript translation and ASR output translation (for ASR output we just trans"
2006.iwslt-evaluation.9,E06-1032,0,0.0178145,"are basic nodes and the other nodes are SYN nodes 2 . Then, if the expression conjoining two or more nodes corresponds to one synonymous group, a SYN node will be added there. For example, in Figure 3,  nearest  is such a SYN node. Furthermore, if one SYN node has a hyper synonymous group in the synonymy database, the SYN node with the hyper SYNID is also added. In this SYNGRAPH data structure, each node has a score, NS (Node Score), which reflects how much the expression of the node is shifted from the original expression. • Automatic evaluation methods are a little advantageous to SMT [9],[10]. • The soundness of dictionaries heavily affects on the accuracy of alignment. • The extension rules of remaining nodes should be revised. • The constraint of selecting translation examples should be more robust. It is currently impossible to use ’almost equal’ exanples to the input sentence, such as those that differ perhaps only with respect to whether or not it contains a negation adverb such as ’not’. 5. Results and Discussion 6. Related Work Our Japanese-English translation system tried two tasks: manual manuscript translation and ASR output translation (for ASR output we just translated"
2006.iwslt-evaluation.9,W01-1402,0,0.157807,"Missing"
2006.iwslt-evaluation.9,W01-1406,0,0.0234825,"Missing"
2006.iwslt-evaluation.9,W99-0604,0,0.0355217,"tant common feature between SMT and EBMT is to use a bilingual corpus, or translation examples, for the translation of new inputs. Both methods exploit translation knowledge implicitly embedded in translation examples, and make MT system maintenance and improvement much easier compared with Rule-Based Machine Translation. On the other hand, EBMT is different from SMT in that SMT hesitates to exploit rich linguistic resources such as a bilingual lexicon and parsers; EBMT does not consider such a constraint. SMT basically combines words or phrases (relatively small pieces) with high probability [2]; EBMT tries to use larger translation examples. When EBMT tries to use larger examples, it can better handle examples which are discontinuous as a word-string, but continuous structurally. Accordingly, though it is not inevitable, EBMT can quite naturally handle syntactic information. Besides that, the difference in the problem setting between EBMT and SMT is also important. SMT is a natural approach when linguistic resources such as parsers and a bilingual lexicon are not available. On the other hand, in case that such linguistic resources are available, it is also natural to see how accurat"
2006.iwslt-evaluation.9,J94-4001,1,0.562861,"ching, we need to recognize the synonymous relations among this expressive divergence. However, the combination of synonymous expressions will cause combinatorial explosion, which makes both pre-unfolding and dynamic search are infeasible. To handle this problem, we introduce SYNGRAPH data structure which packs all synonymous expressions and can generate all the possible paraphrase sentences. Figure 3 shows an example SYNGRAPH which can generate the above paraphrases. The basis of SYNGRAPH is the dependency structure of the original sentence (so, in this paper we always employ a robust parser [3]). In the dependency structure, each 2. Extracted synonymous expressions are effectively handled by SYNGRAPH data structure, which can pack expressive divergence. A thesaurus is a knowledge source to provide synonym and hypernym-hyponym relations. However, existing thesauri are not appropriate for flexible matching. One reason is that the number of words assigned to one unit is often too large, and it is difficult to distinguish synonyms in a narrow sense from similar words. Such distinction is important for 68 there were variety of problems, such as parsing errors of both languages, excess an"
2006.iwslt-evaluation.9,A00-2018,0,0.061125,"Missing"
2006.iwslt-evaluation.9,P06-1043,0,0.0425398,"Missing"
2006.iwslt-evaluation.9,2005.mtsummit-papers.29,1,0.740268,"t) the intersection” is attached on “家に ”, which means “house” is replaced with “the intersection”. On the other hand, a parent bond tells that the translation 3.4. Handling of Numerals Numerals in Japanese are translated into English in several ways. • cardinal : 124 → one hundred twenty four • ordinal (e.g., day) : 2 日 → second • two-figure (e.g., room number, year) : 124 → one twenty four • one-figure (e.g., flight number, phone number) : 124 便 → one two four • non-numeral (e.g., month) : 8 月 → August 1 We proposed a method of selecting translation examples based on translation probability [6]. Though we used size- and similarity-based criteria for IWSLT06 because of time constraints, we are planning to use probabilitybased criteria from now on. At the time of parallel sentence alignment, it is checked in which type Japanese numerals are translated. 67 Translation examples of non-numeral type are used only if the numerals match exactly (“8 月 → August” cannot be used to translate “7 月”). However, translation examples of the other types can be used by generalizing numerals, and the input numeral is transformed according to the type. For example, “2 日 → second” can be used to translat"
2006.iwslt-evaluation.9,2001.mtsummit-ebmt.4,0,\N,Missing
2006.iwslt-evaluation.9,P06-1085,0,\N,Missing
2007.mtsummit-papers.45,P05-1022,0,0.01835,"achieved by the following steps, using a Japanese parser, an English parser, and a bilingual dictionary. 2.1 Dependency Analysis of Sentences Japanese sentences are converted into dependency structures using the morphological analyzer, JUMAN (Kurohashi et al., 1994), and the dependency analyzer, KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by rules defining head words for phrases. In the same way as Japanese, each node in this dependency tree consists of a content word and related function words. Figure 1 shows an example of tree structure. The root of a tree is placed at the extreme left and phrases are placed from top to bottom. ᣣᧄ 䈪 2.2 Detection of Word/Phrase Correspondence Candidates 新宿 → Shinjuku ↔ Shinjuku (similarity:1.0) ローズワイン → rosuwain ↔ rose wine (similarity:0.78) In Figure 1, the correspondence candidates “ 日本 (Japan) ↔ Japan”, “請求 (claim) ↔ claim”, “申し立て"
2007.mtsummit-papers.45,P03-1011,0,0.0315403,"ost meaningful elements of a sentence. With this structure, they proposed a “best-first” alignment method. This method starts from the nodes with the tightest lexical correspondence and then goes to close nodes from the first node. (Groves et al., 2004) used parsed tree structure of an original sentence, and then aligned the trees with some heuristic rules that constrain the order of alignment. Although these structural methods utilize profound knowledge of NLP and achieve high accuracy, the manner of alignment is still heuristic, which is often not in general purpose. To resolve this issue, (Gildea, 2003) proposed a probabilistic tree-based alignment between Korean and English. They use some cloning operations to calculate the probability, so they make the structure more complicated. Moreover, it is not apparent that the same operations are effective and suitable for different language pairs. In this paper, we propose an alignment method applying dependency type distance and distance score function into the structural alignment. Our motivation is to measure the alignment consistency based on distance, which is not only keeping the simple sentence structure but also language independent. Experi"
2007.mtsummit-papers.45,J94-4001,1,0.58696,"and model 2. We performed some experiments to evaluate our proposal, and it is reported in Section 4. At last, we give a short conclusion and introduce our future work. 2 Procedure of Structural Phrase Alignment Our machine translation system works mainly for JapaneseEnglish, and the alignment is achieved by the following steps, using a Japanese parser, an English parser, and a bilingual dictionary. 2.1 Dependency Analysis of Sentences Japanese sentences are converted into dependency structures using the morphological analyzer, JUMAN (Kurohashi et al., 1994), and the dependency analyzer, KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by rules defining head words for phrases. In the same way as Japanese, each node in this dependency tree consists of a content word and related function words. Figure 1 shows an example of tree structure. The root of a tree is placed at the ex"
2007.mtsummit-papers.45,W01-1406,0,0.0144398,"word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big problem on its way. For this reason, it is not easy to achieve high performance for the language pair whose linguistic structure is quite different from each other. While, by using heuristic rules in alignment procedure, structural methods can easily use NLP resources, such as a morphological analyzer and a syntactic analyzer, to grasp characteristics of language pairs with large difference in linguistic structure. (Menezes and Richardson, 2001) proposed a kind of tree structure called “Logical Form”, which is a disordered graph representing the relations among the most meaningful elements of a sentence. With this structure, they proposed a “best-first” alignment method. This method starts from the nodes with the tightest lexical correspondence and then goes to close nodes from the first node. (Groves et al., 2004) used parsed tree structure of an original sentence, and then aligned the trees with some heuristic rules that constrain the order of alignment. Although these structural methods utilize profound knowledge of NLP and achiev"
2007.mtsummit-papers.45,J03-1002,0,0.0109488,"simple statistical method (GIZA++), and 3.0 points over a baseline system. We also conducted a translation experiment and achieved a BLEU score improvement of 0.4 points over a baseline system. 1 Introduction In machine translation task, how to align the training parallel corpus with high accuracy is a big problem, and thus a number of studies have been done. The alignment methods can be categorized into two groups: one is probabilistic method and the other is heuristic method with structural information. Probabilistic methods are mainly used in Statistical Machine Translation (SMT) systems (Och and Ney, 2003a). The main issue is how to decompose the alignment probabilities P r(A|S, T) reasonably to make good use of some approximations. The simplest statistical method is based on word level alignment, in which the IBM Model (Brown et al., 1993) is mostly used as the baseline method. Recently, more sophisticated methods have been proposed by (Watanabe et al., 2002) and (Zhang and Vogel, 2005), which handle not only a word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big probl"
2007.mtsummit-papers.45,W04-2208,0,0.149941,". At the same time, the conflicting correspondence “ 申し 立 て (allegation) ↔ claim” is rejected. After that, the correspondence “ 請求 (claim) ↔ claim” is unambiguous, so it is adopted. 3.4 Proposed Model Here we would like to refine the heuristic definition of distance and distance-score function. We proposed two models. 3.3 Baseline Method Proposed Model 1 First, we refine distance-score function in order to reject unambiguous and incorrect correspondences. Here, the definition of dS (ai , aj ) and dT (ai , aj ) is the same as the baseline model. Using the gold standard alignment data from NICT(Uchimoto et al., 2004), which includes about 40,000 sentence pairs, we learned the frequency distribution of distance pair. Figure 4 shows the result of automatic learning form gold standard data. Based on the observation of gold standard data, we design f (dS , dT ) as follows: Criteria 1: f (dS , dT ) is positive if both dS and dT are small, which means the relation between the two correspondences is appropriate; Criteria 2: f (dS , dT ) is 0 if both dS and dT are large, for the relation is not so important if they are far from each other; Criteria 3: f (dS , dT ) is negative is dS is large but dT is small, or dT"
2007.mtsummit-papers.45,P03-1010,0,0.0330636,"s difference is that Japanese sentences consist of SOV word order, but English word order is SVO. For such language pair as Japanese and English, deeper sentence analysis using NLP resources is necessary, like our method. Sample alignments are shown in Figure 12 and Figure 13. Wrongly aligned parts in the baseline method are modified in the model 2. 4.2 Translation Experiment We also conducted translation experiment. For this purpose, we utilized around 218K parallel sentences for trainFigure 12: Sample Alignment 1. ing, and 500 sentences for testing. All the sentences are on newspaper domain(Utiyama and Isahara, 2003). The translation results are summarized in Table 2 and Table 3. Results were evaluated by n-gram precision based metrics, BLEU and NIST, with only one reference. We show 3, 4, 5-gram evaluation results in the tables. From the result, it is able to be said that the translation quality is improved by our proposed method. The improvement of alignment accuracy leads to the improvement of the quality of translation examples used in translation step. Sample translations are shown in Table 4. The numerals following the method name represent the 4-gram BLEU score of the output. 5 Conclusion We have p"
2007.mtsummit-papers.45,2002.tmi-papers.20,0,0.0171049,"done. The alignment methods can be categorized into two groups: one is probabilistic method and the other is heuristic method with structural information. Probabilistic methods are mainly used in Statistical Machine Translation (SMT) systems (Och and Ney, 2003a). The main issue is how to decompose the alignment probabilities P r(A|S, T) reasonably to make good use of some approximations. The simplest statistical method is based on word level alignment, in which the IBM Model (Brown et al., 1993) is mostly used as the baseline method. Recently, more sophisticated methods have been proposed by (Watanabe et al., 2002) and (Zhang and Vogel, 2005), which handle not only a word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big problem on its way. For this reason, it is not easy to achieve high performance for the language pair whose linguistic structure is quite different from each other. While, by using heuristic rules in alignment procedure, structural methods can easily use NLP resources, such as a morphological analyzer and a syntactic analyzer, to grasp characteristics of language p"
2007.mtsummit-papers.45,2005.eamt-1.39,0,0.0223359,"can be categorized into two groups: one is probabilistic method and the other is heuristic method with structural information. Probabilistic methods are mainly used in Statistical Machine Translation (SMT) systems (Och and Ney, 2003a). The main issue is how to decompose the alignment probabilities P r(A|S, T) reasonably to make good use of some approximations. The simplest statistical method is based on word level alignment, in which the IBM Model (Brown et al., 1993) is mostly used as the baseline method. Recently, more sophisticated methods have been proposed by (Watanabe et al., 2002) and (Zhang and Vogel, 2005), which handle not only a word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big problem on its way. For this reason, it is not easy to achieve high performance for the language pair whose linguistic structure is quite different from each other. While, by using heuristic rules in alignment procedure, structural methods can easily use NLP resources, such as a morphological analyzer and a syntactic analyzer, to grasp characteristics of language pairs with large difference i"
2008.amta-papers.15,P05-1022,0,0.011735,"2.1 Dependency Analysis of Sentences Since our model utilizes dependency tree structures, both source and target sentences are parsed at first. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by handmade rules defining head words for phrases. As is the case with Japanese, each node in this dependency tree consists of a content word and related function words1 . We define function words as the words with tags of “IN”(preposition or subordinating conjunction), “TO”, “MD”(modal), “CC”(coordinating conjunction) by nlparser. 1 There would be some special cases that a phrase has no content word. See a phrase “and” in figure 1. Also, there would be a phrase which has more than one content words. 164 In IBM models (Brown et al., 19"
2008.amta-papers.15,P03-1012,0,0.162161,"about alignment quality (Fraser and Marcu, 2007), if the gold standard data only contains sure alignments, precision is much important than recall. Modified F-measure calculated by the equation 10 shows higher correlation to translation quality if α is set to be more than 0.5 (means emphasizing on precision). Modified F-measure = 1 α Precision + (1−α) Recall (10) However, our results disagree with this argument. There seems no relation between precision and translation quality, indeed, F-measure has good correlation with BLEU score. 7 Related Work Our proposed model is similar to the work of Cherry and Lin (2003). They use dependency tree structure for source side and construct a probabilistic model. 169 Figure 5: Result of phrase-base alignment. The differences between their model and ours are following. • Unit of alignment: Their model aligns words to words. Ours aligns syntactic phrases to phrases. • Parser: They use a parser for source side only, and reproduce the target side dependency structure introducing a cohesion constraint. We use parsers for both source and target side, and do not require any constraints. • Alignment constraint: They can only make one-to-one links. We can make even many-to"
2008.amta-papers.15,P05-1033,0,0.0521376,"and achieve reasonably high quality of alignment compared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing"
2008.amta-papers.15,W06-1628,0,0.0878667,"Missing"
2008.amta-papers.15,J07-3002,0,0.0191839,"ulin 䕔  جand 䕔 decreases 䕔 ؒ  NEFA 䕔  ؤin concentra!on 䕔  ؤin the blood 䕔 䕔 䕔 䕔 䕔 䕔䕔 ج ؒؤ ؒ ؤ ج ؒؤ ؒ ؤ ؒ ؤ ؤ ؤ Figure 4: Result of word-base alignment (grow-diagfinal-and). is more prominence in case only small amount of parallel corpus can be used. What is more, most of these works are done on similar language pairs, such as English v.s. Chinese, French. We are now investigating the contribution of alignment quality improvement to translation quality in JapaneseEnglish, language pair whose structure is very different. According to the study about alignment quality (Fraser and Marcu, 2007), if the gold standard data only contains sure alignments, precision is much important than recall. Modified F-measure calculated by the equation 10 shows higher correlation to translation quality if α is set to be more than 0.5 (means emphasizing on precision). Modified F-measure = 1 α Precision + (1−α) Recall (10) However, our results disagree with this argument. There seems no relation between precision and translation quality, indeed, F-measure has good correlation with BLEU score. 7 Related Work Our proposed model is similar to the work of Cherry and Lin (2003). They use dependency tree s"
2008.amta-papers.15,P06-1121,0,0.0296798,"t experiments on a JapaneseEnglish corpus, and achieve reasonably high quality of alignment compared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knigh"
2008.amta-papers.15,P03-1011,0,0.174341,"ct phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Our method proposed in this paper does not require any operations for controlling tree structures, just align phrase-to-phrase on dependency structure. Though our model is more simple than well-known IBM Model3 or greater, our model can achieve high accuracy of alignment an"
2008.amta-papers.15,N03-1017,0,0.0614156,"Missing"
2008.amta-papers.15,P07-2045,0,0.00534894,") for Japanese sentences and created alignments using freely available word alignment tool GIZA++ (Och and Ney, 2003). We conducted word alignment bidirectionally with its default parameters and merged them using seven types of symmetrization heuristics (Koehn et al., 2003) shown in table 2. Training are run on original forms of words for both proposed model and GIZA++. For translation evaluation, we use 500 paper abstract sentences which are parts of JST corpus. Note that test sentences are not included in training corpus. As a decoder, we used state-of-the-art phrasebased SMT toolkit Moses (Koehn et al., 2007) with its default options except for phrase table limit (20 → 10) and distortion limit (6 → -1 means infinite). Evaluation was done with all the punctuations being deleted and case-insensitively. The BLEU scores of each alignment methods are shown in table 2, in the last column. Actually, it is hard to integrate proposed alignment results into Moses decoder because our model is based on “linguistic phrase”. If we align all words to all words in a corresponding two phrases, Moses would fail to translate a content word with different function words from the learned phrase pair. To avoid this pro"
2008.amta-papers.15,J94-4001,1,0.297584,"ponds to a linguistic phrase. Underlined words are handled as function words, others are content words. Our model uses the linguistic phrase as an unit of alignment rather than a word. The root of a tree is placed at the extreme left and phrases are placed from top to bottom. 2.2 Tree-based Model 2.1 Dependency Analysis of Sentences Since our model utilizes dependency tree structures, both source and target sentences are parsed at first. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by handmade rules defining head words for phrases. As is the case with Japanese, each node in this dependency tree consists of a content word and related function words1 . We define function words as the words with tags of “IN”(preposition or"
2008.amta-papers.15,2006.amta-papers.11,0,0.0183626,"e BLEU score, BLEU score of our proposed model is worse than that of word-base models. One reason of this is that, as mentioned above, the infelicity of integrating our alignment results into Moses decoder. Another reason is that BLEU is essentially insensitive to syntactic structure. The translation result may indeed better from the point of dependency structure. We need to try parsing base line output and the output of the realigned system and see if the parsing results improve. Some of recent studies suggest that there is less relationship between alignment quality and translation results (Lopez and Resnik, 2006; Ayan and Dorr, 2006). Even if the contribution to translation quality is small, there is no doubt that better alignment quality leads to better translation, which [8th AMTA conference, Hawaii, 21-25 October 2008] Propylene 䕔 glycol 䕔 increases 䕔 in 䕔䕔 blood 䕔 glucose 䕔 and insulin 䕔 and decreases in NEFA concentra!on in the blood 䕔  Propylene 䕔  glycol 䕔  جincreases 䕔 ؒ ؒ  blood 䕔 ؒ ؒ  glucose and 䕔 ؒ  ؤin insulin 䕔  جand 䕔 decreases 䕔 ؒ  NEFA 䕔  ؤin concentra!on 䕔  ؤin the blood 䕔 䕔 䕔 䕔 䕔 䕔䕔 ج ؒؤ ؒ ؤ ج ؒؤ ؒ ؤ ؒ ؤ ؤ ؤ Figure 4: Result of word-base a"
2008.amta-papers.15,W01-1406,0,0.0414531,"ent model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Our method proposed in this paper do"
2008.amta-papers.15,J03-1002,0,0.0157923,"and Results 2 3 http://www.jst.go.jp/ http://www.nict.go.jp/ 167 ؒؤ ؒ ؤ ؒ ؤ ؤ ج ؤ ج ؤ We conducted both alignment and translation evaluation experiments. JST 2 Japanese-English paper abstract corpus consists of 1M parallel sentences were used for model training. This corpus was constructed from 2M Japanese-English paper abstract corpus belongs to JST by NICT 3 using the method of Uchiyama and Isahara (2007). We made gold-standard alignment for 100 sentence pairs among the 1M parallel sentences by hand. The annotations are only sure (S) alignments (no possible (P ) alignments (Och and Ney, 2003)). The unit of evaluation was morpheme-base for Japanese and word-base for English. We used precision, recall, and F-measure as evaluation criterion. The evaluation results are shown in table 2. We used “3-best-grow” and “5-best-grow” 5 2  invesgaon 10 ؒ ؤof the hint 10 ؒ  ؤfor challenge 10 ؒ  ؤfor future was made 5 3 5  ؤthrough review 1 5  ؤof the history 7 3 ؒ technology and 10 ؒ جtypical 5 9  ؤof invenons 7 ؒ tweneth  ؤin the century 7 1 Figure 3: Example of growing the alignment points. [8th AMTA conference, Hawaii, 21-25 October 2008] symmetrization heuristic"
2008.amta-papers.15,P05-1034,0,0.0489519,"alignment. We conduct experiments on a JapaneseEnglish corpus, and achieve reasonably high quality of alignment compared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too ri"
2008.amta-papers.15,2007.mtsummit-papers.63,0,0.186669,"Missing"
2008.amta-papers.15,C00-2131,1,0.883089,"pared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Ou"
2008.amta-papers.15,P01-1067,0,0.135001,"successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Our method proposed in this paper does not require any operations for controlling tree structures, just align phrase-to-phrase on dependency structure. Though our model is more simple than well-known IBM Model3 or greater, our model can achieve high accura"
2008.amta-papers.15,J93-2003,0,\N,Missing
2008.amta-papers.15,2001.mtsummit-ebmt.4,0,\N,Missing
2008.amta-papers.15,P06-1002,0,\N,Missing
2011.mtsummit-papers.53,I08-1012,0,0.0164577,". We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental results are shown in Table 8. Common Chinese characters information is detected by our proposed detecting method and Kanconvit. The alignment accuracy of the alignment model we used with"
2011.mtsummit-papers.53,P07-1003,0,0.022513,"JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental results are shown in Table 8. Common Chinese characters information is detected by our proposed detecting method and Kanconvit. The alignment accuracy of the alignment model we used without incorporating the information is indicated as ”Baseline”, the alignment accuracy after adjusting the base distribution to reﬂect the information is indicated as ”BD”, and the alignment accuracy after exploiting the information directly into the 7 469 Settings http://code.google.com/p/berkeleyaligner/ grow-diag-ﬁnal-and BerkelyAligner Baselin"
2011.mtsummit-papers.53,D08-1033,0,0.0424857,"Missing"
2011.mtsummit-papers.53,I05-1059,0,0.254738,"rk Common Chinese characters information have been employed for a number of Japanese-Chinese related tasks. Tan et al. (1995) availed the occurrence of common Chinese characters as a feature of Japanese-Traditional Chinese sentence pair to ﬁnd a Meaning Kanji TC SC snow    country    love    begin hair      Table 1: Examples of Chinese characters (TC denotes Traditional Chinese and SC denotes Simpliﬁed Chinese). SC TC      , ,     , ,  ... ... Table 2: Hanzi converter version 3.0 standard conversion table. direct correspondence in automatic sentence alignment task. Goh et al. (2005) built a JapaneseSimpliﬁed Chinese dictionary partly using direct conversion of Japanese into Chinese for the Japanese words that all the characters in the word are made up of Kanji only, namely Kanji words. They did the conversion using a Chinese encoding converter1 which can convert Traditional Chinese into Simpliﬁed Chinese. It works because most Kanji are identical to Traditional Chinese. And for the Kanji with visual variations that cannot be automatically converted using the converter, they manually converted them by hand. In the context of machine translation, Kondrak et al. (2003) inco"
2011.mtsummit-papers.53,N06-1023,1,0.781188,"ts (Och and Ney, 2003). The unit of evaluation was word. We used precision, recall and alignment error rate (AER) as evaluation criteria. All the experiments were run on original forms of words. We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental resu"
2011.mtsummit-papers.53,N03-1017,0,0.0328458,"Missing"
2011.mtsummit-papers.53,N03-2016,0,0.187254,"nt task. Goh et al. (2005) built a JapaneseSimpliﬁed Chinese dictionary partly using direct conversion of Japanese into Chinese for the Japanese words that all the characters in the word are made up of Kanji only, namely Kanji words. They did the conversion using a Chinese encoding converter1 which can convert Traditional Chinese into Simpliﬁed Chinese. It works because most Kanji are identical to Traditional Chinese. And for the Kanji with visual variations that cannot be automatically converted using the converter, they manually converted them by hand. In the context of machine translation, Kondrak et al. (2003) incorporated cognates (words or languages which have the same origin) information in European languages into the translation models of Brown et al. (1993). They arbitrarily selected a subset from the Europarl corpus as training data and extracted a list of likely cognate word pairs from the training corpus on the basis of orthographic similarity, and appended to the corpus itself in order to reinforce the co-occurrence count between cognates. The results of experiments conducted on a variety of bitexts showed that cognate identiﬁcation can improve word alignments without modifying the statist"
2011.mtsummit-papers.53,P09-1058,0,0.0307693,"Missing"
2011.mtsummit-papers.53,W02-1018,0,0.0346189,"y of JUMAN (Kurohashi et al., 1994). Table 5 gives some examples of Kana-Kanji conversion results. We only do Kana-Kanji conversion for content words because it is proved that do Kana-Kanji conversion for function words may lead to wrong alignment in the alignment experiments we did. 4 Alignment Model We used an alignment model proposed by Nakazawa and Kurohashi (2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazawa and Kurohashi, 2009) and deﬁnes the reorderings on the word dependency trees. 4.1 Generative Story Description Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), the model we used ﬁrst describes the generative story for the"
2011.mtsummit-papers.53,W09-2302,1,0.84403,"(2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazawa and Kurohashi, 2009) and deﬁnes the reorderings on the word dependency trees. 4.1 Generative Story Description Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), the model we used ﬁrst describes the generative story for the joint alignment model. 1. Generate  concepts from which subtree pairs are generated independently. 2. Combine the subtrees in each language so as to create parallel sentences. Here, subtrees are equivalent to phrases in the previous works. One subtree in a concept can be NULL, which represents an unaligned subtree. The model restricts the unaligned subtrees to be compo"
2011.mtsummit-papers.53,I11-1089,1,0.725191,"to be used. The Chinese characters in Kanji expressions are again useful as clues to ﬁnd word-to-word matchings. We can use Kana-Kanji conversion techniques to get the Kanji expressions from Kana expressions, but here, we simply consult a Japanese dictionary of JUMAN (Kurohashi et al., 1994). Table 5 gives some examples of Kana-Kanji conversion results. We only do Kana-Kanji conversion for content words because it is proved that do Kana-Kanji conversion for function words may lead to wrong alignment in the alignment experiments we did. 4 Alignment Model We used an alignment model proposed by Nakazawa and Kurohashi (2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazaw"
2011.mtsummit-papers.53,J03-1002,0,0.00494741,"shown in Table 6) • Kanconvit: Kanconvit Kanji to Hanzi conversion and Kana-Kanji conversion The results shown in Table 7 veriﬁed the effectiveness of our proposed detecting method. Also, there is complementation between Kanconvit and our proposed detecting method. 4 http://www.jst.go.jp http://www.nict.go.jp/ 6 http://kanconvit.ta2o.net/ 5 The training corpus we used is the same one we used in Subsection 5.1. As gold-standard data, we used 510 sentence pairs for Japanese-Chinese which were annotated by hand. There are two types of annotations, sure (S) alignments and possible (P) alignments (Och and Ney, 2003). The unit of evaluation was word. We used precision, recall and alignment error rate (AER) as evaluation criteria. All the experiments were run on original forms of words. We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi"
2012.eamt-1.7,I05-3025,0,0.0307599,"nd “» ‰↔» T(anesthesia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Ja"
2012.eamt-1.7,E09-1063,0,0.316822,"Missing"
2012.eamt-1.7,I08-1033,0,0.17868,"ctive for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the tr"
2012.eamt-1.7,C04-1081,0,0.0241624,"Missing"
2012.eamt-1.7,W08-0336,0,0.0350343,"orter unit standard. Therefore, the segmentation unit in Chinese may be longer than Japanese even for the same concept. This can increase the number of 1-to-n alignments which makes the word alignment task more difficult. Taking “founder” Introduction As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important first step in MT. Studies showed that a MT system with Chinese word segmentation outperforms the one treating each Chinese character as a single word, and the quality of Chinese word segmentation affects the MT performance (Xu et al., 2004; Chang et al., 2008). It has been found that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence"
2012.eamt-1.7,2011.mtsummit-papers.53,1,0.899763,"iation for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Japanese MT and exploit common Chinese characters in Chinese word segmentation optimization. Table 1: Examples of common Chinese characters (TC denotes Traditional Chinese and SC denotes Simplified Chinese). in Figure 1 as an example, the Chinese segmenter recognizes it as one token, while the Japanese segmenter splits"
2012.eamt-1.7,wang-etal-2010-adapting,0,0.0166521,"s mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the transformed data for training the Chinese segmenter. Because the extracted lexicons are derived from Japanese word segmentation results, they follow Japanese 37 从_P/ 有效性_NN /高_VA/的_DEC/ 格要素_NN /… CTB: Lexicon: Lexicon"
2012.eamt-1.7,W02-1001,0,0.0137644,"e same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based word segmentation and POS tagging tool with a system dictionary, weights for the lexicons in the system dictionary are automatically learned from the training data using averaged structured perceptron (Collins, 2002). For Japanese, we used JUMAN (Kurohashi et al., 1994). We conducted Chinese-Japanese translation experiments to show the effectiveness of exploiting common Chinese characters in Chinese word segmentation optimization. 4.1.1 Chinese Annotated Corpus We used two types of manually annotated Chinese corpus for training the Chinese segmenter. One is NICT Chinese Treebank, which is from the same domain as the parallel training corpus and contains 9,792 sentences. Note that the annotated sentences in this corpus are not included in the parallel training corpus. The other corpus is CTB 7 (LDC2010T07)"
2012.eamt-1.7,I11-1035,0,0.118655,"esia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the P"
2012.eamt-1.7,I05-1059,0,0.173956,"und that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Ja"
2012.eamt-1.7,P07-2045,0,0.00948082,"racters, such as “L‚(praise)”, “×L(poem)” etc. Obviously, splitting “L ‚(praise)” into “L(song)” and “‚(eulogy)”, or splitting “× L(poem)” into “×(poem)” and “L(song)” is undesirable. Also, there are few consecutive tokens in the training data that can be combined to one extracted lexicon, we do not consider this pattern. 4 corpus was created by the Japanese project “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of this corpora are shown in Table 3. 4.1.2 4.1.3 4.1.4 5 SMT Model We used the state-of-the-art phrase-based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (6→20). It was tuned by MERT using another 500 development sentence pairs. 4.1.5 Test Sets We translated 5 test sets of Chinese sentences from the same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based wo"
2012.eamt-1.7,xia-etal-2000-developing,0,0.106997,"Missing"
2012.eamt-1.7,W04-1118,0,0.085468,"Missing"
2012.eamt-1.7,W04-3230,0,0.0697192,"Missing"
2012.iwslt-evaluation.12,D11-1047,1,0.920432,"Figure 1 shows the overview of our EBMT system on Chinese-English translation. The translation example database is automatically constructed from a parallel training corpus by means of a Bayesian subtree alignment model. Note that both source and target sides of all the examples are stored in dependency tree structures. An input sentence is also parsed and transformed into a dependency structure. For all the sub-trees in the input dependency structure, matching examples are searched in the example database. This step is the most time consuming part, and we exploit a fast tree retrieval method [2]. There are many available examples for one sub-tree, and also, there are many possible sub-tree combinations. The best combination is detected by a log-linear decoding model with features described in Section 3. In the example in Figure 1, ﬁve examples are used. They are combined and produce an output dependency tree. We call nodes surrounding those of the example, “bond” nodes. The bond nodes of one example are replaced by other examples, and thus examples can be combined. We attended the IWSLT 2012 OLYMPICS task which is a Chinese-to-English text translation task. Based on the characteristi"
2012.iwslt-evaluation.12,I11-1089,1,0.840723,"equential model is prone to many such errors even for short simple sentences of a distant language pair. Even if the word order differs greatly between languages, phrase dependencies tend to hold between languages. This can be seen in Figure 2. Therefore, incorporating dependency analysis into the alignment model is useful for distant lanFigure 3: Alignment results from bi-directional GIZA++. Black boxes depict the system output, while dark (Sure) and light (Possible) gray cells denote gold-standard alignments. guage pairs. We exploit Bayesian subtree alignment model based on dependency trees [3]. This model incorporates dependency relations of words into the alignment model and deﬁne the reorderings on the word dependency trees. Figure 2 shows an example of the dependency trees for Japanese and English. 3. Tree-based Translation As a tree-based translation method, we adopt an examplebased machine translation system [1]. In this section, we brieﬂy introduce the translation procedure in our EBMT system. 3.1. Retrieval of Translation Examples The input sentence is converted into the dependency structure as in the alignment step. Then, for each sub-tree, avail97 The 9th International Wor"
2012.iwslt-evaluation.12,J05-4003,0,0.118733,"Missing"
2012.iwslt-evaluation.12,E09-1063,0,0.0208048,"ﬁer trained on the BTEC corpus does not work well on the HIT corpus because of the difference between these two corpora, thus some parallel sentences are also ﬁltered in this process. BLEU 0.1162 0.1209 0.1271 Table 1: Results of preliminary translation experiments. 4.4. Optimized Chinese Segmenter As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important ﬁrst step in machine translation. Research shows that optimal Chinese word segmentation for machine translation is dependent on the other language, therefore, a bilingual approach is necessary [6]. In this task, we adopted a Chinese segmenter optimized based on a bilingual perspective, which exploits common Chinese characters shared between Chinese and Japanese for Chinese word segmentation optimization [7]. The BLEU scores with and without Chinese segmenter optimization are given in Table 1, indicated as “Optimized” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation"
2012.iwslt-evaluation.12,2012.eamt-1.7,1,0.576691,"e 1: Results of preliminary translation experiments. 4.4. Optimized Chinese Segmenter As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important ﬁrst step in machine translation. Research shows that optimal Chinese word segmentation for machine translation is dependent on the other language, therefore, a bilingual approach is necessary [6]. In this task, we adopted a Chinese segmenter optimized based on a bilingual perspective, which exploits common Chinese characters shared between Chinese and Japanese for Chinese word segmentation optimization [7]. The BLEU scores with and without Chinese segmenter optimization are given in Table 1, indicated as “Optimized” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation results are much more similar to English in number, which can reduce the number of 1-to-n alignments and improve the alignment accuracy. 4.5. Rule-based Decoding Constraints Translating long and complex sentences"
2012.iwslt-evaluation.12,2011.iwslt-evaluation.5,0,0.0312388,"d” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation results are much more similar to English in number, which can reduce the number of 1-to-n alignments and improve the alignment accuracy. 4.5. Rule-based Decoding Constraints Translating long and complex sentences is a critical problem in machine translation, because it increases the computational complexity. Finch et al. [8] presented a simple yet efﬁcient method to solve this problem. They split a sentence into smaller units based on part-of-speech (POS) tags and commas, and translate the split units separately. Following their method, we also split a sentence into smaller units during decoding. Our EBMT system tends to choose large examples. Since the development data of this task also has the sub-sentence problem (described in Section 4.3), our system may use examples across punctuation boundaries which can generate translations with unnatural word order. Therefore, we split a source sentence based on comma, p"
2012.iwslt-evaluation.12,2011.iwslt-papers.6,0,0.0390629,"Missing"
2012.iwslt-evaluation.12,I08-1012,0,0.0274568,"Missing"
2012.iwslt-evaluation.12,N10-1015,0,0.069588,"Missing"
2012.iwslt-evaluation.12,federico-etal-2012-iwslt,0,\N,Missing
2012.iwslt-evaluation.12,E09-1000,0,\N,Missing
2014.amta-wptp.15,aziz-etal-2012-pet,0,0.0300338,"shiaki Nakazawa‡ Daisuke Kawahara† Sadao Kurohashi† † Graduate School of Informatics, Kyoto University, Kyoto 606-8501 ‡ Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012 † {kishimoto,dk,kuro}@nlp.ist.i.kyoto-u.ac.jp ‡ nakazawa@pa.jst.jp 1 Purpose and characteristics Translation has become increasingly important by virtue of globalization. To reduce the cost of translation, it is necessary to use machine translation and further to take advantage of post-editing based on the result of a machine translation for accurate information dissemination. Such post-editing (e.g., PET [Aziz et al., 2012]) can be used practically for translation between European languages, which has a high performance in statistical machine translation. However, due to the low accuracy of machine translation between languages with diﬀerent word order, such as Japanese-English and Japanese-Chinese, post-editing has not been used actively. We propose a post-editing system based on syntaxbased machine translation to deal with diﬀerent word order. For language pairs with diﬀerent word order, it is time-consuming for a translator to understand what a machine translation system did. To solve this problem, our syste"
2015.mtsummit-papers.20,chu-etal-2012-chinese,1,0.92817,"α)Scontext The specified value of α (0 ≤ α ≤1) is determined using a method described in next chapter. The character combination with the highest score is regarded as the final translation result. If there are more than 2 words that cannot be translated by Wikipedia or Wiktionary, we process the procedure in sequence (from beginning to end of the sentence) and use the translation result as context feature to the remaining unknown words. 4 Experiment 4.1 Settings Chu et al. had produced a Chinese character mapping table for Japanese (Kanji), Traditional Chinese (TC) and Simplified Chinese (SC) Chu et al. (2012). Thus, for constructing a table that contains mapping relationship between Korean Hangul and Chinese Hanzi, we need to construct rather Kanji-Hangul or Hanzi-Hangul tables and merge them. We collected HangulHanja mapping information from the web. • We acquired 1365 Hanja characters with their aligned Hangul from a freely accessible webpage4 . These characters are contained by words whose frequency is higher than 5965 times in some Sino-Korean corpus (there is no specific information about the mentioned Sino-Korean corpus). • There are also some materials that contain Hanja characters that are"
2015.mtsummit-papers.20,W00-0803,0,0.0966074,"2 Related Work There are some previous work on character conversion, both within language or between two languages (Chen and Lee (2000), Huang et al. (2004)). During character conversion, a bilingual dictionary is needed for candidate selection. For a low resource language like Korean, a bilingual dictionary containing enough data, including polysemous words, is often hard to obtain. Moreover, unlike sentence translation, character translation often ignores the context information of the input source sentence. Chinese character knowledge is widely used in cross-language information retrieval (Hasan and Matsumoto (2000)), or translation of names of people (Wang et al. (2007, 2008, 2009)). During translation, they select named entities by removing the postpositions or the endings, by applying the maximum matching algorithm. For Sino-Korean words that are written using same Hangul word but expressing different meanings according to various context environments (ambiguous words), they adopt some mutual information score to evaluate the co-relation between the query term and the candidates. In languages such as Japanese and Korean, there is more ambiguity about where word boundaries should be, wherein some parti"
2015.mtsummit-papers.20,W11-2123,0,0.0601652,"Missing"
2015.mtsummit-papers.20,W04-1111,0,0.0559045,"p. 257 ually decreasing but these kinds of words are still frequently used in formal writings, such as newspapers and dissertations. Most of the Sino-Korean words are not written in Hanja directly, but in Hangul. However, we can convert them into Hanja and further to Hanzi because there is a correspondence among them. Actually, some papers are published with combinations of Hangul and Hanzi in order to specify definitions of vocabularies or emphasize them. 2.2 Related Work There are some previous work on character conversion, both within language or between two languages (Chen and Lee (2000), Huang et al. (2004)). During character conversion, a bilingual dictionary is needed for candidate selection. For a low resource language like Korean, a bilingual dictionary containing enough data, including polysemous words, is often hard to obtain. Moreover, unlike sentence translation, character translation often ignores the context information of the input source sentence. Chinese character knowledge is widely used in cross-language information retrieval (Hasan and Matsumoto (2000)), or translation of names of people (Wang et al. (2007, 2008, 2009)). During translation, they select named entities by removing"
2015.mtsummit-papers.20,P00-1050,0,0.0900092,"rds are often not efficient. Moreover, unlike information retrieval, the machine translation method also has to ensure the meaning of the sentence in order to be fluent. In other words, we should also consider context features of the sentences. Since Korean characters are phonograms, we can find corresponding Hangul characters for given Hanzi characters. Actually, almost all of the Hanzi can be converted to one (or in some rare cases, many) Korean characters. Huang et al. constructed a Chinese-Korean Character Transfer Table (CKCT Table) to reflect the correspondence between Hanzi and Hangul (Huang and Choi (2000)). It is reported that the table contains 436 Hangul with corresponding 6763 Hanzi. Practically, there are 4888 common Hanja used in Korean (KATS (Korean Agency for Technology and Standards) (1997), Hanyang Systems (1992)). Moreover, the number of dailyused Hanzi in Korea numbers around 18001 , while 3500 Hanzi are required to learn for practical Chinese character level test2 . Obviously, most of the Hanzi in their table cannot be considered as practical ones. After all, the table is non-public to ordinary users. 3 Proposed Method Figure 1 gives an overview of our Korean-to-Chinese terminologi"
2015.mtsummit-papers.20,I08-7004,0,0.0617686,"Missing"
2015.mtsummit-papers.20,I13-1020,1,0.891531,"Missing"
2015.mtsummit-papers.20,Y07-1051,0,0.0244467,"ion, both within language or between two languages (Chen and Lee (2000), Huang et al. (2004)). During character conversion, a bilingual dictionary is needed for candidate selection. For a low resource language like Korean, a bilingual dictionary containing enough data, including polysemous words, is often hard to obtain. Moreover, unlike sentence translation, character translation often ignores the context information of the input source sentence. Chinese character knowledge is widely used in cross-language information retrieval (Hasan and Matsumoto (2000)), or translation of names of people (Wang et al. (2007, 2008, 2009)). During translation, they select named entities by removing the postpositions or the endings, by applying the maximum matching algorithm. For Sino-Korean words that are written using same Hangul word but expressing different meanings according to various context environments (ambiguous words), they adopt some mutual information score to evaluate the co-relation between the query term and the candidates. In languages such as Japanese and Korean, there is more ambiguity about where word boundaries should be, wherein some particles may also be a part of a noun, or a verb, their met"
2015.mtsummit-papers.20,I08-1037,0,0.0760323,"Missing"
2015.mtsummit-wpslt.7,P05-1022,0,0.0247598,"el only requires monolingual data, for comparison we also trained a separate model on a larger (30M sentences) in-house monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy"
2015.mtsummit-wpslt.7,2006.eamt-1.27,0,0.0997249,"n word translation for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate documents’ is a relative clause (missing the relative pronoun ‘which’ or ‘tha"
2015.mtsummit-wpslt.7,W11-2123,0,0.0174646,"state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy. In human evaluation we asked two native speakers of the target language (English) with knowledge of the source language (Japanese) to decide if the system output was 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ Proceedings of 6th Workshp on Pa"
2015.mtsummit-wpslt.7,O10-5004,0,0.0165403,"tion for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate documents’ is a relative clause (missing the relative pronoun ‘which’ or ‘that’) and that ‘the pap"
2015.mtsummit-wpslt.7,N06-1023,1,0.759196,"trained on the training fold of the ASPEC data. Since our model only requires monolingual data, for comparison we also trained a separate model on a larger (30M sentences) in-house monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks"
2015.mtsummit-wpslt.7,D08-1077,0,0.0255988,"ﬀect on preserving sentence meaning, and that badly formed punctuation impedes understanding. Surprisingly few studies have been made speciﬁcally on improving function word translation for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often un"
2015.mtsummit-wpslt.7,P02-1040,0,0.0925772,"-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy. In human evaluation we asked two native speakers of the target language (English) with knowledge of the source language (Japanese) to decide if the system output was 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 47 better, worse, or neutral compared to the baseline. A random sample of 20 edited sentences were selected for each exper"
2015.mtsummit-wpslt.7,P14-5014,1,0.831472,"tences, 1790 development sentences and 1812 test sentences. We deﬁned English function words as those tokens with POS tags of functional types such as determinants and prepositions, and treated Japanese particles as function words for the purposes of alignment-based ﬁltering. The primary post-editing model was trained on the training fold of the ASPEC data. Since our model only requires monolingual data, for comparison we also trained a separate model on a larger (30M sentences) in-house monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenL"
2015.mtsummit-wpslt.7,D11-1046,0,0.0263646,"e monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy. In human evaluation we asked two native speakers of the target language (English) with knowledge of the sour"
2015.mtsummit-wpslt.7,P13-3025,0,0.01308,"ction words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate documents’ is a relative clause (missing the relative pronoun ‘which’ or ‘that’) and that ‘the paper’ is a prepositional phrase (missing the preposition ‘in’) rather than the direct object of ‘described’. We believe that the intended meaning of a sentence is often unclear from ﬂat MT output. For example, in Figure 1, the"
2015.mtsummit-wpslt.7,N07-1064,0,0.0203737,"ing. Surprisingly few studies have been made speciﬁcally on improving function word translation for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate document"
2020.lrec-1.447,N18-1118,0,0.353724,"ce that contains the antecedent from the current sentence as the previous sentence. Figure 1: Example of the zero pronoun problem. These sentences represent a conversation between a reporter and a paramedic. tence. The current sentence contains a word for which we need the context sentence to translate it correctly, such as ambiguous anaphoric pronouns la and le in French. Thus, a translation model needs to pay attention to the context sentence to output the correct translation. They evaluated whether each model selects the correct translation based on a score computed by the model. Following Bawden et al. (2018), we construct a test set that consists of four components: a current sentence, context, a correct translation, and an incorrect translation. In our test set, since the current sentence contains zero pronoun, translation models need to detect the correct antecedent from the context sentences. 2.2. Test Set Construction To avoid contaminating the test set with noisy instances, we construct our test set with fully hand-crafted. Thus, the construction of our test set consisted of three steps: translation, zero pronoun detection, and incorrect instance construction. To easily detect zero pronouns"
2020.lrec-1.447,E12-3001,0,0.064363,"Missing"
2020.lrec-1.447,P14-2091,0,0.0356914,"Missing"
2020.lrec-1.447,P18-1044,0,0.196074,"Missing"
2020.lrec-1.447,W10-1737,0,0.0896794,"Missing"
2020.lrec-1.447,L16-1147,0,0.0180745,"only one sentence as the context, they prepared only one additional encoder. In contrast, we prepared more encoders because we used at most three sentences as contexts. For this reason, we applied 3-TO-1 and 4-TO-1 in addition to 2-TO1 in Bawden et al. (2018). We trained each method with the same hyper-parameters as Bawden et al. (2018). 3.1. Training Data We have two requirements for the training corpus: the training corpus consists of conversation such as our constructed test set and contains context sentences for each JapaneseEnglish sentence pair. In this paper, we used OpenSubtitles2016 (Lison and Tiedemann, 2016)5 as a corpus that satisfies these requirements. We extracted Japanese-English sentence pairs from OpenSubtitles2016 with applying the 2 We also applied Skip-gram but CBOW achieved better performance in our experiments. 3 https://github.com/OpenNMT/OpenNMT-py 4 They used GRU but we applied LSTM for comparison with the SINGLE - ENCODER. 5 https://www.opensubtitles.org/ja 3632 Model M AJORITY C O - OCCUR ( X , TARGET ) C O - OCCUR ( X , S OURCE +TARGET ) S INGLE 1-TO-1 S INGLE 2-TO-1 S INGLE 3-TO-1 S INGLE 4-TO-1 M ULTI 2-TO-1 M ULTI 3-TO-1 M ULTI 4-TO-1 H UMAN W / O CONTEXT H UMAN W / CONTEXT d"
2020.lrec-1.447,D15-1166,0,0.0560902,"eted as factorized results of a co-occurrence matrix (Levy and Goldberg, 2014). We define the cooccurrence score as follows: C O - OCCUR ( X , W) = X vx · vw , (1) w∈W x where W is a set of words in the sentence, x is a target pronoun in the correct (or incorrect) sentence, and vw is an embedding of a word w. We used two configurations for W : one was from the target side only (TARGET), i.e., W contains words in the correct (or incorrect) sentence and the other was from both of the target and the source (S OURCE +TARGET). S INGLE - ENCODER We applied a widely-used LSTM encoder-decoder model (Luong et al., 2015) as the basic NMT model. We used the implementation of OpenNMTpy3 with the same hyper-parameters as Bawden et al. (2018). In addition to translations from only the current sentence (1-TO-1), we evaluated the performance of the model with contexts. We concatenated context sentences with the current sentence to encode multiple sentences in this method. Figure 3: Example of our test set. These sentences represent a conversation between a reporter and a paramedic. The context sentences are spoken by the reporter and the current sentence is spoken by the paramedic. they answered “undecidable”. More"
2020.lrec-1.447,P14-5010,0,0.00472763,"Missing"
2020.lrec-1.447,C18-1009,0,0.0443329,"Missing"
2020.lrec-1.447,P02-1040,0,0.110051,"Missing"
2020.lrec-1.447,P16-1162,0,0.0997833,"Missing"
2020.lrec-1.447,P18-1054,0,0.17657,"Missing"
2020.lrec-1.447,W12-4213,0,0.0706452,"Missing"
2020.lrec-1.447,W17-4811,0,0.0668043,"Missing"
2020.lrec-1.447,P18-1117,0,0.0319429,"Missing"
2020.lrec-1.447,P19-1116,0,0.103041,"Missing"
2020.lrec-1.459,D19-5214,0,0.0204407,"50) Items (N=50) Texts (N=92) Items (N=83) ntt 68.0 74.0 93.5 97.6 NICT-2 68.0 68.0 93.5 96.4 sarah 72.0 74.0 92.4 94.0 geoduck 76.0 68.5 - 0 or 1, where 1 means that the words (important figures or proper nouns) are correctly translated. 6.3. Participants During the 6th WAT, 4 teams participated in the Japanese–English timely disclosure documents task Morishita et al. as ntt (Nakazawa et al., 2019). (Morishita et al., 2019), Imamura and Sumita as NICT-2 (Imamura and Sumita, 2019), and Susanto et al. as sarah (Susanto et al., 2019) used NMT without other resources. Eriguchi et al. as geoduck (Eriguchi et al., 2019) used translation memory and NMT with 1 million Japanese– English Wikipedia parallel corpus provided by Asai et al. (Asai et al., 2018) as an additional training resource. 6.4. Evaluation Results Figures 2 and 3 denote the official results of the timely disclosure task (Nakazawa et al., 2019). Table 9 shows the evaluation results of the sentences containing important figures or proper nouns. Table 10 shows the sample results of 3723 Figure 2: Official evaluation results of Items (tddc-itm-ja-en) Figure 3: Official evaluation results of Texts (tddc-txt-ja-en) participants. In the results, all s"
2020.lrec-1.459,D19-5217,0,0.0108831,"ted companies Performance Results Corporate Governance Reports Material Facts Others From REITs The whole of the source Test Texts Texts (N=50) Items (N=50) Texts (N=92) Items (N=83) ntt 68.0 74.0 93.5 97.6 NICT-2 68.0 68.0 93.5 96.4 sarah 72.0 74.0 92.4 94.0 geoduck 76.0 68.5 - 0 or 1, where 1 means that the words (important figures or proper nouns) are correctly translated. 6.3. Participants During the 6th WAT, 4 teams participated in the Japanese–English timely disclosure documents task Morishita et al. as ntt (Nakazawa et al., 2019). (Morishita et al., 2019), Imamura and Sumita as NICT-2 (Imamura and Sumita, 2019), and Susanto et al. as sarah (Susanto et al., 2019) used NMT without other resources. Eriguchi et al. as geoduck (Eriguchi et al., 2019) used translation memory and NMT with 1 million Japanese– English Wikipedia parallel corpus provided by Asai et al. (Asai et al., 2018) as an additional training resource. 6.4. Evaluation Results Figures 2 and 3 denote the official results of the timely disclosure task (Nakazawa et al., 2019). Table 9 shows the evaluation results of the sentences containing important figures or proper nouns. Table 10 shows the sample results of 3723 Figure 2: Official evaluat"
2020.lrec-1.459,D10-1092,0,0.0258004,"mpanies in Train-2016-to-2017 is 590, although 3,602 companies are listed on TSE as of the end of 2017. 6. Timely Disclosure Task at the 6th Workshop on Asian Translation TDDC was provided for the 6th WAT, which is an open evaluation campaign that focuses on Asian languages. The participants of the timely disclosure task can submit the results of Texts and/or Items. During the 6th WAT, the translation performance of the results underwent automatic and human evaluations (Nakazawa et al., 2019). As automatic evaluation, the following three metrics were used: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). For human evaluation, two types of evaluations were used: pairwise crowdsourcing evaluation (Nakazawa et al., 2016) and Japan Patent Office (JPO) adequacy evaluation. In addition to these official evaluations during the 6th WAT, to focus on the timely disclosure task, we particularly evaluated the results of JPO adequacy evaluation. 6.1. JPO Adequacy Evaluation The JPO adequacy evaluation was performed by translation experts with a quality evaluation criterion for translated patent documents that was decided by the JPO. Table 8 shows the JPO adequacy criterion"
2020.lrec-1.459,P15-1002,0,0.0172538,"Examples of dates 2. Timely Disclosure Task Timely disclosure task, which is one of the subtasks for the 6th WAT, aims to improve the Japanese to English translation of sentences extracted from timely disclosure documents to avoid mistranslations that would confuse investors. As terms on which investors focus, we define two groups: important figures and proper nouns. These terms cannot be accurately translated using typical Neural Machine Translation (NMT) systems because the NMT systems restrict the vocabulary size and consider rare words (e.g., names and numbers) as out-ofvocabulary words (Luong et al., 2015). The current NMT systems introduce the subword tokenization, which transfers rare words to the sequence of its constituent characters (Sennrich et al., 2016). However, the subword tokenization solves the problem only if a rare word can be translated as constitutive words. Thus, even using the subword tokenization, the NMT systems often are often unable to translate neither numbers with many digits nor constitutive proper nouns. The following sections will explain the summary of the timely disclosure documents and their details. Japanese Japanese English Unbalanced Translation 平成 30 年 6 月 26 日"
2020.lrec-1.459,D19-5211,0,0.0113804,"publishing companies in Train–2016–to–2017 From TSE–listed companies Performance Results Corporate Governance Reports Material Facts Others From REITs The whole of the source Test Texts Texts (N=50) Items (N=50) Texts (N=92) Items (N=83) ntt 68.0 74.0 93.5 97.6 NICT-2 68.0 68.0 93.5 96.4 sarah 72.0 74.0 92.4 94.0 geoduck 76.0 68.5 - 0 or 1, where 1 means that the words (important figures or proper nouns) are correctly translated. 6.3. Participants During the 6th WAT, 4 teams participated in the Japanese–English timely disclosure documents task Morishita et al. as ntt (Nakazawa et al., 2019). (Morishita et al., 2019), Imamura and Sumita as NICT-2 (Imamura and Sumita, 2019), and Susanto et al. as sarah (Susanto et al., 2019) used NMT without other resources. Eriguchi et al. as geoduck (Eriguchi et al., 2019) used translation memory and NMT with 1 million Japanese– English Wikipedia parallel corpus provided by Asai et al. (Asai et al., 2018) as an additional training resource. 6.4. Evaluation Results Figures 2 and 3 denote the official results of the timely disclosure task (Nakazawa et al., 2019). Table 9 shows the evaluation results of the sentences containing important figures or proper nouns. Table 10 sh"
2020.lrec-1.459,L16-1350,1,0.913172,"companies to disclose both Japanese and English documents simultaneously. Consequently, it is not easy to meet the demand for the English translation of timely disclosure documents using manual translation only. The machine translation could therefore be a solution to these problems. For current machine translation systems aimed at a specific field, a parallel corpus adapted to that field is regarded as an essential resource. There are already Japanese–English parallel corpora for training machine translation systems in fields such as patents (Utiyama and Isahara, 2007) and scientific papers (Nakazawa et al., 2016). However, thus far, there is no large-scale Japanese–English parallel corpus specifically aimed at the Investor Relations field. The Timely Disclosure Documents Corpus (TDDC) consists of approximately 1.4 million Japanese–English sentence pairs that have been extracted from past timely disclosure documents and other documents. Timely disclosure documents contain important figures (e.g., sales, profits, and dates) and proper nouns (e.g., names of people, places, companies, businesses, and products). This information is essential for investors; thus, mistranslations need 3719 to be avoided, and"
2020.lrec-1.459,D19-5201,1,0.918153,"disclosed both in Japanese and English; thus the publishing companies are biased. The number of the publishing TSE–listed companies in Train-2016-to-2017 is 590, although 3,602 companies are listed on TSE as of the end of 2017. 6. Timely Disclosure Task at the 6th Workshop on Asian Translation TDDC was provided for the 6th WAT, which is an open evaluation campaign that focuses on Asian languages. The participants of the timely disclosure task can submit the results of Texts and/or Items. During the 6th WAT, the translation performance of the results underwent automatic and human evaluations (Nakazawa et al., 2019). As automatic evaluation, the following three metrics were used: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). For human evaluation, two types of evaluations were used: pairwise crowdsourcing evaluation (Nakazawa et al., 2016) and Japan Patent Office (JPO) adequacy evaluation. In addition to these official evaluations during the 6th WAT, to focus on the timely disclosure task, we particularly evaluated the results of JPO adequacy evaluation. 6.1. JPO Adequacy Evaluation The JPO adequacy evaluation was performed by translation experts with a quality"
2020.lrec-1.459,P02-1040,0,0.107616,"of the publishing TSE–listed companies in Train-2016-to-2017 is 590, although 3,602 companies are listed on TSE as of the end of 2017. 6. Timely Disclosure Task at the 6th Workshop on Asian Translation TDDC was provided for the 6th WAT, which is an open evaluation campaign that focuses on Asian languages. The participants of the timely disclosure task can submit the results of Texts and/or Items. During the 6th WAT, the translation performance of the results underwent automatic and human evaluations (Nakazawa et al., 2019). As automatic evaluation, the following three metrics were used: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). For human evaluation, two types of evaluations were used: pairwise crowdsourcing evaluation (Nakazawa et al., 2016) and Japan Patent Office (JPO) adequacy evaluation. In addition to these official evaluations during the 6th WAT, to focus on the timely disclosure task, we particularly evaluated the results of JPO adequacy evaluation. 6.1. JPO Adequacy Evaluation The JPO adequacy evaluation was performed by translation experts with a quality evaluation criterion for translated patent documents that was decided by the JPO. Table 8 sho"
2020.lrec-1.459,P16-1162,0,0.00856123,"translation of sentences extracted from timely disclosure documents to avoid mistranslations that would confuse investors. As terms on which investors focus, we define two groups: important figures and proper nouns. These terms cannot be accurately translated using typical Neural Machine Translation (NMT) systems because the NMT systems restrict the vocabulary size and consider rare words (e.g., names and numbers) as out-ofvocabulary words (Luong et al., 2015). The current NMT systems introduce the subword tokenization, which transfers rare words to the sequence of its constituent characters (Sennrich et al., 2016). However, the subword tokenization solves the problem only if a rare word can be translated as constitutive words. Thus, even using the subword tokenization, the NMT systems often are often unable to translate neither numbers with many digits nor constitutive proper nouns. The following sections will explain the summary of the timely disclosure documents and their details. Japanese Japanese English Unbalanced Translation 平成 30 年 6 月 26 日 2018 年度第 1 四半期 June 26, 2018 the first quarter of fiscal 2018 FY March 2019 FY May 2018 April 1 of this year consolidated cumulative third quarter of this fi"
2020.lrec-1.459,D19-5219,0,0.060093,"eports Material Facts Others From REITs The whole of the source Test Texts Texts (N=50) Items (N=50) Texts (N=92) Items (N=83) ntt 68.0 74.0 93.5 97.6 NICT-2 68.0 68.0 93.5 96.4 sarah 72.0 74.0 92.4 94.0 geoduck 76.0 68.5 - 0 or 1, where 1 means that the words (important figures or proper nouns) are correctly translated. 6.3. Participants During the 6th WAT, 4 teams participated in the Japanese–English timely disclosure documents task Morishita et al. as ntt (Nakazawa et al., 2019). (Morishita et al., 2019), Imamura and Sumita as NICT-2 (Imamura and Sumita, 2019), and Susanto et al. as sarah (Susanto et al., 2019) used NMT without other resources. Eriguchi et al. as geoduck (Eriguchi et al., 2019) used translation memory and NMT with 1 million Japanese– English Wikipedia parallel corpus provided by Asai et al. (Asai et al., 2018) as an additional training resource. 6.4. Evaluation Results Figures 2 and 3 denote the official results of the timely disclosure task (Nakazawa et al., 2019). Table 9 shows the evaluation results of the sentences containing important figures or proper nouns. Table 10 shows the sample results of 3723 Figure 2: Official evaluation results of Items (tddc-itm-ja-en) Figure 3: Offi"
2020.lrec-1.459,2007.mtsummit-papers.63,0,0.252389,"Missing"
2020.wat-1.1,L18-1548,1,0.762787,"emselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training, development and test data. Par ticipants of this taks must get a copy of BSD corpus by themselves. 2.12 IITB Hindi–English task In this task we use IIT Bombay EnglishHindi Cor pus (Kunchukuttan et al., 2018) which contains EnglishHindi parallel corpus as well as mono lingual Hindi corpus collected from a variety of sources and corpora (Bojar et al., 2014). This cor pus had been developed at the Center for Indian Language Technology, IIT Bombay over the years. The corpus is used for mixed domain tasks hi↔en. The statistics for the corpus are shown in Table 13. 3 4.1 Tokenization We used the following tools for tokenization. 4.1.1 For ASPEC, JPC, TDDC, JIJI, ALT, UCSY, ECCC, and IITB • Juman version 7.024 for Japanese segmenta tion. • Stanford Word Segmenter version 201401 0425 (Chinese Penn"
2020.wat-1.1,E06-1031,0,0.0814841,"ipants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcomes. The scores are thus comparable only wi"
2020.wat-1.1,W17-5701,1,0.728933,"on to documentlevel evaluation. 2.11 Documentlevel Translation Task In WAT2020, we set up 2 documentlevel transla tion tasks: ParaNatCom and BSD. 20 21 http://www2.nict.go.jp/astrec-att/member/ mutiyama/paranatcom/ https://github.com/nlabmpg/Flickr30kEntJP 7 Lang hien hi Train 1,609,682 – Dev 520 – Test 2,507 – Mono – 45,075,279 systems were published on the WAT web page.23 We also have SMT baseline systems for the tasks that started at WAT 2017 or before 2017. The base line systems are shown in Tables 16, 17, and 18. SMT baseline systems are described in the WAT 2017 overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online transla tion systems were operated by the organizers. We note that these RBMT companies and online trans lation companies did not submit themselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training"
2020.wat-1.1,Y18-3001,1,0.753826,"raining, develop ment, and test data of the KhmerEnglish transla tion tasks are listed in Table 6. 1 4 http://opus.nlpl.eu/ Lang.pair Ja↔Ru Ja↔En Ru↔En Table 8: task. Partition train development test train development test train development test #sent. 12,356 486 600 47,082 589 600 82,072 313 600 #tokens 341k / 229k 16k / 11k 22k / 15k 1.27M / 1.01M 21k / 16k 22k / 17k 1.61M / 1.83M 7.8k / 8.4k 15k / 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k 2.7 Indic Multilingual Task In 2018, we had organized an Indic languages task (Nakazawa et al., 2018) but due to lack of reli able evaluation corpora we discontinued it in WAT 2019. However, in 2020, high quality publicly available evaluation (and training) corpora became available which motivated us to relaunch the task. The Indic task involves mixed domain corpora for evaluation consisting of various articles composed by Indian Prime Minister. The languages involved are Hindi (Hi), Marathi (Mr), Tamil (Ta), Telugu (Te), Gujarati (Gu), Malayalam (Ml), Bengali (Bg) and English (En). English is either the source or the target language during evaluation leading to a total of 14 translation dir"
2020.wat-1.1,W12-5611,1,0.853557,"Missing"
2020.wat-1.1,P11-2093,0,0.0137734,"eq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 30 https://github.com/tensorflow/ tensor2tensor https://taku910.github.io/mecab/ 12 (separated by 1000 batches) and performed decod ing with a beam of size 4 and a length penalty of 0.6. 4.2.3 Before the calculation of the automatic evalua tion scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmenta tion, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model33 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.34 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 20140616 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.35 For Korean segmentation, we used mecabko.36 For Myanmar and Khmer segmen tations, we used myseg.py37 and kmseg.py38 . For English and Russian tokenizations, we used tokenizer.perl39 in the Moses toolkit. For Indonesian and Malay tokenizations, we used tokenizer.perl as same as the Engl"
2020.wat-1.1,2020.lrec-1.518,1,0.770784,") which are not publicly available at the time of WAT 2020. Note that phrasetoregion an notation is not included in the test data. There are two settings of submission: with and without resource constraints. In the constrained setting, external resources such as additional data and pretrained models (with external data) are not allowed to use, except for pretrained convo lutional neural networks (for visual analysis) and basic linguistic tools such as taggers, parsers, and morphological analyzers. As the baseline system to compute the Pairwise score, we implement the textonly model in (Nishihara et al., 2020) under the constrained setting. 2.11.1 Documentlevel Scientific Paper Translation Traditional ASPEC translation tasks are sentence level and the translation quality of them seem to be saturated. We think it’s high time to move on to documentlevel evaluation. For the first year, we use ParaNatCom 21 (Parallel EnglishJapanese ab stract corpus made from Nature Communications articles) for the development and test sets of the Documentlevel Scientific Paper Translation sub task. We cannot provide documentlevel training corpus, but you can use ASPEC and any other ex tra resources. 2.11.2 Do"
2020.wat-1.1,P02-1040,0,0.120093,"ase tophrase and phrasetoregion annotations avail able in the training dataset. 5 5.2 Automatic Evaluation System The automatic evaluation system receives transla tion results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each sub mission: Automatic Evaluation 5.1 Procedure for Calculating Automatic Evaluation Score • Human Evaluation: whether or not they sub mit the results for human evaluation; We evaluated translation results by three met rics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.31 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2020 web page.32 All scores for each task were calculated using the corresponding reference translations. 33 http://www.phontron.com/kytea/model.html http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 35 http://nlp.stanford.edu/software/segmenter"
2020.wat-1.1,2020.lrec-1.462,0,0.0440733,"Missing"
2020.wat-1.1,2006.amta-papers.25,0,0.152126,"conducted pairwise evaluation for participants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcome"
2020.wat-1.1,J82-2005,0,0.692397,"Missing"
2020.wat-1.1,L16-1249,1,0.823126,"pur pose of this task was to test the feasibility of multi domain multilingual solutions for extremely low resource language pairs and domains. Naturally the solutions could be onetomany, manytoone or manytomany NMT models. The domains in question are Wikinews and IT (specifically, Soft ware Documentation). The total number of evalu ation directions are 16 (8 for each domain). There is very little clean and publicly available data for these domains and language pairs and thus we en couraged participants to not only utilize the small Asian Language Treebank (ALT) parallel corpora (Thu et al., 2016) but also the parallel corpora from OPUS1 . The ALT dataset contains 18,088, 1,000 and 1,018 training, development and test ing sentences. As for corpora for the IT domain we only provided evaluation (dev and test sets) 2016), consisting of twenty thousand Khmer English parallel sentences from news articles. • The ECCC corpus consists of 100 thousand KhmerEnglish parallel sentences extracted from document pairs of KhmerEnglish bi lingual records in Extraordinary Chambers in the Court of Cambodia, collected by National Institute of Posts, Telecoms & ICT, Cambo dia. The ALT corpus has been"
2020.wat-1.18,D18-1332,0,0.0269958,"ntifier of our team to be ut-mrt, which specifies our affiliation (The University of Tokyo) and first names (Mat¯ıss, Ryokan, Toshiaki). We participated in both EN→JA and JA→EN translation directions. We experimented with mixing and matching several data sets, data processing approaches and training methods. Our main findings are: 1) using source side context mainly improves EN→JA MT, but not always, and mainly degrades or leaves little impact on JA→EN MT; 2) there are no better data than more data - we see the biggest improvements from using larger training data sets; and 3) optimiser delay (Bogoychev et al., 2018) can help a lot - by setting the optimiser delay value to 8 instead of the default 1 increased BLEU scores by more than 1.5 in both translation directions. 2 Data We used multiple dataset combinations to train our models for the shared task. We also filtered some of the larger automatically collected data sets which are usually more noisy and contain duplicates. Aside from using only the provided BSD training dataset (BSD 20 (Rikters et al., 2019)), we had access to an extended version of the BSD four times the size (BSD 80), as well as two other similar corpora - AMI Meeting corpus (AMI) and"
2020.wat-1.18,2012.eamt-1.60,0,0.0105578,"rain our models for the shared task. We also filtered some of the larger automatically collected data sets which are usually more noisy and contain duplicates. Aside from using only the provided BSD training dataset (BSD 20 (Rikters et al., 2019)), we had access to an extended version of the BSD four times the size (BSD 80), as well as two other similar corpora - AMI Meeting corpus (AMI) and a parallel version of OntoNotes 5.0 (ON) (Rikters et al., 2020). We also experimented with using the jParaCrawl (Morishita et al., 2019) dataset, data from WMT 20201 (whcih includes JParaCrawl, Ted Talks (Cettolo et al., 2012), The Kyoto Free Translation Task Corpus (Neubig, 2011), Japanese-English Subtitle Corpus (Pryzant et al., 2018), WikiMatrix (Schwenk et al., 2019) and Wiki Titles v2), and a proprietary document-aligned news dataset gathered from several sources. The full training data statistics are shown in Table 1. The AMI and jParaCrawl corpora contain many duplicates while the rest seem to be of higher quality. BSD 20 BSD 80 AMI ON WMT jParaCrawl News Total 20,000 80,629 110,483 28,429 17,880,587 10,105,351 1,104,549 Unique 18,818 74,377 75,660 24,335 16,501,296 8,790,618 1,101,751 Filtered 17,672 69,742"
2020.wat-1.18,N13-1073,0,0.0204807,"20 data, 8,000 for experiments with BSD 80 data, 16,000 when using BSD 80 / AMI / ON together and 32,000 tokens for experiments involving WMT, jParaCrawl or News data. We did not perform other tokenisation or truecasing for the training data. We used Mecab (Kudo, 2006) to tokenise the Japanese side of the evaluation data, which we used only for scoring. The English side remained as-is. 3 SMT We trained SMT baseline systems using using the Moses (Koehn et al., 2007) toolkit in the Tilde MT platform (Vasil¸jevs et al., 2012). The SMT systems consist of: word alignment performed using fastalign (Dyer et al., 2013); 7-gram translation models and the wbe-msd-bidirectional-fe-allff reordering models; a language model trained with KenLM (Heafield, 2011); models tuned using the improved MERT (Bertoldi et al., 2009). NMT For the sentence-level NMT systems, we used Sockeye (Hieber et al., 2017) or Marian (JunczysDowmunt et al., 2018) to train transformer architecture (Vaswani et al., 2017) models with several different parameter configurations until convergence on development data (no improvement on validation perplexity for 10 checkpoints). Each model was trained on a single Nvidia TITAN V (12GB) GPU, and tr"
2020.wat-1.18,W11-2123,0,0.0483398,", jParaCrawl or News data. We did not perform other tokenisation or truecasing for the training data. We used Mecab (Kudo, 2006) to tokenise the Japanese side of the evaluation data, which we used only for scoring. The English side remained as-is. 3 SMT We trained SMT baseline systems using using the Moses (Koehn et al., 2007) toolkit in the Tilde MT platform (Vasil¸jevs et al., 2012). The SMT systems consist of: word alignment performed using fastalign (Dyer et al., 2013); 7-gram translation models and the wbe-msd-bidirectional-fe-allff reordering models; a language model trained with KenLM (Heafield, 2011); models tuned using the improved MERT (Bertoldi et al., 2009). NMT For the sentence-level NMT systems, we used Sockeye (Hieber et al., 2017) or Marian (JunczysDowmunt et al., 2018) to train transformer architecture (Vaswani et al., 2017) models with several different parameter configurations until convergence on development data (no improvement on validation perplexity for 10 checkpoints). Each model was trained on a single Nvidia TITAN V (12GB) GPU, and training time was about 2-3 days for models with only BSD/ON/AMI data and about 5-6 days when using WMT/News/jParaCrawl data. The main reaso"
2020.wat-1.18,E17-3017,0,0.0248295,"nise the Japanese side of the evaluation data, which we used only for scoring. The English side remained as-is. 3 SMT We trained SMT baseline systems using using the Moses (Koehn et al., 2007) toolkit in the Tilde MT platform (Vasil¸jevs et al., 2012). The SMT systems consist of: word alignment performed using fastalign (Dyer et al., 2013); 7-gram translation models and the wbe-msd-bidirectional-fe-allff reordering models; a language model trained with KenLM (Heafield, 2011); models tuned using the improved MERT (Bertoldi et al., 2009). NMT For the sentence-level NMT systems, we used Sockeye (Hieber et al., 2017) or Marian (JunczysDowmunt et al., 2018) to train transformer architecture (Vaswani et al., 2017) models with several different parameter configurations until convergence on development data (no improvement on validation perplexity for 10 checkpoints). Each model was trained on a single Nvidia TITAN V (12GB) GPU, and training time was about 2-3 days for models with only BSD/ON/AMI data and about 5-6 days when using WMT/News/jParaCrawl data. The main reason for using two different toolkits is that Marian currently does not support source side input factors, which help when training models with"
2020.wat-1.18,P07-2045,0,0.0108395,"depending on the total training data set size for the specific experiment. The vocabulary size was set to 3,000 tokens for experiments with only BSD 20 data, 8,000 for experiments with BSD 80 data, 16,000 when using BSD 80 / AMI / ON together and 32,000 tokens for experiments involving WMT, jParaCrawl or News data. We did not perform other tokenisation or truecasing for the training data. We used Mecab (Kudo, 2006) to tokenise the Japanese side of the evaluation data, which we used only for scoring. The English side remained as-is. 3 SMT We trained SMT baseline systems using using the Moses (Koehn et al., 2007) toolkit in the Tilde MT platform (Vasil¸jevs et al., 2012). The SMT systems consist of: word alignment performed using fastalign (Dyer et al., 2013); 7-gram translation models and the wbe-msd-bidirectional-fe-allff reordering models; a language model trained with KenLM (Heafield, 2011); models tuned using the improved MERT (Bertoldi et al., 2009). NMT For the sentence-level NMT systems, we used Sockeye (Hieber et al., 2017) or Marian (JunczysDowmunt et al., 2018) to train transformer architecture (Vaswani et al., 2017) models with several different parameter configurations until convergence o"
2020.wat-1.18,D18-2012,0,0.0192759,"tences having a majority of characters outside the scope of the specified language; 5) repeating token filter, which removes sentences that have several repeating tokens or phrases in a row; and 6) correct language filter, which uses language identification (Lui and Baldwin, 2012) to remove parallel sentences where the identified language does not match the expected one. Data amounts after filtering are shown in the final column of Table 1. Similar to the amount of duplicates, AMI and jParaCrawl were filtered the most along with the WMT data set. For pre-processing we used only Sentencepiece (Kudo and Richardson, 2018) to create a shared vocabulary with size depending on the total training data set size for the specific experiment. The vocabulary size was set to 3,000 tokens for experiments with only BSD 20 data, 8,000 for experiments with BSD 80 data, 16,000 when using BSD 80 / AMI / ON together and 32,000 tokens for experiments involving WMT, jParaCrawl or News data. We did not perform other tokenisation or truecasing for the training data. We used Mecab (Kudo, 2006) to tokenise the Japanese side of the evaluation data, which we used only for scoring. The English side remained as-is. 3 SMT We trained SMT"
2020.wat-1.18,P12-3005,0,0.0276585,"ask.html 147 Proceedings of the 7th Workshop on Asian Translation, pages 147–153 c December 4, 2020. 2020 Association for Computational Linguistics filter, which removes parallel sentences that are identical in both languages; 3) multiple sources one target and multiple targets - one source filters; 4) non-alphabetical filters - remove sentences having a majority of characters outside the scope of the specified language; 5) repeating token filter, which removes sentences that have several repeating tokens or phrases in a row; and 6) correct language filter, which uses language identification (Lui and Baldwin, 2012) to remove parallel sentences where the identified language does not match the expected one. Data amounts after filtering are shown in the final column of Table 1. Similar to the amount of duplicates, AMI and jParaCrawl were filtered the most along with the WMT data set. For pre-processing we used only Sentencepiece (Kudo and Richardson, 2018) to create a shared vocabulary with size depending on the total training data set size for the specific experiment. The vocabulary size was set to 3,000 tokens for experiments with only BSD 20 data, 8,000 for experiments with BSD 80 data, 16,000 when usin"
2020.wat-1.18,P02-1040,0,0.107142,"table has no previous context, as it is the first one in the respective document. The second sentence has the first one as context, followed by a beginning of sentence tag <bos&gt;, and so on. Source sentences <bos&gt; はい 、 G 社 お客様 相 談 室 の ケ イ ト です 。 はい 、 G 社 お客様 相 談 室 の ケ イ ト です 。<bos&gt; ご 用 件 は ? Source side factors CSSSSSSSSSSSSSS CCCCCCCCCCCCCCCSSSSS Table 2: Examples of training data source sentences and the respective source side factors for the concatenated context-aware experiments. 4 Results We use the SacreBLEU3 tool (Post, 2018) to evaluate automatic translations and calculate BLEU scores (Papineni et al., 2002) in Table 3, which contains results from the intermediate models that were not submitted to the shared task evaluation site. This table shows the incremental BLEU score improvements of switching between the base and small configurations of the transformer model, model averaging, enabling optimiser delay and domain adaptation. It also shows that BLEU scores go both up and down when adding context sentences to the source side. We did not compare how data filtering impacts the final result, but filtering was only performed in experiment settings which involved the jParaCrawl corpus, which was the"
2020.wat-1.18,W18-6319,0,0.0141375,"with context and factors are shown in Table 2. The first sentence in the table has no previous context, as it is the first one in the respective document. The second sentence has the first one as context, followed by a beginning of sentence tag <bos&gt;, and so on. Source sentences <bos&gt; はい 、 G 社 お客様 相 談 室 の ケ イ ト です 。 はい 、 G 社 お客様 相 談 室 の ケ イ ト です 。<bos&gt; ご 用 件 は ? Source side factors CSSSSSSSSSSSSSS CCCCCCCCCCCCCCCSSSSS Table 2: Examples of training data source sentences and the respective source side factors for the concatenated context-aware experiments. 4 Results We use the SacreBLEU3 tool (Post, 2018) to evaluate automatic translations and calculate BLEU scores (Papineni et al., 2002) in Table 3, which contains results from the intermediate models that were not submitted to the shared task evaluation site. This table shows the incremental BLEU score improvements of switching between the base and small configurations of the transformer model, model averaging, enabling optimiser delay and domain adaptation. It also shows that BLEU scores go both up and down when adding context sentences to the source side. We did not compare how data filtering impacts the final result, but filtering was only"
2020.wat-1.18,D19-5204,1,0.80896,"→EN MT; 2) there are no better data than more data - we see the biggest improvements from using larger training data sets; and 3) optimiser delay (Bogoychev et al., 2018) can help a lot - by setting the optimiser delay value to 8 instead of the default 1 increased BLEU scores by more than 1.5 in both translation directions. 2 Data We used multiple dataset combinations to train our models for the shared task. We also filtered some of the larger automatically collected data sets which are usually more noisy and contain duplicates. Aside from using only the provided BSD training dataset (BSD 20 (Rikters et al., 2019)), we had access to an extended version of the BSD four times the size (BSD 80), as well as two other similar corpora - AMI Meeting corpus (AMI) and a parallel version of OntoNotes 5.0 (ON) (Rikters et al., 2020). We also experimented with using the jParaCrawl (Morishita et al., 2019) dataset, data from WMT 20201 (whcih includes JParaCrawl, Ted Talks (Cettolo et al., 2012), The Kyoto Free Translation Task Corpus (Neubig, 2011), Japanese-English Subtitle Corpus (Pryzant et al., 2018), WikiMatrix (Schwenk et al., 2019) and Wiki Titles v2), and a proprietary document-aligned news dataset gathered"
2020.wat-1.18,2020.wmt-1.74,1,0.727581,"delay value to 8 instead of the default 1 increased BLEU scores by more than 1.5 in both translation directions. 2 Data We used multiple dataset combinations to train our models for the shared task. We also filtered some of the larger automatically collected data sets which are usually more noisy and contain duplicates. Aside from using only the provided BSD training dataset (BSD 20 (Rikters et al., 2019)), we had access to an extended version of the BSD four times the size (BSD 80), as well as two other similar corpora - AMI Meeting corpus (AMI) and a parallel version of OntoNotes 5.0 (ON) (Rikters et al., 2020). We also experimented with using the jParaCrawl (Morishita et al., 2019) dataset, data from WMT 20201 (whcih includes JParaCrawl, Ted Talks (Cettolo et al., 2012), The Kyoto Free Translation Task Corpus (Neubig, 2011), Japanese-English Subtitle Corpus (Pryzant et al., 2018), WikiMatrix (Schwenk et al., 2019) and Wiki Titles v2), and a proprietary document-aligned news dataset gathered from several sources. The full training data statistics are shown in Table 1. The AMI and jParaCrawl corpora contain many duplicates while the rest seem to be of higher quality. BSD 20 BSD 80 AMI ON WMT jParaCra"
2020.wat-1.18,W16-2209,0,0.0294688,"en following the domain adaptation tutorial2 . In this case we augmented the training data by adding a domain specifying tag (<AMI&gt;, <BSD&gt; or <ON&gt;) (Tars and Fishel, 2018) at the beginning of each source sentence of training, development and evaluation data. The domain tag approach lead to a similar increase in BLEU score as the usual domain adaptation approach. 148 2 https://awslabs.github.io/sockeye/tutorials/adapt.html 3.3 NMT with Context To train our context-aware systems, we experimented with the approach of sentence concatenation (Tiedemann and Scherrer, 2017) with source side factors (Sennrich and Haddow, 2016). We use the Sockeye toolkit and similar parameters as in our sentence-level systems. For the concatenation context-aware MT, we experimented with two approaches: 1) prepending the previous sentence from the same document, followed by a beginning of sentence tag <bos&gt;, to the source sentence; 2) in addition, providing source side factors to specify if a token represents context or the source sentence. The source side factors that we used for training were either C or S, representing context and the actual source sentence respectively. Examples of source sentences with context and factors are s"
2020.wat-1.18,W17-4811,0,0.012886,"did not work as well for models trained with Sockeye when following the domain adaptation tutorial2 . In this case we augmented the training data by adding a domain specifying tag (<AMI&gt;, <BSD&gt; or <ON&gt;) (Tars and Fishel, 2018) at the beginning of each source sentence of training, development and evaluation data. The domain tag approach lead to a similar increase in BLEU score as the usual domain adaptation approach. 148 2 https://awslabs.github.io/sockeye/tutorials/adapt.html 3.3 NMT with Context To train our context-aware systems, we experimented with the approach of sentence concatenation (Tiedemann and Scherrer, 2017) with source side factors (Sennrich and Haddow, 2016). We use the Sockeye toolkit and similar parameters as in our sentence-level systems. For the concatenation context-aware MT, we experimented with two approaches: 1) prepending the previous sentence from the same document, followed by a beginning of sentence tag <bos&gt;, to the source sentence; 2) in addition, providing source side factors to specify if a token represents context or the source sentence. The source side factors that we used for training were either C or S, representing context and the actual source sentence respectively. Exampl"
2020.wat-1.18,P12-3008,0,0.0674276,"Missing"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.74,E17-3017,0,0.0383282,"Missing"
2020.wmt-1.74,W18-6311,0,0.154952,"Missing"
2020.wmt-1.74,L18-1182,0,0.0217231,"xcept the rather noisy Open Subtitles corpus (Tiedemann, 2016). A document and sentence-aligned conversation parallel corpus should be advantageous to push MT research in this field to the next stage. In this Related Work There are many ready-to-use parallel corpora for training MT systems, but most of them are in written languages such as web crawl, patents (Goto et al., 2011), scientific papers (Nakazawa et al., 2016). Even though some parallel corpora are in spoken language, they are mostly monologues (Cettolo et al., 2012; Di Gangi et al., 2019) or contain a lot of noise (Tiedemann, 2016; Pryzant et al., 2018). Most of the MT evaluation campaigns such as WMT1 , WAT2 adopt the written language, monologue or noisy dialogue parallel corpora for their translation tasks. Among them, there is only one clean, dialogue parallel corpus (Salesky et al., 2018) adopted by IWSLT3 in the conversational speech translation task. JParaCrawl (Morishita et al., 2019) is a recently announced large English-Japanese parallel corpus built by crawling the web and aligning parallel 1 http://www.statmt.org/wmt20/ http://lotus.kuee.kyoto-u.ac.jp/WAT/ 3 http://workshop2019.iwslt.org 2 639 Proceedings of the 5th Conference on"
2020.wmt-1.74,D19-5204,1,0.801973,"d information - syntax and predicate argument structure, word sense linked to an ontology and coreference. We extracted the English subsets of broadcast conversation (BC) and telephone conversation (Tele), and had professional translators translate them into Japanese. Development and Evaluation Sets We provide balanced development and evaluation splits from only the BSD sub-corpus as it is the least noisy part. The documents in these sets are balanced in terms of scenes and original languages. The complete statistics are shown in Table 4. 3.2 Analysis We extend the analysis conducted for BSD (Rikters et al., 2019) to AMI and ON by investigating contextual information requirements for EN→JA 640 JA→EN Scene face-to-face phone call general chatting meeting training presentation sum EN→JA Doc. Sent. Doc. Sent. 535 279 233 224 37 17 1,325 16,481 8,720 7,674 7,647 1,379 499 42,400 458 256 239 265 47 53 1,318 14,858 7,770 7,372 8,952 1,549 1,899 42,400 Development Evaluation BSD AMI ON Table 3: English side word counts for each of the subcorpora and development/evaluation sets. Table 1: Document (Doc.) and sentence (Sent.) statistics for the full BSD corpus. JA→EN represents documents written in Japanese and"
2020.wmt-1.74,W16-2209,0,0.021606,"ber et al., 2017) to train transformer architecture (Vaswani et al., 2017) models with the transformer-base parameters until convergence on development data (no improvement on validation perplexity for 10 checkpoints). Each model was trained 3 times on a single Nvidia TITAN V (12GB) GPU. The reported BLEU score results are an average of 3 runs. Training time was about 2 days for models with only our data and about 5 days when using WMT data. To train our context-aware systems, we experimented with two approaches - sentence concatenation (Tiedemann and Scherrer, 2017) with source side factors (Sennrich and Haddow, 2016) and context-aware decoder (CADec (Voita et al., 2019)). We use the same toolkit and similar parameters as in our SL systems for the former and the CADec toolkit with the default parameters for the latter. For the concatenation context-aware MT, we experimented with two approaches: 1) prepending the previous sentence from the same document, followed by a beginning of sentence tag &lt;bos&gt;, to the source sentence; 2) in addition, providing source side factors to specify if a token represents context or the source sentence. The source side factors that we used for training were either C or S, repre"
2020.wmt-1.74,L16-1559,0,0.0156128,"ement in the context of dialogue or conversation translation. One typical case is the translation from a pro-drop language to a nonpro-drop language where correct pronouns must be supplemented according to the context. The omission of the pronouns occurs more frequently in spoken language than written language. Recently, context-aware MT models attract attention from many researchers (Tiedemann and Scherrer, 2017; Voita et al., 2019) to solve this kind of problem, however, there are almost no parallel conversation corpora with context information except the rather noisy Open Subtitles corpus (Tiedemann, 2016). A document and sentence-aligned conversation parallel corpus should be advantageous to push MT research in this field to the next stage. In this Related Work There are many ready-to-use parallel corpora for training MT systems, but most of them are in written languages such as web crawl, patents (Goto et al., 2011), scientific papers (Nakazawa et al., 2016). Even though some parallel corpora are in spoken language, they are mostly monologues (Cettolo et al., 2012; Di Gangi et al., 2019) or contain a lot of noise (Tiedemann, 2016; Pryzant et al., 2018). Most of the MT evaluation campaigns suc"
2020.wmt-1.74,W17-4811,0,0.219346,"machine translation (MT) for written text and monologue has vastly improved due to the increased amount of available parallel corpora and recent neural network technologies. However, there is much room for improvement in the context of dialogue or conversation translation. One typical case is the translation from a pro-drop language to a nonpro-drop language where correct pronouns must be supplemented according to the context. The omission of the pronouns occurs more frequently in spoken language than written language. Recently, context-aware MT models attract attention from many researchers (Tiedemann and Scherrer, 2017; Voita et al., 2019) to solve this kind of problem, however, there are almost no parallel conversation corpora with context information except the rather noisy Open Subtitles corpus (Tiedemann, 2016). A document and sentence-aligned conversation parallel corpus should be advantageous to push MT research in this field to the next stage. In this Related Work There are many ready-to-use parallel corpora for training MT systems, but most of them are in written languages such as web crawl, patents (Goto et al., 2011), scientific papers (Nakazawa et al., 2016). Even though some parallel corpora are"
2020.wmt-1.74,D18-1334,0,0.0275856,"Missing"
2020.wmt-1.74,P19-1116,0,0.120854,"Missing"
2021.mtsummit-research.19,D17-1098,0,0.0217668,"e evaluation of how well the code-switching method handles inflection of a pre-specified terminology when the terminology is given in the lemma form. Although the code-switching method is flexible, one disadvantage is that it tends to ignore the pre-specified terminology more often than the placeholder method (§5). We propose a placeholder method that handles inflection of pre-specified terms, aiming for both flexibility and faithfulness to terminology constraints. 2.3 Constrained Decoding Another approach to ensure that a pre-specified term appears in the translation is constrained decoding (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018). Constrained decoding can be applied to any existing NMT models without modifying its architecture and training regime, but imposes a significant cost on the decoding speed. It is also unclear how to incorporate lexical inflection into constrained decoding. Therefore, we focus on the placeholder and code-switching methods in this study. 2.4 Modeling Morphological Inflection in Neural Machine Translation Explicitly modeling morphological inflection into NMT models has been studied mainly to enable effective generalization over morphological variatio"
2021.mtsummit-research.19,P16-5005,0,0.0169338,"okens and restore the words in a post-processing step, which we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, some studies propose generating the content of the placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to t"
2021.mtsummit-research.19,P19-1294,0,0.11253,"he placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to the appropriately inflected form given the context. 2.2 The Code-switching Method Another way to introduce terminology constraints is the code-switching method (Song et al., 2019; Dinu et al., 2019; Exel et al., 2020). The model is trained with source sentences where some words are replaced or followed by specific target words and expected to copy the words to the translation. One advantage of the code-switching method is that, unlike the placeholder methods, it preserves the meaning of the original words, which likely leads to better translation quality. Also, the model can incorporate the specified terminology in a flexible way: a model trained with the code-switching method not only copies the pre-specified target words but can inflect the words according to the target-side context ("
2021.mtsummit-research.19,2021.dravidianlangtech-1.36,0,0.0828928,"Missing"
2021.mtsummit-research.19,W18-2501,0,0.0155329,"nces. Then, we split the dictionary into noun and verb entries to facilitate the analysis of the results and remove noise. If both the Japanese and English phrases are noun phrases, the entry is registered in the noun dictionary. If the Japanese phrase is a nominal verb5 and English is a verb, the entry is registered in the verb dictionary. In this study, we evaluate the model’s ability to inflect a provided lemma. Lemmas for the target language (English) are obtained with spaCy. 4.3 Models As the baseline, we implement a Transformer (Vaswani et al., 2017) translation model based on AllenNLP (Gardner et al., 2018). We configure the model in the Transformer-base setting and sentences are tokenized using sentencepiece (Kudo, 2018), which has a shared sourcetarget vocabulary of about 16k sub-words. The overviews of lexically constrained models are summarized in Fig. 2. Placeholder (PH). In the placeholder method, the model is trained to translate sentences with a placeholder token and pass that through to the translation. In our experiments, we use different placeholder tokens [NOUN] and [VERB] for nouns and verbs. Predicted placeholder tokens are replaced by the pre-specified term in the post-processing"
2021.mtsummit-research.19,P17-1141,0,0.028964,"l the code-switching method handles inflection of a pre-specified terminology when the terminology is given in the lemma form. Although the code-switching method is flexible, one disadvantage is that it tends to ignore the pre-specified terminology more often than the placeholder method (§5). We propose a placeholder method that handles inflection of pre-specified terms, aiming for both flexibility and faithfulness to terminology constraints. 2.3 Constrained Decoding Another approach to ensure that a pre-specified term appears in the translation is constrained decoding (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018). Constrained decoding can be applied to any existing NMT models without modifying its architecture and training regime, but imposes a significant cost on the decoding speed. It is also unclear how to incorporate lexical inflection into constrained decoding. Therefore, we focus on the placeholder and code-switching methods in this study. 2.4 Modeling Morphological Inflection in Neural Machine Translation Explicitly modeling morphological inflection into NMT models has been studied mainly to enable effective generalization over morphological variation of words. Tamchyna e"
2021.mtsummit-research.19,N03-1017,0,0.203332,"anese-to-English translation task in the scientific writing domain, and show that our model can incorporate specified terms in the correct form more successfully than other comparable models.1 1 Introduction Over the last several years, neural machine translation (NMT) has pushed the quality of machine translation to near-human performance (Sutskever et al., 2014; Vaswani et al., 2017). However, due to its end-to-end nature, this comes with the cost of losing a certain degree of control over the produced translation, which once was explicitly modeled, for example, in the form of phrase table (Koehn et al., 2003) in statistical machine translation (SMT). In practice, users often want to specify how certain words are translated in order to ensure the consistency of document-level translation or to guarantee the model to produce the correct translation for words that may be underrepresented in the training corpus such as proper nouns, technical terms, or novel words. Given this motivation, a line of previous research has investigated placeholder translation (Post et al., 2019). With a source sentence where certain words are replaced with a special placeholder token, the model produces a translation with"
2021.mtsummit-research.19,P18-1007,0,0.0237552,"oth the Japanese and English phrases are noun phrases, the entry is registered in the noun dictionary. If the Japanese phrase is a nominal verb5 and English is a verb, the entry is registered in the verb dictionary. In this study, we evaluate the model’s ability to inflect a provided lemma. Lemmas for the target language (English) are obtained with spaCy. 4.3 Models As the baseline, we implement a Transformer (Vaswani et al., 2017) translation model based on AllenNLP (Gardner et al., 2018). We configure the model in the Transformer-base setting and sentences are tokenized using sentencepiece (Kudo, 2018), which has a shared sourcetarget vocabulary of about 16k sub-words. The overviews of lexically constrained models are summarized in Fig. 2. Placeholder (PH). In the placeholder method, the model is trained to translate sentences with a placeholder token and pass that through to the translation. In our experiments, we use different placeholder tokens [NOUN] and [VERB] for nouns and verbs. Predicted placeholder tokens are replaced by the pre-specified term in the post-processing step. We evaluate three types of placeholder baselines, each of which differs in what inflected form the target place"
2021.mtsummit-research.19,W16-4602,0,0.0182396,"ecified term in the appropriately inflected form in the translation with higher accuracy than a comparable code-switching method. We also perform a careful error analysis to understand the weaknesses of each system and suggest directions for future work. 2 Related Work 2.1 Placeholder Translation To ensure that certain words appear in the translated sentence, previous studies have explored the method of replacing certain classes of words with special placeholder tokens and restore the words in a post-processing step, which we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, s"
2021.mtsummit-research.19,P15-1002,0,0.0318881,"ethod can include the specified term in the appropriately inflected form in the translation with higher accuracy than a comparable code-switching method. We also perform a careful error analysis to understand the weaknesses of each system and suggest directions for future work. 2 Related Work 2.1 Placeholder Translation To ensure that certain words appear in the translated sentence, previous studies have explored the method of replacing certain classes of words with special placeholder tokens and restore the words in a post-processing step, which we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token wit"
2021.mtsummit-research.19,2020.coling-main.348,0,0.0428395,"h we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, some studies propose generating the content of the placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to the appropriately inflected form given the context. 2.2 The Co"
2021.mtsummit-research.19,L16-1350,1,0.866457,"MT Research Track Page 231 Specified Translation: 管理 → controlling Source: フローセンサーの原理は浮遊式流量計のテーパー管内フロートの位置を差 動トランスで検出し,これの電圧制御により流量を[VERB]する。 Reference: The sensor controls the flow rate by detecting the position of the float in the tepered tube with a a differential transformer and [VERB] it with the obtained voltage. System Output: The principle of the flow sensor is that the position of the float in the taper tube of the floating flowmeter is detected by the differential transformer, and the flow rate is [VERB] by this voltage control. Table 1: A translation example from the ASPEC corpus (Nakazawa et al., 2016) with a placeholder translation model. The specified target term grammatically fits the placeholder in the reference, but not in the system output as it is. the produced translation. To illustrate the problem, we show an actual output from a normal placeholder translation model in Japanese to English translation in Table 1. The system is supposed to translate the word 管理 into controlling as in the reference, but the output has a different grammatical construction and thus the progressive form controlling is invalid in this context; instead, controlled should be injected in the placeholder. The"
2021.mtsummit-research.19,2021.eacl-main.70,0,0.0209484,"th source sentences where some words are replaced or followed by specific target words and expected to copy the words to the translation. One advantage of the code-switching method is that, unlike the placeholder methods, it preserves the meaning of the original words, which likely leads to better translation quality. Also, the model can incorporate the specified terminology in a flexible way: a model trained with the code-switching method not only copies the pre-specified target words but can inflect the words according to the target-side context (Dinu et al., 2019). In parallel to our work, Niehues (2021) offers a quantitative evaluation of how well the code-switching method handles inflection of a pre-specified terminology when the terminology is given in the lemma form. Although the code-switching method is flexible, one disadvantage is that it tends to ignore the pre-specified terminology more often than the placeholder method (§5). We propose a placeholder method that handles inflection of pre-specified terms, aiming for both flexibility and faithfulness to terminology constraints. 2.3 Constrained Decoding Another approach to ensure that a pre-specified term appears in the translation is c"
2021.mtsummit-research.19,P02-1040,0,0.109172,"are code-switched, but we observe no significant difference by adding source factors. Therefore, we simply report the results from the model with minimal components. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 236 and then only update the parameters of the additional modules. In this second training stage, we use the loss value as validation metric and stop the training when the lowest value is updated for 5 epochs. 5 Results 5.1 Evaluation For each model, we evaluate the overall translation quality with BLEU (Papineni et al., 2002).8 We also evaluate the specified term use rate, a metric to check if the model correctly includes the specified target term. Note that this is only an approximate measure of what we want to measure: whether the specified term is used in the correct form in the output translation. Since a single source sentence can be translated into different grammatical constructions, it is possible that the inflected form in the system output is different from the one in the reference but still correct in the context. Still, we find a substantial overlap in the inflectional form of the specified term betwee"
2021.mtsummit-research.19,W18-6319,0,0.033639,"Missing"
2021.mtsummit-research.19,W19-6618,0,0.0504017,"n degree of control over the produced translation, which once was explicitly modeled, for example, in the form of phrase table (Koehn et al., 2003) in statistical machine translation (SMT). In practice, users often want to specify how certain words are translated in order to ensure the consistency of document-level translation or to guarantee the model to produce the correct translation for words that may be underrepresented in the training corpus such as proper nouns, technical terms, or novel words. Given this motivation, a line of previous research has investigated placeholder translation (Post et al., 2019). With a source sentence where certain words are replaced with a special placeholder token, the model produces a translation with the special placeholder token in an appropriate position, and then that placeholder token is replaced with a pre-specified term in a post-processing step. Although this approach ensures that certain words appear in the translation, one limitation is that the user must specify the term that fits in the context surrounding the placeholder token, or specifically, the term should be properly inflected according to the syntactic structure of 1 Code is available at https:"
2021.mtsummit-research.19,N19-1044,0,0.0742236,"lly in translation between grammatically distant languages, such as Japanese and English. As manually correcting the inflection in post-editing significantly hurts the convenience of placeholder translation, we need a way to automatically handle inflection. One possible approach to this problem is the code-switching methods, in which certain words in the source sentence are replaced with the specific target words, and the model is encouraged to include those specific words in the translation. This approach is flexible in that the model can inflect the specified words according to the context (Song et al., 2019), but less faithful to the lexical constraints, often ignoring the specified terms (§5). To address this problem, we propose a model that automatically inflects a pre-specified term according to the context of the produced translation. We extend the sequence-to-sequence encoder and decoder with an additional character-level decoder that predicts the inflected form of the pre-specified term. Our approach combines the advantages of both the placeholder and the code-switching methods: the faithfulness to lexical constraints and the flexibility of dynamically deciding the word form in the output."
2021.mtsummit-research.19,W17-4704,0,0.0422079,"Missing"
2021.mtsummit-research.19,W17-4742,0,0.0141885,"ords. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, some studies propose generating the content of the placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to the appropriately inflected form given the context. 2.2 The Code-switching Method Another way to introduce terminology constraints is the code-switching method (Song et al., 2019; Dinu et al., 2019; Exel et al., 2020). The model is trained with source sentences where some words are replaced or followed by specific target words and expected to"
2021.mtsummit-research.19,2020.acl-main.389,0,0.0871391,"Missing"
2021.wat-1.1,2020.acl-main.703,0,0.187967,"news-commentary corpus.14 This year we also encouraged participants to use any corpora from WMT 202015 and WMT 202116 involving Japanese, Russian, and English as long as it did not belong to the news commentary domain to prevent any test set sentences from being unintentionally seen during training. 2,049 2,050 Table 5: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are Nway parallel. also encouraged the use of monolingual corpora expecting that it would be for pre-trained NMT models such as BART/MBART (Lewis et al., 2020; Liu et al., 2020). In Table 5 we give statistics of the aforementioned corpora which we used for the organizer’s baselines. Note that the evaluation corpora for both domains are created from documents and thus contain document level meta-data. Participants were encouraged to use document level approaches. Note that we do not exhaustively list6 all available corpora here and participants were not restricted from using any corpora as long as they are freely available. 2.7 Partition train development test train development test train development test 8 http://www.phontron.com/kftt/ https://data"
2021.wat-1.1,C18-2019,0,0.0200002,"task and 4 systems for the Japanese→ English.74 On the whole, all the submitted systems are basically lexical-constraint-aware NMT models with lexically constrained decoding method, where the restricted target vocabulary is concatenated into source sentences and, during the beam search at inference time, the models generate translation outputs containing the target vocabulary. We observed that these techniques boost the final translation performance of the NMT models in the restricted translation task. For human evaluation, we conducted the sourcebased direct assessment (Cettolo et al., 2017; Federmann, 2018) and source-based contrastive assessment (Sakaguchi and Van Durme, 2018; Federmann, 2018), to have the top-ranked systems of each team appraised by bilingual human annotators. In the human evaluation campaign, we also include the human reference data. Table 20 reports the final automatic evaluation score and the human evaluation results. In both tasks, the systems from the team “NTT” are the most highly evaluated in all the submitted systems in the final score and the human evaluation, consistently. We also note that our designed automation metric is well correlated Flickr30kEnt-JP Japanese↔En"
2021.wat-1.1,W18-1819,0,0.0552279,"Missing"
2021.wat-1.1,2007.mtsummit-papers.63,0,0.0610664,"om OPUS. We Test set II : A pair of test and reference sentences and context data that are articles including test sentences. The references were automatically extracted from English newswire sentences and manually selected. Therefore, the quality of the references of test set II is better than that of test set I. The statistics of JIJI Corpus are shown in Table 2. The definition of data use is shown in Table 3. Participants submit the translation results of one or more of the test data. The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from (Utiyama and Isahara, 2007). 2.5 ALT and UCSY Corpus The parallel data for Myanmar-English translation tasks at WAT2021 consists of two corpora, the ALT corpus and UCSY corpus. • The ALT corpus is one part from the Asian Language Treebank (ALT) project (Riza et al., 2016), consisting of twenty thousand Myanmar-English parallel sentences from news articles. • The UCSY corpus (Yi Mon Shwe Sin and Khin Mar Soe, 2018) is constructed by the NLP Lab, University of Computer Studies, 3 http://opus.nlpl.eu/ http://www.statmt.org/wmt20/ 5 Software Domain Evaluation Splits 4 3 Task Use Training Test set I Japanese to English Test"
2021.wat-1.11,2020.coling-main.304,0,0.0835648,"Missing"
2021.wat-1.11,N18-1118,0,0.0194207,"l computation at inference time, but significantly improves the accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation o"
2021.wat-1.11,D13-1095,0,0.611688,"most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let the translation model learn heuristic relations between ZPs and local context within the sentence (Hangyo et al., 2013; Kudo et al., 2015) and produce appropriate English pronouns. 2.3 sentence (Kudo et al., 2015), and incorporated into the resulting translation. On the other hand, in neural machine translation, the missing pronouns can be automatically inferred by the translation model because of the nature of end-to-end learning, although the correctness cannot be guaranteed. To improve the quality of ZP translation, previous studies have explored a multi-task approach with ZP prediction (Wang et al., 2016, 2019). In this study, we propose a ZP data augmentation method to provide additional training signals"
2021.wat-1.11,P09-2022,0,0.116993,"Missing"
2021.wat-1.11,W03-1024,0,0.856685,"e also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntactically required on the target language side: the model has to infer the omitted pronoun. The task of identifying the omitted pronouns is called ZP resolution and for Japanese, this has been a long-standing problem (Isozaki and Hirao, 2003; Sasano et al., 2008; Imamura et al., 2009; Shibata and Kurohashi, 2018). Japanese is one of the most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let t"
2021.wat-1.11,2020.acl-main.321,0,0.0160392,"cantly improves the accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresp"
2021.wat-1.11,A92-1028,0,0.643315,"lel corpus in the conversational domain. Besides the published data, we also use the in-house version of the corpus, which amounts to a total of 104,961 sentence pairs. 3.1 Identifying sentence pairs that contain ZPs. As the corpus does not contain annotations of ZPs, we first identify sentence pairs that contain zero pronouns. We exploit the word alignment information from parallel sentences to detect ZPs. The specific procedure is as follows. ZPs in Translation In the context of statistical machine translation, Japanese ZPs are explicitly predicted by considering verbal semantic attributes (Nakaiwa and Ikehara, 1992), local context in the source and target 118 1. We obtain the word alignments of the parallel data with GIZA++1 . We use Mecab2 for Japanese word segmentation, spaCy3 for English. 2. When a pronoun in an English sentence is associated with NULL, the pronoun in the English sentence is considered to correspond to a ZP in the Japanese sentence. The resulting number of pronouns is shown in Figure 2. It can be seen that in the conversational domain, the first person pronoun I and the second 1 https://github.com/moses-smt/giza-pp https://taku910.github.io/mecab/ 3 https://spacy.io/ 2 baseline logist"
2021.wat-1.11,P02-1040,0,0.109125,"ore difficult for the model to find correlations between ZPs and local context. 5 Conclusion Table 3: The number of sentences in the corpus. Model Transformer (Vaswani et al., 2017) was used as the translation model. We adopt the hyperparameters recommended for the corpus of our size in Araabi and Monz (2020) (Appendix B). In addition to the single-sentence translation, we also experimented with the 2to1 setting (Tiedemann and Scherrer, 2017), in which the previous sentence in the document is added to the input. Evaluation We evaluate the overall translation quality on the test set with BLEU (Papineni et al., 2002). We also conduct a targeted evaluation with the ZP evaluation dataset for Japanese-to-English translation (Shimazu et al., 2020). The ZP evaluation dataset contains 724 triples of a source sentence, a target sentence with a correct pronoun, and one with an incorrect pronoun. To evaluate a translation model, we see if the model assigns a lower perplexity to the correct target sentence, and calculate the accuracy. 4.2 To address the problem of zero pronoun translation, we proposed zero pronoun data augmentation. Through the analysis with the Japanese-English conversational parallel corpus, we s"
2021.wat-1.11,D19-5204,1,0.850314,"ZP translation, previous studies have explored a multi-task approach with ZP prediction (Wang et al., 2016, 2019). In this study, we propose a ZP data augmentation method to provide additional training signals useful to correctly translate ZPs. 3 Is Local Context Useful for Predicting Zero Pronouns? Our proposed method is based on the assumption that local context in Japanese sentences is useful for predicting ZPs. We begin by analyzing to what extent ZPs can be inferred from local context, and what kind of local context is useful. For the analysis, we use the Business Scene Dialogue Corpus (Rikters et al., 2019), which is a Japanese and English parallel corpus in the conversational domain. Besides the published data, we also use the in-house version of the corpus, which amounts to a total of 104,961 sentence pairs. 3.1 Identifying sentence pairs that contain ZPs. As the corpus does not contain annotations of ZPs, we first identify sentence pairs that contain zero pronouns. We exploit the word alignment information from parallel sentences to detect ZPs. The specific procedure is as follows. ZPs in Translation In the context of statistical machine translation, Japanese ZPs are explicitly predicted by c"
2021.wat-1.11,2020.wmt-1.74,1,0.843243,"Missing"
2021.wat-1.11,C08-1097,0,0.67291,"of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntactically required on the target language side: the model has to infer the omitted pronoun. The task of identifying the omitted pronouns is called ZP resolution and for Japanese, this has been a long-standing problem (Isozaki and Hirao, 2003; Sasano et al., 2008; Imamura et al., 2009; Shibata and Kurohashi, 2018). Japanese is one of the most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let the translation model"
2021.wat-1.11,2020.acl-main.693,0,0.0228025,"he accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntac"
2021.wat-1.11,P18-1054,0,0.012562,"machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntactically required on the target language side: the model has to infer the omitted pronoun. The task of identifying the omitted pronouns is called ZP resolution and for Japanese, this has been a long-standing problem (Isozaki and Hirao, 2003; Sasano et al., 2008; Imamura et al., 2009; Shibata and Kurohashi, 2018). Japanese is one of the most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let the translation model learn heuristic relations between ZPs and local cont"
2021.wat-1.11,2020.lrec-1.447,1,0.859112,"Missing"
2021.wat-1.11,W17-4811,0,0.0610415,"Missing"
2021.wat-1.11,D19-1081,0,0.0378569,"Missing"
2021.wat-1.11,P19-1116,0,0.0173133,"rence time, but significantly improves the accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challe"
2021.wat-1.11,D19-1085,0,0.0351255,"Missing"
2021.wat-1.11,N16-1113,0,0.022761,"t the translation model learn heuristic relations between ZPs and local context within the sentence (Hangyo et al., 2013; Kudo et al., 2015) and produce appropriate English pronouns. 2.3 sentence (Kudo et al., 2015), and incorporated into the resulting translation. On the other hand, in neural machine translation, the missing pronouns can be automatically inferred by the translation model because of the nature of end-to-end learning, although the correctness cannot be guaranteed. To improve the quality of ZP translation, previous studies have explored a multi-task approach with ZP prediction (Wang et al., 2016, 2019). In this study, we propose a ZP data augmentation method to provide additional training signals useful to correctly translate ZPs. 3 Is Local Context Useful for Predicting Zero Pronouns? Our proposed method is based on the assumption that local context in Japanese sentences is useful for predicting ZPs. We begin by analyzing to what extent ZPs can be inferred from local context, and what kind of local context is useful. For the analysis, we use the Business Scene Dialogue Corpus (Rikters et al., 2019), which is a Japanese and English parallel corpus in the conversational domain. Beside"
C12-1120,N10-1015,0,0.0237969,"ts and derivations, respectively. sentence “音 列 の (of tone sequence)” has no corresponding part in the English sentence. “音 列 (tone sequence)” is correctly aligned to NULL, but the function word “の (of)” is incorrectly derived from “法 (method)”. On the right of Figure 6, the Japanese “HDMI は” should not depend on “異なる (different)”, but on “使用 (use)”. Because of this parsing error, the topic marker “は” is incorrectly derived from “異なる (different)”. One possible short-term solution for the parsing problem is to use the n-best parsing results in the model. An alternative solution was proposed by Burkett et al. (2010), who described a joint parsing and alignment model that exchanges useful information between the parser and aligner. 7 Translation experiments We conducted English-to-Japanese and Japanese-to-English translation experiments on the same corpus used in the alignment experiments. We translated 500 paper abstract sentences from the JST corpus. Note that these sentences were not included in the training corpus. We use Joshua4 , a Java-based opensource implementation of the hierarchical decoder, version 4.0 (Ganitkevitch et al., 2012) with default settings. It was tuned using another 500 developmen"
C12-1120,P05-1022,0,0.695573,"ees, function words giving additional information to content words are placed as children of the content words, thus it preserves the dependency relations between words over languages. In the semantic-head dependency tree (on the right of Figure 2), “medical treatment ↔ 治療”, “may ↔ かも しれ ない”, “not ↔ ない” and “weight ↔ 体重” are all children of “change ↔ 変化”, while the relations are not preserved in the syntactic-head dependency tree (on the left of Figure 2). Because of these advantages, our model uses semantic-head dependency trees. In this paper, English sentences are first parsed by nlparser (Charniak and Johnson, 2005) which outputs phrase structures that are then converted into word dependency trees by defining the head word for phrases. The conversion rules follow Collins’ head percolation table (Collins, 1999) with some modifications for acquiring semantic-head dependency trees. The following head-specifying rules are examples in which the syntactic head (underlined) and the semantic head (double underlined) is different. • VP → MD • VP → VBZ VB (ex. &quot;may change&quot; in Figure 2) JJ (ex. &quot;is large&quot;) Japanese sentences are usually parsed based on a unit called a basic phrase, which consists of one content wor"
C12-1120,D08-1033,0,0.0429933,"Missing"
C12-1120,P07-1003,0,0.025874,"dependency structures by rules defining head words for phrases (Collins, 1999). Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994) and the dependency analyzer KNP (Kawahara and Kurohashi, 2006b). For comparison, we used GIZA++ (Och and Ney, 2003), which implements the well-known word-based statistical alignment model of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristics (Koehn et al., 2003). We also tested the BerkeleyAligner3 (DeNero and Klein, 2007) in the unsupervised training mode with default settings. 6.2 Experimental result and discussion The experimental results are given in Table 2. “Syntactic-head” is the alignment accuracy of the baseline system by Nakazawa and Kurohashi (2011), while “Semantic-head w/o derivation” is the result of using the baseline model on semantic-head dependency trees. The results of incorporating the monolingual derivation are given in the bottom two rows, where “all” means that we evaluated all the alignments including derivations, while “core” means that we only evaluated the core alignments. As mentione"
C12-1120,W12-3134,0,0.0297125,"parsing results in the model. An alternative solution was proposed by Burkett et al. (2010), who described a joint parsing and alignment model that exchanges useful information between the parser and aligner. 7 Translation experiments We conducted English-to-Japanese and Japanese-to-English translation experiments on the same corpus used in the alignment experiments. We translated 500 paper abstract sentences from the JST corpus. Note that these sentences were not included in the training corpus. We use Joshua4 , a Java-based opensource implementation of the hierarchical decoder, version 4.0 (Ganitkevitch et al., 2012) with default settings. It was tuned using another 500 development sentence pairs. Table 3 shows the BLEU (Papineni et al., 2002) scores for the translations. The proposed 4 http://joshua-decoder.org 1975 ! ! ! ! &quot;&quot;&quot;! Figure 6: Alignment errors of the proposed model caused by a NULL part (left) and a parsing error (right). Alignment model En → Ja Ja → En GIZA++ & grow-diag-final-and 23.84 17.75 Syntactic-head (baseline) 24.16 17.83 Semantic-head w/o derivation 24.11 18.06 Semantic-head w/ derivation (all) 24.55† 18.46†‡ 24.45 17.76 Semantic-head w/ derivation (core) Table 3: BLEU scores for En"
C12-1120,W09-1201,0,0.0937976,"Missing"
C12-1120,P09-2059,0,0.048517,"Missing"
C12-1120,W10-1736,0,0.0251259,"Missing"
C12-1120,kawahara-kurohashi-2006-case,1,0.779601,"uency with which deC is connected to A(eC , deC ) in the monolingual corpus. Taking each sentence in Figure 3 as an example sentence in the monolingual corpus, we can enumerate the derivations shown in Table 1 from the sentences. A derivation must be contiguous as a tree, and we do not consider sibling derivations. We distinguish three types of derivations: parent, pre-child (dependent from the left) and post-child (dependent from the right). This lexicalized derivation is excessively specific. For example, the highest probability derivations from “Ph.D.” acquired from the English Web corpus (Kawahara and Kurohashi, 2006a) are “a”, “student”, “thesis” in order. Consequently, using only lexicalized derivation can cause many derivation errors. We consider not only the lexicalized derivation probability, but also another probability using part-of-speech (POS) is used as the condition. Using the notation A pos (eC , deC ) for the POS of the anchor word, the monolingual derivation probability is defined as: 1 (11) p(deC |eC ) = [p(deC |A(eC , deC )) · p(deC |A pos (eC , deC ))] 2 . p(deC |A pos (eC , deC )) is also acquired from the large monolingual corpus in the same manner as p(deC |A(eC , deC )). We take the g"
C12-1120,N06-1023,1,0.945061,"uency with which deC is connected to A(eC , deC ) in the monolingual corpus. Taking each sentence in Figure 3 as an example sentence in the monolingual corpus, we can enumerate the derivations shown in Table 1 from the sentences. A derivation must be contiguous as a tree, and we do not consider sibling derivations. We distinguish three types of derivations: parent, pre-child (dependent from the left) and post-child (dependent from the right). This lexicalized derivation is excessively specific. For example, the highest probability derivations from “Ph.D.” acquired from the English Web corpus (Kawahara and Kurohashi, 2006a) are “a”, “student”, “thesis” in order. Consequently, using only lexicalized derivation can cause many derivation errors. We consider not only the lexicalized derivation probability, but also another probability using part-of-speech (POS) is used as the condition. Using the notation A pos (eC , deC ) for the POS of the anchor word, the monolingual derivation probability is defined as: 1 (11) p(deC |eC ) = [p(deC |A(eC , deC )) · p(deC |A pos (eC , deC ))] 2 . p(deC |A pos (eC , deC )) is also acquired from the large monolingual corpus in the same manner as p(deC |A(eC , deC )). We take the g"
C12-1120,W04-3250,0,0.0209261,"or the translations. The proposed 4 http://joshua-decoder.org 1975 ! ! ! ! &quot;&quot;&quot;! Figure 6: Alignment errors of the proposed model caused by a NULL part (left) and a parsing error (right). Alignment model En → Ja Ja → En GIZA++ & grow-diag-final-and 23.84 17.75 Syntactic-head (baseline) 24.16 17.83 Semantic-head w/o derivation 24.11 18.06 Semantic-head w/ derivation (all) 24.55† 18.46†‡ 24.45 17.76 Semantic-head w/ derivation (core) Table 3: BLEU scores for English-to-Japanese and Japanese-to-English translation experiments. † and ‡ marks indicate significant difference by bootstrap resampling (Koehn, 2004) from the decoder using GIZA++ & grow-diag-final-and alignment and baseline alignment respectively (p &lt; 0.05). model using all the alignments including derivations achieved the best translation quality. We believe this improvement is due to the reduction in function word alignment errors. The BLEU score decreased when only core alignments were used. This is because the exclusion of the derivations increased the ambiguity of the translation rules. Conclusion and future work In this paper, we proposed a novel approach for handling unique function words based on semantic-head dependency trees. Th"
C12-1120,P07-2045,0,0.010475,"Missing"
C12-1120,N03-1017,0,0.0194021,"k and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases (Collins, 1999). Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994) and the dependency analyzer KNP (Kawahara and Kurohashi, 2006b). For comparison, we used GIZA++ (Och and Ney, 2003), which implements the well-known word-based statistical alignment model of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristics (Koehn et al., 2003). We also tested the BerkeleyAligner3 (DeNero and Klein, 2007) in the unsupervised training mode with default settings. 6.2 Experimental result and discussion The experimental results are given in Table 2. “Syntactic-head” is the alignment accuracy of the baseline system by Nakazawa and Kurohashi (2011), while “Semantic-head w/o derivation” is the result of using the baseline model on semantic-head dependency trees. The results of incorporating the monolingual derivation are given in the bottom two rows, where “all” means that we evaluated all the alignments including derivations, while “core”"
C12-1120,W02-1018,0,0.0479938,"Missing"
C12-1120,I11-1089,1,0.605419,"he tree structure. Another advantage of the monolingual derivation model is that it can reduce the gaps between alignments, and preserve the dependency relations between treelets over languages. For example, the model can derive “に より (by)” from “変化 し (change)” or “ 治療 (medical treatment)” and let the dependency relation between “変化 し (change)” and “治 療 (medical treatment)” be direct parent-child like their English counterparts. This is effective for estimating the dependency relation probability described in Section 4.3. 4 Model overview The proposed model is an extension of that proposed by Nakazawa and Kurohashi (2011). This earlier model was overcoming the long-distance reordering issue by incorporating dependency trees. However, it was suffering from alignment errors for function words, which our new model solves by incorporating a monolingual derivation model. First we describe the generative story for the joint alignment model in the same manner as in previous work (Marcu and Wong, 2002; DeNero et al., 2008; Nakazawa and Kurohashi, 2011). 1. Generate ℓ concepts from which bilingual treelet pairs are generated independently. 2. For each treelet pair, derive zero or more treelets monolingually from each t"
C12-1120,J03-1002,0,0.0317637,"ffectiveness of the proposed model. 6.1 Settings For the experiments, we used the JST1 paper abstract corpus. This corpus was created by NICT2 from JST’s 2M English-Japanese paper abstract corpus using the method of Utiyama and Isahara (2007). This corpus consists of 996K parallel sentences: 24.7M words in English and 27.5M words in Japanese. Unfortunately, this corpus is not publicly available now, but they will become available in the near future. As gold-standard data, 500 sentence pairs were annotated by hand using two types of annotations: sure (S) alignments and possible (P) alignments (Och and Ney, 2003). The unit of evaluation was the word. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are as follows: p$ = 0.1, pd = 0.9, pN = 0.1, p t = 0.8, αA = 100, αN = 100, α f e = 100, αe f = 100, p f e = 0.5, pe f = 0.5. They are borrowed from the previous work (DeNero et al., 2008; Nakazawa and Kurohashi, 2011) and changed a little. The training time was about 1 day using 200 CPU cores. It is much slower than the word-sequence-based models because"
C12-1120,P02-1040,0,0.083906,"ent model that exchanges useful information between the parser and aligner. 7 Translation experiments We conducted English-to-Japanese and Japanese-to-English translation experiments on the same corpus used in the alignment experiments. We translated 500 paper abstract sentences from the JST corpus. Note that these sentences were not included in the training corpus. We use Joshua4 , a Java-based opensource implementation of the hierarchical decoder, version 4.0 (Ganitkevitch et al., 2012) with default settings. It was tuned using another 500 development sentence pairs. Table 3 shows the BLEU (Papineni et al., 2002) scores for the translations. The proposed 4 http://joshua-decoder.org 1975 ! ! ! ! &quot;&quot;&quot;! Figure 6: Alignment errors of the proposed model caused by a NULL part (left) and a parsing error (right). Alignment model En → Ja Ja → En GIZA++ & grow-diag-final-and 23.84 17.75 Syntactic-head (baseline) 24.16 17.83 Semantic-head w/o derivation 24.11 18.06 Semantic-head w/ derivation (all) 24.55† 18.46†‡ 24.45 17.76 Semantic-head w/ derivation (core) Table 3: BLEU scores for English-to-Japanese and Japanese-to-English translation experiments. † and ‡ marks indicate significant difference by bootstrap res"
C12-1120,2007.mtsummit-papers.63,0,0.0353745,"ed treelet. If an unaligned treelet is next to an aligned one, EXPAND merges the unaligned and aligned treelets, either as a part of core treelet or derivation treelet. As the opposite direction, it excludes a marginal node from a treelet, and to make the excluded node unaligned. 6 Alignment experiments We conducted alignment experiments on the English-Japanese corpus to show the effectiveness of the proposed model. 6.1 Settings For the experiments, we used the JST1 paper abstract corpus. This corpus was created by NICT2 from JST’s 2M English-Japanese paper abstract corpus using the method of Utiyama and Isahara (2007). This corpus consists of 996K parallel sentences: 24.7M words in English and 27.5M words in Japanese. Unfortunately, this corpus is not publicly available now, but they will become available in the near future. As gold-standard data, 500 sentence pairs were annotated by hand using two types of annotations: sure (S) alignments and possible (P) alignments (Och and Ney, 2003). The unit of evaluation was the word. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in"
C12-1120,P11-1003,0,0.0234908,"Missing"
C12-1120,N09-1028,0,0.0297784,"Missing"
C12-1120,J93-2003,0,\N,Missing
C12-1120,J03-4003,0,\N,Missing
chu-etal-2012-chinese,chou-etal-2008-extended,0,\N,Missing
chu-etal-2012-chinese,W08-1907,0,\N,Missing
chu-etal-2012-chinese,I05-1059,0,\N,Missing
chu-etal-2012-chinese,chou-huang-2006-hantology,0,\N,Missing
chu-etal-2014-constructing,C04-1151,0,\N,Missing
chu-etal-2014-constructing,J93-2003,0,\N,Missing
chu-etal-2014-constructing,J05-4003,0,\N,Missing
chu-etal-2014-constructing,P07-2045,0,\N,Missing
chu-etal-2014-constructing,W13-2505,1,\N,Missing
chu-etal-2014-constructing,P09-2057,0,\N,Missing
chu-etal-2014-constructing,P03-1010,0,\N,Missing
chu-etal-2014-constructing,2012.eamt-1.7,1,\N,Missing
D16-1049,W13-2201,0,0.479219,"i, Sakyo-ku, Kyoto, Japan 2 Japan Science and Technology Agency, Kawaguchi-shi, Saitama, Japan otani.naoki.65v@st.kyoto-u.ac.jp nakazawa@pa.jst.jp Abstract Thus, many previous studies focused on pairwise comparisons instead of absolute evaluations. The same task is given to multiple workers, and their responses are aggregated to obtain a reliable answer. We must, therefore, develop methods that robustly estimate the MT performance based on many pairwise comparisons. Some aggregation methods have been proposed for MT competitions hosted by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013; Hopkins and May, 2013; Sakaguchi et al., 2014), where a ranking of the submitted systems is produced by aggregating many manual judgments of pairwise comparisons of system outputs. However, existing methods do not consider the following important issues. Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system’s performance. Our method applies gra"
D16-1049,J15-2005,0,0.0181214,"resented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons increases rapidly as the number of systems increases. 3 Problem Setting We first describe the problem setting, as shown in Figure 1. Assume that there are a group of systems I indexed by i, a set of segments J indexed by j, and a set of judges K indexed by k. Before a manual evaluation, we fix an arbitrary basel"
D16-1049,goto-etal-2014-crowdsourcing,0,0.0380694,"Missing"
D16-1049,D15-1052,0,0.0127855,"used to produce the WMT13 official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons"
D16-1049,P13-1139,0,0.382213,"Japan 2 Japan Science and Technology Agency, Kawaguchi-shi, Saitama, Japan otani.naoki.65v@st.kyoto-u.ac.jp nakazawa@pa.jst.jp Abstract Thus, many previous studies focused on pairwise comparisons instead of absolute evaluations. The same task is given to multiple workers, and their responses are aggregated to obtain a reliable answer. We must, therefore, develop methods that robustly estimate the MT performance based on many pairwise comparisons. Some aggregation methods have been proposed for MT competitions hosted by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013; Hopkins and May, 2013; Sakaguchi et al., 2014), where a ranking of the submitted systems is produced by aggregating many manual judgments of pairwise comparisons of system outputs. However, existing methods do not consider the following important issues. Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system’s performance. Our method applies graded response model from"
D16-1049,2012.iwslt-papers.5,0,0.085833,"nts to produce reliable rankings. 1 We also show that our method accurately replicated the WMT13 official system scores using a few comparisons. However, this is not the main focus of this paper. 512 Figure 1: Illustration of manual pairwise comparison. Each system yields translations. Judges compare them with a baseline translation and report their preferences. Our goal is to aggregate the judgments to determine the performance of each system. Frequency based approaches were used to produce the WMT13 official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estima"
D16-1049,P15-2097,0,0.0240859,"official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons increases rapidly as t"
D16-1049,W14-3301,0,0.0892015,"Missing"
D16-1049,W15-3001,0,\N,Missing
D16-1247,P05-1022,0,0.0860784,"Missing"
D16-1247,P05-1033,0,0.032199,"are not necessarily adjuncts, but any words or phrases. For example, suppose the Japanese input sentence in Figure 1 has “ 突然 (suddenly)”, but the training corpus provides only a translation rule without the word. In this case we cannot directly use the rule for the translation because it does not know where to insert the translation of the floating word in the output. As another example, there is no context information available for the children of the OOV word in the input sentence, so we need some special process to translate them. Previous work deals with this problem by using glue rules (Chiang, 2005) or limiting the dependency structures to be well-formed (Shen et al., 2008). Richardson et al. (2016) introduces the concept of flexible non-terminals. It provides multiple possible insertion positions for the floating subtree rather than fixed insertion positions. A possible insertion position must satisfy the following conditions: • it must be a child of the aligned word of the parent of the floating subtree 2271 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2271–2277, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Lin"
D16-1247,D14-1063,1,0.845883,"ed”. Also, insertion positions do not violate the projectivity of the target tree. Flexible non-terminals are analogous to the auxiliary tree of the tree adjoining grammars (TAG) (Joshi, 1985), which is successfully adopted in machine translation (DeNeefe and Knight, 2009). The difference is that TAG is defined on the constituency trees rather than the dependency trees. Flexible non-terminals are powerful to handle floating subtrees and it achieve better translation quality. However the computational cost of decoding becomes high even though they are compactly represented in the lattice form (Cromieres and Kurohashi, 2014). In our experiments, using flexible nonterminals causes the decoding to be 3 to 6 times slower than when they are not used. Flexible nonterminals increase the number of translation rules because the insertion positions are selected during the decoding. However, we think it is possible to restrict possible insertion positions or even select only one insertion position by looking at the tree structures on both sides. In this paper, we propose a method to select the appropriate insertion position before decoding. This can not only reduce the decoding time but also improve the translation quality"
D16-1247,D09-1076,0,0.029638,"mber 1-5, 2016. 2016 Association for Computational Linguistics • it must not violate the projectivity of the dependency tree For example, possible insertion positions for the floating word “突然” are shown in gray arrows in Figure 1. Since “突然” is a child of “電話する”, and the translation of “電話する” is “called”, insertion positions must be a child of “called”. Also, insertion positions do not violate the projectivity of the target tree. Flexible non-terminals are analogous to the auxiliary tree of the tree adjoining grammars (TAG) (Joshi, 1985), which is successfully adopted in machine translation (DeNeefe and Knight, 2009). The difference is that TAG is defined on the constituency trees rather than the dependency trees. Flexible non-terminals are powerful to handle floating subtrees and it achieve better translation quality. However the computational cost of decoding becomes high even though they are compactly represented in the lattice form (Cromieres and Kurohashi, 2014). In our experiments, using flexible nonterminals causes the decoding to be 3 to 6 times slower than when they are not used. Flexible nonterminals increase the number of translation rules because the insertion positions are selected during the"
D16-1247,W04-3250,0,0.110387,"SMT 18.45 64.51 - 27.48 68.37 - 27.96 78.90 - 34.65 77.25 Hiero 18.72 65.11 - 30.19 73.47 - 27.71 80.91 - 35.43 81.04 No Flexible 20.28 65.08 1.00 28.77 75.21 1.00 24.85 66.60 1.00 30.51 73.08 1.00 Baseline 21.61 69.82 6.28 30.57 76.13 3.30 28.79 78.11 5.16 34.32 77.82 5.28 Proposed 22.07† 70.49† 2.25 30.50 76.69† 1.27 29.83† 79.73† 2.21 34.71† 79.25† 1.89 Table 4: The results of the translation experiments. † means the Proposed method achieved significantly better score than the Baseline (p &lt; 0.01). 2002) and RIBES (Isozaki et al., 2010) with the significance testing by bootstrap resampling (Koehn, 2004). RIBES is more sensitive to word order than BLEU, so we expect an improvement in RIBES. We also investigated relative decoding time compared to the No Flexible setting. Note that we used the word “decoding” for only exploring the search space, and it does not include constructing the search space (as the table lookup in Phrase-based SMT). Our whole translation process is: 1. translation rule extraction 2. insertion-position selection 3. decoding At the time of the second step, we have all the translation rules applicable to the input sentence. The computation time for each step is 3 ≫ 1 ≫ 2 s"
D16-1247,J94-4001,1,0.321808,"Missing"
D16-1247,P15-2047,0,0.0263548,"Missing"
D16-1247,D15-1279,0,0.0372074,"Missing"
D16-1247,P02-1040,0,0.0995467,"Missing"
D16-1247,P14-5014,1,0.907225,"Missing"
D16-1247,N16-1002,1,0.844414,"Missing"
D16-1247,D11-1046,0,0.0232715,"→ Ja Ja → Zh Zh → Ja 15.7M 5.7M 160K 58K 160K 58K 3.39 3.15 3.72 3.41 89 71 61 79 0.089 0.058 0.105 0.056 97.08 97.72 96.51 97.99 55.00 89.03 68.04 83.16 Table 2: The statistics of the data and results of the insertion position selection experiments. sentences. English sentences are first parsed by nlparser (Charniak and Johnson, 2005) and then converted into word dependency trees using Collins’ head percolation table (Collins, 1999). We used Chinese word segmenter KKN (Shen et al., 2014) and dependency parser SKP (Shen et al., 2012) for Chinese sentences. The supervised word alignment Nile (Riesa et al., 2011) was used. We used a state-of-the-art dependency tree-to-tree decoder (Richardson et al., 2014) with the default settings. The neural network is constructed and trained using the Chainer (Tokui et al., 2015). 3.1 Insertion Position Selection The training, development and test data for the neural network is automatically generated by the procedure explained in Section 2.2. The size of the generated data from the ASPEC and the average number of insertion positions for each floating subtree are shown in Table 2. We trained the model for 100 epochs and used the best model on the development data f"
D16-1247,P08-1066,0,0.0357112,"ppose the Japanese input sentence in Figure 1 has “ 突然 (suddenly)”, but the training corpus provides only a translation rule without the word. In this case we cannot directly use the rule for the translation because it does not know where to insert the translation of the floating word in the output. As another example, there is no context information available for the children of the OOV word in the input sentence, so we need some special process to translate them. Previous work deals with this problem by using glue rules (Chiang, 2005) or limiting the dependency structures to be well-formed (Shen et al., 2008). Richardson et al. (2016) introduces the concept of flexible non-terminals. It provides multiple possible insertion positions for the floating subtree rather than fixed insertion positions. A possible insertion position must satisfy the following conditions: • it must be a child of the aligned word of the parent of the floating subtree 2271 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2271–2277, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics • it must not violate the projectivity of the dependency tree For e"
D16-1247,Y12-1033,1,0.907194,"Missing"
D16-1247,P14-2042,1,0.862962,"Missing"
D16-1247,J03-4003,0,\N,Missing
D16-1247,D10-1092,0,\N,Missing
D19-5201,E06-1031,0,0.0437063,"ion web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 http://lotus.kuee.kyoto-u.ac.jp/WAT/ km-en-d"
D19-5201,W16-4601,1,0.763733,"Missing"
D19-5201,W19-6613,1,0.928331,"Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each particJaRuNC Corpus For the Russian↔Japanese task we asked participants to use the JaRuNC corpus5 (Imankulova et al., 2019) which belongs to the news commentary domain. This dataset was manually aligned and cleaned and is trilingual. It can be used to evaluate Russian↔English 6 http://www.phontron.com/kftt/ https://datarepository.wolframcloud.com/ resources/Japanese-English-Subtitle-Corpus 8 https://wit3.fbk.eu/ 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 10 https://cms.unov.org/UNCorpus/ 11 https://translate.yandex.ru/corpus?lang=en 12 http://lotus.kuee.kyoto-u.ac.jp/WAT/ News-Commentary/news-commentary-v14.en-ru. filtered.tar.gz 7 4 http://ufal.mff.cuni.cz/~ramasamy/parallel/ html/ 5 https://github.com/aizhanti/JaR"
D19-5201,W17-5701,1,0.779618,"Missing"
D19-5201,D10-1092,0,0.105912,"t of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for each task were calculated using the corresponding reference translations. The automatic evaluation system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provid"
D19-5201,W14-7001,1,0.794275,"t 400 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. WAT receives submissions at any time; i.e., there is no submission deadline of translation results w.r.t automatic evaluation of translation quality. Introduction The Workshop on Asian Translation (WAT) is an open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2018 (Nakazawa et al., 2014, 2015, 2016, 2017, 2018), WAT2019 brings together machine 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/ 3 https://ufal.mff.cuni.cz/hindi-visual-genome/ wat-2019-multimodal-task 1 One paper was withdrawn post acceptance and hence only 6 papers will be in the proceedings. 1 Proceedings of the 6th Workshop on Asian Translation, pages 1–35 Hong Kong, China, November 4, 2019. ©2019 Association for Computational Linguistics Lang JE JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Lang zh-ja ko-ja en-ja Test 1,812 2,107 Lang zh-ja ko-ja en-ja Table 1: Statistics for ASPEC Dataset"
D19-5201,W15-5001,1,0.855752,"Missing"
D19-5201,P17-4012,0,0.147541,"MT RBMT Other Other ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ja-hi ✓ ✓ ✓ ✓ EnTam ta-en en-ta ✓ ✓ IITB en-hi hi-ja ✓ ✓ ✓ ✓ TDDC ja-en ✓ hi-en ✓ ✓ NewsCommentary ru-ja ja-ru JIJI ja-en en-ja ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ALT en-my km-en ✓ ✓ Multimodal en-hi ✓ my-en ✓ ✓ en-km ✓ model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko16 for Korean segmentation. • Indic NLP Library17 for Indic language segmentation. • The tools included in the ALT corpus for Myanmar and Khmer segmentation. • subword-nmt18 for all languages. 3.3.1 NMT with Attention We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems of NMT with attention (System ID: NMT). We used the following OpenNMT configuration. • • • • • • • • When we built BPE-codes, we merged source and target sentences and we used 100,000 for s option. We used 10 for vocabulary-threshold when subword-nmt applied BPE. 3.2.2 The default values were used for the other system parameters. For EnTam, News Commentary • The Moses toolkit for English and Russian only for the News Commentary data. 3.3.2 Transformer (Tensor2Tensor) For the News Commentary and English↔Tamil tasks, we used tensor2tensor’s20 im"
D19-5201,W18-1819,0,0.026575,"multilingual model. As for the English↔Tamil task, we train separate baseline models for each translation direction with 32,000 separate sub-word vocabularies. • Mecab19 for Japanese segmentation. • The EnTam corpus is not tokenized by any external toolkits. • Both corpora are further processed by tensor2tensor’s internal pre/postprocessing which includes sub-word segmentation. 3.2.3 For Multi-Modal Task • Hindi Visual Genome comes untokenized and we did not use or recommend any specific external tokenizer. 3.3.3 Transformer (OpenNMT-py) For the Multimodal task, we used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017) and used the “base” model with default parameters for the multi-modal task baseline. We have generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. • The standard OpenNMT-py sub-word segmentation was used for pre/postprocessing for the baseline system and each participant used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_freque"
D19-5201,P11-2093,0,0.0158555,"tion system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each submission: • Human Evaluation: whether or not they submit the results for human evaluation; Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model23 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.24 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.25 For Korean segmentation, we used mecab-ko.26 For Myanmar and Khmer segmentations, we used myseg.py27 and kmseg.py28 . For English and Russian tokenizations, we used tokenizer.perl29 in the Moses toolkit. For Hindi and Tamil tokenizations, we used Indic NLP Library.30 The detailed procedu"
D19-5201,P02-1040,0,0.106619,"used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 Baseline NMT Methods We used the following NMT with attention for most of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for e"
D19-5201,W16-2342,0,0.0144454,"on results that participants permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyot"
D19-5201,J82-2005,0,0.731235,"Missing"
D19-5201,D19-5224,0,0.0224104,"Missing"
D19-5201,W15-3049,0,0.0158524,"s permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-da"
D19-5201,W12-5611,1,0.827974,"/ 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k translation quality as well but this is beyond the scope of this years sub-task. Refer to Table 10 for the statistics of the in-domain parallel corpora. In addition we encouraged the participants to use out-of-domain parallel corpora from various sources such as KFTT,6 JESC,7 TED,8 ASPEC,9 UN,10 Yandex11 and Russian↔English news-commentary corpus.12 EnTam Corpus For Tamil↔English translation task we asked the participants to use the publicly available EnTam mixed domain corpus4 (Ramasamy et al., 2012). This corpus contains training, development and test sentences mostly from the news-domain. The other domains are Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation r"
D19-5201,2006.amta-papers.25,0,0.0857626,"hed are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 htt"
D19-5201,2007.mtsummit-papers.63,0,0.678495,",000 2,000 Dev 2,000 2,000 2,000 Table 2: Statistics for JPC • Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In the future, we will add more Asian languages such as Vietnamese, Thai and so on. 2 Train 1,000,000 1,000,000 1,000,000 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from Utiyama and Isahara (2007). Each sentence pair is accompanied by a similarity score calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the 2.2 JPC JPO Patent Corpus (JPC) for the patent tasks was constructed by the Japan Patent Office (JPO) in collaboration with NICT. The corpus consists of Chinese-Japanese, KoreanJapanese and English-Japanese patent descriptions whose International Patent Classi2 Disclosure Period 2016-01-01 to 2017-12-31 2018-01-01 to 2018-06-30 Train 1,089,346 (614,817) 314,649 (218,495) Dev Texts Items 1,153 2,"
D19-5204,P07-2045,0,0.00518534,"3. We used Sentencepiece (Kudo and Richardson, 2018) to create a shared vocabulary of 4000 tokens. We did not perform other tokenisation or truecasing for the training data. We used Mecab (Kudo, 2006) to tokenise the Japanese side of the evaluation data, which we used only for scoring. The English side remained as-is. 4.2 EN-JA 5.16 8.34 5.76 13.53 5.74 10.00 words, checkpoint frequency of 4000 updates. All models were trained until they reached convergence (no improvement for 10 checkpoints) on development data. For contrast we also trained statistical MT (SMT) systems using using the Moses (Koehn et al., 2007) toolkit and the following parameters: Word alignment using fast-align (Dyer et al., 2013); 7-gram translation models and the ‘wbe-msd-bidirectional-fe-allff‘ reordering models; Language model trained with KenLM (Heafield, 2011); Tuned using the improved MERT (Bertoldi et al., 2009). The BSD corpus was created with the intended use of training NMT systems. Thus, we trained NMT models using the corpus in both translation directions. As the BSD corpus is rather small for training reasonable MT systems, we also experimented with combining it with two larger conversational domain corpora. We emplo"
D19-5204,N18-1118,0,0.0685351,"Missing"
D19-5204,N19-1313,0,0.0214934,"Missing"
D19-5204,2012.eamt-1.60,0,0.0692099,"ng examples for automatic translation. We also experiment with adding the corpus in a machine translation training scenario and show how the resulting system benefits from its use. 1 Introduction There are a lot of ready-to-use parallel corpora for training machine translation systems, however, most of them are in written languages such as web crawl, news-commentary1 , patents (Goto et al., 2011), scientific papers (Nakazawa et al., 2016) and so on. Even though some of the parallel corpora are in spoken language, they are mostly spoken by only one person (in other words, they are monologues) (Cettolo et al., 2012; Di Gangi et al., 2019) or contain a lot of noise (Tiedemann, 2016; Pryzant et al., 2018). Most of the machine translation evaluation campaigns such as WMT2 , IWSLT3 and WAT4 adopt the written language, monologue or noisy dialogue parallel corpora for their translation tasks. Among them, there is only one clean, dialogue parallel corpus (Salesky et al., 2018) adopted by IWSLT in the conversational ∗ equal contribution http://www.statmt.org/wmt19/translation-task.html 2 http://www.statmt.org/wmt19/ 3 http://workshop2019.iwslt.org 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 1 54 Proceedings of the 6"
D19-5204,C96-2137,0,0.0114861,"The NMT system incorrectly translates the zero pronouns into I. It is worth noting that the type of the zero pronoun differs from the one in Figure 2 in that the referent in Figure 3 does not linguistically appear within the text (called exophora), while the other does (endophora) (Brown and Yule, 1983). The referent of the zero pronoun in Figure 3 is the listener of the utterance (you), and it usually does not have another linguistic item (such as the name of the person) that can be referred to. Although some modality expressions and verb types can give constraints to the possible referents (Nakaiwa and Shirai, 1996), essentially, the resolution of exophora needs the reference to situation. In this case, the correct translation depends on who is speaking. In the original conversation, the utterance is from Speaker 2 to Speaker 1, and Qualitative Analysis This section exemplifies some zero-anaphora translation errors and discusses what kind of information is needed to perform correct translation. A translation that needs world knowledge and inference In Figure 2, the subjects of the verbs are omitted in the source sentence 「（彼は: he）仕事もあ まりしない上に、（彼は: he）休み、早退ば かりを希望するから」. This causes the NMT system to incor"
D19-5204,N19-1202,0,0.0301141,"Missing"
D19-5204,N13-1073,0,0.0303799,"tokens. We did not perform other tokenisation or truecasing for the training data. We used Mecab (Kudo, 2006) to tokenise the Japanese side of the evaluation data, which we used only for scoring. The English side remained as-is. 4.2 EN-JA 5.16 8.34 5.76 13.53 5.74 10.00 words, checkpoint frequency of 4000 updates. All models were trained until they reached convergence (no improvement for 10 checkpoints) on development data. For contrast we also trained statistical MT (SMT) systems using using the Moses (Koehn et al., 2007) toolkit and the following parameters: Word alignment using fast-align (Dyer et al., 2013); 7-gram translation models and the ‘wbe-msd-bidirectional-fe-allff‘ reordering models; Language model trained with KenLM (Heafield, 2011); Tuned using the improved MERT (Bertoldi et al., 2009). The BSD corpus was created with the intended use of training NMT systems. Thus, we trained NMT models using the corpus in both translation directions. As the BSD corpus is rather small for training reasonable MT systems, we also experimented with combining it with two larger conversational domain corpora. We employed translators to translate the AMI Meeting Corpus (McCowan et al., 2005) (AMI) and the E"
D19-5204,W15-3049,0,0.0803312,"Missing"
D19-5204,W11-2123,0,0.0153772,"f the evaluation data, which we used only for scoring. The English side remained as-is. 4.2 EN-JA 5.16 8.34 5.76 13.53 5.74 10.00 words, checkpoint frequency of 4000 updates. All models were trained until they reached convergence (no improvement for 10 checkpoints) on development data. For contrast we also trained statistical MT (SMT) systems using using the Moses (Koehn et al., 2007) toolkit and the following parameters: Word alignment using fast-align (Dyer et al., 2013); 7-gram translation models and the ‘wbe-msd-bidirectional-fe-allff‘ reordering models; Language model trained with KenLM (Heafield, 2011); Tuned using the improved MERT (Bertoldi et al., 2009). The BSD corpus was created with the intended use of training NMT systems. Thus, we trained NMT models using the corpus in both translation directions. As the BSD corpus is rather small for training reasonable MT systems, we also experimented with combining it with two larger conversational domain corpora. We employed translators to translate the AMI Meeting Corpus (McCowan et al., 2005) (AMI) and the English part of Onto Notes 5.0 (Weischedel et al., 2013) (ON) into Japanese with the same instructions as for translating the BSD corpus. A"
D19-5204,E17-3017,0,0.0511702,"2.18 7.08 Table 4: NMT and SMT experiments using the conversational corpora. Evaluated on the Business Conversation evaluation set. given the context, one can infer that Speaker 2 is speaking to give a consolation to Speaker 1 and thus the subject should be you (Speaker 1). However, if the utterance was from Speaker 1, he would then just be complaining about his situation saying “I just need a bit more patience”. This example emphasises that the speaker information is essential to translate some utterances in conversation correctly. 4 SMT NMT SMT NMT SMT NMT Experiment Setup We used Sockeye (Hieber et al., 2017) to train transformer architecture models with 6 encoder and decoder layers, 8 transformer attention heads per layer, word embeddings and hidden layers of size 512, dropout of 0.2, maximum sentence length of 128 symbols, and a batch size of 1024 58 Source: Our Best NMT: Google Translate: Reference: では、終了する前に、この健康とストレスに関するセルフチェックシートに記入を して頂きたいと思います。 So before we finish, I’d like to fill in the health check-streams with this health and staff checkbook. I would like you to fill out this health and stress self-check sheet before you finish. Before we finish off, we would like you to fill out this"
D19-5204,L16-1559,0,0.0825276,"e corpus in a machine translation training scenario and show how the resulting system benefits from its use. 1 Introduction There are a lot of ready-to-use parallel corpora for training machine translation systems, however, most of them are in written languages such as web crawl, news-commentary1 , patents (Goto et al., 2011), scientific papers (Nakazawa et al., 2016) and so on. Even though some of the parallel corpora are in spoken language, they are mostly spoken by only one person (in other words, they are monologues) (Cettolo et al., 2012; Di Gangi et al., 2019) or contain a lot of noise (Tiedemann, 2016; Pryzant et al., 2018). Most of the machine translation evaluation campaigns such as WMT2 , IWSLT3 and WAT4 adopt the written language, monologue or noisy dialogue parallel corpora for their translation tasks. Among them, there is only one clean, dialogue parallel corpus (Salesky et al., 2018) adopted by IWSLT in the conversational ∗ equal contribution http://www.statmt.org/wmt19/translation-task.html 2 http://www.statmt.org/wmt19/ 3 http://workshop2019.iwslt.org 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 1 54 Proceedings of the 6th Workshop on Asian Translation, pages 54–61 c Hong Kong, China, N"
D19-5204,W17-4811,0,0.083923,"for written text and monologue has vastly improved due to the increase in the amount of the available parallel corpora and the recent neural network technologies. However, there is much room for improvement in the context of dialogue or conversation translation. One typical case is the translation from pro-drop language to the non-pro-drop language where correct pronouns must be supplemented according to the context. The omission of the pronouns occurs more frequently in spoken language than written language. Recently, context-aware translation models attract attention from many researchers (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019) to solve this kind of problem, however, there are almost no conversational parallel corpora with context information except noisy OpenSubtitles corpus. Taking into consideration the factors mentioned above, a document and sentence-aligned conversational parallel corpus should be advantageous to push machine translation research in this field to the next stage. In this paper, we introduce a newly constructed Japanese-English business conversation parallel corpus. This corpus contains 955 scenarios, 30,000 parallel sentences. Table 1 shows an example of the corpus. We"
D19-5204,P19-1116,0,0.13598,"Missing"
D19-5204,P18-1117,0,0.0137495,"e has vastly improved due to the increase in the amount of the available parallel corpora and the recent neural network technologies. However, there is much room for improvement in the context of dialogue or conversation translation. One typical case is the translation from pro-drop language to the non-pro-drop language where correct pronouns must be supplemented according to the context. The omission of the pronouns occurs more frequently in spoken language than written language. Recently, context-aware translation models attract attention from many researchers (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019) to solve this kind of problem, however, there are almost no conversational parallel corpora with context information except noisy OpenSubtitles corpus. Taking into consideration the factors mentioned above, a document and sentence-aligned conversational parallel corpus should be advantageous to push machine translation research in this field to the next stage. In this paper, we introduce a newly constructed Japanese-English business conversation parallel corpus. This corpus contains 955 scenarios, 30,000 parallel sentences. Table 1 shows an example of the corpus. We choose the business"
I05-1060,C04-1176,0,0.0660833,". There are several related work which can contribute the modification and extension of our methods. When using a Japanese-English dictionary, if we understand the translation is transliteration, we can utilize the information more effectively, handling inflections. In this sense, work by Knight and Graehl can be incorporated into our method [2]. 692 T. Nakazawa, D. Kawahara, and S. Kurohashi In order to handle spelling variation problems, there have been many methods proposed [3], and we can utilize recently proposed robust treatment of Japanese Katakana spelling variation by Masuyama et al. [5]. Our second method using Japanese-English dictionary and the English corpus can be considered as a translation acquisition method. It is interesting to compare these results with other web-based methods, such as Utsuro et al. [8, 1]. There have been many studies that extract compound nouns. Nakagawa et al. focused on the tendency that most of technical terms are compound nouns, and proposed a method of extracting technical terms by using frequency and variety of its neiboring words [10, 7]. In view of information retrieval, Yamada et al. aimed at imporving information retrieval using matching"
I05-1060,C04-1149,0,0.0204093,"e effectively, handling inflections. In this sense, work by Knight and Graehl can be incorporated into our method [2]. 692 T. Nakazawa, D. Kawahara, and S. Kurohashi In order to handle spelling variation problems, there have been many methods proposed [3], and we can utilize recently proposed robust treatment of Japanese Katakana spelling variation by Masuyama et al. [5]. Our second method using Japanese-English dictionary and the English corpus can be considered as a translation acquisition method. It is interesting to compare these results with other web-based methods, such as Utsuro et al. [8, 1]. There have been many studies that extract compound nouns. Nakagawa et al. focused on the tendency that most of technical terms are compound nouns, and proposed a method of extracting technical terms by using frequency and variety of its neiboring words [10, 7]. In view of information retrieval, Yamada et al. aimed at imporving information retrieval using matching of compounds [9]. It is similar to our study in handling compounds. 8 Conclusion This paper proposed an automatic segmentation method of Japanese Katakana compounds, which makes it possible to construct precise and concise Katakana"
I05-1060,J98-4003,0,\N,Missing
I11-1089,N10-1015,0,0.21905,"Missing"
I11-1089,P05-1022,0,0.0543808,"nments (Och and Ney, 2003). The unit of evaluation was the word for all the languages. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases. Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of the training corpus. 4.1 Settings 2 En-Ja En Ja 996K 24.7M 27.5M 24.8 27.6 αA 100 10 αN 100 100 pt 0.8 0.8 αf e , αef 10"
I11-1089,I08-1012,0,0.20628,"e summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases. Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of the training corpus. 4.1 Settings 2 En-Ja En Ja 996K 24.7M 27.5M 24.8 27.6 αA 100 10 αN 100 100 pt 0.8 0.8 αf e , αef 100 100 pf e , pef 0.5 0.5 Table 2: Hyper parameters used in experiments. of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristic (Koehn et al., 2003). Furthermore, we used the BerkeleyAligner4"
I11-1089,P03-1012,0,0.029688,"me corpus used in Section 4. 794 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 794–802, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP phrase, and incorrectly aligned “the ↔ は “. The word sequential model is prone to many such errors even for short simple sentences of a distant language pair. Even if the word order differs greatly between languages, phrase dependencies tend to hold between languages. This is also true in Figure 1. Therefore, incorporating dependency analysis into the alignment model is useful for distant language pairs. Cherry and Lin (2003) proposed a model that uses a source side dependency tree structure and constructs a discriminative model. However, the drawback is that the alignment unit is the word, and thus, it can only find one-to-one alignments. The capability of generating many-to-many correspondences is also important because one or more words often correspond to more than one word on the other side. Nakazawa and Kurohashi (2009) also proposed a model focusing on dependency relations. They modeled phrase dependency relations in dependency trees on both sides. The model is also capable of estimating many-to-many corres"
I11-1089,W02-1018,0,0.0641453,"++. Black boxes depict the system output, while dark (Sure) and light (Possible) gray cells denote gold-standard alignments. as a sequence of words works adequately. This does not hold for distant language pairs such as English-Japanese or Chinese-Japanese, in which word orders differ greatly. We incorporate dependency relations of words into the alignment model and define the reorderings on the word dependency trees. Figure 1 shows an example of the dependency trees for Japanese and English. 2.1 Generative Story Description Dependency Tree-based Alignment Model Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), we first describe the generative story for the joint alignment model. Our model is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences 1. Generate ` concepts from which subtree pairs are generated independently. 2. Combine the subtrees in each language so as to create parallel sentences. 795 Here, sub"
I11-1089,P10-1146,0,0.0419626,"Missing"
I11-1089,D08-1077,0,0.0302391,"Missing"
I11-1089,P07-1003,0,0.228894,"For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of the training corpus. 4.1 Settings 2 En-Ja En Ja 996K 24.7M 27.5M 24.8 27.6 αA 100 10 αN 100 100 pt 0.8 0.8 αf e , αef 100 100 pf e , pef 0.5 0.5 Table 2: Hyper parameters used in experiments. of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristic (Koehn et al., 2003). Furthermore, we used the BerkeleyAligner4 (DeNero and Klein, 2007) with default settings for unsupervised training. Experimental results for English-Japanese are shown in Table 4.1, and those for ChineseJapanese are shown in Table 4.1. The alignment accuracy of the initialization described in Section 3.1 is indicated as “Initialization”, while the accuracy after conducting Gibbs sampling is indicated as “Proposed”. 4.2 Discussion For English-Japanese, our proposed model achieved reasonably high alignment accuracy compared with that of GIZA++ and the BerkeleyAligner. Using tree structures combined with the bi-directional alignment results leads to better accu"
I11-1089,W09-2302,1,0.889421,"nguages, phrase dependencies tend to hold between languages. This is also true in Figure 1. Therefore, incorporating dependency analysis into the alignment model is useful for distant language pairs. Cherry and Lin (2003) proposed a model that uses a source side dependency tree structure and constructs a discriminative model. However, the drawback is that the alignment unit is the word, and thus, it can only find one-to-one alignments. The capability of generating many-to-many correspondences is also important because one or more words often correspond to more than one word on the other side. Nakazawa and Kurohashi (2009) also proposed a model focusing on dependency relations. They modeled phrase dependency relations in dependency trees on both sides. The model is also capable of estimating many-to-many correspondences automatically without any heuristics through maximum likelihood estimation. One serious drawback of their model is that it tends to acquire incorrect larger subtrees. For models that can handle multiple levels (or sizes) of structures, larger structures always defeat smaller ones in maximum likelihood estimation, and the best solution is to align one sentence as a structure with the other for al"
I11-1089,D08-1033,0,0.19178,"Missing"
I11-1089,J03-1002,0,0.0157033,"aper abstract corpus provided by JST and NICT. This corpus was developed during a project in Japan called the “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of these corpora are shown in Table 4.1. Unfortunately, these two corpora are not freely available now, but they will become available to everyone in near future. As gold-standard data, we used 479 sentence pairs of English-Japanese and 510 sentence pairs of Chinese-Japanese. These were annotated by hand using two types of annotations: sure (S) alignments and possible (P ) alignments (Och and Ney, 2003). The unit of evaluation was the word for all the languages. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005"
I11-1089,N06-1023,1,0.874151,"f Chinese-Japanese. These were annotated by hand using two types of annotations: sure (S) alignments and possible (P ) alignments (Och and Ney, 2003). The unit of evaluation was the word for all the languages. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases. Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of"
I11-1089,C10-1123,0,0.0870215,"Missing"
I11-1089,N03-1017,0,0.0765982,"Missing"
I11-1089,2007.mtsummit-papers.63,0,0.0964688,"es and update the current counts by counting subtree pairs and dependency relations in the samples. EXPAND-1 does not have any restrictions on its operation. However, for EXPAND-2, if the root 798 4. Go back to step 2. 4 # of sentences # of words ave. sent. length Alignment Experiments We conducted alignment experiments on EnglishJapanese and Chinese-Japanese corpora to show the effectiveness of the proposed model. En-Ja Zh-Ja For English-Japanese, the JST2 paper abstract corpus was used. This corpus was created by NICT3 from JST’s 2M English-Japanese paper abstract corpus using the method of Utiyama and Isahara (2007). For Chinese-Japanese, we used the paper abstract corpus provided by JST and NICT. This corpus was developed during a project in Japan called the “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of these corpora are shown in Table 4.1. Unfortunately, these two corpora are not freely available now, but they will become available to everyone in near future. As gold-standard data, we used 479 sentence pairs of English-Japanese and 510 sentence pairs of Chinese-Japanese. These were annotated by hand using two types of annotations: sure (S) alig"
I11-1089,P07-2045,0,0.0131114,"sed”) and 2 steps going down (via “for”), so the dependency relation is (U p, Down) = (1, 2). Consequently, the tuple is represented as a triplet of non-negative integers Rf = (N, U p, Down). The dependency relation probabilities for the foreign language side are drawn from an unknown probability distribution θf e and for the English side from θef , with both obeying the DP: θf e (Rf ) Mf e (Rf ) θef (Re ) Mef (Re ) sults of the standard word alignment tool GIZA++. Many machine translation studies use heuristics to combine the two alignment results, one of which is called grow-diag-final-and (Koehn et al., 2007). Our heuristic is similar to this, but the difference is that we combine the two results based on dependency trees, and not on word sequences. The initialization is carried out by the following steps: 1. Take the intersection of the two results. 2. Add alignment points connected to at least one accepted point in terms of the dependency tree (corresponds to grow-diag). 3. Add alignment points between two unaligned words (corresponds to final-and). Initial boundaries of subtrees and their alignments, and also the counts of subtree pairs and dependency relations are thus acquired. 3.2 Sampling O"
I11-1089,P09-1058,0,0.102602,"Missing"
I11-1089,J93-2003,0,\N,Missing
I11-1089,P05-1033,0,\N,Missing
I13-1030,P07-2045,0,0.0133813,"Missing"
I13-1030,P07-1016,0,0.0276113,"ration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, which is the consideration of inherent semantic information in transliterations. Their example is the inﬂuence of the source language and gender of foreign names on their transliterations into Chinese. Hagiwara and Sekine (2012) expanded upon this idea by considering a ‘latent class’ 1 Introduction A large, high-quality bilingual lexicon is of great utility to any dictionary-based system that processes bilingual data. The abili"
I13-1030,D09-1092,0,0.700347,"o recover the letter sequence ‘gli’ than if it were originally French. While these methods consider limited semantic features, they do not make use of the rich contextual information available from comparable corpora. We show such contextual information, in the form of bilingual topic distributions, to be highly eﬀective in generating transliterations. Bilingual lexicon mining from non-parallel data has been tackled in recent research such as Tamura et al. (2012) and Haghighi et al. (2008), and we build upon the techniques of multilingual topic extraction from Wikipedia pioneered by Ni et al. (2009). Previous research in lexicon mining has tended to focus on semantic features, such as context similarity vectors and topic models, but these have yet to be applied to the task of transliteration mining. We use the word-topic distribution similarities explored in Vulić et al. (2011) as baseline word similarity measures. In some cases it is possible to use monolingual corpora for transliteration mining, as English is often written alongside transliterations (Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー"
I13-1030,I08-6006,0,0.1384,"eindependent transliteration framework applicable to bilingual lexicon extraction. Our approach is to employ a bilingual topic model to enhance the output of a state-of-the-art graphemebased transliteration baseline. We demonstrate that this method is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem. 2 Previous Work Previous work has considered various methods for transliteration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) prop"
I13-1030,W10-2408,0,0.129572,"le to bilingual lexicon extraction. Our approach is to employ a bilingual topic model to enhance the output of a state-of-the-art graphemebased transliteration baseline. We demonstrate that this method is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem. 2 Previous Work Previous work has considered various methods for transliteration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, whic"
I13-1030,P11-2010,0,0.0543863,"Missing"
I13-1030,P03-1021,0,0.00875904,"(Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we turn to the task of combining a semantic model with our transliteration engine. We employ the met"
I13-1030,W12-4404,0,0.0779492,"edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, which is the consideration of inherent semantic information in transliterations. Their example is the inﬂuence of the source language and gender of foreign names on their transliterations into Chinese. Hagiwara and Sekine (2012) expanded upon this idea by considering a ‘latent class’ 1 Introduction A large, high-quality bilingual lexicon is of great utility to any dictionary-based system that processes bilingual data. The ability to automatically generate such a l"
I13-1030,J03-1002,0,0.00413423,"e monolingual corpora for transliteration mining, as English is often written alongside transliterations (Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we"
I13-1030,W09-3524,0,0.31861,"ethod is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem. 2 Previous Work Previous work has considered various methods for transliteration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, which is the consideration of inherent semantic information in transliterations. Their example is the inﬂuence of the source language and gender of foreign names on their transliterations into"
I13-1030,2001.mtsummit-papers.68,0,0.0205042,"ever we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we turn to the task of combining a semantic model with our transliteration engine. We employ the method of bilingual LDA (Mimno et al.,"
I13-1030,W10-2405,0,0.0252588,"007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we turn to the task of combining a semantic model with our transliteration engine. We employ the method of bilingual LDA (Mimno et al., 2009), an extension of monolingual Latent Dirichlet Allocation (LDA) (Blei et al., 2003) as the semantic model. Monolingual LDA takes as its input a set of monolingual documents and generates a wordtopic distribution ϕ classifying words appearing in these documents into semantically simila"
I13-1030,P95-1050,0,0.391041,"features used for SVM training are baseline, Cos, Cue and KL scores. The similarity measures Cos, Cue and KL are deﬁned below. Figure 2: Graphical model for Bilingual LDA with K topics, D document pairs and hyperparameters α and β. Topics for each document are sampled from the common distribution θ, and the two languages have word-topic distributions ϕ and ψ. 4.1 Motivation for Bilingual LDA We choose to employ a bilingual topic model to measure semantic similarity (i.e. topic similarity) of word pairs rather than the more intuitive method of comparing monolingual context similarity vectors (Rapp, 1995) for reasons of robustness and scalability. Measuring context similarity on a word level requires a bilingual lexicon to match crosslanguage word pairs and such bilingual data is often expensive or unavailable. There are also problems with directly comparing collocations and word concurrence of distant language pairs as they do not always correspond predictably. Therefore our proposed method provides a more robust approach using coarser semantic features. The use of topic models as a semantic similarity measure is a scalable method because document-aligned bilingual training data is growing ev"
I13-1030,D11-1089,0,0.0461589,"c distributions, to be highly eﬀective in generating transliterations. Bilingual lexicon mining from non-parallel data has been tackled in recent research such as Tamura et al. (2012) and Haghighi et al. (2008), and we build upon the techniques of multilingual topic extraction from Wikipedia pioneered by Ni et al. (2009). Previous research in lexicon mining has tended to focus on semantic features, such as context similarity vectors and topic models, but these have yet to be applied to the task of transliteration mining. We use the word-topic distribution similarities explored in Vulić et al. (2011) as baseline word similarity measures. In some cases it is possible to use monolingual corpora for transliteration mining, as English is often written alongside transliterations (Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration re"
I13-1030,D12-1003,0,\N,Missing
I13-1030,I05-1060,1,\N,Missing
I13-1030,P02-1040,0,\N,Missing
I13-1030,P08-1088,0,\N,Missing
I13-1030,P11-2084,0,\N,Missing
I13-1163,2012.eamt-1.7,1,0.845483,"s. Fragments with less than 3 words may be produced in this process, and we discard them like previous studies. 4 Experiments In our experiments, we compared our proposed fragment extraction method with (Munteanu and Marcu, 2006). We manually evaluated the accuracy of the extracted fragments. Moreover, we used the extracted fragments as additional MT training data, and evaluated the effectiveness of the fragments for MT. We conducted experiments on Chinese–Japanese data. In all our experiments, we preprocessed the data by segmenting Chinese and Japanese sentences using a segmenter proposed by Chu et al. (2012) and JUMAN (Kurohashi et al., 1994) respectively. 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST4 and NICT5 . This corpus was created by the Japanese project “Development and Research of Chinese–Japanese Natural Language Processing Technology”, containing 680k sentences (18.2M Chinese and 21.8M Japanese tokens respectively). This corpus contains various domains such as chemistry, physics, biology and agriculture etc. 4.1.2 Quasi–Comparable Corpora The quasi–comparable corpora we used are scientific paper abstracts collected from"
I13-1163,C04-1151,0,0.226919,"several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments"
I13-1163,W11-1209,0,0.0172353,"wo generative alignment models for extracting parallel fragments from comparable sentences. However, the extracted fragments slightly decrease MT performance when appending them to in–domain training data. We think the reason is that because the comparable sentences are quite noisy, the alignment models cannot accurately extract parallel fragments. To solve this problem we only use alignment models for parallel fragment candidate detection, and use an accurate lexicon filter to guarantee the accuracy of the extracted parallel fragments. Besides the above studies, there are some other efforts. Hewavitharana and Vogel (2011) propose a method that calculates both the inside and outside probabilities for fragments in a comparable sentence pair, and show that the context of the sentence helps fragment extraction. However, the proposed method only can be efficient in a controlled manner that supposes the source fragment was known, and search for the target fragment. Another study uses a syntax–based alignment model to extract parallel fragments from noisy parallel data (Riesa and Marcu, 2012). Since their method is designed for noisy parallel data, we believe that the method cannot accurately extract parallel fragmen"
I13-1163,P07-2045,0,0.0250392,"rom quasi–comparable corpora. To solve this problem, we propose an accurate parallel fragment extraction system that uses an alignment model to locate the parallel fragment candidates, and uses an accurate lexicon filter to identify the truly parallel ones. Experimental results indicate that our system can accurately extract parallel fragments, and our proposed method significantly outperforms a state–of–the–art approach. Furthermore, we investigate the factors that may affect the performance of our system in detail. 1 Introduction In statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009"
I13-1163,J05-4003,0,0.53616,"(Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot"
I13-1163,N10-1063,0,0.0218879,"since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accuratel"
I13-1163,P09-2057,0,0.013739,"et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel"
I13-1163,P03-1010,0,0.0486948,"machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the"
I13-1163,C12-1166,0,0.0237647,"Missing"
I13-1163,P11-2084,0,0.0442669,"Missing"
I13-1163,P06-1011,0,0.56179,"le in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable (Munteanu and Marcu, 2006). Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007). Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese–Japanese corpora show that our proposed method significantly outperforms a state–of–the– art approach, which indicate the effectiveness of our para"
I13-1163,2007.mtsummit-papers.50,0,0.444915,"However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable (Munteanu and Marcu, 2006). Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007). Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese–Japanese corpora show that our proposed method significantly outperforms a state–of–the– art approach, which indicate the effectiveness of our parallel fragment extraction system. Moreover, we investigate the factors that may affect the performance of our system"
I13-1163,N12-1061,0,0.0190235,"o guarantee the accuracy of the extracted parallel fragments. Besides the above studies, there are some other efforts. Hewavitharana and Vogel (2011) propose a method that calculates both the inside and outside probabilities for fragments in a comparable sentence pair, and show that the context of the sentence helps fragment extraction. However, the proposed method only can be efficient in a controlled manner that supposes the source fragment was known, and search for the target fragment. Another study uses a syntax–based alignment model to extract parallel fragments from noisy parallel data (Riesa and Marcu, 2012). Since their method is designed for noisy parallel data, we believe that the method cannot accurately extract parallel fragments from comparable sentences. 3 Proposed Method 3.1 System Overview Figure 1 shows an overview of our parallel fragment extraction system. We first apply comparable sentence extraction using a combination method of (Abdul-Rauf and Schwenk, 2011) (1)(2) and (Munteanu and Marcu, 2005) (3), which were originally used for extracting parallel sentences from comparable corpora. We translate the source sentences to target language with a SMT system trained on a parallel corpu"
I13-1163,J93-2003,0,\N,Missing
I17-5004,Q17-1024,0,0.035587,"Missing"
I17-5004,P16-1009,0,0.014262,"TRA) Overview of implementations of the NMT models and the frameworks: KNMT1 , Lamtram2 , Open NMT3 , Nematus4 , Tensor2Tensor5 . Coffee Break ( 30 min) 3. Practical NMT ( 45 min) The objective of this part of the tutorial is to augment the audience’s understanding of NMT with various practical ideas that can help improve the quality and speed of NMT as well as showcase the many black box applications of NMT. 5. Summary and Conclusion 3 • Preprocessing and management of rare words. • Subword units to enable infinite vocabulary. • BPE (Byte Pair Encoding) and its impact on translation quality (Sennrich et al., 2016b). • Using monolingual corpora to improve NMT (Gülçehre et al., 2015; Sennrich et al., 2016a). • Training and Translation search. • Optimization algorithms (ADAM etc). • Residual connections. • Training schedules for optimal results (ADAM –&gt; SGD –&gt; annealing –&gt; early stopping). About the Speakers • Raj Dabre: Graduate School of Informatics, Kyoto University, Japan (raj@nlp.ist.i.kyotou.ac.jp) Raj Dabre is a 3rd year PhD student at Kyoto University. His research interests center on natural language processing, particularly neural machine translation for low resource languages and domain adapta"
I17-5004,P16-1162,0,0.0400068,"TRA) Overview of implementations of the NMT models and the frameworks: KNMT1 , Lamtram2 , Open NMT3 , Nematus4 , Tensor2Tensor5 . Coffee Break ( 30 min) 3. Practical NMT ( 45 min) The objective of this part of the tutorial is to augment the audience’s understanding of NMT with various practical ideas that can help improve the quality and speed of NMT as well as showcase the many black box applications of NMT. 5. Summary and Conclusion 3 • Preprocessing and management of rare words. • Subword units to enable infinite vocabulary. • BPE (Byte Pair Encoding) and its impact on translation quality (Sennrich et al., 2016b). • Using monolingual corpora to improve NMT (Gülçehre et al., 2015; Sennrich et al., 2016a). • Training and Translation search. • Optimization algorithms (ADAM etc). • Residual connections. • Training schedules for optimal results (ADAM –&gt; SGD –&gt; annealing –&gt; early stopping). About the Speakers • Raj Dabre: Graduate School of Informatics, Kyoto University, Japan (raj@nlp.ist.i.kyotou.ac.jp) Raj Dabre is a 3rd year PhD student at Kyoto University. His research interests center on natural language processing, particularly neural machine translation for low resource languages and domain adapta"
I17-5004,D14-1179,0,0.0156838,"Missing"
I17-5004,N16-1101,0,0.0130943,") The objective of this part of the tutorial is to bring the audience up to speed with the current SOTA (state-of-the-art) NMT models and advancements. We plan to enumerate the most important ones and thereby provide the audience members a roadmap to understanding the big picture. • Facebook’s CNN (Convolutional Neural Network) based NMT model (Gehring et al., 2017). • Google’s Transformer (Vaswani et al., 2017) that replies purely on attention and feedforward networks (SOTA for WMT tasks). • Results and speedup in training achieved by these architectures. • Multilingual Multiway NMT (MLNMT) (Firat et al., 2016) and Zero Shot NMT (Johnson et al., 2016). • Other advances (search-guided, latent graph, pointer networks) (EXTRA) Overview of implementations of the NMT models and the frameworks: KNMT1 , Lamtram2 , Open NMT3 , Nematus4 , Tensor2Tensor5 . Coffee Break ( 30 min) 3. Practical NMT ( 45 min) The objective of this part of the tutorial is to augment the audience’s understanding of NMT with various practical ideas that can help improve the quality and speed of NMT as well as showcase the many black box applications of NMT. 5. Summary and Conclusion 3 • Preprocessing and management of rare words. •"
L16-1348,W06-2810,0,0.0605838,"Missing"
L16-1348,P91-1022,0,0.765952,", and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are generally based on two kinds of algorithms: length-based algorithms align sentences according solely to their lengths (Brown et al., 1991), while lexicon-based algorithms use lexical information to calculate similarity between source and target sentences (Ma, 2006). Length-based algorithms are typically faster but not suited for processing noisy corpus (corpus containing article pairs with omitted or wrong translations). Lexicon-based algorithms, like the one used in Champollion (Ma, 2006)1 are more robust. However their performance highly depends on the coverage and quality of the lexicon, and large-scale lexicon with high quality is not easy to obtain especially for low resource language pairs. In this paper, we propose to use"
L16-1348,Y15-1033,1,0.941209,"sentences (Ma, 2006). Length-based algorithms are typically faster but not suited for processing noisy corpus (corpus containing article pairs with omitted or wrong translations). Lexicon-based algorithms, like the one used in Champollion (Ma, 2006)1 are more robust. However their performance highly depends on the coverage and quality of the lexicon, and large-scale lexicon with high quality is not easy to obtain especially for low resource language pairs. In this paper, we propose to use lexicons generated by a pivotbased MT system, which could be constructed even for low resource languages (Dabre et al., 2015). Experiments conducted on Chinese-Japanese scientific articles verify the effectiveness of our proposed method. 2. Simultaneous Sentence Boundary Detection and Alignment Our proposed alignment method consists in the following steps 1 1. Split source and target articles into sentence candidates. 2. Normalize words in sentence candidates to maximize matching rate between words in source, target, and lexicons. 3. Compute alignment path with the highest similarity between source and target. An alignment path is composed of pairs of article segments.2 4. Adjust sentence boundaries by merging sente"
L16-1348,W04-1101,0,0.0367831,"Missing"
L16-1348,D07-1103,0,0.100786,"Missing"
L16-1348,N03-1017,0,0.00709525,"he input article pairs using pivot-based MT, achieving better coverage of the input words with fewer entries than pre-existing dictionaries. Pivot-based MT makes it possible to build dictionaries for language pairs that have scarce parallel data. The alignment method is implemented in a tool that will be freely available in the near future. Keywords: Sentence Alignment, Sentence Segmentation, Pivot-based Machine Translation 1. Introduction Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair, which are crucial for machine translation (MT) (Koehn et al., 2003). Previous studies first split the source and target articles into sentences respectively using punctuation information, and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are"
L16-1348,C10-2081,0,0.0412132,"Missing"
L16-1348,ma-2006-champollion,0,0.123676,"l data. The alignment method is implemented in a tool that will be freely available in the near future. Keywords: Sentence Alignment, Sentence Segmentation, Pivot-based Machine Translation 1. Introduction Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair, which are crucial for machine translation (MT) (Koehn et al., 2003). Previous studies first split the source and target articles into sentences respectively using punctuation information, and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are generally based on two kinds of algorithms: length-based algorithms align sentences according solely to their lengths (Brown et al., 1991), while lexicon-based algorithms use lexical information to calculate similarity between so"
L16-1348,moore-2002-fast,0,0.168305,"Missing"
L16-1348,2010.amta-papers.14,0,0.0401683,"Missing"
L16-1348,P94-1012,0,0.0759164,".2871 0.1824 0.7646 0.7653 0.7883 0.7932 0.7955 0.7968 0.4820 0.2372 0.5619 0.5393 0.4352 0.3049 Table 5: Sentence alignment results of the proposed method “Hard+Soft” with similarity threshold tuned on the development set for maximum F measure and precision ≥ 0.9. Dictionary None EDR MT-Noun MT-NVAA EDR+MT-Noun EDR+MT-NVAA Coverage 0.27 0.39 0.42 0.45 0.46 0.48 Table 6: Lexicons coverage on the test set. segmentation and alignment issues, taking advantage of intermediate alignment data to adjust sentence boundaries. Lexicon-based algorithms require a dictionary that may be generated offline (Wu, 1994; Ma, 2006), or online, for example by comparing the number of occurrences and distribution of words on both sides of the bilingual corpus (Kay and R¨oscheisen, 1993). Our proposed method follows an intermediate approach, where lexicons are generated automatically using a pivot-based MT system. Since lexicons are generated from words in the corpus, they achieve a high coverage ratio with a low number of entries. MT systems have previously been used for sentence alignment, by calculating similarity scores between target and MT translation of the source. (Adafre and De Rijke, 2006) uses word-lev"
L16-1348,P11-2111,0,0.0604551,"Missing"
L16-1348,J93-1006,0,\N,Missing
L16-1350,W14-7001,1,0.377751,"her unit belonging to the same paper in the translated data are extracted. Therefore, there is no sentence pairs sharing the same paper across the training, development, development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acq"
L16-1350,W15-5001,1,0.860841,"development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acquired in the workshop. WAT is unique in the following points: As described in Section 2., ASPEC-JC includes only 8 scientific fields. The distribution of the fields is s"
L16-1350,2007.mtsummit-papers.63,1,0.812108,"Missing"
N16-1002,P05-1022,0,0.257273,"the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For referenc"
N16-1002,N12-1047,0,0.0282404,"lation quality (BLEU and RIBES). Results marked with † are signiﬁcantly higher than the baseline system and those marked with ‡ are signiﬁcantly higher than the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules b"
N16-1002,P05-1033,0,0.605916,"ine’ into the rule ‘[X] を 読んだ → read [X]’. We cannot however decide clearly where to insert the rule ‘昨日 → yesterday’ as there is no matching non-terminal in the rule containing its parent in the input sentence (‘読んだ’). We use the term ﬂoating to describe words such as ‘yesterday’ in this example, i.e. for an input subtree matched to the source side of a rule, children of the input root that are not contained in the source side of the rule as terminals and cannot be inserted using ﬁxed-position non-terminals in the rule. Previous work deals with this problem by either using simple glue rules (Chiang, 2005) or limiting rules in a way to avoid isolated ﬂoating children (Shen et al., 2008). For example, it is possible to disallow the ﬁrst rule in Figure 1 when translating a sentence such as that in Figure 2 with uncovered children (in this case the word ‘yesterday’). This method greatly reduces the expressiveness and ﬂexibility of translation rules. In our generalized model, we allow any number of terminals and non-terminals and permit arbitrarily many ﬂoating children in each rule. To our knowledge this is the ﬁrst study to take this more comprehensive approach. Note that in the case of constitue"
N16-1002,P10-1146,0,0.0632456,"Missing"
N16-1002,W06-1628,0,0.0734738,"Missing"
N16-1002,D14-1063,1,0.772667,"e than four ﬂexible non-terminals, each with more than four possible insertion positions. Such rules will already generate hundreds of simple rules. In the most extreme cases, we may encounter rules having more than ten ﬂexible non-terminals, leading to the generation of many millions of simple rules. This explosion of rules can lead to impractical decoding time and memory usage. It is therefore important to make use of the compact encoding of many simple rules provided by the concept of ﬂexible non-terminals in the decoding process itself. We use the decoding approach of right-hand lattices (Cromières and Kurohashi, 2014), an eﬃcient way of encoding many simple rules. The idea is to encode the translation rules into a lattice form, then use this lattice to decode eﬃciently without the need to expand the ﬂexible non-terminals explicitly. Figure 5 shows how the concept of ﬂexible non-terminals can be eﬃciently encoded into lattice form. The top half shows a target-side tree translation rule with ﬂexible non-terminals X1, X2, X3 and X4 allowed to be inserted at any position that is a child of the word ‘a’, with the constraint that X1 comes before X2 and that X2 comes before X3. X5 is another ﬂexible nonterminal t"
N16-1002,N04-1014,0,0.0958519,"Missing"
N16-1002,W11-2123,0,0.0177269,"atures to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For reference, we also show results using Moses (Koehn et al., 2007) with default settings and distortion limit set to 20 (‘Moses’). The proposed system (‘Flexible’) adds ﬂexible non-terminals with multiple insertion positions, how"
N16-1002,D10-1092,0,0.11044,"nals with multiple insertion positions, however we do not yet add the insertion choice features. This means that the insertion positions are in practice chosen by the language model. Note that we do not get a 16 substantial hit in performance by adding the ﬂexible non-terminals because of their compact lattice representation. The systems ‘+Pref’, ‘+Pref+Ins’ and ‘+Pref+Ins+Rel’ show the results of adding insertion choice position features (left/right preference, insertion position choice, relative position choice). We give translation scores measured in BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), which is designed to reﬂect quality of translation word order more eﬀectively than BLEU. The translation evaluation is shown in Table 2. 5 Discussion and Error Analysis The experimental results showed a signiﬁcantly positive improvement in terms of both BLEU and RIBES over the baseline tree-to-tree system. The baseline system uses ﬁxed non-terminals and is competitive with the most popular stringto-string system (Moses). The extensions of the proposed model (adding a variety of features) also all showed signiﬁcant improvement over the baseline, and approximately half of the extended settings"
N16-1002,N06-1023,1,0.700597,"y higher than the baseline system and those marked with ‡ are signiﬁcantly higher than the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), jo"
N16-1002,N03-1017,0,0.0791733,"Missing"
N16-1002,P07-2045,0,0.0109225,"with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For reference, we also show results using Moses (Koehn et al., 2007) with default settings and distortion limit set to 20 (‘Moses’). The proposed system (‘Flexible’) adds ﬂexible non-terminals with multiple insertion positions, however we do not yet add the insertion choice features. This means that the insertion positions are in practice chosen by the language model. Note that we do not get a 16 substantial hit in performance by adding the ﬂexible non-terminals because of their compact lattice representation. The systems ‘+Pref’, ‘+Pref+Ins’ and ‘+Pref+Ins+Rel’ show the results of adding insertion choice position features (left/right preference, insertion pos"
N16-1002,P06-1077,0,0.0513765,"input sentence into manageable parts, translating these segments, then arranging them in an appropriate order. The ﬁrst two steps have roughly the same diﬃculty for close and distant language pairs, however the reordering step is considerably more challenging for language pairs with dissimilar syntax. We need to In order to improve upon linear string-based approaches, syntax-based approaches have also been proposed. Tree-to-string translation has been the most popular syntax-based paradigm in recent years, which is reﬂected by a number of reordering approaches considering source-only syntax (Liu et al., 2006; Neubig, 2013). One particularly interesting approach is to project source dependency parses to the target side and then learn a probability model for reordering children using features such as source and target head words (Quirk et al., 2005). While tree-to-tree translation (Graehl and 11 Proceedings of NAACL-HLT 2016, pages 11–19, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Examples of tree-to-tree translation rules extracted from an aligned and parsed bitext. Colored boxes represent aligned phrases and [X] is a non-terminal. Figure 2:"
N16-1002,P13-4016,0,0.0159157,"to manageable parts, translating these segments, then arranging them in an appropriate order. The ﬁrst two steps have roughly the same diﬃculty for close and distant language pairs, however the reordering step is considerably more challenging for language pairs with dissimilar syntax. We need to In order to improve upon linear string-based approaches, syntax-based approaches have also been proposed. Tree-to-string translation has been the most popular syntax-based paradigm in recent years, which is reﬂected by a number of reordering approaches considering source-only syntax (Liu et al., 2006; Neubig, 2013). One particularly interesting approach is to project source dependency parses to the target side and then learn a probability model for reordering children using features such as source and target head words (Quirk et al., 2005). While tree-to-tree translation (Graehl and 11 Proceedings of NAACL-HLT 2016, pages 11–19, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Examples of tree-to-tree translation rules extracted from an aligned and parsed bitext. Colored boxes represent aligned phrases and [X] is a non-terminal. Figure 2: Combination of"
N16-1002,P02-1040,0,0.11352,"‘Flexible’) adds ﬂexible non-terminals with multiple insertion positions, however we do not yet add the insertion choice features. This means that the insertion positions are in practice chosen by the language model. Note that we do not get a 16 substantial hit in performance by adding the ﬂexible non-terminals because of their compact lattice representation. The systems ‘+Pref’, ‘+Pref+Ins’ and ‘+Pref+Ins+Rel’ show the results of adding insertion choice position features (left/right preference, insertion position choice, relative position choice). We give translation scores measured in BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), which is designed to reﬂect quality of translation word order more eﬀectively than BLEU. The translation evaluation is shown in Table 2. 5 Discussion and Error Analysis The experimental results showed a signiﬁcantly positive improvement in terms of both BLEU and RIBES over the baseline tree-to-tree system. The baseline system uses ﬁxed non-terminals and is competitive with the most popular stringto-string system (Moses). The extensions of the proposed model (adding a variety of features) also all showed signiﬁcant improvement over the baseline, and approximat"
N16-1002,P05-1034,0,0.0590265,"ly more challenging for language pairs with dissimilar syntax. We need to In order to improve upon linear string-based approaches, syntax-based approaches have also been proposed. Tree-to-string translation has been the most popular syntax-based paradigm in recent years, which is reﬂected by a number of reordering approaches considering source-only syntax (Liu et al., 2006; Neubig, 2013). One particularly interesting approach is to project source dependency parses to the target side and then learn a probability model for reordering children using features such as source and target head words (Quirk et al., 2005). While tree-to-tree translation (Graehl and 11 Proceedings of NAACL-HLT 2016, pages 11–19, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Examples of tree-to-tree translation rules extracted from an aligned and parsed bitext. Colored boxes represent aligned phrases and [X] is a non-terminal. Figure 2: Combination of translation rules, demonstrating non-terminal substitution and multiple possible insertion positions for a non-matching input phrase (‘昨日’). Knight, 2004; Cowan and Collins, 2006; Chiang, 2010) has been somewhat less popular tha"
N16-1002,P14-5014,1,0.849806,"is relatively simple when the target structure of rules is restricted to ‘well-formed’ dependencies (Shen et al., 2008), however in this paper we consider more general rules with ﬂexible non-terminal insertion positions. 2 Dependency Tree-To-Tree Translation Dependency tree-to-tree translation begins with the extraction of translation rules from a bilingual corpus that has been parsed and word aligned. Figure 1 shows an example of three rules that can be extracted from aligned and parsed sentence pairs. In this paper we consider rules similar to previous work on tree-to-tree de12 pendency MT (Richardson et al., 2014). The simplest type of rule, containing only terminal symbols, can be extracted trivially from aligned subtrees (see rules 2 and 3 in Figure 1). Non-terminals can be added to rules (see rule 1 in Figure 1) by omitting aligned subtrees and replacing on each side with non-terminal symbols. We can naturally express phrase reordering as the source/target-side non-terminals are aligned. Decoding is performed by combining these rules to form a complete translation, as shown in Figure 2. We are able to translate part of the sentence with non-ambiguous reordering (‘read a magazine’), as we can insert"
N16-1002,D11-1046,0,0.356462,".05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For reference, we also show results using Moses (Koehn et al., 2007) with default settings and distortion limit set to 20 (‘Moses"
N16-1002,P08-1066,0,0.599498,"nt aligned phrases and [X] is a non-terminal. Figure 2: Combination of translation rules, demonstrating non-terminal substitution and multiple possible insertion positions for a non-matching input phrase (‘昨日’). Knight, 2004; Cowan and Collins, 2006; Chiang, 2010) has been somewhat less popular than treeto-string translation, we believe there are many beneﬁts of considering target-side syntax. In particular, reordering can be deﬁned naturally with non-terminals in the target-side grammar. This is relatively simple when the target structure of rules is restricted to ‘well-formed’ dependencies (Shen et al., 2008), however in this paper we consider more general rules with ﬂexible non-terminal insertion positions. 2 Dependency Tree-To-Tree Translation Dependency tree-to-tree translation begins with the extraction of translation rules from a bilingual corpus that has been parsed and word aligned. Figure 1 shows an example of three rules that can be extracted from aligned and parsed sentence pairs. In this paper we consider rules similar to previous work on tree-to-tree de12 pendency MT (Richardson et al., 2014). The simplest type of rule, containing only terminal symbols, can be extracted trivially from"
N16-1002,Y12-1033,1,0.706535,"ked with ‡ are signiﬁcantly higher than the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightm"
N16-1002,N04-4026,0,0.343208,"rder for dependency parses, in which words can have arbitrarily many children. Previous approaches have tackled this problem by restricting grammar rules, reducing the expressive power of the translation model. The ﬁrst approaches to reordering were based on linear distortion (Koehn et al., 2003), which models the probability of swapping pairs of phrases over some given distance. The linear distance is the only parameter, ignoring any contextual information, however this model has been shown to work well for string-to-string translation. Linear reordering was improved with lexical distortion (Tillmann, 2004), which characterizes reordering in terms of type (monotone, swap, or discontinuous) as opposed to distance. This approach however is prone to sparsity problems, in particular for distant language pairs. In this paper we propose a general model for dependency tree-to-tree reordering based on ﬂexible non-terminals that can compactly encode multiple insertion positions. We explore how insertion positions can be selected even in cases where rules do not entirely cover the children of input sentence words. The proposed method greatly improves the ﬂexibility of translation rules at the cost of only"
P14-5014,C12-1120,1,0.90165,"Missing"
P14-5014,Y12-1033,1,0.51826,"Missing"
P14-5014,P05-1032,0,0.0621194,"Missing"
P14-5014,J07-2003,0,0.0747281,"a number of features and create a linear model scoring each possible combination of hypotheses (see Section 5). We then attempt to ﬁnd the combination that maximizes this model score. The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can eﬃciently ﬁnd the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁ"
P14-5014,P05-1022,0,0.0606666,"Missing"
P14-5014,D12-1107,0,0.120955,"only consider local features1 , then a simple bottom-up dynamic programming approach can eﬃciently ﬁnd the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 4). This lattice representation also allows the decoder to make choices between various morphological variations of a • Ex"
P14-5014,D11-1047,1,0.910385,"eval and translation hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed oﬄine. However, for a given input sentence i, the steps for ﬁnding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and sco"
P14-5014,W08-0402,0,0.0545771,"f rules is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can eﬃciently ﬁnd the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 4). This lattice representation also allows the d"
P14-5014,D07-1104,0,0.0190578,"ded translation. 3 Example retrieval and translation hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed oﬄine. However, for a given input sentence i, the steps for ﬁnding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full"
P14-5014,D11-1125,0,0.0337923,"m examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 4). This lattice representation also allows the decoder to make choices between various morphological variations of a • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the Pairwise Ranking Optimization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM4 . We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM. 6 Experiments In order to evaluate our system, we conducted translation experiments on four language pairs: Japanese-English (JA–EN), EnglishJapanese (EN–JA), Japanese-Chinese (JA– ZH) and Chinese-Japanese (ZH–JA). For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the oﬃcial baseline scores. For JapaneseChinese, we used parallel scientiﬁc paper excerpts from the ASP"
P14-5014,P05-1034,0,0.286199,"Missing"
P14-5014,N06-1023,1,0.797643,"Missing"
P14-5014,W11-2123,0,\N,Missing
P14-5014,2011.iwslt-evaluation.24,0,\N,Missing
P16-3002,N03-1017,0,0.0321069,"Missing"
P16-3002,li-etal-2010-enriching,0,0.0603641,"Missing"
P16-3002,W09-3818,0,0.0243735,"word if the nodes in dependency trees have different spans. For example, in Figure 1 there are two nodes for the word “boy” because they have different spans (i.e., (2, 4) and (2, 7)). The construction of a dependency forest from dependency trees is done by sharing the common nodes and edges (Line 1). The common nodes are those with the same span and part-of-speech (POS) . Note that the dependency forest obtained from this method does not necessarily encode exactly the dependency trees from which they are created. Usually there are more trees that can be extracted from the dependency forests (Boullier et al., 2009). In our experiment, when we use the term “a n-best dependency forest”, we indicate a dependency forest that is created from n-best dependency trees. 2.2 Finding Alignments over Forest Following the hierarchical alignment model (Riesa et al., 2011), our model searches for the best alignment by constructing partial alignments (hypotheses) over target dependency forests in a bottom-up manner as shown in Figure 1. The algorithm for constructing alignments is shown in Algorithm 2. Note that source dependency forests are included in the input to the algorithm. This is optional but can be included f"
P16-3002,I11-1089,1,0.748713,"e used 300, 100, 100 sentences from ASPEC-JE2 for training, development and test data, respectively.4 Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200.5 The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile6 , the Bayesian subtree alignment model (Nakazawa model) (Nakazawa and Kurohashi, 2011) and IBM Model4.7 We used Nile without automatically extracted rule features and constellation features to make a fair comparison with our model. We observed the improvement of alignments by using forests. We checked whether good parse trees were chosen when higher F-scores were achieved. It turned out that better parse trees led to higher F-scores, as shown in Figure 2a, but it was not always the case. Figure 2a shows an improved example by using 100-best trees on the target side. In the figure, we can observe that “の” and “of” are correctly aligned. We observe that the English 1-best parse t"
P16-3002,J04-4002,0,0.342213,"Missing"
P16-3002,J93-2003,0,0.150318,"Missing"
P16-3002,J07-2003,0,0.103676,"t, we compute its score using local features (Line 7) and pushed to a priority queue Bv (Line 8). These partial alignments are represented by black squares in a blue container in Figure 1. Then, we compute partial alignments for the target words covered by the node, by combining tails’ partial alignments and one column alignments for its word using nonlocal features (Line 10 - 14), which is represented by the orange arrows in Figure 1. k-best combined partial alignments are put in Yv (Line 14). They are represented by black squares in a yellow container in Figure 1. Here, we use cube pruning (Chiang, 2007) to get the approximate k-best combinations. Note that in the search over constituency parse trees, one column alignment matrices are generated only on the leaf node (Riesa et al., 2011), whereas we generate them also on nonleaf nodes in the search over dependency forests. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Input : Source and target sentence s, t Dependency forest Fs over s Dependency forest Ft over t Set of feature functions h Weight vector w Beam size k Output: A k-best list of alignments over s and t for v ∈TopologicalSort(Ft ) do links = ∅ Bv = ∅ i = word-index-of(v) links = {(0, i)} ∪Sin"
P16-3002,P11-1042,0,0.0350707,"Missing"
P16-3002,D11-1046,0,0.0967341,"one by sharing the common nodes and edges (Line 1). The common nodes are those with the same span and part-of-speech (POS) . Note that the dependency forest obtained from this method does not necessarily encode exactly the dependency trees from which they are created. Usually there are more trees that can be extracted from the dependency forests (Boullier et al., 2009). In our experiment, when we use the term “a n-best dependency forest”, we indicate a dependency forest that is created from n-best dependency trees. 2.2 Finding Alignments over Forest Following the hierarchical alignment model (Riesa et al., 2011), our model searches for the best alignment by constructing partial alignments (hypotheses) over target dependency forests in a bottom-up manner as shown in Figure 1. The algorithm for constructing alignments is shown in Algorithm 2. Note that source dependency forests are included in the input to the algorithm. This is optional but can be included for richer features. Each node in the forest has partial alignments sorted by alignment scores. Because it is computationally expensive to keep all possible partial alignments for each node, we keep a beam size of k. A partial alignment for a node i"
P16-3002,P08-1067,0,0.0388108,"alignments sorted by alignment scores. Because it is computationally expensive to keep all possible partial alignments for each node, we keep a beam size of k. A partial alignment for a node is an alignment matrix for target words that are cov9 ered by the node. In Figure 1, each partial alignment is represented as a black square. Scores of the partial alignments are a linear combination of features. There are two types of features: local and non-local features. A feature f is defined to be local if and only if it can be factored among the local productions in a tree, and non-local otherwise (Huang, 2008). We visit the nodes in the topological order, to guarantee that we visit a node after visiting all its tail nodes (Line 1). For each node, we first generates partial alignments, which are one column alignment matrices for its word. Because of time complexity, we only generates null, single link and double link alignment (Line 5). A single and double link alignment refer to a column matrix having exactly one and two alignments, respectively, as shown in Figure 1. For each partial alignment, we compute its score using local features (Line 7) and pushed to a priority queue Bv (Line 8). These par"
P16-3002,D10-1052,0,0.0546363,"Missing"
P16-3002,P08-1066,0,0.0553831,"Missing"
P16-3002,P14-1138,0,0.0195662,"ult: Black boxes represent golden alignments. Triangles represent 1-best model alignments. Circles represent the alignments of proposed model. Black and red arcs represent 1-best parses and chosen parses respectively. humans. However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests resp"
P16-3002,C10-1123,0,0.238834,"ing syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees. It achieved significantly better results than the IBM Model4 in Arabic-English and ChineseEnglish word alignment tasks, even though the model was trained on only 2,280 and 1,102 parallel sentences as gold standard alignments. However, their models rely only on 1-best source and target side parse trees, which are not necessarily good for word alignment tasks. In SMT, forest-based decoding has been proposed for both constituency and dependency parse trees (Mi et al., 2008; Tu et al., 2010). A forest is a compact representation of n-best parse trees. It provides more alternative parse trees to choose from during decoding, leading to significant improvements in translation quality. In this paper, we borrow this idea to build an alignment model using dependency forests rather than 1-best parses, which makes it possible to provide the model with more alternative parse trees that may be suitable for word alignment tasks. The motivation of using dependency forests instead of constituency forests in our model is that dependency forests are more appropriate for alignments between langu"
P16-3002,2008.amta-papers.18,0,0.0237822,". However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests respectively for decoding. The best path that is suitable for translation is chosen from the forest during decoding, leading to significant improvement in translation quality. Note that they do not use forests for obtaining word ali"
P16-3002,P13-1017,0,0.0179165,"riangles represent 1-best model alignments. Circles represent the alignments of proposed model. Black and red arcs represent 1-best parses and chosen parses respectively. humans. However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests respectively for decoding. The best path that is s"
P16-3002,N07-1051,0,\N,Missing
P16-3002,D09-1106,0,\N,Missing
P16-3002,D14-1197,0,\N,Missing
richardson-etal-2014-bilingual,W10-2408,0,\N,Missing
richardson-etal-2014-bilingual,P07-2045,0,\N,Missing
richardson-etal-2014-bilingual,J03-1002,0,\N,Missing
richardson-etal-2014-bilingual,I13-1030,1,\N,Missing
richardson-etal-2014-bilingual,J98-4003,0,\N,Missing
richardson-etal-2014-bilingual,I08-6006,0,\N,Missing
W06-0123,I05-3017,0,0.0673595,"sks, which are: (c) Wn (n=-1,0,1) Feature Wn (n=-1,0,1) mean the lexicon words in different positions (the word containing C0 and one word to its left and right) and they are also binary features. We select all the possible words in the lexicon that satisfy the requirements, not like only selecting the longest one in (J.K.Low et al.,2005). To create the lexicon, we use following steps. First, a lexicon from NICT (National Institute of Information and Communications Technology, Japan) is used as the basic lexicon, which is extracted from Peking University Corpus of the second SIGHAN Bakeoff (T.Emerson, 2005), Penn Chinese Treebank 4.0 (N.Xue et al.,2002), a Chinese-to-English Wordlist 1 and part of NICT corpus (K.Uchimoto et al.,2004; Y.J.Zhang et al.,2005). Then, all the words containing digits and letters are removed 1 http://projects.ldc.upenn.edu/Chinese/ from this lexicon. At last, all the punctuations in Penn Chinese Treebank 5.1 (N.Xue et al.,2002) and all the words in the training data of UPUC and MSRA corpuses are added into the lexicon. Besides of above features, some extra features are defined only for NER task. First, we add some character-based features to improve the accuracy of per"
W06-0123,I05-3025,0,0.0494544,"Missing"
W06-0123,C04-1067,0,0.0727626,"Missing"
W06-0123,W96-0213,0,0.260018,"Missing"
W06-0123,W04-2208,0,0.040584,"Missing"
W06-0123,C02-1145,0,0.0585979,"Missing"
W06-0123,I05-2015,0,0.0323549,"Missing"
W06-0123,W03-1026,0,\N,Missing
W06-0123,2005.mtsummit-papers.10,0,\N,Missing
W09-2302,P03-1012,0,0.0940334,"nformation from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also proposed a model focusing on the dependency relations. Their model has the constraint that content words can only correspond to content words on the other side, and the same applies for function words. This sometimes leads to an incorrect alignment. We have removed this constraint to make more flexible alignments possible. Moreover, in their model,"
W09-2302,W06-1628,0,0.0396246,"Missing"
W09-2302,P03-1011,0,0.0171911,"methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also pro"
W09-2302,N06-1023,1,0.761571,"thm. During the Step 2 iterations, word correspondences are grown into phrase correspondences. 2 Proposed Model We suppose that Japanese is the source language and English is the target language in the description of our model. Note that the model is not specialized for this language pair, and it can be applied to any language pair. Because our model uses dependency tree structures, both source and target sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment ˆ a between a given source sentence f and its target sentence e is acquired by the following equation: ˆ = argmax p(f , a|e) a a = argmax p(f |e, a) · p(a|e) a (1) 11 ཷ (accept) ග A"
W09-2302,N03-1017,0,0.0740256,"Missing"
W09-2302,N06-1014,0,0.0253971,"positions in a sentence. Finally, the proposed model can find the best ˆ by not using f -to-e alone, but simultaalignment a neously with e-to-f . That is, Equation 1 is modified as follows: ˆ = argmax p(f |e, a) · p(a|e) · a a p(e|f , a) · p(a|f ) (4) Since our model regards a phrase as a basic unit, the above formula is calculated in a straightforward way. In contrast, the IBM models can consider a many-to-one alignment by combining one-to-one alignments, but they cannot consider a one-to-many or many-to-many alignment. The models are estimated by EM-like algorithm which is very similar to (Liang et al., 2006). The important difference is that we are using tree structures. We maximize the data likelihood: ∑ max (log pef (f , e; θef ) + log pf e (f , e; θf e )) θef ,θf e f ,e (5) In the E-step, we compute the posterior distribution of the alignments with the current parameter θ: q(a; f , e) := pef (a|f , e; θef ) · pf e (a|f , e; θf e ) (6) In the M-step, we update the parameter θ: θ0 := argmax θ ∑ q(a; f , e) log pef (a, f , e; θef ) a,f ,e + ∑ q(a; f , e) log pf e (a, f , e; θf e ) a,f ,e = argmax θ ∑ a,f ,e + ∑ a,f ,e q(a; f , e) log p(e) · pef (a, f |e; θef ) q(a; f , e) log p(f ) · pf e (a, e|f"
W09-2302,H05-1066,0,0.0162692,"rrespondences are grown into phrase correspondences. 2 Proposed Model We suppose that Japanese is the source language and English is the target language in the description of our model. Note that the model is not specialized for this language pair, and it can be applied to any language pair. Because our model uses dependency tree structures, both source and target sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment ˆ a between a given source sentence f and its target sentence e is acquired by the following equation: ˆ = argmax p(f , a|e) a a = argmax p(f |e, a) · p(a|e) a (1) 11 ཷ (accept) ග A (light) ⣲Ꮚ photogate (device) 䛻 is"
W09-2302,W01-1406,0,0.0181239,"he other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discr"
W09-2302,2008.amta-papers.15,1,0.523133,"da and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also proposed a model focusing on the dependency relations. Their model has the constraint that content words can only correspond to content words on the other side, and the same applies for function words. This sometimes leads to an incorrect alignment. We have removed this constraint to make more flexible alignments possible. Moreover, in their model, some function words are brought together, and thus they cannot handle the situation where each function word corresponds to a different part. The smallest unit of our model is a single word, which should solve this problem. Proceedings of SSST"
W09-2302,J03-1002,0,0.0107511,"ග A (ni) 䛿 (ha) ཷ ཷ (accept) (light) photodetector ⏝䛔䛯 . 䚹 the 䜢 photodetector . 䚹 Figure 2: An example of hill-climbing. 4 Experimental Results We conducted alignment experiments. A JST1 Japanese-English paper abstract corpus consisting of 1M parallel sentences was used for the model training. This corpus was constructed from a 2M Japanese-English paper abstract corpus by NICT2 using the method of Uchiyama and Isahara (2007). As gold-standard data, we used 475 sentence pairs which were annotated by hand. The annotations were only sure (S) alignments (there were no possible (P ) alignments) (Och and Ney, 2003). The unit of evaluation was word-base for both Japanese and English. We used precision, recall, and F-measure as evaluation criteria. We conducted two experiments to reveal 1) the contribution of our proposed model compared to the existing models, and 2) the effectiveness of using dependency tree structure and phrases, which are larger alignment units than words. Trainings were run on the original forms of words for both the proposed model and the models used for comparison. 4.1 Comparison with Word Sequential Model For comparison, we used GIZA++ (Och and Ney, 2003) which implements the promi"
W09-2302,P05-1034,0,0.0606573,"Missing"
W09-2302,2007.mtsummit-papers.63,0,0.258947,"Missing"
W09-2302,C00-2131,1,0.765909,"difference in word order. The other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tre"
W09-2302,P01-1067,0,0.0712088,"nces. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohas"
W09-2302,J93-2003,0,\N,Missing
W09-2302,2001.mtsummit-ebmt.4,0,\N,Missing
W13-2505,C04-1151,0,0.794908,", 606-8501, Japan {chu,nakazawa}@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp Abstract Non–parallel corpora include various levels of comparability: noisy parallel, comparable and quasi–comparable. Noisy parallel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is signi"
W13-2505,I05-1059,0,0.0349896,"n Hanzi and Kanji. Table 1 gives some examples of common Chinese characters in Traditional Chinese, Simplified Chinese and Japanese with their Unicode. Since Chinese characters contain significant semantic information, and common Chinese characters share the same meaning, they can be valuable linguistic clues for many Chinese–Japanese NLP tasks. Many studies have exploited common Chinese characters. Tan et al. (1995) used the occurrence of identical common Chinese characters in Chinese and Japanese (e.g. “snow” in Table 1) in automatic sentence alignment task for document– level aligned text. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese, but different from Simplified Chinese (e.g. “love” in Table 1). Using a Chinese encoding converter1 that can convert Traditional Chinese into Simplified Chinese, they built a Japanese–Simplified Chinese dictionary partly using direct conversion of Japanese into Chinese for Japanese Kanji words. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chi"
W13-2505,P07-2045,0,0.00752983,"rable corpora. We adopt a system proposed by Munteanu and Marcu (2005), which is for parallel sentence extraction from comparable corpora. We extend the system in several aspects to make it even suitable for quasi–comparable corpora. The core component of the system is a classifier which can identify parallel sentences from non–parallel sentences. Previous method of classifier training by the Cartesian product is not practical, because it differs from the real process of parallel sentence extraction. We propose a novel Introduction In statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), the quality and quantity of the parallel sentences are crucial, because translation knowledge is acquired from a sentence–level aligned parallel corpus. However, except for a few language pairs, such as English–French, English–Arabic and English– Chinese, parallel corpora remain a scarce resource. The cost of manual construction for parallel corpora is high. As non–parallel corpora are far more available, constructing parallel corpora from non–parallel corpora is an attractive research field. 34 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 34–42, c Sofia, B"
W13-2505,N03-2016,0,0.0538577,"Missing"
W13-2505,W06-2810,0,0.0821037,"Missing"
W13-2505,2011.mtsummit-papers.53,1,0.83952,"Missing"
W13-2505,J05-4003,0,0.130834,"lel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conduct"
W13-2505,2012.eamt-1.7,1,0.810272,"training and testing method is that features from the IR results can be used. Here, we use the ranks of the retrieved documents returned by the IR framework as feature. 4 4.2.1 Settings • Probabilistic dictionary: We took the top 5 translations with translation probability larger than 0.1 created from the parallel corpus. Experiments We conducted classification and translation experiments to evaluate the effectiveness of our proposed parallel sentence extraction system. • IR tool: Indri7 with the top 10 results. • Segmenter: For Chinese, we used a segmenter optimized for Chinese–Japanese SMT (Chu et al., 2012a). For Japanese, we used JUMAN (Kurohashi et al., 1994). 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST3 and NICT4 . This corpus was created by the Japanese project “Development and Research of Chinese– Japanese Natural Language Processing Technology”, containing various domains such as chemistry, physics, biology and agriculture etc. This corpus is aligned in both sentence–level and document–level, containing 680k sentences and 100k articles. • Alignment: GIZA++8 . • SMT: We used the state–of–the–art phrase– based SMT toolkit"
W13-2505,P06-1011,0,0.0243421,"ank (Proposed)”, where parallel subsentential fragments are in bold. We investigated the alignment results of the extracted sentences. We found that most of the parallel subsentential fragments were correctly aligned with the help of the parallel sentences in the baseline system. Therefore, translation performance was improved by appending the extracted sentences. However, it also led to many wrong alignments among the non– parallel fragments which are harmful to translation. In the future, we plan to further extract these parallel subsentential fragments, which can be more effective for SMT (Munteanu and Marcu, 2006). 6 Conclusion and Future Work In this paper, we proposed a novel method of classifier training and testing that simulates the real parallel sentence extraction process. Furthermore, we used linguistic knowledge of Chinese character features. Experimental results of parallel sentence extraction from quasi–comparable corpora indicated that our proposed system performs significantly better than the previous study. 5 Related Work As parallel sentences trend to appear in similar document pairs, many studies first conduct document matching, then identify the parallel sen40 Our approach can be impro"
W13-2505,chu-etal-2012-chinese,1,0.833746,"training and testing method is that features from the IR results can be used. Here, we use the ranks of the retrieved documents returned by the IR framework as feature. 4 4.2.1 Settings • Probabilistic dictionary: We took the top 5 translations with translation probability larger than 0.1 created from the parallel corpus. Experiments We conducted classification and translation experiments to evaluate the effectiveness of our proposed parallel sentence extraction system. • IR tool: Indri7 with the top 10 results. • Segmenter: For Chinese, we used a segmenter optimized for Chinese–Japanese SMT (Chu et al., 2012a). For Japanese, we used JUMAN (Kurohashi et al., 1994). 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST3 and NICT4 . This corpus was created by the Japanese project “Development and Research of Chinese– Japanese Natural Language Processing Technology”, containing various domains such as chemistry, physics, biology and agriculture etc. This corpus is aligned in both sentence–level and document–level, containing 680k sentences and 100k articles. • Alignment: GIZA++8 . • SMT: We used the state–of–the–art phrase– based SMT toolkit"
W13-2505,N10-1063,0,0.489733,"anslated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extract Chinese–Japanese parallel sentences from quasi–comparable corpora, which are"
W13-2505,2012.eamt-1.37,0,0.0343286,"Missing"
W13-2505,P09-2057,0,0.767195,"igned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting"
W13-2505,P03-1010,0,0.55095,"asi–comparable. Noisy parallel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many"
W13-2505,2007.mtsummit-papers.63,0,0.0421083,"lations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extra"
W13-2505,I05-1023,0,0.0313432,"with two different approaches: binary classification (Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; S¸tefˇanescu et al., 2012) and translation similarity measures (Utiyama and Isahara, 2003; Fung and Cheung, 2004; Abdul-Rauf and Schwenk, 2011). We adopt the binary classification approach with a novel classifier training and testing method and Chinese character features. Few studies have been conducted for extracting parallel sentences from quasi–comparable corpora. We are aware of only two previous efforts. Fung and Cheung (2004) proposed a multi-level bootstrapping approach. Wu and Fung (2005) exploited generic bracketing Inversion Transduction Grammars (ITG) for this task. Our approach differs from the previous studies that we extend the approach for comparable corpora in several aspects to make it work well for quasi–comparable corpora. Table 4: BLEU scores for Chinese–to–Japanese translation experiments (“†” and “‡” denotes the result is better than “Munteanu+ 2005 (Cartesian)” significantly at p &lt; 0.05 and p &lt; 0.01 respectively, “*” denotes the result is better than “Baseline” significantly at p &lt; 0.01). 4.3.3 Discussion The translation results indicate that compared to the pre"
W13-2505,J93-2003,0,\N,Missing
W14-7001,W14-7005,0,0.0335781,"Missing"
W14-7001,W14-7011,1,0.826327,"Missing"
W14-7001,P11-2093,0,0.053248,"h language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ configuration for the string-to-tree syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring 5 6 http://nlp.stanford.edu/software/segmenter.shtml 3 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html 4 Figure 1: The submission web page for participants For Japanese segmentation we use three different tools, which are Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 7 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 8 . For Chinese segmentation we use two different tools, which are KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 9 (Tseng, 2005). For English segmentation we use tokenizer.perl 10 in the Moses toolkit. The detailed procedures for the automatic evaluation are shown at WAT2014 evaluation web page 11 . after WAT2014. Everybody can use the system by registering on the registration web page 12 . 5 Human Evaluat"
W14-7001,W14-7007,0,0.033389,"Missing"
W14-7001,W14-7002,0,0.0428991,"ieved better quality WEBLIO-EJ1 1 in the automatic evaluation, howthan RBMT system. ever it is much worse in the human evaluation. According to the descriptions of the two submissions, • The translation quality of the widely used the difference of the two is whether it uses the forsystems was Phrase-based SMT < Hierarchiest input or not. It is natural that using the forcal PBSMT < Syntax-based SMT (S2T and est input improves the translation quality, thus we T2S). conducted the human evaluation of WEBLIO-EJ1 • Forest-to-String Syntax-based SMT system 2 compared to WEBLIO-EJ1 1, which means we (Neubig, 2014) achieved the best quality for used WEBLIO-EJ1 1 as the baseline for the huall the translation directions. man evaluation. The HUMAN score was 2.50 ± 4.17 which Statistical Significance Testing between means there is no significant difference between Submissions the two, and this result is far from the results of Tables 5, 6, 7 and 8 show the results of statistical the official results. Actually, taking the confidence significance testings of JE, EJ, JC and CJ transintervals into consideration, this conclusion can be lations respectively where all the pairs of submisderived under some probabil"
W14-7001,P13-2121,0,0.018775,"Missing"
W14-7001,W14-7006,0,0.0412465,"Missing"
W14-7001,2009.iwslt-papers.4,0,0.110246,"ine system at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this paper. We describe the d"
W14-7001,P02-1040,0,0.11454,"Words. • distortion-limit = 20 • msd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT 4 Automatic Evaluation We used the following Moses’ configuration for the hierarchical phrase-based SMT system. 4.1 Procedure of Calculating Automatic Evaluation Score • max-chart-span = 1000 • Phrase score option: GoodTuring We calculated automatic evaluation scores of the translation results applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated with multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated with RIBES.py version 1.02.4 6 . All scores of each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results have been tokenized with word segmentation tools on each language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ c"
W14-7001,P06-1055,0,0.0857644,"we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this paper. We describe the detail of the baseline SMT systems. 2.2 ASPEC-JC ASPEC-JC is a parallel corpus consisting of J"
W14-7001,W14-7008,0,0.0763585,"Missing"
W14-7001,D10-1092,0,0.117188,"sd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT 4 Automatic Evaluation We used the following Moses’ configuration for the hierarchical phrase-based SMT system. 4.1 Procedure of Calculating Automatic Evaluation Score • max-chart-span = 1000 • Phrase score option: GoodTuring We calculated automatic evaluation scores of the translation results applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated with multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated with RIBES.py version 1.02.4 6 . All scores of each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results have been tokenized with word segmentation tools on each language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ configuration for the string-to-tr"
W14-7001,W14-7012,1,0.854354,"Missing"
W14-7001,P07-2045,0,0.0113934,"ilarity score. baseline system at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this pap"
W14-7001,W14-7003,0,0.0378915,"Missing"
W14-7001,W04-3250,0,0.570886,"Missing"
W14-7001,2007.mtsummit-papers.63,0,0.0465476,"submit translation results at any time. 2.1 ASPEC-JE The training data of ASPEC-JE was constructed by the NICT from approximately 2 million Japanese-English scientific paper abstracts owned by the JST. Because the paper abstracts are kind • Domain and language pairs WAT is the world’s first workshop that uses 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 1 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 1‒19, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). of comparable corpora, the sentence correspondences are automatically found using the method of (Utiyama and Isahara, 2007). Each sentence pair is accompanied with the similarity score and the field symbol. The similarity scores are calculated by the method of (Utiyama and Isahara, 2007). The field symbols are single letters AZ and show the scientific field of each document2 . The correspondance between the symbols and field names, along with the frequency and occurance ratios for the training data, are given in the README of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from Japanese-English paper abstracts owned by JST that are not contained in the training data"
W14-7001,W14-7010,0,0.0441404,"Missing"
W14-7001,W14-7004,0,0.0675856,"Missing"
W14-7001,W15-5003,0,\N,Missing
W14-7001,W15-5009,0,\N,Missing
W14-7001,W15-5007,0,\N,Missing
W14-7001,W14-7009,0,\N,Missing
W14-7001,W15-5008,0,\N,Missing
W14-7001,W15-5012,0,\N,Missing
W14-7001,W15-5010,0,\N,Missing
W14-7001,W15-5013,0,\N,Missing
W14-7001,W15-5005,0,\N,Missing
W14-7012,P05-1022,0,0.0795328,"Missing"
W14-7012,D11-1047,1,0.840049,"trict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in (Cromières and Kurohashi, 2011) to perform this step as efficiently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from 1. We project the part of s that is matched into the target side t using the alignment of s and t. This is trivial if each word of s and t is aligned, but this is not typically the case. Therefore our translation hypotheses will often have some target words/nodes marked as optionals: this means that we will decide if they should be added to the final translation only at the moment of combination. 2. We insert the"
W14-7012,N06-1023,1,0.85285,"Missing"
W14-7012,Y12-1033,1,0.90464,"Missing"
W14-7012,D07-1104,0,0.0445887,"Missing"
W14-7012,P05-1032,0,0.0996159,"Missing"
W14-7012,P05-1034,0,0.142807,"Missing"
W14-7012,N12-1047,0,0.0343404,"earch space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our system using an additional two language models: a standard 7-gram language model with Modified Kneser-Ney smoothing and a Recurrent Neural Network Language Model (RNNLM) (Mikolov et. al, 2010). The RNNLM model was trained with hidden layer size 200, and 5000 sentences from the training fold were used as validation data. Reranking was conducted by first calculating the various language model scores for each 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation"
W14-7012,J07-2003,0,0.0674616,"er of features and create a linear model scoring each possible combination of hypotheses (see Section 5). We then attempt to find the combination that maximizes this model score. The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our sys"
W14-7012,2011.iwslt-evaluation.24,0,0.0249287,"Missing"
W14-7012,D12-1107,0,0.0121569,"sider local features1 , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our system using an additional two language models: a standard 7-gram language model with Modified Kneser-Ney smoothing and a Recurrent Neural Network Language Model (RNNLM) (Mikolov et. al, 2010). The RNNLM model was trained with hidden layer size 200, and 5000 sentences from the training fold"
W14-7012,W08-0402,0,0.017326,"is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our system using an additional two language models: a standard 7-gram language model with Modified Kneser-Ney smoothing and a Recurrent Neural Network Language Model (RNNLM) (Mikolov et. al, 2010). The RNNLM model was tra"
W14-7012,C12-1120,1,\N,Missing
W14-7012,D14-1063,1,\N,Missing
W14-7012,W11-2123,0,\N,Missing
W15-5001,W15-5008,0,0.0448879,"Missing"
W15-5001,W15-5013,0,0.0370801,"Missing"
W15-5001,W15-5002,0,0.0289439,"Missing"
W15-5001,W14-7001,1,0.205867,"ve been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Evaluation method Evaluation is done both automatically and manually. For human evaluation, WAT uses crowdsourcing, which is low cost and allows multiple evaluations, as the first-stage evaluation. Also, JPO adequacy evaluation is conducted for the selected submissions according to the crowdsourcing evaluation results. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshop WAT2014(Nakazawa et al., 2014), WAT2015 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 2nd WAT, we adopt new translation subtasks “Chinese-to-Japanese and Koreanto-Japanese patent translation” in addition to the subtasks that were conducted in WAT2014. WAT is unique for the following reasons: 2 Dataset WAT uses the Asian Scientific Paper Excerpt Corpus (ASPEC)1 and JPO Patent Corpus (JPC) 2 as the dataset. 2.1 ASPEC ASPEC is constructed by t"
W15-5001,P11-2093,1,0.853461,"on results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific papers subtask (J ↔ E, J ↔ C); – Patents subtask (C → J, K → J); • Method (SMT, RBMT, SMT and RBMT, EBMT, Other); 10 http://code.google.com/p/mecab/downloads/detail? name=mecab-ipadic-2.7.0-20070801.tar.gz 11 http://nlp.stanford.edu/software/segmenter.shtml 12 https://bitbucket.org/eunjeon/mecab-ko/ 13 https://github.com/moses-smt/mosesdecoder/tree/ RELEASE-2.1.1/scripts/tokenizer/tokenizer.perl 14 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html http://w"
W15-5001,P13-2121,0,0.0884737,"Missing"
W15-5001,2009.iwslt-papers.4,0,0.108842,"em as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignment"
W15-5001,W15-5003,1,0.888058,"Missing"
W15-5001,D10-1092,0,0.23015,"2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific paper"
W15-5001,P02-1040,0,0.109103,"the other system parameters. 4 4.2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005)"
W15-5001,P06-1055,0,0.112959,"we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignments, GIZA++ and growdiag-final-and heuristics were used. We used 5gram language models with mod"
W15-5001,P07-2045,0,0.0208005,"ich is the same system as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To"
W15-5001,W15-5006,1,0.742862,"Missing"
W15-5001,W04-3250,0,0.568022,"Missing"
W15-5001,W15-5010,0,0.0468531,"Missing"
W15-5001,W15-5005,0,0.0362327,"Missing"
W15-5001,W15-5012,0,0.0480478,"Missing"
W15-5001,W15-5009,0,0.0622163,"Missing"
W15-5001,2007.mtsummit-papers.63,0,0.08182,"2015. 2015 Copyright is held by the author(s). LangPair ASPEC-JE ASPEC-JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Test 1,812 2,107 LangPair JPC-CJ JPC-KJ Table 1: Statistics for ASPEC. Train 1,000,000 1,000,000 Dev 2,000 2,000 DevTest 2,000 2,000 Test 2,000 2,000 Table 2: Statistics for JPC. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by the NICT from approximately 2 million Japanese-English scientific paper abstracts owned by the JST. Because the abstracts are comparable corpora, the sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score and the field symbol. The similarity scores are calculated by the method from (Utiyama and Isahara, 2007). The field symbols are single letters A-Z and show the scientific field for each document3 . The correspondence between the symbols and field names, along with the frequency and occurrence ratios for the training data, are given in the README file from ASPECJE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts owned by JST that are not contained in the tr"
W15-5001,W15-5011,0,0.0304094,"Missing"
W15-5001,W15-5007,0,0.153198,"Missing"
W15-5006,P05-1022,0,0.129705,"Missing"
W15-5006,N12-1047,0,0.0279359,"2014), which makes use of our dependency parses in order to capture non-local reorderings. 3.2 • Forest parse scores • Number of content/function words aligned to content/function words • Number of times a subtree is inserted in a position (left or right of parent) that is not the most common in the training data • Number of examples sharing the same information used to create an initial hypothesis • Similarity between source and input word embeddings (Mikolov et al., 2013) Forest Input The optimal weights for each feature are as before estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. We found that the quality of the source-side dependency parsing had an important impact 3 http://kheaﬁeld.com/code/kenlm/ 57 3.4 Reranking translation in the n-best list. These features were added to those used in the ﬁrst round of tuning, then one ﬁnal iteration of tuning was run. The tuning algorithm and settings were the same as for standard tuning. This retuning step was added in order to ﬁnd an optimal combination of the additional features with related features such as sentence length and the score given by the 5-gram language model used inside the decoder. A ﬁnal rer"
W15-5006,J07-2003,0,0.0620462,"X2, morphological forms of “be”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the"
W15-5006,D11-1047,1,0.850442,"view Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and sco"
W15-5006,C12-1120,1,0.883803,"Missing"
W15-5006,P14-2024,0,0.0236533,"ble combination of hypotheses. This linear model is based on a linear combination of both local features (local to each translation hypothesis) and non-local features (such as a 5-gram language model score of the ﬁnal translation). Despite our already relatively large set of dense features, we found there were a number of cases where these features were not enough to diﬀerentiate between good and bad translation hypotheses. This year we have added ten new features, now reaching a total of 52, a selection of which are shown below: Improvements from WAT2014 3.1 Alignment Based on the ﬁndings of Neubig and Duh (2014), we experimented with supervised alignment using Nile (Riesa et al., 2011) as part of our translation framework. We found that using supervised alignments made a considerable improvement to translation quality. Since Nile supports only constituency parses, we also perform constituency parsing for source and target languages for generating bidirectional word alignments. For the initial alignments for Nile, we use the alignments generated from the model described in last year’s system description (Richardson et al., 2014), which makes use of our dependency parses in order to capture non-local r"
W15-5006,W11-2123,0,0.0137398,"”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-term"
W15-5006,N15-3009,0,0.0231441,"Missing"
W15-5006,2011.iwslt-evaluation.24,0,0.0153808,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,P06-1055,0,0.256442,"Missing"
W15-5006,D12-1107,0,0.0117256,"n-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the decoder to make choices between various morphological variations of a wo"
W15-5006,P05-1034,0,0.166664,"Missing"
W15-5006,N06-1023,1,0.726271,"Missing"
W15-5006,W08-0402,0,0.0128267,"hat we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the"
W15-5006,D11-1046,0,0.0924047,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,Y12-1033,1,0.720835,"Missing"
W15-5006,D07-1104,0,0.0236985,"g. We believe that 2 System Overview Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full"
W16-4601,W16-4616,1,0.921374,"n and Communication Technology Waseda University Ehara NLP Research Laboratory NTT Communication Science Laboratories Weblio, Inc. Indian Institute of Technology Bombay Japan Patent Information Organization Indian Institute of Technology Patna University of Tokyo University of Tokyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Offi"
W16-4601,W16-4609,0,0.0501384,"Missing"
W16-4601,W16-4617,0,0.0323933,"Missing"
W16-4601,P13-2121,0,0.0165266,"evelopment data. 3.2 Common Settings for Baseline SMT We used the following tools for tokenization. • Juman version 7.08 for Japanese segmentation. • Stanford Word Segmenter version 2014-01-049 (Chinese Penn Treebank (CTB) model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko10 for Korean segmentation. • Indic NLP Library11 for Hindi segmentation. To obtain word alignments, GIZA++ and grow-diag-final-and heuristics were used. We used 5-gram language models with modified Kneser-Ney smoothing, which were built using a tool in the Moses toolkit (Heafield et al., 2013). 3.3 Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT We used the following Moses configuration for the hierarchical phrase-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-"
W16-4601,2009.iwslt-papers.4,0,0.0367694,"tp://lotus.kuee.kyoto-u.ac.jp/WAT/Hindi-corpus/WAT2016-Ja-Hi.zip 3 LangPair IITB-EH IITB-JH Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 http://lotus.kuee.kyot"
W16-4601,W16-4611,0,0.032371,"Missing"
W16-4601,D10-1092,0,0.148885,"rser to obtain source language syntax. We used the following Moses configuration for the baseline tree-to-string syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. The default values were used for the other system parameters. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994"
W16-4601,W16-4612,0,0.0719596,"kyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Official Evaluation Results Figures 2, 3, 4 and 5 show the official evaluation results of ASPEC subtasks, Figures 6, 7, 8, 9 and 10 show those of JPC subtasks, Figures 11 and 12 show those of BPPT subtasks and Figures 13 and 14 show those of IITB subtasks. Each figure contains automa"
W16-4601,P07-2045,0,0.0173325,"ee syntax-based 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/Hindi-corpus/WAT2016-Ja-Hi.zip 3 LangPair IITB-EH IITB-JH Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 h"
W16-4601,W04-3250,0,0.322367,"Missing"
W16-4601,W15-5008,0,0.0203255,"eement between the workers, we calculated the Fleiss’ κ (Fleiss and others, 1971) values. The results are shown in Table 20. We can see that the κ values are larger for X → J translations than for J → X translations. This may be because the majority of the workers are Japanese, and the evaluation of one’s mother tongue is much easier than for other languages in general. 7.3 Chronological Evaluation Figure 15 shows the chronological evaluation results of 4 subtasks of ASPEC and 2 subtasks of JPC. The Kyoto-U (2016) (Cromieres et al., 2016), ntt (2016) (Sudoh and Nagata, 2016) and naver (2015) (Lee et al., 2015) are NMT systems, the NAIST (2015) (Neubig et al., 2015) is a forest-to-string SMT system, Kyoto-U (2015) (Richardson et al., 2015) is a dependency tree-to-tree EBMT system and JAPIO (2016) (Kinoshita et al., 2016) system is a phrase-based SMT system. What we can see is that in ASPEC-JE and EJ, the overall quality is improved from the last year, but the ratio of grade 5 is decreased. This is because the NMT systems can output much fluent translations 24 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html 12 but the adequacy is worse. As for ASPEC-JC and CJ, the quality is very much impro"
W16-4601,W16-4608,0,0.0513487,"Missing"
W16-4601,W14-7001,1,0.885529,"red tasks from the 3rd workshop on Asian translation (WAT2016) including J↔E, J↔C scientific paper translation subtasks, C↔J, K↔J, E↔J patent translation subtasks, I↔E newswire subtasks and H↔E, H↔J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014) and WAT2015 (Nakazawa et al., 2015), WAT2016 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 3rd WAT, we adopt new translation subtasks with English-Japanese patent description, Indonesian-English news description and Hindi-English and Hindi-Japanese mixed domain corpus in addition to the subtasks that were conducted in WAT2015. Furthermore, we invited research papers on topics related to the machine translation"
W16-4601,W15-5001,1,0.853738,"sian translation (WAT2016) including J↔E, J↔C scientific paper translation subtasks, C↔J, K↔J, E↔J patent translation subtasks, I↔E newswire subtasks and H↔E, H↔J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014) and WAT2015 (Nakazawa et al., 2015), WAT2016 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 3rd WAT, we adopt new translation subtasks with English-Japanese patent description, Indonesian-English news description and Hindi-English and Hindi-Japanese mixed domain corpus in addition to the subtasks that were conducted in WAT2015. Furthermore, we invited research papers on topics related to the machine translation, especially for Asian languages. Th"
W16-4601,P11-2093,1,0.785718,"s et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 14 . For Chinese segmentation we used two different tools: KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 15 (Tseng, 2005). For Korean segmentation we used mecabko 16 . For English and Indonesian segmentations we used tokenizer.perl 17 in the Moses toolkit. For Hindi segmentation we used Indic NLP Library 18 . Detailed procedures for the automatic evaluation are shown on the WAT2016 evaluation web page 19"
W16-4601,W16-4610,1,0.870902,"Missing"
W16-4601,P02-1040,0,0.0994357,"sed SMT We used the Berkeley parser to obtain source language syntax. We used the following Moses configuration for the baseline tree-to-string syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. The default values were used for the other system parameters. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman versi"
W16-4601,P06-1055,0,0.0157429,"Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 4 5 System ID SMT Phrase SMT Hiero SMT S2T SMT T2S RBMT X RBMT X RBMT X RBMT X"
W16-4601,W15-5006,1,0.884753,"Missing"
W16-4601,W16-4622,0,0.0346093,"Missing"
W16-4601,W16-4623,0,0.0353036,"Missing"
W16-4601,W16-4604,0,0.0409014,"Missing"
W16-4601,W16-4621,0,0.131935,"mation Organization Indian Institute of Technology Patna University of Tokyo University of Tokyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Official Evaluation Results Figures 2, 3, 4 and 5 show the official evaluation results of ASPEC subtasks, Figures 6, 7, 8, 9 and 10 show those of JPC subtasks, Figures 11 and 12 show those of"
W16-4601,W16-4618,0,0.0231168,"Missing"
W16-4601,2007.mtsummit-papers.63,0,0.716723,"Information and Communications Technology (NICT). It consists of a JapaneseEnglish scientific paper abstract corpus (ASPEC-JE), which is used for J↔E subtasks, and a JapaneseChinese scientific paper excerpt corpus (ASPEC-JC), which is used for J↔C subtasks. The statistics for each corpus are described in Table1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by the NICT from approximately 2 million JapaneseEnglish scientific paper abstracts owned by the JST. Because the abstracts are comparable corpora, the sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score and the field symbol. The similarity scores are calculated by the method from (Utiyama and Isahara, 2007). The field symbols are single letters A-Z and show the scientific field for each document5 . The correspondence between the symbols and field names, along with the frequency and occurrence ratios for the training data, are given in the README file from ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts owned by JST that are not contained in the t"
W16-4601,W16-4620,0,0.0195884,"Missing"
W16-4601,W16-4619,0,0.038307,"Missing"
W16-4616,C16-2064,1,0.550675,"Japanese translation direction of the ASPEC-JC task, and further updated the state-of-the-art result (46.04 → 46.36 BLEU). 3 NMT NMT is a new approach to MT that, although recently proposed, has quickly achieved state-of-the-art results (Bojar et al., 2016). We implemented our own version of the sequence-to-sequence with attention mechanism model, first proposed in (Bahdanau et al., 2015). Our implementation was done using the Chainer3 toolkit (Tokui et al., 2015). We make this implementation available under a GPL license.4 3 4 http://chainer.org/ https://github.com/fabiencro/knmt . See also (Cromieres, 2016) 167 Figure 2: The structure of a NMT system with attention, as described in (Bahdanau et al., 2015) (but with LSTMs instead of GRUs). The notation “&lt;1000&gt;” means a vector of size 1000. The vector sizes shown here are the ones suggested in the original paper. 3.1 Overview of NMT We describe here briefly the (Bahdanau et al., 2015) model. As shown in Figure 2, an input sentence is first converted into a sequence of vector through an embedding layer; these vectors are then fed to two LSTM layers (one going forward, the other going backward) to give a new sequence of vectors that encode the input"
W16-4616,W16-4601,1,0.866497,"Missing"
W16-4616,L16-1350,1,0.87803,"Missing"
W16-4616,W15-5006,1,0.864664,"Missing"
W16-4616,W16-2323,0,0.0339875,"els instead of one. The two “classic” ways of combining the prediction of different systems are to either take the geometric average or the arithmetic average of their predicted probabilities. Interestingly, although it seems other researchers have reported that using the arithmetic average works better (Luong et al., 2016), we actually found that geometric average was giving better results for us. Ensembling usually works best with independently trained parameters. We actually found that even using parameters from a single run could improve results. This had also been previously observed by (Sennrich et al., 2016). Therefore, for the cases when we could only run one training session, we ensembled the three parameters corresponding to the best development loss, the best development BLEU, and the final parameters (obtained after continuing training for some time after the best development BLEU was found). We refer to this as “self-ensembling” in the result section. When we could do n independent training, we kept these three parameters for each of the independent run and ensembled the 3 · n parameters. 3.7 Preprocessing The preprocessing steps were essentially the same as for KyotoEBMT. We lowercased the"
W16-4616,C16-1029,1,0.86956,"Missing"
W16-4616,P16-1008,0,0.0308784,"luency and meaning of the translations. This is due to the reason that the word order of the translations in EBMT depends on the parse trees of the input sentences, but the parsing accuracy is not perfect especially for Chinese. NMT tends to produce fluent translations, however it lacks of adequacy sometimes. The most common problem of NMT is that it could produce under or over translated translations, due to the lack of a way for the attention mechanism to memorize the source words that have been translated during decoding. We plan to address this problem with the coverage model proposed in (Tu et al., 2016). The UNK words are also a big problem. Although we deal with them using the UNK replacement method (Luong et al., 2015), it could simply fail because of errors for finding the corresponding source words using attention. We show a Japanese-to-English translation example by our EBMT and NMT systems in Table 2 to illustrate some of the above problems. The translation produced by the EBMT system has a word order problem that changes the meaning, making “the basic composition, standard” independent from “this flow sensor.” It also has an agreement violation problem between the argument and the pre"
W16-5407,C96-2184,0,0.0597353,"nnotation are selected from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chin"
W16-5407,P07-2045,0,0.00552999,"bowl wall section 26 defines an opening 28 and a leachate chamber 29 is located in the wall 24 beneath the opening 28. annular 碗状壁区 section 26 defines opening 28, leach liquid chamber 29 in the wall 24 opening 28 below. the annular bowl-shaped wall section 26 defines opening 28, leach liquid chamber 29 is positioned below the opening 28 in the wall 24. Table 5: An improved MT example. task at the NTCIR-10 workshop10 (Goto et al., 2013). The NTCIR-CE task uses 1,000,000, 2,000, and 2,000 sentences for training, development, and testing, respectively. We used the Moses tree-to-string MT system (Koehn et al., 2007) for all of our MT experiments. In our experiments, Chinese is in the tree format, and Japanese/English is in the string format. For Chinese, we used KyotoMorph for segmentations and the Berkeley parser for joint POS tagging and parsing. We binarized the parsing results for better translation rule extraction. We compared the MT performance of the “Baseline” and “Baseline+SCTB” settings in Section 3.1. For Japanese, we used JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-g"
W16-5407,W04-3250,0,0.13052,"d a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements are due to analysis improvements of the source sentences. Table 5 shows an improved MT example from the NTCIR-CE task. We can see that there is an out-ofvocabulary word"
W16-5407,J93-2004,0,0.0540819,"Missing"
W16-5407,W15-5001,1,0.854089,"ences to the baseline treebanks for training the analyzers. Figure 3 shows the results. We can see that for segmentation and POS tagging, the accuracy improvements slow down when more annotated sentences are used for training the analyzers; while for parsing, there is still a large potential of improvement by annotating more sentences. 3.2 MT Experiments For Chinese-to-Japanese translation, we conducted experiments on the scientific domain MT task on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ)8 (Nakazawa et al., 2016), which is one subtask of the workshop on Asian translation (WAT)9 (Nakazawa et al., 2015). The ASPEC-CJ task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. For Chinese-to-English translation, we conducted experiments on the Chinese-English subtask (NTCIR-CE) of the patent MT 8 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ http://orchid.kuee.kyoto-u.ac.jp/WAT/ 63 System Baseline Baseline+SCTB ASPEC-CJ 39.12 40.08† NTCIR-CE 33.19 33.90† Table 4: BLEU-4 scores for ASPEC-CJ and NTCIR-CE translation tasks (“†” indicates that the result is significantly better than “Baseline” at p < 0.01). Source Reference Baseline Baseline +SCTB 环形碗状壁区段２６限定了开口"
W16-5407,L16-1262,0,0.021801,"Missing"
W16-5407,P03-1021,0,0.0123856,"d JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements"
W16-5407,C14-1026,0,0.0271706,"urce sentence in Table 5 “浸出 (leach ) /液 (liquid) /腔室 (chamber) /２９/位于 (is located) /壁 (wall) /２４/中 (in) /开口 (opening) /２８/之下 (beneath)”. chamber)” into “leach liquid chamber”, this is due to the similar analysis results of both systems, while the correct analysis for this noun phrase should be “(NP (NP 浸出 NN 液 SFN) 腔室 NN) (leachate chamber)”. 4 Related Work Besides the widely used CTB (Xue et al., 2005), there are two other treebanks for Chinese. The Peking University (PKU) annotated a Chinese treebank, firstly only for segmentations and POS tags (Yu et al., 2003), and later also for syntax (Qiu et al., 2014). The Harbin Institute of Technologys (HIT) also annotated a treebank for dependency structures (Che et al., 2012). Besides the difference in annotation standards and syntactic structures, all the three treebanks are in news domain. CTB selected the raw sentences from People’s Daily, Hong Kong newswire, Xinhua newswire etc., and PKU and HIT selected the raw sentences from People’s Daily newswire. To the best of our knowledge, our treebank is the first publicly available Chinese treebank in scientific domain. Three are two types of syntactic grammars for treebanking: phrase structures and depen"
W16-5407,P14-2042,1,0.898261,"Missing"
W16-5407,C16-1029,1,0.912447,"of downstream applications such as text mining and machine translation (MT). Motivated by this, we decide to construct a Chinese treebank in the scientific domain (SCTB) to promote Chinese NLP research in this domain. This paper presents the details of our treebank annotation process and the experiments conducted on the annotated treebank. The raw sentences are selected from Chinese scientific papers. Our annotation process follows that of CTB (Xue et al., 2005) with an exception of the segmentation standard. We apply a Chinese word segmentation standard based on character-level POS patterns (Shen et al., 2016), aiming to circumvent inconsistency and address data 1 2 https://catalog.ldc.upenn.edu/LDC2005T01 Statistics from Japan Patent Office. 59 Proceedings of the 12th Workshop on Asian Language Resources, pages 59–67, Osaka, Japan, December 12 2016. Figure 1: A screenshot of the annotation interface containing an annotation example of a Chinese sentence “烟草 (tobacco) /使用 (use) /是 (is) /当今 (nowadays) /世界 (world) /最大 (biggest) /的 (’s) /可 (can) /预防 (prevention) /死因 (cause of death) /，/烟草 (tobacco) /使用 (use) /者 (person) /中 (among) /近 (about) /一半 (half) /将 (will) /死于 (die) /烟草 (tobacco) /使用 (use) /。” ("
W16-5407,L16-1249,0,0.024799,"rase structures and dependency structures. We adopt the phrase structures used in CTB (Xue et al., 2005), because phrase structures can be converted to dependency structures based on predefined head rules using e.g. the Penn2Malt toolkit.14 Treebanks with multi-view of both phrase structures and dependency structures also have been proposed (Qiu et al., 2014). Recently, with more needs of multilingual NLP, the interests of constructing multilingual treebanks have increased. Multilingual treebanks such as the universal dependency treebank15 (Nivre et al., 2016) and the Asian language treebank (Thu et al., 2016) are being constructed. As the raw sentences of our treebank were selected from parallel data and the translated Japanese and English sentences are available, we leave the potential to develop our treebank to a trilingual one. 5 Conclusion In this paper, we presented the details of the annotation of SCTB: a Chinese treebank in scientific domain. Experiments conducted for Chinese analysis and MT verified the effectiveness of the annotated SCTB. As future work, firstly, we plan to annotate more sentences, and we aim to finish the annotation for 10k sentences within this year. Secondly, we also p"
W16-5407,xia-etal-2000-developing,0,0.591073,"ed from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chinese word segmentat"
W16-5407,zhang-etal-2004-interpreting,0,0.102482,"Missing"
W17-5701,W17-5715,0,0.0507339,"Missing"
W17-5701,W04-3250,0,0.243434,"Missing"
W17-5701,W17-5714,1,0.815327,"Missing"
W17-5701,P07-2045,0,0.0182137,"f a hierarchical phrase-based SMT system, a string-to-tree syntax-based SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 h"
W17-5701,W17-5710,0,0.0950012,"Missing"
W17-5701,P13-2121,0,0.0263037,"Missing"
W17-5701,2009.iwslt-papers.4,0,0.017237,"ase-based SMT system, a string-to-tree syntax-based SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 http://lotus.kuee.kyot"
W17-5701,W17-5709,0,0.0346313,"Missing"
W17-5701,W17-5711,0,0.0304192,"Missing"
W17-5701,W17-5716,0,0.0397813,"Missing"
W17-5701,D10-1092,0,0.0485922,"GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. The default values were used for the other system parameters. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl which was distributed 9 http://nlp.ist.i.kyotou.ac.jp/EN/index.php?JUMAN 10 http://nlp.stanford.edu/software/segmenter.shtml 11 https://bitbucket.org/eunjeon/mecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding reference. Before the calculat"
W17-5701,W17-5706,0,0.0506466,"Missing"
W17-5701,W17-5713,0,0.03618,"Missing"
W17-5701,W14-7001,1,0.898058,"ic evaluation server, and selected submissions were manually evaluated. 1 Sadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. There is no deadline of translation result submission with respect to automatic evaluation of translation quality and WAT receives submissions at any time. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014), WAT2015 (Nakazawa et al., 2015) and WAT2016 (Nakazawa et al., 2016), WAT2017 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 4th WAT, we adopted new translation subtasks with English-Japanese news corpus and English-Japanese recipe corpus in addition to the subtasks at WAT2016 1 . Fur• Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Ko"
W17-5701,2007.mtsummit-papers.63,0,0.586487,"nstitute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for J↔E subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for J↔C subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence 2.2 JPC JPC was constructed by the Japan Patent Office (JPO). The corpus consists of ChineseJapanese patent description corpus (JPC-CJ), Korean-Japanese patent description corpus (JPC-KJ) and English-Japanese patent description corpus (JPC-EJ) with the sections of Chemistry, Electricity, Mechanical engineering, and Physics on the basis of International Patent Classification (IPC). Each corpus is partitioned into training, development, development-test and test data. This corpus is used for patent subtasks C↔J, K↔J and E↔J. The statistics for each corpus are shown 2 http://lotus"
W17-5701,W15-5001,1,0.885412,"d submissions were manually evaluated. 1 Sadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. There is no deadline of translation result submission with respect to automatic evaluation of translation quality and WAT receives submissions at any time. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014), WAT2015 (Nakazawa et al., 2015) and WAT2016 (Nakazawa et al., 2016), WAT2017 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 4th WAT, we adopted new translation subtasks with English-Japanese news corpus and English-Japanese recipe corpus in addition to the subtasks at WAT2016 1 . Fur• Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In"
W17-5701,W17-5707,0,0.110884,"Missing"
W17-5701,W17-5708,0,0.0494718,"Missing"
W17-5701,P11-2093,1,0.766986,"ecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 14 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 15 . For Chinese segmentation, we used two different tools: KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 16 . For Korean segmentation we used mecab-ko 17 . For English segmentation, we used tokenizer.perl 18 in the Moses toolkit. For Hindi segmentation, we used Indic NLP Library 19 . The detailed procedures for the automatic evaluation are shown on the WAT2017 evaluation web page 20 . 4.2 N"
W17-5701,W17-5712,1,0.882844,"Missing"
W17-5701,P02-1040,0,0.105207,"n = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. The default values were used for the other system parameters. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl which was distributed 9 http://nlp.ist.i.kyotou.ac.jp/EN/index.php?JUMAN 10 http://nlp.stanford.edu/software/segmenter.shtml 11 https://bitbucket.org/eunjeon/mecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding"
W17-5701,P06-1055,0,0.0160054,"d SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 4 5 ✓ ✓ ✓ ✓ ✓ ✓ ✓ JPC IITB JIJI RECIPE EJ JC CJ JK KJHE EH JE EJ JE EJ ✓ ✓ ✓ ✓"
W17-5714,C16-2064,1,0.872421,"Missing"
W17-5714,W16-4616,1,0.879551,"Missing"
W17-5714,W17-3206,0,0.0819212,". Which is why all results presented in this paper are related to the LSTM-based model. Such feed-forward models probably have high potentials for the future, as they are more computationally efficient and do obtain state-of-the-art results on certain language directions. But, currently, we do not find that they should be necessarily preferred to recurrent architectures. 2.2 Direct connection from previous word to attention model There is an interesting flaw in the original architecture of the model (as well as in the model described in (Bahdanau et al., 2015)). This is briefly mentioned3 in (Goto and Tanaka, 2017), but we will expand on the details a bit more here. The attention mechanism computes the current context using only the previous decoder state as input. But the previous decoder state has been itself computed before the previously generated target word was selected. Therefore, when computing the current context, the attention mechanism is totally unaware of the previously generated word. Intuitively, this seems wrong: the attention should certainly depend on the previously generated word. Therefore, we add another input to the attention model: the previous word embedding. To be precise, re-us"
W17-5714,W11-2123,0,0.0728444,"Missing"
W17-5714,W16-2316,0,0.0291316,"semble, having 3 layers on the encoder and 2 on the decoder. 3.5 Averaging and Ensembling It is well known that using an ensemble of several independently trained models can boost NMT performances by several BLEU points. We did this in the same way as was described in (Cromi`eres et al., 2016). On top of ensembling independently trained models, we had found it useful to also make an ensemble with the parameters of the same model corresponding respectively to the best loss, best dev BLEU and last obtained during the training process (a practice which we will call here selfensemble). Following (Junczys-Dowmunt et al., 2016), we tried to compute averaged parameters instead of ensembling models. We found this to work surprisingly well. We observed only nonsignificant BLEU drops (by about 0.1 BLEU). But with the benefit that the averaged model has the same time and space complexity as a single model, while an ensemble of N models has N times the time and space complexity of a single model. We therefore switched to this averaging approach instead of the self-ensemble approach10 . 4 Zh → Ja Submission 1 corresponds to an ensemble of 5 models, three of them having 2 layers for encoders and decoders, and two of them ha"
W17-5714,P15-1002,0,0.0345332,"or sizes shown here are the ones suggested in the original paper. We use this general architecture for our model, but the single LSTMs are replaced by stacks of LSTMs. We also add a connection from the target embedding to the attention model, as suggested by (Goto and Tanaka, 2017), which was not in the original model (see section 2.2) We used subword segmentation for all target languages, so as to reduce the target vocabulary size. This makes the translation process more efficient memory-wise and computation-wise, while mostly avoiding the need for unknown-word replacement tricks such as in (Luong et al., 2015). The subword segmentation was done using the BPE algorithm (Sennrich et al., 2015) 6 . For the Japanese-Chinese language pair, we learned a joint segmentation (as suggested in (Sennrich et al., 2015)). We used a character equivalence map (Chu et al., 2013) to maximize the number of common characters between Japanese and Chinese when learning the joint segmentation. The joint segmentation was aimed at producing a vocabulary size of about 40,000 words for both the source and target vocabulary. For the Japanese-English language pair, we did not use a joint segmentation. We created a BPE model of"
W17-5714,W16-4601,1,0.753152,"ne for NMT. Our Kyoto-NMT system largely relies on an implementation of this model, with small modifications. Kyoto-NMT is implemented using the Chainer1 toolkit (Tokui et al., 2015). We make this implementation available under a GPL license.2 Introduction This paper describes our experiments for the WAT 2017 shared translation task. For more details refer to the overview paper (Nakazawa et al., 2017). This translation task contains several subtasks, but we focused on the ASPEC dataset, for the Japanese-English and Japanese-Chinese language pairs. Following up on our findings during WAT 2016 (Nakazawa et al., 2016) that our Neural Machine Translation system yielded significantly better results than our Example-Based Machine Translation system, we only experimented with NMT this year. Our improvements are actually quite incremental, with only small changes in the model architectures, model sizes, training and decoding approaches. Together, these small changes, however, allow us to improve over our past year’s results by several BLEU points, leading to the best official results for the Japanese-Chinese pair. In terms of pairwise human evaluation scores we have the best official results for all language di"
W17-5714,C16-1029,1,0.799847,"of our models, as well as the pre-processing we applied to the data. where eij is the unnormalized attention coefficient on source word j when decoding target word at step i, si−1 is the decoder state at step i − 1, and hj is the encoding of source word j. The matrices Wa and Ua , and the vector va are the parameters of the alignment model. We replace this equation with: 3.1 Preprocessing As a first preprocessing step, English sentences were tokenized and lowercased. Both Japanese sentences and Chinese sentences were automatically segmented, respectively with JUMAN5 (Kurohashi, 1994) and SKP (Shen et al., 2016). eij = vaT tanh(Wa ·si−1 +Ua ·hj +Xa ·Ey−1 ) (2) 4 3 tion The author had previously mentioned this to us in private communications. 5 147 but see section 4.2.1 for our attempt at system combinahttp://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN Figure 1: The structure of a NMT system with attention, as described in (Bahdanau et al., 2015) (but with LSTMs instead of GRUs). The notation “<1000>” means a vector of size 1000. The vector sizes shown here are the ones suggested in the original paper. We use this general architecture for our model, but the single LSTMs are replaced by stacks of LSTMs."
Y14-1032,N13-1075,0,0.210807,"262–271 !262 PACLIC 28 pairs to compare the similarity between them. Data sparseness makes the vector representations sparse (e.g., the vector of a low frequent word tends to have many zero entries), thus they do not always reliably represent the meanings of words. Therefore, the similarity of word pairs can be inaccurate. Smoothing technology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc.,"
Y14-1032,P05-1074,0,0.0563113,"rase pairs more accurately. Finally, we compute the similarity of phrase pairs based on the smoothed source and target vectors. In this way, we improve the quality of comparable features, which can improve the accuracy of the phrase table thus improve SMT performance. Details of paraphrase generation, comparable feature estimation and vector smoothing with paraphrases will be described in Section 4.1, 4.2 and 4.3 respectively. 4.1 Paraphrase Generation In this paper, we generate both source and target phrasal level paraphrases from the parallel corpus used for SMT4 through bilingual pivoting (Bannard and Callison-Burch, 2005). The idea of this method is that if two source phrases f1 and f2 are translated to the same target phrase e, we can assume that f1 and f2 are a paraphrase pair. Probability of this paraphrase pair can be assigned by marginalizing over 4 Paraphrases also can be generated from external parallel corpora and monolingual corpora, however we leave it as future work. !265 all shared target translations e in the parallel corpus, defined as follows: p(f1 |f2 ) = ! e φ(f1 |e)φ(e|f2 ) (1) where, φ(f1 |e) and φ(e|f2 ) are phrase translation probability. Target paraphrases can be generated in a similar wa"
Y14-1032,N06-1003,0,0.0324651,"udi, 2011; Irvine et al., 2013). Moreover, studies have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and th"
Y14-1032,2012.eamt-1.7,1,0.851342,"vectors for the phrase “unemployment figures” before and after smoothing. 5 Experiments In our experiments, we compared our proposed method with (Klementiev et al., 2012). We estimated comparable features from comparable corpora using the method of (Klementiev et al., 2012) and our proposed method respectively. We appended the comparable features to the phrase table, and evaluated the two methods in the perspective of SMT performance. We conducted experiments on Chinese-English data. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012), and tokenizing English sentences. 5.1 Experimental Settings SMT Settings We conducted Chinese-to-English translation experiments. The parallel corpus we used is from Chinese-English NIST open MT.6 The “NIST” column of Table 4 shows the statistics of this parallel corpus. For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting"
Y14-1032,P11-2071,0,0.0449146,"Missing"
Y14-1032,D10-1041,0,0.014692,"ng φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are incorrect. Another method is constructing a paraphrase lattice for the tuning and testing data, and performing lattice decoding (Du et al., 2010; Bar and Dershowitz, 2014). Paraphrases also can be incorporated as additional training data, which may improve both coverage and accuracy of SMT (Pal et al., 2014). Previous studies require external data in addition to the parallel corpus used for SMT for paraphrase generation to make their methods effective. These paraphrases can be generated from external parallel corpora (Callison-Burch et al., 2006; Du et al., 2010), or monolingual corpora based on distributional similarity (Marton et al., 2009; Razmara et al., 2013; Pal et al., 2014; Bar and Dershowitz, 2014). Our study differs from pre"
Y14-1032,N04-1035,0,0.0348047,"hnology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc., therefore it is suitable for the data sparseness problem. We generate paraphrases from the parallel corpus used for translation model learning. Then, we use the paraphrases to smooth the vectors of the translation pairs in the translation model for comparable feature estimation. Smoothing is done by learning vectors that combine the v"
Y14-1032,W12-3134,0,0.0156949,"hrases 6,273 46,191 # paraphrases 39.8 21.6 # Bigrams w/ paraphrases 15,026 223,299 Avg # paraphrases 34.6 17.7 # Trigrams w/ paraphrases 5,419 185,609 # paraphrases 20.0 14.9 Table 6: Statistics the generated paraphrases for the phrases and individual words inside the phrases in the filtered phrase table. tered phrase table. We can see that each Chinese phrase has a large number of translations on average especially for the lower order n-gram phrases, which can indicate the inaccuracy of the filtered phrase table. Our proposed method requires paraphrases for vector smoothing. We used Joshua (Ganitkevitch et al., 2012) to generate both Chinese and English paraphrases from the parallel corpus. We kept the paraphrase pairs that satisfy logp(x1 |x2 ) &gt; −7 and logp(x2 |x1 ) &gt; −7 14 for smoothing, where p(x1 |x2 ) is the probability that x1 is a paraphrase of x2 , and p(x2 |x1 ) is the probability that x2 is a paraphrase of x1 . Table 6 shows the statistics of the paraphrase generation results for the Chinese and English phrases, and individual words inside the phrases in the filtered phrase table. Note that, for some phrase pairs, their comparable feature scores may be 0, because of data sparseness. In that cas"
Y14-1032,W09-1117,0,0.016764,"ntax-based context etc. In this paper, we use window-based context, and leave the comparison of using different definitions of context as future work. Given a phrase, we count all its immediate context words, with a window size of 4 (2 preceding words and 2 following words). We build a context by collecting the counts in a bag of words fashion, namely we do not distinguish the positions that the context words appear in. The number of dimensions of the constructed vector is equal to the vocabulary size. We further reweight each component in the vector by multiplying by the IDF score following (Garera et al., 2009; Chu et al., 2014), which is defined as follows: IDF (t, D) = log |D| 1 + |{d ∈ D : t ∈ d}| (2) where |D |is the total number of documents in the corpus, and |{d ∈ D : t ∈ d} |denotes number of documents where the term t appears.5 We model the source and target vectors using the method described above, and project the source vector onto the vector space of the target language using a seed dictionary. The contextual similarity of the phrase pair is the similarity of the vectors, which is computed using cosine similarity defined as follows: &quot;K Fk × Ek #&quot; K 2 2 k=1 (Fk ) × k=1 (Ek ) Cos(f, e) ="
Y14-1032,P08-1088,0,0.0286005,"ation. The con6 LDC2007T02, LDC2002T01, LDC2003T17, LDC2004T07, HK News part of LDC2004T08, LDC2005T10 and LDC2006T04 7 http://www.speech.sri.com/projects/srilm !268 # Zh articles # En articles # Zh sentences # En sentences # Zh tokens # En tokens NIST N/A N/A 991k 991k 26.1M 27.2M Gigaword 3.6M 4.3M 42.6M 56.9M 1.1B 1.3B Wikipedia 248k 248k 2.8M 10.1M 70.5M 240.5M Table 4: Statistics of the comparable data used for comparable feature estimation. textual feature was estimated on the parallel corpus. We treated the two sides of the parallel corpus as independent monolingual corpora, following (Haghighi et al., 2008; Klementiev et al., 2012). Contextual feature estimation requires a seed dictionary. The seed dictionary we used is NIST ChineseEnglish translation lexicon Version 3.0,8 containing 82k entries. The temporal feature was estimated on Chinese9 and English10 Gigaword version 5.0. We used the afp, cna and xin sections with date range 1994/05-2010/12 of the corpora. The topical feature was estimated on Chinese and English Wikipedia data. We downloaded Chinese11 (2012/09/21) and English12 (2012/10/01) Wikipedia database dumps. We used an open-source Python script13 to extract and clean the text from"
Y14-1032,W13-2233,0,0.23776,"ns.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1 Scarceness of parallel corpora al"
Y14-1032,N13-1056,0,0.109849,"ns.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1 Scarceness of parallel corpora al"
Y14-1032,D13-1109,0,0.0221904,"Missing"
Y14-1032,P06-1103,0,0.128113,"odels. !263 BLE (Klementiev et al., 2012; Irvine and CallisonBurch, 2013a). The results verify the effectiveness of using BLE together with paraphrases for the accuracy problem of SMT. 2 Related Work 2.1 Bilingual Lexicon Extraction (BLE) for SMT From the pioneering work of (Rapp, 1995), BLE from comparable corpora has been studied for a long time. BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages. Contextual similarity (Rapp, 1995), topical similarity (Vuli´c et al., 2011) and temporal similarity (Klementiev and Roth, 2006) can be important clues for BLE. Orthographic similarity may also be used for BLE for some similar language pairs (Koehn and Knight, 2002). Moreover, some studies try to use the combinations of different similarities for BLE (Irvine and Callison-Burch, 2013b; Chu et al., 2014). To address the data sparseness problem of BLE, smoothing technology has been proposed (Pekar et al., 2006; Andrade et al., 2013). BLE can be used to address the accuracy problem of SMT, which estimates comparable features for the translation pairs in the translation model (Klementiev et al., 2012). BLE also can be used"
Y14-1032,E12-1014,0,0.0677873,"language pairs and domains.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1"
Y14-1032,W02-0902,0,0.136879,"Missing"
Y14-1032,N03-1017,0,0.120216,"d pairs can be inaccurate. Smoothing technology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc., therefore it is suitable for the data sparseness problem. We generate paraphrases from the parallel corpus used for translation model learning. Then, we use the paraphrases to smooth the vectors of the translation pairs in the translation model for comparable feature estimation. Smoothing is do"
Y14-1032,P07-2045,0,0.00477633,"e phrase table, and evaluated the two methods in the perspective of SMT performance. We conducted experiments on Chinese-English data. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012), and tokenizing English sentences. 5.1 Experimental Settings SMT Settings We conducted Chinese-to-English translation experiments. The parallel corpus we used is from Chinese-English NIST open MT.6 The “NIST” column of Table 4 shows the statistics of this parallel corpus. For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting, and used it for all the experiments. We used NIST open MT 2002 and 2003 data sets for tuning and testing, containing 878 and 919 sentence pairs respectively. Note that both MT 2002 and 2003 data sets contain 4 references for each Chinese sentence. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experi"
Y14-1032,W04-3250,0,0.0529101,"“Baseline” denotes the baseline system that does not use comparable features. “Klementiev+” denotes the system that appends the comparable features estimated following (Klementiev et al., 2012) to the phrase table. “Proposed” denotes the system that uses the comparable features estimated by our proposed method. “+Contextual”, “+Topical” and “+Temporal” denote the systems that append contextual, topical and temporal features respectively. “+All” denotes the system that appends all the three types of features. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). We can see that “Klementiev+” does not always outperform “Baseline”. The reason for this is that the comparable features estimated by (Klementiev et al., 2012) are inaccurate. “Proposed” performs significantly better than both “Baseline” and “Klementiev+”. The reason for this is that “Proposed” deals with the data sparseness problem of BLE for comparable feature estimation, making the features more accurate thus improve the SMT performance. As for different comparable features of “Proposed”, “+Contextual”, “+Topical” and “+Temporal” are all helpful, and combining them can be more effective."
Y14-1032,D09-1040,0,0.125371,"have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are incorrect. Another me"
Y14-1032,P03-1021,0,0.0189184,"heart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting, and used it for all the experiments. We used NIST open MT 2002 and 2003 data sets for tuning and testing, containing 878 and 919 sentence pairs respectively. Note that both MT 2002 and 2003 data sets contain 4 references for each Chinese sentence. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. Comparable Feature Estimation Settings Table 4 shows the statistics of the comparable data used for comparable feature estimation. The con6 LDC2007T02, LDC2002T01, LDC2003T17, LDC2004T07, HK News part of LDC2004T08, LDC2005T10 and LDC2006T04 7 http://www.speech.sri.com/projects/srilm !268 # Zh articles # En articles # Zh sentences # En sentences # Zh tokens # En tokens NIST N/A N/A 991k 991k 26.1M 27.2M Gigaword 3.6M 4.3M 42.6M 56.9M 1.1B 1.3B Wikipedia 248k 248k 2.8M 10.1M 70.5M 240.5M Table 4: Statistics of the comparable data used for comparable fea"
Y14-1032,P95-1050,0,0.428112,"h Phrase-based SMT (PBSMT) (Koehn et al., 2003).2 Experimental results show that our proposed method can improve SMT performance, compared to the previous studies that estimate comparable features without dealing with the data sparseness problem of 2 Our proposed method can also be applied to other language pairs and SMT models. !263 BLE (Klementiev et al., 2012; Irvine and CallisonBurch, 2013a). The results verify the effectiveness of using BLE together with paraphrases for the accuracy problem of SMT. 2 Related Work 2.1 Bilingual Lexicon Extraction (BLE) for SMT From the pioneering work of (Rapp, 1995), BLE from comparable corpora has been studied for a long time. BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages. Contextual similarity (Rapp, 1995), topical similarity (Vuli´c et al., 2011) and temporal similarity (Klementiev and Roth, 2006) can be important clues for BLE. Orthographic similarity may also be used for BLE for some similar language pairs (Koehn and Knight, 2002). Moreover, some studies try to use the combinations of different similarities for BLE (Irvine and Callison-Burch, 2013b; C"
Y14-1032,P13-1109,0,0.0524129,"3). Moreover, studies have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are"
Y14-1032,P11-2084,0,0.0522009,"Missing"
Y14-1032,J93-2003,0,\N,Missing
Y15-1033,D14-1179,0,0.0191739,"Missing"
Y15-1033,D07-1103,0,0.657787,"ntences and 4.5M terms) parallel data via pivot-based SMT. We generate a large pivot translation model using the Ja-En and En-Zh parallel data. Moreover, a small direct Ja-Zh translation model is generated using small-scale Ja-Zh parallel data. (680k sentences and 561k terms). Both the direct and pivot translation models are used to translate the Ja terms in the Ja-En dictionaries to Zh and the Zh terms in the Zh-En dictionaries to Ja to construct a large-scale Ja-Zh dictionary (about 3.6M terms). We address the noisy nature of pivoting large phrase tables by statistical significance pruning (Johnson et al., 2007). In addition, we exploit linguistic knowledge of common Chinese characters (Chu et al., 2013) shared in Ja-Zh to further improve the translation model. Large-scale experiments on scientific domain data indicate that our proposed method achieves high quality dictionaries which we manually verify to have a high quality. Reranking the n-best list produced by the SMT decoder is known to help improve the translation quality given that good quality features are used (Och et al., 2004). In this paper, we use bilingual neural network language model features for reranking the n-best list produced by t"
Y15-1033,P07-2045,0,0.143247,"o be a possible way of constructing a dictionary for the language pairs that have scarce parallel data (Tsunakawa et al., 2009; Chu et al., 2015). The assumption of this method is that there is a pair of large-scale parallel data: one between the source language and an intermediate resource rich language (henceforth called pivot), and one between that pivot and the target language. We can use the source-pivot and pivot-target parallel data to develop a source-target term1 translation model for dictionary construction. Pivot-based SMT uses the log linear model as conventional phrase-based SMT (Koehn et al., 2007) does. This method can address the data sparseness problem of directly merging the source-pivot and pivot-target terms, because it can use the portion of terms to generate new terms. Small-scale experiments in (Tsunakawa et al., 2009) showed very low 1 In this paper, we call the entries in the dictionary terms. A term consists of one or multiple tokens. accuracy of pivot-based SMT for dictionary construction.2 This paper presents our study to construct a largescale Japanese-Chinese (Ja-Zh) scientific dictionary, using large-scale Japanese-English (Ja-En) (49.1M sentences and 1.4M terms) and En"
Y15-1033,D09-1141,0,0.0502113,"iplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007) has shown to be quite effective in a bilingual scenario, it has never been considered in a pivot language scenario. (Tsunakawa et al., 2009) was the first work that constructs a dictionary"
Y15-1033,P02-1040,0,0.092256,"ioned in Section 4. The following scores are reported: • BS+RRCBLEU: Using character BLEU to rerank the n-best list. • BS+RRWBLEU: Using word BLEU to rerank the n-best list. • BS+RRSVM: Using SVM to rerank the n-best list. This is followed by substituting the OOVs with the character level translations using the learned neural translation models (which we label as +OOVsub). 5.2.3 Evaluation Criteria Following (Tsunakawa et al., 2009), we evaluated the accuracy on the test set using three metrics: 1 best, 20 best and Mean Reciprocal Rank (MRR)(Voorhees, 1999). In addition, we report the BLEU-4 (Papineni et al., 2002) scores that were computed on the word level. 5.2.4 Results of Automatic Evaluation Table 3 shows the evaluation results. We also show the percentage of OOV terms,11 and the accuracy with and without OOV terms respectively. In general, we can see that Pivot performs better than Direct, because the data of Ja-En and En-Zh is larger than that of Ja-Zh. Direct+Pivot shows better performance than either method. Different pruning methods show different performances, where Pr:P-T improves the accuracy, while the other two not. To understand the reason for this, we also investigated the statistics of"
Y15-1033,E12-1055,0,0.0162567,"loped a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007"
Y15-1033,P14-2042,1,0.828997,"AS LCAS title ISTIC pc ASPEC Size 3,588,800 22,610,643 19,905,978 3,013,886 6,090,535 1,070,719 1,562,119 680,193 shows the statistics of the bilingual dictionaries used for training. • Parallel corpora: the scientific Ja-En, En-Zh and Ja-Zh corpora we used were also provided by JST and ISTIC, containing 49.1M , 8.7M and 680k sentence pairs respectively. Table 2 shows the statistics of parallel corpora used for training. Among which ISTIC pc was provided by ISTIC, and the others were provided by JST. 5.2.1 In our experiments, we segmented the Chinese and Japanese data using a tool proposed by Shen et al. (2014) and JUMAN (Kurohashi et al., 1994) respectively. For decoding, we used Moses (Koehn et al., 2007) with the default options. We trained a word 5-gram language model on the Zh side of all the En-Zh and Ja-Zh training data (14.4M sentences) using the SRILM toolkit10 with interpolated Keneser-Ney discounting. Tuning was performed by minimum error rate training which also provides us with the n-best lists used to learn reranking weights. As a baseline, we compared following three methods for training the translation model: • Direct: Only use the Ja-Zh data to train a direct Ja-Zh model. Table 2: S"
Y15-1033,N07-1061,0,0.02745,"character based neural MT to eliminate the out-of-vocabulary (OOV) terms, which further improves the quality. The rest of this paper is structured as follows: Section 2 reviews related work. Section 3 presents our dictionary construction using pivot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weig"
Y15-1033,P07-1108,0,0.0645188,"Missing"
Y15-1033,P09-1018,0,0.0175042,"ot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked we"
Y15-1042,D09-1092,0,0.0869491,"Missing"
Y15-1042,P95-1050,0,0.257109,"ning data. After describing the baseline model (bilingual LDA), we introduce three novel methods of taking advantage of data including a pivot language P , such as SP + P -T and S-P -T data. 370 3.1 Baseline: Bilingual LDA We begin with a baseline non-pivot lexicon extraction model MST : S × T → R that gives a similarity score to a source-target word pair (using S-T training data). The non-pivot lexicon extraction model MST makes use of a bilingual topic similarity measure. We elected to use bilingual topic models rather than the more intuitive method of comparing monolingual context vectors (Rapp, 1995) as we believe topic modelling is more suitable for processing uncommon language pairs. This is because a bilingual seed lexicon is required for methods that learn a mapping between source and target vector spaces, such as Haghighi et al. (2008), in order to match crosslanguage word pairs. This data is unlikely to be available in sufficient quantity for low-resource language pairs, however comparable documents can be found from sources such as Wikipedia. We base our implementation on the state-ofthe-art system of Vulić et al. (2011) for comparison. This method uses the bilingual Latent Dirichl"
Y15-1042,P10-1011,0,0.0195602,"hods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have considered a variety of methods such as canonical correlation analysis (Haghighi et al., 2008) and label propagation (Tamura et al., 2012). We use the method of bilingual topic modelling (Vulić et al., 2011), which has been recently applied to a variety of fields such as transliteration mining (Richardson et al., 2013). 3 Model Details We consider the task of translating a source word s from language S to a target word t from language T . The baseline mode"
Y15-1042,C96-2098,0,0.202542,"ama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have considered a variety of methods such as canonical correlation analysis (Haghighi et al., 2008) and label propagation (Tamura et al., 2012). We use the method of bilingual topic modelling (Vulić et al., 2011), which has been recently applied to a variety of fields such as transliteration mining (Richardson et al., 2013). 3 Model Details We consider the task of translating a source word s from languag"
Y15-1042,C94-1048,0,0.0388793,"ting their application to a practical scenario, and compare their effectiveness to mainstream approaches. 2 Related Work The use of pivot models has been a common theme in the development of Natural Language Processing systems that deal with low-resource languages. In the field of Machine Translation, pivot models can be used in both decoding and the construction of parallel training data. Utiyama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have consi"
Y15-1042,N07-1061,0,0.0391231,"ion pages 369 - 377 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by John Richardson, Toshiaki Nakazawa and Sadao Kurohashi PACLIC 29 present a variety of solutions to this problem, demonstrating their application to a practical scenario, and compare their effectiveness to mainstream approaches. 2 Related Work The use of pivot models has been a common theme in the development of Natural Language Processing systems that deal with low-resource languages. In the field of Machine Translation, pivot models can be used in both decoding and the construction of parallel training data. Utiyama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 19"
Y15-1042,P08-1089,0,0.0352667,"the development of Natural Language Processing systems that deal with low-resource languages. In the field of Machine Translation, pivot models can be used in both decoding and the construction of parallel training data. Utiyama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have considered a variety of methods such as canonical correlation analysis (Haghighi et al., 2008) and label propagation (Tamura et al., 2012). We use the method of bilingual"
Y15-1042,P13-2050,0,0.0157914,"ge pairs, particularly when one of those languages is English, previous direct approaches fail when there is no such data available. For many language pairs there simply does not exist comparable (and even less so parallel) data. Even for languages with a large volume of available parallel data, most corpora cover only limited domains. There are two natural methods to deal with this problem: constructing or mining new data for the direct approach, and finding new ways to make better use of what data is already available. For an example of the construction of comparable corpora, see Zhu et al. (2013). We take the second approach and design pivot-based models for bilingual lexicon extraction. The major advantage of using a pivot language is that it is possible to take advantage of the large volume of comparable data sharing a common language such as English. In this paper we develop pivot-based approaches to make use of modern bilingual lexicon extraction methods that can be trained on comparable corpora. We present a selection of efficient algorithms using the framework of topic modelling (Blei et al., 2003). Topic modelling has been a popular approach for bilingual lexicon extraction, ho"
Y15-1042,D12-1003,0,\N,Missing
Y15-1042,P08-1088,0,\N,Missing
Y15-1042,P11-2084,0,\N,Missing
Y15-1042,I13-1030,1,\N,Missing
Y15-1042,N15-1125,1,\N,Missing
Y18-3001,Y18-3013,1,0.888887,"Missing"
Y18-3001,Y18-3003,1,0.889659,"Missing"
Y18-3001,Y18-3005,0,0.0459044,"Missing"
Y18-3001,P17-4012,0,0.0426364,". 3 each participant’s system. That is, the specific baseline system was the standard for human evaluation. At WAT 2018, we adopted a neural machine translation (NMT) with attention mechanism as a baseline system except for the IITB tasks. We used a phrasebased statistical machine translation (SMT) system, which is the same system as that at WAT 2017, as the baseline system for the IITB tasks. The NMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page.5 We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems. In addition to the NMT baseline systems, we have SMT baseline systems for the tasks that started at last year or before last year. The baseline systems are shown in Tables 8, 9, and 10. SMT baseline systems are described in the previous WAT overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online tra"
Y18-3001,Y18-3002,0,0.0375601,"Missing"
Y18-3001,P07-2045,0,0.0120426,"in frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeC"
Y18-3001,W04-3250,0,0.312277,"Missing"
Y18-3001,Y18-3007,0,0.0326187,"Missing"
Y18-3001,Y18-3014,0,0.0304859,"Missing"
Y18-3001,W14-7001,1,0.458489,"ion (WAT2018) including Ja↔En, Ja↔Zh scientific paper translation subtasks, Zh↔Ja, K↔Ja, En↔Ja patent translation subtasks, Hi↔En, My↔En mixed domain subtasks and Bn/Hi/Ml/Ta/Te/Ur/Si↔En Indic languages multilingual subtasks. For the WAT2018, 17 teams participated in the shared tasks. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2017 (Nakazawa et al., 2014; Nakazawa et al., 2015; Nakazawa et al., 2016; Nakazawa et al., 2017), WAT2018 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 5th WAT, we adopted new translation subtasks with Myanmar ↔ EnSadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp glish mixed domain corpus1 and Bengali/Hindi/Malayalam/Tamil/Telugu/Urdu/Sinhalese ↔ English OpenSubtitles corpus2 in addition to the subtasks at WAT2017. WAT is the uniq"
Y18-3001,W16-4601,1,0.938773,"Missing"
Y18-3001,P11-2093,0,0.0504246,"leu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.14 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.15 For Korean segmentation, we 11 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html 12 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/ 13 http://www.phontron.com/kytea/model. html 14 http://code.google.com/p/mecab/ downloads/detail?name=mecab-ipadic-2.7. 0-20070801.tar.gz 15 http://nlp.stanford.ed"
Y18-3001,Y18-3011,0,0.141475,"Missing"
Y18-3001,P02-1040,0,0.119424,"://bitbucket.org/anoopk/indic_nlp_ library 10 https://github.com/rsennrich/ subword-nmt • tgt vocab size = 100000 • src words min frequency = 1 • tgt words min frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanes"
Y18-3001,Y18-3010,0,0.0585011,"Missing"
Y18-3001,Y18-3012,0,0.0444067,"Missing"
Y18-3001,2007.mtsummit-papers.63,0,0.0425147,"itute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for ja↔en subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for ja↔zh subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million JapaneseEnglish scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score that are calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the frequency and occurrence ratios for the training data, are described in the README file of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts that exclude the sentences in the training data. Each dataset consists of 400 documents and contains sentences in each field at the same rate. The document ali"
Y18-3001,Y18-3017,0,0.0591037,"Missing"
Y18-3001,Y18-3006,1,0.868302,"Missing"
