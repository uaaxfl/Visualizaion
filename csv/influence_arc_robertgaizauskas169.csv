A00-1012,H92-1022,0,0.0136449,"Missing"
A00-1012,J96-2004,0,0.017698,"xed Mixed Mixed Upper Mixed 84 93 90 97 96 97 5 5 68 78 76 90 89 67 5 5 76 85 82 94 92 79 5 5 Table 1: Results from Human Annotation Experiment The performance of the human annotators on the upper case text is quite significantly lower than the reported performance of the algorithms which performed punctuation disambiguation on standard text as described in Section 2. This suggests that the performance which may be obtained for this task may be lower than has been achieved for standard text. ~Sarther insight into the task can be gained from determining the degree to which the subjects agreed. Carletta (1996) argues that the kappa statistic (a) should be adopted to judge annotator consistency for classification tasks in the area of discourse and dialogue analysis. It is worth noting that the problem of sentence boundary detection presented so far in this paper has been formulated as a classification task in which each token boundary has to be classifted as either being a sentence boundary or not. Carletta argues that several incompatible measures of annotator agreement have been used in discourse analysis, making comparison impossible. Her solution is to look to the field of content analysis, whic"
A00-1012,P98-2140,0,0.023444,"Missing"
A00-1012,A97-1001,0,0.0225575,"generally in single case (usually upper), unpunctuated and may contain transcription errors. 1 Figure 1 compares a short text in the format which would be produced by an ASR system with a fully punctuated version which includes case information. For the remainder of this paper errorfree texts such as newspaper articles or novels shall be referred to as &quot;standard text&quot; and the output from a speech recognition system as &quot;ASR text&quot;. There are many possible situations in which an NLP system may be required to process ASR text. The most obvious examples are NLP systems which take speech input (eg. Moore et al. (1997)). Also, dictation software programs do not punctuate or capitalise their output but, if this information could be added to ASR text, the results would be far more usable. One of the most important pieces of informlike an execution. Schools inspections are going to be tougher to force bad teachers out. And the four thousand couples who shared the Queen&apos;s golden day. Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information. However, knowledge of sentence boundaries is required by many NLP technologies. Part of speech tag"
A00-1012,A94-1013,0,0.196013,"Missing"
A00-1012,A97-1004,0,0.137396,"g indicating whether following word is a stop word Flag indicating whether following word is capitalised word sentence_boundary or no_boundary Table 2: Features used in Timbl representation Case information [I P I R I F Applied I 78 [ 75 [ 76 Not applied 36 35 35 Table 3: Results of the sentence boundary detection program suming a zero word error rate. This result is in agreement with the results from the human annotation experiments described in Section 3. However, there is a far greater difference between the automatic system&apos;s performance on standard and ASR text than the human annotators. Reynar and Ratnaparkhi (1997) (Section 2) argued that a context of one word either side is sufficient for the punctuation disambiguation problem. However, the results of our system suggest that this may be insufficient for the sentence boundary detection problem even assuming reliable part of speech tags (cf note 5). These experiments do not make use of prosodic information which may be included as part of the ASR output. Such information includes pause length, pre-pausal lengthening and pitch declination. If this information was made available in the form of extra features to a machine learning algorithm then it is possi"
A00-1012,C98-2135,0,\N,Missing
A00-1040,H92-1022,0,0.0427965,"Missing"
A00-1040,M98-1028,0,0.0177751,"ts are applied and their performance compared against hand-crafted lists. In the next section the NE task is described in further detail. 2 NE background 2.1 NE Recognition of Broadcast News The NE task itself was first introduced as part of the MUC6 (MUC, 1995) evaluation exercise and was continued in MUC7 (MUC, 1998). This formulation of the NE task defines seven types of NE: PERSON, ORGANIZATION, LOCATION, DATE, TIME, MONEY and PERCENT. Figure 1 shows a short text marked up in S G M L with NEs in the M U C style. The task was duplicated for the D A R P A / N I S T HUB4 evaluation exercise (Chinchor et al., 1998) but this time the corpus to be processed consisted of single case transcribed speech, rather than mixed case newswire text. Participants were asked to carry out NE recognition on North American broadcast news stories recorded from radio and television and processed by automatic speech recognition (ASR) software. The participants were provided with a training corpus consisting of around 32,000 words of transcribed broadcast news stories from 1997 annotated with NEs. Participants used these text to 290 ""It's a chance to think about first-level questions,"" said Ms. &lt;enamex type=""PERS0N"">Cohn&lt;ena"
A00-1040,M98-1015,0,0.0419794,"Missing"
A00-1040,E99-1001,0,0.149493,"Missing"
A00-1040,C96-1071,1,0.886753,"Missing"
A00-1040,M98-1004,0,\N,Missing
A97-1035,C96-2187,1,0.703341,", morphological analysers, discourse planning modules, etc, - be readily available for experimentation and reuse. But the pressure towards theoretical diversity means that there is no point attempting to gain agreement, in the short term, on what set of component technologies should be developed or on the informational content or syntax of representations that these components should require or produce. Our response to these considerations has been to design and implement a software environment called GATE - a General Architecture for Text Engineering (Cunningham, Gaizauskas, and Wilks, 1995; Cunningham, Wilks, and Gaizauskas, 1996) - which attempts to meet the following objectives: 1. support information interchange between LE modules at the highest common level possible without prescribing theoretical approach (though it allows modules which share theoretical presuppositions to pass data in a mutually accepted common form); • maturing NLP technology which is now able, for some tasks, to achieve high levels of accuracy repeatedly on real data. Aside from the host of fundamental theoretical problems that remain to be answered in NLP, language engineering faces a variety of problems of its own. Two features of the curren"
A97-1035,C96-1082,0,0.0553257,"in SGML for processing by LT-NSL tools, and convert the SGML results back into native format. Work is underway to integrate the LT-NSL API with G A T E and provide SGML I / O for TIPS T E R (and we acknowledge valuable assistance from colleagues at Edinburgh in this task). 2.6 ICE ICE, the Intarc Communication Environment (Amtrup, 1995), is an 'environment for the development of distributed AI systems'. As part of the Verbmobil real-time speech-to-speech translation project ICE has addressed two key problems for this type of system, viz. distributed processing and incremental interpretation (Gorz et al., 1996): distribution to contribute to processing speed in what is a very compute-intensive application area; incremental interpretation both for speed reasons and to facilitate feedback of results from downstream modules to upstream ones (e.g. to inform the selection of word interpretations from phone lattices using partof-speech information). ICE provides a distribution and communication layer based on PVM (Parallel Virtual Machine). The infrastructure that ICE delivers doesn't fit into our tripartite classification because the communication channels do not use data structures specific to NLP needs"
A97-1035,C96-1079,0,0.0690796,"action-based: A fourth category might be added to cater for those systems that provide communication and control infrastructure without addressing the text-specific needs of NLP (e.g. Verbmobil's ICE architecture (Amtrup, 1995)). We begin by reviewing examples of the three approaches we sketched above (and a system that falls into the fourth category). Next we discuss current trends in the field and motivate a set of requirements that have formed the design brief for G A T E , which is then described. The initial distribution of the system includes a MUC-6 (Message Understanding Conference 6 (Grishman and Sundheim, 1996)) style information extraction (IE) system and an overview 1These texts may sometimes be the results of automatic speech recognition - see section 2.6. 237 of these modules is given. G A T E is now available for research purposes - see http ://ul;w.dcs. shef. ac. u_k/research/groups/ nlp/gate/ for details of how to obtain the system. It is written in C + + and T c l / T k and currently runs on UNIX (SunOS, Solaris, Irix, Linux and AIX are known to work); a Windows N T version is in preparation. 2 2.1 Managing Abstraction Information about Text Approaches T h e abstraction-based approach to man"
A97-1035,A97-1034,0,0.137842,"Missing"
A97-1035,J92-2002,0,0.010504,"UNIX (SunOS, Solaris, Irix, Linux and AIX are known to work); a Windows N T version is in preparation. 2 2.1 Managing Abstraction Information about Text Approaches T h e abstraction-based approach to managing information a b o u t texts is primarily motivated by theories of the nature of the information to be represented. One such position is t h a t declarative, constraint-based representations using featurestructure matrices manipulated under unification are an appropriate vehicle by which &quot;many technical problems in language description and computer manipulation of language can be solved&quot; (Shieber, 1992). Information in these models may be characterised as abstract in our present context as there is no requirement to tie d a t a elements back to the original text - these models represent abstractions from the text. One recent example of an infrastructure project based on abstraction is A L E P - the Advanced Language Engineering Platform (Simkins, 1994). A L E P aims to provide &quot;the NLP research and engineering community in Europe with an open, versatile, and general-purpose development environment&quot;. ALEP, while in principle open, is primarily an advanced system for developing and manipulatin"
A97-1035,A97-1036,0,0.074113,"al access to data (McKelvie, Brew, and Thompson, 1997). 5. T I P S T E R can easily support multi-level access control via a database's protection mechanisms - this is again not straightforward in SGML. 6. Distributed control is easy to implement in a database-centred system like T I P S T E R - the DB can act as a blackboard, and implementations can take advantage of well-understood access control (locking) technology. How to do distributed control in LT-NSL is not obvious. We plan to provide this type of control in G A T E via collaboration with the Corelli project at CRL, New Mexico - see (Zajac, 1997) for more details. 2.5 Combining Addition and Reference We believe the above comparison demonstrates that there are significant advantages to the T I P S T E R model and it is this model that we have chosen for GATE. We also believe that SGML and the T E I must remain central to any serious text processing strategy. 240 The points above do not contradict this view, but indicate that SGML should not form the central representation format of every text processing system. Input from SGML text and T E I conformant output are becoming increasingly necessary for LE applications as more and more publ"
A97-1035,M95-1017,0,\N,Missing
A97-1035,A97-2017,1,\N,Missing
A97-2017,A97-1035,1,0.70965,"s the best practical support that can be given to advance the field? Clearly, the pressure to build on the efforts of others demands that LE tools or component technologies be readily available for experimentation and reuse. But the pressure towards theoretical diversity means that there is no point attempting to gain agreement, in the short term, on what set of component technologies should be developed or on the informational content or syntax of representations that these components should require or produce. Our response has been to design and implement a software environment called GATE (Cunninham et al., 1997), which we will demonstrate at ANLP. GATE attempts to meet the following objectives: 29 ing modules are active buttons: clicking on them will, if conditions are right, cause the module to be executed. The paths through the graph indicate the dependencies amongst the various modules making up this subsystem. At any point in time, the state of execution of the system, or, more accurately, the availability of data from various modules, is depicted through colour-coding of the module boxes. After execution, the results of completed modules are available for viewing by clicking again on the module"
A97-2017,M95-1017,0,0.0521059,"escribed above. In addition, modules can be &apos;reset&apos;, i.e. their results removed from the GDM, to allow the user to pick another path through the graph, or re-execute having altered some tailorable data-resource (such as a grammar or lexicon) interpreted by the module at run-time. (Modules running as external executables might also be recompiled between runs.) To illustrate the process of converting pre-existing LE systems into GATE-compatible C R E O L E sets we use as an example the creation of VIE (Vanilla Information Extraction system) from LaSIE (LargeScale Information Extraction system) (Gaizauskas et al., 1995), Sheffield&apos;s entry in the MUC-6 system evaluations. LaSIE module interfaces were not standardised when originally produced and its CREOLEization gives a good indication of the ease of integrating other LE tools into GATE. The work took around 2 person-months. The resulting system, VIE, is distributed with GATE. developed from scratch for the architecture - in either case the object provides a standardised API to the underlying resources which allows access via GGI and I / O via GDM. Tile CREOLE APIs may also be used for programming new objects. When the user initiates a particular C R E O L E"
A97-2017,J93-2004,0,\N,Missing
A97-2017,C94-1070,0,\N,Missing
A97-2017,A97-1034,0,\N,Missing
A97-2017,A97-1036,0,\N,Missing
A97-2017,A97-1051,0,\N,Missing
A97-2017,C96-1079,0,\N,Missing
aker-etal-2012-light,W02-1037,0,\N,Missing
aker-etal-2012-light,D09-1040,0,\N,Missing
aker-etal-2012-light,C04-1138,0,\N,Missing
aker-etal-2012-light,P99-1067,0,\N,Missing
aker-etal-2012-light,P01-1008,0,\N,Missing
aker-etal-2012-light,J05-4003,0,\N,Missing
aker-etal-2012-light,R11-1106,0,\N,Missing
aker-etal-2012-light,P06-2095,0,\N,Missing
aker-etal-2012-light,N06-1058,0,\N,Missing
aker-etal-2012-light,P06-1144,0,\N,Missing
aker-etal-2012-light,P06-1011,0,\N,Missing
aker-etal-2012-light,P99-1068,0,\N,Missing
aker-etal-2012-light,N06-1003,0,\N,Missing
aker-etal-2012-light,P08-1116,0,\N,Missing
aker-etal-2012-light,C10-2054,0,\N,Missing
aker-etal-2012-light,N04-1034,0,\N,Missing
aker-etal-2014-bilingual,steinberger-etal-2012-dgt,0,\N,Missing
aker-etal-2014-bilingual,C00-2163,0,\N,Missing
aker-etal-2014-bilingual,P07-1108,0,\N,Missing
aker-etal-2014-bilingual,P06-1011,0,\N,Missing
aker-etal-2014-bilingual,J03-1002,0,\N,Missing
aker-etal-2014-bilingual,P09-1018,0,\N,Missing
aker-etal-2014-bilingual,W13-2502,0,\N,Missing
aker-etal-2014-bilingual,R13-1074,1,\N,Missing
aker-etal-2014-bilingual,C12-2003,1,\N,Missing
aker-etal-2014-bilingual,P13-1040,1,\N,Missing
aker-etal-2014-bootstrapping,J93-2004,0,\N,Missing
aker-etal-2014-bootstrapping,steinberger-etal-2012-dgt,0,\N,Missing
aker-etal-2014-bootstrapping,C00-2163,0,\N,Missing
aker-etal-2014-bootstrapping,H01-1035,0,\N,Missing
aker-etal-2014-bootstrapping,E03-1035,0,\N,Missing
aker-etal-2014-bootstrapping,petrov-etal-2012-universal,0,\N,Missing
aker-etal-2014-bootstrapping,N03-1028,0,\N,Missing
aker-etal-2014-bootstrapping,J03-1002,0,\N,Missing
aker-etal-2014-bootstrapping,P13-1040,1,\N,Missing
aker-etal-2014-bootstrapping,gimenez-marquez-2004-svmtool,0,\N,Missing
aker-gaizauskas-2010-model,W08-1407,1,\N,Missing
aker-gaizauskas-2010-model,P07-1126,0,\N,Missing
aker-gaizauskas-2010-model,W04-1013,0,\N,Missing
aker-gaizauskas-2010-model,P08-1032,0,\N,Missing
aker-gaizauskas-2010-model,R09-1002,1,\N,Missing
aswani-gaizauskas-2010-english,N03-2016,0,\N,Missing
aswani-gaizauskas-2010-english,A97-1029,0,\N,Missing
barker-gaizauskas-2012-assessing,W04-3254,0,\N,Missing
barker-gaizauskas-2012-assessing,C04-1051,0,\N,Missing
barker-gaizauskas-2012-assessing,J05-4003,0,\N,Missing
barker-gaizauskas-2012-assessing,N03-1003,0,\N,Missing
barker-gaizauskas-2012-assessing,P11-2083,0,\N,Missing
barker-gaizauskas-2012-assessing,J08-4005,0,\N,Missing
barker-gaizauskas-2012-assessing,aker-etal-2012-light,1,\N,Missing
C08-1102,W04-0807,0,0.0652784,"el 3 query and 83 for the more relaxed level 2 query. For this sense, abstracts returned by the level 2 query would be used if 83 or fewer examples were required, otherwise abstracts returned by the level 1 query would be used. Note that the queries submitted to Entrez are restricted so the terms only match against the title and abstract of the PubMed articles. This avoids spurious matches against other parts of the records including metadata and authors’ names. 4 WSD System The basis of our WSD system was developed by Agirre and Mart´ınez (2004a) and participated in the Senseval-3 challenge (Mihalcea et al., 2004) with a performance which was close to the best system for the English and Basque lexical sample tasks. The system has been adapted to the biomedical domain (Stevenson et al., 2008) and has the best reported results over the NLM-WSD corpus (Weeber et al., 2001), a standard data set for evaluation of WSD algorithms in this domain. The system uses a wide range of features which The process of relaxing queries is carried out as are commonly employed for WSD: Local collocations: A total of 41 features which follows. Assume we have an ambiguous term, a, and a set of terms T identified using the pro"
C08-1102,P03-1058,0,0.0241659,"D in the biomedical domain using information derived from UMLS (Humphreys et al., 1998). However, results from SemEval (Agirre et al., 2007) and its predecessors have shown that supervised approaches to WSD generally outperform unsupervised ones. It has also been shown that results obtained from supervised methods improve with access to additional labeled data for training (Ng, 1997). Consequently various techniques for automatically generating training data have been developed. One approach makes use of the fact that different senses of ambiguous words often have different translations (e.g. Ng et al. (2003)). Parallel text is used as training data with the alternative translations serving as sense labels. However, disadvantages of this approach are that the alternative translations do not always correspond to the sense distinctions in the original language and parallel text is not always available. Another approach, developed by Leacock et al. (1998) and extended by Agirre and Mart´ınez (2004b), is to examine a lexical resource, WordNet in both cases, to identify unambiguous terms which are closely related to each of the senses of an ambiguous term. These “monosemous relatives” are used to as qu"
C08-1102,W97-0201,0,0.413193,"icantly improve the performance of a state-of-the-art WSD system. 1 Introduction The resolution of lexical ambiguities has long been considered an important part of the process of understanding natural language. Supervised approaches to Word Sense Disambiguation (WSD) have been shown to perform better than unsupervised ones (Agirre and Edmonds, 2007) but require examples of ambiguous words used in context annotated with the appropriate sense (labeled examples). However these often prove difficult to obtain since manual sense annotation of text is a complex and time consuming process. In fact, Ng (1997) estimated that 16 person years of manual effort would be required to create enough labeled examples to train a wide-coverage WSD system. This c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. limitation is commonly referred to as the data acquisition bottleneck. It is particularly acute in specific domains, such as biomedicine, where terms may have technical usages which only domain experts are likely to be aware of. For example, possible meanings of the term “gangli"
C08-1102,N01-1011,0,0.0339728,"))”. Similarly, level noun-modifier, preposition and sibling. These are |T |− 2 queries return documents containing the identified using heuristic patterns and regular exambiguous term and all but two of the terms pressions applied to PoS tag sequences around the in T . Level 1 queries, the most relaxed, return ambiguous word (Agirre and Mart´ınez, 2004a). Salient bigrams: Salient bigrams within the abdocuments containing the ambiguous term and one of the terms in T . We do not use just the stract with high log-likelihood scores, as described ambiguous term as the query since this does not by Pedersen (2001). Unigrams: Lemmas of all content words contain any information which could discriminate between the possible meanings. Figure 1 shows (nouns, verbs, adjectives, adverbs) in the target the queries which are formed for the ambigu- word’s sentence and, as a separate feature, lemous term “culture” and the three most salient mas of all content words within a 4-word window terms identified for the ‘anthropological culture’ around the target word, excluding those in a list sense. The “matches” column lists the number of corpus-specific stopwords (e.g. “ABSTRACT”, of PubMed abstracts the query matche"
C08-1102,W08-0611,1,0.774651,"ts returned by the level 1 query would be used. Note that the queries submitted to Entrez are restricted so the terms only match against the title and abstract of the PubMed articles. This avoids spurious matches against other parts of the records including metadata and authors’ names. 4 WSD System The basis of our WSD system was developed by Agirre and Mart´ınez (2004a) and participated in the Senseval-3 challenge (Mihalcea et al., 2004) with a performance which was close to the best system for the English and Basque lexical sample tasks. The system has been adapted to the biomedical domain (Stevenson et al., 2008) and has the best reported results over the NLM-WSD corpus (Weeber et al., 2001), a standard data set for evaluation of WSD algorithms in this domain. The system uses a wide range of features which The process of relaxing queries is carried out as are commonly employed for WSD: Local collocations: A total of 41 features which follows. Assume we have an ambiguous term, a, and a set of terms T identified using the process extensively describe the context of the ambiguous in Section 3.1. The first, most specific query, word and fall into two main types: (1) bigrams is formed from the conjunction"
C08-1102,W03-1302,0,0.0296664,"o each of the senses of an ambiguous term. These “monosemous relatives” are used to as query terms for a search engine and the examples returned used as additional training data. In the biomedical domain, Humphrey et al. (2006) use journal descriptors to train models based on the terms which are likely to co-occur with each sense. Liu et al. (2002) used information in UMLS to disambiguate automatically retrieved examples which were then used as labeled training data. The meanings of 35 ambiguous abbreviations were identified by examining the closeness of concepts in the same abstract in UMLS. Widdows et al. (2003) employ a similar approach, although their method also makes use of parallel corpora when available. All of these approaches rely on the existence of an external resource (e.g. parallel text or a domain ontology). In this paper we present a novel approach, inspired by the relevance feedback technique used in IR, which automatically identifes additional training examples using existing labeled data. 3 Generating Examples using Relevance Feedback The aim of relevance feedback is to generate improved search queries based on manual analysis of a set of retrieved documents which has been shown to i"
C08-1102,J98-1006,0,\N,Missing
C08-1102,W04-0813,0,\N,Missing
C08-1102,P04-1036,0,\N,Missing
C08-1102,W04-3204,0,\N,Missing
C12-2003,aker-etal-2012-light,1,0.808146,"repare the parallel phrases used to train and test the SVM classifier. For each language pair we split the corpus into two parts: a training set and a test set. The test set contains 10K parallel sentences. The training set contains 99K sentences for EN-DE, 423K for EN-EL and 53K sentences for EN-LV. 4.1.2 Comparable Corpora We used comparable corpora in English-Greek, English-Latvian and English-German language pairs. These corpora were collected from news articles using a light weight approach that only compares titles and date of publication of two articles to judge them for comparability (Aker et al., 2012). The corpora are aligned at the document level and are detailed in Table 1. language pair EN-DE EN-EL EN-LV document pairs 66K 122K 87K EN sentences 623K 1600K 1122K target sentences 533K 313K 285K EN words 14837K 27300K 18704K target words 6769K 8258K 5356K Table 1: Size of comparable corpora. 4.2 Phrase Extraction for Classifier Training and Testing On both parallel training and testing data sets (see Section 4.1.1) we separately applied GIZA++ to obtain the word alignment information used in our parallel phrase extraction method (see Section 2.1). Then we ran the training example extractio"
C12-2003,aswani-gaizauskas-2010-english,1,0.688491,"rposes Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology. Because of this we also use cognate-based methods to perform the mapping between source and target words or vice versa. We only apply the cognate-based methods for the firstWordTranslationScore and lastWordTranslationScore features. For these two features it is easy to compare the first or the last words from both the source and target phrases. The score of the cognate methods becomes the translation score for the features. We adopt several string similarity measures described in Aswani and Gaizauskas (2010): (1) Longest Common Subsequence Ratio, (2) Longest Common Substring, (3) Dice Similarity, (4) Needleman-Wunsch Distance and (5) Levenshtein Distance. Each of these measures returns a score between 0 and 1. We use a weighted linear combination of the scores to compute the final score. We learn the weights using linear regression over training data consisting of pairs of truely and falsely aligned city names available from Wikipedia1 . For the truely aligned named entities we assign a score of 1 and for the falsely aligned ones a score of 0. We take the cognate similarity score as the translati"
C12-2003,P01-1008,0,0.076278,"es or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target la"
C12-2003,N06-1003,0,0.0261749,"re not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible"
C12-2003,P05-1033,0,0.119284,"e pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side and on the target side, the length"
C12-2003,W11-1209,0,0.0789101,"is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted by our method from comparable corpora lead to improved SMT quality, as measured using BLEU (Papineni et al., 2002) . Hewavitharana and Vogel (2011) also adopt a classification approach for phrase extraction. However, their approach requires manual intervention in data preparation, whereas we perform the preparation of training and testing data fully automatically. In addition, Hewavitharana and Vogel (2011) do not report any SMT performance evaluation of their approach, so it is difficult to estimate how useful their approach is for the actual task it is meant to improve. We test the impact of our extracted phrases on the performance of an SMT system, which allows us to draw conclusions about the likely utility of our approach for SMT in"
C12-2003,ion-2012-pexacc,0,0.0248044,"better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then compute all possible phrase pairings co"
C12-2003,N06-1058,0,0.0195444,"ficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we"
C12-2003,W04-3250,0,0.194912,"Missing"
C12-2003,N03-1017,0,0.0331396,"nguages. During testing candidate phrase pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side"
C12-2003,2007.tmi-papers.12,0,0.0404297,"cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach."
C12-2003,W02-1018,0,0.0390414,"nder-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase"
C12-2003,D09-1040,0,0.0161754,"ystems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then co"
C12-2003,J05-4003,0,0.0603772,"Missing"
C12-2003,P06-1011,0,0.106121,"h parallel resources (corpora). However, in many cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is"
C12-2003,J04-4002,0,0.0446812,"ing candidate phrase pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side and on the target s"
C12-2003,C00-2163,0,0.75263,"etween parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then compute all possible phrase pairings consisting of one phrase from S and one phrase from T. In the test phase we use a binary SVM classifier to determine for each generated phrase pair whether it is or is not parallel. The SVM classifier is trained using phrase pairs taken from parallel data word aligned using Giza++ (Och and Ney, 2000, 2003). We have tested our approach on the English-German, English-Greek and English-Latvian language pairs. Latvian is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted"
C12-2003,J03-1002,0,0.0125168,"Missing"
C12-2003,P02-1040,0,0.0910989,"language pairs. Latvian is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted by our method from comparable corpora lead to improved SMT quality, as measured using BLEU (Papineni et al., 2002) . Hewavitharana and Vogel (2011) also adopt a classification approach for phrase extraction. However, their approach requires manual intervention in data preparation, whereas we perform the preparation of training and testing data fully automatically. In addition, Hewavitharana and Vogel (2011) do not report any SMT performance evaluation of their approach, so it is difficult to estimate how useful their approach is for the actual task it is meant to improve. We test the impact of our extracted phrases on the performance of an SMT system, which allows us to draw conclusions about the likely u"
C12-2003,P06-2095,0,0.0256329,"ora). However, in many cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generat"
C12-2003,skadina-etal-2012-collecting,1,0.887104,"Missing"
C12-2003,P08-1116,0,0.0191513,"chine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S"
C14-1147,agirre-soroa-2008-using,0,0.0281026,"e, use an N-cliques graph partitioning technique to identify sets of highly related senses. However, this approach has not been used for NED. Our second proposed model uses the Page-Rank algorithm (PR), which to our knowledge has also not previously been applied to NED. PR was proposed by Page et al. (1999) to produce a global rank for web pages based on the hyperlink structure of the web. Xing and Ghorbani (2004) adapted PR to take into account the weights of links and the nodes’ importance. PR and Personalized PR algorithms have been used successfully in WSD (e.g. Sinha and Mihalcea (2007), Agirre and Soroa (2008; 2009)). 3 Named Entity Candidates Selection Given an input document D containing a set of pre-tagged NE textual mentions M = {m1 , m2 , m3 . . . mk }, we need to select all possible candidate interpretations for each mi from the knowledge base. I.e. for each NE textual mention mi ∈ M we select a set of candidates Ei = {ei,1 , ei,2 , ei,3 . . . ei,j } from the KB. The NE textual mention mi is used to search the KB entry titles using Lucene1 to find entries with titles that fully or partially contain the NE textual mention. The following example shows the possible candidates for the textual me"
C14-1147,E09-1005,0,0.0708001,"Missing"
C14-1147,E06-1002,0,0.0413024,"ms to deal with the case where there is no entry for the NE in the reference KB. Ji et al. (2011) group and summarise the different approaches to EL taken by participating systems. In general, there are two main lines of approach to the NED problem. The first, single entity disambiguation approaches (SNED), disambiguates one entity at a time without considering the effect of other NEs. These approaches use local context textual features of the mention and compare them to the textual features of NE candidate documents in the KB, and link to the most similar. The first approach in this line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate. More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy by calculating the nodes’ coherence based on the their incoming links in Wikipedia and the overlaps in Wikipedia categories. Milne and Witten (2008) improve Cucerzan’s work by calculating the topical coherence using Normalized Google Distance and restrict the context entities to the unambiguous entities"
C14-1147,D07-1074,0,0.0714798,"In general, there are two main lines of approach to the NED problem. The first, single entity disambiguation approaches (SNED), disambiguates one entity at a time without considering the effect of other NEs. These approaches use local context textual features of the mention and compare them to the textual features of NE candidate documents in the KB, and link to the most similar. The first approach in this line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate. More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy by calculating the nodes’ coherence based on the their incoming links in Wikipedia and the overlaps in Wikipedia categories. Milne and Witten (2008) improve Cucerzan’s work by calculating the topical coherence using Normalized Google Distance and restrict the context entities to the unambiguous entities. Different query expansion approaches are incorporated into this framework, such as using context term expansion (Gottipati and Jiang, 2011) and acronym expansion (Zhang e"
C14-1147,C10-1032,0,0.0223457,"e topical coherence using Normalized Google Distance and restrict the context entities to the unambiguous entities. Different query expansion approaches are incorporated into this framework, such as using context term expansion (Gottipati and Jiang, 2011) and acronym expansion (Zhang et al., 2011). Sen (2012) proposed a latent topic model to learn the context entity association. Machine learning is widely used in SNED as some approaches deal with the problem as a search result ranking problem. Supervised learn-to-rank models are used to re-rank the ambiguous candidate set (Zheng et al., 2010; Dredze et al., 2010; Alhelbawy and Gaizauskas, 2012; Nebhi, 2013). 1545 The second line of approach is collective named entity disambiguation (CNED), where all mentions of entities in the document are disambiguated jointly. These approaches try to model the interdependence between the different candidate entities for different NE mentions in the query document, and reformulate the problem of NED as a global optimization problem whose aim is to find the best set of entities. As this new formulation is NP-hard, many approximations have been proposed. Kulkarni et. al. (2009) presents a collective approach for entit"
C14-1147,D11-1074,0,0.0252419,"e. More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy by calculating the nodes’ coherence based on the their incoming links in Wikipedia and the overlaps in Wikipedia categories. Milne and Witten (2008) improve Cucerzan’s work by calculating the topical coherence using Normalized Google Distance and restrict the context entities to the unambiguous entities. Different query expansion approaches are incorporated into this framework, such as using context term expansion (Gottipati and Jiang, 2011) and acronym expansion (Zhang et al., 2011). Sen (2012) proposed a latent topic model to learn the context entity association. Machine learning is widely used in SNED as some approaches deal with the problem as a search result ranking problem. Supervised learn-to-rank models are used to re-rank the ambiguous candidate set (Zheng et al., 2010; Dredze et al., 2010; Alhelbawy and Gaizauskas, 2012; Nebhi, 2013). 1545 The second line of approach is collective named entity disambiguation (CNED), where all mentions of entities in the document are disambiguated jointly. These approaches try to model t"
C14-1147,D11-1072,0,0.0785966,"Missing"
C14-1147,P11-1115,0,0.0647591,"Missing"
C14-1147,P11-1138,0,0.0295482,". (2009) presents a collective approach for entity linking that models the coherence between all pairs of entity candidates for different mentions as a probabilistic factor graph. They present two approximations to solve this optimization problem where the interdependence between decisions is modelled as the sum of the pairs’ dependencies. Alhelbawy and Gaizauskas (2013) proposed a sequence dependency model using HMMs to model NE interdependency. Another approximation uses a mixture of local and global features to train the coefficients of a linear ranking SVM to rank different NE candidates (Ratinov et al., 2011). Shirakawa et al. (2011) cluster related textual mentions and assign a concept to each cluster using a probabilistic taxonomy. The concept associated with a mention is used in selecting the correct entity from the Freebase KB. Graph models are widely used in collective disambiguation approaches. All these approaches model NE interdependencies, while different methods may be used for disambiguation. Han (2011) uses local dependency between NE mention and the candidate entity, and semantic relatedness between candidate entities to construct a referent graph, proposing a collective inference alg"
C14-1147,N10-1072,0,0.019317,"rk by calculating the topical coherence using Normalized Google Distance and restrict the context entities to the unambiguous entities. Different query expansion approaches are incorporated into this framework, such as using context term expansion (Gottipati and Jiang, 2011) and acronym expansion (Zhang et al., 2011). Sen (2012) proposed a latent topic model to learn the context entity association. Machine learning is widely used in SNED as some approaches deal with the problem as a search result ranking problem. Supervised learn-to-rank models are used to re-rank the ambiguous candidate set (Zheng et al., 2010; Dredze et al., 2010; Alhelbawy and Gaizauskas, 2012; Nebhi, 2013). 1545 The second line of approach is collective named entity disambiguation (CNED), where all mentions of entities in the document are disambiguated jointly. These approaches try to model the interdependence between the different candidate entities for different NE mentions in the query document, and reformulate the problem of NED as a global optimization problem whose aim is to find the best set of entities. As this new formulation is NP-hard, many approximations have been proposed. Kulkarni et. al. (2009) presents a collecti"
C96-1071,J93-2004,0,\N,Missing
C96-1071,M95-1017,0,\N,Missing
C96-2187,C94-1070,0,0.025987,"s of the M U L T E X T project. G D M provides a central repository or server t h a t stores all the information an LE system generates about the texts it processes. All communication between the components of an LE system goes through GDM, insulating parts fi&apos;om each other and providing a uniform A P I (applications p r o g r a m m e r interface) for manipulating the data produced by the system. 3 Benefits of this approach include the ability to exploit the maturity and efficiency of database technology, easy modelling of blackboard-type distributed control regimes (of the type proposed by: (Boitet and Seligman, 1994) and in the section on control in (Black ed., 1991)) and reduced interdependence of components. G G I is in development at Sheffield. It is a graphical launchpad for LE subsystems, and provides various facilities for viewing and testing results and playing software lego with LE components - interactively assembling objects into different system configurations. All the real work of analysing texts (and m a y b e producing summaries of them, or translations, or SQL statements, etc.) in a GATE-based LE system is done by C R E O L E modules. Note that we use the terms module and object rather loos"
C96-2187,J93-2004,0,0.0245392,"nefficency have been extensively studied, and a number of solutions are now available (Prieto-Diaz and t~h&apos;eeman, 1987; Prieto-Diaz, 1993). Similarly, the Natural Language Engineering (NLE l) community has identified the potential benefits of reducing repetition, and work has been flmded to promote reuse. This work concerns either reusable resources which are primarily data or those which are primarily algorithmic (i.e. processing &apos;tools&apos;, or programs, or code libraries). Successflfl examples of reuse of data resources include: the WordNet thesaurus (Miller el; al., 1993); the Penn Tree Bank (Marcus et al., 1993); the Longmans Dictionary of Contemporary English (Summers, 1995). A large number of papers report results relative to these and other resources, and these successes have spawned a numOur view is that succesful algorithmic reuse in NLE will require the provision of support software for NLE in the form of a general architecture and development environment which is specifically designed for text processing systems. Under EPSRC 2 grant GR/K25267 the NLP group at, the University of Sheffield are developing a system that aims to implement this new approach. The system is called GATE - the General A"
C98-1011,C96-1079,0,0.118886,"Missing"
C98-1011,J81-4001,0,0.842697,"s recently been renewed with the introduction of a quantitative evaluation regime as part of the Message Understanding Conference MUC) evaluations of Information Extraction (IE) systelns (Grishman and Sundheim, 1996) This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature. This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on aI1 extension of Sidner&apos;s algorithm (Sidner, 1981) proposed in (Azzam, 1996), with further refinements from development on real-world texts. The approach * This work was carried out in the context of the EU AVENTINUS project (Thumair, 1996), which aims to develop a multilingual IE system for drug enforcement, and including a language-independent coreference mechanism (Azzam et al., 1998). 74 is implemented within the general coreference mechanism provided by the LaSIE (Large Scale Information Extraction) system (Gaizauskas et al., 1995) and (Humphreys et al., 1998), Sheffield University&apos;s entry in the MUC-6 and 7 evaluations. 2 Focus in A n a"
C98-1011,M95-1005,0,0.0748492,"nition included only &apos;identity&apos; relations between text strings: proper nouns, common nouns and pronouns. Other possible coreference relations, such as &apos;part-whole&apos;, and nontext strings (zero anaphora) were excluded. 76 Evaluation with the MUC Corpora The definition was used to manually annotate several corpora of newswire texts, using SGML markup to indicate relations between text strings. Automatically annotated texts, produced by systems using the same markup scheme, wcrc then compared with the manually annotated versions, using scoring software made available to MUC participants, based on (Vilain et al., 1995). The scoring software calculates the standard Information Retrieval metrics of &apos;recall&apos; and &apos;precision&apos;, ~ together with an overall f-measure. The following section presents the results obtained using the corpora and scorer provided for MUC-7 training (60 texts, average 581 words per text, 19 words per sentence) and evaluation (20 texts, average 605 words per text, 20 words per sentence), the latter provided for the formal MUC-7 run and kept blind during development. 6 Results The MUC scorer does not distinguish between different classes of anaphora (pronouns, definite noun phrases, bare noun"
C98-1011,M95-1010,0,\N,Missing
C98-1011,M95-1017,0,\N,Missing
C98-1011,P96-1035,1,\N,Missing
C98-1111,P97-1021,0,0.0298271,"Missing"
C98-1111,P96-1025,0,0.0388392,"Missing"
C98-1111,P98-1115,1,0.0540771,"Missing"
C98-1111,H94-1020,0,0.0262288,"art-of-speech tags), we may be able to achieve parsing performance similar to the best results in the field obtained in (Collins, 1996). 2 Growth of the Rule Set One could investigate whether there is a finite g r a m m a r that should account for any text within a class of related texts (i.e. a domain oriented sub-grammar of English). If there is, the number of extracted rules will approach a limit as more sentences are processed, i.e. as the rule n u m b e r approaches the size of such an underlying and finite grammar. We had hoped that some approach to a limit would be seen using P T B II (Marcus et al., 1994), which larger and more consistent for bracketting than P T B I. As shown in Figure 1, however, the rule number growth continues unabated even after more t h a n 1 million part-ofspeech tokens have been processed. 700 Rule Growth and Partial Bracketting Why should the set of rules continue to grow in this way? P u t t i n g aside the possibility that natural languages do not have finite rule sets, we can think of two possible answers. First, it may be that the full ""underlying g r a m m a r "" is much larger than the rule set that has so far been produced, requiring a much larger tree-banked co"
C98-1111,1995.iwpt-1.26,0,0.0914475,"Missing"
C98-1111,J95-2002,0,0.0866185,"Missing"
catizone-etal-2010-using,W03-2113,0,\N,Missing
catizone-etal-2010-using,I05-5003,0,\N,Missing
catizone-etal-2010-using,W06-2204,0,\N,Missing
D10-1047,R09-1002,1,0.933025,"asured with the final parameters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-b"
D10-1047,P10-1127,1,0.336084,"eters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-best lists,5 and use MERT to"
D10-1047,aker-gaizauskas-2010-model,1,0.872562,"eters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-best lists,5 and use MERT to"
D10-1047,W08-1405,0,0.0350931,"Missing"
D10-1047,J97-1003,0,0.104665,"oblem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE (Lin, 2004)."
D10-1047,W01-0100,0,0.699137,"ve training algorithm which directly maximises the quality of the best summary, rather than assuming a sentence-level decomposition as in earlier work. Our approach leads to significantly better results than earlier techniques across a number of evaluation metrics. 1 Introduction Multi-document summarization aims to present multiple documents in form of a short summary. This short summary can be used as a replacement for the original documents to reduce, for instance, the time a reader would spend if she were to read the original documents. Following dominant trends in summarization research (Mani, 2001), we focus solely on extractive summarization which simplifies the summarization task to the problem of identifying a subset of units from the document collection (here sentences) which are concatenated to form the summary. Most multi-document summarization systems define a model which assigns a score to a candidate summary based on the features of the sentences included in the summary. The research challenges are then twofold: 1) the search problem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a trai"
D10-1047,P03-1021,0,0.0638436,"nd -SU4. The paper is structured as follows. Section 2 presents the summarization model. Next in section 3 we present an A* search algorithm for finding the best scoring (argmax) summary under the model with a constraint on the maximum summary length. We show that this algorithm performs search efficiently, even for very large document sets composed of many sentences. The second contribution of the paper is a new training method which directly optimises the summarization system, and is presented in section 4. This uses the minimum error-rate training (MERT) technique from machine translation (Och, 2003) to optimise the summariser’s output to an arbitrary evaluation metric. Section 5 describes our experimental setup and section 6 the results. Finally we conclude in section 7. 2 Summarization Model Extractive multi-document summarization aims to find the most important sentences from a set of documents, which are then collated and presented to the user in form of a short summary. Following the predominant approach to data-driven summarisation, we define a linear model which scores summaries as the weighted sum of their features, s(y|x) = Φ(x, y) · λ , (1) where x is the document set, composed"
D10-1047,W04-1013,0,\N,Missing
D15-1022,D14-1082,0,0.0330149,"prepositional term for sentence generation. Although they also present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) exam4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1 The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011) 215 covering geometric relations such as distance, orientation, relative bounding box sizes and overlaps between bounding boxes (Table 1). We chose to use continuous features as we felt these may be more powerful and expressive compared to discrete, binned features. Despite some of these features being correlated, we left it to the classifier to determine the most useful features for disc"
D15-1022,W14-5403,0,0.0311135,"description generation work, Kulkarni et al. (2011) manually map spatial relations to pre-defined prepositions, whilst Yang et al. (2011) predict prepositions from largescale text corpora solely based on the complement term, with the prepositions constrained to describing scenes (on the street). Elliott and Keller (2013) define a list of eight spatial relations and their corresponding prepositional term for sentence generation. Although they also present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) exam4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1 The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 20"
D15-1022,J03-4003,0,0.0663819,"cted from captions and matched to instances in images are available in the supplementary material. We consider two variants of trajector and landmark terms in our experiments: (i) using the provided high level categories as terms (80 for MSCOCO and 8 for Flickr30k); (ii) using the terms occurring in the sentence directly, which constitute a bigger and more realistic challenge. For Flickr30k, the descriptive phrases may cause data sparseness (the furry, black and white dog). Thus, we extracted the lemmatised head word of each phrase, using a ‘semantic head’ variant of the head finding rules of Collins (2003) in Stanford CoreNLP. Entities from the same coreference chain are denoted with a common head noun chosen by majority vote among the group, with ties broken by the most frequent head noun in the corpus, and further ties broken at random. 5 Image Features: While it is ideal to have vision systems produce a firm decision about the visual entity instance detected in an image, in reality it may be beneficial to defer the decision by allowing several possible interpretations of the instance being detected. In such cases, we will not have a single concept label for the entity, but instead a high-lev"
D15-1022,D13-1128,0,0.441819,"from textual descriptions; (iii) visual features from images. Previous work exists (Yang et al., 2011) that uses text corpora to ‘guess’ the prepositions given the context without considering the appropriate spatial relations between the entities in the image, signifying a gap between visual content and its corresponding description. For example, although person on horse might commonly occur in text corpora, a particular image might actually depict a person standing beside a horse. On the other hand, work that does consider the image content for generating prepositions (Kulkarni et al., 2011; Elliott and Keller, 2013) map geometric relations to a limited set of prepositions using manually defined rules, *A. Ramisa and J. Wang contributed equally to this work. 214 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 214–220, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. not as humans would naturally use them with a richer vocabulary. We would like to have the best of both worlds, by considering image content as well as textual information to select the preposition best used to express the relation between two entities. Our"
D15-1022,S07-1005,0,0.0391698,"Missing"
D15-1022,P14-5010,0,0.00344892,"so present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) exam4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1 The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011) 215 covering geometric relations such as distance, orientation, relative bounding box sizes and overlaps between bounding boxes (Table 1). We chose to use continuous features as we felt these may be more powerful and expressive compared to discrete, binned features. Despite some of these features being correlated, we left it to the classifier to determine the most useful features for discrimination without having to withhold any unnecessarily. B"
D15-1022,S12-1048,0,0.0474918,"how likely they are to express the appropriate spatial relation between the given trajector and landmark entities that are either known (Section 6.1) or only represented by visual features (Section 6.2). Related Work The Natural Language Processing Community has significant interest in different aspects of prepositions. The Prepositions Project (Litkowski and Hargraves, 2005) analysed and produced a lexicon of English prepositions and their senses, and subsequently used them in the Word Sense Disambiguation of Prepositions task in SemEval2007 (Litkowski and Hargraves, 2007). In SemEval-2012, Kordjamshidi et al. (2012) introduce the more fine-grained task of spatial role labelling to detect and classify spatial relations expressed by triples (trajector, landmark, spatial indicator). In the latest edition of SemEval2015, the SpaceEval task (Pustejovsky et al., 2015) introduce further tasks of identifying spatial and motion signals, as well as spatial configurations/orientation and motion relation. In work that links prepositions more strongly to image content, Gupta and Davis (2008) model prepositions implicitly to disambiguate image regions, rather than for predicting prepositions. Their work also require m"
D15-1022,D11-1041,0,0.24634,"Missing"
D15-1022,Q14-1006,0,0.0482958,"solely based on the complement term, with the prepositions constrained to describing scenes (on the street). Elliott and Keller (2013) define a list of eight spatial relations and their corresponding prepositional term for sentence generation. Although they also present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) exam4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1 The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011) 215 covering geometric relations such as distance, orientation, relative bounding box sizes and overlaps between bounding boxes (Table 1). We chose to use continuous features as we felt"
D15-1022,S15-2149,0,\N,Missing
demetriou-etal-2008-annalist,A97-1051,0,\N,Missing
demetriou-etal-2008-annalist,H05-1004,0,\N,Missing
demetriou-etal-2008-annalist,J01-2003,0,\N,Missing
demetriou-gaizauskas-2000-automatically,C96-1047,0,\N,Missing
demetriou-gaizauskas-2000-automatically,P95-1026,0,\N,Missing
demetriou-gaizauskas-2000-automatically,W99-0613,0,\N,Missing
demetriou-gaizauskas-2000-automatically,W99-0612,0,\N,Missing
demetriou-gaizauskas-2000-automatically,A97-1029,0,\N,Missing
demetriou-gaizauskas-2000-automatically,X98-1031,0,\N,Missing
derczynski-gaizauskas-2010-analysing,C08-3012,0,\N,Missing
derczynski-gaizauskas-2010-analysing,S07-1098,1,\N,Missing
derczynski-gaizauskas-2010-analysing,P06-1095,0,\N,Missing
H01-1040,M91-1022,0,0.0948498,"Indices Dynamic Page Creator Scenario Summaries Figure 1: TRESTLE Architecture Notice that both types of information seeking require the identification of entities and events in the news – precisely the functionality that IE systems are intended to deliver. 3. THE TRESTLE SYSTEM The overall archictecture of the TRESTLE system is shown in Figure 1. The system comprises an on-line and an off-line component. The off-line component runs automatically whenever a new electronic delivery of Scrip takes place. It runs an IE System (the LaSIE system, developed for participation in the MUC evaluations [6]), which yields as output Named Entity (NE) tagged texts and Scenario Templates. To address the domain of interest, the MUC-7 NE categories of person, location and organisation have been retained and the categories of drug names and diseases have been added. The system generates three scenario templates: person tracking (a minor modification of the MUC-6 management succession scenario), clinical trials experimental results (drug, phase of trial, experimental parameters/outcomes) and regulatory announcements (drugs approved, rejected by various agencies). After the IE system outputs the NE tagg"
harkema-etal-2004-large,A00-1026,0,\N,Missing
harkema-etal-2004-large,W02-0312,0,\N,Missing
L16-1070,L16-1494,1,0.823648,"a summary, and comments grouped appropriately and in thread context. The page is dynamically generated on the web server starting from a master document (an example of a metadocument) for a particular summarization of an article’s set of comments; the master document’s features include the information used to put the rest of the page together, in particular identifiers for other documents retrieved to generate the pie chart with its labels, the summary paragraphs, and the groups of comments. This UI prototype is described in more detail in our separate paper here on the task-based evaluation (Barker et al., 2016). 6. Conclusion We have presented a novel approach to storing, processing, and using conversational data of various kinds, which has been proven in use with a successful software deployment, with which several working input and output models are already interacting. We are currently adding asynchronous response handling through Spring’s ThreadPoolTaskExecutor (Johnson et al., 2014, §33) to the REST service, but otherwise the repository software is complete and we expect it to require no further essential work except for debugging as necessary and handling any additional features that might be"
L16-1070,S10-1021,0,0.0250374,"Missing"
L16-1070,A97-2017,1,0.369274,"d REST service with JSON for data interchange. This offers a number of advantages for easy interaction, testing, and debugging, and JSON avoids the verbosity and high-level parsing hassles associated with XML (e.g., walking a DOM tree or constructing one with SAX); it is also trivially easy to process with Python’s standard library and fairly easy to process in Java with the Jackson library (MongoLab, 2011; Saloranta, 2013; Ecma International, 2013). 2. Document model The main unit of data storage in the repository is a document object similar to the GATE or TIPSTER model (Wilks et al., 2000; Cunningham et al., 1997), which allows arbitrary data to be stored as stand-off annotations (grouped in named sets), annotation features, and document-wide features. The document is represented in the repository input and output as a JSON object1 containing an unique integer 1 The JSON object corresponds to other languages’ map, dictionary, or associative array. (Ecma International, 2013) id assigned by the repository, an optional name string, a content string representing the plain text content of the document (e.g., of a web page with the HTML tags stripped), a features object, and an annotations object. The featur"
L16-1070,P11-4015,1,0.817324,"Missing"
L16-1489,W14-3346,0,0.0156441,"t to each reference one-to-one, and takes the maximum score out of all references as the final score. We use the official version 1.5 of Meteor for this paper, with the default recommended parameters for English. (1) n=1 where wn is usually set to N1 , and N being the maximum n-gram order of BLEUN . While BLEU was originally devised as a corpus-level metric, it has also been used to measure sentence-level performance, with various smoothing techniques proposed to address the issue of n-gram sparseness at sentence level, especially for higher order n-grams (Lin and Och, 2004; Gao and He, 2013; Chen and Cherry, 2014). For this paper, we evaluate sentencelevel BLEU-1, BLEU-2, BLEU-3 and BLEU-4 with smoothing.4 Our implementation is different from the BLEU evaluation used in previous image description generation work; in their case, no smoothing is performed and the brevity penalty is always set to 1. Although the latter may have been useful in earlier, more constrained work (“a cow” is considered a good sentence), recent progress in techniques and the availability of larger datasets have shifted the focus of the task 4 Our implementation of smoothing is based on the official mteval-v13a.pl script, which as"
L16-1489,N15-1053,0,0.0158041,"set of descriptions for one image against an unrelated sentence (descriptions of another image, random words, etc.). This allows us to investigate the variation in human-authored descriptions within and across datasets. Finally, section 6. offers conclusions. 2. Image description datasets In this section, we provide a review of existing image datasets that are coupled with multiple human-authored descriptions of image content. As mentioned, noisy, largescale datasets with user-generated captions exist for news images (Berg et al., 2004; Feng and Lapata, 2008) and Flickr (Ordonez et al., 2011; Chen et al., 2015; Thomee et al., 2015). However, in this paper, we are mainly interested in literal descriptions of what is depicted in the image, rather than non-literal or non-visual descriptions that require significant inference from additional knowledge about the image context. As such, we only explore image datasets that are annotated with multiple, sentential descriptions of the visually observable content of the corresponding image. The requirement for multiple descriptions per image also rules out the IAPR TC-12 dataset (Grubinger et al., 2006) which contains only one English description per image.2"
L16-1489,W14-3348,0,0.0264882,"array of evaluation measures (and their variants) that have been proposed or adopted for the task: 1. BLEU (Papineni et al., 2002) is a precision-based metric adopted from the machine translation community. It measures the number of n-grams in a candidate sentence also appearing in at least one reference sentence, with the count clipped to avoid positive terms being over-repeated in the candidate sentence. BLEUn is the geometric mean between the modified precision (pn ) for each n, multiplied by the brevity penalty (BP ) to penalise short sentences: BLEUN = BP × exp N X wn log pn  3. Meteor (Denkowski and Lavie, 2014), again adopted from machine translation, is an f-measure-based measure that finds the optimal alignment of chunks of matched text, incorporating semantic knowledge by allowing terms to be matched to stemmed words, synonyms and paraphrases. Content and function word matches can be assigned different weights, and each type of matcher (exact, stemmed, synonym, paraphrase) is also weighted individually. Word ordering is accounted for by encouraging fewer matched chunks, indicating less fragmentation. Meteor matches a candidate text to each reference one-to-one, and takes the maximum score out of"
L16-1489,W15-2806,0,0.0202022,"ale of 14, and computing the correlation between the human scores and the scores from the same system for each metric. They found that Meteor correlates best with human judgements, followed by ROUGE-SU4 and BLEU-4 (with smoothing). Vedantam et al. (2015) also compared the four metrics in terms of how well they correlate with human judgement on a consensus task, i.e. which is more similar to sentence A? Sentence B or sentence C? They found CIDEr captured human consensus best. Other metrics have also been proposed to evaluate the content of image descriptions, for example using semantic tuples (Ellebracht et al., 2015) and concentrating only on the content selection phase (Wang and Gaizauskas, 2015). These however require additional annotations. score we report the raw CIDEr/CIDEr-D scores before this multiplication process. We also compute IDF scores independently per dataset.7 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 ROUGE-1 ROUGE-L ROUGE-W1.2 ROUGE-SU4 S S 5 L1K VLT2K Abstract Flickr30k MSCOCO LEF201 SCAL50Abstract50 PASCA ImageC PA Figure 2: Upper-bound, micro-averaged ROUGE scores for the eight datasets. BLEU. Figure 1 shows the absolute BLEU scores for the dataset. We found the overall BLEU-1 score"
L16-1489,D13-1128,0,0.0229241,"uch, images and the descriptions may be biased towards these categories. 1. UIUC PASCAL Sentence Dataset (PASCAL1K) (Farhadi et al., 2010) contains 1,000 real-world images and five crowd-sourced descriptions per image. The images are taken from the PASCAL Visual Object Classes (VOC) 2008 Challenge (Everingham et al., 2015) (which in turn are sourced from Flickr), and are thus biased towards 20 selected object categories (aeroplane, bird, chair, etc.). The descriptions are authored by Amazon’s Mechanical Turk (AMT) workers based in the US. 2. The Visual and Linguistic Treebank Dataset (VLT2K) (Elliott and Keller, 2013) comprises 2,424 images of various human actions (e.g. person using computer, riding a horse or a bicycle), along with three crowd-sourced descriptions per image. The images are again taken from the PASCAL VOC challenge, specifically the 2011 Action Classification Taster Competition to recognise 10 action classes (jumping, playing instrument, etc.). The descriptions are produced by AMT workers, and are generally made up of two sentences: the first sentence describes the main action in the image (“A band is playing on stage.”), and the second covers other background objects (“They are in a whit"
L16-1489,P14-2074,0,0.020634,"e Abstract Scenes Dataset and Abstract50S, which ended up with much higher CIDEr scores. We presume that this is mainly due to the words ‘mike’ and ‘jenny’, common in these datasets, being assigned inflated IDF weights as they do not occur often (if ever) in the dominant MS COCO and Flickr30k datasets. 8 http://visualsense.github.io/loocv/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 BLEU-1 BLEU-2 BLEU-3 BLEU-4 S S 5 L1K VLT2K Abstract Flickr30k MSCOCO LEF201 SCAL50Abstract50 PASCA ImageC PA Figure 1: Upper-bound, micro-averaged BLEU scores for the eight datasets (best viewed in colour). score Elliott and Keller (2014) evaluated how well the first three metrics correspond to human judgements. This is done by asking human annotators to score the output sentences from one of the systems of Hodosh et al. (2013) on a scale of 14, and computing the correlation between the human scores and the scores from the same system for each metric. They found that Meteor correlates best with human judgements, followed by ROUGE-SU4 and BLEU-4 (with smoothing). Vedantam et al. (2015) also compared the four metrics in terms of how well they correlate with human judgement on a consensus task, i.e. which is more similar to sente"
L16-1489,P08-1032,0,0.0281365,"evaluation, i.e. the scores obtained if one evaluates a set of descriptions for one image against an unrelated sentence (descriptions of another image, random words, etc.). This allows us to investigate the variation in human-authored descriptions within and across datasets. Finally, section 6. offers conclusions. 2. Image description datasets In this section, we provide a review of existing image datasets that are coupled with multiple human-authored descriptions of image content. As mentioned, noisy, largescale datasets with user-generated captions exist for news images (Berg et al., 2004; Feng and Lapata, 2008) and Flickr (Ordonez et al., 2011; Chen et al., 2015; Thomee et al., 2015). However, in this paper, we are mainly interested in literal descriptions of what is depicted in the image, rather than non-literal or non-visual descriptions that require significant inference from additional knowledge about the image context. As such, we only explore image datasets that are annotated with multiple, sentential descriptions of the visually observable content of the corresponding image. The requirement for multiple descriptions per image also rules out the IAPR TC-12 dataset (Grubinger et al., 2006) whic"
L16-1489,D15-1021,0,0.0365927,"Missing"
L16-1489,N13-1048,0,0.0130855,"es a candidate text to each reference one-to-one, and takes the maximum score out of all references as the final score. We use the official version 1.5 of Meteor for this paper, with the default recommended parameters for English. (1) n=1 where wn is usually set to N1 , and N being the maximum n-gram order of BLEUN . While BLEU was originally devised as a corpus-level metric, it has also been used to measure sentence-level performance, with various smoothing techniques proposed to address the issue of n-gram sparseness at sentence level, especially for higher order n-grams (Lin and Och, 2004; Gao and He, 2013; Chen and Cherry, 2014). For this paper, we evaluate sentencelevel BLEU-1, BLEU-2, BLEU-3 and BLEU-4 with smoothing.4 Our implementation is different from the BLEU evaluation used in previous image description generation work; in their case, no smoothing is performed and the brevity penalty is always set to 1. Although the latter may have been useful in earlier, more constrained work (“a cow” is considered a good sentence), recent progress in techniques and the availability of larger datasets have shifted the focus of the task 4 Our implementation of smoothing is based on the official mteval-"
L16-1489,C04-1072,0,0.0629584,"ation. Meteor matches a candidate text to each reference one-to-one, and takes the maximum score out of all references as the final score. We use the official version 1.5 of Meteor for this paper, with the default recommended parameters for English. (1) n=1 where wn is usually set to N1 , and N being the maximum n-gram order of BLEUN . While BLEU was originally devised as a corpus-level metric, it has also been used to measure sentence-level performance, with various smoothing techniques proposed to address the issue of n-gram sparseness at sentence level, especially for higher order n-grams (Lin and Och, 2004; Gao and He, 2013; Chen and Cherry, 2014). For this paper, we evaluate sentencelevel BLEU-1, BLEU-2, BLEU-3 and BLEU-4 with smoothing.4 Our implementation is different from the BLEU evaluation used in previous image description generation work; in their case, no smoothing is performed and the brevity penalty is always set to 1. Although the latter may have been useful in earlier, more constrained work (“a cow” is considered a good sentence), recent progress in techniques and the availability of larger datasets have shifted the focus of the task 4 Our implementation of smoothing is based on th"
L16-1489,W04-1013,0,0.0236148,"s abstract:concrete word ratios, syntactic complexity and perplexity. Most related to our work is their proposed measure of pairwise perplexity across different datasets to predict the words in a test set given a language model trained on another dataset. Our proposed LOOCV method can also achieve this, albeit in a different manner, and additionally allows us to evaluate the metrics as well as the datasets. 3. to “describing the image as a human would”. As such we argue that precision alone is insufficient, and that recall should now be factored in as part of the evaluation process. 2. ROUGE (Lin, 2004) is a recall-based metric used to evaluate automatic summarisation systems. In its original formulation, ROUGE-N computes the n-gram recall between a candidate summary and a set of reference summaries. Its variants, such as ROUGE-L and ROUGE-S, are f-measure-based metrics. ROUGE-L considers the longest common subsequence between two summaries, while ROUGE-S uses skip-bigram cooccurrences as statistics for measuring the similarity between two summaries, allowing for gaps between the two terms of a bigram. ROUGE-W is a variant of ROUGE-L, and awards higher scores to contiguous n-grams over skip-"
L16-1489,P02-1040,0,0.0971856,"am. ROUGE-W is a variant of ROUGE-L, and awards higher scores to contiguous n-grams over skip-grams. ROUGE-SU is an extension of ROUGE-S which also captures unigram cooccurrences in addition to skip bigrams. In this paper, we evaluate – using the official rouge-1.5.5.pl script – the following variants: ROUGE-15 , ROUGEL, ROUGE-W1.2, and ROUGE-SU4. Evaluation metrics Several automatic metrics have been proposed for evaluating image description generation systems. We review and compare an array of evaluation measures (and their variants) that have been proposed or adopted for the task: 1. BLEU (Papineni et al., 2002) is a precision-based metric adopted from the machine translation community. It measures the number of n-grams in a candidate sentence also appearing in at least one reference sentence, with the count clipped to avoid positive terms being over-repeated in the candidate sentence. BLEUn is the geometric mean between the modified precision (pn ) for each n, multiplied by the brevity penalty (BP ) to penalise short sentences: BLEUN = BP × exp N X wn log pn  3. Meteor (Denkowski and Lavie, 2014), again adopted from machine translation, is an f-measure-based measure that finds the optimal alignment"
L16-1489,W10-0721,0,0.0442482,"is depicted in the image, rather than non-literal or non-visual descriptions that require significant inference from additional knowledge about the image context. As such, we only explore image datasets that are annotated with multiple, sentential descriptions of the visually observable content of the corresponding image. The requirement for multiple descriptions per image also rules out the IAPR TC-12 dataset (Grubinger et al., 2006) which contains only one English description per image.2 Eight datasets meet the above criteria: 4. Flickr30k (Young et al., 2014), an extension of the Flickr8k (Rashtchian et al., 2010) dataset, contains over 30,000 Flickr images with five AMT crowdsourced descriptions each. The original Flickr8k dataset is the successor of PASCAL1K ((1) above), and later extended as the Flickr30k dataset. Images are collected directly from Flickr, and depict various actions, events and human activities. 5. MS COCO (Microsoft Common Objects in Context) (Lin et al., 2014) contains approximately 80,000 training images and 40,000 validation images with at least five AMT crowd-sourced descriptions per image. Like previous datasets, the images are sourced from Flickr. The emphasis on this dataset"
L16-1489,W15-4722,1,0.84914,"from the same system for each metric. They found that Meteor correlates best with human judgements, followed by ROUGE-SU4 and BLEU-4 (with smoothing). Vedantam et al. (2015) also compared the four metrics in terms of how well they correlate with human judgement on a consensus task, i.e. which is more similar to sentence A? Sentence B or sentence C? They found CIDEr captured human consensus best. Other metrics have also been proposed to evaluate the content of image descriptions, for example using semantic tuples (Ellebracht et al., 2015) and concentrating only on the content selection phase (Wang and Gaizauskas, 2015). These however require additional annotations. score we report the raw CIDEr/CIDEr-D scores before this multiplication process. We also compute IDF scores independently per dataset.7 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 ROUGE-1 ROUGE-L ROUGE-W1.2 ROUGE-SU4 S S 5 L1K VLT2K Abstract Flickr30k MSCOCO LEF201 SCAL50Abstract50 PASCA ImageC PA Figure 2: Upper-bound, micro-averaged ROUGE scores for the eight datasets. BLEU. Figure 1 shows the absolute BLEU scores for the dataset. We found the overall BLEU-1 scores to be high (0.56-0.91), and as expected are lower for increased ngram length. We"
L16-1489,Q14-1006,0,0.0528906,"mainly interested in literal descriptions of what is depicted in the image, rather than non-literal or non-visual descriptions that require significant inference from additional knowledge about the image context. As such, we only explore image datasets that are annotated with multiple, sentential descriptions of the visually observable content of the corresponding image. The requirement for multiple descriptions per image also rules out the IAPR TC-12 dataset (Grubinger et al., 2006) which contains only one English description per image.2 Eight datasets meet the above criteria: 4. Flickr30k (Young et al., 2014), an extension of the Flickr8k (Rashtchian et al., 2010) dataset, contains over 30,000 Flickr images with five AMT crowdsourced descriptions each. The original Flickr8k dataset is the successor of PASCAL1K ((1) above), and later extended as the Flickr30k dataset. Images are collected directly from Flickr, and depict various actions, events and human activities. 5. MS COCO (Microsoft Common Objects in Context) (Lin et al., 2014) contains approximately 80,000 training images and 40,000 validation images with at least five AMT crowd-sourced descriptions per image. Like previous datasets, the imag"
L16-1489,W10-0707,0,\N,Missing
L16-1494,W15-4631,0,0.150136,"ary of reader comments. Furthermore, the evaluations proposed so far, despite in several cases being called user studies, are not task-based evaluations that might let us understand how well systems are meeting user needs. A different, but promising, line of work, not yet deployed in summarization systems, is that on argument mining. Much of reader comment is argumentative and one appealing type of summary is one that would summarise the main points of contention in comments, something it is not clear an extractive summary could do. Work by e.g. Ghosh et al., (2014) Habernal et al. (2014) and Swanson et al. (2015) amongst others, focuses on defining and identifying key argumentative units and their relations. They mention summarization of argumentative texts as one potential application of their work. However, they do not specify what an end-user summary of reader comment on news might be like. In this paper we make three contributions to advancing work in this area. First, we offer a specification of one possible summary type for reader comment based on the notions of viewpoint and issue, which we define below (Section 2.). Second, we propose a task-based evaluation framework in which users are offere"
L16-1494,W14-2106,0,\N,Missing
llorens-etal-2012-timen,S10-1062,0,\N,Missing
llorens-etal-2012-timen,derczynski-gaizauskas-2010-analysing,1,\N,Missing
llorens-etal-2012-timen,S10-1071,0,\N,Missing
llorens-etal-2012-timen,P00-1010,0,\N,Missing
llorens-etal-2012-timen,W03-0502,0,\N,Missing
llorens-etal-2012-timen,P05-3021,0,\N,Missing
llorens-etal-2012-timen,S10-1010,0,\N,Missing
llorens-etal-2012-timen,S10-1063,1,\N,Missing
mitchell-gaizauskas-2002-comparison,W97-0109,0,\N,Missing
mitchell-gaizauskas-2002-comparison,J93-2004,0,\N,Missing
mitchell-gaizauskas-2002-comparison,J93-1005,0,\N,Missing
mitchell-gaizauskas-2002-comparison,H94-1048,0,\N,Missing
mitchell-gaizauskas-2002-comparison,P90-1004,0,\N,Missing
mitchell-gaizauskas-2004-labelled,W97-1016,0,\N,Missing
mitchell-gaizauskas-2004-labelled,W97-0317,0,\N,Missing
mitchell-gaizauskas-2004-labelled,J93-1005,0,\N,Missing
mitchell-gaizauskas-2004-labelled,H94-1048,0,\N,Missing
mitchell-gaizauskas-2004-labelled,H94-1020,0,\N,Missing
mitchell-gaizauskas-2004-labelled,mitchell-gaizauskas-2002-comparison,1,\N,Missing
O98-4002,C92-4200,0,0.0182789,"Missing"
O98-4002,J81-4005,0,0.374817,"Missing"
O98-4002,A92-1024,0,0.0239485,"Missing"
O98-4002,A83-1020,0,0.775815,"Missing"
O98-4002,A83-1024,0,0.851833,"Missing"
O98-4002,X93-1016,0,0.0717482,"Missing"
O98-4002,M95-1005,0,0.0195094,"Missing"
O98-4002,M93-1008,0,0.0919055,"Missing"
O98-4002,M93-1009,0,0.0652359,"Missing"
O98-4002,A97-1035,1,0.697843,"Missing"
O98-4002,H92-1022,0,0.0203744,"Missing"
O98-4002,J93-2004,0,0.0415785,"Missing"
O98-4002,C96-1071,1,0.812755,"Missing"
O98-4002,M91-1030,0,0.0570721,"Missing"
O98-4002,M92-1036,0,0.0555612,"Missing"
O98-4002,M93-1019,0,0.00831222,"Missing"
O98-4002,M95-1012,0,0.0136154,"Missing"
O98-4002,M95-1018,0,0.0850266,"Missing"
O98-4002,M95-1010,0,0.0242138,"Missing"
O98-4002,M95-1006,0,0.0568592,"Missing"
O98-4002,A83-1009,0,\N,Missing
O98-4002,C96-1079,0,\N,Missing
O98-4002,M92-1024,0,\N,Missing
P02-1020,P91-1022,0,0.0252695,"ave been suggested for aligning multilingual parallel corpora (Wu, 2000). These algorithms have been used to map translation equivalents across di erent languages. In this speci c case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more eÆcient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are de ned as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in the"
P02-1020,1996.amta-1.36,0,0.0913455,"n the production of daily newspapers. The question is not just whether agency copy has been reused, but to what extent and subject to what transformations. Using existing approaches from computational text analysis, we investigate their ability to classify newspapers articles into categories indicating their dependency on agency copy. 2 Journalistic reuse of a newswire The process of gathering, editing and publishing newspaper stories is a complex and specialised task often operating within speci c publishing constraints such as: 1) short deadlines; 2) prescriptive writing practice (see, e.g. Evans (1972)); 3) limits of physical size; 4) readability and audience comprehension, e.g. a tabloid&apos;s vocabulary limitations; 5) journalistic bias, e.g. political and 6) a newspaper&apos;s house style. Often newsworkers, such as the reporter and editor, will rely upon news agency copy as the basis of a news story or to verify facts and assess the Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 152-159. importance of a story in the context of all those appearing on the newswire. Because of the nature of journalistic text reuse, di eren"
P02-1020,W01-0515,0,0.213735,"uality papers (e.g. Daily Telegraph, The Guardian, The Independent and The Times). to the text reuse issues we are investigating. Finally, alignment (treating the derived text as a 	ranslation&quot; of the rst) seemed an intriguing idea, and contrasts, certainly with the ngram approach, by focusing more on local, as opposed to global measures of similarity. 4.1 Ngram Overlap An initial, straightforward approach to assessing the reuse between two texts is to measure the number of shared word ngrams. This method underlies many of the approaches used in copy detection including the approach taken by Lyon et al. (2001). They measure similarity using the settheoretic measures of containment and resemblance of shared trigrams to separate texts written independently and those with suÆcient similarity to indicate some form of copying. We treat each document as a set of overlapping n-word sequences (initially considering only n-word types) and compute a similarity score from this. Given two sets of ngrams, we use the set-theoretic containment score to measure similarity between the documents for ngrams of length 1 to 10 words. For a source text A and a possibly derived text B represented by sets of ngrams Sn (A)"
P02-1020,J99-1003,0,0.0119568,"di erent languages. In this speci c case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more eÆcient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are de ned as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in the candidate derived text DT the sentences in the candidate source text ST are compared in order to nd the best match. A DT sentence is allowe"
P02-1020,1992.tmi-1.7,0,0.0605392,"n equivalents across di erent languages. In this speci c case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more eÆcient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are de ned as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in the candidate derived text DT the sentences in the candidate source text ST are compared in order to nd the best match. A DT se"
P02-1020,J93-1004,0,\N,Missing
P10-1127,R09-1002,1,0.572386,"ility of GPS (Global Position System) equipped cameras and phones, as well as by the widespread use of online social sites. The majority of these images are indexed with GPS coordinates (latitude and longitude) only and/or have minimal captions. This typically small amount of textual information associated with the image is of limited usefulness for image indexing, organization and search. Therefore methods which could automatically supplement the information available for image indexing and lead to improved image retrieval would be extremely useful. Following the general approach proposed by Aker and Gaizauskas (2009), in this paper we describe a method for automatic image captioning or caption enhancement starting with only a scene or subject type and a set of place names pertaining to an image – for example hchurch, {St. Paul’s,London}i. Scene type and place names can be obtained automatically given GPS coordinates and compass information using techniques such as those described in Xin et al. (2010) – that task is not the focus of this paper. Our method applies only to images of static features of the built or natural landscape, i.e. objects with persistent geo-coordinates, such as buildings and mountain"
P10-1127,aker-gaizauskas-2010-model,1,0.832858,"ute the values for the different features. This gives information about each feature’s value for each sentence. Then the ROUGE scores and feature score values for every sentence were input to the linear regression algorithm to train the weights. Given the weights, Equation 2 is used to compute the final score for each sentence. The final sentence scores are used to sort the sentences in the descending order. This sorted list is then used by the summarizer to generate the final summary as described in Aker and Gaizauskas (2009). Data sets For evaluation we use the image collection described in Aker and Gaizauskas (2010). The image collection contains 310 different images with manually assigned toponyms. The images cover 60 of the 107 object types identified from Wikipedia (see Table 2). For each image there are up to four short descriptions or model summaries. The model summaries were created manually based on image descriptions taken from VirtualTourist and contain a minimum of 190 and a maximum of 210 words. An example model summary about the Eiffel Tower is shown in Table 4. 23 of this image collection was used to train the weights and the remaining 31 (105 images) for evaluation. To generate automatic ca"
P10-1127,H05-1091,0,0.0274904,"20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have been due to the small number of documents they used to deri"
P10-1127,P04-1054,0,0.0522634,"1.2 6.7 5.7 4.0 10.2 6.0 0.5 2.6 3.3 focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have be"
P10-1127,P07-1126,0,0.0269608,"tures the Wikipedia baseline summaries obtained better scores than our automated summaries. This comparison show that there is a gap to fill in order to obtain better readable summaries. 5 Related Work Our approach has an advantage over related work in automatic image captioning in that it requires only GPS information associated with the image in order to generate captions. Other attempts towards automatic generation of image captions generate captions based on the immediate textual context of the image with or without consideration of image related features such as colour, shape or texture (Deschacht and Moens, 2007; Mori et al., 2000; Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2003; Pan et al., 2004; Feng and Lapata, 2008; Satoh et al., 1999; Berg et al., 2005). However, Marsch & White (2003) argue that the content of an image and its immediate text have little semantic agreement and this can, according to Purves et al. (2008), be misleading to image retrieval. Furthermore, these approaches assume that the image has been obtained from a document. In cases where there is no document associated with the image, which is the scenario we are principally concerned with, these techniques"
P10-1127,P08-1032,0,0.0102871,"fill in order to obtain better readable summaries. 5 Related Work Our approach has an advantage over related work in automatic image captioning in that it requires only GPS information associated with the image in order to generate captions. Other attempts towards automatic generation of image captions generate captions based on the immediate textual context of the image with or without consideration of image related features such as colour, shape or texture (Deschacht and Moens, 2007; Mori et al., 2000; Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2003; Pan et al., 2004; Feng and Lapata, 2008; Satoh et al., 1999; Berg et al., 2005). However, Marsch & White (2003) argue that the content of an image and its immediate text have little semantic agreement and this can, according to Purves et al. (2008), be misleading to image retrieval. Furthermore, these approaches assume that the image has been obtained from a document. In cases where there is no document associated with the image, which is the scenario we are principally concerned with, these techniques are not applicable. 1256 Table 7: Readability evaluation results: Each cell shows the percentage of summaries scoring the ranking s"
P10-1127,W04-1013,0,0.0785449,"ed sentences and finally the last two sentences from the “surrounding” and “visiting” categories. However, in cases where we have not reached the summary word limit because of uncovered categories, i.e. there were not, for instance, sentences about “location”, we add to the end of the summary the next top sentence from the ranked list that was not taken. 3.2 Sentence Selection To compute the final score for each sentence Aker and Gaizauskas (2009) use a linear function with weighted features: n X Sscore = ( 4 Evaluation To evaluate our approach we used two different assessment methods: ROUGE (Lin, 2004) and manual readability. In the following we first describe the data sets used in each of these evaluations, and then we present the results of each assessment. 4.1 f eaturei ∗ weighti ) (2) i=1 We use the same approach, but whereas the feature weights they use are experimentally set rather than learned, we learn the weights using linear regression instead. We used 23 of the 310 images from our image set (see Section 4.1) to train the weights. The image descriptions from this data set are used as model summaries. Our training data contains for each image a set of image descriptions taken from"
P10-1127,nobata-etal-2002-summarization,0,0.024792,"3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have been due to the small number of documents they used to derive their dependency patterns – they gathered dependency patterns from only ten domain specific documents which are unlikely to be sufficient to capture repeated features in a domain"
P10-1127,P05-1047,0,0.0210837,"5 2.6 3.3 focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have been due to the small number of d"
P10-1127,H01-1009,0,0.135272,"We replicate the experiments of Aker and Gaizauskas and generate a bi-gram language model for each object type corpus. In later sections we use LM to refer to these models. 2.3 Dependency patterns We use the same object type corpora to derive dependency patterns. Our patterns are derived from dependency trees which are obtained using the Stanford parser1 . Each article in each object type corpus was pre-processed by sentence splitting and named entity tagging2 . Then each sentence was parsed by the Stanford dependency parser to obtain relational patterns. As with the chain model introduced by Sudo et al. (2001) our relational patterns are concentrated on the verbs in the sentences and contain n+1 words (the verb and n words in direct or indirect relation with the verb). The number n is experimentally set to two words. For illustration consider the sentence shown in Table 3 that is taken from an article in the bridge corpus. The first two rows of the table show the original sentence and its form after named entity tagging. The next step in processing is to replace any occurrence of a string denoting the object type by the term “OBJECTTYPE” as shown in the third row of Table 3. The final two rows of t"
P10-1127,C00-2136,0,0.00873769,"SLMD clarity 72.6 50.5 53.6 21.7 30.0 31.4 1.2 6.7 5.7 4.0 10.2 6.0 0.5 2.6 3.3 focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small cont"
P13-2114,A00-2031,0,0.158814,"Missing"
P13-2114,P07-2044,0,0.0762794,"detailed analysis of the performance of a set of labelling techniques when using temporal signals. 3 class, aspect, modality, tense, polarity, part of speech; and, for times: value, type, function in document, mod, quant. To these are added same-tense and same-aspect features, as well as the string values of events/times. The feature groups we use here are: • Base – The attributes of TimeML annotations involved (includes tense, aspect, polarity and so on as above), as with previous approaches. • Argument Ordering – Two features: a boolean set if both arguments are in the same sentence (as in Chambers et al. (2007)), and the text order of argument intervals (as in Hepple et al. (2007)). Experimental Setup We only approach the relation typing task, and we use existing signal annotations – that is, we do not attempt to automatically identify temporal signals. The corpus used is the signal-curated version of TimeBank (Pustejovsky et al., 2003). This corpus, TBsig,1 adds extra events, times and relations to TimeBank, in an effort to correct signal under-annotation in the original corpus (Derczynski and Gaizauskas, 2011). Like the original TimeBank corpus, it comprises 183 documents. In these, we are interes"
P13-2114,W13-0107,1,0.845944,"Missing"
P13-2114,D12-1062,0,0.120211,"Missing"
P13-2114,R11-1004,0,0.0603566,"Missing"
P13-2114,S07-1098,1,0.899661,"Missing"
P13-2114,S13-2001,1,0.872271,"Missing"
P13-2114,P03-1054,0,0.0200137,"Missing"
P13-2114,P06-1095,0,0.0907548,"Missing"
P13-2114,P10-1073,0,0.0608226,"Missing"
P13-2114,S10-1063,0,0.152025,"Missing"
P13-2114,P09-1046,0,0.0706006,"Missing"
P13-2114,S07-1046,0,0.072222,"Missing"
P14-2013,P11-1115,0,0.0512652,"Missing"
P14-2013,E09-1005,0,0.0104788,"as one of finding a dense sub-graph, which is infeasible in a huge graph. So, an algorithm originally used to find strongly interconnected, size-limited groups in social media is adopted to prune the graph, and then a greedy algorithm is used to find the densest graph. Our proposed model uses the Page-Rank (PR) algorithm (Page et al., 1999), which to our knowledge has not previously been applied to NED. Xing and Ghorbani (2004) adopted PR to consider the weights of links and the nodes’ importance. PR and Personalized PR algorithms have been used successfully in WSD (Sinha and Mihalcea, 2007; Agirre and Soroa, 2009). ranks all nodes by combining the initial confidence and graph ranking scores. We consider several different measures for computing the initial confidence assigned to each node and several measures for determining and weighting the graph edges. Node linking relies on the fact that the textual portion of KB entries typically contains mentions of other NEs. When these mentions are hyper-linked to KB entries, we can infer that there is some relation between the real world entities corresponding to the KB entries, i.e. that they should be linked in our solution graph. These links also allow us to"
P14-2013,E06-1002,0,0.0458545,"quires systems to deal with the case where there is no entry for the NE in the reference KB. Ji et al. (2011) group and summarise the different approaches to EL taken by participating systems. In general, there are two main lines of approach to the NED problem. Single entity disambiguation approaches (SNED), disambiguate one entity at a time without considering the effect of other NEs. These approaches use local context textual features of the mention and compare them to the textual features of NE candidate documents in the KB, and link to the most similar. The first approach in this line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate. More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy and by Milne and Witten (2008) who built on Cucerzan’s work. Han and Sun (2011) combine different forms of disambiguation knowledge using evidence from mention-entity associations and 3 Solution Graph In this section we discuss the construction of a graph representation that we call the solution 1 Graph"
P14-2013,D07-1074,0,0.228202,"ng systems. In general, there are two main lines of approach to the NED problem. Single entity disambiguation approaches (SNED), disambiguate one entity at a time without considering the effect of other NEs. These approaches use local context textual features of the mention and compare them to the textual features of NE candidate documents in the KB, and link to the most similar. The first approach in this line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate. More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy and by Milne and Witten (2008) who built on Cucerzan’s work. Han and Sun (2011) combine different forms of disambiguation knowledge using evidence from mention-entity associations and 3 Solution Graph In this section we discuss the construction of a graph representation that we call the solution 1 Graph models are also widely used in Word Sense Disambiguation (WSD), which has lots of similarities to NED (Guti´errez et al., 2011; Guti´errez et al., 2012). 76 NE title in th"
P14-2013,P11-1138,0,0.540002,"ed jointly. These approaches try to model the interdependence between the different candidate entities for different NE mentions in the query document, and reformulate the problem of NED as a global optimization problem whose aim is to find the best set of entities. As this new formulation is NPhard, many approximations have been proposed. Alhelbawy and Gaizauskas (2013) proposed a sequence dependency model using HMMs to model NE interdependency. Another approximation uses a mixture of local and global features to train the coefficients of a linear ranking SVM to rank different NE candidates (Ratinov et al., 2011). Shirakawa et al. (2011) cluster related textual mentions and assign a concept to each cluster using a probabilistic taxonomy. The concept associated with a mention is used in selecting the correct entity from the Freebase KB. Graph models are widely used in collective approaches1 . All these approaches model NE interdependencies, while different methods may be used for disambiguation. Han (2011) uses local dependency between NE mention and the candidate entity, and semantic relatedness between candidate entities to construct a referent graph, proposing a collective inference algorithm to inf"
P14-2013,P11-1095,0,0.370047,"Missing"
P14-2013,D11-1072,0,0.796108,"Missing"
P98-1011,C96-1079,0,0.33421,"basic focusing mechanism can be easily tested, resulting in refinements to the mechanism and resolution rules. Results show that the focusing algorithm is highly sensitive to the quality of syntactic-semantic analyses, when compared to a simpler heuristic-based approach. 1 Introduction Anaphora resolution is still present as a significant linguistic problem, both theoretically and practically, and interest has recently been renewed with the introduction of a quantitative evaluation regime as part of the Message Understanding Conference (MUC) evaluations of Information Extraction (IE) systems (Grishman and Sundheim, 1996). This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature. This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on an extension of Sidner&apos;s algorithm (Sidner, 1981) proposed in (Azzam, 1996), with further refinements from development on real-world texts. The approach * This work was carried out in the context of the EU AVENTINUS project (Thumair, 1996), which aims"
P98-1011,J81-4001,0,0.826947,"s recently been renewed with the introduction of a quantitative evaluation regime as part of the Message Understanding Conference (MUC) evaluations of Information Extraction (IE) systems (Grishman and Sundheim, 1996). This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature. This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on an extension of Sidner&apos;s algorithm (Sidner, 1981) proposed in (Azzam, 1996), with further refinements from development on real-world texts. The approach * This work was carried out in the context of the EU AVENTINUS project (Thumair, 1996), which aims to develop a multilingual IE system for drug enforcement, and including a language-independent coreference mechanism (Azzam et al., 1998). 74 is implemented within the general coreference mechanism provided by the LaSIE (Large Scale Information Extraction) system (Gaizauskas et al., 1995) and (Humphreys et al., 1998), Sheffield University&apos;s entry in the MUC-6 and 7 evaluations. 2 Focus in Anaph"
P98-1011,M95-1005,0,0.0662426,"ably evaluated. The final definition included only &apos;identity&apos; relations between text strings: proper nouns, c o m m o n nouns and pronouns. Other possible coreference relations, such as &apos;part-whole&apos;, and nontext strings (zero anaphora) were excluded. The definition was used to manually annotate several corpora of newswire texts, using SGML markup to indicate relations between text strings. Automatically annotated texts, produced by systems using the same markup scheme, were then compared with the manually annotated versions, using scoring software made available to MUC participants, based on (Vilain et al., 1995). The scoring software calculates the standard Information Retrieval metrics of &apos;recall&apos; and &apos;precision&apos;, 2 together with an overall f-measure. The following section presents the results obtained using the corpora and scorer provided for MUC-7 training (60 texts, average 581 words per text, 19 words per sentence) and evaluation (20 texts, average 605 words per text, 20 words per sentence), the latter provided for the formal MUC-7 run and kept blind during development. 6 Results The MUC scorer does not distinguish between different classes of anaphora (pronouns, definite noun phrases, bare noun"
P98-1011,M95-1010,0,\N,Missing
P98-1011,M95-1017,0,\N,Missing
P98-1011,P96-1035,1,\N,Missing
P98-1115,P97-1021,0,0.136357,"Missing"
P98-1115,P96-1025,0,0.0607813,"Missing"
P98-1115,P98-1115,1,0.0512067,"Missing"
P98-1115,H94-1020,0,0.0495892,"ay be able to achieve parsing performance similar to the best results in the field obtained in (Collins, 1996). 2 Rule Growth and Partial Bracketting Growth of the Rule Set One could investigate whether there is a finite grammar that should account for any text within a class of related texts (i.e. a domain oriented sub-grammar of English). If there is, the number of extracted rules will approach a limit as more sentences are processed, i.e. as the rule number approaches the size of such an underlying and finite grammar. We had hoped that some approach to a limit would be seen using P T B II (Marcus et al., 1994), which larger and more consistent for bracketting than P T B I. As shown in Figure 1, however, the rule number growth continues unabated even after more than 1 million part-ofspeech tokens have been processed. 700 Why should the set of rules continue to grow in this way? P u t t i n g aside the possibility that natural languages do not have finite rule sets, we can think of two possible answers. First, it may be that the full &quot;underlying grammar&quot; is much larger than the rule set that has so far been produced, requiring a much larger tree-banked corpus than is now available for its extraction."
P98-1115,1995.iwpt-1.26,0,0.255817,"Missing"
P98-1115,J95-2002,0,0.140312,"Missing"
paramita-etal-2012-correlation,W99-0625,0,\N,Missing
paramita-etal-2012-correlation,W06-2810,0,\N,Missing
paramita-etal-2012-correlation,W11-1212,0,\N,Missing
paramita-etal-2012-correlation,W09-1605,0,\N,Missing
paramita-etal-2012-correlation,N10-1063,0,\N,Missing
paramita-etal-2012-correlation,J05-4003,0,\N,Missing
paramita-etal-2012-correlation,W11-2206,0,\N,Missing
paramita-etal-2012-correlation,D07-1036,0,\N,Missing
paramita-etal-2012-correlation,N09-2031,0,\N,Missing
paramita-etal-2012-correlation,zesch-etal-2008-extracting,0,\N,Missing
R09-1002,W08-1407,1,0.850161,"t applicable. In this paper, we propose a technique for automatic image captioning or caption enhancement starting with only a set of place names pertaining to an image. The technique applies just to images of static features of the built or natural landscape (e.g. buildings, mountains, etc.) and not to images of objects which move about in such landscapes (e.g. people, cars, clouds, etc.). Our approach is based on extractive multi-document summarization techniques, where the documents to be summarized are web-documents retrieved using the place names associated with an image. In earlier work [1] we have shown that in this scenario query-based summaries outperform generic summaries, i.e. extractive summaries of multiple web pages retrieved using the place names which bias the summarizer to include sentences mentioning these place names tend to be better than generic summaries of the same pages. However, the resulting summaries were still far from ideal. We examined information selected by humans for inclusion in a caption from the same place-nameretrieved web-documents made available to the summarizer and observed high levels of agreement between humans on which information to include"
R09-1002,P07-1126,0,0.171497,"has grown immensely, facilitated by the development of cheap digital hardware and the availability of online image sharing social sites. Many of these images are tagged only with place names or contain minimal captions that include locational information. This small amount of textual information associated with the image is of limited usefulness for image indexing, organization and search. What would be useful is a means to generate or augment captions automatically based on existing data. Attempts towards automatic generation of image captions have been previously reported. Deschacht & Moens [6] and Mori et al. [14] generate image captions automatically by analyzing image-related text from the immediate context of the image, e.g. the surrounding text in HTML documents. The authors identify named entities and other noun phrases in the imagerelated text and assign these to the image as captions. Other approaches create image captions by taking into consideration image features (colour, shape and texture) as well as image-related text [22, 14, 4, 7, 3, 15, 8]. These approaches analyze only the immediate textual context of the image. However, generating image captions based on the immedi"
R09-1002,P08-1032,0,0.088501,"aptions automatically based on existing data. Attempts towards automatic generation of image captions have been previously reported. Deschacht & Moens [6] and Mori et al. [14] generate image captions automatically by analyzing image-related text from the immediate context of the image, e.g. the surrounding text in HTML documents. The authors identify named entities and other noun phrases in the imagerelated text and assign these to the image as captions. Other approaches create image captions by taking into consideration image features (colour, shape and texture) as well as image-related text [22, 14, 4, 7, 3, 15, 8]. These approaches analyze only the immediate textual context of the image. However, generating image captions based on the immediate context of the image can result in an image description which does not describe the image at all. Marsch & White [13] argue that the content of an image and its immediate text have little semantic agreement and this can, according to Purves et al. [16], be misleading to image retrieval. Furthermore, these approaches assume that the image has been obtained from a document. In cases where there is no document associated with the image, which is the scenario we are"
R09-1002,C92-2082,0,0.0139715,"Missing"
R09-1002,W04-1013,0,0.0342275,"n anyway they want, resulting in image captions of different length, coherence, focus, grammaticality etc. To ensure a good standard for our model captions we asked 11 human subjects to generate up to four model captions per object by modifying Virtualtourist captions. The modifications included deleting personal information, ensuring consistency and coherence of the text and generating a summary of 190-210 words in length (because our automatic summaries have similar word counts). An example model summary about Parc Guell is shown in Table 6. For comparison between summaries the ROUGE metric [11] is used. ROUGE compares automatically generated summaries against human-created reference summaries and can be used to estimate content coverage in an automatically generated summary. Following the Document Understanding Conference (DUC) [5] evaluation standards we use ROUGE 2 and ROUGE SU4 as evaluation metrics. ROUGE 2 gives recall scores for bi-gram overlap between the automatically generated summaries and the reference ones. ROUGE SU4 allows bi-grams to be composed of non-contiguous words, with a maximum of four words between the bi-grams. 3 www.virtualtourist.com Table 4: ROUGE scores fo"
R09-1002,C00-1072,0,0.220542,"ation that humans appear to have a conceptual model of what is salient regarding a specific object type, the question arises as to whether we can represent or approximate such a conceptual model in a way that allows us to improve content selection for our caption summaries. While there are many ways this could be done, one simple way is to view a corpus of descriptions of objects of a given type as containing an implicit model of that type and use language models derived from the corpus to bias sentence selection by an extractive summarizer. In this paper we explore the use of signature words [12] and language models [21] to represent such concep6 International Conference RANLP 2009 - Borovets, Bulgaria, pages 6–11 tual models and investigate their impact on the quality of automatically generated image captions. Our results show that using these conceptual models does indeed improve the results over those of a standard query-based summarizer. In the following we first describe how the object type corpora were collected (section 2) and how language models are generated from these corpora (section 3). Next, we describe the set of our images, their categorization by object type and the re"
R15-1017,S13-2001,1,0.803658,"nd us. When we automatically extract temporal information, we are often concerned with events and times – referred to collectively as temporal intervals. We might ask, for example, “Who is the current President of the USA?.” In order to extract a single contemporary answer to this question, we need to identify events related to persons becoming president and the times of those events. Crucially, however, we also need to identify the ordering between these events and times, by assigning a temporal relation type (from e.g. Allen (1983)). This last task, temporal relation typing, is challenging (UzZaman et al., 2013; Bethard et al., 2015), and is the focus of this paper. When events are expressed as verbs, tense and aspect are used to convey temporal features of these events. Thus, it is intuitive that tense and aspect will be of value in determining the type of temporal relation that holds between two verb events, and evidence in human-annotated corpora supports this intuition. 1 TimeBank is a corpus semantically annotated for temporal information in TimeML (Pustejovsky et al., 2003; Pustejovsky et al., 2004) 2 In TimeML v1.2, the tense attribute of events has values that are conflated with verb form. T"
R15-1017,Q14-1022,0,0.0613993,"b event orderings based on the Reichenbachian tenses that map directly to those in TimeML. Cell values describe the e1 [rel] e2 relationship. 3 In Example 1, the reference point is determined positionally with an explicit time (10 o’clock). TimeML provides some of the information that Reichenbach’s framework alone does not cater for and vice versa. A combination of the two may lead to better labelling performance, but relying on Reichenbach’s framework alone for rule-based temporal relation label constraint is insufficient. However, the framework has shown to inform prior systems effectively (Chambers et al., 2014). The situations we examine are those where two verb events occur in the same temporal context, where a timex directly influences a verb event, and also verb events that report other verb events. Reichenbach’s framework is used as a linguistic model that generates temporal ordering features, which are added to a base feature set. The base features are those as in Mani et al. (2007), i.e.: (1) It was 10 o’clock, and Sarah had brushed her teeth. The verb group had brushed is anterior past tense; that is, E &lt; R &lt; S. The event is complete before the reference time – that is, at any point until 10"
R15-1017,W13-0107,1,0.794775,"brushed is anterior past tense; that is, E &lt; R &lt; S. The event is complete before the reference time – that is, at any point until 10 o’clock – and so the relation between the event and timex can be determined (brushed BE FORE 10 o’clock). 2.3 The Framework in TLINK Typing Feature Extraction Two interpretations of the model are used in feature extraction. Firstly, a simple view is taken assuming permanence of the reference point. This provides a constraint dependent on the pairing of Reichenbachian tenses used, and is detailed in Table 2. Secondly, an advanced interpretation is used, following Derczynski and Gaizauskas (2013). This approach fully populates all Reichenbachian tense combinations using Freksa’s temporal semi-interval algebra (Freksa, 1992) to derive a (large) temporal constraint table, which for space reasons is omitted here. In all cases, the gold standard tense and aspect features annotated on the events in TimeBank are used as the basis for Reichenbachian representations. For each event: text; TimeML tense and aspect; modality; cardinality; polarity; event class; partof-speech tag. For each event pair: booleans for: are events in the same sentence; are events in adjacent sentences; do events have"
R15-1017,P06-1095,0,0.0934359,"Missing"
R15-1017,W13-1903,0,0.0540506,"Missing"
R15-1017,S15-2136,1,\N,Missing
S07-1014,P06-1095,1,0.245989,"ovsky et al., 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al., 2003b; Boguraev et al., forthcoming) was originally conceived of as a proof of concept that illustrates the TimeML language, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al., 2006; Boguraev et al., forthcoming). An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expressions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemed more effective. Thus we here present an initial evaluation exercise based on three limited tasks that we bel"
S07-1098,P06-1095,0,0.144129,"‘lite’ approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions. 1 Introduction The Sheffield team were involved in TempEval as co-proposers/co-organisers of the task.1 For our participation in the task, we decided to pursue an MLbased approach, the benefits of which have been explored elsewhere (Boguraev and Ando, 2005; Mani et al., 2006). For the TempEval tasks, this is easily done by treating the assignment of temporal relation types as a simple classification task, using readily available information for the instance features. More specifically, the features used were ones provided as 1 We maintained a strict separation between persons assisting in annotation of the test corpus and those involved in system development. In what follows, we will first describe how our system was constructed, before going on to discuss our main observations around the key aims mentioned above. For example, in regard to our ‘lite’ approach, we"
S07-1098,S07-1014,1,0.902509,"for the task. (These were produced using WEKA’s rules.ZeroR algorithm, which does exactly that.) The best results observed for each task are shown in bold in the table. These best performing algorithms were used for the corresponding tasks in the competition. Observe that the lazy.KStar 2 These scores are computed under the ‘strict’ requirement that key and response labels should be identical. The TempEval competition also uses a ‘relaxed’ metric which gives partial credit when one (or both) label is disjunctive and there is a partial match, e.g. between labels AFTER and OVERLAP-ORAFTER. See (Verhagen et al., 2007) for details. USFD ave. max. Task A FS FR 0.59 0.60 0.56 0.59 0.62 0.64 Task B FS FR 0.73 0.74 0.74 0.75 0.80 0.81 Task C FS FR 0.54 0.59 0.51 0.60 0.55 0.66 Table 3: Competition task scores for Sheffield system (USFD), plus average/max scores across all competing systems method, which gives the best performance for Task A, gives a rather ‘middling’ performance for Task B. Similarly, the SVM method that gives the best results for Task C falls quite a way below the performance of KStar on Task A. A more extreme case is seen with the results for rules.JRip (Weka’s implementation of the RIPPER al"
S10-1075,S07-1052,0,0.0357776,"Missing"
S10-1075,S07-1098,1,0.930026,"Missing"
S10-1075,P00-1010,0,0.572047,"oral Expresions and TLINKs for TempEval-2 Robert Gaizauskas Dept of Computer Science University of Sheffield Regent Court 211 Portobello Sheffield S1 4DP, UK robertg@dcs.shef.ac.uk Leon Derczynski Dept of Computer Science University of Sheffield Regent Court 211 Portobello Sheffield S1 4DP, UK leon@dcs.shef.ac.uk Abstract from the TempEval-2 training data annotation, augmented by features that can be directly derived from the annotated texts. There are two main aims of this work: (1) to create a rule-based temporal expression annotator that includes knowledge from work published since GUTime (Mani and Wilson, 2000) and measure its performance, and (2) to measure the performance of a classifier that includes features based on temporal signals. Our entry to the challenge, USFD2, is a successor to USFD (Hepple et al., 2007). In the rest of this paper, we will describe how USFD2 is constructed (Section 2), and then go on to discuss its overall performance and the impact of some internal parameters on specific TempEval tasks. Regarding classifiers, we found that despite using identical feature sets across relation classification tasks, performance varied significantly. We also found that USFD2 performance tr"
S10-1075,P06-1095,0,0.113693,"Missing"
S10-1075,C08-1070,0,0.0339013,"Missing"
S10-1075,S07-1046,0,0.0399903,"Missing"
S10-1075,W09-2418,0,0.0368921,"Missing"
S10-1075,P09-1046,0,0.0656148,"Missing"
saggion-gaizauskas-2006-language,W05-1527,1,\N,Missing
setzer-gaizauskas-2000-annotating,M95-1005,0,\N,Missing
setzer-gaizauskas-2000-annotating,O98-4002,1,\N,Missing
skadina-etal-2012-collecting,W06-2810,0,\N,Missing
skadina-etal-2012-collecting,E06-1020,0,\N,Missing
skadina-etal-2012-collecting,W11-1217,0,\N,Missing
skadina-etal-2012-collecting,E09-1003,0,\N,Missing
skadina-etal-2012-collecting,W07-1702,0,\N,Missing
skadina-etal-2012-collecting,J03-3002,0,\N,Missing
skadina-etal-2012-collecting,W09-1605,0,\N,Missing
skadina-etal-2012-collecting,P02-1040,0,\N,Missing
skadina-etal-2012-collecting,J05-4003,0,\N,Missing
skadina-etal-2012-collecting,C10-1073,0,\N,Missing
skadina-etal-2012-collecting,R11-1106,0,\N,Missing
skadina-etal-2012-collecting,P07-2045,0,\N,Missing
skadina-etal-2012-collecting,P12-3016,1,\N,Missing
skadina-etal-2012-collecting,2005.mtsummit-papers.11,0,\N,Missing
skadina-etal-2012-collecting,aker-etal-2012-light,1,\N,Missing
skadina-etal-2012-collecting,su-babych-2012-development,1,\N,Missing
skadina-etal-2012-collecting,pinnis-2012-latvian,1,\N,Missing
skadina-etal-2012-collecting,C10-2054,0,\N,Missing
skadina-etal-2012-collecting,P00-1056,0,\N,Missing
W01-1004,J92-4003,0,0.0165778,"Missing"
W01-1004,M98-1007,0,0.0295886,"Missing"
W01-1004,W01-1002,0,0.0327066,"Missing"
W01-1004,J96-2003,0,0.0220405,"Missing"
W01-1004,C00-2136,0,0.0501815,"Missing"
W01-1311,setzer-gaizauskas-2000-annotating,1,\N,Missing
W01-1311,M95-1005,0,\N,Missing
W02-0311,H01-1040,1,\N,Missing
W02-0311,C00-1030,0,\N,Missing
W02-0311,demetriou-gaizauskas-2000-automatically,1,\N,Missing
W04-3110,O98-4002,1,0.749256,"base for storing terminological information and compiling finite state machines from this database to do term look-up. 1.2 Context Termino is being developed in the context of two ongoing projects: CLEF, for Clinical E-Science Framework (Rector et al., 2003) and myGrid (Goble et al., 2003). Both these projects involve an Information Extraction component. Information Extraction is the activity of identifying pre-defined classes of entities and relationships in natural language texts and storing this information in a structured format enabling rapid and effective access to the information, e.g. Gaizauskas and Wilks (1998), Grishman (1997). The goal of the CLEF project is to extract information from patient records regarding the treatment of cancer. The treatment of cancer patients may extend over several years and the resulting clinical record may include many documents, such as clinic letters, case notes, lab reports, discharge summaries, etc. These documents are generally full of medical terms naming entities such as body parts, drugs, problems (i.e. symptoms and diseases), investigations and interventions. Some of these terms are particular to the hospital from which the document originates. We aim to ident"
W04-3110,W02-0312,0,0.0259296,"Missing"
W04-3110,A00-1026,0,0.198023,"ized that the biomedical literature is now so large, and growing so quickly, that it is becoming increasingly difficult for researchers to access the published results that are relevant to their research. Consequently, any technology that can facilitate this access should help to increase research productivity. This has led to an increased interest in the application of natural language processing techniques for the automatic capture of biomedical content from journal abstracts, complete papers, and other textual documents (Gaizauskas et al., 2003; Hahn et al., 2002; Pustejovsky et al., 2002; Rindflesch et al., 2000). An essential processing step in these applications is the identification and semantic classification of technical terms in text, since these terms often point to entities about which information should be extracted. Proper semantic classification of terms also helps in resolving anaphora and extracting relations whose arguments are restricted semantically. 1.1 Challenge Any technical domain generates very large numbers of terms – single or multiword expressions that have some specialised use or meaning in that domain. For example, the UMLS Metathesaurus (Humphreys et al., 1998), which provid"
W05-0808,P91-1022,0,0.292601,"ages. Building on this, alignment results can be used in the creation of other Hindi language processing resources (e.g. part-of-speech taggers). We present a simple sentence length approach to align English-Hindi sentences and a hybrid approach with local word grouping and dictionary lookup as the primary techniques to align words. 2 Sentence Alignment Sentence alignment techniques vary from simple character-length or word-length techniques to more sophisticated techniques which involve lexical constraints and correlations or even cognates (Wu 2000). Examples of such alignment techniques are Brown et al. (1991), Kay and Roscheisen (1993), Warwick et al. (1989), and the “align” programme by Gale and Church (1993). 1 Introduction Text alignment is not only used for the tasks such as bilingual lexicography or machine translation but also in other language processing applications such as multilingual information retrieval and word 2.1 Length-based methods Length-based approaches are computationally better, while lexical methods are more resource 57 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 57–64, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 hu"
W05-0808,P93-1002,0,0.114799,"Missing"
W05-0808,J93-1004,0,0.166061,"sing resources (e.g. part-of-speech taggers). We present a simple sentence length approach to align English-Hindi sentences and a hybrid approach with local word grouping and dictionary lookup as the primary techniques to align words. 2 Sentence Alignment Sentence alignment techniques vary from simple character-length or word-length techniques to more sophisticated techniques which involve lexical constraints and correlations or even cognates (Wu 2000). Examples of such alignment techniques are Brown et al. (1991), Kay and Roscheisen (1993), Warwick et al. (1989), and the “align” programme by Gale and Church (1993). 1 Introduction Text alignment is not only used for the tasks such as bilingual lexicography or machine translation but also in other language processing applications such as multilingual information retrieval and word 2.1 Length-based methods Length-based approaches are computationally better, while lexical methods are more resource 57 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 57–64, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 hungry. Brown et al. (1991) and Gale and Church (1993) are amongst the most cited works in text alignment"
W05-0808,P96-1018,0,0.0610161,"Missing"
W05-0808,meyers-etal-1998-multilingual,0,0.0423796,"Missing"
W05-0808,1996.amta-1.14,0,0.105051,"Missing"
W05-0808,H01-1035,0,0.037267,"Missing"
W05-0808,J93-1006,0,\N,Missing
W05-1527,J93-2004,0,\N,Missing
W08-0602,P05-3007,0,0.019812,"spread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)). Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition. The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Na¨ıve Bayes classifier to identify entities for information retrieval applications. To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text. This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narratives. The set of relationships extracted are considered to be of interest for clinical and research applications down line of IE, such as querying to support clinical research. We apply Support V"
W08-0602,roberts-etal-2008-combining,1,0.805113,"ts. CLEF uses information extraction (IE) technology to make information from the textual portion of the medical record available for integration with the structured record, and thus available for clinical care and research. The CLEF IE system analyses the textual records to extract entities, events and the relationships between them. These relationships give information that is often not available in the structured record. Why was a drug given? What were the results of a physical examination? What problems were not present? We have previously reported entity extraction in the CLEF IE system (Roberts et al., 2008b). This paper examines relationship extraction. Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system. Several such systems have been described. They generally use a syntactic parse with domainspecific grammar rules. The Linguistic String project (Sager et al., 1994) used a full syntactic and 10 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical"
W08-0602,P05-1053,0,0.396045,"m et al., 1995) also used a full parse, a conceptual representation of the text, and a large scale knowledge base. In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)). Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition. The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Na¨ıve Bayes classifier to identify entities for information retrieval applications. To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text. This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narrati"
W08-0602,P02-1022,0,\N,Missing
W08-0611,W04-0813,1,0.901904,"Missing"
W08-0611,P04-1036,0,0.341367,"Missing"
W08-0611,W04-0807,0,0.200016,"Missing"
W08-0611,N01-1011,0,0.180154,"ncies of the ambiguous words than can be represented by the local collocations. Five relations are extracted: object, subject, noun-modifier, preposition and sibling. These are identified using heuristic patterns and regular expressions applied to PoS tag sequences around the ambiguous word. In the above example, “heparin” is noun-modifier feature of “adjustment”. 2 A maximum-entropy-based part of speech tagger was used (Ratnaparkhi, 1996) without the adaptation to the biomedical domain. 83 • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of unigrams which appear more frequently than a predefined threshold in the entire corpus, excluding those in a list of stopwords. We empirically set the threshold to 1. This feature was not used by Agirre and Mart´ınez (2004), but Joshi et al. (2005) found them to be useful for this task. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features based on UMLS Concept Unique Identifiers (CUIs). The MetaMap program (Aronson, 2001) identifies all words and terms in a text which could be mapped onto a UMLS CUI. MetaMap d"
W08-0611,W96-0213,0,0.0227158,"t-POS “NNS IN”, leftcontent-word-form “area adjustments”, rightfunction-word-form “adjustment of ”, etc. • Syntactic Dependencies: These features model longer-distance dependencies of the ambiguous words than can be represented by the local collocations. Five relations are extracted: object, subject, noun-modifier, preposition and sibling. These are identified using heuristic patterns and regular expressions applied to PoS tag sequences around the ambiguous word. In the above example, “heparin” is noun-modifier feature of “adjustment”. 2 A maximum-entropy-based part of speech tagger was used (Ratnaparkhi, 1996) without the adaptation to the biomedical domain. 83 • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of unigrams which appear more frequently than a predefined threshold in the entire corpus, excluding those in a list of stopwords. We empirically set the threshold to 1. This feature was not used by Agirre and Mart´ınez (2004), but Joshi et al. (2005) found them to be useful for this task. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features ba"
W08-0611,J01-3001,1,\N,Missing
W08-0611,J06-1003,0,\N,Missing
W08-1407,P07-1126,0,0.167376,"Missing"
W08-1407,W04-1013,0,0.121202,"ed with coordinates (latitude and longitude) and compass information, that show things with fixed locations (e.g. buildings, mountains, etc.). This paper reports an initial study that aims to assess the viability of a state-of-the-art multi-document summarizer for automatic captioning of geo-referenced images. The automatic captioning procedure requires summarizing multiple web documents that contain information related to images’ location. We use SUMMA (Saggion and Gaizauskas, 2005) to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics (Lin, 2004) relative to human generated summaries. Results show that, even though query-based summaries perform better than generic ones, they are still not selecting the information that human participants do. In particular, the areas of interest that human summaries display (history, travel information, etc.) are not contained in the query-based summaries. For our future work in automatic image captioning this result suggests that developing the query-based summarizer further and biasing it to account for user-specific requirements will prove worthwhile. 1 Attempts towards automatic generation of image"
W08-1407,W01-0100,0,0.162697,"mprovements. documents, however, the challenge lies in being able to summarize unrestricted web documents. Various multi-document summarization tools have been developed: SUMMA (Saggion and Gaizauskas, 2005), MEAD (Radev et al., 2004), CLASSY (Conroy et al., 2005), CATS (Farzinder et al., 2005) and the system of Boros et al. (2001), to name just a few. These systems generate either generic or query-based summaries or both. Generic summaries address a broad readership whereas query-based summaries are preferred by specific groups of people aiming for quick knowledge gain about specific topics (Mani, 2001). SUMMA and MEAD generate both generic and query-based multi-document summaries. Boros et al. (2001) create only generic summaries, while CLASSY and CATS create only query-based summaries from multiple documents. The performance of these tools has been reported for DUC tasks1 . As Sekine and Nobata (2003) note, although DUC tasks provide a common evaluation standard, they are restricted in topic and are somewhat idealized. For our purposes the summarizer needs to create summaries from unrestricted web input, for which there are no previous performance reports. For this reason we evaluate the p"
W08-1407,W03-0509,0,\N,Missing
W08-1805,H05-1020,0,0.0239307,"pared to those which had proven to be helpful as extension words. Relevance Feedback Relevance feedback is a widely explored technique for query expansion. It is often done using a specific measure to select terms using a limited set of ranked documents of size r; using a larger set will bring term distribution closer to values over the whole corpus, and away from ones in documents relevant to query terms. Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996). In the context of QA, Pizzato (2006) employs blind RF using the AQUAINT corpus in an attempt to improve performance when answering factoid questions on personal names. This is a similar approach to some content in this paper, though limited to the study of named entities, and does not attempt to examine extensions from the existing answer data. Monz (2003) finds a negative result when applying blind feedback for QA in TREC 9, 10 and 11, and a neutral result for TREC 7 and 8’s ad hoc retrieval tasks. Monz’s experiment, using r = 10 and standard Rocchio term weighting, also found"
W08-1805,U06-1013,0,\N,Missing
W08-1808,W07-0734,0,0.0160724,"an can also be discounted as overall its scores are too large and for this reason it would be difficult to measure improvements using it. Of the five remaining metrics – DiceSimilarity, JaccardSimilarity, BlockDistance, EuclideanDistance and CosineSimilarity – we decided that we should discount EuclideanDistance as it had the smallest gap between with target and without target. We now look at the other four metrics in more detail3 : Choosing a metric There are many different systems which attempt to measure string similarity. We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. ROUGE and METEOR were developed to compare larger stretches of text – they are usually used to compare paragraphs rather than sentences. We decided developing our own metric would be simpler than trying to adapt one of these existing tools. To explore candidate similarity measures we created a program which would take as input a list of reformulations to be assessed and a list of gold standard reformulations and compare them to each other using a selection of different string comparison metrics. To find out which of these metrics best scored ref"
W08-1808,W04-1013,0,\N,Missing
W09-1309,J08-4004,0,0.0645861,"een widely used in classification tasks. SVMs map feature vectors onto a high dimensional space and construct a classifier by searching for the hyperplane that gives the greatest separation between the classes. We used our own implementation of the Vector Space Model and Weka implementations (Witten and Frank, 2005) of the other two algorithms. 4 Evaluation Corpus The most common method for generating corpora to train and test WSD systems is to manually annotate instances of ambiguous terms found in text with the appropriate meaning. However, this process is both time-consuming and difficult (Artstein and Poesio, 2008). An alternative to manual tagging is to find a way of automatically creating sense tagged corpora. For the translation of ambiguous English words Ng et al. (2003) made use of the fact that the various senses are often translated differently. For example when “bank” is used in the ‘financial institution’ sense it is translated to French as “banque” and “bord” when it is used to mean ‘edge of river’. However, a disadvantage of this approach is that it relies on the existence of parallel text which may not be available. In the biomedical domain Liu et al. (2001)(2002) created a corpus using unam"
W09-1309,W04-0807,0,0.0216742,"scribe an approach which starts by identifying the set of candidate expansions in the same sentence as an abbreviation. The most likely one is identified by searching for the shortest candidate which contains all the characters in the abbreviation in the correct order. 3 Abbreviation Disambiguation System Our abbreviation disambiguation system is based on a state-of-the-art WSD system that has been adapted to the biomedical domain by augmenting it with additional knowledge sources. The system on which our approach is based (Agirre and Mart´ınez, 2004) participated in the Senseval-3 challenge (Mihalcea et al., 2004) with a performance close to the best system for the lexical sample tasks in two languages while the version adapted to the biomedical domain has achieved the best recorded results (Stevenson et al., 2008) on a standard test set consisting of ambiguous terms (Weeber et al., 2001). This system is based on a supervised learning approach with features derived from text around the ambiguous word that are domain independent. We refer to these as general features. This feature set has been adapted for the disambiguation of biomedical text by adding further linguistic features and two different types"
W09-1309,P03-1058,0,0.0376679,"atest separation between the classes. We used our own implementation of the Vector Space Model and Weka implementations (Witten and Frank, 2005) of the other two algorithms. 4 Evaluation Corpus The most common method for generating corpora to train and test WSD systems is to manually annotate instances of ambiguous terms found in text with the appropriate meaning. However, this process is both time-consuming and difficult (Artstein and Poesio, 2008). An alternative to manual tagging is to find a way of automatically creating sense tagged corpora. For the translation of ambiguous English words Ng et al. (2003) made use of the fact that the various senses are often translated differently. For example when “bank” is used in the ‘financial institution’ sense it is translated to French as “banque” and “bord” when it is used to mean ‘edge of river’. However, a disadvantage of this approach is that it relies on the existence of parallel text which may not be available. In the biomedical domain Liu et al. (2001)(2002) created a corpus using unambiguous related terms (see Section 2) although they found that it was not always possible to identify suitable related terms. 4.1 Corpus Creation Liu et al. (2001)"
W09-1309,C08-1083,0,0.142956,"Missing"
W09-1309,P02-1021,0,0.495723,"Missing"
W09-1309,N01-1011,0,0.150567,"following lemma/word-form of the content words (adjective, adverb, noun and verb) in the same sentence as the ambiguous abbreviation. For example, consider the sentence below with the target abreviation BSA. “Lean BSA was obtained from height and lean body weight ...” The features would include the following: left-content-word-lemma “lean BSA”, rightfunction-word-lemma “BSA be”, left-POS “JJ NNP”, right-POS “NNP VBD”, left-contentword-form “Lean BSA”, right-function-wordform “BSA was”, etc. • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of all content words in the abstract and words within a ±4-word window around the target word, excluding those in a list of stopwords. In addition, the lemmas of any unigrams appearing at least twice in the entire corpus and which are found in the abstract are also included as features. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features based on UMLS Concept Unique Identifiers (CUIs). The MetaMap program (Aronson, 2001) identifies all words and terms in a text which could be mapped onto a UMLS CUI. MetaMap does"
W09-1309,W02-0312,0,0.272251,"is often the case in biomedical documents, in this domain ubiquitous abbreviations (such as DNA and mRNA) often appear without an expansion. It has been reported that misinterpretation of abbreviations in biomedical documents has lead to medical practitioners making fatal errors (Fred and Cheng, 1999). However, identifying the correct expansion is not a straightforward task since an abbreviation may have several possible expansions. Chang et al. (2002) reported that abbreviations in biomedical journal articles consisting of six characters or less have an average of 4.61 possible meanings and Pustejovsky et al. (2002) mention that the simple abbreviation “AC” is associated with at least 10 strings in different biomedical documents including “atrioventricular connection”, “anterior colporrhaphy procedure”, “auditory cortex” and “atypical carcinoid”. The problem of identifying the correct expansion of an ambiguous abbreviation can be viewed as a Word Sense Disambiguation (WSD) task where the various expansions are the “senses” of the abbreviation. In this paper we approach the problem in this way by applying a WSD system which has previously been applied to biomedical text (Stevenson et al., 2008). The WSD s"
W09-1309,W04-0813,0,\N,Missing
W13-0107,E12-1027,0,0.0502818,"Missing"
W13-0107,W10-4205,0,0.0221874,"s framework not only offers a means of formally describing the tenses of verbs, but also rules for temporally arranging the events related by these verbs, using the its three abstract points. This can, for a subset of cases, form a basis for describing the temporal ordering of these events. The framework is currently used in approaches to many computational linguistics problems. These include language generation, summarisation, and the interpretation of temporal expressions. When automatically creating text, it is necessary to make decisions on when to shift tense to properly describe events. Elson and McKeown (2010) relate events based on a “perspective” which is calculated from the reference and event times of two verbs that each describe events. They construct a natural language generation system that uses reference times in order to correctly write stories. Further, reference point management is critical to medical summary generation. In order to helpfully unravel the meanings of tense shifts in minute-by-minute patient reports, Portet et al. (2009) required understanding of the reference point. The framework also helps interpret linguistic expressions of time (timexes). Reference time is required to"
W13-0107,S07-1098,1,0.875554,"r accurate reference time management is so persistent that state of the art systems for converting times expressed in natural language to machine-readable format now contain extra layers solely for handling reference time (Llorens et al., 2012). Given the difficulty of automatically determining the orderings, or temporal relations, between events, and the suggested ability of Reichenbach’s framework to provide information for this, it is natural to apply this framework to the temporal ordering task. Although tense has played a moderately useful part in machine learning approaches to the task (Hepple et al., 2007), its exact role in automatic temporal annotation is not fully understood. Further, though it was not the case when the framework was originally proposed, there now exist resources annotated with some temporal semantics, using TimeML (Pustejovsky et al., 2005). Comparing the explicit temporal annotations within these resources with the modes of interaction proposed by Reichenbach’s framework permits an evaluation of the validity of this established account of tense and aspect. This paper addresses the following questions: 1. How can Reichenbach’s framework be related to a modern temporal annot"
W13-0107,llorens-etal-2012-timen,1,0.831033,"eference time is required to interpret anaphoric expressions such as “last April”. Creation of recent timex corpora prompted the comment that there is a “need to develop sophisticated methods for temporal focus tracking if we are to extend current time-stamping technologies” (Mazur and Dale, 2010) – focus as a rˆole filled by Reichenbach’s reference point. In fact, demand for accurate reference time management is so persistent that state of the art systems for converting times expressed in natural language to machine-readable format now contain extra layers solely for handling reference time (Llorens et al., 2012). Given the difficulty of automatically determining the orderings, or temporal relations, between events, and the suggested ability of Reichenbach’s framework to provide information for this, it is natural to apply this framework to the temporal ordering task. Although tense has played a moderately useful part in machine learning approaches to the task (Hepple et al., 2007), its exact role in automatic temporal annotation is not fully understood. Further, though it was not the case when the framework was originally proposed, there now exist resources annotated with some temporal semantics, usi"
W13-0107,D10-1089,0,0.0285059,". Further, reference point management is critical to medical summary generation. In order to helpfully unravel the meanings of tense shifts in minute-by-minute patient reports, Portet et al. (2009) required understanding of the reference point. The framework also helps interpret linguistic expressions of time (timexes). Reference time is required to interpret anaphoric expressions such as “last April”. Creation of recent timex corpora prompted the comment that there is a “need to develop sophisticated methods for temporal focus tracking if we are to extend current time-stamping technologies” (Mazur and Dale, 2010) – focus as a rˆole filled by Reichenbach’s reference point. In fact, demand for accurate reference time management is so persistent that state of the art systems for converting times expressed in natural language to machine-readable format now contain extra layers solely for handling reference time (Llorens et al., 2012). Given the difficulty of automatically determining the orderings, or temporal relations, between events, and the suggested ability of Reichenbach’s framework to provide information for this, it is natural to apply this framework to the temporal ordering task. Although tense h"
W13-0107,P87-1021,0,0.656234,", S2 , ..., Sn to be interpreted as a narrative discourse, the reference time of each sentence Si (for i such that 1 &lt; i − n) is interpreted to be: (a) a time consistent with the definite time adverbials in Si , if there are any; (b) otherwise, a time which immediately follows the reference time of the previous sentence Si−1 . The TDIP accounts for a set of sentences which share a reference and speech point. However, as with other definitions of temporal context, this principle involves components that are difficult to automatically determine (e.g. “consistent with definite time adverbials”). Webber (1987) introduces a listener model, incorporating R as a means of determining temporal focus. Her focus resumption and embedded discourse heuristics capture the nesting behaviour of temporal contexts. Further, Eijck and Kamp (2010) describe context-bounding, tense-based rules for applicability of Reichenbach’s framework. These comprise a qualitative model of temporal context. As described in Chapter 4 of Hornstein (1990), permanence of the reference point does not apply between main verb events and those in embedded phrases, relative clauses or quoted speech. These latter events occur within a separ"
W14-4408,P98-1013,0,0.0722621,"Missing"
W14-4408,W09-0613,0,0.0108735,"other, a generate-andrank approach may be best (allowing each component to express alternative ‘good’ choices and choosing the best combination of these choices at the end). Our text planner is responsible for analyzing the input text reviews, extracting perattribute rating distributions and other meta-data from each review, and synthesizing this information to produce one or more discourse plans. Our sentence planner, J SPA RK Y – a freely-available toolkit (Stent and Molina, 2009) – can produce several candidate sentence plans and their corresponding surface realizations through SimpleNLG (Gatt and Reiter, 2009). The candidate summaries are ranked by calculating their perplexity with a language model trained over a large number of sentences from additional restaurant reviews collected over the Web. Hybrid summarization Most NLG research has converged around a “consensus architecture” (Reiter, 1994; Rambow and Korelsky, 1992), a pipeline architecture including the following modules: 1) text planning, which determines how the presentation content is selected, structured, and ordered; 2) sentence planning, which assigns content to sentences, inserts discourse cues to communicate the structure of the pre"
W14-4408,W00-0405,0,0.0182323,"ctive summarizer: using extractive summarization techniques, it selects salient quotes from the input reviews and embeds them into an automatically generated abstractive summary to provide evidence for, exemplify or justify positive or negative opinions. We demonstrate that, compared to extractive methods, summaries generated with abstractive and hybrid summarization approaches are more readable and compact. 1 Introduction Text summarization is a well-established area of research. Many approaches are extractive, that is, they select and stitch together pieces of text from the input documents (Goldstein et al., 2000; Radev et al., 2004). Other approaches are abstractive; they use natural language generation (NLG) techniques to paraphrase and condense the content of the input documents (Radev and McKeown, 1998). Most summarization methods focus on distilling factual information by identifying the input documents’ main topics, removing redundancies, and coherently ordering extracted phrases or sentences. Summarization of sentiment-laden text (e.g., product or service reviews) is substantially different from the traditional text summarization task: instead of presenting facts, the summarizer must present th"
W14-4408,W04-1013,0,0.0164874,"the quote selection module; and 4) the hybrid summarizer S TARLET-H. We used the Amazon Mechanical Turk3 crowdEvaluating an abstractive review summarizer involves measuring how accurately the opinion content present in the reviews is reflected in the summary and how understandable the generated content is to the reader. Traditional multi-document summarization evaluation techniques utilize both qualitative and quantitative metrics. The former require human subjects to rate different evaluative characteristics on a Likert-like scale, while the latter relies on automatic metrics such as ROUGE (Lin, 2004), which is based on the common number of n-grams between a peer, and one or several gold-standard reference summaries. 3 59 http://www.mturk.com restaurant review documents. sourcing system to post subjective evaluation tasks, or HITs, for 20 restaurant summaries. Each HIT consists of a set of ten randomly ordered reviews for one restaurant, and four randomly ordered summaries of reviews for that restaurant, each one accompanied by a set of evaluation widgets for the different evaluation metrics described below. To minimize reading order bias, both reviews and summaries were shuffled each time"
W14-4408,radev-etal-2004-mead,0,0.0204223,"Missing"
W14-4408,C10-1039,0,0.265289,"n of S TARLET-H. Another potential area of future research concerns the ability to personalize summaries to the user’s needs. For instance, the text planner can adapt its communicative goals based on polarity orientation – a user can be more interested in exploring in detail negative reviews – or it can focus more on specific (user-tailored) aspects and change the order of the presentation accordingly. Finally, it could be interesting to customize the summarizer to provide an overview of what is available in a specific geographic neighborhood and compare and contrast the options. Related work Ganesan et al. (2010) propose a method to extract salient sentence fragments that are both highly frequent and syntactically well-formed by using a graph-based data structure to eliminate redundancies. However, this approach assumes that the input sentences are already selected in terms of aspect and with highly redundant opinion content. Also, the generated summaries are very short and cannot be compared to a full-length output of a typical multi-document summarizer (e.g., 100-200 words). A similar approach is described in Ganesan et al. (2012), where very short phrases (from two to five words) are collated toget"
W14-4408,J98-3005,0,0.038586,"e for, exemplify or justify positive or negative opinions. We demonstrate that, compared to extractive methods, summaries generated with abstractive and hybrid summarization approaches are more readable and compact. 1 Introduction Text summarization is a well-established area of research. Many approaches are extractive, that is, they select and stitch together pieces of text from the input documents (Goldstein et al., 2000; Radev et al., 2004). Other approaches are abstractive; they use natural language generation (NLG) techniques to paraphrase and condense the content of the input documents (Radev and McKeown, 1998). Most summarization methods focus on distilling factual information by identifying the input documents’ main topics, removing redundancies, and coherently ordering extracted phrases or sentences. Summarization of sentiment-laden text (e.g., product or service reviews) is substantially different from the traditional text summarization task: instead of presenting facts, the summarizer must present the range of opinions and the consensus opinion (if any), and instead of focusing on one topic, the summarizer must present information about multiple aspects of the target entity. ∗ Robert Gaizauskas"
W14-4408,W94-0319,0,0.171353,"eta-data from each review, and synthesizing this information to produce one or more discourse plans. Our sentence planner, J SPA RK Y – a freely-available toolkit (Stent and Molina, 2009) – can produce several candidate sentence plans and their corresponding surface realizations through SimpleNLG (Gatt and Reiter, 2009). The candidate summaries are ranked by calculating their perplexity with a language model trained over a large number of sentences from additional restaurant reviews collected over the Web. Hybrid summarization Most NLG research has converged around a “consensus architecture” (Reiter, 1994; Rambow and Korelsky, 1992), a pipeline architecture including the following modules: 1) text planning, which determines how the presentation content is selected, structured, and ordered; 2) sentence planning, which assigns content to sentences, inserts discourse cues to communicate the structure of the presentation, and performs sentence aggregation and optionally referring expression generation; and 3) surface realization, which performs lexical selection, resolves syntactic issues such as subject-verb and noun-determiner agreement, and assigns morphological inflection to produce the final"
W14-4408,W09-3941,1,0.816615,"the decisions to be made at each stage of the NLG process just outlined are complex, and because they are not truly independent of each other, a generate-andrank approach may be best (allowing each component to express alternative ‘good’ choices and choosing the best combination of these choices at the end). Our text planner is responsible for analyzing the input text reviews, extracting perattribute rating distributions and other meta-data from each review, and synthesizing this information to produce one or more discourse plans. Our sentence planner, J SPA RK Y – a freely-available toolkit (Stent and Molina, 2009) – can produce several candidate sentence plans and their corresponding surface realizations through SimpleNLG (Gatt and Reiter, 2009). The candidate summaries are ranked by calculating their perplexity with a language model trained over a large number of sentences from additional restaurant reviews collected over the Web. Hybrid summarization Most NLG research has converged around a “consensus architecture” (Reiter, 1994; Rambow and Korelsky, 1992), a pipeline architecture including the following modules: 1) text planning, which determines how the presentation content is selected, structured,"
W14-4408,A92-1006,0,\N,Missing
W14-4408,C98-1013,0,\N,Missing
W14-4802,aker-etal-2012-light,1,0.843294,"contain translations for terms the user seeks. 3 For example Bess´e et al. (1997) define term as “a lexical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to i"
W14-4802,P13-1040,1,0.835823,"fine term as “a lexical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Various classification systems were considered,"
W14-4802,aker-etal-2014-bootstrapping,1,0.841188,"ical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Various classification systems were considered, including the Dewe"
W14-4802,aker-etal-2014-bilingual,1,0.795415,"ical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Various classification systems were considered, including the Dewe"
W14-4802,P13-1052,0,0.0626131,"Missing"
W14-4802,drouin-2004-detection,0,0.0434918,"n the information used to extract terms: approaches using purely linguistic information, approaches using purely statistical information and those using combinations of both. An analysis of different approaches is given by Pazienza et al. (2005). For the most part, however, such approaches make the assumption that domain-specific, and perhaps also non-domain-specific, collections of texts are available. Justeson and Katz (1995), for example, assume that term frequency of a limited sort of noun phrases in domainspecific texts is sufficicent to indicate termhood. Others such as Chung (2003) and Drouin (2004) look at statistical contrasts between domain-specific and general comparison or reference corpus. See also 18 (Kim et al., 2009; Marciniak and Mykowiecka, 2013; Kilgariff, 2014). By contrast our approach does not presuppose the existence of documents pre-classified by domain (though we could benefit from this). Rather our approach starts by classifying a document into a domain and then extracting terms from it and assigning them the domain of the document. Utsuro et al. (2006) and Kida et al. (2007) extract terms from web-documents. The domain specification of a term is determined in two stag"
W14-4802,E14-2014,0,0.0204856,"An analysis of different approaches is given by Pazienza et al. (2005). For the most part, however, such approaches make the assumption that domain-specific, and perhaps also non-domain-specific, collections of texts are available. Justeson and Katz (1995), for example, assume that term frequency of a limited sort of noun phrases in domainspecific texts is sufficicent to indicate termhood. Others such as Chung (2003) and Drouin (2004) look at statistical contrasts between domain-specific and general comparison or reference corpus. See also 18 (Kim et al., 2009; Marciniak and Mykowiecka, 2013; Kilgariff, 2014). By contrast our approach does not presuppose the existence of documents pre-classified by domain (though we could benefit from this). Rather our approach starts by classifying a document into a domain and then extracting terms from it and assigning them the domain of the document. Utsuro et al. (2006) and Kida et al. (2007) extract terms from web-documents. The domain specification of a term is determined in two stage approach. In the first stage for a term under inspection web-documents which mention the term are collected. Then these documents are divided into two sets: domain relevant and"
W14-4802,skadina-etal-2012-collecting,1,0.870619,"Missing"
W14-4802,W12-0102,0,0.0205244,"seeks. 3 For example Bess´e et al. (1997) define term as “a lexical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Vario"
W14-5406,P08-1032,0,0.0244764,"atasets annotated with a fixed set of labels as training data. For example, Duygulu et al. (2002) investigated learning from images annotated with a set of keywords, posing the problem as a machine translation task between image regions and textual labels. Gupta and Davis (2008) includes some semantic information by incorporating prepositions and comparative adjectives, which also requires manual annotation as no such data is readily available. Recent work has moved beyond learning image annotation from constrained text labels to learning from real world texts, for example from news captions (Feng and Lapata, 2008) and sports articles (Socher and Fei-Fei, 2010). There is also recent interest in treating texts as richer sources of information than just simple bags of keywords, for example with the use of semantic hierarchies for object recognition (Marszałek and Schmid, 2008; Deng et al., 2012b) and the inclusion of attributes for a richer representation (Lampert et al., 2009; Farhadi et al., 2009). Another line of recent work uses textual descriptions of images for various vision tasks, for example for recognizing butterfly species from butterfly descriptions (Wang et al., 2009) and discovering attribut"
W14-5406,padro-stanilovsky-2012-freeling,0,0.0297307,"Missing"
W14-5406,D11-1041,0,\N,Missing
W15-2805,N12-1094,0,0.114411,"Missing"
W15-2805,N10-1125,0,0.0309081,"r difficult cases. Lastly, we demonstrate the viability of our definition via an annotation exercise across several text genres and analyse inter-annotator agreement. Results show reasonably high levels of agreement between annotators can be reached. 1 Arnau Ramisa2 Introduction Recent years have seen rapid growth in research integrating visual and textual modalities, including associating named entities in captions with faces in images (Berg et al., 2004), generating image descriptions (Kulkarni et al., 2011; Yang et al., 2011), text/image retrieval (Hodosh et al., 2013), story illustration (Feng and Lapata, 2010), and learning visual recognition of fine-grained object categories (Wang et al., 2009). This previous work concentrates on solving image-based tasks, and is heavily reliant upon datasets with pre-aligned images and texts, most of which have been manually collected and/or annotated. Thus, such imagecentric texts are assumed to be at least partially, if not predominantly, ‘visually descriptive’ in nature. This raises some interesting research questions: (i) how much text out there without associated images is ‘visually descriptive’ and thus potentially 10 Proceedings of the 2015 Workshop on Vis"
W15-2805,D11-1041,0,0.0928263,"Missing"
W15-4722,D13-1128,0,0.470735,"the image description generation task, rather than dealing with the noise arising from visual detection. This task also allows us to evaluate specific phases of the conventional generation pipeline, providing insights into which specific phases of the generation pipeline contribute to the performance of an image description generation system. 2 Motivation and Related Work marking challenge (Villegas et al., 2015; Gilbert et al., 2015). More specifically, we assume that perfectly labelled object instances and their localisations are available to image description generation systems, as done in Elliott and Keller (2013) and Yatskar et al. (2014). Given this knowledge, we would like to evaluate how well image description generation systems perform through the various stages of Natural Language Generation (Reiter and Dale, 2000): content determination (what objects to describe), microplanning (how to describe objects) and realisation (generating the complete sentence). This pilot task is an attempt at encouraging fine-grained evaluation specifically for image descriptions, compared to general-purpose metrics like METEOR (Denkowski and Lavie, 2014) that evaluates text at a global, coarsegrained level. For our p"
W15-4722,P12-1038,0,0.119183,"l to be tuned at surface realisation stages. scription generation by retrieving existing textual descriptions from similar images. A common approach would be to map text and images to a common meaning space (Farhadi et al., 2010; Hodosh et al., 2013; Socher et al., 2014) or by using some similarity measure (Ordonez et al., 2011). Although such methods produce descriptions that are more expressive, they rely on a large amount of training data, and are unable to produce novel sentences. There have been attempts at retrieving only text fragments and combining them to generate novel descriptions (Kuznetsova et al., 2012; Kuznetsova et al., 2014) or by pruning irrelevant fragments for better generalisation (Kuznetsova et al., 2013). However, the resulting descriptions may still be pure ‘guesswork’ and may reference text fragments that are irrelevant to image content. Most recently, work using deep learning approaches has produced state-of-the-art results (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015), by utilising Convolutional Neural Networks (CNN) (Krizhevsky et al., 2012; Razavian et al., 2014) as image features, and a Recurrent Neural Network (RNN) (Mikolov et al., 2010) for lang"
W15-4722,P13-2138,0,0.0178101,"om similar images. A common approach would be to map text and images to a common meaning space (Farhadi et al., 2010; Hodosh et al., 2013; Socher et al., 2014) or by using some similarity measure (Ordonez et al., 2011). Although such methods produce descriptions that are more expressive, they rely on a large amount of training data, and are unable to produce novel sentences. There have been attempts at retrieving only text fragments and combining them to generate novel descriptions (Kuznetsova et al., 2012; Kuznetsova et al., 2014) or by pruning irrelevant fragments for better generalisation (Kuznetsova et al., 2013). However, the resulting descriptions may still be pure ‘guesswork’ and may reference text fragments that are irrelevant to image content. Most recently, work using deep learning approaches has produced state-of-the-art results (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015), by utilising Convolutional Neural Networks (CNN) (Krizhevsky et al., 2012; Razavian et al., 2014) as image features, and a Recurrent Neural Network (RNN) (Mikolov et al., 2010) for language modelling, and learning to generate descriptions jointly from images and their descriptions. The advantages"
W15-4722,Q14-1028,0,0.0194332,"realisation stages. scription generation by retrieving existing textual descriptions from similar images. A common approach would be to map text and images to a common meaning space (Farhadi et al., 2010; Hodosh et al., 2013; Socher et al., 2014) or by using some similarity measure (Ordonez et al., 2011). Although such methods produce descriptions that are more expressive, they rely on a large amount of training data, and are unable to produce novel sentences. There have been attempts at retrieving only text fragments and combining them to generate novel descriptions (Kuznetsova et al., 2012; Kuznetsova et al., 2014) or by pruning irrelevant fragments for better generalisation (Kuznetsova et al., 2013). However, the resulting descriptions may still be pure ‘guesswork’ and may reference text fragments that are irrelevant to image content. Most recently, work using deep learning approaches has produced state-of-the-art results (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015), by utilising Convolutional Neural Networks (CNN) (Krizhevsky et al., 2012; Razavian et al., 2014) as image features, and a Recurrent Neural Network (RNN) (Mikolov et al., 2010) for language modelling, and learni"
W15-4722,W11-0326,0,0.118363,"ot discuss in great detail how the dataset has been collected and annotated; we instead refer readers to Gilbert et al. (2015) for more details about the challenge. There are currently three main groups of approaches to generating image descriptions. The most common and intuitive paradigm is the knowledge-based, generative approach that takes an image as input, detects instances of pre-defined object categories in the image using a visual recogniser, and then reasons about the detected objects to generate a novel textual description (Yao et al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012). However, these approaches are constrained to a limited number of categories, for example 20 in Kulkarni et al. (2011). We found that these approaches are generally sensitive to errors from visual input detection, as such errors tend to propagate and accumulate through the generation pipeline. The problem is accentuated when scaling up to a larger number of categories (e.g. 1000), where it becomes difficult to reason about what to describe amongst the candidate instances produced by the noisy visual detectors. Thus, generating image descriptions with gold standard visu"
W15-4722,W04-1013,0,0.0315864,"s. Zitnick and Parikh (2013) take a unique approach of generating scenes from clipart as an abstraction to real world images to address the issue of noisy input. Our work takes a similar direction as Elliott and Keller (2013) and Yatskar et al. (2014), but with bounding boxes as gold standard input, and with an emphasis on fine-grained evaluation of image description generation systems. 3.1 Evaluation of image description generation systems. Existing image description generation systems are most commonly evaluated using automatic evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014) and most recently CIDEr (Vedantam et al., 2015). However, such global measures only allow evaluation of image description generation systems as a whole, without being able to ascertain which parts of the generation process, or components of the generation system, are responsible for performance gains or losses. Although evaluations based on human judgments could provide a more finegrained metric (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012), they are expensive and difficult to scale. We propose instead to exploit the pipeline of knowle"
W15-4722,E12-1076,0,0.308346,"at detail how the dataset has been collected and annotated; we instead refer readers to Gilbert et al. (2015) for more details about the challenge. There are currently three main groups of approaches to generating image descriptions. The most common and intuitive paradigm is the knowledge-based, generative approach that takes an image as input, detects instances of pre-defined object categories in the image using a visual recogniser, and then reasons about the detected objects to generate a novel textual description (Yao et al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012). However, these approaches are constrained to a limited number of categories, for example 20 in Kulkarni et al. (2011). We found that these approaches are generally sensitive to errors from visual input detection, as such errors tend to propagate and accumulate through the generation pipeline. The problem is accentuated when scaling up to a larger number of categories (e.g. 1000), where it becomes difficult to reason about what to describe amongst the candidate instances produced by the noisy visual detectors. Thus, generating image descriptions with gold standard visual input allows research"
W15-4722,D11-1041,0,0.159215,"Missing"
W15-4722,P02-1040,0,0.0935424,"ttributes, parts, and activities. Zitnick and Parikh (2013) take a unique approach of generating scenes from clipart as an abstraction to real world images to address the issue of noisy input. Our work takes a similar direction as Elliott and Keller (2013) and Yatskar et al. (2014), but with bounding boxes as gold standard input, and with an emphasis on fine-grained evaluation of image description generation systems. 3.1 Evaluation of image description generation systems. Existing image description generation systems are most commonly evaluated using automatic evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014) and most recently CIDEr (Vedantam et al., 2015). However, such global measures only allow evaluation of image description generation systems as a whole, without being able to ascertain which parts of the generation process, or components of the generation system, are responsible for performance gains or losses. Although evaluations based on human judgments could provide a more finegrained metric (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012), they are expensive and difficult to scale. We propose instead to exploit the"
W15-4722,S14-1015,0,0.276103,"on task, rather than dealing with the noise arising from visual detection. This task also allows us to evaluate specific phases of the conventional generation pipeline, providing insights into which specific phases of the generation pipeline contribute to the performance of an image description generation system. 2 Motivation and Related Work marking challenge (Villegas et al., 2015; Gilbert et al., 2015). More specifically, we assume that perfectly labelled object instances and their localisations are available to image description generation systems, as done in Elliott and Keller (2013) and Yatskar et al. (2014). Given this knowledge, we would like to evaluate how well image description generation systems perform through the various stages of Natural Language Generation (Reiter and Dale, 2000): content determination (what objects to describe), microplanning (how to describe objects) and realisation (generating the complete sentence). This pilot task is an attempt at encouraging fine-grained evaluation specifically for image descriptions, compared to general-purpose metrics like METEOR (Denkowski and Lavie, 2014) that evaluates text at a global, coarsegrained level. For our pilot, we concentrated on j"
W15-4722,W14-3348,0,\N,Missing
W15-4722,Q14-1017,0,\N,Missing
W16-2802,W14-2107,0,0.0554755,"Missing"
W16-2802,W16-3605,1,0.846901,"ion of issue and viewpoint). In reader comment this structured information is missing and the debate is framed solely by a document (the article), with issues, as we define them, rarely explicitly signalled in the article or comments. Thus, the task of structuring the debate by discovering the issues, which our framework addresses, is a challenge of particular importance for reader comment. Comparison with Human Summaries Figure 3 shows a human-authored summary of the first 30 comments on the bin collection article, created as part of a corpus of gold standard reader comment summaries (SENSEI Project, 2016). Annotators created the gold standard summaries using a novel 3 stage method: (1) each comment in the source set is annotated with a label (i.e. a mini-summary of the main points in the comment); (2) related labels are sorted into groups that the annotator believes will be helpful for writing an overview summary and a group label is produced to indicate common content in the group; (3) based on the analysis and annotations created in stages (1) and (2), an overview summary is written, which should identify the main points raised in the discussion, different views, areas of consensus, the prop"
W16-2802,P12-2041,0,0.0739255,"Missing"
W16-2802,W15-0504,0,0.0715187,"y like that shown in Figure 3, Summary 2. In this summary for the extended argument graph underlying Tables 1 and 2, we assume the root issue has been summarised (sentences 1-3) and that one further issue (2) has also been chosen for inclusion using the sort of extension to the algorithm described in the last paragraph (sentences 4-6). 3.3 4 Related Work In recent years various authors have begun work on argument mining in on-line discussion forums (e.g., Cabrio and Vilatta (2012); Boltuˇzi´c and ˇ Snajder (2014); Swanson et al. (2015)) and reader comment on news (e.g., Sobhani et al. (2015); Carstens and Toni (2015); Sardianos et al. (2015)). While sharing some features, such as allowing multiple participants to exchange views, make claims and supply supporting arguments, these two sources of argumentative discourse also exhibit notable differences. For instance, in online discussion forums such as debatepedia.org or convinceme.net, debates are topically organized or tagged with key words, e.g. climate change, and a debate is typically framed by a starting motion or question and an example of a supporting or counter statement (similar to our notion of issue and viewpoint). In reader comment this structur"
W16-2802,W15-0509,0,0.014533,"ed to generate a summary like that shown in Figure 3, Summary 2. In this summary for the extended argument graph underlying Tables 1 and 2, we assume the root issue has been summarised (sentences 1-3) and that one further issue (2) has also been chosen for inclusion using the sort of extension to the algorithm described in the last paragraph (sentences 4-6). 3.3 4 Related Work In recent years various authors have begun work on argument mining in on-line discussion forums (e.g., Cabrio and Vilatta (2012); Boltuˇzi´c and ˇ Snajder (2014); Swanson et al. (2015)) and reader comment on news (e.g., Sobhani et al. (2015); Carstens and Toni (2015); Sardianos et al. (2015)). While sharing some features, such as allowing multiple participants to exchange views, make claims and supply supporting arguments, these two sources of argumentative discourse also exhibit notable differences. For instance, in online discussion forums such as debatepedia.org or convinceme.net, debates are topically organized or tagged with key words, e.g. climate change, and a debate is typically framed by a starting motion or question and an example of a supporting or counter statement (similar to our notion of issue and viewpoint). In re"
W16-2802,W14-2106,0,0.291711,"of these multi-way conversations, since single comments taken from clusters do not reflect the argumentative structure of the conversation. I.e. such summaries do not identify the issues about which commenters are arguing, the alternative viewpoints commenters take on the issues or key evidence supporting one viewpoint or another, which a truly informative summary must do. By contrast, researchers working on argument mining from social media, including reader comment and on-line debate, have articulated various schemes defining argument elements and relations in argumentative discourse (e.g. Ghosh et al. (2014), Habernal et al. (2014), Swanson et al. (2015)). If such elements and relations could be automatically extracted then they could potentially serve as a basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. However, to the best of our knowledge none have proposed how, given a formal analysis of an conversation in response to a news article, one might produce a summary of that conversation. This is a non-trivial issue. In this paper we make two main co"
W16-2802,W15-4631,0,0.421599,"le comments taken from clusters do not reflect the argumentative structure of the conversation. I.e. such summaries do not identify the issues about which commenters are arguing, the alternative viewpoints commenters take on the issues or key evidence supporting one viewpoint or another, which a truly informative summary must do. By contrast, researchers working on argument mining from social media, including reader comment and on-line debate, have articulated various schemes defining argument elements and relations in argumentative discourse (e.g. Ghosh et al. (2014), Habernal et al. (2014), Swanson et al. (2015)). If such elements and relations could be automatically extracted then they could potentially serve as a basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. However, to the best of our knowledge none have proposed how, given a formal analysis of an conversation in response to a news article, one might produce a summary of that conversation. This is a non-trivial issue. In this paper we make two main contributions. First (Section 2) we present a lig"
W16-2802,N15-1046,0,0.125178,"wever, with the exception of Kunz and Rittel (1970) we are not aware of any argumentation model that puts the notion of issue in the form of a question at the centre of the model and organises argument elements and relations around it. Aside from differences in the text type addressed (reader comment rather than on-line debate) and the prominence given to notion of issue in our anaytical framework, our principal difference to other work in argument mining is the task we focus on: summarization. Some authors have cited summarization as a motivating end-user task, e.g. Swanson et al. (2015) and Misra et al. (2015). However, both these works aim at summarising an argument on a single topic like “gun control” across multiple dialogues and do not address the summarization of single, multi-party argumentative conversations that may address multiple issues, such as those found in reader comments. To the best of our knowledge no one has addressed the form that an end-user overview summary of reader comment might take or how it might be generated from the abstract representation of an argument, as we do in this paper. 5 Discussion and Future Work In this paper we have defined notions of issue, viewpoint and a"
W16-2802,W15-0508,0,\N,Missing
W16-3605,W16-6610,1,0.619014,"of comments in themselves, and as a target for what an automated system might deliver online. Misra et al. (2015) have created manual summaries of short dialogue sequences, extracted from different conversations on similar issues on debat49 relating it with human judgements on system output quality. If it cannot be validated, the challenge arises to develop a metric better suited to this evaluation need. Our summary corpus has already proved useful in providing insights for system development, and for training and evaluation. We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016). Acknowledgments The authors would like to thank the European Commission for supporting this work, carried out as part of the FP7 SENSEI project, grant reference: FP7-ICT-610916. We would also like to thank all the annotators without whose work the SENSEI corpus would not have been created, Jonathan Foster for his help in recruit"
W16-3605,W16-2802,1,0.902481,"oint or assertion already expressed. Issues are questions on which multiple viewpoints are possible; e.g., the issue of whether reducing bin collection to once every three weeks is a good idea, or whether reducing bin collection will lead to an increase in vermin. Issues are very often implicit, i.e not directly expressed in the comments (e.g., the issue of whether reducing bin collection will lead to an increase in vermin is never explicitly mentioned yet this is clearly what comments 1-4 are addressing). A fuller account of this issue-based framework for analysing reader comment is given in Barker and Gaizauskas (2016). Aside from argumentative content, reader comments exhibit other features as well. For example, commenters may seek clarification about facts (e.g. comment 4 where the commenter asks Is Bury going to provide larger bins for families . . . ?). But these clarifications are typically carried out in the broader context of making an argument, i.e. advancing evidence to support a viewpoint. Comments may also express jokes or emotion, though these too are often in the service of advancing some viewpoint (e.g. sarcasm or as in comments 4 and 6 emotive terms like lamebrained and crazy clearly indicati"
W16-3605,L16-1494,1,0.816562,"judgements on system output quality. If it cannot be validated, the challenge arises to develop a metric better suited to this evaluation need. Our summary corpus has already proved useful in providing insights for system development, and for training and evaluation. We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016). Acknowledgments The authors would like to thank the European Commission for supporting this work, carried out as part of the FP7 SENSEI project, grant reference: FP7-ICT-610916. We would also like to thank all the annotators without whose work the SENSEI corpus would not have been created, Jonathan Foster for his help in recruiting annotators and The Guardian for allowing us access to their materials. Finally, thanks to our anonymous reviewers whose comments and suggestions have helped us to improve the paper. Even so, there are limitations to the work done which give pointers to further wor"
W16-3605,W14-2106,0,0.0607293,"ctive summary. A significant weakness of such summaries is that they fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended ar"
W16-3605,W15-4631,0,0.029459,"ch summaries is that they fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, su"
W16-3605,W02-0406,0,0.189098,"Missing"
W16-3605,W04-1013,0,0.0550558,"Missing"
W16-3605,N15-1046,0,0.194431,"hey fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, such as those found in"
W16-3605,D08-1081,0,0.0695176,"Missing"
W16-6631,W14-3348,0,0.0109604,"generation image descriptions, for example (Yao et al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015), among others. Most previous work concentrates on solving the problem ‘end-to-end’, that is to generate a description given an image as input. Such systems are also evaluated in an extrinsic manner, that is by comparing output image descriptions to multiply-annotated gold standard descriptions of the same image using global measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), Meteor (Denkowski and Lavie, 2014) or CIDEr (Vedantam et al., 2015). Whilst such evaluation methodologies are useful to evaluate image description generation systems as a whole (how similar is the generated description to human-authored descriptions?), they make it hard to identify which components of the generation process contribute to any performance gains or losses. Wang and Gaizauskas (2015) propose evaluating image description generation systems in a fine-grained manner, i.e. evaluating each component of the image description generation pipeline independently. To demonstrate this, they proposed the task of content select"
W16-6631,N12-1094,0,0.0160355,"in each image. The annotations are then aggregated: important objects are those that are mentioned by many annotators. Most related to our work is Berg et al. (2012), who explore factors (compositional, semantic, and contextual) that can be used to predict what is being described in an image. For prediction, they focus on a binary prediction problem – is this object described? yes or no? – and treat bounding boxes as independent of each other. In our case, we treat other bounding boxes as context, as a frequently occurring object may not be mentioned when co-occurring with some other object. Dodge et al. (2012) tackle an inverse problem: learning to predict segments of Flickr captions (noun phrases) that are ‘visual’, i.e. predicting whether a noun phrase in the caption is depicted in the image. There has also been some work on measuring image memorability (what makes an image memorable to humans?), for example, Isola et al. (2011), among others. However, most work deals with memorability at image-level, rather than object level. Dubey et al. (2015) tackle image memorability at object level, that is, what objects are memorable (worth remembering) to a person in an image. This acts as a precursor to"
W16-6631,W04-1013,0,0.0109245,"ture for the task of generation image descriptions, for example (Yao et al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015), among others. Most previous work concentrates on solving the problem ‘end-to-end’, that is to generate a description given an image as input. Such systems are also evaluated in an extrinsic manner, that is by comparing output image descriptions to multiply-annotated gold standard descriptions of the same image using global measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), Meteor (Denkowski and Lavie, 2014) or CIDEr (Vedantam et al., 2015). Whilst such evaluation methodologies are useful to evaluate image description generation systems as a whole (how similar is the generated description to human-authored descriptions?), they make it hard to identify which components of the generation process contribute to any performance gains or losses. Wang and Gaizauskas (2015) propose evaluating image description generation systems in a fine-grained manner, i.e. evaluating each component of the image description generation pipeline independently. To demonstrate this, they"
W16-6631,E12-1076,0,0.0306695,"utomatic stopping criteria to select important objects to be described from the ranking list. Experimental results are presented in Section 4, with regards to concatenating all features (Section 4.2) as well as treating individual features independently (Section 4.3). We also provide a summary of our feature ablation study in Section 4.4, and present conclusions in Section 5. 2 Related work Image description generation. Various approaches have been proposed in the literature for the task of generation image descriptions, for example (Yao et al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015), among others. Most previous work concentrates on solving the problem ‘end-to-end’, that is to generate a description given an image as input. Such systems are also evaluated in an extrinsic manner, that is by comparing output image descriptions to multiply-annotated gold standard descriptions of the same image using global measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), Meteor (Denkowski and Lavie, 2014) or CIDEr (Vedantam et al., 2015). Whilst such evaluation methodologies are useful to evaluate image"
W16-6631,N15-1174,0,0.0153764,"rases) that are ‘visual’, i.e. predicting whether a noun phrase in the caption is depicted in the image. There has also been some work on measuring image memorability (what makes an image memorable to humans?), for example, Isola et al. (2011), among others. However, most work deals with memorability at image-level, rather than object level. Dubey et al. (2015) tackle image memorability at object level, that is, what objects are memorable (worth remembering) to a person in an image. This acts as a precursor to the content selection problem of choosing what to describe in an image description. Ortiz et al. (2015) treat image description generation as a Statistical Machine Translation (SMT) task, and concentrate on describing abstract, clipart scenes. Part of their pipeline involves a content selection module where rankings of object pairs are optimised as an integer linear programming (ILP) problem, allowing object pairs that frequently co-occur and are close to each other to be ranked higher than those that are not. Our approach is not constrained to pairwise features, and automatically learns to optimise rankings across all instances directly from a training set, using arbitrary feature vectors. Dir"
W16-6631,P02-1040,0,0.103226,"ave been proposed in the literature for the task of generation image descriptions, for example (Yao et al., 2010; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015), among others. Most previous work concentrates on solving the problem ‘end-to-end’, that is to generate a description given an image as input. Such systems are also evaluated in an extrinsic manner, that is by comparing output image descriptions to multiply-annotated gold standard descriptions of the same image using global measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), Meteor (Denkowski and Lavie, 2014) or CIDEr (Vedantam et al., 2015). Whilst such evaluation methodologies are useful to evaluate image description generation systems as a whole (how similar is the generated description to human-authored descriptions?), they make it hard to identify which components of the generation process contribute to any performance gains or losses. Wang and Gaizauskas (2015) propose evaluating image description generation systems in a fine-grained manner, i.e. evaluating each component of the image description generation pipeline independently. To dem"
W16-6631,W15-4722,1,0.216444,"st detect instances of pre-defined concepts in the image to be described, and then to reason about the detected concepts to generate image descriptions. Thus, such approaches may involve various components of a standard Natural Language Generation pipeline (Reiter and Dale, 2000), such as document planning (including content determination), microplanning (lexicalisation/referring expression generation) and realisation. In this paper, we concentrate on a specific subproblem in such an image description generation pipeline. More specifically, we explore the content selection problem proposed by Wang and Gaizauskas (2015). In this setting, object instances are assumed to have already been localised in an image. Thus, given gold standard labelled bounding boxes of object instances in an image, the task is to select the appropriate bounding box instances to be mentioned in the eventual image description that is to be generated (see Figure 1 for an example). To our knowledge, there has been minimal work specifically tackling the content selection problem. However, the task is important to image description generation as not all entities depicted in an image will be mentioned by humans. For example, a fork lying o"
W16-6631,P15-1173,0,0.0486741,"Missing"
W16-6631,D11-1041,0,0.0800326,"Missing"
W97-1311,M95-1017,0,0.0462864,"or further details of the task descriptions). In addition, the system can generate a brief natural language summary of any scenario it has detected in a text. All these tasks are carried out by building a single rich discourse model of the text from which the various results are read off. The system is a pipelined architecture which processes a text one sentence at a time and consists of three principal processing stages: lexical preprocessing, parsing plus semantic interpretation, and discourse interpretation. The overall contributions of these stages may be briefly described as follows (see Gaizauskas et al. (1995) for further details): Both of these pairs of sentences refer to a single management succession event (though the second sentence in 2 also identifies a further one). Such event/sub-event relations are similar to the familiar part-whole or related-object anaphora exemplified in sentences such as The airplane crashed a~ter the wings/ell off or When John entered the kitchen the stove was on (Allen, 1987). The second thing to note is the variety of surface forms used to refer to events. Events are referred to by verb phrases in main clauses (1 above), and in relative clauses (second sentence in 2"
W97-1311,C96-1079,0,0.0104885,"But coreference involving events, expressed via verbs or nominalised verb forms, is also common, and can play an important role in practical applications of natural language processing (NLP) systems. One application area of increasing interest is information extraction (IE) (see, e.g., Cowie and Lehnert (1996)). Information extraction systems attempt to fill predefined template structures with information extracted from short natural language texts, such as newswire articles. The prototypical IE tasks are those specified in the Message Understanding Conference (MUC) evaluations (DARPA, 1995; Grishman and Sundheim, 1996). In these exercises the main template filling task centres around a &apos;scenario&apos; which is defined in terms of a key event type and various roles pertaining to it. Examples (1) Mr. Jones succeeds M. James Bird, 50, as president off Wholistic Therapy. more frequently multiple aspects or sub-events of a single succession event are identified in separate clauses by separate verb phrases or nominalised forms: (2) Daniel Wood was named president and chief executive officer off E F C Records Group, a unit off London&apos;s Spear E F C PLC. He succeeds Charles Paulson, who was recently made chairman and chi"
W97-1311,P91-1003,0,0.022139,"for constructing complex event representations, such as those required for information extraction tasks. Within information extraction the problem has typically been addressed by attempting to merge, or unify, extracted templates (e.g. Sown (1984) or Appelt et al. (1995)), but a more generally useful 4Of course in some events, roles may be filled by other events, but this complication does not affect the basic point that object coreference is primary and event coreference dependent upon it. 80 mechanism will operate within a more general representation. Our approach can be compared to that of Whittemore and Macpherson (1991) who discuss incremental building of event representations within a modified form of DRT (Kamp, 1981). However, the representation used here is preferred because it allows a tighter coupling between world or conceptual modelling and discourse modelling. The representation and the coreference mechanism are fully implemented within the LaSIE information extraction system and are currently being extended to make use of a richer model of event times, the importance of which is demonstrated in Crowe (1996). The mechanism described here is used in the LaSIE system for both object and event coreferen"
W97-1311,P95-1042,0,\N,Missing
W97-1311,M95-1006,0,\N,Missing
W97-1311,M95-1011,0,\N,Missing
W99-0211,P98-1011,1,0.823206,"ssing, parsing plus semantic interpretation, and discourse interpretation. The overall contributions of these stages may be briefly described as follows (see (Gaizauskas et al., 1995) for further details): such as pronouns, which can act to rule out a proposed merge. These rules can refer to various lexical, syntactic, semantic, and positional information about instances, and have mainly been developed through the analysis of training data. A recent addition, however, has been the integration of a more theoretically motiv-. ated focus-based algorithm for the resolution of pronominal anaphora (Azzam et al., 1998). This includes the maintenance of a set of focus registers within the discourse interpreter, to m o d e l changes of focus through a text and provide additional information for the selection of antecedents. The discourse interpreter maintains an explicit representation of coreference chains created as a result of instances being merged in the discourse model. Each instance has an associated attribute recording its position in the original text in terms of character positions. When instances are merged, the result is a single instance with multiple positions which, taken together, represent a"
W99-0211,P98-1012,0,0.0604925,"t there may be more t h a n one entity of central concern, or events or relations between entities may be the principal topic of the text. However, it is at the very least an interesting experiment to see to what extent a principal coreference chain can be used to generate a summary. Further, this approach, which we have implemented and preliminarily evaluated, could easily be extended to allow summaries to be generated from (parts of) the best n coreference chains, or from event, as well as object , coreference chains. The use of document extracts formed from coreference chains is not novel. Bagga and Baldwin (1998) describe a technique for crossdocument coreference which involves extracting the set of all sentences containing expressions in a coreference chain for a specific entity (e.g. John Smith) from each of several documents. T h e y then employ a thresholded vector space similarity measure between these document extracts to decide whether the documents are discussing the same entity (i.e. the same John Smith). Baldwin and Morton (1998) describe a query-sensitive (i.e. user-focused) summarization technique that involves extracting sentences from a document which contain phrases that corefer with ex"
W99-0211,W98-1501,0,0.122446,"s of) the best n coreference chains, or from event, as well as object , coreference chains. The use of document extracts formed from coreference chains is not novel. Bagga and Baldwin (1998) describe a technique for crossdocument coreference which involves extracting the set of all sentences containing expressions in a coreference chain for a specific entity (e.g. John Smith) from each of several documents. T h e y then employ a thresholded vector space similarity measure between these document extracts to decide whether the documents are discussing the same entity (i.e. the same John Smith). Baldwin and Morton (1998) describe a query-sensitive (i.e. user-focused) summarization technique that involves extracting sentences from a document which contain phrases that corefer with expressions in the query. The resulting extract is used to support relevancy judgments with respect to the query. The use of chains of related expressions in documents to select sentences for inclusion in a generic (i.e. non-user-focused) s u m m a r y is also not novel. Barzilay and Elhadad (1997) describe a technique for text summarization based on lexical chains. Their technique, which builds on work of Morris and Hirst (1994), an"
W99-0211,W97-0703,0,0.0950973,"milarity measure between these document extracts to decide whether the documents are discussing the same entity (i.e. the same John Smith). Baldwin and Morton (1998) describe a query-sensitive (i.e. user-focused) summarization technique that involves extracting sentences from a document which contain phrases that corefer with expressions in the query. The resulting extract is used to support relevancy judgments with respect to the query. The use of chains of related expressions in documents to select sentences for inclusion in a generic (i.e. non-user-focused) s u m m a r y is also not novel. Barzilay and Elhadad (1997) describe a technique for text summarization based on lexical chains. Their technique, which builds on work of Morris and Hirst (1994), and ultimately Halliday and Hasan (1976) who stressed the role of lexical cohesion in text coherence, is to form chains of lexical items across a text based on the items' semantic relatedness as inWe describe the use of coreference chains for the production of text summaries, using a variety of criteria to select a 'best' chain to represent the main topic of a text. The approach has been implemented within an existing MUC coreference system, which constructs a"
W99-0211,M95-1017,0,0.00869215,"together - they are literally ' a b o u t ' the same thing; lexical semantic relatedness, as indicated by an external resource, can never conclusively establish this degree of relatedness, nor indeed can the resource guarantee that semantic relatedness will be found when it exists. Further, lexical cohesion techniques ignore pronomial anaphora, and hence their frequency counts of key terms, used b o t h for identifying best chains and best sentences within best chains, may often be inaccurate, as focal referents will often be pronominalised. 2 Coreference in the LaSIE system The LaSIE system (Gaizauskas et al., 1995) has been designed as a general purpose IE system which can conform to the M U C task specifications for named entity identification, coreference resolution, IE template element and relation identification, and the construction of scenario-specific IE templates. T h e system has a pipeline architecture which processes a text one sentence at a time and consists of three prinOf course there are drawbacks to a coreference-based approach. Lexical cohesion relations are relatively easy to c o m p u t e and do not rely on full text processing - this makes summarisation techniques based on them rapid"
W99-0211,J91-1002,0,\N,Missing
W99-0211,C98-1012,0,\N,Missing
W99-0211,C98-1011,1,\N,Missing
X96-1059,M93-1014,0,0.03054,"~ Takahiro Wakaot Hiroshi Yamada$ Robert Gaizauskast YorickWilkst U n i v e r s i t y of Sheffield C o m p u t e r Science D e p a r t m e n t t N E C C o r p o r a t i o n , I n f o r m a t i o n T e c h n o l o g y R e s e a r c h Laboratories.~ { takemoto, wakao, h-yamada} @hum. cl.nec, co.jp { R. Gaizauskas, Y. Wilks} @dcs.shef. ac. uk 1 Introduction Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]). It has also been studied in the framework of Japanese information extraction ([3]) in recent years. Our approach to the Multi-lingual Evaluation Task (MET) for Japanese text is to consider the given task as a morphological analysis problem in Japanese. Our morphological analyzer has done all the necessary work for the recognition and classification of proper names, numerical and temporal expressions, i.e. Named Entity (NE) items in the Japanese text. The analyzer is called &quot;Amorph&quot;. Amorph recognizes NE items in two stages: dictionary lookup and rule application. First, it uses several kinds of dictionaries to segment and tag Japanese character strings. Second, based on th"
X96-1059,M95-1017,0,\N,Missing
X96-1059,M93-1010,0,\N,Missing
