2020.acl-main.648,D19-1572,0,0.0177444,"Dataset We use Chinese Wikipedia articles for pretraining the language model. Script conversion is an issue in Chinese Wikipedia, and currently, they use a server-side mechanism to automatically convert the scripts (dictionary-based) based on the location of the user. However, Wikipedia provides an option to view the article without conversion, which For classification baselines, we use characterbased SVM (Support Vector Machines, Joachims (1998)), CNN (Convolutional Nets, Zhang et al. (2015)) and Chinese BERT (Devlin et al., 2019). We also employ a state-of-the-art text classifier, MultiFiT (Eisenschlos et al., 2019), a lightweight RNN-based language model based classifier, which has shown to achieve a performance competitive with BERT (Devlin et al., 2019) and ULMFiT (Howard and Ruder, 2018). The base architecture of MultiFiT is a 4-layer QRNN (Bradbury et al., 2016) with classifier head. We choose rectified Adam (Liu et al., 2019) with Lookahead (Zhang et al., 2019) as the optimizer. We employ the cosine cyclic learning scheduler (Smith, 2015), where the limits of learning rate cycles are found by increasing the learning rate logarithmically and computing the evaluation loss for each learning rate (Smit"
2020.acl-main.648,C94-1032,0,0.685818,"mapping sequence. We perform beam search to find the best TC sequence from the mapping sequence. (Alt text: §F.2). Max Pool Linear Figure 1: Language model architecture with subword and subsequence sampling. (Alt text: §F.1). Once the mapping sequence m has been found, all possible TC sequences are found from the set mt , which is the Cartesian product for all ti in m. From (7), we calculate approximate final sequence using beam search. 2.2 Model Architecture Viterbi, a dynamic programming (DP) algorithm, considers phrases (or subsequences) and performs segmentation in a ‘bottom-up’ fashion (Nagata, 1994; Sproat et al., 1996). RNN-based language models are theoretically considered to be ‘∞’gram (Khandelwal et al., 2018), which consitutes a challenge. Consider this sentence, 维 护发展 中 国 家 共 同 利 益. A potential challenge could be to adquately estimate the probability of 共同 利 益. As this sequence occurs infrequently in the beginning of sentences in the corpus, an RNN would under-estimate the probability of this subsequence. Moreover, an RNN would likely lose some useful context and perform worse without it (Kim et al., 2019). So for Viterbi to perform well with an RNN, we train the language model on"
2020.acl-main.648,N19-4009,0,0.0253958,"SA 39.3 58.9 13.3 72.7 3.8 88.5 5.2 83.9 5.2 83.9 10.0 81.1 4.8 87.7 3.9 87.8 2.9 91.9 Overall DED SA 34.2 55.6 14.4 72.6 4.3 85.3 5.3 84.0 5.4 84.4 11.5 82.7 4.5 88.9 3.7 89.3 3.0 92.4 Table 3: Results of the intrinsic evaluation experiments which are reported as a mean across 10 different seeds. We use disambiguation error density (DED, the lower, the better) and sentence accuracy (SA, the higher the better) metrics for evaluation. Bold: best, Underlined: second-best. construct a 20-layer neural convolutional sequence model (Gehring et al., 2017) (both in encoder and decoder) using fairseq (Ott et al., 2019). We perform ablation tests by inserting following segmentation models. Word tokenization: We use Jieba, which is a commonly used hidden markov model based word tokenizer for Chinese NLP. 9 Dictionary substrings: We apply maximal string matching, which is a dictionary based greedy tokenizer (Pranav A et al., 2019; Wong and Chan, 1996). Unigram from Sentencepiece: Subword segmentation is performed by sampling unigram language model perplexity values (Kudo, 2018). Joint subwords: As discussed in §2.3, we use joint SC-TC subwords. 3.4 Results for Intrinsic Evaluation We evaluate our models using"
2020.acl-main.648,W19-4302,0,0.0510763,"Missing"
2020.acl-main.648,P16-1162,0,0.0563135,"benchmark datasets by constructing datasets for script conversion and TC topic classification. 2 2kenize: Joint Segmentation and Conversion We employ subword tokenization, as it addresses the issue of rare and unknown words (Mikolov et al., 2012) and has been shown advantageous for the language modelling of morphologicallyrich languages (Czapla et al., 2018; Mielke and Eisner, 2019). This achieves improvements in accuracy for neural machine translation (NMT) tasks and has now become a prevailing practice (Denkowski and Neubig, 2017). The most widelyutilized method is Byte Pair Encoding (BPE, Sennrich et al. (2016)), a compression algorithm that combines frequent sequences of characters, which results in rare strings being segmented into subwords. Unigram (Kudo, 2018) and BPE-Drop (Provilkov et al., 2019) use subword ambiguity as noise, as well as stochastically-corrupted BPE segmentation to make it less deterministic. For NMT tasks generally, subword segmentation is seen as a monolingual task and applied independently on source and target corpora. We hypothesize that translation tasks, and specifically conversion tasks, as investigated here, would have a bet3 The ChineseNLP website states that script c"
2020.acl-main.648,J96-3004,0,0.823579,"nce. We perform beam search to find the best TC sequence from the mapping sequence. (Alt text: §F.2). Max Pool Linear Figure 1: Language model architecture with subword and subsequence sampling. (Alt text: §F.1). Once the mapping sequence m has been found, all possible TC sequences are found from the set mt , which is the Cartesian product for all ti in m. From (7), we calculate approximate final sequence using beam search. 2.2 Model Architecture Viterbi, a dynamic programming (DP) algorithm, considers phrases (or subsequences) and performs segmentation in a ‘bottom-up’ fashion (Nagata, 1994; Sproat et al., 1996). RNN-based language models are theoretically considered to be ‘∞’gram (Khandelwal et al., 2018), which consitutes a challenge. Consider this sentence, 维 护发展 中 国 家 共 同 利 益. A potential challenge could be to adquately estimate the probability of 共同 利 益. As this sequence occurs infrequently in the beginning of sentences in the corpus, an RNN would under-estimate the probability of this subsequence. Moreover, an RNN would likely lose some useful context and perform worse without it (Kim et al., 2019). So for Viterbi to perform well with an RNN, we train the language model on subsequences. We appr"
2020.acl-main.648,C96-1035,0,0.713858,"s are constructed from existing resources (Denisowski, 2019; Chu et al., 2012). We heuristically convert selected TC sentences to SC using OpenCC. We asked the annotators to manually correct any incorrect conversions.4 4 3.2 Language Model Training We choose the SIGHAN-2005 Bakeoff dataset to train the segmentation-based language model (Emerson, 2005). For SC, we select the PKU and MSR partitions, and for TC, we use the Academia Sinica and CityU partitions. We apply maximal matching (or heuristic dictionary-based word segmenter) to pre-process these datasets by segmenting words into subwords (Wong and Chan, 1996). Here, ‘dictionary’ refers to the word-list in the mapping table. We then train a 2-layer LSTM language model LSTM with tied weights, and embedding and hidden sizes of 512 (Sundermeyer et al., 2012) on this segmented dataset with subsequence sampling and stochastic tokenization as discussed in §2.2. 3.3 Baselines and Ablations5 We implement the following baselines for the experimentation: Off-the-shelf Converters: Hanziconv6 and Mafan7 are dictionary-based script character converters. Evaluating this could be useful to understand the lower accuracy bound. OpenCC8 uses a hybrid of characters a"
2020.acl-main.648,I17-3016,0,0.028861,"ic tokenization as discussed in §2.2. 3.3 Baselines and Ablations5 We implement the following baselines for the experimentation: Off-the-shelf Converters: Hanziconv6 and Mafan7 are dictionary-based script character converters. Evaluating this could be useful to understand the lower accuracy bound. OpenCC8 uses a hybrid of characters and words (specifically trie based tokenizer) for script conversion (Pranav A et al., 2019). Language Model Disambiguation: A strong baseline to this problem would be to build a language model to disambiguate between the characters, which is quite similar to STCP (Xu et al., 2017). We use a 2-layer LSTM language model trained on Traditional Chinese corpus. Neural Sequence Models: We heuristically convert Traditional Chinese Wikipedia to Simplified Chinese using OpenCC and use it for training the seq2seq model (Sutskever et al., 2014). We A detailed data statement is given in the appendix. 7260 5 If in case you are looking for ‘Related Work’ section. https://github.com/berniey/hanziconv 7 https://github.com/hermanschaaf/mafan 8 https://github.com/BYVoid/OpenCC 6 Conversion System Dictionary based conversion, Hanziconv Dictionary based conversion, Mafan Trie dictionary b"
2020.acl-main.648,chu-etal-2012-chinese,0,\N,Missing
2020.acl-main.648,I05-3017,0,\N,Missing
2020.acl-main.648,P17-1137,0,\N,Missing
2020.acl-main.648,N18-2085,0,\N,Missing
2020.acl-main.648,Q18-1041,0,\N,Missing
2020.acl-main.648,N19-1114,0,\N,Missing
2020.acl-main.648,P19-1267,0,\N,Missing
2020.acl-main.648,P19-1158,0,\N,Missing
2020.acl-main.648,N19-1423,0,\N,Missing
2020.acl-main.656,W18-5513,0,0.51261,"r at explaining the correct veracity label than explanations learned solely to mimic human justifications. 2 Dataset Existing fact checking websites publish claim veracity verdicts along with ruling comments to support the verdicts. Most ruling comments span over long pages and contain redundancies, making them hard to follow. Textual explanations, by contrast, are succinct and provide the main arguments behind the decision. PolitiFact 1 provides a summary of a claim’s ruling comments that summarises the whole explanation in just a few sentences. We use the PolitiFact-based dataset LIAR-PLUS (Alhindi et al., 2018), which contains 12,836 statements with their veracity justifications. The justifications are automatically extracted from the long ruling comments, as their location is clearly indicated at the end of the ruling comments. Any sentences with words indicating the label, which Alhindi et al. (2018) select to be identical or similar to the label, are removed. We follow the same procedure to also extract the ruling comments without the summary at hand. We remove instances that contain fewer than three sentences in the ruling comments as they indicate short veracity reports, where no summary is pre"
2020.acl-main.656,W19-4813,0,0.0549585,"Missing"
2020.acl-main.656,D19-1475,1,0.820491,"Missing"
2020.acl-main.656,W18-5516,0,0.0411118,"dently of the speaker profile. Once trained, such methods could then be applied to other fact checking instances without human-provided explanations or even to perform end-to-end veracity prediction and veracity explanation generation given a claim. Substantial research on fact checking methods exists for the FEVER dataset (Thorne et al., 2018), which comprises rewritten claims from Wikipedia. Systems typically perform document retrieval, evidence selection, and veracity prediction. Evidence selection is performed using keyword matching (Malon, 2018; Yoneda et al., 2018), supervised learning (Hanselowski et al., 2018; Chakrabarty et al., 2018) or sentence similarity scoring (Ma et al., 2018; Mohtarami et al., 2018; Xu et al., 2018). More recently, the multi-domain dataset MultiFC (Augenstein et al., 2019) has been proposed, which is also distributed with evidence pages. Unlike FEVER, it contains real-world claims, crawled from different fact checking portals. While FEVER and MultiFC are larger datasets for fact checking than LIAR-PLUS, they do not contain veracity explanations and can thus not easily be used to train joint veracity prediction and explanation generation models, hence we did not use them in"
2020.acl-main.656,C18-1131,0,0.0237792,"ng any supporting facts. In our work, we research models that generate humanreadable explanations, and directly optimise the quality of the produced explanations instead of using attention weights as a proxy. We use the LIAR dataset to train such models, which contains fact checked single-sentence claims that already contain professional justifications. As a result, we make an initial step towards automating the generation of professional fact checking justifications. Veracity Prediction. Several studies have built fact checking systems for the LIAR dataset (Wang, 2017). The model proposed by Karimi et al. (2018) reaches 0.39 accuracy by using metadata, ruling comments, and justifications. Alhindi et al. (2018) also trains a classifier, that, based on the statement and the justification, achieves 0.37 accuracy. To the best of our knowledge, Long et al. (2017) is the only system that, without using justifications, achieves a performance above the baseline of Wang (2017), an accuracy of 0.415—the current state-ofthe-art performance on the LIAR dataset. Their model learns a veracity classifier with speaker profiles. While using metadata and external speaker profiles might provide substantial information"
2020.acl-main.656,D19-1387,0,0.113255,"f the claim yF ∈ YF , YF = {true, false, half-true, barely-true, mostly-true, pants-on-fire}. The function g(X) takes the contextual token representations from the last layer of DistilBERT and feeds them to a task-specific feed-forward layer h ∈ Rh . It is followed by the prediction layer with a softmax activation pF ∈ R6 . We use the prediction to optimise a cross-entropy loss function LF = H(pF , yF ). 3.3 3.1 Generating Explanations Our explanation model, shown in Figure 1 (left) is inspired by the recent success of utilising the transformer model architecture for extractive summarisation (Liu and Lapata, 2019). It learns to maximize the similarity of the extracted explanation with the human justification. We start by greedily selecting the top k sentences from each claim’s ruling comments that achieve the highest ROUGE-2 F1 score when compared to the gold justification. We choose k = 4, as that is the average number of sentences in veracity justifications. The selected sentences, referred to as oracles, serve as positive gold labels - yE ∈ {0, 1}N , where N is the total number of sentences present in the ruling comments. Appendix A.1 provides an overview of the coverage that the extracted oracles a"
2020.acl-main.656,I17-2043,0,0.0246252,", which contains fact checked single-sentence claims that already contain professional justifications. As a result, we make an initial step towards automating the generation of professional fact checking justifications. Veracity Prediction. Several studies have built fact checking systems for the LIAR dataset (Wang, 2017). The model proposed by Karimi et al. (2018) reaches 0.39 accuracy by using metadata, ruling comments, and justifications. Alhindi et al. (2018) also trains a classifier, that, based on the statement and the justification, achieves 0.37 accuracy. To the best of our knowledge, Long et al. (2017) is the only system that, without using justifications, achieves a performance above the baseline of Wang (2017), an accuracy of 0.415—the current state-ofthe-art performance on the LIAR dataset. Their model learns a veracity classifier with speaker profiles. While using metadata and external speaker profiles might provide substantial information for fact checking, they also have the potential to introduce biases towards a certain party or a speaker. In this study, we propose a method to generate veracity explanations that would explain the reasons behind a certain veracity label independently"
2020.acl-main.656,W18-5521,0,0.0233581,"ile. Once trained, such methods could then be applied to other fact checking instances without human-provided explanations or even to perform end-to-end veracity prediction and veracity explanation generation given a claim. Substantial research on fact checking methods exists for the FEVER dataset (Thorne et al., 2018), which comprises rewritten claims from Wikipedia. Systems typically perform document retrieval, evidence selection, and veracity prediction. Evidence selection is performed using keyword matching (Malon, 2018; Yoneda et al., 2018), supervised learning (Hanselowski et al., 2018; Chakrabarty et al., 2018) or sentence similarity scoring (Ma et al., 2018; Mohtarami et al., 2018; Xu et al., 2018). More recently, the multi-domain dataset MultiFC (Augenstein et al., 2019) has been proposed, which is also distributed with evidence pages. Unlike FEVER, it contains real-world claims, crawled from different fact checking portals. While FEVER and MultiFC are larger datasets for fact checking than LIAR-PLUS, they do not contain veracity explanations and can thus not easily be used to train joint veracity prediction and explanation generation models, hence we did not use them in this study. 7 Conclusions"
2020.acl-main.656,N19-1423,0,0.0171895,"where no summary is present. The final dataset consists of 10,146 training, 1,278 validation, and 1,255 test data points. A claim’s ruling comments in the dataset span over 39 sentences or 904 words on average, while the justification fits in four sentences or 89 words on average. 7353 1 https://www.politifact.com/ 3 Method 3.2 We now describe the models we employ for training separately (1) an explanation extraction and (2) veracity prediction, as well as (3) the joint model trained to optimise both. The models are based on DistilBERT (Sanh et al., 2019), which is a reduced version of BERT (Devlin et al., 2019) performing on par with it as reported by the authors. For each of the models described below, we take the version of DistilBERT that is pre-trained with a language-modelling objective and further fine-tune its embeddings for the specific task at hand. For the veracity prediction model, shown in Figure 1 (right), we learn a function g(X) = pF that, based on the input X, predicts the veracity of the claim yF ∈ YF , YF = {true, false, half-true, barely-true, mostly-true, pants-on-fire}. The function g(X) takes the contextual token representations from the last layer of DistilBERT and feeds them"
2020.acl-main.656,W18-5517,0,0.0229861,"lain the reasons behind a certain veracity label independently of the speaker profile. Once trained, such methods could then be applied to other fact checking instances without human-provided explanations or even to perform end-to-end veracity prediction and veracity explanation generation given a claim. Substantial research on fact checking methods exists for the FEVER dataset (Thorne et al., 2018), which comprises rewritten claims from Wikipedia. Systems typically perform document retrieval, evidence selection, and veracity prediction. Evidence selection is performed using keyword matching (Malon, 2018; Yoneda et al., 2018), supervised learning (Hanselowski et al., 2018; Chakrabarty et al., 2018) or sentence similarity scoring (Ma et al., 2018; Mohtarami et al., 2018; Xu et al., 2018). More recently, the multi-domain dataset MultiFC (Augenstein et al., 2019) has been proposed, which is also distributed with evidence pages. Unlike FEVER, it contains real-world claims, crawled from different fact checking portals. While FEVER and MultiFC are larger datasets for fact checking than LIAR-PLUS, they do not contain veracity explanations and can thus not easily be used to train joint veracity predi"
2020.acl-main.656,N18-1070,0,0.047927,"Missing"
2020.acl-main.656,P18-1032,0,0.0894028,"Missing"
2020.acl-main.656,P17-2067,0,0.491875,"bility of verifying the truthfulness of the item is often passed on to the audience. To alleviate this problem, independent teams of professional fact checkers manually verify the veracity and credibility of common or particularly checkworthy statements circulating the web. However, these teams have limited resources to perform manual fact checks, thus creating a need for automating the fact checking process. The current research landscape in automated fact checking is comprised of systems that estimate the veracity of claims based on available metadata and evidence pages. Datasets like LIAR (Wang, 2017) and the multi-domain dataset MultiFC (Augenstein et al., 2019) provide real-world Table 1: Example instance from the LIAR-PLUS dataset, with oracle sentences for generating the justification highlighted. benchmarks for evaluation. There are also artificial datasets of a larger scale, e.g., the FEVER (Thorne et al., 2018) dataset based on Wikipedia articles. As evident from the effectiveness of state-of-theart methods for both real-world – 0.492 macro F1 score (Augenstein et al., 2019), and artificial data – 68.46 FEVER score (label accuracy conditioned on evidence provided for ‘supported’ and"
2020.acl-main.656,W18-5515,0,0.0591443,"ons behind a certain veracity label independently of the speaker profile. Once trained, such methods could then be applied to other fact checking instances without human-provided explanations or even to perform end-to-end veracity prediction and veracity explanation generation given a claim. Substantial research on fact checking methods exists for the FEVER dataset (Thorne et al., 2018), which comprises rewritten claims from Wikipedia. Systems typically perform document retrieval, evidence selection, and veracity prediction. Evidence selection is performed using keyword matching (Malon, 2018; Yoneda et al., 2018), supervised learning (Hanselowski et al., 2018; Chakrabarty et al., 2018) or sentence similarity scoring (Ma et al., 2018; Mohtarami et al., 2018; Xu et al., 2018). More recently, the multi-domain dataset MultiFC (Augenstein et al., 2019) has been proposed, which is also distributed with evidence pages. Unlike FEVER, it contains real-world claims, crawled from different fact checking portals. While FEVER and MultiFC are larger datasets for fact checking than LIAR-PLUS, they do not contain veracity explanations and can thus not easily be used to train joint veracity prediction and explanation"
2020.acl-main.656,P19-1487,0,0.0757445,"Missing"
2020.acl-main.656,D19-6616,0,0.0134138,"n dataset MultiFC (Augenstein et al., 2019) provide real-world Table 1: Example instance from the LIAR-PLUS dataset, with oracle sentences for generating the justification highlighted. benchmarks for evaluation. There are also artificial datasets of a larger scale, e.g., the FEVER (Thorne et al., 2018) dataset based on Wikipedia articles. As evident from the effectiveness of state-of-theart methods for both real-world – 0.492 macro F1 score (Augenstein et al., 2019), and artificial data – 68.46 FEVER score (label accuracy conditioned on evidence provided for ‘supported’ and ‘refuted’ claims) (Stammbach and Neumann, 2019), the task of automating fact checking remains a significant and poignant research challenge. A prevalent component of existing fact checking systems is a stance detection or textual entailment model that predicts whether a piece of evidence 7352 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7352–7364 c July 5 - 10, 2020. 2020 Association for Computational Linguistics contradicts or supports a claim (Ma et al., 2018; Mohtarami et al., 2018; Xu et al., 2018). Existing research, however, rarely attempts to directly optimise the selection of releva"
2020.acl-main.656,N18-1074,0,0.182147,"rces to perform manual fact checks, thus creating a need for automating the fact checking process. The current research landscape in automated fact checking is comprised of systems that estimate the veracity of claims based on available metadata and evidence pages. Datasets like LIAR (Wang, 2017) and the multi-domain dataset MultiFC (Augenstein et al., 2019) provide real-world Table 1: Example instance from the LIAR-PLUS dataset, with oracle sentences for generating the justification highlighted. benchmarks for evaluation. There are also artificial datasets of a larger scale, e.g., the FEVER (Thorne et al., 2018) dataset based on Wikipedia articles. As evident from the effectiveness of state-of-theart methods for both real-world – 0.492 macro F1 score (Augenstein et al., 2019), and artificial data – 68.46 FEVER score (label accuracy conditioned on evidence provided for ‘supported’ and ‘refuted’ claims) (Stammbach and Neumann, 2019), the task of automating fact checking remains a significant and poignant research challenge. A prevalent component of existing fact checking systems is a stance detection or textual entailment model that predicts whether a piece of evidence 7352 Proceedings of the 58th Annu"
2020.blackboxnlp-1.8,P18-2124,0,0.0902065,"Missing"
2020.blackboxnlp-1.8,2020.emnlp-main.442,1,0.872458,"Missing"
2020.blackboxnlp-1.8,N19-1423,0,0.0628855,"Missing"
2020.blackboxnlp-1.8,W18-5446,0,0.0748859,"Missing"
2020.blackboxnlp-1.8,W12-2601,0,0.0875554,"Missing"
2020.blackboxnlp-1.8,P02-1040,0,0.110996,"Missing"
2020.emnlp-main.256,D19-1475,1,0.842174,"Textual Similarity (STS) objective. We additionally use the extracted universal adversarial triggers to generate adversarial examples with low perplexity. 2.2 Fact Checking Fact checking systems consist of components to identify check-worthy claims (Atanasova et al., 2018; Hansen et al., 2019; Wright and Augenstein, 2020), retrieve and rank evidence documents (Yin and Roth, 2018; Allein et al., 2020), determine the relationship between claims and evidence documents (Bowman et al., 2015; Augenstein et al., 2016; Baly et al., 2018), and finally predict the claims’ veracity (Thorne et al., 2018; Augenstein et al., 2019). As this is a relatively involved task, models easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the FEVER 2.0 task (Thorne et al., 2019b), where participants design adversarial attacks for existing FC systems. The first two winning systems (Niewinski et al., 2019; Hidey et al., 2020) produce claims requiring multi-hop reasoning, which has been shown to be challenging for fact checking models (Ostrow"
2020.emnlp-main.256,D16-1084,1,0.88475,"Missing"
2020.emnlp-main.256,N18-2004,0,0.020375,"ub.com/copenlu/fever-adversarial-attacks method by including an auxiliary Semantic Textual Similarity (STS) objective. We additionally use the extracted universal adversarial triggers to generate adversarial examples with low perplexity. 2.2 Fact Checking Fact checking systems consist of components to identify check-worthy claims (Atanasova et al., 2018; Hansen et al., 2019; Wright and Augenstein, 2020), retrieve and rank evidence documents (Yin and Roth, 2018; Allein et al., 2020), determine the relationship between claims and evidence documents (Bowman et al., 2015; Augenstein et al., 2016; Baly et al., 2018), and finally predict the claims’ veracity (Thorne et al., 2018; Augenstein et al., 2019). As this is a relatively involved task, models easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the FEVER 2.0 task (Thorne et al., 2019b), where participants design adversarial attacks for existing FC systems. The first two winning systems (Niewinski et al., 2019; Hidey et al., 2020) produce claims requiring mul"
2020.emnlp-main.256,D15-1075,0,0.283801,"contrast, we extend the HotFlip 1 https://github.com/copenlu/fever-adversarial-attacks method by including an auxiliary Semantic Textual Similarity (STS) objective. We additionally use the extracted universal adversarial triggers to generate adversarial examples with low perplexity. 2.2 Fact Checking Fact checking systems consist of components to identify check-worthy claims (Atanasova et al., 2018; Hansen et al., 2019; Wright and Augenstein, 2020), retrieve and rank evidence documents (Yin and Roth, 2018; Allein et al., 2020), determine the relationship between claims and evidence documents (Bowman et al., 2015; Augenstein et al., 2016; Baly et al., 2018), and finally predict the claims’ veracity (Thorne et al., 2018; Augenstein et al., 2019). As this is a relatively involved task, models easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the FEVER 2.0 task (Thorne et al., 2019b), where participants design adversarial attacks for existing FC systems. The first two winning systems (Niewinski et al., 2019; Hid"
2020.emnlp-main.256,P18-2006,0,0.0277901,"atically generated text (Ren et al., 2019) or perturbations of existing input instances (Jin et al.; Ebrahimi et al., 2018). For a detailed literature overview, see Zhang et al. (2019). One potent type of adversarial techniques are universal adversarial attacks (Gao and Oates, 2019; Wallace et al., 2019) – single perturbation changes that can be applied to a large number of input instances and that cause significant performance decreases of the model under attack. Wallace et al. (2019) find universal adversarial triggers that can change the prediction of the model using the HotFlip algorithm (Ebrahimi et al., 2018). However, for NLI tasks, they also change the meaning of the instance they are appended to, and the prediction of the model remains correct. Michel et al. (2019) address this by exploring only perturbed instances in the neighborhood of the original one. Their approach is for instancedependent attacks, whereas we suggest finding universal adversarial triggers that also preserve the original meaning of input instances. Another approach to this are rule-based perturbations of the input (Ribeiro et al., 2018) or imposing adversarial constraints on the produced perturbations (Dia et al., 2019). By"
2020.emnlp-main.256,2020.acl-main.761,0,0.0198704,"015; Augenstein et al., 2016; Baly et al., 2018), and finally predict the claims’ veracity (Thorne et al., 2018; Augenstein et al., 2019). As this is a relatively involved task, models easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the FEVER 2.0 task (Thorne et al., 2019b), where participants design adversarial attacks for existing FC systems. The first two winning systems (Niewinski et al., 2019; Hidey et al., 2020) produce claims requiring multi-hop reasoning, which has been shown to be challenging for fact checking models (Ostrowski et al., 2020). The other remaining system (Kim and Allan, 2019) generates adversarial attacks manually. We instead find universal adversarial attacks that can be applied to most existing inputs while markedly decreasing fact checking performance. Niewinski et al. (2019) additionally feed a pre-trained GPT-2 model with the target label of the instance along with the text for conditional adversarial claim generation. Conditional language generation has also been employed by K"
2020.emnlp-main.256,D19-6615,0,0.0425437,"easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the FEVER 2.0 task (Thorne et al., 2019b), where participants design adversarial attacks for existing FC systems. The first two winning systems (Niewinski et al., 2019; Hidey et al., 2020) produce claims requiring multi-hop reasoning, which has been shown to be challenging for fact checking models (Ostrowski et al., 2020). The other remaining system (Kim and Allan, 2019) generates adversarial attacks manually. We instead find universal adversarial attacks that can be applied to most existing inputs while markedly decreasing fact checking performance. Niewinski et al. (2019) additionally feed a pre-trained GPT-2 model with the target label of the instance along with the text for conditional adversarial claim generation. Conditional language generation has also been employed by Keskar et al. (2019) to control the style, content, and the task-specific behavior of a Transformer. 3 3.1 Methods Models We take a RoBERTa (Liu et al., 2019) model pretrained with a LM"
2020.emnlp-main.256,2021.ccl-1.108,0,0.0843559,"Missing"
2020.emnlp-main.256,W18-5517,0,0.0196331,"the target label of the instance along with the text for conditional adversarial claim generation. Conditional language generation has also been employed by Keskar et al. (2019) to control the style, content, and the task-specific behavior of a Transformer. 3 3.1 Methods Models We take a RoBERTa (Liu et al., 2019) model pretrained with a LM objective and fine-tune it to classify claim-evidence pairs from the FEVER dataset as SUPPORTS, REFUTES, and NOT ENOUGH INFO (NEI). The evidence used is the gold evidence, available for the SUPPORTS and REFUTES classes. For NEI claims, we use the system of Malon (2018) to retrieve evidence sentences. To measure the semantic similarity between the claim 3169 before and after prepending a trigger, we use a large RoBERTa model fine-tuned on the Semantic Textual Similarity Task.2 For further details, we refer the reader to §A.1. 3.2 Universal Adversarial Triggers Method The Universal Adversarial Triggers method is developed to find n-gram trigger tokens tα , which, appended to the original input x, f (x) = y, cause the model to predict a target class ye : f (tα , x) = ye. In our work, we generate unigram triggers, as generating longer triggers would require add"
2020.emnlp-main.256,N19-1314,0,0.0229299,"Zhang et al. (2019). One potent type of adversarial techniques are universal adversarial attacks (Gao and Oates, 2019; Wallace et al., 2019) – single perturbation changes that can be applied to a large number of input instances and that cause significant performance decreases of the model under attack. Wallace et al. (2019) find universal adversarial triggers that can change the prediction of the model using the HotFlip algorithm (Ebrahimi et al., 2018). However, for NLI tasks, they also change the meaning of the instance they are appended to, and the prediction of the model remains correct. Michel et al. (2019) address this by exploring only perturbed instances in the neighborhood of the original one. Their approach is for instancedependent attacks, whereas we suggest finding universal adversarial triggers that also preserve the original meaning of input instances. Another approach to this are rule-based perturbations of the input (Ribeiro et al., 2018) or imposing adversarial constraints on the produced perturbations (Dia et al., 2019). By contrast, we extend the HotFlip 1 https://github.com/copenlu/fever-adversarial-attacks method by including an auxiliary Semantic Textual Similarity (STS) objecti"
2020.emnlp-main.256,D19-6604,0,0.0661876,"uments (Bowman et al., 2015; Augenstein et al., 2016; Baly et al., 2018), and finally predict the claims’ veracity (Thorne et al., 2018; Augenstein et al., 2019). As this is a relatively involved task, models easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the FEVER 2.0 task (Thorne et al., 2019b), where participants design adversarial attacks for existing FC systems. The first two winning systems (Niewinski et al., 2019; Hidey et al., 2020) produce claims requiring multi-hop reasoning, which has been shown to be challenging for fact checking models (Ostrowski et al., 2020). The other remaining system (Kim and Allan, 2019) generates adversarial attacks manually. We instead find universal adversarial attacks that can be applied to most existing inputs while markedly decreasing fact checking performance. Niewinski et al. (2019) additionally feed a pre-trained GPT-2 model with the target label of the instance along with the text for conditional adversarial claim generation. Conditional language generation has al"
2020.emnlp-main.256,D19-1292,0,0.0628468,"Missing"
2020.emnlp-main.256,D19-6601,0,0.0902617,"Missing"
2020.emnlp-main.256,D19-1221,0,0.0181818,"pace that are outside the training data distribution where the model is unstable. It is important to reveal such vulnerabilities and correct for them, especially for tasks such as fact checking (FC). In this paper, we explore the vulnerabilities of FC models trained on the FEVER dataset (Thorne et al., 2018), where the inference between a claim and evidence text is predicted. We particularly ∗ denotes equal contribution CLAIM Dissociative identity disorder, or DID, may be the result of memory disruptions that have been induced by psychological trauma. construct universal adversarial triggers (Wallace et al., 2019) – single n-grams appended to the input text that can shift the prediction of a model from a source class to a target one. Such adversarial examples are of particular concern, as they can apply to a large number of input instances. However, we find that the triggers also change the meaning of the claim such that the true label is in fact the target class. For example, when attacking a claim-evidence pair with a ‘SUPPORTS’ label, a common unigram found to be a universal trigger when switching the label to ‘REFUTES’ is ‘none’. Prepending this token to the claim drastically changes the meaning of"
2020.emnlp-main.256,2020.findings-emnlp.43,1,0.808428,"ther approach to this are rule-based perturbations of the input (Ribeiro et al., 2018) or imposing adversarial constraints on the produced perturbations (Dia et al., 2019). By contrast, we extend the HotFlip 1 https://github.com/copenlu/fever-adversarial-attacks method by including an auxiliary Semantic Textual Similarity (STS) objective. We additionally use the extracted universal adversarial triggers to generate adversarial examples with low perplexity. 2.2 Fact Checking Fact checking systems consist of components to identify check-worthy claims (Atanasova et al., 2018; Hansen et al., 2019; Wright and Augenstein, 2020), retrieve and rank evidence documents (Yin and Roth, 2018; Allein et al., 2020), determine the relationship between claims and evidence documents (Bowman et al., 2015; Augenstein et al., 2016; Baly et al., 2018), and finally predict the claims’ veracity (Thorne et al., 2018; Augenstein et al., 2019). As this is a relatively involved task, models easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the F"
2020.emnlp-main.256,D18-1010,0,0.0282546,"eiro et al., 2018) or imposing adversarial constraints on the produced perturbations (Dia et al., 2019). By contrast, we extend the HotFlip 1 https://github.com/copenlu/fever-adversarial-attacks method by including an auxiliary Semantic Textual Similarity (STS) objective. We additionally use the extracted universal adversarial triggers to generate adversarial examples with low perplexity. 2.2 Fact Checking Fact checking systems consist of components to identify check-worthy claims (Atanasova et al., 2018; Hansen et al., 2019; Wright and Augenstein, 2020), retrieve and rank evidence documents (Yin and Roth, 2018; Allein et al., 2020), determine the relationship between claims and evidence documents (Bowman et al., 2015; Augenstein et al., 2016; Baly et al., 2018), and finally predict the claims’ veracity (Thorne et al., 2018; Augenstein et al., 2019). As this is a relatively involved task, models easily overfit to shallow textual patterns, necessitating the need for adversarial examples to evaluate the limits of their performance. Thorne et al. (2019a) are the first to propose hand-crafted adversarial attacks. They follow up on this with the FEVER 2.0 task (Thorne et al., 2019b), where participants d"
2020.emnlp-main.263,2020.emnlp-main.256,1,0.659048,"asks and three different model architectures. • We study the attributions of the explainability techniques and human annotations of salient regions to compare and contrast the rationales of humans and machine learning models. 2 Related Work Explainability methods can be divided into explanations by simplification, e.g., LIME (Ribeiro et al., 2016); gradient-based explanations (Sundararajan et al., 2017); perturbation-based explanations (Shapley, 1953; Zeiler and Fergus, 2014). Some studies propose the generation of text serving as an explanation, e.g., (Camburu et al., 2018; Lei et al., 2016; Atanasova et al., 2020a). For extensive overviews of existing explainability approaches, see Arrieta et al. (2020). Explainability methods provide explanations of different qualities, so assessing them systematically is pivotal. A common attempt to reveal shortcomings in explainability techniques is to reveal a model’s reasoning process with counter-examples (Alvarez-Melis and Jaakkola, 2018; Kindermans et al., 2019; Atanasova et al., 2020b), finding different explanations for the same output. However, single counter-examples do not provide a measure to evaluate explainability techniques (Jacovi and Goldberg, 2020)"
2020.emnlp-main.368,N15-1151,0,0.0296389,"ss-Lingual NLU: Cross-lingual learning has a fairly short history in NLP, and has mainly been restricted to traditional NLP tasks, such as PoS tagging, morphological inflection and parsing. In contrast to these tasks, which have seen much cross-lingual attention (Plank et al., 2016; Bjerva, 2017; Kementchedjhieva et al., 2018; de Lhoneux et al., 2018), there has been relatively little work on cross-lingual NLU, partly due to lack of benchmark datasets. Existing work has mainly been focused on NLI (Agic and Schluter, 2018; Conneau et al., 2018; Zhao et al., 2020), and to a lesser degree on RE (Faruqui and Kumar, 2015; Verga et al., 2016) and QA (Abdou et al., 2019; Lewis et al., 2020). Previous research generally reports that cross-lingual learning is challenging and that it is hard to beat a machine translation baseline (e.g., Conneau et al. (2018)). Such a baseline is for instance suggested by Faruqui and Kumar (2015), where the text in the target language is automatically translated to English. We achieve competitive performance compared to a machine translation baseline (for XNLI), and propose a method that requires no training instances for the target task in the target language. Furthermore, our met"
2020.emnlp-main.368,2020.acl-main.747,0,0.148164,"Missing"
2020.emnlp-main.368,D18-1398,0,0.424643,"nd apply it directly to another language where only limited training data is available (i.e., low-resource languages). Therefore, it is essential to investigate strategies that allow one to use the large amount of training data available for English for the benefit of other languages. Meta-learning has recently been shown to be beneficial for several machine learning tasks (Koch et al., 2015; Vinyals et al., 2016; Santoro et al., 2016; Finn et al., 2017; Ravi and Larochelle, 2017; Nichol et al., 2018). For NLP, recent work has also shown the benefits of this sharing between tasks and domains (Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019). Although cross-lingual transfer with meta-learning has been investigated for machine translation (Gu et al., 2018), this paper – to best of our knowledge – is the first attempt to study meta-learning for cross-lingual natural language understanding. Our contributions are as follows: Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourc"
2020.emnlp-main.368,D18-1269,0,0.533938,"is beneficial. 1 • We propose X-MAML1 , a cross-lingual metalearning architecture, and study it for two natural language understanding tasks (Natural Language Inference and Question Answering); Introduction There are more than 7,000 languages spoken in the world, over 90 of which have more than 10 million native speakers each (Eberhard et al., 2019). Despite this, very few languages have proper linguistic resources when it comes to natural language understanding tasks (Joshi et al., 2020). Although there is growing awareness in the field, as evidenced by the release of datasets such as XNLI (Conneau et al., 2018), most NLP research still only considers English (Bender, 2019). While one solution to • We test X-MAML on cross-domain, crosslingual, standard supervised, few-shot as well as zero-shot learning, across a total of 15 languages; • We observe consistent improvements over strong models including Multilingual BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020); 1 Our code is available at https://github.com/ copenlu/X-MAML 4547 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4547–4562, c November 16–20, 2020. 2020 Association for Computati"
2020.emnlp-main.368,N19-1423,0,0.576193,"hard et al., 2019). Despite this, very few languages have proper linguistic resources when it comes to natural language understanding tasks (Joshi et al., 2020). Although there is growing awareness in the field, as evidenced by the release of datasets such as XNLI (Conneau et al., 2018), most NLP research still only considers English (Bender, 2019). While one solution to • We test X-MAML on cross-domain, crosslingual, standard supervised, few-shot as well as zero-shot learning, across a total of 15 languages; • We observe consistent improvements over strong models including Multilingual BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020); 1 Our code is available at https://github.com/ copenlu/X-MAML 4547 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4547–4562, c November 16–20, 2020. 2020 Association for Computational Linguistics • We perform an extensive error analysis, which reveals that cross-lingual trends can partly be explained by typological commonalities between languages. 2 Meta-Learning Meta-learning tries to tackle the problem of fast adaptation to a handful of new training data instances. It discovers the structure among multipl"
2020.emnlp-main.368,D19-1112,0,0.0776258,"Missing"
2020.emnlp-main.368,D16-1264,0,\N,Missing
2020.emnlp-main.368,W18-5446,0,\N,Missing
2020.emnlp-main.368,D18-1543,1,\N,Missing
2020.emnlp-main.368,D18-1456,0,\N,Missing
2020.emnlp-main.368,D19-1077,0,\N,Missing
2020.emnlp-main.368,D18-1514,0,\N,Missing
2020.emnlp-main.368,N18-1101,0,\N,Missing
2020.emnlp-main.368,N16-1103,0,\N,Missing
2020.emnlp-main.368,2020.emnlp-main.484,0,\N,Missing
2020.emnlp-main.368,W18-0207,1,\N,Missing
2020.emnlp-main.442,C10-1004,0,0.0284291,"were quite impressive. Figure 1: Our data collection pipeline able for verification or objective observation. It is this type of state which is referred to as subjectivity (Banfield, 1982; Banea et al., 2011). Whereas subjectivity has been investigated in isolation, it can be argued that subjectivity is only meaningful given sufficient context. Regardless, much previous work has focused on annotating words (Heise, 2001), word senses (Durkin and Manning, 1989; Wiebe and Mihalcea, 2006; Akkaya et al., 2009), or sentences (Pang and Lee, 2004), with notable exceptions such as Wiebe et al. (2005); Banea et al. (2010), who investigate subjectivity in phrases in the context of a text or conversation. There is limited work that studies subjectivity using architectures that allow for contexts to be incorporated efficiently (Vaswani et al., 2017). As subjectivity relies heavily on context, and we have access to methods which can encode such context, what then of access to data which encodes subjectivity? We argue that in order to fully investigate research questions dealing with subjectivity in contexts, a large-scale dataset is needed. We choose to frame this as a QA dataset, as it not only offers the potenti"
2020.emnlp-main.442,P17-1147,0,0.0947055,"Missing"
2020.emnlp-main.442,2020.acl-main.653,0,0.0722396,"Missing"
2020.emnlp-main.442,D18-1543,1,0.899108,"Missing"
2020.emnlp-main.442,2020.blackboxnlp-1.8,1,0.761925,"argeted at understanding subjectivity. This can be attributed to how these datasets are constructed. Large-scale QA datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), CoQA (Reddy et al., 2019), MLQA (Lewis et al., 2020) are based on factual data. We are the first to attempt to create a review-based QA dataset for the purpose of understanding subjectivity. Recent work has corroborated our findings on the benefits of modelling subjectivity QA, and highlights the differences in the distributions of hidden representation between S UBJ QA and the factual SQuAD data (Muttenthaler et al., 2020). 7 Conclusion In this paper we investigate subjectivity in QA by leveraging end-to-end architectures. We release S UBJ QA, a question-answering dataset which contains subjectivity labels for both questions and answers. The dataset allows i) evaluation and development of architectures for subjective content, and ii) investigation of subjectivity and its interactions in broad and diverse contexts. We further implement a subjectivity-aware model and evaluate it, along with 4 strong baseline models. We hope this dataset opens new avenues for research on querying subjective content, and into subje"
2020.emnlp-main.442,P04-1035,0,0.547843,"iews, containing subjectivity annotations for questions and answer spans across 6 domains. 1 Introduction Subjectivity is ubiquitous in our use of language (Banfield, 1982; Quirk et al., 1985; Wiebe et al., 1999; Benamara et al., 2017), and is therefore an important aspect to consider in Natural Language Processing (NLP). For example, subjectivity can be associated with different senses of the same word. BOILING is objective in the context of hot water, but subjective in the context of a person boiling with anger (Wiebe and Mihalcea, 2006). The same applies to sentences in discourse contexts (Pang and Lee, 2004). While early work has shown subjectivity to be an important feature for low-level tasks such as word-sense disambiguation and sentiment analysis, subjectivity in NLP has not been explored in many contexts where it is prevalent. ∗ JB and NB contributed equally to this work. In recent years, there has been renewed interest in areas of NLP for which subjectivity is important, and a specific topic of interest is question answering (QA). This includes work on aspect extraction (Poria et al., 2016), opinion mining (Sun et al., 2017) and community QA (Gupta et al., 2019). Many QA systems are based o"
2020.emnlp-main.442,P18-2124,0,0.0970882,"Missing"
2020.emnlp-main.442,Q19-1016,0,0.132296,"ained traction recently with the availability of new datasets and architectures (Grail and Perez, 2018; Gupta et al., 2019; Fan et al., 2019; Xu et al., 2019b; Li et al., 2019), these are agnostic with respect to how subjectivity is expressed in the questions and the reviews. Furthermore, the datasets are either very small (< 2000 questions) or have target-specific question types (e.g., yes-no). Most QA datasets and systems focus on answering questions over factual data such as Wikipedia articles and News (Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018; Abdou et al., 2019; Reddy et al., 2019). In this work, on the other hand, we focus on QA over subjective data from reviews on product and service websites. In this work, we investigate the relation between subjectivity and question answering (QA) in the context of customer reviews. As no such QA 5480 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5480–5494, c November 16–20, 2020. 2020 Association for Computational Linguistics Contributions (i) We release a challenging QA dataset with subjectivity labels for questions and answers, spanning 6 domains; (ii) We investigate the relationshi"
2020.emnlp-main.442,N13-1008,0,0.0188715,"/www.yelp.com/dataset Given a review corpus, we extract opinions about various aspects of the items being reviewed (Opinion Extraction). Consider the following review snippets and extractions. Example 2 Review: [...] character development was quite impressive. e1 :‹‘impressive’, ‘character development’› Review: 3 stars for good power and good writing. e2 :‹‘good’, ‘writing’› In the next (Neighborhood Model Construction) step, we characterize the items being reviewed and their subjective extractions using latent features between two items. In particular, we use matrix factorization techniques (Riedel et al., 2013) to construct a neighborhood model N via a set of weights we,e0 , where each corresponds to a directed association strength between extraction e and e0 . For instance, e1 and e2 in Example 2 could have a similarity score 0.93. This neighborhood model forms the core of data collection. We select a subset of extractions from N as topics (Topic Selection) and ask crowd workers to translate them to natural language questions (Question Generation). For each topic, a subset of its neighbors from N and reviews which mention them are selected (Review Selection). In this manner, question-review pairs a"
2020.emnlp-main.442,W17-2623,0,0.310605,"e highly subjective as well. Although QA over customer reviews has gained traction recently with the availability of new datasets and architectures (Grail and Perez, 2018; Gupta et al., 2019; Fan et al., 2019; Xu et al., 2019b; Li et al., 2019), these are agnostic with respect to how subjectivity is expressed in the questions and the reviews. Furthermore, the datasets are either very small (< 2000 questions) or have target-specific question types (e.g., yes-no). Most QA datasets and systems focus on answering questions over factual data such as Wikipedia articles and News (Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018; Abdou et al., 2019; Reddy et al., 2019). In this work, on the other hand, we focus on QA over subjective data from reviews on product and service websites. In this work, we investigate the relation between subjectivity and question answering (QA) in the context of customer reviews. As no such QA 5480 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5480–5494, c November 16–20, 2020. 2020 Association for Computational Linguistics Contributions (i) We release a challenging QA dataset with subjectivity labels for questions and"
2020.emnlp-main.442,P18-4005,1,0.876594,"Missing"
2020.emnlp-main.442,P06-1134,0,0.209631,"ective answer. We release an English QA dataset (S UBJ QA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains. 1 Introduction Subjectivity is ubiquitous in our use of language (Banfield, 1982; Quirk et al., 1985; Wiebe et al., 1999; Benamara et al., 2017), and is therefore an important aspect to consider in Natural Language Processing (NLP). For example, subjectivity can be associated with different senses of the same word. BOILING is objective in the context of hot water, but subjective in the context of a person boiling with anger (Wiebe and Mihalcea, 2006). The same applies to sentences in discourse contexts (Pang and Lee, 2004). While early work has shown subjectivity to be an important feature for low-level tasks such as word-sense disambiguation and sentiment analysis, subjectivity in NLP has not been explored in many contexts where it is prevalent. ∗ JB and NB contributed equally to this work. In recent years, there has been renewed interest in areas of NLP for which subjectivity is important, and a specific topic of interest is question answering (QA). This includes work on aspect extraction (Poria et al., 2016), opinion mining (Sun et al."
2020.emnlp-main.442,P99-1032,0,0.642808,"Missing"
2020.emnlp-main.442,N19-1242,0,0.0487657,"Missing"
2020.emnlp-main.442,D12-1036,0,0.0299843,"7.05 0.40 -0.48 7.37 11.07 8.28 9.68 7.24 3.41 1.01 5.71 3.08 4.06 -0.01 -1.57 7.42 8.65 5.84 6.67 7.26 2.20 Average 13.35 15.40 3.69 7.93 14.27 16.19 3.63 7.84 2.05 6.34 Table 9: MTL gains/losses over the fine-tuning condition (F1 and Exact match), across subj./fact. QA. mining (Blair-Goldensohn et al., 2008), with a focus on text polarity. There is renewed intereste in incorporating subjective opinion data into general data management systems (Li et al., 2019; Kobren et al., 2019) and for querying subjective data. In this work, we revisit subjectivity in the context of review QA. Yu et al. (2012); McAuley and Yang (2016b) also use review data, as they leverage question types and aspects to answer questions. However, no prior work has modeled subjectivity explicitly using end-to-end architectures. Furthermore, none of the existing reviewbased QA datasets are targeted at understanding subjectivity. This can be attributed to how these datasets are constructed. Large-scale QA datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), CoQA (Reddy et al., 2019), MLQA (Lewis et al., 2020) are based on factual data. We are the first to attempt to create a review-based"
2020.emnlp-main.442,J17-1006,0,\N,Missing
2020.emnlp-main.639,P07-1056,0,0.363798,"eriments on text classification problems with data from multiple domains. To this end, we experiment with sentiment analysis from Amazon product reviews and rumour detection from tweets. For both tasks, we perform crossvalidation on each domain, holding out a single domain for testing and training on the remaining domains, allowing a comparison of each method on how well they perform under domain shift. The code to reproduce all of the experiments in this paper can be found here1 . Sentiment Analysis Data The data used for sentiment analysis come from the legacy Amazon Product Review dataset (Blitzer et al., 2007). This dataset consists of 8,000 total tweets from four product categories: books, DVDs, electronics, and kitchen and housewares. Each domain contains 1,000 positive and 1,000 negative reviews. In addition, each domain has associated unlabelled data. Following previous work we focus on the transductive setting (Guo et al., 2018; Ziser and Reichart, 2017) where we use the same 2,000 out of domain tweets as unlabelled data for training the domain adversarial models. This data has been well studied in the context of domain adaptation, making for easy comparison with previous work. Rumour Detectio"
2020.emnlp-main.639,W06-1615,0,0.242607,"domain adaptation with LPX models. We first review domain adaptation in general, followed by studies into domain adaptation with LPX models. 2.1 Domain Adaptation Domain adaptation approaches generally fall into three categories: supervised approaches (e.g. Daum´e (2007); Finkel and Manning (2009); Kulis et al. (2011)), where both labels for the source and the target domain are available; semi-supervised approaches (e.g. Donahue et al. (2013); Yao et al. (2015)), where labels for the source and a small set of labels for the target domain are provided; and lastly unsupervised approaches (e.g. Blitzer et al. (2006); Ganin and Lempitsky (2015); Sun et al. (2016); Lipton et al. (2018)), where only labels for the source domain are given. Since the focus of this paper is the latter, we restrict our discussion to unsupervised approaches. A more complete recent review of unsupervised domain adaptation approaches is given in Kouw and Loog (2019). A popular approach to unsupervised domain adaptation is to induce representations which are invariant to the shift in distribution between source and target data. For deep networks, this can be accomplished via domain adversarial training using a simple gradient rever"
2020.emnlp-main.639,Q17-1010,0,0.0415072,"cases. 5.2 Is Mixture of Experts Useful with LPX Models? Comparing agreement To provide some potential explanation for why it is difficult to learn to attend to domain experts, we compare the agreement on the predictions of domain experts of one of our models based on DistilBert, versus a model based on CNNs (Figure 4). CNN models are chosen in order to compare the agreement using our approach with an approach which has been shown to work well with mixture of experts on this data (Guo et al., 2018). Each CNN consists of an embedding layer initialized with 300 dimensional FastText embeddings (Bojanowski et al., 2017), a series of 100 dimensional convolutional layers with widths 2, 4, and 5, and a classifier. The end performance is on par with previous work using CNNs (Li et al., 2018) (78.8 macro averaged accuracy, validation accuracies of the individual models are between 80.0 and 87.0). Agreement is measured using Krippendorff’s alpha (Krippendorff, 2011) between the predictions of domain experts on test data. We observe that the agreement between DistilBert domain experts on test data is significantly higher than that of CNN domain experts, indicating that the learned classifiers of each expert are muc"
2020.emnlp-main.639,P07-1033,0,0.722816,"Missing"
2020.emnlp-main.639,N19-1423,0,0.0459734,"learning method is to induce domain invariant representations using unsupervised target data and domain adversarial learning (Ganin and Lempitsky, 2015). Adding to this, mixture of experts techniques attempt to learn both domain specific and global shared representations and combine their predictions (Guo et al., 2018; Li et al., 2018; Ma et al., 2019). These methods have been primarily studied using convolutional nets (CNNs) and recurrent nets (RNNs) trained from scratch, while the NLP community has recently begun to rely more and more on large pretrained transformer (LPX) models e.g. BERT (Devlin et al., 2019). To date 7963 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7963–7974, c November 16–20, 2020. 2020 Association for Computational Linguistics there has been some preliminary investigation of how LPX models perform under domain shift in the single source-single target setting (Ma et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). What is lacking is a study into the effects of and best ways to apply classic multi-source domain adaptation techniques with LPX models, which can give insight into possible avenues"
2020.emnlp-main.639,N09-1068,0,0.0677849,"Missing"
2020.emnlp-main.639,D17-1256,0,0.0304045,"vector αX then attends to the individual predictions of each domain expert and the global shared model. X (k) (g) ¯ = pX (x, K) pk (x) ∗ αX (x) + pg (x) ∗ αX (x) ¯ k∈K (8) To ensure that each model is trained as a domain specific expert, a similar training procedure to that of Guo et al. 2018 is utilized, described in §3.3. 3.2 Domain Adversarial Training The method of domain adversarial adaptation we investigate here is the well-studied technique described in Ganin and Lempitsky (2015). It has been shown to benefit both convolutional nets and recurrent nets on NLP problems (Li et al., 2018; Gui et al., 2017), so is a prime candidate to study in the context of LPX models. Additionally, some preliminary evidence indicates that adversarial training might improve LPX generalizability for singlesource domain adaptation (Ma et al., 2019). To learn domain invariant representations, we train a model such that the learned representations maximally confuse a domain classifier fd . This is accomplished through a min-max objective between the domain classifier parameters θD and the parameters θG of an encoder fg . The objective can then be described as follows. LD = max min −d log fd (fg (x)) θD θG (9) where"
2020.emnlp-main.639,2020.acl-main.740,0,0.0670279,"Missing"
2020.emnlp-main.639,P17-1060,0,0.0336473,"orks, this can be accomplished via domain adversarial training using a simple gradient reversal trick (Ganin and Lempitsky, 2015). This has been shown to work in the multi-source domain adaptation setting too (Li et al., 2018). Other popular representation learning methods include minimizing the covariance between source and target features (Sun et al., 2016) and using maximum-mean discrepancy between the marginal distribution of source and target features as an adversarial objective (Guo et al., 2018). Mixture of experts has also been shown to be effective for multi-source domain adaptation. Kim et al. (2017) use attention to combine the predictions of domain experts. Guo et al. (2018) propose learning a mixture of experts using a point to set metric, which combines the posteriors of models trained on individual domains. Our work attempts to build on this to study how multi-source domain adaptation can be improved with LPX models. 2.2 Transformer Based Domain Adaptation There are a handful of studies which investigate how LPX models can be improved in the presence of domain shift. These methods tend to focus on the data and training objectives for single-source single-target unsupervised domain ad"
2020.emnlp-main.639,N18-2076,0,0.387494,"ed multi-source domain adaptation. Multi-source domain adaptation is a well studied problem in deep learning for natural language processing. Prominent techniques are generally based on data selection strategies and representation learning. For example, a popular representation learning method is to induce domain invariant representations using unsupervised target data and domain adversarial learning (Ganin and Lempitsky, 2015). Adding to this, mixture of experts techniques attempt to learn both domain specific and global shared representations and combine their predictions (Guo et al., 2018; Li et al., 2018; Ma et al., 2019). These methods have been primarily studied using convolutional nets (CNNs) and recurrent nets (RNNs) trained from scratch, while the NLP community has recently begun to rely more and more on large pretrained transformer (LPX) models e.g. BERT (Devlin et al., 2019). To date 7963 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7963–7974, c November 16–20, 2020. 2020 Association for Computational Linguistics there has been some preliminary investigation of how LPX models perform under domain shift in the single source-single target"
2020.emnlp-main.639,D19-6109,0,0.34581,"omain adaptation. Multi-source domain adaptation is a well studied problem in deep learning for natural language processing. Prominent techniques are generally based on data selection strategies and representation learning. For example, a popular representation learning method is to induce domain invariant representations using unsupervised target data and domain adversarial learning (Ganin and Lempitsky, 2015). Adding to this, mixture of experts techniques attempt to learn both domain specific and global shared representations and combine their predictions (Guo et al., 2018; Li et al., 2018; Ma et al., 2019). These methods have been primarily studied using convolutional nets (CNNs) and recurrent nets (RNNs) trained from scratch, while the NLP community has recently begun to rely more and more on large pretrained transformer (LPX) models e.g. BERT (Devlin et al., 2019). To date 7963 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7963–7974, c November 16–20, 2020. 2020 Association for Computational Linguistics there has been some preliminary investigation of how LPX models perform under domain shift in the single source-single target setting (Ma et al."
2020.emnlp-main.639,2020.lrec-1.607,0,0.182855,"y studied using convolutional nets (CNNs) and recurrent nets (RNNs) trained from scratch, while the NLP community has recently begun to rely more and more on large pretrained transformer (LPX) models e.g. BERT (Devlin et al., 2019). To date 7963 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7963–7974, c November 16–20, 2020. 2020 Association for Computational Linguistics there has been some preliminary investigation of how LPX models perform under domain shift in the single source-single target setting (Ma et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). What is lacking is a study into the effects of and best ways to apply classic multi-source domain adaptation techniques with LPX models, which can give insight into possible avenues for improved application of these models in settings where there is domain shift. Given this, we present a study into unsupervised multi-source domain adaptation techniques for large pretrained transformer models. Our main research question is: do mixture of experts and domain adversarial training offer any benefit when using LPX models? The answer to this is not immediately obvious, as"
2020.emnlp-main.639,C16-1038,0,0.0688697,"domain adaptation in general, followed by studies into domain adaptation with LPX models. 2.1 Domain Adaptation Domain adaptation approaches generally fall into three categories: supervised approaches (e.g. Daum´e (2007); Finkel and Manning (2009); Kulis et al. (2011)), where both labels for the source and the target domain are available; semi-supervised approaches (e.g. Donahue et al. (2013); Yao et al. (2015)), where labels for the source and a small set of labels for the target domain are provided; and lastly unsupervised approaches (e.g. Blitzer et al. (2006); Ganin and Lempitsky (2015); Sun et al. (2016); Lipton et al. (2018)), where only labels for the source domain are given. Since the focus of this paper is the latter, we restrict our discussion to unsupervised approaches. A more complete recent review of unsupervised domain adaptation approaches is given in Kouw and Loog (2019). A popular approach to unsupervised domain adaptation is to induce representations which are invariant to the shift in distribution between source and target data. For deep networks, this can be accomplished via domain adversarial training using a simple gradient reversal trick (Ganin and Lempitsky, 2015). This has"
2020.emnlp-main.639,2020.findings-emnlp.43,1,0.630933,"s as unlabelled data for training the domain adversarial models. This data has been well studied in the context of domain adaptation, making for easy comparison with previous work. Rumour Detection Data The data used for rumour detection come from the PHEME dataset of rumourous tweets (Zubiaga et al., 2016). There are a total of 5,802 annotated tweets from 5 different events labelled as rumourous or non-rumourous (1,972 rumours, 3,830 non-rumours). Methods which have been shown to work well on this data include context-aware classifiers (Zubiaga et al., 2017) and positive-unlabelled learning (Wright and Augenstein, 2020). Again, we use this data in the transductive setting when testing domain adversarial training. 4.1 Baselines What’s in a Domain? We use the model from Li et al. (2018) as a baseline for sentiment analysis. This model consists of a set of domain experts and one general CNN, and is trained with a domain adversarial auxiliary objective. Mixture of Experts Additionally, we present the results from Guo et al. (2018) representing the most recent state of the art on the Amazon reviews dataset. Their method consists of domain expert classifiers trained on top of a shared encoder, with predictions bei"
2020.emnlp-main.639,K17-1040,0,0.0178323,"ch method on how well they perform under domain shift. The code to reproduce all of the experiments in this paper can be found here1 . Sentiment Analysis Data The data used for sentiment analysis come from the legacy Amazon Product Review dataset (Blitzer et al., 2007). This dataset consists of 8,000 total tweets from four product categories: books, DVDs, electronics, and kitchen and housewares. Each domain contains 1,000 positive and 1,000 negative reviews. In addition, each domain has associated unlabelled data. Following previous work we focus on the transductive setting (Guo et al., 2018; Ziser and Reichart, 2017) where we use the same 2,000 out of domain tweets as unlabelled data for training the domain adversarial models. This data has been well studied in the context of domain adaptation, making for easy comparison with previous work. Rumour Detection Data The data used for rumour detection come from the PHEME dataset of rumourous tweets (Zubiaga et al., 2016). There are a total of 5,802 annotated tweets from 5 different events labelled as rumourous or non-rumourous (1,972 rumours, 3,830 non-rumours). Methods which have been shown to work well on this data include context-aware classifiers (Zubiaga"
2020.findings-emnlp.112,D19-1049,0,0.0284852,"licies are kept. • Authors: lack of willingness to actively monitor policy changes10 , lack of ability to request reports on them and access the review data to conduct independent analysis.11 • Reviewers: lack of recognition for metaresearch as a valid part of NLP, which, as we learned in writing this paper, makes it difficult to publish on it. In a way, NLP peer review... prevents research on NLP peer review. To illustrate the latter point: a quick search in the ACL anthology revealed only four conference papers on peer review from a meta-research perspective: a paper-reviewer matching tool (Anjum et al., 2019), a corpus of reviews (Kang et al., 2018), and two experimental studies using NLP to explain the observed reviews (Caragea et al., 2019; Gao et al., 2019). We could not find any ACL-published 9 E.g. ACL 2020 opted to handle the increased reviewer load by making all authors register as reviewers, and EMNLP 2020 required a senior reviewer who would mentor secondary reviewers. How can we tell which strategy worked better and should be used next year? 10 For instance, Findings was announced on the conference website and social media, but after acceptance notifications there was still confusion abo"
2020.findings-emnlp.112,D19-1236,0,0.0556833,"Missing"
2020.findings-emnlp.112,N19-1423,0,0.0178582,"Missing"
2020.findings-emnlp.112,D19-1224,0,0.0368753,"en conference rejection rates and conference impact in terms of citations is not strong (Freyne et al., 2010; Ragone et al., 2011, 2013), and rejects from one conference sometimes receive awards1 at another. The problem is that both expectations are unrealistic to begin with. A peer reviewer cannot perform real quality control, because that would mean ensuring that a paper is reproducible. Not only is that impossible, only having a few hours to review a paper, but it is a general problem for Deep Learning (DL)-based NLP (Crane, 2018; Rogers, 2019). The reproducibility checklist at EMNLP 2020 (Dodge et al., 2019) is the first step in that direction. 1 acceptance threshold Mani (2011) discusses the example of a paper by Branavan et al. that received the award at ACL 2009 after being rejected from NAACL (scored at 2.3/5). More recently, ELMo (Peters et al., 2018) received low scores from ICLR reviewers and was resubmitted to NAACL to win the award there. As for paper impact, it is distinct from its scientific merit (Bhattacharya and Packalen, 2020), and strongly depends on completely orthogonal factors: how niche is the topic, how much promotion was done, whether the paper offers room to innovate with a"
2020.findings-emnlp.112,2020.emnlp-main.393,0,0.0272758,"If reviewers feel that a paper was already accepted by the research community, they do not need to do any more vetting. For example, there was no way for BERT to go through fully anonymous peer review (Cotterell, 2019). Early preprint citations are arguably a better indicator of paper quality than peer review (Church, 2017, 2020), but they are also influenced by how famous the authors are4 , and how much they pub3 Inter alia: unfair comparisons (Musgrave et al., 2020), dependence on non-architecture-related factors (Dodge et al., 2020; Crane, 2018), no incentives for producing robust systems (Ethayarajh and Jurafsky, 2020), flawed benchmarks (Jia and Liang, 2017; McCoy et al., 2019), which become a tool for producing incremental papers (Reiter, 2020) 4 Peters and Ceci (1982) resubmitted 12 articles to psychology journals that already published these articles, with the author names changed to unknown names. Many were rejected for ‘methodology flaws’! See Rogers (2020c) for discussion of anonymity in upcoming ACL peer review reform. 1257 licize their work. Well-known labs tend to have large online followings or even PR departments, propagating the ‘rich get richer’ phenomenon (also known as the ‘Matthew Effect’,"
2020.findings-emnlp.112,N19-1129,0,0.0284849,"to conduct independent analysis.11 • Reviewers: lack of recognition for metaresearch as a valid part of NLP, which, as we learned in writing this paper, makes it difficult to publish on it. In a way, NLP peer review... prevents research on NLP peer review. To illustrate the latter point: a quick search in the ACL anthology revealed only four conference papers on peer review from a meta-research perspective: a paper-reviewer matching tool (Anjum et al., 2019), a corpus of reviews (Kang et al., 2018), and two experimental studies using NLP to explain the observed reviews (Caragea et al., 2019; Gao et al., 2019). We could not find any ACL-published 9 E.g. ACL 2020 opted to handle the increased reviewer load by making all authors register as reviewers, and EMNLP 2020 required a senior reviewer who would mentor secondary reviewers. How can we tell which strategy worked better and should be used next year? 10 For instance, Findings was announced on the conference website and social media, but after acceptance notifications there was still confusion about what it meant. 11 Compulsory data collection opt-in for authors and reviewers is a less radical change than making all reviews public, and it might als"
2020.findings-emnlp.112,D17-1215,0,0.0143761,"by the research community, they do not need to do any more vetting. For example, there was no way for BERT to go through fully anonymous peer review (Cotterell, 2019). Early preprint citations are arguably a better indicator of paper quality than peer review (Church, 2017, 2020), but they are also influenced by how famous the authors are4 , and how much they pub3 Inter alia: unfair comparisons (Musgrave et al., 2020), dependence on non-architecture-related factors (Dodge et al., 2020; Crane, 2018), no incentives for producing robust systems (Ethayarajh and Jurafsky, 2020), flawed benchmarks (Jia and Liang, 2017; McCoy et al., 2019), which become a tool for producing incremental papers (Reiter, 2020) 4 Peters and Ceci (1982) resubmitted 12 articles to psychology journals that already published these articles, with the author names changed to unknown names. Many were rejected for ‘methodology flaws’! See Rogers (2020c) for discussion of anonymity in upcoming ACL peer review reform. 1257 licize their work. Well-known labs tend to have large online followings or even PR departments, propagating the ‘rich get richer’ phenomenon (also known as the ‘Matthew Effect’, Merton (1968)). The proposed solution se"
2020.findings-emnlp.112,N18-1149,0,0.0933314,"Missing"
2020.findings-emnlp.112,P19-1334,0,0.0285213,"unity, they do not need to do any more vetting. For example, there was no way for BERT to go through fully anonymous peer review (Cotterell, 2019). Early preprint citations are arguably a better indicator of paper quality than peer review (Church, 2017, 2020), but they are also influenced by how famous the authors are4 , and how much they pub3 Inter alia: unfair comparisons (Musgrave et al., 2020), dependence on non-architecture-related factors (Dodge et al., 2020; Crane, 2018), no incentives for producing robust systems (Ethayarajh and Jurafsky, 2020), flawed benchmarks (Jia and Liang, 2017; McCoy et al., 2019), which become a tool for producing incremental papers (Reiter, 2020) 4 Peters and Ceci (1982) resubmitted 12 articles to psychology journals that already published these articles, with the author names changed to unknown names. Many were rejected for ‘methodology flaws’! See Rogers (2020c) for discussion of anonymity in upcoming ACL peer review reform. 1257 licize their work. Well-known labs tend to have large online followings or even PR departments, propagating the ‘rich get richer’ phenomenon (also known as the ‘Matthew Effect’, Merton (1968)). The proposed solution seems too simple. Since"
2020.findings-emnlp.112,N18-1202,0,0.0240052,"Missing"
2020.findings-emnlp.43,2020.acl-main.656,1,0.886646,"Missing"
2020.findings-emnlp.43,2020.emnlp-main.256,1,0.713836,"mber of organizations performing fact checking (Graves and Cherubini, 2016). However, the rate at which misinformation is introduced and spread vastly outpaces the ability of any organization to perform fact checking, so only the most salient claims are checked. This obviates the need for being able to automatically find check-worthy content online and verify it. The natural language processing and machine learning communities have recently begun to address the problem of automatic fact checking (Vlachos and Riedel, 2014; Hassan et al., 2017; Thorne and Vlachos, 2018; Augenstein et al., 2019; Atanasova et al., 2020a,b; Ostrowski et al., 2020; Allein et al., 2020). The first step of automatic fact checking is claim check-worthiness detection, a text classification problem where, given a statement, one must predict if the content of that statement makes “an assertion about the world that is checkable” (Konstantinovskiy et al., 2018). There are multiple isolated lines of research which have studied variations of this problem. Figure 1 provides examples from three tasks which are studied in this work: rumour detection on Twitter (Zubiaga et al., 2016, 2018), check-worthiness ranking in political debates and"
2020.findings-emnlp.43,Q17-1010,0,0.00720268,"2: micro-F1 (µF1) and ensembled F1 (eF1) performance of each system on the PHEME dataset. Performance is averaged across the five splits of (Zubiaga et al., 2017). Results show the mean, standard deviation, and ensembled score across 15 seeds. Bold indicates best performance, underline indicates second best. tags, and various lexical features, and social features include tweet count, listed count, follow ratio, age, and whether or not a user is verified. The CRF acts on a timeline of tweets, making it contextual. In addition, we include results from a 2-layer BiLSTM with FastText embeddings (Bojanowski et al., 2017). There exist other deep learning models which have been developed for this task, including (Ma et al., 2019) and (Abulaish et al., 2019), but they do not publish results on the standard splits of the data and we were unable to recreate their results, and thus are omitted. 4.3.2 Results The results for the tested systems are given in Table 2. Again we see large gains from BERT based models over the baseline from (Zubiaga et al., 2017) and the 2-layer BiLSTM. Compared to training solely on PHEME, fine tuning from basic citation needed detection sees little improvement (0.1 F1 points). However,"
2020.findings-emnlp.43,D17-1070,0,0.0245258,"erts were informed to label the statements based on the definition of check-worthy given above. We then compared the manual annotation to the original labels using F1 score. Higher F1 score indicates the dataset better reflects the definition of check-worthy we adopt in this work. Our results are given in Table 4. Does PU Citation Needed Detection Transfer to Political Speeches? 4.4.1 Baselines The baselines we compare to are the state of the art models from Hansen et al. (2019) and Konstantinovskiy et al. (2018). The model from Konstantinovskiy et al. (2018) consists of InferSent embeddings (Conneau et al., 2017) concatenated with POS tag and NER features passed through a logistic regression classifier. The model from Hansen et al. (2019) is a bidirectional GRU network acting on syntatic parse features concatenated with word embeddings as the input representation. R Table 4: F1 score comparing manual relabelling of the top 100 predictions by PUC model with the original labels in each dataset by two different annotators. Italics are average value between the two annotators. Table 3: Mean average precision (MAP) of models on political speeches. Bold indicates best performance, underline indicates second"
2020.findings-emnlp.43,P19-1231,0,0.0523322,"hiness among the domains we consider. Therefore, we experiment with using data from this domain as a source for transfer learning, training variants of PU learning models on it, then applying them to target data from other domains. 2.2 Positive Unlabelled Learning PU learning methods attempt to learn good binary classifiers given only positive labelled and unlabelled data. Recent applications where PU learning has been shown to be beneficial include detecting deceptive reviews online (Li et al., 2014; Ren et al., 2014), keyphrase extraction (Sterckx et al., 2016) and named entity recognition (Peng et al., 2019). For a survey on PU learning, see (?), and for a formal definition of PU learning, see §3.2. Methods for learning positive-negative (PN) classifiers from PU data have a long history (Denis, 1998; De Comit´e et al., 1999; Letouzey et al., 2000), with one of the most seminal papers being from Elkan and Noto (2008). In this work, the authors show that by assuming the labelled samples are a random subset of all positive samples, one can utilize a classifier trained on PU data in order to train a different classifier to predict if a sample is positive or negative. The process involves training a P"
2020.findings-emnlp.43,D14-1055,0,0.335357,"Missing"
2020.findings-emnlp.43,D16-1198,0,0.0983286,"nstitutes the most general formulation of check-worthiness among the domains we consider. Therefore, we experiment with using data from this domain as a source for transfer learning, training variants of PU learning models on it, then applying them to target data from other domains. 2.2 Positive Unlabelled Learning PU learning methods attempt to learn good binary classifiers given only positive labelled and unlabelled data. Recent applications where PU learning has been shown to be beneficial include detecting deceptive reviews online (Li et al., 2014; Ren et al., 2014), keyphrase extraction (Sterckx et al., 2016) and named entity recognition (Peng et al., 2019). For a survey on PU learning, see (?), and for a formal definition of PU learning, see §3.2. Methods for learning positive-negative (PN) classifiers from PU data have a long history (Denis, 1998; De Comit´e et al., 1999; Letouzey et al., 2000), with one of the most seminal papers being from Elkan and Noto (2008). In this work, the authors show that by assuming the labelled samples are a random subset of all positive samples, one can utilize a classifier trained on PU data in order to train a different classifier to predict if a sample is positi"
2020.findings-emnlp.43,C18-1283,0,0.0227332,"response, there has been a large increase in the number of organizations performing fact checking (Graves and Cherubini, 2016). However, the rate at which misinformation is introduced and spread vastly outpaces the ability of any organization to perform fact checking, so only the most salient claims are checked. This obviates the need for being able to automatically find check-worthy content online and verify it. The natural language processing and machine learning communities have recently begun to address the problem of automatic fact checking (Vlachos and Riedel, 2014; Hassan et al., 2017; Thorne and Vlachos, 2018; Augenstein et al., 2019; Atanasova et al., 2020a,b; Ostrowski et al., 2020; Allein et al., 2020). The first step of automatic fact checking is claim check-worthiness detection, a text classification problem where, given a statement, one must predict if the content of that statement makes “an assertion about the world that is checkable” (Konstantinovskiy et al., 2018). There are multiple isolated lines of research which have studied variations of this problem. Figure 1 provides examples from three tasks which are studied in this work: rumour detection on Twitter (Zubiaga et al., 2016, 2018),"
2020.findings-emnlp.43,N18-5006,0,0.0613685,"Missing"
2020.findings-emnlp.43,W14-2508,0,0.0618236,"World Economic Forum (Howell et al., 2013). In response, there has been a large increase in the number of organizations performing fact checking (Graves and Cherubini, 2016). However, the rate at which misinformation is introduced and spread vastly outpaces the ability of any organization to perform fact checking, so only the most salient claims are checked. This obviates the need for being able to automatically find check-worthy content online and verify it. The natural language processing and machine learning communities have recently begun to address the problem of automatic fact checking (Vlachos and Riedel, 2014; Hassan et al., 2017; Thorne and Vlachos, 2018; Augenstein et al., 2019; Atanasova et al., 2020a,b; Ostrowski et al., 2020; Allein et al., 2020). The first step of automatic fact checking is claim check-worthiness detection, a text classification problem where, given a statement, one must predict if the content of that statement makes “an assertion about the world that is checkable” (Konstantinovskiy et al., 2018). There are multiple isolated lines of research which have studied variations of this problem. Figure 1 provides examples from three tasks which are studied in this work: rumour dete"
2020.findings-emnlp.43,N19-1423,0,\N,Missing
2020.sigtyp-1.1,Q19-1038,0,0.0255088,"o Edoardo M. Ponti‡ Ekaterina Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by"
2020.sigtyp-1.1,N18-1083,1,0.844133,"l., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typolog"
2020.sigtyp-1.1,W18-0207,1,0.747207,"l., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typolog"
2020.sigtyp-1.1,N19-1156,1,0.642604,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,P19-1382,1,0.844402,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,P07-1009,0,0.847176,"Missing"
2020.sigtyp-1.1,C16-1298,0,0.0351469,"Missing"
2020.sigtyp-1.1,N19-1423,0,0.0121619,"na Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva a"
2020.sigtyp-1.1,J19-2006,1,0.848772,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,W19-4208,1,0.845374,"ographic proximity is a challenging one. We expect that further exploration of unconstrained systems to have the most potential for predicting features in such cases, where little or nothing is known about a language. 7 Typologically Informed Sharing Cross-lingual sharing informed by typology has been investigated for, among others, parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; de Lhoneux et al., 2018), language modeling (Tsvetkov et al., 2016; Ponti et al., 2019b), machine translation (Daiber et al., 2016; Ponti et al., 2018), and morphological inflection (Chaudhary et al., 2019). Many of these approaches use language embeddings with sparse features encoding WALS feature values. Oncevay et al. (2020) find that combining information from typological databases with embeddings learned during training of an NMT model can be beneficial for multilingual NMT. 6.3 Conclusions Acknowledgments This research has received funding from the Swedish Research Council under grant agreement No 2019-04129, as well as the German Research Foundation (DFG project number 408121292). Typological Probing Several recent papers study typological feature prediction as a probing task for evaluati"
2020.sigtyp-1.1,D18-1029,1,0.907351,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.3,0,0.0348564,"ure values for non-ancestral languages can be inferred individually by rerooting the tree to a related language. NUIG (Choudhary (2020), NUI Galway) submitted a constrained system with independent classifiers to predict each WALS feature. The outputs of independent classifiers are then fed into a shared encoder with feed-forward and self-attention layers in order to make use of feature correlations. Their model does not use other known features for WALS feature prediction at inference time, relying only on the 5-dimensional inputs of longitude, latitude, genus, family, and country-code. NEMO (Gutkin and Sproat (2020), Google London and Tokyo) submitted constrained systems which first computed probabilities of represented feature values across each language’s genetic (genus and family), and areal (features from languages within a 2,500 kilometre radius, computed from provided latitude and longitude with the Haversine formula), and implicational universals or rather, priors for certain features given commonly associated feature-value pairs in the data. Figure 2: Macro-averaged rankings of all submissions They compared several classifiers’ performance using these sparse features, ultimately submitting system"
2020.sigtyp-1.1,2020.acl-main.747,0,0.0779042,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.5,0,0.0676552,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.2,0,0.0945652,"Missing"
2020.sigtyp-1.1,N18-2085,1,0.860078,"Missing"
2020.sigtyp-1.1,D18-1543,1,0.916053,"Missing"
2020.sigtyp-1.1,E17-2002,0,0.0174388,"m 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known. 1 Introduction Linguistic typology is the study of structural properties of languages (Comrie, 1988; Croft, 2002; Velupillai, 2012). Approaches to the categorisation of the languages of the world according to their linguistic properties are represented by, e.g., typological features in databases such as WALS (Dryer and Haspelmath, 2013), URIEL (Littell et al., 2017), and AUTOTYP (Nichols et al., 2013), e.g. in terms of their syntax, morphology, and phonology. One example of such a typological feature is the basic word order feature in WALS. For instance, English is best described as a subject-verb-object (SVO) language, whereas Japanese is best described as a subject-object-verb (SOV) language. Once a relatively niche topic in the NLP community, studying typological features has recently risen in popularity and importance for a number 1 Proceedings of the Second Workshop on Computational Research in Linguistic Typology, pages 1–11 c Online, November 19,"
2020.sigtyp-1.1,P18-1142,1,0.887137,"Missing"
2020.sigtyp-1.1,D17-1268,0,0.394103,"t causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic prope"
2020.sigtyp-1.1,D19-1288,1,0.883931,"Missing"
2020.sigtyp-1.1,I17-1046,0,0.0877507,"gly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the wor"
2020.sigtyp-1.1,N13-1126,0,0.0591155,"Missing"
2020.sigtyp-1.1,P12-1066,0,0.170714,"Missing"
2020.sigtyp-1.1,N16-1161,0,0.0680928,"Missing"
2020.sigtyp-1.1,2020.emnlp-main.368,1,0.920027,"2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Camp"
2020.sigtyp-1.1,2020.emnlp-main.187,0,0.0638701,"al for predicting features in such cases, where little or nothing is known about a language. 7 Typologically Informed Sharing Cross-lingual sharing informed by typology has been investigated for, among others, parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; de Lhoneux et al., 2018), language modeling (Tsvetkov et al., 2016; Ponti et al., 2019b), machine translation (Daiber et al., 2016; Ponti et al., 2018), and morphological inflection (Chaudhary et al., 2019). Many of these approaches use language embeddings with sparse features encoding WALS feature values. Oncevay et al. (2020) find that combining information from typological databases with embeddings learned during training of an NMT model can be beneficial for multilingual NMT. 6.3 Conclusions Acknowledgments This research has received funding from the Swedish Research Council under grant agreement No 2019-04129, as well as the German Research Foundation (DFG project number 408121292). Typological Probing Several recent papers study typological feature prediction as a probing task for evaluating cross-lingual sentence encoders (Choenni and Shutova, 2020; Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zh"
2020.sigtyp-1.1,P19-1300,0,0.0206958,"J. Mielke Aditi Chaudhary2 Giuseppe G. A. Celano Edoardo M. Ponti‡ Ekaterina Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and t"
2020.sigtyp-1.1,E17-2102,0,0.458015,"t with 5k samples each. 3.2 Baselines We provide two baselines. The first is a simple lower-bound baseline based on observing feature frequencies in WALS (Baseline frequency in Figure 2). For each unobserved feature in the test set, we predict the most frequent feature value from the training set. The second uses the k-nearest neighbours (kNN) algorithm with a simple feature set to predict each unobserved feature, with k = 1 (Baseline knn-imputation in Figure 2). Each language is represented by a language vector (~l ∈ R64 ) trained as a part of a multilingual character-based language ¨ model (Ostling and Tiedemann, 2017). During inference, for a language l and unobserved feature y, we find the nearest neighbour to ~l for which y has been observed, similar to Bjerva and Augenstein (2018a,b). 1 3 Distances calculated with WALS language locations. 3.3 Submissions We received eight submissions from five teams across the constrained and unconstrained subtasks, as described below. ´ (Vastl et al. (2020), Charles University) UFAL submitted a constrained system which ensembled two approaches: first, estimating the correlation of feature values within languages enables missing feature prediction, and second, using a n"
2020.sigtyp-1.1,D15-1213,0,0.0604974,"Missing"
2021.acl-short.17,2020.emnlp-main.263,1,0.912862,"sparsity can be tuned via the hyperparameter 2 ( 1, 1); a larger encourages more sparsity in the minimizing solution. 3 Model Interpretability Model interpretability and explainability have been framed in different ways (Gehrmann et al., 2019)— as model understanding tasks, where (spurious) features learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 202"
2021.acl-short.17,2020.acl-main.656,1,0.929314,"sparsity can be tuned via the hyperparameter 2 ( 1, 1); a larger encourages more sparsity in the minimizing solution. 3 Model Interpretability Model interpretability and explainability have been framed in different ways (Gehrmann et al., 2019)— as model understanding tasks, where (spurious) features learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 202"
2021.acl-short.17,2020.emnlp-main.747,0,0.0299302,"ures learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 2020). Further, we acknowledge the specific requirement that an interpretable model obeys some set of structural constraints of the domain in which it is used, such as monotonicity or physical constraints (Rudin, 2019). For NLP tasks such as sentiment analysis or topic classification, such const"
2021.acl-short.17,2020.lrec-1.220,0,0.1378,"of a recurrent network or contextual embeddings of a Transformer (see Fig. 1). Importantly, these internal representations do not solely encode information from the input token they are co-indexed with (Salehinejad et al., 2017; Brunner et al., 2020), but rather from a range of inputs. This presents the question: if internal representations themselves may not be interpretable, can we actually deduce anything from “interpretable” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions. We find we are unable to identify such a set w"
2021.acl-short.17,2020.acl-main.386,0,0.071391,"; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 2020). Further, we acknowledge the specific requirement that an interpretable model obeys some set of structural constraints of the domain in which it is used, such as monotonicity or physical constraints (Rudin, 2019). For NLP tasks such as sentiment analysis or topic classification, such constraints may logically include the utilization of only a few key words in the input when making a decision, in which case, knowing the magnitude of the influence each input token has on a model’s prediction through, e.g., feature importance metrics, may suffice to verify the model obeys such constraints. While"
2021.acl-short.17,N19-1357,0,0.34537,"ion internal to the model, e.g. the hidden states of a recurrent network or contextual embeddings of a Transformer (see Fig. 1). Importantly, these internal representations do not solely encode information from the input token they are co-indexed with (Salehinejad et al., 2017; Brunner et al., 2020), but rather from a range of inputs. This presents the question: if internal representations themselves may not be interpretable, can we actually deduce anything from “interpretable” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions"
2021.acl-short.17,2020.coling-main.474,0,0.0285215,"(3) where the degree of sparsity can be tuned via the hyperparameter 2 ( 1, 1); a larger encourages more sparsity in the minimizing solution. 3 Model Interpretability Model interpretability and explainability have been framed in different ways (Gehrmann et al., 2019)— as model understanding tasks, where (spurious) features learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models ("
2021.acl-short.17,P18-2059,0,0.0550343,"Missing"
2021.acl-short.17,N18-1100,0,0.0232581,"natural language processing (NLP) is becoming increasingly important as complex models are applied to more and more downstream decision making tasks. In light of this, many researchers have turned to the attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al., 2019). Yet, there lacks c"
2021.acl-short.17,P18-1032,0,0.0196337,"Missing"
2021.acl-short.17,2020.acl-main.432,0,0.0410922,"on may simply lead to the dispersion of information to different intermediate representations, a behavior similar to that seen when constraining attention for divergence from another distribution, i.e., in the adversarial experiments of Wiegreffe and Pinter (2019) compared to those of Jain and Wallace (2019). 5 unique optimal attention weights.3 Subsequently, other studies arrived at similar results: Grimsley et al. (2020) found evidence that causal explanations are not attainable from attention layers over text data; Jacovi and Goldberg (2020) explored the faithfulness of attention heatmaps; Pruthi et al. (2020) showed that attention masks can be trained to give deceptive explanations. We view this work as another such study, exploring attention’s innate interpretability on a different axis. Further, this work fits into the context of a larger body of interpretability research in NLP, which has challenged the informal use of terms such as faithfulness, plausibility, and explainability (Lipton, 2018; Arrieta et al., 2020; Jacovi and Goldberg, 2021, inter alia) and tried to quantify the reliability of current definitions (Atanasova et al., 2020a). While we consider their findings in our experimental de"
2021.acl-short.17,P19-1282,0,0.0567097,"l, e.g. the hidden states of a recurrent network or contextual embeddings of a Transformer (see Fig. 1). Importantly, these internal representations do not solely encode information from the input token they are co-indexed with (Salehinejad et al., 2017; Brunner et al., 2020), but rather from a range of inputs. This presents the question: if internal representations themselves may not be interpretable, can we actually deduce anything from “interpretable” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions. We find we are unable t"
2021.acl-short.17,P16-1008,0,0.0284336,"attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al., 2019). Yet, there lacks concrete reasoning or evidence that sparse attention weights leads to more interpretable models: customarily, attention is not directly over the model’s inputs, but rather over some representation"
2021.acl-short.17,2020.repl4nlp-1.17,0,0.0187892,"k | Inputs and Intermediate Representations are not Interchangeable. We first explore how strongly-related inputs are to their co-indexed intermediate representations. A strong relationship on its own may validate the use of sparse attention, as the ability to identify a subset of influential intermediate representations would then directly translate to a set of influential inputs. Previous works show that the “contribution” of a token xi to its intermediate representation hi is often quite low for various model architectures (Salehinejad et al., 2017; Ming et al., 2017; Brunner et al., 2020; Tutek and Snajder, 2020). In the context of attention, we find this property to be evinced by the adversarial experiments of Wiegreffe and Pinter (2019) (§4) and Jain and Wallace (2019) (§4), which we verify in App. C. They construct adversarial attention distributions by optimizing for divergence from a baseline 124 Figure 2: Entropy of gradient-based gyˆ(x) and LOO Dyˆ(x) FI distributions. Results are from models with full spectrum of projection functions. Figure 1: Correlation between the attention distribution and gradient-based FI measures. We see a notably stronger correlation between attention and FI of interm"
2021.acl-short.17,D19-1002,0,0.219077,"able” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions. We find we are unable to identify such a set when using sparse attention; rather, it appears that encouraging sparsity may simultaneously encourage a higher degree of 122 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 122–129 August 1–6, 2021. ©2021 Association for Computational Linguistics contextualization in intermediate representations."
2021.acl-short.17,P17-1088,0,0.0286273,"bility research in natural language processing (NLP) is becoming increasingly important as complex models are applied to more and more downstream decision making tasks. In light of this, many researchers have turned to the attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al.,"
2021.acl-short.17,N16-1174,0,0.0366465,"have turned to the attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al., 2019). Yet, there lacks concrete reasoning or evidence that sparse attention weights leads to more interpretable models: customarily, attention is not directly over the model’s inputs, but rather over som"
2021.eacl-main.38,N18-1083,1,0.930305,"Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a), who find that, e.g., language embeddings trained on a morphological task can encode morphological features from WALS. In contrast with previous work, we blind a model to typological information, by using adversarial 480 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 480–486 April 19 - 23, 2021. ©2021 Association for Computational Linguistics techniques based on gradient reversal (Ganin and Lempitsky, 2014). We evaluate on the structured prediction and classification tasks in XTREME (Hu et al., 2020), yielding a total of 40"
2021.eacl-main.38,W18-0207,1,0.942532,"Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a), who find that, e.g., language embeddings trained on a morphological task can encode morphological features from WALS. In contrast with previous work, we blind a model to typological information, by using adversarial 480 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 480–486 April 19 - 23, 2021. ©2021 Association for Computational Linguistics techniques based on gradient reversal (Ganin and Lempitsky, 2014). We evaluate on the structured prediction and classification tasks in XTREME (Hu et al., 2020), yielding a total of 40"
2021.eacl-main.38,N19-1156,1,0.928317,"the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020)."
2021.eacl-main.38,P19-1382,1,0.863597,"the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020)."
2021.eacl-main.38,2020.sigtyp-1.1,1,0.673001,"y to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020). In contrast to such post-hoc approaches, our experimental setting allows for me"
2021.eacl-main.38,J19-2006,1,0.908887,"the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020)."
2021.eacl-main.38,2020.acl-main.493,0,0.0236422,"op in performance when blinding a model to syntactic features, but we also observe that the α sharing weights in our model do not appear to correlate with linguistic similarities in this setting. Conversely, encouraging a model to consider typology, by jointly optimising it for typological feature prediction, improves performance in general. Furthermore, α weights in this scenario converge towards correlating with structural similarities of languages. This is in line with recent work which has found that m-BERT uses fine-grained syntactic distinctions in its crosslingual representation space (Chi et al., 2020). We interpret this as evidence for the fact that typology can be a necessity for modelling in NLP. Our results furthermore corroborate previous work in that we only find moderate benefits from including typological information explicitly. We expect that this to a large degree is due to the typological similarities of languages being encoded implicitly based on correlations between patterns in the input data. As low-resource languages often do not even have access to any substantial amount of raw text, but often do have annotations in WALS, we expect that using typological information can go s"
2021.eacl-main.38,P07-1009,0,0.150682,"Missing"
2021.eacl-main.38,N19-1423,0,0.0199481,"l from exploiting typology severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to typology somewhat improves performance. 1 Figure 1: A PoS tagger is exposed (or blinded with gradient reversal, −λ) to typological features. Observing α values tells us how typology affects sharing. Introduction Most languages in the world have little access to NLP technology due to data scarcity (Joshi et al., 2020). Nonetheless, high-quality multilingual representations can be obtained using only a raw text signal, e.g. via multilingual language modelling (Devlin et al., 2019). Furthermore, structural similarities of languages are to a large extent documented in typological databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NLP models, our work differs significantly from such efforts in that we blind a model to this information. Most previous work includes language information as features"
2021.eacl-main.38,D18-1029,0,0.199366,"Missing"
2021.eacl-main.38,2020.acl-main.560,0,0.011474,"on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this model from exploiting typology severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to typology somewhat improves performance. 1 Figure 1: A PoS tagger is exposed (or blinded with gradient reversal, −λ) to typological features. Observing α values tells us how typology affects sharing. Introduction Most languages in the world have little access to NLP technology due to data scarcity (Joshi et al., 2020). Nonetheless, high-quality multilingual representations can be obtained using only a raw text signal, e.g. via multilingual language modelling (Devlin et al., 2019). Furthermore, structural similarities of languages are to a large extent documented in typological databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NL"
2021.eacl-main.38,D18-1543,1,0.903419,"Missing"
2021.eacl-main.38,D17-1268,0,0.0180311,"ypological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020). In contrast to such post-hoc approaches, our experimental setting allows for measuring the impact of typology on crosslingual sharing performance in a direct manner as part of the model architecture. Syntactic Features We first blind/expose the model to syntactic features from WALS (Dryer and Haspelmath, 2013). We take the set of word order features which are annotated for all languages in our experiments, resulting in 33 features. This includes features such as 81A: Order of Subject, Object and Ver"
2021.eacl-main.38,I17-1046,0,0.0196742,"fixing morphology. We hypothesise that mainly the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 201"
2021.eacl-main.38,2020.emnlp-main.368,1,0.834918,"jerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020). In contrast to such post-hoc approaches, our experimental setting allows for measuring the impact of typology on crosslingual sharing performance in a direct manner as part of the model architecture. Syntactic Features We first blind/expose the model to syntactic features from WALS (Dryer and Haspelmath, 2013). We take the set of word order features which are annotated for all languages in our experiments, resulting in 33 features. This includes features such as 81A: Order of Subject, Object and Verb, which encodes what the preferred word ordering is (if any) in a transit"
2021.eacl-main.38,C16-1123,0,0.354646,"Missing"
2021.eacl-main.38,2020.emnlp-main.187,0,0.0108029,"tures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NLP models, our work differs significantly from such efforts in that we blind a model to this information. Most previous work includes language information as features, by using language IDs, or language embeddings (e.g. Ammar et al. (2016); O’Horan et al. (2016); ¨ Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a), who find that, e.g., language embeddings"
2021.eacl-main.38,E17-2102,0,0.156223,"databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NLP models, our work differs significantly from such efforts in that we blind a model to this information. Most previous work includes language information as features, by using language IDs, or language embeddings (e.g. Ammar et al. (2016); O’Horan et al. (2016); ¨ Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a"
2021.eacl-main.38,J19-3005,0,0.155697,"Missing"
2021.emnlp-main.28,P07-1056,0,0.153821,"ine two sources — (a) lexica and (b) pivot words. For the first, we use the same lexica for understanding the construct-driven modification strategies in RQ2, i.e., affect words for sentiment, gender words for sexism, and identity-based hate words for hate speech. Note that, while for sentiment, we have a list of core features for both classes, for sexism and hate speech, we only have core features for the positive class for sexist and hate cases, and not for non-sexist and non-hate cases. For the second source, we turn to the literature on domain adaptation, particularly work on pivot words (Blitzer et al., 2007). Concretely, for a given construct, we 5 Related Work find words that are highly frequent in both domains; Our work connects the area of learning with counthen find their correlation with the out-of-domain terfactuals to improve NLP models’ robustness dataset labels to reduce the inclusion of in-domain with the area of social NLP. artifacts. We rank these words based on mutual information and use the first 100 words as a set Counterfactuals in NLP. Counterfactuals in of core features. The list of pivot words is in Ap- NLP have been used for model testing, and ex331 planation, but in this work"
2021.emnlp-main.28,2020.acl-main.485,0,0.0332332,", subjective and ambiguous (Vidgen et al., 2019; Jurgens et al., 2019; Nakov et al., 2021), where misclassifications can cause harm (Blackwell et al., 2017). We use lexica to determine core features of sexism or hate speech, but we acknowledge that both of these may manifest in context-dependent ways and there is no single objective determinant of hate speech or sexism (or even sentiment). Furthermore, promoting features like identity terms can increase the risk of misclassifying non-hate content with such terms, such as disclosure or reports of facing hate speech, leading to unintended bias (Blodgett et al., 2020). We do not undertake any further data generation or data annotation by human subjects, as we use data made available by previous researchers and use lexica for annotating counterfactual types. Nonetheless, as we show the potential of CAD in improving some aspects of model robustness, we hope that the community will adopt annotation guidelines that factor in the risk of harm that annotators and CAD designers working on abusive language might face (Vidgen and Derczynski, 2020). We aim to understand how CAD improves model robustness, but we acknowledge and caution that these types of data augmen"
2021.emnlp-main.28,N19-1423,0,0.0260545,"ask on sexism dedomain datasets we consider come with counter- tection (Ródriguez-Sánchez et al., 2021) available at factually augmented data, annotated by trained http://nlp.uned.es/exist2021/ 327 domain dataset. Note that due to the nature of these perturbations, adversarial data can only be generated for a subset of the training data, e.g., if an example does not contain any named entities, then we cannot generate an adv_swap version of it. 3.2 mode construct sentiment Text Classification Methods. We use two different text classification models: logistic regression (LR) and finetuned-BERT (Devlin et al., 2019). We do so as we want to contrast a basic model trained from scratch, which only learns simple features directly observed in the dataset (LR); and one which encodes a combination of background knowledge and application dataset knowledge, and is capable of learning complex inter-dependencies between features (BERT). We train LR with TF-IDF bag-of-words feature representations using sklearn (Pedregosa et al., 2011), while the BERT base model is used for finetuning in conjunction with the subword tokenizer using HuggingFace Transformers (Wolf et al., 2020). Each model is trained using 5-fold cros"
2021.emnlp-main.28,D19-1461,0,0.0246718,"2021; Madaan et al., 2021). Concurrent and closely related to our work, Joshi and He assess the efficacy of CAD for Natural Language Inference and Question Answering, and find that diverse CAD is crucial for improving generalizability, in line with our current work. On the other hand, CAD generated by human annotators has not been analyzed in detail to see which strategies are used for generating counterfactuals nor which strategies are more effective, particularly for social computing NLP tasks. 2016). Several solutions have been proposed for these issues such as adversarial data generation (Dinan et al., 2019), dynamic benchmarking (Kiela et al., 2021) and debiasing techniques (Nozza et al., 2019). Building on these threads of research, we aim to understand the benefits of different types and proportions of CAD in training social NLP models. 6 Discussion: Designing Counterfactually Augmented Data NLP models are now embedded in many real-world applications and understanding their limits and robustness is of the utmost importance, especially for social computing applications. In this work, using a detailed and systematic set of analyses we establish convergent validity of the use of counterfactually"
2021.emnlp-main.28,N18-1170,0,0.0215285,"work, we are interested in using them for training models. Counterfactuals can be used for augmenting training data where previous research, focused on sentiment and NLI, has shown models trained on this augmented data are more robust to data artifacts (Kaushik et al., 2020; Teney et al., 2020). Counterfactuals need not always be label-flipping, but usually entail making minimal changes to original data either, and can be generated by manually or automatically (Nie et al., 2020). Recent work has also addressed automatic CAD generation through lexical or paraphrase changes (Garg et al., 2019; Iyyer et al., 2018), templates (Nie et al., 2020), and controlled text generation (Wu et al., 2021; Madaan et al., 2021). Concurrent and closely related to our work, Joshi and He assess the efficacy of CAD for Natural Language Inference and Question Answering, and find that diverse CAD is crucial for improving generalizability, in line with our current work. On the other hand, CAD generated by human annotators has not been analyzed in detail to see which strategies are used for generating counterfactuals nor which strategies are more effective, particularly for social computing NLP tasks. 2016). Several solution"
2021.emnlp-main.28,N19-1357,0,0.0605985,"Missing"
2021.emnlp-main.28,P19-1357,0,0.312659,"r social NLP tasks— do they reduce dependence on spurious features and to what extent? This work. We analyze how CAD affects social NLP models. Unlike previous work, we leverage multiple, related social computing constructs to avoid confounds that may arise due to the specific settings of a single construct. We conduct our experiments on three text classification tasks: sentiment, sexism, and hate speech identification. Sentiment has been thoroughly analyzed in past NLP robustness work, and abusive content has been widely studied in NLP (Schmidt and Wiegand, 2017; Vidgen and Derczynski, 2020; Jurgens et al., 2019; Sarwar et al., 2021). However, sexism and hate speech have not been studied in as much detail in the specific context of the impact of training on CAD. The multifaceted nature of these constructs warrants further investigation, especially in the context of developing models with less spurious features. First, we ask: (RQ1) do models trained on CAD outperform models trained on original, unaltered data? We assess the overall performance of these two types of models and find that while models trained on original data outperform those trained on CAD in-domain, the opposite is true out-of-domain—"
2021.emnlp-main.28,2020.emnlp-demos.16,0,0.081335,"Missing"
2021.emnlp-main.28,2020.acl-main.441,0,0.0229088,"actuals in of core features. The list of pivot words is in Ap- NLP have been used for model testing, and ex331 planation, but in this work, we are interested in using them for training models. Counterfactuals can be used for augmenting training data where previous research, focused on sentiment and NLI, has shown models trained on this augmented data are more robust to data artifacts (Kaushik et al., 2020; Teney et al., 2020). Counterfactuals need not always be label-flipping, but usually entail making minimal changes to original data either, and can be generated by manually or automatically (Nie et al., 2020). Recent work has also addressed automatic CAD generation through lexical or paraphrase changes (Garg et al., 2019; Iyyer et al., 2018), templates (Nie et al., 2020), and controlled text generation (Wu et al., 2021; Madaan et al., 2021). Concurrent and closely related to our work, Joshi and He assess the efficacy of CAD for Natural Language Inference and Question Answering, and find that diverse CAD is crucial for improving generalizability, in line with our current work. On the other hand, CAD generated by human annotators has not been analyzed in detail to see which strategies are used for g"
2021.emnlp-main.28,2020.emnlp-main.199,0,0.0715326,"Missing"
2021.emnlp-main.28,2020.emnlp-demos.6,0,0.0978481,"Missing"
2021.emnlp-main.28,W19-3509,0,0.25406,"the affect-laden word in sentiment. On the other hand, construct-agnostic CAD are generated by indirectly acting on the construct, through general-purpose strategies such as introducing sarcasm or negation which yields CAD for several constructs (see Table 5). Since construct-driven CAD directly act on the construct, we hypothesize that construct-driven strategies are more effective. To determine which instances represent which modification strategy, we use a simple lexiconbased automatic annotation strategy. Based on strategies manually assessed in previous literature (Kaushik et al., 2020; Vidgen et al., 2019), we devise 5 specific strategies — affect, gender, identity, hedges, and negation. The first three are construct-driven strategies for sentiment, sexism, and hate speech, respectively, while the last two are construct-agnostic. 4 We use a set of lexica for discerning each strategy — a lexicon of positive and negative words for affect (Hu and Liu, 2004),5 list of gender words 6 and a list of identity-based hateful terms and slurs (Silva et al., 2016).7 For negation, we use the list compiled by Ribeiro et al. (2020) and for hedges, we use Islam et al. (2020). Table 5 enumerates the different ty"
2021.emnlp-main.28,2021.acl-long.523,0,0.196465,"used for augmenting training data where previous research, focused on sentiment and NLI, has shown models trained on this augmented data are more robust to data artifacts (Kaushik et al., 2020; Teney et al., 2020). Counterfactuals need not always be label-flipping, but usually entail making minimal changes to original data either, and can be generated by manually or automatically (Nie et al., 2020). Recent work has also addressed automatic CAD generation through lexical or paraphrase changes (Garg et al., 2019; Iyyer et al., 2018), templates (Nie et al., 2020), and controlled text generation (Wu et al., 2021; Madaan et al., 2021). Concurrent and closely related to our work, Joshi and He assess the efficacy of CAD for Natural Language Inference and Question Answering, and find that diverse CAD is crucial for improving generalizability, in line with our current work. On the other hand, CAD generated by human annotators has not been analyzed in detail to see which strategies are used for generating counterfactuals nor which strategies are more effective, particularly for social computing NLP tasks. 2016). Several solutions have been proposed for these issues such as adversarial data generation (Dina"
2021.emnlp-main.28,W16-5618,0,0.0281769,"Missing"
2021.emnlp-main.28,D19-1670,0,0.0392353,"Missing"
2021.emnlp-main.710,N18-2004,1,0.842859,"(Mohammad et al., 2017; Aldayel and Magdy, 2019). Debating platforms were used as data source for stance (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain le"
2021.emnlp-main.710,N19-1053,0,0.110634,"ion techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out-ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance. 1 Introduction vs. long news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016) vs. news outlets (Stefanov et al., 2020) vs. people (Darwish et al., 2020), (ii) with respect to a claim (Chen et al., 2019) vs. a topic, either explicit (Qazvinian et al., 2011; Walker et al., 2012) or implicit (Hasan and Ng, 2013; Gorrell et al., 2019). Moreover, there is substantial variation in (iii) the label inventory, in the exact label definition, in the data collection, in the annotation setup, in the domain, etc. The most crucial of these, which has not been investigated, currently preventing cross-domain studies, is that the label inventories differ between the settings, as shown in Table 1. Labels include not only variants of agree, disagree, and unrelated, but also difficult to cross-map ones, such as"
2021.emnlp-main.710,P07-1033,0,0.456725,"Missing"
2021.emnlp-main.710,N19-1423,0,0.0104585,"is set to a small positive number to prevent this regulariser from dominating the overall loss. We set γ to 0.01. Furthermore, since our dataset is quite diverse even in the four source domains that we outlined, we optimise the domain-adaptive loss towards a meta-class for each dataset, instead of the domain. 5 Experiments Baselines Logistic Regression A logistic regression trained using TF.IDF word unigrams. The input is a concatenation of the target and context vectors. Multi-task learning (MTL) A single projection layer for each dataset is added on top of a pretrained language model (BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019)). We then pass the [CLS] token representations through the datasetspecific layer. Finally, we propagate the errors only through that layer (and the base model), without updating parameters for other datasets. 5.2 Evaluation Results In-Domain Experiments We train and test on all datasets; the results are in Table 5. First, to find the best base model and set a baseline for MoLE, we evaluate two strong models: BERTBase uncased (Devlin et al., 2019), and RoBERTaBase cased6 (Liu et al., 2019). On our 16 datasets, RoBERTa outperforms BERT by 2 F1 points absolute on av"
2021.emnlp-main.710,N16-1138,0,0.533109,"end-to-end unsupervised framework for outof-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out-ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance. 1 Introduction vs. long news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016) vs. news outlets (Stefanov et al., 2020) vs. people (Darwish et al., 2020), (ii) with respect to a claim (Chen et al., 2019) vs. a topic, either explicit (Qazvinian et al., 2011; Walker et al., 2012) or implicit (Hasan and Ng, 2013; Gorrell et al., 2019). Moreover, there is substantial variation in (iii) the label inventory, in the exact label definition, in the data collection, in the annotation setup, in the domain, etc. The most crucial of these, which has not been investigated, currently preventing cross-domain studies, is that the label inventories differ between the settings, as shown i"
2021.emnlp-main.710,N09-1068,0,0.0598913,"Missing"
2021.emnlp-main.710,2020.acl-main.157,0,0.0330081,"Missing"
2021.emnlp-main.710,2021.acl-long.127,0,0.0829382,"Missing"
2021.emnlp-main.710,C18-1158,0,0.183785,"Missing"
2021.emnlp-main.710,K19-1046,0,0.0167675,"gdy, 2019). Debating platforms were used as data source for stance (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain eval"
2021.emnlp-main.710,N18-1070,1,0.816523,"Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain evaluation. Another direction is data-centric adaptation: Han and Eisenstein (2019); Rietzler et al. (2020) used MLM fin"
2021.emnlp-main.710,D19-1452,1,0.945163,", 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain evaluation. Another direction is data-centric adaptation: Han and Eisenstein (2019); Rietzler et al. (2020) used MLM fine-tuning on target-domain data. Gururangan et al. (2020) showed alternate domain-adaptive (in-domain data) and task-adaptive (out-of-domain unlabelled data) pre-training. Label Embeddings Label embeddings can capture, in an unsupervised fashion, the complex relations between target labels for multiple datasets or tasks. They can boost"
2021.emnlp-main.710,D14-1162,0,0.0841684,"Missing"
2021.emnlp-main.710,D11-1147,0,0.0543127,"Missing"
2021.emnlp-main.710,2020.lrec-1.607,0,0.035895,"al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain evaluation. Another direction is data-centric adaptation: Han and Eisenstein (2019); Rietzler et al. (2020) used MLM fine-tuning on target-domain data. Gururangan et al. (2020) showed alternate domain-adaptive (in-domain data) and task-adaptive (out-of-domain unlabelled data) pre-training. Label Embeddings Label embeddings can capture, in an unsupervised fashion, the complex relations between target labels for multiple datasets or tasks. They can boost the end-task performance for various deep learning architectures, e.g., CNNs (Zhang et al., 2018; Pappas and Henderson, 2019), RNNs (Augenstein et al., 2018, 2019), and Transformers (Chang et al., 2020). Recent work has proposed different perspective"
2021.emnlp-main.710,D18-1131,1,0.824208,"te the source groupings used in our experiments and analysis (Section 3.3). Domain Adaptation Domain adaptation was studied in supervised settings, where in addition to the source-domain data, a (small) amount of labeled data in the target domain is also available (Daumé III, 2007; Finkel and Manning, 2009; 3.1 Datasets Donahue et al., 2013; Yao et al., 2015; Mou et al., 2016; Lin and Lu, 2018), and in unsupervised set- arc The Argument Reasoning Comprehension dataset has posts from the New York Times debate tings, without labeled target-domain data (Blitzer et al., 2006; Lipton et al., 2018; Shah et al., 2018; section on immigration and international affairs. argmin The Argument Mining corpus presents arMohtarami et al., 2019; Bjerva et al., 2020; Wright and Augenstein, 2020). Recently, domain adap- guments relevant to a particular topic from heterogenous texts. Topics include controversial keytation was applied to pre-trained Transformers words like death penalty and gun control. (Lin et al., 2020). One direction therein are aremergent The Emergent2 dataset is a collection of chitectural changes (method-centric): Ma et al. (2019) proposed curriculum learning with domain- articles from rumour site"
2021.emnlp-main.710,W15-0509,0,0.0250797,"y, we release our code, models, and data.1 substantial differences in the settings, e.g., stance 1 (i) expressed in tweets (Qazvinian et al., 2011; The datasets and code are available for research purposes: Mohammad et al., 2016; Conforti et al., 2020b) https://github.com/checkstep/mole-stance 9011 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9011–9028 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Related Work Stance Detection Prior work on stance explored its connection to argument mining (Boltuži´c and Šnajder, 2014; Sobhani et al., 2015), opinion mining (Wang et al., 2019), and sentiment analysis (Mohammad et al., 2017; Aldayel and Magdy, 2019). Debating platforms were used as data source for stance (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al.,"
2021.emnlp-main.710,E17-2088,0,0.0583081,"Missing"
2021.emnlp-main.710,walker-etal-2012-corpus,0,0.015902,"with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out-ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance. 1 Introduction vs. long news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016) vs. news outlets (Stefanov et al., 2020) vs. people (Darwish et al., 2020), (ii) with respect to a claim (Chen et al., 2019) vs. a topic, either explicit (Qazvinian et al., 2011; Walker et al., 2012) or implicit (Hasan and Ng, 2013; Gorrell et al., 2019). Moreover, there is substantial variation in (iii) the label inventory, in the exact label definition, in the data collection, in the annotation setup, in the domain, etc. The most crucial of these, which has not been investigated, currently preventing cross-domain studies, is that the label inventories differ between the settings, as shown in Table 1. Labels include not only variants of agree, disagree, and unrelated, but also difficult to cross-map ones, such as discuss and question. Our goal in this paper is to design a common stance d"
2021.emnlp-main.845,D19-1475,1,0.89635,"Missing"
2021.emnlp-main.845,D16-1084,1,0.870604,"Missing"
2021.emnlp-main.845,N18-2004,0,0.0278253,"Missing"
2021.emnlp-main.845,2021.emnlp-main.594,0,0.0472679,"Missing"
2021.emnlp-main.845,2021.emnlp-main.710,1,0.85423,"PET compared to performance of MT-PET using 200 instances from T2 and 100 from T1. Additionally, MT-PET performs at or above supervised performance up to 1000 input samples, and at or above PET up to 500 samples, again using only 200 samples from T2 and 100 from T1. 5.2 Error Analysis Related Work Scientific Misinformation Detection Misinformation detection focuses on a variety of problems, including fact verification (Thorne et al., 2018; Augenstein et al., 2019), check-worthiness detection (Wright and Augenstein, 2020; Nakov et al., 2021), stance (Augenstein et al., 2016; Baly et al., 2018; Hardalov et al., 2021a) and clickbait detection (Potthast et al., 2018). While most work has focused on social media and general domain text, recent work has begun to explore different problems in detecting misinformation in scientific text such as SciFact (Wadden et al., 2020) and CiteWorth (Wright and Augenstein, 2021), as well as related tasks such as summarization (DeYoung et al., 2021; Dangovski et al., 2021). Most work on scientific exaggeration detection has focused on flagging when the primary finding of a scientific paper has been exaggerated by a press release or news article (Sumner et al., 2014; Bratto"
2021.emnlp-main.845,W17-4219,0,0.359773,"al., 2014; Bratton et al., 2019; Woloshin et al., 2009; Woloshin and 1 The code and data are available online at https://github.com/copenlu/ scientific-exaggeration-detection Schwartz, 2002). In this paper, we explore how natural language processing can help identify exaggerations of scientific papers in press releases. While Sumner et al. (2014) and Bratton et al. (2019) performed manual analyses to understand the prevalence of exaggeration in press releases of scientific papers from a variety of sources, recent work has attempted to expand this using methods from NLP (Yu et al., 2019, 2020; Li et al., 2017). These works focus on the problem of automatically detecting the difference in the strength of causal claims made in scientific articles and press releases. They accomplish this by first building datasets of main claims taken from PubMed abstracts and (unrelated) press releases from EurekAlert2 labeled for their strength. With this, they train machine learning models to predict claim strength, and analyze unlabelled data using these models. This marks an important first step toward the goal of automatically identifying exaggerated scientific claims in science reporting. However, existing work"
2021.emnlp-main.845,2021.ccl-1.108,0,0.0302077,"Missing"
2021.emnlp-main.845,W19-5034,0,0.029361,"Missing"
2021.emnlp-main.845,C18-1127,0,0.0471967,"Missing"
2021.emnlp-main.845,2020.emnlp-main.609,0,0.0515851,"Missing"
2021.emnlp-main.845,2020.coling-main.488,0,0.0423481,"king them unsuitable for benchmarking exaggeration detection. Given this, we curate a dataset of paired sentences from abstracts and associated press releases, labeled by experts for exaggeration based on their claim strength. We then collect a large set of unlabeled press release/abstract pairs useful for semi-supervised learning. We use MT-PET to learn from data labeled for both of our formulations of the problem (T1, T2). In this, the first step is to define PVPs for exaggeration detection (T1) and claim strength prediction (T2). To do this, we develop an initial set of PVPs and use PETAL (Schick et al., 2020) to automatically find verbalizers which adequately represent the labels for each task. We then update the patterns manually and re-run PETAL, iterating as such until we find a satisfactory combination of verbal- 4.1 Gold Data izers and patterns which adequately reflect the task. The gold test data used in this work are from SumAdditionally, we ensure that the patterns between ner et al. (2014) and Bratton et al. (2019), who T1 and T2 are roughly equivalent. This yields 2 annotate scientific papers, their abstracts, and asso10827 Pattern Label Downplays PT01 Same Exaggerates Downplays PT11 Sam"
2021.emnlp-main.845,2020.findings-emnlp.43,1,0.879682,"Missing"
2021.emnlp-main.845,2021.findings-acl.157,1,0.811616,"Missing"
2021.emnlp-main.845,D19-1473,0,0.251587,"ic articles (Sumner et al., 2014; Bratton et al., 2019; Woloshin et al., 2009; Woloshin and 1 The code and data are available online at https://github.com/copenlu/ scientific-exaggeration-detection Schwartz, 2002). In this paper, we explore how natural language processing can help identify exaggerations of scientific papers in press releases. While Sumner et al. (2014) and Bratton et al. (2019) performed manual analyses to understand the prevalence of exaggeration in press releases of scientific papers from a variety of sources, recent work has attempted to expand this using methods from NLP (Yu et al., 2019, 2020; Li et al., 2017). These works focus on the problem of automatically detecting the difference in the strength of causal claims made in scientific articles and press releases. They accomplish this by first building datasets of main claims taken from PubMed abstracts and (unrelated) press releases from EurekAlert2 labeled for their strength. With this, they train machine learning models to predict claim strength, and analyze unlabelled data using these models. This marks an important first step toward the goal of automatically identifying exaggerated scientific claims in science reporting"
2021.emnlp-main.845,2020.coling-main.427,0,0.501782,"second (defined as T2) is as a classification task consisting of a training set of documents d ∈ D0 from both the source and the target domain, and a classifier is trained to predict the claim strength l0 of sentences from these documents. In other words, we don’t require paired documents (t, s) at train time. At test time, these classifiers are then applied to document pairs (t, s) and the predicted claim strengths (ls0 , lt0 ) are compared to get the final label l. Previous work has used this formulation to estimate the prevalence of correlation to causation exaggeration in press releases (Yu et al., 2020), but have not evaluated this on paired labeled instances. Following previous work (Yu et al., 2020), we simplify the problem by focusing on detecting when the main finding of a paper is exaggerated. The first step is then to identify the main finding from s, and the sentence describing the main finding in s from t. In our semi-supervised approach, we do this as an intermediate step to acquire unlabeled data, but for all labeled training and test data, we assume the sentences are already identified and evaluate on the sentence-level exaggeration detection task. We first provide a formal defini"
2021.emnlp-main.845,N18-1074,0,0.0659624,"Missing"
2021.findings-acl.157,2020.acl-main.740,0,0.0117327,"or the ACLARC dataset (including 2 points F1 improvement over the minumum and maximum model performance of SciBERT) and 0.5 F1 points on SciCite. The best average performance is from the model which incorporates both MLM and cite-worthiness as an objective, which we call C ITE B ERT.6 For other tasks, fine-tuning the language model on C ITE W ORTH data tends to be sufficient for improving performance, though the margin of improvement tends to be minimal. This is in line with previous work reporting that language model finetuning on in-domain data leads to improvements on end-task fine-tuning (Gururangan et al., 2020). C ITE W ORTH is relatively small compared to the corpus on which SciBERT is originally trained (30.7M tokens for the train and dev splits on which we train versus 3.1B), so one could potentially see further improvements by incorporating more data or including cite-worthiness as an auxiliary task during language model pre-training. However, this is outside the scope of this work. 7 Conclusion In this work, we present an in-depth study into the problem of cite-worthiness detection in English. We rigorously curate C ITE W ORTH, a high-quality dataset for cite-worthiness detection; present a 6 W"
2021.findings-acl.157,W04-1213,0,0.362515,"Missing"
2021.findings-acl.157,D18-1360,0,0.0254502,"iliary task in a multi-task setup (Cohan et al., 2019). However, cite-worthiness detection has not been studied in a transfer learning setup as a pretraining task for multiple scientific text problems. In this work, we seek to understand to what extent cite-worthiness detection is a transferable task. Scientific Document Understanding Numerous problems related to scientific document understanding have been studied previously. Popular tasks include named entity recognition (Li et al., 2016; Kim et al., 2004; Do˘gan et al., 2014; 1 http://www.comp.nus.edu.sg/~sugiyama/ SchPaperRecData.html 1797 Luan et al., 2018) and linking (Wright et al., 2019), keyphrase extraction (Augenstein et al., 2017; Augenstein and Søgaard, 2017), relation extraction (Kringelum et al., 2016; Luan et al., 2018), dependency parsing (Kim et al., 2003), citation prediction (Holm et al., 2020), citation intent classification (Jürgens et al., 2018; Cohan et al., 2019), summarization (Collins et al., 2017), and fact checking (Wadden et al., 2020). Datasets for scientific document understanding tasks tend to be limited in size and restricted to only one or a few fields, making it difficult to build models with which one can study cr"
2021.findings-acl.157,W19-5034,0,0.0406302,"in Context Metric As we are interested in using sentence context for prediction, we perform extraction at the paragraph level, ensuring that all of the sentences in a given paragraph meet the checks given in §3.1. As such, our dataset construction pipeline for a given paper begins by first extracting all paragraphs from the body text which belong to sections with titles coming from a constrained list of permissible titles (e.g. “Introduction,” “Methods,” “Discussion”) . The full list is provided in Appendix A. For a given paragraph, we first word and sentence tokenize the text with SciSpacy (Neumann et al., 2019). Each sentence is then checked for containing citations using the provided citation spans in the S2ORC dataset. In some cases, the sentence contains citations which were missed by S2ORC; these are checked using regular expressions (see Appendix B). If a match is found the paragraph is ignored, as we only consider paragraphs where all citations have been extracted by S2ORC. Otherwise, the location and format of the citation is checked, again using regular expressions (see Appendix B). If the citation is not at the end of the sentence, the paragraph is ignored. We then remove the citation text"
2021.findings-acl.157,P18-1019,0,0.0456055,"Missing"
2021.findings-acl.157,2020.emnlp-main.609,0,0.127541,"Missing"
2021.findings-acl.157,2020.findings-emnlp.43,1,0.40813,"al., 2019) due to the strong performance of large pretrained Transformer models on downstream tasks. SciBERT SciBERT (Beltagy et al., 2019) is a BERT model pretrained on a large corpus of scientific text from Semantic Scholar (Ammar et al., 2018), and is therefore potentially better suited to fine-tuning on scientific cite-worthiness detection. SciBERT + PU Learning We experiment with SciBERT trained using positive-unlabelled (PU) learning (Elkan and Noto, 2008) which has been shown to significantly improve performance on citation needed detection in Wikipedia and rumour detection on Twitter (Wright and Augenstein, 2020a). The intuition behind PU learning is to assume that cite-worthy data is labelled and noncite-worthy data is unlabelled, containing some cite-worthy examples. This is to mitigate the subjectivity involved in adding citations to sentences. Technically, this involves training a classifier on the positive-unlabeled data which will predict the probability that a sample is labeled, and using this to estimate the probability that a sample is positive given that it is unlabeled. One then trains a second model where positive samples are trained on normally and unlabeled samples are duplicated and tr"
2021.findings-acl.157,2020.emnlp-main.639,1,0.794095,"al., 2019) due to the strong performance of large pretrained Transformer models on downstream tasks. SciBERT SciBERT (Beltagy et al., 2019) is a BERT model pretrained on a large corpus of scientific text from Semantic Scholar (Ammar et al., 2018), and is therefore potentially better suited to fine-tuning on scientific cite-worthiness detection. SciBERT + PU Learning We experiment with SciBERT trained using positive-unlabelled (PU) learning (Elkan and Noto, 2008) which has been shown to significantly improve performance on citation needed detection in Wikipedia and rumour detection on Twitter (Wright and Augenstein, 2020a). The intuition behind PU learning is to assume that cite-worthy data is labelled and noncite-worthy data is unlabelled, containing some cite-worthy examples. This is to mitigate the subjectivity involved in adding citations to sentences. Technically, this involves training a classifier on the positive-unlabeled data which will predict the probability that a sample is labeled, and using this to estimate the probability that a sample is positive given that it is unlabeled. One then trains a second model where positive samples are trained on normally and unlabeled samples are duplicated and tr"
2021.findings-acl.157,2020.acl-main.447,0,0.553824,"nt over SciBERT which considers only individual sentences. Finally, we demonstrate that language model fine-tuning with cite-worthiness as a secondary task leads to improved performance on downstream scientific document understanding tasks. 1 Introduction Building effective NLP systems from scientific text is challenging due to the highly domain-specific and diverse nature of scientific language, and a lack of abundant sources of labelled data to capture this. While large scale repositories of extracted, structured, and unlabelled plain-text scientific documents have recently been introduced (Lo et al., 2020), most datasets for downstream tasks such as named entity recognition (Li et al., 2016) and citation intent classification (Cohan et al., 2019) remain limited in size and highly domain specific. This begs the question: what useful training signals can be automatically extracted from massive unlabelled scientific text corpora to help improve systems for scientific document processing? Scientific documents contain much inherent structure (sections, tables, equations, citations, etc.), which can facilitate creating large labelled datasets. Some recent examples include using paper field (Beltagy e"
2021.sdp-1.1,D19-1371,0,0.0416291,"Missing"
2021.sdp-1.1,N18-3011,0,0.0565743,"Missing"
2021.sdp-1.1,N18-1149,0,0.0654513,"Missing"
2021.sdp-1.1,2020.nlp4if-1.1,0,0.0416699,"ed by urbanization [1]. Remote sensing images are important data sources that can efficiently detect land changes. Meanwhile, remote sensing image-based change detection is the change identification of surficial objects or geographic phenomena through the remote observation of two or more different phases [2]. Table 1: Excerpts from training samples in C ITE W ORTH (Wright and Augenstein, 2021) from the Biology and Computer Science fields. Green sentences are cite-worthy sentences, from which citation markers are removed during dataset construction. tecting outright scientific misinformation (Vijjali et al., 2020; Lima et al., 2021). Here, we highlight two important and so far understudied tasks to address issues with such smaller nuances of untrustworthy scientific writing, which can come into play at different stages of the life cycle of scientific research. The first one is cite-worthiness detection, which is about detecting whether or not a sentence ought to contain a citation to prior work. This task could help to ensure that claims are not made without supporting evidence, i.e. support researchers in writing more trustworthy scientific publications. The second task is exaggeration detection, whi"
2021.sdp-1.1,2020.inlg-1.44,0,0.0867892,"Missing"
2021.sdp-1.1,W17-4219,0,0.027039,"ly fine-tuned SciBERT (Beltagy et al., 2019) by over 5 points in F1. ner et al., 2014; Bratton et al., 2019; Woloshin et al., 2009; Woloshin and Schwartz, 2002). Exaggeration can mean a sensationalised take-away of the applicability of the work in terms, i.e. giving advice for which there is no scientific basis. Moreover, the strength of the main causal claims and conclusions of a paper can be exaggerated. Table 2 shows examples of those two types of claims from the datasets curated by Sumner et al. (2014) and Bratton et al. (2019), which we use in our work. Prior work (Yu et al., 2019, 2020; Li et al., 2017) uses datasets based on PubMed abstracts and paired press releases from EurekAlert.1 Their core limitations of is that they are limited to only observational studies from PubMed, which have structured abstracts, which strongly simplifies the task of identifying the main claims of a paper. This also holds for the test settings they consider, meaning that the proposed models have a limited applicability. By contrast, we study how to best identify exaggerated claims in popular science communication in the wild, without highly curated data with annotations about core claims. This represents a more"
2021.sdp-1.1,2020.acl-main.447,0,0.0224099,"nd Physics. Given this dataset, we then study: how citeworthy sentences can be detected automatically; to what degree there are domain shifts between how different fields use citations; and if cite-worthiness data can be used to perform transfer learning to downstream scientific text tasks. Cite-Worthiness Detection The C ITE W ORTH Dataset To study citeworthiness detection, we first introduce a new rigorously curated dataset, C ITE W ORTH (Wright and Augenstein, 2021), for cite-worthiness detection from scientific articles. It is created from S2ORC, the Semantic Scholar Open Research Corpus (Lo et al., 2020). C ITE W ORTH consists of 1.2M sentences, balanced across 10 diverse scientific fields. While others have studied this task for few and/or narrow domains (Sugiyama et al., 2010; Färber Methods for Cite-Worthiness Detection We find that the best performance can be achieved by a Longformer-based model (Beltagy et al., 2020), which encodes entire paragraphs in papers and jointly predicts cite-worthiness labels for each of the sentences contained in the paragraph. Additional gains in recall can be achieved by using positive unlabelled learning, as documented in Wright and Augenstein (2020a) for t"
2021.sdp-1.1,2020.findings-emnlp.43,1,0.935571,", which is to determine whether a statement describing the findings of a scientific study exaggerates them, e.g. by claiming that two variables are strongly correlated when in reality they only co-occur. We argue that this task could be useful to verify if popular science reporting faithfully describes scientific research, or also to determine whether citation sentences (sentences which contain a citation; also called citances) faithfully describe the research documented in the cited papers. 2.1 et al., 2018), and have also studied very related tasks, such as claim check-worthiness detection (Wright and Augenstein, 2020a) or citation recommendation (Jürgens et al., 2018), this is the largest and most diverse dataset for this task to date. An excerpt of our introduced dataset, C ITE W ORTH can be found in Table 1. The dataset curation process involves: 1) data filtering, to identify credible papers with relevant metadata such as venue information; 2) citation span identification and masking, of which we only keep papers with citation spans at the end of sentences to avoid rendering sentences ungrammatical; 3) discarding paragraphs without citations, or where not all sentences have citation spans in accordance"
2021.sdp-1.1,2020.acl-main.464,0,0.018981,"n automatically generating peer reviews from paper content (Wang et al., 2020), as well as on studying how well review scores can be predicted from review texts (Kang et al., 2018; Plank and van Dalen, 2019). Finally, post-publication, the impact of scientific work can be tracked, using citations and citation counts as a proxy for this. It is again worth noting that there are significant biases in this – e.g. author information is among the, if not the most salient feature for predicting citation counts (Yan et al., 2011; Holm et al., 2020). Looking further into what papers are cited and why, Mohammad (2020b,a) find that there are significant topical as well as gender biases when it comes to who is cited and by whom. Most work on scholarly document processing assumes that the information processed is trustworthy and factually correct. However, this is not always the case. There are two core challenges, which should be addressed: 1) ensuring that scientific publications are credible – e.g. that claims are not made without supporting evidence, and that all relevant supporting evidence is provided; and 2) that scientific findings are not misrepresented, distorted or outright misreported when commun"
2021.sdp-1.1,2020.emnlp-main.639,1,0.92429,", which is to determine whether a statement describing the findings of a scientific study exaggerates them, e.g. by claiming that two variables are strongly correlated when in reality they only co-occur. We argue that this task could be useful to verify if popular science reporting faithfully describes scientific research, or also to determine whether citation sentences (sentences which contain a citation; also called citances) faithfully describe the research documented in the cited papers. 2.1 et al., 2018), and have also studied very related tasks, such as claim check-worthiness detection (Wright and Augenstein, 2020a) or citation recommendation (Jürgens et al., 2018), this is the largest and most diverse dataset for this task to date. An excerpt of our introduced dataset, C ITE W ORTH can be found in Table 1. The dataset curation process involves: 1) data filtering, to identify credible papers with relevant metadata such as venue information; 2) citation span identification and masking, of which we only keep papers with citation spans at the end of sentences to avoid rendering sentences ungrammatical; 3) discarding paragraphs without citations, or where not all sentences have citation spans in accordance"
2021.sdp-1.1,2020.acl-main.702,0,0.0406109,"Missing"
2021.sdp-1.1,2021.findings-acl.157,1,0.896133,"Ozark National Forest in Arkansas (Trauth et al., 2000). Computer Science Land use or cover change is a direct reflection of human activity, such as land use, urban expansion, and architectural planning, on the earth’s surface caused by urbanization [1]. Remote sensing images are important data sources that can efficiently detect land changes. Meanwhile, remote sensing image-based change detection is the change identification of surficial objects or geographic phenomena through the remote observation of two or more different phases [2]. Table 1: Excerpts from training samples in C ITE W ORTH (Wright and Augenstein, 2021) from the Biology and Computer Science fields. Green sentences are cite-worthy sentences, from which citation markers are removed during dataset construction. tecting outright scientific misinformation (Vijjali et al., 2020; Lima et al., 2021). Here, we highlight two important and so far understudied tasks to address issues with such smaller nuances of untrustworthy scientific writing, which can come into play at different stages of the life cycle of scientific research. The first one is cite-worthiness detection, which is about detecting whether or not a sentence ought to contain a citation t"
2021.sdp-1.1,2020.findings-emnlp.112,1,0.830398,"Missing"
2021.sdp-1.1,D19-1473,0,0.0181656,"lines such as a carefully fine-tuned SciBERT (Beltagy et al., 2019) by over 5 points in F1. ner et al., 2014; Bratton et al., 2019; Woloshin et al., 2009; Woloshin and Schwartz, 2002). Exaggeration can mean a sensationalised take-away of the applicability of the work in terms, i.e. giving advice for which there is no scientific basis. Moreover, the strength of the main causal claims and conclusions of a paper can be exaggerated. Table 2 shows examples of those two types of claims from the datasets curated by Sumner et al. (2014) and Bratton et al. (2019), which we use in our work. Prior work (Yu et al., 2019, 2020; Li et al., 2017) uses datasets based on PubMed abstracts and paired press releases from EurekAlert.1 Their core limitations of is that they are limited to only observational studies from PubMed, which have structured abstracts, which strongly simplifies the task of identifying the main claims of a paper. This also holds for the test settings they consider, meaning that the proposed models have a limited applicability. By contrast, we study how to best identify exaggerated claims in popular science communication in the wild, without highly curated data with annotations about core claims"
2021.sdp-1.1,2020.coling-main.427,0,0.0655007,"Missing"
2021.starsem-1.22,P19-1310,0,0.0562086,"Missing"
2021.starsem-1.22,N19-1391,0,0.0198998,"ν : L(C ν , fΘ ) X = X kfΘ (i, s) − fΘ (j, t))k22 (s,t)∈C ν (i,j)∈a(s,t) (1) where Θ are the parameters of the encoder f . As in Cao et al. (2020), we use a regularization term to avoid for the resulting (re-aligned) embeddings to drift too far away from the initial encoder state f0 : 231 R(C ν , fΘ ) = X len(t) X kfΘ (i, t) − f0 (i, t)k22 t∈C ν i=1 (2) Like for the multilingual pre-training of m-BERT and XLM-R, we fine-tune the encoder f on the concatenation of k parallel corpora to handle resourcelean languages, which is in contrast to offline alignment with language-independent rotations (Aldarmaki and Diab, 2019; Schuster et al., 2019). Assume that English is a common pivot (source language) in all our k parallel corpora. Then the following objective function orients all non-English embeddings toward English: min Θ k X L(C ν , fΘ ) + R(C ν , fΘ ) (3) ν=1 In §5, we refer to the above described realignment step as J OINT-A LIGN. 3.2 Vector space normalization We add a batch normalization layer that constrains all embeddings of different languages into a distribution with zero mean and unit variance: f (i, s) − µβ f¯(i, s) = q σβ2 +  (4) where  is a constant value for numerical stability, µβ and σβ ar"
2021.starsem-1.22,P17-1042,0,0.0327956,"ing tasks; (ii) normalizing vector spaces is surprisingly effective, rivals much more resource-intensive methods such as remapping, and leads to more consistent gains; (iii) all three techniques—vector space normalization, re-mapping and input normalization—are orthogonal and their gains often stack. This is a very important finding as it allows for improvements on a much larger scale, especially for typologically 230 2 Related Work Cross-lingual Transfer Static cross-lingual representations have long been used for effective crosslingual transfer and can even be induced without parallel data (Artetxe et al., 2017; Lample et al., 2018). In the monolingual case, static cross-lingual embeddings have recently been succeeded by contextualized ones, which yield considerably better results. The capabilities and limitations of the contextualized multilingual BERT (m-BERT) representations is a topic of vivid discourse. Pires et al. (2019) show surprisingly good transfer performance for mBERT despite it being trained without parallel data, and that transfer is better for typologically similar languages. Wu et al. (2019) show that language representations are not correctly aligned in m-BERT, but can be linearly"
2021.starsem-1.22,N18-1083,1,0.80958,"y broader sample, and shows that vector space normalization is as effective as other recently proposed fixes for m-BERT’s limitations (especially re-mapping), but is much cheaper and orthogonal to other solutions (e.g., input normalization) in that gains are almost additive. Linguistic Typology in NLP. Structural properties of many of the world’s languages can be queried via databases such as WALS (Dryer and Haspelmath, 2013). O’Horan et al. (2016); Ponti et al. (2019) suggest to inject typological information into models to bridge the performance gap between high- and low-resource languages. Bjerva and Augenstein (2018); de Lhoneux et al. (2018); Bjerva and Augenstein (2021) show that crossOriginal - m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful betwee"
2021.starsem-1.22,2021.eacl-main.38,1,0.736113,"ion is as effective as other recently proposed fixes for m-BERT’s limitations (especially re-mapping), but is much cheaper and orthogonal to other solutions (e.g., input normalization) in that gains are almost additive. Linguistic Typology in NLP. Structural properties of many of the world’s languages can be queried via databases such as WALS (Dryer and Haspelmath, 2013). O’Horan et al. (2016); Ponti et al. (2019) suggest to inject typological information into models to bridge the performance gap between high- and low-resource languages. Bjerva and Augenstein (2018); de Lhoneux et al. (2018); Bjerva and Augenstein (2021) show that crossOriginal - m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful between languages which share, e.g., morphological properties."
2021.starsem-1.22,N19-1156,1,0.902609,"For instance, according to WALS, English firmly follows the subject-verb-object (SVO) structure, whereas there is no dominant order in German. We apply this reordering in order to decrease the linguistic gap between languages. For instance, when considering English and French, we reverse all noun-adjective pairings from French to match English. This alignment is done while considering a dependency tree. We re-align according to the typological features from WALS. Since such feature annotations are available for a large amount of languages, and can be obtained automatically with high accuracy (Bjerva et al., 2019a), we expect this method to scale to languages for which basic dependencies (such as noun-adjective attachment) can be obtained automatically. In §5, we refer to the above re-alignment step as T EXT. Input normalization In addition to joint alignment and vector space normalization, we investigate decreasing crosslinguistic differences between languages via the following surface form manipulation of input texts. 232 4 4.1 Experiments Transfer tasks Cross-lingual embeddings are usually evaluated via zero-shot cross-lingual transfer for supervised text classification tasks, or via unsupervised c"
2021.starsem-1.22,J19-2006,1,0.915658,"For instance, according to WALS, English firmly follows the subject-verb-object (SVO) structure, whereas there is no dominant order in German. We apply this reordering in order to decrease the linguistic gap between languages. For instance, when considering English and French, we reverse all noun-adjective pairings from French to match English. This alignment is done while considering a dependency tree. We re-align according to the typological features from WALS. Since such feature annotations are available for a large amount of languages, and can be obtained automatically with high accuracy (Bjerva et al., 2019a), we expect this method to scale to languages for which basic dependencies (such as noun-adjective attachment) can be obtained automatically. In §5, we refer to the above re-alignment step as T EXT. Input normalization In addition to joint alignment and vector space normalization, we investigate decreasing crosslinguistic differences between languages via the following surface form manipulation of input texts. 232 4 4.1 Experiments Transfer tasks Cross-lingual embeddings are usually evaluated via zero-shot cross-lingual transfer for supervised text classification tasks, or via unsupervised c"
2021.starsem-1.22,W17-4755,0,0.011817,"infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each language pair has approximately 3k source sentences, each associA Typologically Varied Language Sample We evaluate multilingual representations on two sets of languages: (1) a default language set with 4 languages from the official XNLI test sets and 2 languages from the WMT17 test sets; (2) a diagnostic language set which contains 19 languages with different levels of data resources from a typologically diverse sample4 covering five language families (each with at least three languages): Austronesi"
2021.starsem-1.22,P19-4007,0,0.0589207,"Missing"
2021.starsem-1.22,D18-1269,0,0.0246154,"result in high quality target language embeddings and gives a false impression of cross-lingual abilities (Libovick´y et al., 2020). Zhao et al. (2020) use the more difficult task of reference-free machine translation evaluation (RFEval) to expose limitations of cross-lingual encoders, i.e., a failure to properly represent finegrained language aspects, which may be exploited by natural adversarial inputs such as word-by-word translations. XNLI. The goal of natural language inference (NLI) is to infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each languag"
2021.starsem-1.22,N19-1423,0,0.67862,"2 4 6 0.65 0.70 0.75 0.80 0.85 Language Similarity RFEval (66.9) 3 Task Performance XNLI (91.7) 4 0.90 0.95 XNLI (72.8) 2 1 0 1 2 0.0 0.5 1.0 1.5 2.0 Wikipedia articles (in millions) 2.5 Figure 1: Zero-shot performance on XNLI and RFEval vs. language similarity to English (top), and data sizes in Wikipedia (bottom). Each point is a language; brackets give the Pearson correlation of points on the xand y-axis. Zero-shot performance is based on the last layer of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al"
2021.starsem-1.22,N13-1073,0,0.0394373,"Missing"
2021.starsem-1.22,D19-1006,0,0.0275513,"between languages which share, e.g., morphological properties. We draw inspiration from Wang and Eisner (2016), who use dependency statistics to generate a large collection of synthetic languages to augment training data for low-resource languages. This intuition of modifying languages based on syntactic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers of monolingual BERT. We examine this in a cross-lingual setting, by randomly selecting 500 German-English mutual word translations and random word pairs within parallel sentences from Europarl (Koehn, 2005). Fig. 2 (left) shows histograms based on the last layers of m-BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019), respectively, which show that XLM-R wrongly assigns nearly perfect cosine similarity scores (+1) to both mutual word translations (matched word pairs) and random word pairs, wher"
2021.starsem-1.22,2005.mtsummit-papers.11,0,0.0416832,"ic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers of monolingual BERT. We examine this in a cross-lingual setting, by randomly selecting 500 German-English mutual word translations and random word pairs within parallel sentences from Europarl (Koehn, 2005). Fig. 2 (left) shows histograms based on the last layers of m-BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019), respectively, which show that XLM-R wrongly assigns nearly perfect cosine similarity scores (+1) to both mutual word translations (matched word pairs) and random word pairs, whereas m-BERT sometimes assigns low scores to mutual translations. This reaffirms that both mBERT and XLM-R have difficulty in distinguishing matched from random word pairs. Surprisingly, vector space re-mapping does not seem to help for XLM-R, but better separates random from matched pairs for m-BER"
2021.starsem-1.22,D18-2012,0,0.0412338,"Missing"
2021.starsem-1.22,2020.emnlp-main.363,0,0.0533972,"r of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on word or sentence level, which may not be available abundantly in low-resource set1 https://github.com/AIPHES/ Language-Agnostic-Contextualized-"
2021.starsem-1.22,D18-1543,1,0.866387,"Missing"
2021.starsem-1.22,P19-1493,0,0.167712,"Missing"
2021.starsem-1.22,2020.findings-emnlp.150,0,0.0338733,"Missing"
2021.starsem-1.22,2020.lrec-1.497,0,0.0468248,"Missing"
2021.starsem-1.22,C16-1123,0,0.0581884,"Missing"
2021.starsem-1.22,J19-3005,0,0.0199628,"Missing"
2021.starsem-1.22,N19-1162,0,0.072225,"now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on word or sentence level, which may not be available abundantly in low-resource set1 https://github.com/AIPHES/ Language-Agnostic-Contextualized-Encoders 2 We consider language similarity as the cosine similarity between the average representations of two languages over monolingual corpora from Wikipedia. 229 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 229–240 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguisti"
2021.starsem-1.22,L16-1680,0,0.0315374,"Missing"
2021.starsem-1.22,Q16-1035,0,0.0287008,"m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful between languages which share, e.g., morphological properties. We draw inspiration from Wang and Eisner (2016), who use dependency statistics to generate a large collection of synthetic languages to augment training data for low-resource languages. This intuition of modifying languages based on syntactic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers o"
2021.starsem-1.22,N18-1101,0,0.0109127,"et al., 2020). Zhao et al. (2020) use the more difficult task of reference-free machine translation evaluation (RFEval) to expose limitations of cross-lingual encoders, i.e., a failure to properly represent finegrained language aspects, which may be exploited by natural adversarial inputs such as word-by-word translations. XNLI. The goal of natural language inference (NLI) is to infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each language pair has approximately 3k source sentences, each associA Typologically Varied Language Sample We evaluate multilingu"
2021.starsem-1.22,D19-1077,0,0.0168753,"ach point is a language; brackets give the Pearson correlation of points on the xand y-axis. Zero-shot performance is based on the last layer of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on"
2021.starsem-1.22,2020.acl-main.151,1,0.879145,"Missing"
2021.starsem-1.22,D19-1053,1,0.832568,"h French Italian Indonesian Dutch Portuguese German English α α γ γ η γ γ η η β α δ δ δ α β δ β β low low low low middle low middle middle middle low middle high high high middle high high high high 29.3 26.5 24.8 24.0 23.8 22.2 21.7 20.1 19.8 19.6 19.2 18.5 18.2 18.0 17.7 16.3 16.2 15.6 0.0 0.08 0.06 0.08 0.06 0.20 0.13 0.15 0.47 0.46 0.09 0.33 1.56 2.16 1.57 0.51 1.99 1.02 2.37 5.98 low low low low low middle middle middle middle middle middle high high high high high high high high ated with one human reference translation and with the automatic translations of participating systems. As in Zhao et al. (2019, 2020), we use the Earth Mover Distance to compute the distances between source sentence and target language translations, based on the semantic similarities of their contextualized cross-lingual embeddings. We refer to this score as XMoverScore (Zhao et al., 2020) and report its Pearson correlation with human judgments in our experiments. 4.2 Table 1: Languages used, with their language families: Austronesian (α), Germanic (β), Indo-Aryan (γ), Romance (δ), and Uralic (η). The cosine distances between target languages and English are measured using m-BERT. This is, however, not likely to resu"
D15-1086,P12-2011,0,0.010186,"t al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenst"
D15-1086,D14-1164,0,0.0148031,"s at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenstein et al., 2014; Augenstein et al., 2015). To date, there is very little research on improving NERC for distant supervision to extract relations between non-standard entities such as musical artists and albums. Some research has been done on impr"
D15-1086,C14-1199,0,0.0948878,"Missing"
D15-1086,P14-5010,0,0.00440395,"Missing"
D15-1086,N13-1095,0,0.0189979,"sing specialised Web features, such as appearances of entities in lists and links to other Web pages, improves average precision by 7 points, which other Web search-based relation extraction approaches could also benefit from (Xin et al., 2014; Augenstein et al., 2014). In future work, the proposed approach could be combined with other approaches to solve typical issues arising in the context of distant supervision, such as dealing with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries"
D15-1086,P09-1113,0,0.716408,"partment, University College London a.vlachos@cs.ucl.ac.uk Abstract knowledge bases and can then be accessed by an information retrieval system, a commercial example for this being Google’s knowledge vault (Xin et al., 2014). In order to keep knowledge bases up to date should new facts emerge, and to quickly adapt to new domains, there is a need for flexible and accurate information extraction (IE) approaches which do not require manual effort to be developed for new domains. A popular approach for creating IE methods to extract such relations is distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009) which is a method for learning relation extractors using relations stored in a knowledge base combined with raw text to automatically generate training data. An important first step in distant supervision is to identify named entities (NEs) and their types to determine if a pair of NEs is a suitable candidate for the relation. As an example, the album relation has a Musical Artist and an Album as arguments. Existing works use supervised named entity recognisers and classifiers (NERC) with either a small set of types such as the Stanford NER system (Manning et al., 2014), or fine-grained NE ty"
D15-1086,P11-2048,0,0.00989521,"heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenstein et al., 2014; Augenstein et al., 2015). To date, there is very little research on improving NERC for distant supervision to extract relations between non-standard entities such as musical artists and albums. Some research has been done on improving distant supervision by using fine-grained named entity classifiers (Ling and Weld, 2012; Liu et al., 2014) and on using named entity linking for distant supervision (Koch et al.,"
D15-1086,D14-1140,0,0.0244847,"Missing"
D15-1086,D13-1152,0,0.0207776,"Missing"
D15-1086,N13-1008,0,0.0402883,"with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 20"
D15-1086,P11-1055,0,0.392904,"best to integrate multiple NE labels as features could be performed, as shown by Liu et al. (2014). NEC features For the one-stage and imitation learning model, we use the following Web features based on HTML markup, both as local features if the entity mention contains the markup, and as global features if a mention somewhere else in the document with the same lexicalisation contains that markup: is link, is list element, is header or subheader, is bold, is emphasised, is italics, is title, is in title. In addition, the following NEC features are extracted, based on Nadeau et al. (2007) and Hoffmann et al. (2011): Word features (mentfeats): • Object occurrence • Sequence and BOW of occurrence • Sequence and bag of POS of occurrence 751 Musical Artist Relation type NE type album MISC record label ORG track MISC Business Relation type NE type employees PER founders PER Film Relation type NE type director PER producer PER actor PER character MISC River Relation type NE type origin LOC mouth LOC Model RelOnly Stanf FIGER OS IL Politician Relation type NE type birthplace LOC educational institution ORG spouse PER Educational Institution Relation type NE type mascot MISC city LOC Book Relation type NE type"
D15-1086,D14-1203,0,0.0141929,"ts which involved rather standard entity types and they did not compare against using off-the shelf NEC systems. vision as done by existing works often causes errors which can be prevented by instead separating NEC and RE with imitation learning. We also showed that using Web features increases precision for NEC. Finally, it is worth noting that the recall for some of the relations is quite low because they only infrequently occur in text, especially in the same sentence as the subject of the relation. These issues can be overcome by performing coreference resolution (Augenstein et al., 2014; Koch et al., 2014), by retrieving more Web pages or improving the information retrieval component of the approach (West et al., 2014) and by combining extractors operating on sentences with other extractors for semi-structured content on Web pages (Carlson et al., 2010). 7 8 Conclusion and Future Work In this paper, we proposed a method for extracting non-standard relations with distant supervision that learns a NEC jointly with relation extraction using imitation learning. Our proposed imitation learning approach outperforms models with supervised NEC for relations involving nonstandard entities as well as rel"
D15-1086,D13-1003,0,0.00864977,"heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenstein et al., 2014; Augen"
D15-1086,D12-1042,0,0.0465296,"text of distant supervision, such as dealing with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the"
D15-1086,P12-1076,0,0.0187977,"nes with FIGER and Stanford NE labels. We further demonstrate that using specialised Web features, such as appearances of entities in lists and links to other Web pages, improves average precision by 7 points, which other Web search-based relation extraction approaches could also benefit from (Xin et al., 2014; Augenstein et al., 2014). In future work, the proposed approach could be combined with other approaches to solve typical issues arising in the context of distant supervision, such as dealing with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al.,"
D15-1086,W14-4501,1,0.900852,"eir effect using a (possibly non-decomposable) loss function on the complete structure predicted. The dependencies between the actions are learnt via appropriate generation of training examples. The ability to learn by assessing only the final prediction and not the intermediate steps is very useful in the face of missing labels, such as in the case of the labels for the NEC stage. Recall that the action sequence in our case consists of one NEC action and possibly one RE action, dependent on whether the NEC action is true, i.e. the entity is of the appropriate type for the relation. Following Vlachos and Clark (2014), for each training instance, we obtain supervision for the NEC stage by taking both options for this stage, true or false, obtaining the prediction from the RE stage in the former case and then comparing the outcomes against the label obtained from distant supervision. Thus the NEC stage is learned so that it enhances the performance of RE. In parallel, the RE stage is learned using only instances that actually reach this stage. The process is iterated so that the models learned adjust to each other. For more details on this we refer the reader to Vlachos and Clark (2014). 4.2 • Noun phrases:"
D15-1086,W11-0307,1,0.840046,"precision, since precision errors can be dealt with by the NEC stage. For a relation candidate identification stage with higher recall we instead rely on POS-based heuristics for detecting NEs2 and HTML markup. We use the following POS heuristics: Imitation learning1 algorithms such as S EARN (Daum´e III et al., 2009) and DAGGER (Ross et al., 2011) have been applied successfully to a variety of structured prediction tasks due to their flexibility in incorporating features and their ability to learn with non-decomposable loss functions. Sample applications include biomedical event extraction (Vlachos and Craven, 2011), dynamic feature selection (He et al., 2013), and machine translation (Grissom II et al., 2014). Imitation learning algorithms for structured prediction decompose the prediction task into a sequence of actions; these actions are predicted by classifiers which are trained to take into account the effect of their predictions on the whole sequence by assessing their effect using a (possibly non-decomposable) loss function on the complete structure predicted. The dependencies between the actions are learnt via appropriate generation of training examples. The ability to learn by assessing only the"
D15-1086,P13-2117,0,0.0304912,"Missing"
D15-1086,D10-1099,0,0.0167606,"ations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (X"
D16-1084,W15-1516,0,0.0260991,"ork mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applie"
D16-1084,S16-1063,1,0.714724,"models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a"
D16-1084,D15-1075,0,0.0850398,"tweets on targets outperform all baselines on the test set. It is further worth noting that the Bag-of-WordVectors baseline achieves results comparable with TweetOnly, Concat and one of the conditional encoding models, TarCondTweet, on the dev set, even though it achieves significantly lower performance on the test set. This indicates that the pre-trained word embeddings on their own are already very useful for stance detection. This is consistent with findings of other works showing the usefulness of such a Bag-of-Word-Vectors baseline for the related tasks of recognising textual entailment Bowman et al. (2015) and sentiment analysis Eisner et al. (2016). Our best result in the test setup with BiCond is the second highest reported result on the Twitter Stance Detection corpus, however the first, third and fourth best approaches achieved their results by automatically labelling Donald Trump training data. BiCond for the unseen target setting outperforms the third and fourth best approaches by a large margin (5 and 7 points in Macro F1, respectively), as can be seen in Table 7. Results for weakly supervised stance detection are discussed in Section 6. Pre-Training Table 4 shows the effect of unsupervi"
D16-1084,S16-1061,0,0.0879263,"0.5078 TweetOnly FAVOR AGAINST Macro 0.5284 0.5774 0.6284 0.4615 0.5741 0.5130 0.5435 Concat FAVOR AGAINST Macro 0.5506 0.5794 0.5878 0.4883 0.5686 0.5299 0.5493 TarCondTweet FAVOR AGAINST Macro 0.5636 0.5947 0.6284 0.4515 0.5942 0.5133 0.5538 TweetCondTar FAVOR AGAINST Macro 0.5868 0.5915 0.6622 0.4649 0.6222 0.5206 0.5714 BiCond FAVOR AGAINST Macro 0.6268 0.6057 0.6014 0.4983 0.6138 0.5468 0.5803 Table 6: Stance Detection test results for weakly supervised setup, trained on automatically labelled pos+neg+neutral Trump data, and reported on the official test set. Marsh, 2016) and INF-UFRGS (Dias and Becker, 2016) considered a different experimental setup. They automatically annotated training data for the test target Donald Trump, thus converting the task into weakly supervised seen target stance detection. The pkudblab system uses a deep convolutional neural network that learns to make 2-way predictions on automatically labelled positive and negative training data for Donald Trump. The neutral class is predicted according to rules which are applied at test time. Since the best performing systems which participated in the shared task consider a weakly supervised setup, we further compare our proposed"
D16-1084,W16-6208,1,0.849006,"Missing"
D16-1084,N16-1138,1,0.817213,"tioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applied to the related task of recognising textual entailment (Rockt¨aschel et al., 2016), using a dataset of half a million training examples (Bowman et al., 2015) and numerous different hypotheses. Our experiments here show that conditional encoding is also successful on a relatively small training set and when applied to an unseen testing target. Moreover, we augment conditional encoding with bidirectional encoding and demonstrate"
D16-1084,I13-1191,0,0.15284,"es which also learn representations for the targets (BoWV, Concat). By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond). This shows that 3 Method Note that “|” indiates “or”, ( ?) indicates optional space 883 in Dias and Becker (2016) such models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell"
D16-1084,D13-1171,0,0.0742182,"Missing"
D16-1084,S16-1003,0,0.327289,"Missing"
D16-1084,D11-1147,0,0.135482,"tection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applied to the related task of recognising textual entailment (Rockt¨aschel et al., 2016), using a dataset of half a million training examples (Bowman et al., 2015) and numerous different hypotheses. Our experiments her"
D16-1084,D13-1170,0,0.00386016,"la and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis (Pang and Lee, 2008; Socher et al., 2013). However, such an approach cannot capture the stance of a tweet with respect to a particular target, unless training data is available for each of the test targets. In such cases, we could learn that a tweet mentioning Donald Trump in a positive manner expresses a negative stance towards Hillary Clinton. Despite this limitation, we use two such baselines, one implemented with a Support Vector Machine (SVM) classifier and one with an LSTM network, in order to assess whether we are successful in incorporating the target in stance prediction. A naive approach to incorporate the target in stance"
D16-1084,N12-1072,0,0.115068,"representations for the targets (BoWV, Concat). By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond). This shows that 3 Method Note that “|” indiates “or”, ( ?) indicates optional space 883 in Dias and Becker (2016) such models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et"
D16-1084,S16-1062,0,0.392335,"data is not provided. Systems need to classify the stance of each tweet as “positive” (FAVOR), “negative” (AGAINST) or “neutral” (NONE) towards the target. The official metric reported for the shared task is F1 macroaveraged over the classes FAVOR and AGAINST. Although the F1 of NONE is not considered, systems still need to predict it to avoid precision errors for the other two classes. Even though participants were not allowed to manually label data for the test target Donald Trump, they were allowed to label data automatically. The two best-performing systems submitted to Task B, pkudblab (Wei et al., 2016) and LitisMind (Zarrella and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis"
D16-1084,S16-1074,0,0.326839,"ed to classify the stance of each tweet as “positive” (FAVOR), “negative” (AGAINST) or “neutral” (NONE) towards the target. The official metric reported for the shared task is F1 macroaveraged over the classes FAVOR and AGAINST. Although the F1 of NONE is not considered, systems still need to predict it to avoid precision errors for the other two classes. Even though participants were not allowed to manually label data for the test target Donald Trump, they were allowed to label data automatically. The two best-performing systems submitted to Task B, pkudblab (Wei et al., 2016) and LitisMind (Zarrella and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis (Pang and Lee, 2008; Socher et al., 2013"
D16-1084,D15-1073,0,0.0469489,"Missing"
D16-1101,D11-1010,0,0.0462723,"udio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So far, SEC approaches focus on short distance semantic agreement, whereas our approach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error correction, however they neither ground in numeric quantities nor incorporate background KBs. 991 6 Conclusion In this paper, we proposed a simple technique to model language in relation to numbers it refers to, as well as conditionally on incomplete knowledge"
D16-1101,P08-1118,0,0.104465,"Missing"
D16-1101,W14-1702,0,0.014453,"models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So far, SEC approaches focus on short distance semantic agreement, whereas our approach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error correction,"
D16-1101,P08-1015,0,0.500252,"all (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), a"
D16-1101,D15-1293,0,0.0348071,"ision (MAP), precision (P), recall (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), col"
D16-1101,P15-2038,0,0.049027,"of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So fa"
D16-1101,Q15-1008,0,0.0310714,"d. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective"
D16-1101,P14-1068,0,0.0613459,"Best results in bold. average precision (MAP), precision (P), recall (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choice"
D16-1101,Q14-1017,0,0.0281215,"), recall (R) and F1. Best results in bold. average precision (MAP), precision (P), recall (R) and F1. Best results in bold. yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models. 5 Related Work Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at"
D16-1101,W11-1301,0,0.0319503,"eption (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding. Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So far, SEC approaches focus on short distance semantic agreement, whereas our approach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error correction, however they neither ground in numeric quantities nor incorporate background KBs. 991 6 Conclusion In this paper, we proposed a simple technique to model language in relation to numbers it refers to, as well as conditionally on incomplete knowledge bases. We found that the proposed techniques lead to performance improvements in the tasks of language modelling,"
D18-1515,D16-1084,1,0.852568,"k-specific classification layer for each output. The hyper-parameters, after doing grid search, optimizing performance on the validation data, are given in Figure 2. LSTM baseline We compare our MLP ranker to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) model. It takes two sequences inputs: sequence 1 and sequence 2, and a stack of three bidirectional LSTM layers, which encode sequence 1 and sequence 2, respectively. The outputs are then concatenated, to enable representing the differences between the two sequences. Instead of relying only on this presentation (Bowman et al., 2015; Augenstein et al., 2016), we also concatenate our distance features and feed everything into our MLP ranker described above. 3 Datasets For our experiments, we use data from SemEval shared tasks, but we also take advantage of potential synergies with other existing datasets for classification of sentence pairs. Below we present the datasets used for our main and auxiliary tasks. We provide some summary statistics for each dataset in Table 3. SemEval 2016 and 2017 As our main dataset we use the queries from SemEval’s subtask B which consists of an original query and 10 possibly related queries. As an auxiliary task, w"
D18-1515,N18-1172,1,0.831957,"CONTRADICTION or NEUTRAL , given a hypothesis and a premise. We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres. Our model is not built to be a strong NLI system; we use the similarity between premise and hypothesis as a weak signal to improve the generalization on our main task. Fake News Challenge The Fake News Challenge2 (FNC) was introduced to combat misleading and false information online. This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018). Formally, the FNC task consists in, given a headline and the body of 2 http://www.fakenewschallenge.org/ text which can be from the same news article or not, classify the stance of the body of text relative to what is claimed in the headline. There are four labels: • AGREES: The body of the article is in agreement with the headline • D ISAGREES: The body of the article is in disagreement with the headline • D ISCUSSES: The body of the article does not take a position • U NRELATED: the body of the article discusses a different topic We include fake news detection as a weak auxiliary signal th"
D18-1515,P98-1013,0,0.0965072,"imple model introduces a new strong baseline which is particularly useful when there is a lack of labeled data. Acknowledgments Table 3: We perform an ablation test, where we remove one feature at a time and report performance on development data of our single-task baseline. We observe that our baseline suffers most from removing the Euclidean distance over trigrams and the cosine similarity over unigrams. Note also that the Jaccard index over unigrams seems to carry a strong signal, albeit a very simple feature. distributed representations of words, knowledge graphs and frames from FrameNet (Baker et al., 1998) as some of their features, and used SVMs for ranking. For a more direct comparison, we also train a more expressive model than the simple MTLbased model we propose. This architecture is based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997). For this model, we input sequences of embedded words (using pre-trained word embeddings) from each query into independent BiLSTM blocks and output a vector representation for each query. We then concatenate the vector representations with the similarity features from our MTL model and feed it into a dense layer and a classification layer. This w"
D18-1515,S16-1138,0,0.0570048,"Missing"
D18-1515,D15-1075,0,0.242462,"r and, finally, a task-specific classification layer for each output. The hyper-parameters, after doing grid search, optimizing performance on the validation data, are given in Figure 2. LSTM baseline We compare our MLP ranker to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) model. It takes two sequences inputs: sequence 1 and sequence 2, and a stack of three bidirectional LSTM layers, which encode sequence 1 and sequence 2, respectively. The outputs are then concatenated, to enable representing the differences between the two sequences. Instead of relying only on this presentation (Bowman et al., 2015; Augenstein et al., 2016), we also concatenate our distance features and feed everything into our MLP ranker described above. 3 Datasets For our experiments, we use data from SemEval shared tasks, but we also take advantage of potential synergies with other existing datasets for classification of sentence pairs. Below we present the datasets used for our main and auxiliary tasks. We provide some summary statistics for each dataset in Table 3. SemEval 2016 and 2017 As our main dataset we use the queries from SemEval’s subtask B which consists of an original query and 10 possibly related querie"
D18-1515,S16-1172,0,0.0263758,"he SemEval shared tasks on CQA, several authors used complex recurrent and convolutional neural network architectures (Severyn and Moschitti, 2015; Barr´on-Cedeno et al., 2016). For example, Barr´on-Cedeno et al. used a convolutional neural network in combination with feature vectors representing lexical, syntactic, and semantic similarity as well as tree kernels. Their performance was slightly lower than the best system (SemEval-Best for 2016 in Table 1). The best system used lexical and semantic similarity measures in combination with a ranking model based on support vector machines (SVMs) (Filice et al., 2016; Franco-Salvador et al., 2016). Both systems are harder to implement and train than the model we propose here. For SemEval-17, FrancoSalvador et al. (2016), the winning team used 4813 F EATURE R EMOVED MAP ACC C OSINE unigram trigram embedding 69.25 69.93 69.11 74.99 76.28 76.40 M ANHATTAN unigram trigram embedding 70.33 69.29 66.90 76.71 76.28 75.28 B HATTACHARYA unigram trigram embedding 70.83 71.14 71.72 75.85 77.50 77.28 E UCLIDEAN unigram trigram embedding 71.43 65.55 70.60 76.57 76.14 75.57 JACCARD unigram trigram 67.41 69.27 75.70 75.98 ward neural architecture is appropriate for a sma"
D18-1515,S17-2003,0,0.0474685,"Missing"
D18-1515,S15-2047,0,0.077163,"Missing"
D18-1515,W17-5301,0,0.0217855,"baselines and the best SemEval systems. We show results for three auxiliary tasks, Question-Comment relevancy prediction, Fake News detection and Natural Language Inference. The asterisks for the MTL results represent the significance of the improvements over the STL systems with ** representing a p-value of < 0.01 and * representing a p-value between 0.01 and 0.05 Natural Language Inference Natural Language Inference (NLI), consists in predicting EN TAILMENT, CONTRADICTION or NEUTRAL , given a hypothesis and a premise. We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres. Our model is not built to be a strong NLI system; we use the similarity between premise and hypothesis as a weak signal to improve the generalization on our main task. Fake News Challenge The Fake News Challenge2 (FNC) was introduced to combat misleading and false information online. This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018). Formally, the FNC task consists in, given a headline and the body of 2 http://www.fakenewschallenge.org/ text which can be from th"
D18-1543,N18-1083,1,0.545746,"a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on the properties of datasets, typically with reinforcement learning (Zoph and Le, 2017; Wong and Gesmundo, 2018; Liang et al., 2018). We also do not explore the option of sharing selectively based on more ﬁne-grained typological information about languages, which related work has indicated could be useful (Bjerva and Augenstein, 2018). Rather, we stick to sharing between languages of the same language families. The strategies explored here do not exhaust the space of possible parameter sharing strategies. For example, we completely ignore soft sharing based on mean-constrained regularisation (Duong et al., 2015). 8 Conclusions We present evaluations of 27 parameter sharing strategies for the Uppsala parser across 10 languages, representing ﬁve language pairs from ﬁve different language families. We repeated the experiment with pairs of unrelated languages. We made several observations: (a) Generally, multitask learning hel"
D18-1543,P15-1166,0,0.0238529,"evaluated on development data); B EST and W ORST are the overall best and worst sharing strategy across languages; C HAR shares only the character-based LSTM parameters; W ORD shares only the word-based LSTM parameters; A LL shares all parameters. refers to hard sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing. � (soft sharing of word parameters, hard sharing of the rest) improves parsing accuracy when training on related languages, and is especially useful in the low resource case. Similar effects have been observed in machine translation (Dong et al., 2015; Johnson et al., 2017), for example. Most studies have only explored a small number of parameter sharing strategies, however. Vilares et al. (2016) evaluate parsing with hard parameter sharing for 100 language pairs with a statistical parser. Naseem et al. (2012) proposed to selectively share subsets of a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on"
D18-1543,P15-2139,0,0.475239,"alidation data. This model is linguistically motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treebank in a common standard. Many of these languages are low resource and have small UD treebanks. It seems interesting t"
D18-1543,Q16-1023,0,0.0441809,"rs is tuned. The novel architecture signiﬁcantly outperforms our monolingual baseline on our set of 10 languages. We additionally investigate parameter sharing of unrelated languages. 2 The Uppsala dependency parser The Uppsala parser (de Lhoneux et al., 2017a,b) consists of three sets of parameters; the parameters of the character-based LSTM, those of the word-based LSTM, and the parameters of the MLP that predicts transitions. The character-based LSTM produces representations for the wordbased LSTM, which produces representations for the MLP. The Uppsala parser is a transition-based parser (Kiperwasser and Goldberg, 2016), adapted to the Universal Dependencies (UD) scheme,1 and using the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). The S WAP 1 http://universaldependencies.org/ transition is used to generate non-projective dependency trees (Nivre, 2009). For an input sentence of length n with words w1 , . . . , wn , the parser creates a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding and the ﬁnal state of the character-based LSTM after proce"
D18-1543,P11-1068,0,0.107639,"Missing"
D18-1543,K17-3022,1,0.895862,"Missing"
D18-1543,W17-6314,1,0.900687,"Missing"
D18-1543,P12-1066,0,0.340049,"d sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing. � (soft sharing of word parameters, hard sharing of the rest) improves parsing accuracy when training on related languages, and is especially useful in the low resource case. Similar effects have been observed in machine translation (Dong et al., 2015; Johnson et al., 2017), for example. Most studies have only explored a small number of parameter sharing strategies, however. Vilares et al. (2016) evaluate parsing with hard parameter sharing for 100 language pairs with a statistical parser. Naseem et al. (2012) proposed to selectively share subsets of a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on the properties of datasets, typically with reinforcement learning (Zoph and Le, 2017; Wong and Gesmundo, 2018; Liang et al., 2018). We also do not explore the option of sharing selectively based on more ﬁne-grained typological information about languages, which re"
D18-1543,P09-1040,0,0.040354,"STM, and the parameters of the MLP that predicts transitions. The character-based LSTM produces representations for the wordbased LSTM, which produces representations for the MLP. The Uppsala parser is a transition-based parser (Kiperwasser and Goldberg, 2016), adapted to the Universal Dependencies (UD) scheme,1 and using the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). The S WAP 1 http://universaldependencies.org/ transition is used to generate non-projective dependency trees (Nivre, 2009). For an input sentence of length n with words w1 , . . . , wn , the parser creates a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding and the ﬁnal state of the character-based LSTM after processing the characters of wi . The character vector ch(wi ) is obtained by running a (bi-directional) LSTM over the characters chj (1 ≤ j ≤ m) of wi . Each input element is represented by the word-level, bi-directional LSTM, as a vector vi = B I L STM(x1:n , i). For each conﬁguration, the feature extractor concatenates the LSTM representations of core"
D18-1543,K18-2011,1,0.882793,"Missing"
D18-1543,P17-2007,0,0.118529,"ally motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treebank in a common standard. Many of these languages are low resource and have small UD treebanks. It seems interesting to ﬁnd out ways to leverage the wealth of in"
D18-1543,P16-2069,0,0.0948361,"Missing"
D18-1543,I08-3008,0,0.140566,"n classiﬁer is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treeb"
D18-1543,W14-4606,0,0.0296745,"0.5 0.8 0.1 0.3 -0.1 1.0 1.4 1.0 0.8 78.8 78.2 0.6 � � ID ID ✗ � � ID ✗ ✗ ✗ � � ID ✗ av. Table 3: LAS on the test sets of the best of 9 sharing strategies and the monolingual baseline. δ is the difference between O URS AND M ONO . 6 Unrelated languages We repeated the same set of experiments with unrelated language pairs. We hypothesise that parameter sharing between unrelated language pairs will be less useful in general than with related language pairs. However, it can still be useful, it has been shown previously that unrelated languages can beneﬁt from being trained jointly. For example, Lynn et al. (2014) have shown that Indonesian was surprisingly particularly useful for Irish. The results are presented in Table 4. The table only presents part of the results, the rest can be found in the supplementary material. As expected, there is much less to be gained from sharing parameters between unrelated pairs. However, it is possible to improve the monolingual baseline by sharing some of the parameters. In general, sharing the MLP is still a helpful thing to do. It is most helpful to share the MLP and optionally one of the two other sets of parameters. Results are close to the monolingual baseline w"
D19-1475,S16-1063,1,0.817759,"eving evidence documents as part of their task description, but do not distribute them. Finally, category IV comprises datasets annotated for both veracity and stance. Thus, every document is annotated with a label indicating whether the document supports or denies the claim, or is unrelated to it. Additional labels can then be added to the datasets to better predict veracity, for instance by jointly training stance and veracity prediction models. Methods not shown in the table, but related to fact checking, are stance detection for claims (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017; Augenstein et al., 2016a; Kochkina et al., 2017; Augenstein et al., 2016b; Zubiaga et al., 2018; Riedel et al., 2017), satire detection (Rubin et al., 2016), clickbait detection (Karadzhov et al., 2017), conspiracy news detection (Tacchini et al., 2017), rumour cascade detection (Vosoughi et al., 2018) and claim perspectives detection (Chen et al., 2019). Claims are obtained from a variety of sources, including Wikipedia, Twitter, criminal reports and fact checking websites such as politifact.com and snopes.com. The same goes for documents – these are often websites obtained through Web search queries, or Wikipedia"
D19-1475,C08-1006,0,0.103921,"Missing"
D19-1475,N18-2004,0,0.123839,"Missing"
D19-1475,K18-1050,0,0.0315294,"Missing"
D19-1475,P09-2078,0,0.0740254,"Contributions: We provide a dataset that, uniquely among extant datasets, contains a large number of naturally occurring claims and rich additional meta-information. 2.2 Methods Fact checking methods partly depend on the type of dataset used. Methods only taking into account claims typically encode those with CNNs or RNNs (Wang, 2017; P´erez-Rosas et al., 2018), and potentially encode metadata (Wang, 2017) in a similar way. Methods for small datasets often use handcrafted features that are a mix of bag of word and other lexical features, e.g. LIWC, and then use those as input to a SVM or MLP (Mihalcea and Strapparava, 2009; P´erez-Rosas et al., 2018; Baly et al., 2018). Some use additional Twitter-specific features (Enayet and El-Beltagy, 2017). More involved methods taking into account evidence documents, often trained on larger datasets, consist of evidence identification and ranking following a neural model that measures the compatibility between claim and evidence (Thorne et al., 2018; Mihaylova et al., 2018; Yin and Roth, 2018). Contributions: The latter category above is the most related to our paper as we consider evidence documents. However, existing models are not trained jointly for evidence identific"
D19-1475,P16-2022,0,0.294844,"erall performance as discussed in Section 5, learns the compatibility between an instance’s claim and each evidence page. It ranks evidence pages by their utility for the veracity prediction task, and then uses the resulting ranking to obtain a weighted combination of all claim-evidence pairs. No direct labels are available to learn the ranking of individual documents, only for the veracity of the associated claim, so the model has to learn evidence ranks implicitly. To combine claim and evidence representations, we use the matching model proposed for the task of natural language inference by Mou et al. (2016) and adapt it to combine an instance’s claim representation with each evidence representation, i.e. srij = [hai ; heij ; hai − heij ; hai · heij ] (5) where srij is the resulting encoding for training example i and evidence page j , [·; ·] denotes vector concatenation, and · denotes the dot product. All joint claim-evidence representations sri0 , . . . , sri10 are then projected into the binary space via a fully connected layer FC, followed by a non-linear activation function f , to obtain a soft ranking of claim-evidence pairs, in practice a 10-dimensional vector, Model Micro F1 Macro F1 clai"
D19-1475,C18-1287,0,0.139806,"Missing"
D19-1475,N18-1202,0,0.0109662,"ng model. Rather, our model has to implicitly learn these weights for each claim-evidence pair in an end-to-end fashion given the veracity labels. Metadata 5.1 Experiments Experimental Setup The base sentence embedding model is a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following Augenstein et al. (2018). We opt for this strong baseline sentence encoding model, as opposed to engineering sentence embeddings that work particularly well for this dataset, to showcase the dataset. We would expect pre-trained contextual encoding models, e.g. ELMO (Peters et al., 2018), ULMFit (Howard and Ruder, 2018), BERT (Devlin et al., 2018), to offer complementary performance gains, as has been shown for a few recent papers (Wang et al., 2018; Rajpurkar et al., 2018). For claim veracity prediction without evidence documents with the MTL with LEL model, we use the following sentence encoding variants: claim4691 only, which uses a BiLSTM-based sentence embedding as input, and claim-only embavg, which uses a sentence embedding based on mean averaged word embeddings as input. We train one multi-task model per task (i.e., one model per domain). We perform a grid search over"
D19-1475,P18-2124,0,0.0257929,"tup The base sentence embedding model is a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following Augenstein et al. (2018). We opt for this strong baseline sentence encoding model, as opposed to engineering sentence embeddings that work particularly well for this dataset, to showcase the dataset. We would expect pre-trained contextual encoding models, e.g. ELMO (Peters et al., 2018), ULMFit (Howard and Ruder, 2018), BERT (Devlin et al., 2018), to offer complementary performance gains, as has been shown for a few recent papers (Wang et al., 2018; Rajpurkar et al., 2018). For claim veracity prediction without evidence documents with the MTL with LEL model, we use the following sentence encoding variants: claim4691 only, which uses a BiLSTM-based sentence embedding as input, and claim-only embavg, which uses a sentence embedding based on mean averaged word embeddings as input. We train one multi-task model per task (i.e., one model per domain). We perform a grid search over the following hyperparameters, tuned on the respective dev set, and evaluate on the correspoding test set (final settings are underlined): word embedding size [64, 128, 256], BiLSTM hidden"
D19-1475,W16-0802,0,0.12816,"for both veracity and stance. Thus, every document is annotated with a label indicating whether the document supports or denies the claim, or is unrelated to it. Additional labels can then be added to the datasets to better predict veracity, for instance by jointly training stance and veracity prediction models. Methods not shown in the table, but related to fact checking, are stance detection for claims (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017; Augenstein et al., 2016a; Kochkina et al., 2017; Augenstein et al., 2016b; Zubiaga et al., 2018; Riedel et al., 2017), satire detection (Rubin et al., 2016), clickbait detection (Karadzhov et al., 2017), conspiracy news detection (Tacchini et al., 2017), rumour cascade detection (Vosoughi et al., 2018) and claim perspectives detection (Chen et al., 2019). Claims are obtained from a variety of sources, including Wikipedia, Twitter, criminal reports and fact checking websites such as politifact.com and snopes.com. The same goes for documents – these are often websites obtained through Web search queries, or Wikipedia documents, tweets or Facebook posts. Most datasets contain a fairly small number of claims, and those that do not, often lack evidenc"
D19-5006,D16-1084,1,0.863874,". In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We evaluate different classifiers that predict frames for unlabeled tweets in Golovchenko et al. (2018)’s dataset, in order to increase the number of polarized edges in the retweet network derived from the data. This is challenging due to a sk"
D19-5006,Q17-1010,0,0.0155149,"Missing"
D19-5006,P15-2072,0,0.0192645,"in the propaganda literature, established media outlets too are vulnerable to state-driven disinformation campaigns, even if they are regarded as credible sources (Jowett and O’donnell, 2014; Taylor, 2003; Chomsky and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We e"
D19-5006,N19-1142,1,0.804456,"campaigns, even if they are regarded as credible sources (Jowett and O’donnell, 2014; Taylor, 2003; Chomsky and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We evaluate different classifiers that predict frames for unlabeled tweets in Golovchenko et al. (2018)’s dataset,"
D19-5006,I13-1191,0,0.0287891,"and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We evaluate different classifiers that predict frames for unlabeled tweets in Golovchenko et al. (2018)’s dataset, in order to increase the number of polarized edges in the retweet network derived from the data. This i"
D19-5006,P16-2044,0,0.0268532,"escribed in Section 4 on the training set and measure performance on the test set. For the CNN and L OG R EG models, we upsample the training examples such that each class has as many instances as the largest class (Neutral). The final reported scores are averages over the 10 splits.9 Classification Models For our classification experiments, we compare three classifiers, a hashtag-based baseline, a logistic regression classifier and a convolutional neural network (CNN). Hashtag-Based Baseline Hashtags are often used as a means to assess the content of a tweet (Efron, 2010; Godin et al., 2013; Dhingra et al., 2016). We identify hashtags indicative of a class in the annotated dataset using the pointwise mutual information (pmi) between a hashtag hs and a class c, which is defined as pmi(hs, c) = log p(hs, c) p(hs) p(c) Experimental Setup 5.1 Tweet Preprocessing Before embedding the tweets, we replace urls, retweet syntax (RT @user_name: ) and @mentions (@user_name) by placeholders. We lowercase all text and tokenize sentences using the StandfordNLP pipeline (Qi et al., 2018). If a tweet contains multiple sentences, these are concatenated. Finally, we remove all tokens that contain non-alphanumeric symbol"
D19-5006,P17-1092,0,0.0178711,"media outlets too are vulnerable to state-driven disinformation campaigns, even if they are regarded as credible sources (Jowett and O’donnell, 2014; Taylor, 2003; Chomsky and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We evaluate different classifiers that predict"
D19-5006,D18-1393,0,0.0122749,"riven disinformation campaigns, even if they are regarded as credible sources (Jowett and O’donnell, 2014; Taylor, 2003; Chomsky and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We evaluate different classifiers that predict frames for unlabeled tweets in Golovchenko"
D19-5006,P17-1069,0,0.0439104,"Missing"
D19-5006,D18-1330,0,0.0228912,"Missing"
D19-5006,N12-1072,0,0.0323361,"Taylor, 2003; Chomsky and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We evaluate different classifiers that predict frames for unlabeled tweets in Golovchenko et al. (2018)’s dataset, in order to increase the number of polarized edges in the retweet network derived f"
D19-5006,D14-1181,0,0.00423193,"Missing"
D19-5006,P15-1035,0,0.0311968,"e users are retweeting each other.14 In order to track the flow of polarized information, Golovchenko et al. (2018) label an edge as polarized if at least one tweet contained in the edge was manually annotated as From the error analysis, we conclude that category I errors need further investigation, as here the model makes mistakes on seemingly easy instances. This might be due to the model not being able to correctly represent Twitter specific language or unknown words, such as Eukraine in example e). Category II and III errors are harder to avoid and could be improved by applying reasoning (Wang and Cohen, 2015) or irony detection methods (Van Hee et al., 2018). 14 Golovchenko et al. (2018) use the k10 core of the network, which is the maximal subset of nodes and edges, such that all included nodes are connected to at least k other nodes (Seidman, 1983), i.e. all users in the network have interacted with at least 10 other users. 52 Figure 2: The left plot shows the original k10 retweet network as computed by Golovchenko et al. (2018) together with the new edges that were added after manually re-annotating the classifier predictions. The right plot only visualizes the new edges that we could add by fi"
D19-5006,naderi-hirst-2017-classifying,0,0.030718,"re vulnerable to state-driven disinformation campaigns, even if they are regarded as credible sources (Jowett and O’donnell, 2014; Taylor, 2003; Chomsky and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter. Contributions We evaluate different classifiers that predict frames for unlabeled tw"
D19-5006,K18-2016,0,0.0500399,"Missing"
D19-5006,C14-1008,0,0.120556,"Missing"
D19-5006,P15-1157,0,0.0132469,"s often emphasized in the propaganda literature, established media outlets too are vulnerable to state-driven disinformation campaigns, even if they are regarded as credible sources (Jowett and O’donnell, 2014; Taylor, 2003; Chomsky and Herman, 1988)1 . In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis (Boydstun et al., 2014; Tsur et al., 2015; Card et al., 2015; Johnson et al., 2017; Ji and Smith, 2017; Naderi and Hirst, 2017; Field et al., 2018; Hartmann et al., 2019). Similarly, automatically labeling attitudes expressed in text (Walker et al., 2012; Hasan and Ng, 2013; Augenstein et al., 2016; Zubiaga et al., 2018) can aid the analysis of disinformation and misinformation spread (Zubiaga et al., 2016). In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter."
D19-5006,S18-1005,0,0.0662966,"Missing"
D19-6128,C16-1333,1,0.894016,"Missing"
D19-6128,P17-1014,0,0.0285848,"his condition, the test set is still relatively small. Thus, the word embeddings do not have much distributional information with which to arrive at good word representations for previously out-of-vocabulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on"
D19-6128,E17-2039,1,0.851467,"Missing"
D19-6128,E17-1005,0,0.0343576,"Missing"
D19-6128,N18-1172,1,0.854624,"f the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples where multi-task learning is combined with other methods is the semi"
D19-6128,D15-1041,0,0.0150694,"cies obtained from our training data. 2.2 We approach sequence labelling by using a variant of a bidirectional recurrent neural network, which uses both preceding and succeeding context when predicting the label of a word. This choice was made as such models at the same time obtain high performance on all three tasks and lend themselves nicely to multi-task training via hard parameter sharing. This system is based on the hierarchical bi-LSTM of Plank et al. (2016) and is implemented using DyNet (Neubig et al., 2017). On the subword-level, the LSTM is bi-directional and operates on characters (Ballesteros et al., 2015; Ling et al., 2015). Second, a context bi-LSTM operates on the word level, from which output is passed on to a classification layer. Multi-task training is approached using hard parameter sharing (Caruana, 1993). We consider T datasets, each containing pairs of input-output set ), w ∈ V , y t ∈ Lt . The input quences (w1:n , y1:n i i vocabulary V is shared across tasks, but the outputs (tagsets) Lt are task dependent. At each step in the training process we choose a random task t, followed by a randomly chosen batch of training instance. Each task is associated with an independent classificat"
D19-6128,L16-1262,0,0.0622977,"Missing"
D19-6128,N18-1202,0,0.0189821,"for self-training. Second, we choose a transductive approach, because we assume that not all auxiliary task examples will lead to equal improvements on the main task. In particular, we expect auxiliary task labels for the test instances to be most useful, since information about those instances is most relevant for the prediction of the main task labels on this data. Similarly to contextualised word representations, this offers an additional signal for the test set instances, as we obtain this through predicted auxiliary labels rather than direct encoding of the context (Devlin et al., 2018; Peters et al., 2018). 3.1 question how it compares to adding additional (expensive) gold-standard annotations for the main and the auxiliary tasks. To ensure that our findings are generalisable, we use a large sample of 56 treebanks, covering 41 languages and several domains. Although this experimental set-up would allow us to run multilingual experiments, we only train monolingual models, and aggregate results across languages and treebanks. We investigate three tasks; two of them being morpho-syntactic (POS tagging and DepRel tagging) and one being semantic (semantic tagging). In all cases, POS is the auxiliary"
D19-6128,P16-2067,0,0.0199108,"multi-task model for DepRel tagging and semantic tagging, respectively. 2 (2016). We use this task as an unsupervised auxiliary baseline, with frequencies obtained from our training data. 2.2 We approach sequence labelling by using a variant of a bidirectional recurrent neural network, which uses both preceding and succeeding context when predicting the label of a word. This choice was made as such models at the same time obtain high performance on all three tasks and lend themselves nicely to multi-task training via hard parameter sharing. This system is based on the hierarchical bi-LSTM of Plank et al. (2016) and is implemented using DyNet (Neubig et al., 2017). On the subword-level, the LSTM is bi-directional and operates on characters (Ballesteros et al., 2015; Ling et al., 2015). Second, a context bi-LSTM operates on the word level, from which output is passed on to a classification layer. Multi-task training is approached using hard parameter sharing (Caruana, 1993). We consider T datasets, each containing pairs of input-output set ), w ∈ V , y t ∈ Lt . The input quences (w1:n , y1:n i i vocabulary V is shared across tasks, but the outputs (tagsets) Lt are task dependent. At each step in the t"
D19-6128,W03-0404,0,0.107928,"ulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For"
D19-6128,D17-1038,0,0.0171906,"can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples where multi-task learning is combined with"
D19-6128,P16-2038,0,0.159736,"05) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples w"
D19-6128,L16-1680,0,0.0458928,"Missing"
D19-6128,P95-1026,0,0.760946,"ary rate is reduced to zero in this condition, the test set is still relatively small. Thus, the word embeddings do not have much distributional information with which to arrive at good word representations for previously out-of-vocabulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-t"
D19-6128,D15-1176,0,\N,Missing
D19-6128,P19-1301,0,\N,Missing
D19-6130,L18-1614,0,0.080506,"Missing"
D19-6130,P16-1145,0,0.0607532,"Missing"
D19-6130,Q17-1010,0,0.0479708,"Missing"
D19-6130,D18-1456,0,0.0281005,"Missing"
D19-6130,P17-1171,0,0.0517213,"Missing"
D19-6130,K17-1034,0,0.0289666,"Missing"
D19-6130,P16-1105,0,0.0502904,"Missing"
D19-6130,D18-1269,0,0.0592708,"Missing"
D19-6130,W18-5511,0,0.0327858,"Missing"
D19-6130,D14-1162,0,0.0823085,"Missing"
D19-6130,P18-2124,0,0.0491031,"Missing"
D19-6130,D11-1142,0,0.194172,"Missing"
D19-6130,N13-1008,0,0.115232,"Missing"
D19-6130,N15-1151,0,0.0572468,"Missing"
D19-6130,N15-1118,0,0.0751173,"Missing"
D19-6130,P18-1156,1,0.892562,"Missing"
D19-6130,W17-2623,0,0.0644235,"Missing"
D19-6130,N16-1103,0,0.0476727,"Missing"
D19-6130,P05-1053,0,0.179761,"Missing"
D19-6130,N07-4013,0,\N,Missing
D19-6130,S17-2001,0,\N,Missing
J19-2006,N18-1083,1,0.697995,"rds can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the similarities between languages in an embedded language space were to be found to encode geographical distances (Figure 1), any conclusions drawn from use of these representations would not likely be of much use for most NLP tasks. The importan"
J19-2006,W17-6508,0,0.029981,"gly correlated with genealogical distance, significant differences can be observed. Romanian, as a member of the Balkan sprachbund, is distinct from the other Romance languages. The North Germanic (Danish, Swedish) and West Germanic (Dutch, German) branches are separated through considerable structural differences, with English grouped with the North Germanic languages despite its West Germanic origin. The Baltic languages (Latvian, Lithuanian) are grouped with the nearby Finnic languages (Estonian, Finnish) rather than their distant Slavic relatives. This idea has been explored previously by Chen and Gerdes (2017), who use a combination of relative frequency, length, and direction of deprels. We, by comparison, achieve an even richer representation by also taking head and dependent POS into account. 386 Bjerva et al. What Do Language Representations Really Represent? Figure 4 Correlations between similarities (Genetic, Geo., and Struct.) and language representations (Raw, Func, POS, Phrase, Deprel). Significance at p &lt; 0.001 is indicated by *. 5. Analysis of Similarities Although we are able to reconstruct phylogenetic language trees in a similar manner to previous work, we wish to investigate whether"
J19-2006,P17-1109,0,0.0302127,"ddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the similarities between languages in an embedded language space were to be found to encode geographical distances (Figure 1), any conclusions drawn from use of these representations would not likely be of much use for"
J19-2006,D18-1029,0,0.102725,"Missing"
J19-2006,2005.mtsummit-papers.11,0,0.200565,"lations as, e.g., speakers from certain 195 195 nt tothe abstract away from the surface forms of distance measures provide more explanation (RQ2). distance measures provide more explanation (RQ2). 194 tigate representation learning on monolingual English sentences, which are translations are captured by language representations, or if other talksame about thecertain same issues. We therefore dtions to will talk about the issues. We therefore 196 196 as,tend e.g.,tospeakers from regions 195 distance provide more explanation from various source languages English from the (RQ2). Europarl corpus (Koehn 2005). 4.1toGenetic Distance introduce several levels ofWe abstraction: i) training onmeasures 4.1 Genetic Distance eo several levels of abstraction: i) training on 197 talk about the same issues. therefore 197 196 They use a feature-engineering approach to predict source languages and learn an 198 Following Rabinovich al. (2017), phylo4.1 etFollowing Genetic Distance several1This levels of abstraction: i) training is the exactassame used byon Rabinovich al. 198 Rabinovich et al.et(2017), we we use use phylo197 is the exact same data useddata by asRabinovich et al. tree Indo-European family using t"
J19-2006,D17-1268,0,0.260431,"Missing"
J19-2006,E17-2102,1,0.817678,"national (CC BY-NC-ND 4.0) license Computational Linguistics Volume 45, Number 2 1. Introduction Words can be represented with distributed word representations, currently often in the form of word embeddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the sim"
J19-2006,P17-1049,0,0.0968921,"Missing"
J19-2006,L16-1680,0,0.0311049,"Missing"
J19-2006,P15-2034,1,\N,Missing
J19-2006,Q17-1024,0,\N,Missing
K17-1021,P82-1020,0,0.790042,"Missing"
K17-1021,P17-2054,1,0.83311,"scale measurement data, we introduce techniques to make the sfmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Each paper in this dataset is guaranteed to have a title,"
K17-1021,P15-2136,0,0.216262,"score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation 2 The dataset along with the code is available here: https://github.com/EdCo95/ scientific-paper-summarisation 196 2.1 judgements of good summaries (Lin, 2004). We elect to use ROUGE-L, inline with other research into summarisation of scientific articles (Cohan and Goharian, 2015; Jaidka et al., 2016). Problem Formulation As shown by Cao et al. (2015), sentences can be good summaries even when taken out of the context of the surrounding sentences. Most of the highlights have this characteristic, not relying on any previous or subsequent sentences to make sense. Consequently, we frame the extractive summarisation task here as a binary sentence classification task, where we assign each sentence in a document a label y ∈ 0, 1. Our training data is therefore a list of sentences, sentence features to encode context and a label all stored in a randomly ordered list. 2.2 3.1 HighlightROUGE is a method used to generate additional training data for"
K17-1021,P16-1046,0,0.163516,"ncerned with summarising scientific publications. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standard for news documents (N"
K17-1021,S10-1004,0,0.0254187,"nformation by leveraging the graph structure. To cope with large-scale measurement data, we introduce techniques to make the sfmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document i"
K17-1021,N16-1012,0,0.0357437,"scientific publications. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standard for news documents (Nallapati et al., 2016"
K17-1021,D15-1232,0,0.0714735,"e document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequence encoder. In this work, we therefore opt to only encode the target sequence with an RNN and the global context with simpler features. We leave fully neural approaches to encoding publications to future work. Figure 6: Comparison of ROUGE scores of the Features Only, SAFNet and SFNet models when trained with (bars on the left) and without"
K17-1021,D15-1045,0,0.205431,"the global context of a summary statement, which contribute most to the overall score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation 2 The dataset along with the code is available here: https://github.com/EdCo95/ scientific-paper-summarisation 196 2.1 judgements of good summaries (Lin, 2004). We elect to use ROUGE-L, inline with other research into summarisation of scientific articles (Cohan and Goharian, 2015; Jaidka et al., 2016). Problem Formulation As shown by Cao et al. (2015), sentences can be good summaries even when taken out of the context of the surrounding sentences. Most of the highlights have this characteristic, not relying on any previous or subsequent sentences to make sense. Consequently, we frame the extractive summarisation task here as a binary sentence classification task, where we assign each sentence in a document a label y ∈ 0, 1. Our training data is therefore a list of sentences, sentence features to encode context and a label all stored in a randomly ordered list. 2.2 3.1"
K17-1021,W04-1013,0,0.0352079,"rk several neural as well traditional summarisation methods on the dataset and use simple features to model the global context of a summary statement, which contribute most to the overall score • We compare our best performing system to several well-established baseline methods, some of which use more elaborate methods to model the global context than we do, and show that our best performing model outperforms them on this extractive summarisation 2 The dataset along with the code is available here: https://github.com/EdCo95/ scientific-paper-summarisation 196 2.1 judgements of good summaries (Lin, 2004). We elect to use ROUGE-L, inline with other research into summarisation of scientific articles (Cohan and Goharian, 2015; Jaidka et al., 2016). Problem Formulation As shown by Cao et al. (2015), sentences can be good summaries even when taken out of the context of the surrounding sentences. Most of the highlights have this characteristic, not relying on any previous or subsequent sentences to make sense. Consequently, we frame the extractive summarisation task here as a binary sentence classification task, where we assign each sentence in a document a label y ∈ 0, 1. Our training data is ther"
K17-1021,W04-1017,0,0.0104513,"uments. Extractive Summarisation Methods Early work on extractive summarisation focuses exclusively on easy to compute statistics, e.g. word frequency (Luhn, 1958), location in the document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequence encoder. In this work, we therefore opt to only encode the target sequence with an RNN and the global context with simpler features. We leave fully neural approaches to enco"
K17-1021,P16-4013,0,0.0212315,"ferent ways: as their mean averaged word embeddings and as their Recurrent Neural Network (RNN) encoding. 4.1 Summariser Features As the sentences in our dataset are randomly ordered, there is no readily available context for each sentence from surrounding sentences (taking this into account is a potential future development). To provide local and global context, a set of 8 features are used for each sentence which are described below. These contextual features contribute to achieving the best performances. Some recent work in summarisation uses as many as 30 features (Dlikman and Last, 2016; Litvak et al., 2016). We choose only a minimal set of features to focus more on learning from raw data than on feature engineering, although this could potentially further improve results. Keyphrase Score Authors such as Sp¨arck Jones (2007) refer to the keyphrase score as a useful summarisation feature. The feature uses author defined keywords and counts how many of these keywords a sentence contains, the idea being that important sentences will contain more keywords. TF-IDF Term Frequency, Inverse Document Frequency (TF-IDF) is a measure of how relevant a word is to a document (Ramos et al., 2003). It takes int"
K17-1021,I11-1001,0,0.0198738,"fmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Each paper in this dataset is guaranteed to have a title, abstract, author written highlight statements and autho"
K17-1021,D15-1057,0,0.0199481,"the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Each paper in this dataset is guaranteed to have a title, abstract, author written highlight statements and author defined keywords. The"
K17-1021,N09-1041,0,0.0480757,"aste score. The title has the highest ROUGE score in relation to the gold summary, which is intuitive as the aim of a title is to convey information about the research in a single line. 5.2 Comparison of Model Performance and Error Analysis Figure 3 shows comparisons of the best model we developed to well-established external baseline methods. Our model can be seen to significantly outperform these methods, including graph-based methods which take account of global context: LexRank (Radev, 2004) and TextRank (Mihalcea and Tarau, 2004); probabilistic methods in KLSum (KL divergence summariser, Haghighi and Vanderwende (2009)); methods based on singular value decomposition with LSA (latent semantic analysis, Steinberger and Jeˇzek (2004)); and simple methods based on counting in SumBasic (Vanderwende et al., 2007). This is an encouraging result showing that our methods that combine neural sentence encoding and simple features for representing the global context and positional information are very effective for modelling an extractive summarisation problem. Figure 4 shows the performance of all models developed in this work measured in terms of accuracy and ROUGE-L on CSPubSumExt Test and CSPubSum Test, respectivel"
K17-1021,K16-1028,0,0.0552598,"Missing"
K17-1021,C14-1002,0,0.0413014,"Missing"
K17-1021,D16-1198,0,0.0142963,"raging the graph structure. To cope with large-scale measurement data, we introduce techniques to make the sfmap framework scalable. We validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links. The remainder of this paper is organized as follows: section2 summarizes the related work. [...] We expect the research documented in this paper to be relevant beyond the document summarisation community, for other tasks in the space of automatically understand scientific publications, such as keyphrase extraction (Kim et al., 2010; Sterckx et al., 2016; Augenstein et al., 2017; Augenstein and Søgaard, 2017), semantic relation extraction (Gupta and Manning, 2011; Marsi and ¨ urk, 2015) or topic classification of scientific Ozt¨ ´ S´eaghdha and Teufel, 2014). articles (O 2 Dataset and Problem Formulation We release a novel dataset for extractive summarisation comprised of 10148 Computer Science publications.2 Publications were obtained from ScienceDirect, where publications are grouped into 27 domains, Computer Science being one of them. As such, the dataset could easily be extended to more domains. An example document is shown in Table 1. Ea"
K17-1021,J02-4002,0,0.898922,"y sentences from the Abstract, Introduction or Conclusion, thinking these more salient to summaries; and we show that certain sections within a paper are more relevant to summaries than others (see Section 5.1). Therefore we assign sentences an integer location for 7 different sections: Highlight, Abstract, Introduction, Results / Discussion / Analysis, Method, Conclusion, all else.3 Location features have been used in other ways in previous work on summarising scientific literature; Visser and Wieling (2009) extract sentence location features based on the headings they occurred beneath while Teufel and Moens (2002) divide the paper into 20 equal parts and assign each sentence a location based on which segment it occurred in - an attempt to capture distinct zones of the paper. Document TF-IDF Document TF-IDF calculates the same metric as TF-IDF, but uses the count of words in a sentence as the term frequency and count of words in the rest of the paper as the background corpus. This gives a representation of how important a word is in a sentence in relation to the rest of the document. Sentence Length Teufel et al. (2002) created a binary feature for if a sentence was longer than a threshold. We simply in"
K17-1021,radev-etal-2004-mead,0,0.117015,"luation campaigns for summarisation of news, organised by the 202 challenges centered around how to encode very large documents. Extractive Summarisation Methods Early work on extractive summarisation focuses exclusively on easy to compute statistics, e.g. word frequency (Luhn, 1958), location in the document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequence encoder. In this work, we therefore"
K17-1021,E17-2112,0,0.0146892,"risation often emerged as part of evaluation campaigns for summarisation of news, organised by the 202 challenges centered around how to encode very large documents. Extractive Summarisation Methods Early work on extractive summarisation focuses exclusively on easy to compute statistics, e.g. word frequency (Luhn, 1958), location in the document (Baxendale, 1958), and TF-IDF (Salton et al., 1996). Supervised learning methods which classify sentences in a document binarily as summary sentences or not soon became popular (Kupiec et al., 1995). Exploration of more cues such as sentence position (Yang et al., 2017), sentence length (Radev et al., 2004), words in the title, presence of proper nouns, word frequency (Nenkova et al., 2006) and event cues (Filatova and Hatzivassiloglou, 2004) followed. Recent approaches to extractive summarisation have mostly focused on neural approaches, based on bag of word embeddings approaches (Kobayashi et al., 2015; Yogatama et al., 2015) or encoding whole documents with CNNs and/or RNNs (Cheng and Lapata, 2016). In our setting, since the documents are very large, it is computationally challenging to read a whole publication with a (possibly hierarchical) neural sequen"
K17-1021,D15-1228,0,0.0239125,"Missing"
K17-1021,D15-1044,0,0.0437486,"act Here, we are concerned with summarising scientific publications. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standa"
K17-1021,W16-1520,0,0.076196,"Missing"
K17-1021,P17-1099,0,0.0263366,"ons. Since scientific publications are a technical domain with fairly regular and explicit language, we opt for the task of extractive summarisation. Although there has been work on summarisation of scientific publications before, existing datasets are very small, consisting of tens of documents (Kupiec et al., 1995; Visser and Wieling, 2009). Such small datasets are not sufficient to learn supervised summarisation models relying on neural methods for sentence and document encoding, usually trained on many thousands of documents (Rush et al., 2015; Cheng and Lapata, 2016; Chopra et al., 2016; See et al., 2017). In this paper, we introduce a dataset for automatic summarisation of computer science publications which can be used for both abstractive and extractive summarisation. It consists of more than 10k documents and can easily be extended automatically to an additional 26 domains. The dataset is created by exploiting an existing resource, ScienceDirect,1 where many journals require authors to submit highlight statements along with their manuscripts. Using such highlight statements as gold statements has been proven a good gold standard for news documents (Nallapati et al., 2016a). This new datase"
K17-1021,W04-3252,0,\N,Missing
K17-1021,D11-1014,0,\N,Missing
K17-1021,W16-1511,0,\N,Missing
K18-3011,P17-1182,0,0.0605568,"Missing"
K18-3011,W16-2010,0,0.138201,"Missing"
K18-3011,D18-1543,1,0.841915,"Missing"
K18-3011,P16-2067,0,0.0639877,"Missing"
K18-3011,K17-2002,0,0.036597,"Missing"
K18-3011,P16-2038,0,0.0797609,"Missing"
K18-3011,W17-0225,1,0.857404,"Missing"
K18-3011,C16-1333,1,0.89768,"Missing"
L16-1729,S16-1063,1,0.862706,"Missing"
L16-1729,bentivogli-etal-2010-building,0,0.0187668,"ortable across languages and domains, but 11 To assess the PHEME RTE pilot dataset, we use the MaxEnt classifier-based model (Wang and Neumann, 2007) distributed with the Excitement Open Platform (EOP, Pado et train http://hltfbk.github.io/Excitement-Open-Platform/ generated by the sklearn metric classification report, see http://scikit-learn.org 13 http://nlp.stanford.edu/RTE3-pilot/RTE3 dev 3class.xml 4604 12 requires event and claim annotations. The manual effort spent to create such annotations is feasible to replace by automatic means which are currently being implemented in the project. Bentivogli et al. (2010) stress the importance of creating specialized data sets for RTE, in order to facilitate more targeted assessment and decomposition of the RTE task’s complexity. In our resource, the text snippets that form a RTE pair deliberately keep reoccurring across all three judgement labels in systematically varied pairings, allowing to investigate, model and evaluate linguistic and extralinguistic phenomena that underly semantic inference in the misinformation detection scenario. Previous RTE research has mainly focused on achieving good performance on the Entailment relation, whereas our method is mot"
L16-1729,D15-1075,0,0.0337061,"neither of those. A bottleneck for this task is obtaining training data. The creation of natural language data annotated for inference phenomena is so far a nontrivial and largely manual procedure, yielding expensive resources that are nonetheless problematically portable to new text genres and application domains. Existing initiatives have often created RTE data by syntactic and lexical transformations with predictable effects asking annotators to (re)write sentences taken from gold standards for other tasks such as question answering (Bar-Haim et al., 2006) and image and video description (Bowman et al., 2015; Marelli et al., 2014). RTE tasks may involve 2-way or 3-way inference judgements. In case of a 2-way judgement, the class to guess is either Entailment or Nonentailment. On the 3-way judgement scheme the Nonentailment class is further differentiated into Contradiction and Unknown. The presence of contradictory statements in social media can be indicative 1 for mis-/disinformation, controversy or speculation, which are important triggers in veracity checking procedures. Our present contribution therefore addresses the transformation of a project-internal corpus of annotated microblog texts in"
L16-1729,P08-1118,0,0.593236,"Missing"
L16-1729,N16-1138,0,0.160666,"Missing"
L16-1729,marelli-etal-2014-sick,0,0.035106,"bottleneck for this task is obtaining training data. The creation of natural language data annotated for inference phenomena is so far a nontrivial and largely manual procedure, yielding expensive resources that are nonetheless problematically portable to new text genres and application domains. Existing initiatives have often created RTE data by syntactic and lexical transformations with predictable effects asking annotators to (re)write sentences taken from gold standards for other tasks such as question answering (Bar-Haim et al., 2006) and image and video description (Bowman et al., 2015; Marelli et al., 2014). RTE tasks may involve 2-way or 3-way inference judgements. In case of a 2-way judgement, the class to guess is either Entailment or Nonentailment. On the 3-way judgement scheme the Nonentailment class is further differentiated into Contradiction and Unknown. The presence of contradictory statements in social media can be indicative 1 for mis-/disinformation, controversy or speculation, which are important triggers in veracity checking procedures. Our present contribution therefore addresses the transformation of a project-internal corpus of annotated microblog texts into a 3-way RTE dataset."
L16-1729,S16-1003,0,0.0544845,"t is centered on contradictory claims present in the data, and is extended to the other two classes to a limited extent. The resulting pilot dataset is balanced across the three classes. In future work we will investigate and evaluate the relevance of our data and compilation approach with respect to the RTE-5 Entailment Search pilot task14 and the RTE-6 Entailment Summarisation task15 , in which RTE systems are required to find all sentences in a document or a set that entail a given Hypothesis. RTE and its resources also tend to be utilized in the recently emerging task of stance detection (Mohammad et al., 2016), i.e. classification of the standpoint of an expression such as ”Climate change is a real concern” towards a piece of (social media) text as either supportive, denying, or neutral (Augenstein et al., 2016; Ferreira and Vlachos, 2016). It remains to be evaluated if the approaches built for stance detection are reusable or need specific adaptation to our goal of RTE in social media verification. Our current efforts include further development of the reported approach and the curation of project-internal data in other languages, in order to release16 several monolingual RTE benchmark resources."
L16-1729,D11-1147,0,0.0721362,"hod for the special purpose of information verification, which draws on manually established rumourous claims reported during crisis events. From about 500 English tweets related to 70 unique claims we compile and evaluate 5.4k RTE pairs, while continue automatizing the workflow to generate similar-sized datasets in other languages. Keywords: textual entailment, social media, verification 1. Introduction In this paper, we report on building a special-purpose Recognizing Textual Entailment (RTE) dataset in the context of information verification in user-generated content (Mendoza et al., 2010; Qazvinian et al., 2011; Procter et al., 2013) for the PHEME project1 . The dataset is compiled based on naturally occurring contradiction in manually labeled claims in crisis events discussed on Twitter, and to our knowledge is the first resource for RTE in the social media and verification domain. The detection of semantic inference phenomena between natural language text snippets, such as contradiction, entailment, and stance, is targeted by a number of research communities. Its most focused interest group formalizes inference tasks in the generic framework of RTE2 . RTE is applied to benefit several Natural Lang"
L16-1729,W07-1401,0,\N,Missing
L16-1729,N16-1170,0,\N,Missing
N18-1083,D17-1011,0,0.0714889,"Missing"
N18-1083,W18-0207,1,0.707325,"1957), summarised in the catchy phrase You shall know a word by the company it keeps (Firth, 1957). We argue that language embeddings, or distributed representations of language, can also be theoretically motivated by Chomsky’s Principles and Parameters framework (Chomsky and Lasnik, 1993; Chomsky, 1993, 2014). Language embeddings encode languages as dense real-valued vectors, in which the dimensions are reminiscent of the parameters found in this framework. Briefly put, Chomsky argues that languages can be described in terms of principles Contributions Our work bears the most resemblance to Bjerva and Augenstein (2018), who finetune language embeddings on the task of PoS tagging, and investigate how a handful of typological properties are coded in these for four Uralic languages. We expand on this and thus contribute to previous work by: (i) introducing novel qualitative investigations of language embeddings, in addition to thorough quantitative evaluations; (ii) considering four tasks at three different typological levels; (iii) considering a far larger sample of 908 (abstract rules) and parameters (switches) which can be turned either on or off for a given language (Chomsky and Lasnik, 1993). An example o"
N18-1083,P17-1109,0,0.0851896,"eural computational models. Even so, recent work only deals with fragments of typology compared to what we consider here. Computational typology has to a large extent focused on exploiting word or morpheme alignments on the massively parallel New Testament, in approximately 1,000 languages, in order to in¨ fer word order (Ostling, 2015) or assign linguistic categories (Asgari and Sch¨utze, 2017). W¨alchli (2014) similarly extracts lexical and grammatical markers using New Testament data. Other work has taken a computational perspective on language evolution (Dunn et al., 2011), and phonology (Cotterell and Eisner, 2017; Alishahi et al., 2017). Language embeddings In this paper, we follow an approach which has seen some attention in the past year, namely the use of distributed language representations, or language embeddings. Some ¨ typological experiments are carried out by Ostling and Tiedemann (2017), who learn language embeddings via multilingual language modelling and show that they can be used to reconstruct genealogical trees. Malaviya et al. (2017) learn language embeddings via neural machine translation, and predict syntactic, morphological, and phonetic features. 3.1.1 Language embeddings as contin"
N18-1083,K17-2001,0,0.0843525,"entations by training a recurrent neural language model (Mikolov et al., 2010) simultaneously for different languages ¨ (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017). In these recurrent multilingual language models with long short-term memory cells (LSTM, Hochreiter and Schmidhuber, 1997), languages are embedded into an n-dimensional space. In order for multilingual parameter sharing to be successful in this setting, the neural network is encouraged to use the language embeddings to encode features of language. In this pa¨ per, we explore the embeddings trained by Ostling and Tiedemann (2017), both in their original state, and by further tuning them for our four tasks. These are trained by training a multilingual language model with language representations on a collection of texts from the New Testament, covering 975 languages. While other work has looked at the types of representations encoded in different layers of deep neural models (K´ad´ar et al., 2017), we choose to look at the representations only in the bottom-most embedding layer. This is motivated by the fact that we look at several tasks using different neural architectures, and want to ensure comparability between the"
N18-1083,P16-1038,0,0.0438529,"uage embedding set on the typological property relevant to that dataset. In addition, we also evaluate on a set of all typological properties in WALS. Note that the evaluation on all properties is only comparable to the evaluation on each specific property, as the set of languages under consideration differs between tasks. Dataset Class G2P ASJP SIGMORPHON UD Phonology Phonology Morphology Syntax |Ltask | 311 4,664 52 50 |Ltask ∩ Lpre | 102 824 29 27 Table 1: Overview of tasks and datasets. 5 Phonology 5.1 Grapheme-to-phoneme We use grapheme-to-phoneme (G2P) as a proxy of a phonological task (Deri and Knight, 2016; Peters et al., 2017). The dataset contains over 650,000 such training instances, for a total of 311 languages (Deri and Knight, 2016). The task is to produce a phonological form of a word, given its orthographic form and the language in which it is written. Crucially, this mapping is highly different depending on the language at hand. For instance, take the word written variation, which exists in both English and French: (English, variation) -&gt; vE@ri""eIS@n (French, variation) -&gt; vaKja""sj˜O 5.1.1 Experiments and Analysis We train a sequence-to-sequence model with attention for the task of g"
N18-1083,K17-2012,1,0.887927,"17). All PoS tagging results tasks (morphological inflection, G2P,147 and phonological the morphological system, using grapheme-toported are the average of five runs, each with reconstruction). Figure adapted with148 permissiontofrom phoneme data. ferent initialisation seeds, so as to minimise r 149 ¨ 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 128 3 Morphology Unimorph 131 132 133 134 135 136 137 138 139 140 141 142 3.2 Morphological Experiments We train a sequence-to-sequence model based ¨ on the system developed by Ostling and Bjerva (2017). The neural architecture is modified so as to include an embedded language representation. During training, the errors are also backpropagated into this embedding, meaning that the encoded representation will be fine-tuned as the task is learned. The system architecture is depicted in Figure ??. ~l 4 Phonology 143 4.1 Grapheme-to-phoneme 144 g2p data 145 146 147 148 149 6 Method and experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, based on Plank et al. (2016). The system is implemented using DyNet (Neubig et al., 2017). We train using t"
N18-1083,E17-2102,0,0.128552,"¨ Ostling, 2015; Cotterell and Eisner, 2017; Asgari and Sch¨utze, 2017; Malaviya et al., 2017; Bjerva and Augenstein, 2018). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., ¨ 2016; Ostling and Tiedemann, 2017; Malaviya et al., 2017). We hypothesise that these language embeddings encode typological properties of language, reminiscent of the features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993). In this paper, we model languages in deep neural networks using language embeddings, considering three typological levels: phonology, morphology and syntax. We consider four NLP tasks to be representative of these levels: grapheme-tophoneme prediction and phoneme reconstruction, morphological inflection, and part-of-speech tagging. We pose three research que"
N18-1083,W17-5403,0,0.179856,"Missing"
N18-1083,P16-2067,0,0.421153,"Most Frequent Accuracy Accuracy 1.0 k-NN Most Frequent 0.4 0.2 0 200 Iterations 400 600 0.0 0 200 Iterations 400 600 Figure 2: Prediction of two morphological features in WALS with morphological language embeddings, one data point per 50 iterations. we are mainly interested in observing the language embeddings, we down-sample all training sets to 1,500 sentences (approximate number of sentences of the smallest data sets) so as to minimise any size-related effects. 7.1.1 Word-order experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture based on Plank et al. (2016), detailed in Section 8. Quantitative results Table 5 contains results on WALS feature prediction using language embeddings fine-tuned on PoS tagging. We consider both the set of word order features, which are relevant for the dataset, and a set of all WALS features. Using the fine-tuned embeddings is significantly better than both the baseline and the pre-trained embeddings (p < 0.05), in both the random and the unseen conditions, indicating that the model learns something about word order typology. This can be expected, as word order features are highly relevant when assigning a PoS tag to a"
N18-1083,D17-1268,0,0.326748,"d Eisner, 2017; Asgari and Sch¨utze, 2017; Malaviya et al., 2017; Bjerva and Augenstein, 2018). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., ¨ 2016; Ostling and Tiedemann, 2017; Malaviya et al., 2017). We hypothesise that these language embeddings encode typological properties of language, reminiscent of the features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993). In this paper, we model languages in deep neural networks using language embeddings, considering three typological levels: phonology, morphology and syntax. We consider four NLP tasks to be representative of these levels: grapheme-tophoneme prediction and phoneme reconstruction, morphological inflection, and part-of-speech tagging. We pose three research questions (RQs): A core par"
N18-1083,N16-1161,0,0.341773,"guage representations differ considerably depending on the target task. For instance, for grapheme-to-phoneme mapping, the differences between the representations for Norwegian Bokm˚al and Danish increase rapidly during training. This is due to the fact that, although the languages are typologically close to one another, they are phonologically distant. 2 3 Background 3.1 Distributed language representations There are several methods for obtaining distributed language representations by training a recurrent neural language model (Mikolov et al., 2010) simultaneously for different languages ¨ (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017). In these recurrent multilingual language models with long short-term memory cells (LSTM, Hochreiter and Schmidhuber, 1997), languages are embedded into an n-dimensional space. In order for multilingual parameter sharing to be successful in this setting, the neural network is encouraged to use the language embeddings to encode features of language. In this pa¨ per, we explore the embeddings trained by Ostling and Tiedemann (2017), both in their original state, and by further tuning them for our four tasks. These are trained by training a multilingual language mod"
N18-1083,P15-2034,0,0.0903062,"is motivated by the fact that we look at several tasks using different neural architectures, and want to ensure comparability between these. Related work Computational linguistics approaches to typology are now possible on a larger scale than ever before due to advances in neural computational models. Even so, recent work only deals with fragments of typology compared to what we consider here. Computational typology has to a large extent focused on exploiting word or morpheme alignments on the massively parallel New Testament, in approximately 1,000 languages, in order to in¨ fer word order (Ostling, 2015) or assign linguistic categories (Asgari and Sch¨utze, 2017). W¨alchli (2014) similarly extracts lexical and grammatical markers using New Testament data. Other work has taken a computational perspective on language evolution (Dunn et al., 2011), and phonology (Cotterell and Eisner, 2017; Alishahi et al., 2017). Language embeddings In this paper, we follow an approach which has seen some attention in the past year, namely the use of distributed language representations, or language embeddings. Some ¨ typological experiments are carried out by Ostling and Tiedemann (2017), who learn language em"
N18-1083,J17-4003,0,\N,Missing
N18-1083,L16-1262,0,\N,Missing
N18-1083,K17-1037,0,\N,Missing
N18-1172,E17-2026,1,0.796307,"ks, possibly accounting for its poor performance on the inference task. We did not evaluate the correlation between label embeddings and task performance, but Bjerva (2017) recently suggested that mutual information of target and auxiliary task label sets is a good predictor of gains from multi-task learning. 6.2 Auxilary Tasks For each task, we show the auxiliary tasks that achieved the best performance on the development data in Table 4. In contrast to most existing work, we did not restrict ourselves to performing multitask learning with only one auxiliary task (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017). Indeed we find that most often a combination of auxiliary tasks achieves the best performance. Indomain tasks are less used than we assumed; only Target is consistently used by all Twitter main tasks. In addition, tasks with a higher number of labels, e.g. Topic-5 are used more often. Such tasks provide a more fine-grained reward signal, which may help in learning representations that generalise better. Finally, tasks with large amounts Table 4: Best-performing auxiliary tasks for different main tasks. of training data such as FNC-1 and MultiNLI are also used more often. Even if not directly"
N18-1172,W17-0225,0,0.0664442,"o provides us with a picture of what auxilary tasks are beneficial, and to what extent we can expect synergies from multitask learning. For instance, the notion of positive sentiment appears to be very similar across the topic-based and aspect-based tasks, while the conceptions of negative and neutral sentiment differ. In addition, we can see that the model has failed to learn a relationship between MultiNLI labels and those of other tasks, possibly accounting for its poor performance on the inference task. We did not evaluate the correlation between label embeddings and task performance, but Bjerva (2017) recently suggested that mutual information of target and auxiliary task label sets is a good predictor of gains from multi-task learning. 6.2 Auxilary Tasks For each task, we show the auxiliary tasks that achieved the best performance on the development data in Table 4. In contrast to most existing work, we did not restrict ourselves to performing multitask learning with only one auxiliary task (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017). Indeed we find that most often a combination of auxiliary tasks achieves the best performance. Indomain tasks are less used than we assumed; only"
N18-1172,P17-1031,1,0.833536,"ed data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis. 1 Introduction Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP. Applications of MTL in NLP, for example, include partial parsing (Søgaard and Goldberg, 2016), text normalisation (Bollman et al., 2017), neural machine translation (Luong et al., 2016), and keyphrase boundary classification (Augenstein and Søgaard, 2017). Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks (Collobert et al., 2011; Søgaard and Goldberg, 2016). If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements (Baxter, 2000). However, while sharing hidden layers of neural networks is an effective regulariser (Søgaard and Goldberg, 2016), we pot"
N18-1172,S16-1044,0,0.060276,"Missing"
N18-1172,D16-1070,0,0.0255161,"can thus not be directly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers ha"
N18-1172,W17-5307,0,0.0309675,"Missing"
N18-1172,P14-2009,0,0.0469174,"o power at home, sat in the dark listening to AC/DC in the hope it’ll make the electricity come back again” known to be about the topic “AC/DC”, which is labelled as a positive sentiment. The evaluation metrics for Topic-2 and Topic-5 are macro-averaged recall (ρP N ) and macro-averaged mean absolute error (M AE M ) respectively, which are both averaged across topics. Target-dependent sentiment analysis Targetdependent sentiment analysis (Target) seeks to classify the sentiment of a text’s author towards an entity that occurs in the text as positive, negative, or neutral. We use the data from Dong et al. (2014). An example instance is the expression “how do you like settlers of catan for the wii?” which is labelled as neutral towards the target “wii’.’ The evaluation metric is macroaveraged F1 (F1M ). Fake news detection The goal of fake news detection in the context of the Fake News Challenge2 is to estimate whether the body of a news article agrees, disagrees, discusses, or is unrelated towards a headline. We use the data from the first stage of the Fake News Challenge (FNC-1). An example for this dataset is the document “Dino Ferrari hooked the whopper wels catfish, (...), which could be the bigg"
N18-1172,D16-1084,1,0.940888,"C in the hope it’ll make the electricity come back again Topic: AC/DC Label: positive Target-dependent sentiment analysis: Text: how do you like settlers of catan for the wii? Target: wii Label: neutral Aspect-based sentiment analysis: Text: For the price, you cannot eat this well in Manhattan Aspects: restaurant prices, food quality Label: positive Stance detection Stance detection (Stance) requires a model, given a text and a target entity, which might not appear in the text, to predict whether the author of the text is in favour or against the target or whether neither inference is likely (Augenstein et al., 2016). We use the data of SemEval-2016 Task 6 Subtask B (Mohammad et al., 2016). An example from this dataset would be to predict the stance of the tweet “Be prepared - if we continue the policies of the liberal left, we will be #Greece” towards the topic “Donald Trump”, labelled as “favor”. The evaluation metric is the macro-averaged F1 score of the “favour” and “against” classes (F1F A ). Stance detection: Tweet: Be prepared - if we continue the policies of the liberal left, we will be #Greece Target: Donald Trump Label: favor Fake news detection: Document: Dino Ferrari hooked the whopper wels ca"
N18-1172,W16-6208,1,0.846241,"we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabelled and auxiliary task"
N18-1172,S16-1010,0,0.0663037,"Missing"
N18-1172,D17-1169,1,0.779568,"and Goldberg, 2016), we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabel"
N18-1172,D17-1206,0,0.0278696,"ers of neural networks is an effective regulariser (Søgaard and Goldberg, 2016), we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-la"
N18-1172,S16-1023,0,0.0416916,"Missing"
N18-1172,P17-1186,0,0.0246557,"rectly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using au"
N18-1172,P16-2067,1,0.828447,"f our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate la"
N18-1172,P15-1046,0,0.0715286,"with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces Problem definition In our multi-task learning scenario, we have access to labelled datasets for T tasks T1 , . . . , TT at training time with a target task TT that we particularly care about. The training dataset for task Ti consists of Nk examples XTi = {xT1 i , . . . , xTNik } and their Ti labels YTi = {y1Ti , . . . , yN }. Our base model is k a deep neural network that performs classic hard parameter"
N18-1172,S16-1174,0,0.0165112,"e often highly specialised, taskdependent architectures. Our architectures, in contrast, have not been optimised to compare favourably against the state of the art, as our main objective is to develop a novel approach to multi-task learning leveraging synergies between label sets and knowledge of marginal distributions from unlabeled data. For example, we do not use pre-trained word embeddings (Augenstein et al., 2016; Palogiannidi et al., 2016; Vo and Zhang, 2015), class weighting to deal with label imbalance (Balikas and Amini, 2016), or domainspecific sentiment lexicons (Brun et al., 2016; Kumar et al., 2016). Nevertheless, our approach outperforms the state-of-the-art on two-way topic-based sentiment analysis (Topic-2). The poor performance compared to the stateof-the-art on FNC and MultiNLI is expected; as we alternate among the tasks during training, our model only sees a comparatively small number of examples of both corpora, which are one and two orders of magnitude larger than the other datasets. For this reason, we do not achieve good performance on these tasks as main tasks, but they are still useful as auxiliary tasks as seen in Table 4. 6 6.1 Analysis Label Embeddings Our results above s"
N18-1172,P17-1001,0,0.0348797,"009), induce a shared prior (Yu et al., 2005; Xue et al., 2007; Daumé III, 2009), or learn a grouping (Kang et al., 2011; Kumar and Daumé III, 2012). These approaches focus on homogeneous tasks and employ linear or Bayesian models. They can thus not be directly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closel"
N18-1172,S16-1003,0,0.11624,"Missing"
N18-1172,P17-1194,0,0.0446297,"a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces P"
N18-1172,N13-1008,0,0.0450068,"t of task Ti . In practice, we apply the same weight to all tasks. We show the full set-up in Figure 1a. 3.2 Label Embedding Layer In order to learn the relationships between labels, we propose a Label Embedding Layer (LEL) that embeds the labels of all tasks in a joint space. Instead of training separate softmax output layers as above, we introduce a label compatibility function c(·, ·) that measures how similar a label with embedding l is to the hidden representation h: c(l, h) = l · h (3) where · is the dot product. This is similar to the Universal Schema Latent Feature Model introduced by Riedel et al. (2013). In contrast to 1897 12/6/2017 multi-task_learning.html 12/6/2017 1  1 = (p ,y 1 ) 2  2 = (p ,y 2 ) 3  3 = (p label_embedding_layer.html ,y 3 i  i = (p ) 1 ∈ ℝ 2 L1 p L2 ∈ ℝ 3 p i ) l i p ,y 12/6/2017 p L3 ∈ ℝ 1 1 l Li ∈ ℝ 2 3 l label_transfer_network.html i  i = (p 2 1 l l ∈ ℝ l 1 2 ,y i ) 3 l 1 1 l 2 i  pseudo = p Li ∈ ℝ 2 3 l 2 1 l l ∈ ℝ l 3 2 1 2 l T MSE(p ,y T oi−1 ∈ ℝ ) ∗ l Label Embedding Layer Label Embedding Layer oi ∈ ℝ l oi+1 ∈ ℝ [⋅, ⋅] Label Transfer Network h h ∈ ℝ h h h ∈ ℝ h ∈ ℝ z i y ∈ Y i x i ∈ X i i (a) Multi-task learning y ∈ Y i"
N18-1172,D16-1103,1,0.852079,"able 1, and summarise examples in Table 2: Topic-based sentiment analysis Topic-based sentiment analysis aims to estimate the sentiment of a tweet known to be about a given topic. We use the data from SemEval-2016 Task 4 Subtask B and C (Nakov et al., 2016) for predicting on a twopoint scale of positive and negative (Topic-2) and five-point scale ranging from highly negative to highly positive (Topic-5) respectively. An example from this dataset would be to classify the 1899 whether an aspect, i.e. a particular property of an item is associated with a positive, negative, or neutral sentiment (Ruder et al., 2016). We use the data of SemEval-2016 Task 5 Subtask 1 Slot 3 (Pontiki et al., 2016) for the laptops (ABSA-L) and restaurants (ABSA-R) domains. An example is the sentence “For the price, you cannot eat this well in Manhattan”, labelled as positive towards both the aspects “restaurant prices” and “food quality”. The evaluation metric for both domains is accuracy (Acc). Topic-based sentiment analysis: Tweet: No power at home, sat in the dark listening to AC/DC in the hope it’ll make the electricity come back again Topic: AC/DC Label: positive Target-dependent sentiment analysis: Text: how do you lik"
N18-1172,D17-1038,1,0.926539,"onding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabelled and auxiliary task data by utilising the ‘dark knowledge’ (Hinton et al., 2015) contained in auxiliary model predictions. This pseudo-labelled data is then incorporated into the model via semisupervised learning, leading to a natural combination of multi-task learning and semi-supervised learning. We additionally augment the LTN with data-specific diversity features (Ruder and Plank, 2017) that aid in learning. Contributions Our contributions are: a) We model the relationships between labels by inducing a joint label space for multi-task learning. b) We propose a Label Transfer Network that learns to transfer labels between tasks and propose to use semi-supervised learning to leverage them for training. c) We evaluate MTL approaches on a variety of classification tasks and shed new light on settings where multi-task learning works. d) We perform an extensive ablation study of our model. 1896 Proceedings of NAACL-HLT 2018, pages 1896–1906 c New Orleans, Louisiana, June 1 - 6, 20"
N18-1172,P16-2038,1,0.857175,"eddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis. 1 Introduction Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP. Applications of MTL in NLP, for example, include partial parsing (Søgaard and Goldberg, 2016), text normalisation (Bollman et al., 2017), neural machine translation (Luong et al., 2016), and keyphrase boundary classification (Augenstein and Søgaard, 2017). Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks (Collobert et al., 2011; Søgaard and Goldberg, 2016). If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements (Baxter, 2000). However, while sharing hidden layers of neural networks is an effective regul"
N18-1172,D12-1125,0,0.0236666,"ereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces Problem definition In our multi-task learning scenario, we have access to labelled datasets for T tasks T1 , . . . , TT at training time with a target task TT that we particularly care about. The training dat"
N18-1172,W17-5301,0,\N,Missing
N19-1065,baccianella-etal-2010-sentiwordnet,0,0.0565213,"ayesian approach for aligning sentiment lexica with different continuous scales. SentiMerge consists of two steps: (i) aligning the lexica via rescaling, and (ii) combining the rescaled lexica using a Gaussian distribution. The authors perform token-level evaluation using a single sentiment analysis dataset where each token is labeled with its contextually dependent sentiment. Because SentiMerge can only combine lexica with continuous scales, we do not include it in our evaluation. 2 Sentiment Lexica and Scales We use the following commonly used Englishlanguage sentiment lexica: SentiWordNet (Baccianella et al., 2010), MPQA (Wilson et al., 2005), SenticNet 5 (Cambria et al., 2014), Hu-Liu (Hu and Liu, 2004), GI (Stone et al., 1962), and VADER (Hutto and Gilbert, 2014). Descriptive statistics for each lexicon are shown in Tab. 1. Each word in SentiWordNet is labeled with two real values, each in the interval [0, 1], corresponding to the strength of positive and negative sentiment (e.g., the label (0 0) is neutral, while the label (1 0) is maximally positive). Each word in VADER is labeled by ten different human evaluators, with each evaluator providing a polarity value on a nine-point scale (where the midpo"
N19-1065,P07-1056,0,0.143257,"ibed in §2. For each word w in the combined vocabulary, we obtain an estimate of z w by taking the mean of Qβw (z w ) = Dir(β w )—i.e., by normalizing β w . We compare this representation to using β w directly, because β w contains information about SentiVAE’s certainty about the word’s latent polarity value. We evaluate our common latent representation via a text classification task involving nine English-language sentiment analysis datasets: IMDB (Maas et al., 2011), Yelp (Zhang et al., 2015), SemEval 2017 Task 4 (SemEval, Rosenthal et al. (2017)), multi-domain sentiment analysis (MultiDom, Blitzer et al. (2007)), and PeerRead (Kang et al., 2018) with splits ACL 2017 and ICLR 2017 (Kang et al., 2018). Each dataset consists of multiple texts (e.g., tweets, articles), each labeled with an overall sentiment (e.g., positive). Descriptive statistics for each dataset are shown in Tab. 2. For the datasets with more than three sentiment labels, we consider two versions—the original and a version with only three (bucketed) sentiment labels. For each dataset, we transform each text into an average polarity value using either our representation, one of the six lexica,4 or a straightforward combination thereof,"
N19-1065,W14-5805,0,0.105563,"Ma et al., 2018). Indeed, given their utility, a veritable cottage industry has emerged focusing on the design of sentiment lexica. In practice, using any single lexicon, unless specifically and carefully designed for the particular domain of interest, has several downsides. For example, any lexicon will typically have low coverage compared to the language’s entire vocabulary, and may have misspecified labels for the domain. In many cases, it may therefore be desirable to combine multiple sentiment lexica into a single representation. Indeed, some research on unifying such lexica has emerged (Emerson and Declerck, 2014; Altrabsheh et al., 2017), borrowing ideas from crowdsourcing (Raykar et al., 2010; Hovy et al., 2013). However, this is a non-trivial task, because lexica can use binary, categorical, or continuous scales to quantify polarity—in addition to different interpretations for each—and thus cannot easily be combined. In Fig. 1, we show an example of the same word labeled using different lexica to illustrate the nature of the challenge. To combine sentiment lexica with disparate scales, we introduce SentiVAE, a novel multiview variant of the variational autoencoder (VAE) (Kingma and Welling, 2014)."
N19-1065,N13-1132,0,0.0300975,"of sentiment lexica. In practice, using any single lexicon, unless specifically and carefully designed for the particular domain of interest, has several downsides. For example, any lexicon will typically have low coverage compared to the language’s entire vocabulary, and may have misspecified labels for the domain. In many cases, it may therefore be desirable to combine multiple sentiment lexica into a single representation. Indeed, some research on unifying such lexica has emerged (Emerson and Declerck, 2014; Altrabsheh et al., 2017), borrowing ideas from crowdsourcing (Raykar et al., 2010; Hovy et al., 2013). However, this is a non-trivial task, because lexica can use binary, categorical, or continuous scales to quantify polarity—in addition to different interpretations for each—and thus cannot easily be combined. In Fig. 1, we show an example of the same word labeled using different lexica to illustrate the nature of the challenge. To combine sentiment lexica with disparate scales, we introduce SentiVAE, a novel multiview variant of the variational autoencoder (VAE) (Kingma and Welling, 2014). SentiVAE, visualized as a graphical model in Fig. 2, differs from the original VAE in two ways: (i) it"
N19-1065,N18-1149,0,0.0425344,"Missing"
N19-1065,P11-1015,0,0.0780531,"to compute the accuracy of the classifier. Experiments and Results To evaluate our approach, we first use SentiVAE to combine the six lexica described in §2. For each word w in the combined vocabulary, we obtain an estimate of z w by taking the mean of Qβw (z w ) = Dir(β w )—i.e., by normalizing β w . We compare this representation to using β w directly, because β w contains information about SentiVAE’s certainty about the word’s latent polarity value. We evaluate our common latent representation via a text classification task involving nine English-language sentiment analysis datasets: IMDB (Maas et al., 2011), Yelp (Zhang et al., 2015), SemEval 2017 Task 4 (SemEval, Rosenthal et al. (2017)), multi-domain sentiment analysis (MultiDom, Blitzer et al. (2007)), and PeerRead (Kang et al., 2018) with splits ACL 2017 and ICLR 2017 (Kang et al., 2018). Each dataset consists of multiple texts (e.g., tweets, articles), each labeled with an overall sentiment (e.g., positive). Descriptive statistics for each dataset are shown in Tab. 2. For the datasets with more than three sentiment labels, we consider two versions—the original and a version with only three (bucketed) sentiment labels. For each dataset, we t"
N19-1065,R15-1064,0,0.0606215,"Missing"
N19-1065,S16-1023,0,0.0456651,"Missing"
N19-1065,P79-1022,0,0.264382,"Missing"
N19-1065,H05-1044,0,0.0391287,"timent lexica with different continuous scales. SentiMerge consists of two steps: (i) aligning the lexica via rescaling, and (ii) combining the rescaled lexica using a Gaussian distribution. The authors perform token-level evaluation using a single sentiment analysis dataset where each token is labeled with its contextually dependent sentiment. Because SentiMerge can only combine lexica with continuous scales, we do not include it in our evaluation. 2 Sentiment Lexica and Scales We use the following commonly used Englishlanguage sentiment lexica: SentiWordNet (Baccianella et al., 2010), MPQA (Wilson et al., 2005), SenticNet 5 (Cambria et al., 2014), Hu-Liu (Hu and Liu, 2004), GI (Stone et al., 1962), and VADER (Hutto and Gilbert, 2014). Descriptive statistics for each lexicon are shown in Tab. 1. Each word in SentiWordNet is labeled with two real values, each in the interval [0, 1], corresponding to the strength of positive and negative sentiment (e.g., the label (0 0) is neutral, while the label (1 0) is maximally positive). Each word in VADER is labeled by ten different human evaluators, with each evaluator providing a polarity value on a nine-point scale (where the midpoint is neutral), yielding a"
N19-1065,S17-2088,0,\N,Missing
N19-1142,D16-1084,1,0.929529,"Missing"
N19-1142,N18-1172,1,0.852195,"y describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The baseline LSTM model corresponds to the same architecture with only one task. Figure 2: Improvement in F-score over"
N19-1142,P17-2054,1,0.857426,"t domain, multi-task learning and adversarial learning. We briefly describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The baseline LSTM model corresponds to the same architectur"
N19-1142,N16-1138,0,0.0231077,"evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et al., 2015) with a subset of the frames in the Policy Frames Codebook. The Argument Extraction 1 Code and annotations are available at https:// github.com/coastalcph/issue_framing. Corpus is a collection of argumentative dialogues across topics and platforms.2 The corpus contains posts on the following topics: gay marriage, gun control, death penalty and evolution. A subset of the corpus was annotated with argument qual"
N19-1142,D18-1393,0,0.0563443,"ructure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resou"
N19-1142,N15-1171,0,0.0255579,"-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al"
N19-1142,E17-2026,1,0.902594,"Missing"
N19-1142,P17-1092,0,0.0373499,"distribution in the online discussion test set. The frame labels correspond to the classes Economic (1), Political (13), Legality, Jurisprudence and Constitutionality (5), Policy prescription and evaluation (6) and Crime and Punishment (7). multi-task and adversarial learning, leading to improved results in the target domain.1 Related Work Previous work on automatic frame classification focused on news articles and social media. Card et al. (2016) predict frames in news articles at the document level, using clusters of latent dimensions and word-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in po"
N19-1142,P15-2072,0,0.369883,"inion (Dardis et al., 2008; Iyengar, 1991). As an illustration, contrast the statement Illegal workers depress wages with This country is abusing and terrorizing undocumented immigrant workers. The first statement puts focus on the economic consequences of immigration, whereas the second one evokes a morality frame by pointing out the inhumane conditions under which immigrants may have to work. Being exposed to primarily one of those perspectives might affect the publics attitude towards immigration. Computational methods for frame classification have previously been studied in news articles (Card et al., 2015) and social media posts (Johnson et al., 2017). In this work, we introduce a new benchmark dataset, based on a subset of the 15 generic frames in the Policy Frames Codebook by Boydstun et al. (2014). We focus on frame classification in online discussion fora, which have beLegality Frame, Topic: Same sex marriage Congress must fight to ensure LGBT people have the full protection of the law everywhere in America. #EqualityAct Table 1: Example instances from the datasets described in §2 and 3. come crucial platforms for public dialogue on social and political issues. Table 1 shows example annotat"
N19-1142,D16-1148,0,0.0133088,"01–1407 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Frames # instances 1 78 13 96 5 234 6 166 7 186 Table 2: Class distribution in the online discussion test set. The frame labels correspond to the classes Economic (1), Political (13), Legality, Jurisprudence and Constitutionality (5), Policy prescription and evaluation (6) and Crime and Punishment (7). multi-task and adversarial learning, leading to improved results in the target domain.1 Related Work Previous work on automatic frame classification focused on news articles and social media. Card et al. (2016) predict frames in news articles at the document level, using clusters of latent dimensions and word-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All t"
N19-1142,P15-1166,0,0.035307,"iew over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The baseline LSTM model corresponds to the same architecture with only one task. Figure 2: Improvement in F-score over the random baseline by class. The absolute"
N19-1142,P17-1069,0,0.187939,"Missing"
N19-1142,I11-1129,0,0.0641425,"Missing"
N19-1142,S16-1003,0,0.0875545,"Missing"
N19-1142,naderi-hirst-2017-classifying,0,0.154555,"tical (13), Legality, Jurisprudence and Constitutionality (5), Policy prescription and evaluation (6) and Crime and Punishment (7). multi-task and adversarial learning, leading to improved results in the target domain.1 Related Work Previous work on automatic frame classification focused on news articles and social media. Card et al. (2016) predict frames in news articles at the document level, using clusters of latent dimensions and word-based features in a logistic regression model. Ji and Smith (2017) improve on previous work integrating discourse structure into a recursive neural network. Naderi and Hirst (2017) use the same resource, but make predictions at the sentence level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the"
N19-1142,D14-1162,0,0.0838758,"ific features. During training, the model alternates between 1) pre5 Experiments We compare the multi-task learning and the adversarial setup with two baseline models: (a) a Random Forest classifier using tf-idf weighted bagof-words-representations, and (b) the LSTM baseline model. For the multi-task model, we use both the Twitter dataset and the argument quality dataset as auxiliary tasks. For all models, we report results on the test set using the optimal hyperparameters that we found averaged over 3 runs on the validation set. For the neural models, we use 100-dimensional GloVe embeddings (Pennington et al., 2014), pre-trained on Wikipedia and Gigaword.6 Details about hyper-parameter tuning and optimal settings can be found in Appendix B. Results The results in Table 5 show that both the multi-task and the adversarial model improve over 5 In the forward pass, this layer multiplies its input with the identity matrix. 6 https://nlp.stanford.edu/projects/ glove/ 1404 Nr. Gold Adv MTL LSTM (1) 5 5 5 7 Sentence But, star gazer, we had guns then when the Constitution was written and enshrined in the BOR and now incorporated into th 14th Civil Rights Amendment. (2) 6 6 5 1 Gun control is about preventing such"
N19-1142,P13-1162,0,0.0259738,"t al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et al., 2015) with a subset of the frames in the Policy Frames Codebook. The Argument Extraction 1 Code and annotations are available at https:// github.com/coastalcph/issue_framing. Corpus is a collection of argumentative dialogues across topics and platforms.2 The corpus contains posts on the following topics: gay marriage,"
N19-1142,P17-1194,0,0.0148855,"nformation from source to target domain, multi-task learning and adversarial learning. We briefly describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7 13 Figure 1: Overview over the multi-task model (left) and the adversarial model (right). The base"
N19-1142,P16-2038,1,0.826517,"nd Schmidhuber) trained on only the news articles data. We then apply two strategies to facilitate the transfer of information from source to target domain, multi-task learning and adversarial learning. We briefly describe both setups in the following. An overview over tasks and data used in the different models is shown in Table 3. Multi-Task Learning To exploit synergies between additional datasets/annotations, we explore a simple multi-task learning with hard parameter sharing strategy, pioneered by Caruana (1993), introduced in the context of NLP by Collobert et al. (2011), and to RNNs by Søgaard and Goldberg (2016), which has been shown to be useful for a variety of NLP tasks, e.g. sequence labelling (Rei, 2017; Ruder et al., 2019; Augenstein and Søgaard, 2017), pairwise sequence classification (Augenstein et al., 2018) or machine translation (Dong et al., 2015). Here, parameters are shared between hidden layers. Intuitively, it works by training several networks in parallel, tying a subset of the hidden parameters so that updates in one network affect the parameters of the others. By sharing parameters, the networks regularize each other, and the network for one task can benefit from repre1403 1 5 6 7"
N19-1142,W15-4631,0,0.0851032,"al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et al., 2015) with a subset of the frames in the Policy Frames Codebook. The Argument Extraction 1 Code and annotations are available at https:// github.com/coastalcph/issue_framing. Corpus is a collection of argumentative dialogues across topics and platforms.2 The corpus contains posts on the following topics: gay marriage, gun control, death penalty and evolution. A subset of the corpus was annotated with argument quality scores by Swanson et al. (2015), which we exploit in our multi-task setup (see §3). We collect new issue frame annotations for each argument in the argument-quality annotated data.3 We"
N19-1142,P15-1157,0,0.0885542,"e level, using topic models and recurrent neural networks. Johnson et al. (2017) predict frames in social media data at the micro-post level, using probabilistic soft logic based on lists of keywords, as well as temporal similarity and network structure. All the work mentioned above uses the generic frames of Boydstun et al. (2014)’s Policy Frames Codebook. Baumer et al. (2015) predict words perceived as frame-evoking in political news articles with hand-crafted features. Field et al. (2018) analyse how Russian news articles frame the U.S. using a keyword-based cross-lingual projection setup. Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011), detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; Augenstein et al., 2016; Ferreira and Vlachos, 2016). 2 Online Discussion Annotations We create a new resource of issue-frame annotated online fora discussions, by annotating a subset of the Argument Extraction Corpus (Swanson et"
N19-1142,walker-etal-2012-corpus,0,0.0383126,"have labeled training data for this domain, we exploit additional corpora and additional annotations, which are described in the next subsection. Statistics of the filtered datasets as well as preprocessing details are given in Appendix A. Media Frames Corpus The Media Frames Corpus (Card et al., 2015) contains US newspaper articles on three topics: Immigration, smoking and same-sex marriage. The articles are annotated with the 15 framing dimensions defined in the Policy Frames Codebook.4 The annotations are on 2 The corpus is a combination of dialogues from http: //www.createdebate.com/, and Walker et al. (2012)’s Internet Argument Corpus, which contains dialogues from 4forums.com. 3 Topic cluster Evolution was dropped, because it contained too few examples matching our frame categories. 4 We discard all instances that do not correspond to the frame categories in the online discussions data. 1402 Model Baseline Task Main task Target task Domain Labelset # classes # sequences News articles Online disc. (test) Frames Frames 5 5 10,480 692 Multitask +Aux task +Aux task Tweets Online disc. Frames Argument quality 5 2 1,636 3,785 Adversarial +Adv task Online disc. + News articles Domain 2 4,731 + 10,480 O"
N19-1156,E17-2102,0,0.104682,"ded, so can languages, by associating each language with a real-valued vector known as a language embedding. Training such representations as a part of a multilingual model allows us to infer similarities between languages. This is due to the fact that in order for multilingual parameter sharing to be successful in this setting, the neural network needs to use the language embeddings to encode features of the languages. Previous work has explored this type of representation learning in various tasks, such as NMT (Malaviya et al., 2017), ¨ language modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017), and tasks representing morphological, phonological, and syntactic linguistic levels (Bjerva and Augenstein, 2018a). In the context of computational typology, representations obtained through language modelling ¨ have been the most successful (Ostling and Tiedemann, 2017). This approach is particularly interesting since unlabelled data is available for a large portion of the world’s languages, meaning that high quality language embeddings can be obtained for more than 1,000 of the world’s languages. Language Embeddings through LMs In this work, we use a language modelling objective to pre-tra"
N19-1156,P14-1130,0,0.0321674,"ar (Breese et al., 1998) to overcome the cold start problem arising for unseen users or items at test time. The most successful one of these, in turn, is matrix factorisation, as applied in this paper, which represents users and items as (dense) vectors in the same latent feature space and measures their compatibility by taking the dot product between the two representations (Koren et al., 2009; Bokde et al., 2015). Beyond recommender systems, matrix factorisation has shown successes in a wide variety of subareas of NLP (Riedel et al., 2013; Rockt¨aschel et al., 2015; Levy and Goldberg, 2014; Lei et al., 2014; Augenstein et al., 2018). 10 Conclusion We introduce a generative model inspired by the principles-and-parameters framework, drawing on the correlations between typological features of languages to solve the novel task of typological collaborative filtering. We further show that raw text can be utilised to infer similarities between languages, thus allowing for extending the method with semi-supervised language embeddings. Acknowledgements We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corporation with the"
N19-1156,D17-1268,0,0.167387,"rrently often in the form of word embeddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language embedding. Training such representations as a part of a multilingual model allows us to infer similarities between languages. This is due to the fact that in order for multilingual parameter sharing to be successful in this setting, the neural network needs to use the language embeddings to encode features of the languages. Previous work has explored this type of representation learning in various tasks, such as NMT (Malaviya et al., 2017), ¨ language modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017), and tasks representing morphological, phonological, and syntactic linguistic levels (Bjerva and Augenstein, 2018a). In the context of computational typology, representations obtained through language modelling ¨ have been the most successful (Ostling and Tiedemann, 2017). This approach is particularly interesting since unlabelled data is available for a large portion of the world’s languages, meaning that high quality language embeddings can be obtained for more than 1,000 of the world’s languages. Language Embeddings"
N19-1156,N13-1095,0,0.0145181,"s. 6 Data WALS. The World Atlas of Language Structures (WALS) is a large knowledge base of typological properties at the lexical, phonological, syntactic and semantic level on which we will run our experiments. The documentation of linguistic structure is spread throughout a wide variety of academic works, ranging from field linguistics to grammars describing the nuances of individual grammatical uses. KB creation is a laborious tasks as it involves distilling knowledge into a single, standardised resource, which, naturally, will be incomplete, prompting the need for methods to complete them (Min et al., 2013). In the case of WALS, few languages have all values annotated for all of the properties. In this section, we offer a formalisation of typological KBs to allow for our development of a probabilistic model over vectors of properties. WALS, for instance, contains n = 202 different parameters (Dryer and Haspelmath, 2013). Binarisation of WALS. Many common typological KBs, including WALS, the one studied here, contain binary as well as non-binary parameters. To deal with this, we binarise the KB as follows: Whenever there is a typological parameter that takes ≥ 3 values, e.g., ‘Feature 81A: Order"
N19-1156,I17-1046,0,0.0669546,"rvey of typology in NLP, see Ponti et al. (2018). Figure 7: Accuracy per feature group (Germanic). stein (2018a), might also be useful in our semisupervised extension of typological collaborative filtering. 9 Related Work Computational Typology The availability of unlabelled datasets for hundreds of languages permits inferring linguistic properties and categories ¨ (Ostling, 2015; Asgari and Sch¨utze, 2017). Individual prediction of typological features has been attempted in conjunction with several NLP tasks (Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b). Our work is most similar to Murawaki (2017), who presents a Bayesian approach to utilising relations between features and languages for feature prediction. However, our work differs on several important counts, as we (i) include language information obtained through unsupervised learning, which allows us to take advantage of raw data and predict features for completely unannotated languages, (ii) analyse the effects of varying amounts of known features, especially in situations with and without in-branch training data, and (iii) view the problem of typological features through the lens of parameters from principles and parameters (Chom"
N19-1156,N13-1008,0,0.312537,"oes the language admit word-final voiced obstruents? 3 A Generative Model of Typology We now seek a probabilistic formalisation of the linguistic theory presented in §2; specifically, for every language `, we seek to explain the observed binary vector of parameters π ` : πi` = 1 indicates that the ith parameter is “on” in language `. The heart of our model will be quite simple: every language ` will have a language embedding λ` ∈ Rd and every parameter will have a parameter embed` ding eπi ∈ Rd . Now, πi` ∼ sigmoid(e&gt; πi λ ). This model also takes inspiration from work in relation extraction (Riedel et al., 2013). Writing the joint distribution over the entire binary vector of parameters, we arrive at p(π ` |λ` ) = |π| Y (1)   ` sigmoid e&gt; πi λ (2) i=1 = |π| Y i=1 = |π| Y 1 ` 1 + exp(−e&gt; πi λ ) i=1 (3) 1 For an overview of differences between these schools, we refer the reader to Haspelmath (2008). `∈L Note that p(λ` ) is, spiritually at least, a universal grammar: it is the prior over what sort of languages can exist, albeit encoded as a real vector. In the parlance of principles and parameters, the prior represents the principles. Then our model parameters are Θ = {eπ1 , . . . , eπ|π |, λ1 , . . ."
N19-1156,N15-1118,0,0.0408962,"Missing"
P13-2052,W09-1604,0,0.0267083,"(2012) extract product properties from an e-commerce website and align equivalent properties using a supervised maximum entropy classification method. We study linking relations on Linked Data and propose an unsupervised method. Fu et al. (2012) identify similar relations using the overlap of the subjects of two relations and the overlap of their objects. On the contrary, we aim at identifying strictly equivalent relations rather than similarity in general. Additionally, the techniques introduced our work is also related to work on aligning multilingual Wikipedia resources (Adar et al., 2009; Bouma et al., 2009) and semantic relatedness (Budanitsky and Hirst, 2006). 3 Triple overlap evaluates the degree of overlap2 in terms of the usage of relations in triples. Let SO(p) be the collection of subject-object pairs from rp and SOint the intersection [1] then the triple overlap TO(p, p’) is calculated as SOint ( p , p&apos; )  SO( rp )  SO( rp&apos; ) MAX { Method Let t denote a 3-tuple (triple) consisting of a subject (ts), predicate (tp) and object (to). Linked Data resources are typed and its type is called class. We write type (ts) = c meaning that ts is of class c. p denotes a relation and rp is a set of tr"
P13-2052,J06-1003,0,\N,Missing
P13-2052,P12-1085,0,\N,Missing
P17-2054,J14-1002,0,0.0173998,"urer, 2007). Here, we follow the probably most common approach to multi-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (Qase"
P17-2054,P05-1045,0,0.0412449,"Missing"
P17-2054,P14-1119,0,0.0792035,"harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurrent neural networks. We leave exploring such models in combination with multi-task learning for future work. Keyphrase detection methods specific to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014). However, previous methods relied on corpora annotated for type-level identification, not for mention-level identification (Kim et al., 2010; Sterckx et al., 2016). While most applications Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected layers (Søgaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). Søgaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on"
P17-2054,S14-1001,1,0.774038,"Missing"
P17-2054,S17-2091,1,0.835359,"Missing"
P17-2054,D15-1168,0,0.0322281,"to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014). However, previous methods relied on corpora annotated for type-level identification, not for mention-level identification (Kim et al., 2010; Sterckx et al., 2016). While most applications Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected layers (Søgaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). Søgaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on heterogeneous tasks such as the ones considered here. Hard parameter sharing has been studied for several tasks, including CCG super tagging (Søgaard and Goldberg, 2016), text normalisation (Bollman and Søgaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart´ınez Alonso and Plank, 2017). Sharing of information can further be achieved by extending LSTMs with an external memory shared 344 rely on extracting keyphrases (as types), this has t"
P17-2054,D16-1012,0,0.0275467,"+ Multi-word BiLSTM + Super-sense 83.36 84.11 83.94 84.86 84.67 79.46 79.39 79.12 76.92 78.29 59.26 60.64 60.18 59.81 61.35 57.24 57.24 56.73 54.21 56.73 81.37 81.68 81.46 80.69 81.36 57.84 58.89 58.40 56.87 58.95 Table 3: Results for keyphrase boundary classification on the ACL RD-TEC corpus automatic discovery of text in parallel translation’, ‘honeycomb network of graphite bricks’). Being able to recognise long keyphrases correctly is part of the reason our multi-task models outperform the baselines, especially on the SemEval dataset, which contains many such long keyphrases. across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and autoencoding, and in Rei (2017) for different sequence labelling tasks and language modelling. 6 Related Work Boundary Classification KBC is very similar to named entity recognition (NER), though arguably harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the pro"
P17-2054,C16-1013,1,0.853895,"Missing"
P17-2054,E17-1005,0,0.115293,"Missing"
P17-2054,L16-1294,0,0.104901,"014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). The SemEval 2017 dataset is annotated with three keyphrase types, the ACL RD-TEC dataset with seven. For the former, we test on the development portion of the dataset, as the test set is not released yet. We randomly split ACL RD-TEC into a training and test set, reserv342 SemEval 2017 Task 10 Labels ACL RD-TEC Material, Process, Task Technology and Method, Tool and Library, Language Resource, Language Resource Product, Measures and Measurements, Models, Other Topics Computer Science, Physics, Natural Language Processing Material Science Number all keyphrases 5730 2939 Proportion singleton k"
P17-2054,P17-1194,0,0.0589963,"e boundary classification on the ACL RD-TEC corpus automatic discovery of text in parallel translation’, ‘honeycomb network of graphite bricks’). Being able to recognise long keyphrases correctly is part of the reason our multi-task models outperform the baselines, especially on the SemEval dataset, which contains many such long keyphrases. across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and autoencoding, and in Rei (2017) for different sequence labelling tasks and language modelling. 6 Related Work Boundary Classification KBC is very similar to named entity recognition (NER), though arguably harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurrent neural networks. We leave exploring such models in combination wi"
P17-2054,N15-1177,0,0.0282534,"ontext of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). The SemEval 2017 dataset is annotated with three keyphrase types, the ACL RD-TEC dataset with seven. For the former, we test on the development po"
P17-2054,P16-2038,1,0.934379,"st as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007). Here, we follow the probably most common approach to multi-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classifi"
P17-2054,P10-1130,0,0.0132169,"lti-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1 , · · · , DT , Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following Søgaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels. Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). The SemEval 2017 dataset is annotated with thr"
P17-2054,D16-1198,0,0.102576,"ds, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurrent neural networks. We leave exploring such models in combination with multi-task learning for future work. Keyphrase detection methods specific to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014). However, previous methods relied on corpora annotated for type-level identification, not for mention-level identification (Kim et al., 2010; Sterckx et al., 2016). While most applications Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected layers (Søgaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). Søgaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on heterogeneous tasks such as the ones considered here. Hard parameter sharing has been studied for several tasks, including CCG super tagging (Søgaard and Goldberg,"
P17-2054,D16-1138,0,0.023168,"46 80.69 81.36 57.84 58.89 58.40 56.87 58.95 Table 3: Results for keyphrase boundary classification on the ACL RD-TEC corpus automatic discovery of text in parallel translation’, ‘honeycomb network of graphite bricks’). Being able to recognise long keyphrases correctly is part of the reason our multi-task models outperform the baselines, especially on the SemEval dataset, which contains many such long keyphrases. across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and autoencoding, and in Rei (2017) for different sequence labelling tasks and language modelling. 6 Related Work Boundary Classification KBC is very similar to named entity recognition (NER), though arguably harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurre"
P18-4005,D16-1244,0,0.221957,"ther popular MR task is Natural Language Inference, also known as Recognising Textual Entailment (RTE). The task is to predict whether a hypothesis is entailed by, contradicted by, or neutral with respect to a given premise. In JACK, NLI is viewed as 28 Model Original F1 JACK F1 Speed #Params BiDAF FastQA JackQA 77.3 76.3 – 77.8 77.4 79.6 1.0x 2.2x 2.0x 2.02M 0.95M 1.18M Dataset Table 1: Metrics on the SQuAD development set comparing F1 metric from the original implementation to that of JACK, number of parameters, and relative speed of the models. Model cBiLSTM (Rocktäschel et al., 2016) DAM (Parikh et al., 2016) ESIM (Chen et al., 2017) Original JACK – 86.6 88.0 82.0 84.6 87.2 Model MRR Hits@3 Hits@10 WN18 DistMult ComplEx 0.822 0.941 0.914 0.936 0.936 0.947 WN18RR DistMult ComplEx 0.430 0.440 0.443 0.461 0.490 0.510 FB15k-237 DistMult ComplEx 0.241 0.247 0.263 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al.,"
P18-4005,D16-1084,1,0.852145,"o quickly set up, load and run the existing systems for QA and NLI. The model training notebook demonstrates training, testing, evaluating and saving QA and NLI models programmatically. However, normally the user will simply use the provided training script from command line. The model implementation notebook delves deeper into implementing new models from scratch by writing all modules for a custom model. 7 Natural Language Inference. For NLI, we report results for our implementations of conditional BiLSTMs (cBiLSTM) (Rocktäschel et al., 2016), the bidirectional version of conditional LSTMs (Augenstein et al., 2016), the Decomposable Attention Model (DAM, Parikh et al., 2016) and Enhanced LSTM (ESIM, Chen et al., 2017). ESIM was entirely implemented as a modular NLI model, i.e. its architecture was purely defined in a configuration file – see Appendix A for more details. Our models or training configurations contain slight modifications from the original which we found to perform better than the original setup. Our results are slightly differ from those reported, since we did not always perform an exhaustive hyper-parameter search. Conclusion We presented Jack the Reader (JACK), a shared framework for Ma"
P18-4005,D16-1264,0,0.300836,"efore, existing input- and output modules that are responsible for pre- and post-processing can be reused in most cases, which enables researchers to focus on prototyping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or"
P18-4005,D15-1075,0,0.0435049,"yping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: ea"
P18-4005,W15-4007,0,0.0320485,"e that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: each (s, p, o) triple denotes a fact, represented as a relationship of type p between entities s and o, such as: (L ONDON, CAPITAL O F, UK). Realworld Knowledge Graphs, such as Freebase (Bo"
P18-4005,P17-1152,0,0.157395,"ural Language Inference, also known as Recognising Textual Entailment (RTE). The task is to predict whether a hypothesis is entailed by, contradicted by, or neutral with respect to a given premise. In JACK, NLI is viewed as 28 Model Original F1 JACK F1 Speed #Params BiDAF FastQA JackQA 77.3 76.3 – 77.8 77.4 79.6 1.0x 2.2x 2.0x 2.02M 0.95M 1.18M Dataset Table 1: Metrics on the SQuAD development set comparing F1 metric from the original implementation to that of JACK, number of parameters, and relative speed of the models. Model cBiLSTM (Rocktäschel et al., 2016) DAM (Parikh et al., 2016) ESIM (Chen et al., 2017) Original JACK – 86.6 88.0 82.0 84.6 87.2 Model MRR Hits@3 Hits@10 WN18 DistMult ComplEx 0.822 0.941 0.914 0.936 0.936 0.947 WN18RR DistMult ComplEx 0.430 0.440 0.443 0.461 0.490 0.510 FB15k-237 DistMult ComplEx 0.241 0.247 0.263 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) on various dataset"
P18-4005,W17-2623,0,0.0840343,"Missing"
P18-4005,P02-1022,0,0.0771673,"I N PUT , M ODEL and O UTPUT modules that compose a JTR EADER instance. On the right, the data format that is used to interact with a JTR EADER (dotted lines indicate that the component is optional). Related Work Machine Reading requires a tight integration of Natural Language Processing and Machine Learning models. General NLP frameworks include C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), O PEN NLP6 and SPAC Y. All these frameworks offer pre-built models for standard NLP preprocessing tasks, such as tokenisation, sentence splitting, named entity recognition and parsing. GATE (Cunningham et al., 2002) and UIMA (Ferrucci and Lally, 2004) are toolkits that allow quick assembly of baseline NLP pipelines, and visualisation and annotation via a Graphical User Interface. GATE can utilise NLTK and C ORE NLP models and additionally enable development of rule-based methods using a dedicated pattern language. UIMA offers a text analysis pipeline which, unlike GATE, also includes retrieving information, but does not offer its own rule-based language. It is further worth mentioning the Information Retrieval frameworks A PACHE L UCENE and A PACHE S OLR which can be used for building simple, keyword-bas"
P18-4005,K17-1028,1,0.844327,"63 0.275 0.419 0.428 Table 3: Link Prediction results, measured using the Mean Reciprocal Rank (MRR) and Hits@10, for DistMult (Yang et al., 2015), and ComplEx (Trouillon et al., 2016). Link Prediction. For Link Prediction in Knowledge Graphs, we report results for our implementations of DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) on various datasets. Results are otlined in Table 3. Table 2: Accuracy on the SNLI test set achieved by cBiLSTM, DAM, and ESIM. Question Answering. For the Question Answering (QA) experiments we report results for our implementations of FastQA (Weissenborn et al., 2017), BiDAF (Seo et al., 2016) and, in addition, our own JackQA implementations. With JackQA we aim to provide a fast and accurate QA model. Both BiDAF and JackQA are realised using high-level architecture descriptions, that is, their architectures are purely defined within their respective configuration files. Results of our models on the SQuAD (Rajpurkar et al., 2016) development set along with additional run-time and parameter metrics are presented in Table 1. Apart from SQuAD, JACK supports the more recent NewsQA (Trischler et al., 2017) and TriviaQA (Joshi et al., 2017) datasets too. 6 Demo W"
P18-4005,N18-1101,0,0.0218508,"lthough we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Knowledge Graph is a set of (s, p, o) triples, where s, o denote the subject and object of the triple, and p denotes its predicate: each (s, p, o) triple denotes a fact, re"
P18-4005,P17-1147,0,0.188943,"odules that are responsible for pre- and post-processing can be reused in most cases, which enables researchers to focus on prototyping and implementing new models. Although we acknowledge that most of the pre-processing can easily be performed by third-party libraries such as C ORE NLP, NLTK or SPAC Y, we argue that additional functional7 For instance, see https://github.com/uclmr/ jack/blob/master/conf/nli/esim.yaml 27 Dataset Coverage. JACK allows parsing a large number of datasets for QA, NLI, and Link Prediction. The supported QA datasets include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and QAngaroo (Welbl et al., 2017). The supported NLI datasets include SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). The supported Link Prediction datasets include WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), and FB15k-237 (Toutanova and Chen, 2015). an instance of multiple-choice Question Answering problem, by casting the hypothesis as the question, and the premise as the support. The answer candidates to this question are the three possible outcomes or classes – namely entails, contradicts or neutral. Link Prediction. A Kno"
P18-4005,P14-5010,0,0.00961028,"Missing"
P19-1167,S13-1035,0,0.0607054,"djectives used to describe men? Can we quantify such patterns using existing semantic resources (Tsvetkov et al., 2014)? gender from grammatical gender because the latter does not necessarily convey anything meaningful about the referent. 2 Men are written about more often than women. Indeed, the corpus we use exhibits this trend, as shown in Tab. 1. Female Male other daughter lady wife mother girl woman 2.2 1.4 2.4 3.3 4.2 5.1 11.5 Total 30.2 other husband king son father boy man 6.8 1.8 2.1 2.9 4.2 5.1 39.9 62.7 Table 1: Counts, in millions, of male and female nouns present in the corpus of Goldberg and Orwant (2013). Q3 Does the overall sentiment of the language used to describe men and women differ? To answer these questions, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We use a form of posterior regularization to guide inference of the latent variables (Ganchev et al., 2010). We then use this model to study the syntactic n-gram corpus of (Goldberg and Orwant, 2013). To answer Q1, we conduct an analysis that reveals differences between descriptions of male and female nouns t"
P19-1167,P84-1044,0,0.340361,"Missing"
P19-1167,N19-1065,1,0.870199,"Missing"
P19-1167,1998.amta-tutorials.1,0,0.664349,"female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men. 1 Introduction Word choice is strongly influenced by gender— both that of the speaker and that of the referent (Lakoff, 1973). Even within 24 hours of birth, parents describe their daughters as beautiful, pretty, and cute far more often than their sons (Rubin et al., 1974). To date, much of the research in sociolinguistics on gendered language has focused on laboratory studies and smaller corpora (McKee and Sherriffs, 1957; Williams and Bennett, 1975; Baker, 2005); however, more recent work has begun to focus on larger-scale datasets (Pearce, 2008; CaldasCoulthard and Moon, 2010; Baker, 2014; Norberg, 2016). These studies compare the adjectives (or Female Positive Negative Positive Negative beautiful lovely chaste gorgeous fertile beauteous sexy classy exquisite vivacious vibrant just sound righteous rational peaceable prodigious brave paramount reliable sinless honorable unsuitable unreliable lawless inseparable brutish idle unarmed wounded bigoted unjust brutal Male battered untreated barren shrewish sheltere"
P19-1167,D17-1323,0,0.0916673,"Missing"
P19-1167,N18-2003,0,0.16078,"Missing"
P19-1167,N18-1067,0,0.0283022,"Missing"
P19-1167,W16-0204,0,0.195615,"Missing"
P19-1167,H93-1061,0,\N,Missing
P19-1167,tsvetkov-etal-2014-augmenting-english,0,\N,Missing
P19-1167,W17-1609,0,\N,Missing
P19-1382,D17-1011,0,0.030622,"Missing"
P19-1382,N18-1083,1,0.898096,"d of knowledge base population. Path Ranking Algorithm (PRA) is an algorithm which finds relation paths by traversing the knowledge graph, which can then be used to predict implicatures and feature values (Lao and Cohen, 2010; Lao et al., 2011).1 We train PRA using the standard hyperparameters of the existing implementation, which includes regularising with `1 = 0.001 and `2 = 0.001, as well as using negative sampling. Baseline #4: Language embeddings Although we aim to predict implications, and not only feature values, we compare with previous work on predicting typological features in WALS (Bjerva and Augenstein, 2018a). As their setup is different, we use their highest reported score as a baseline. Feature Prediction Results. Table 1 contains the results from feature prediction across the chapters outlined in WALS. Our implementation is able to predict features across categories above baseline levels. At increasing numbers of implicants, prediction power tends to increase. This is not the case for all feature categories, however. One such case is Nominal Syntax, in which performance peaks at 3 implicants. This is expected, as correlations only exist between some features, thus at a certain point access to"
P19-1382,W18-0207,1,0.510671,"d of knowledge base population. Path Ranking Algorithm (PRA) is an algorithm which finds relation paths by traversing the knowledge graph, which can then be used to predict implicatures and feature values (Lao and Cohen, 2010; Lao et al., 2011).1 We train PRA using the standard hyperparameters of the existing implementation, which includes regularising with `1 = 0.001 and `2 = 0.001, as well as using negative sampling. Baseline #4: Language embeddings Although we aim to predict implications, and not only feature values, we compare with previous work on predicting typological features in WALS (Bjerva and Augenstein, 2018a). As their setup is different, we use their highest reported score as a baseline. Feature Prediction Results. Table 1 contains the results from feature prediction across the chapters outlined in WALS. Our implementation is able to predict features across categories above baseline levels. At increasing numbers of implicants, prediction power tends to increase. This is not the case for all feature categories, however. One such case is Nominal Syntax, in which performance peaks at 3 implicants. This is expected, as correlations only exist between some features, thus at a certain point access to"
P19-1382,N19-1156,1,0.624091,"rtain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphical model (Figure 1) with the PC algorithm of Neapolitan (2004) and then applying the belief propagation (BP) algorithm (Pearl, 1982). We draw inspiration from manual linguistic efforts to this problem (Greenberg, 1963; Lehmann, 1978), as well as from previous computational methods (Daum´e III and Campbell, 2007; Bjerva et al., 2019a). Additionally, we provide a qualitative analysis of predicted implications, as well as performing an empirical evaluation on typological feature prediction, com3924 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3924–3930 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics paring to strong baselines. 2 From A Generative Model to Probabilistic Implications Notation. We now seek a probabilistic formalisation of typological implications. First, we will introduce the relevant notation. Let ` be a language. W"
P19-1382,P07-1009,0,0.36283,"Missing"
P19-1382,C10-1044,0,0.0875114,"Missing"
P19-1382,J19-2006,1,0.853583,"rtain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphical model (Figure 1) with the PC algorithm of Neapolitan (2004) and then applying the belief propagation (BP) algorithm (Pearl, 1982). We draw inspiration from manual linguistic efforts to this problem (Greenberg, 1963; Lehmann, 1978), as well as from previous computational methods (Daum´e III and Campbell, 2007; Bjerva et al., 2019a). Additionally, we provide a qualitative analysis of predicted implications, as well as performing an empirical evaluation on typological feature prediction, com3924 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3924–3930 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics paring to strong baselines. 2 From A Generative Model to Probabilistic Implications Notation. We now seek a probabilistic formalisation of typological implications. First, we will introduce the relevant notation. Let ` be a language. W"
P19-1382,D11-1049,0,0.0918847,"Missing"
P19-1382,P17-1109,1,0.846546,"typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigation by typologists. Additionally, our approach outperforms strong baselines for prediction of typological features. Acknowledgments We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corporation with the donation of the Titan Xp GPU used for this"
P19-1382,D18-1543,1,0.696833,"Missing"
P19-1382,N18-1004,1,0.902651,"Missing"
P19-1382,D17-1268,0,0.257826,"7), we are the first to introduce a probabilisation of typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigation by typologists. Additionally, our approach outperforms strong baselines for prediction of typological features. Acknowledgments We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corp"
P19-1382,P15-2034,0,0.0142401,"this direction has been manual, typological knowledge bases do exist now (Dryer and Haspelmath, 2013; Partick Littel and Levin, 2016), which allows for automated discovery of implications. Although previous computational work exists (Daum´e III and Campbell, 2007), we are the first to introduce a probabilisation of typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigati"
P19-1382,E17-2102,0,0.153608,"er workings of language and define the space of plausible languages. Universals can aid cognitive scientists examining the underlying processes of language, as there arguably is a cognitive reason for why, e.g., languages with OV ordering are postpositional (Greenberg, 1963). In the context of natural language processing (NLP), when creating synthetic data for multilingual NLP, one should consider universals to maintain the plausibility of the data (Wang and Eisner, 2016). Computational typology can furthermore be used to induce language representations, useful in, e.g., lan¨ guage modelling (Ostling and Tiedemann, 2017) and syntactic parsing (de Lhoneux et al., 2018). In this paper, we argue that the deterministic Greenbergian view of implications (Greenberg, 1963) is outdated. Instead, we suggest that a probabilistic view of implications is more suitable, and define the notion of a probabilistic typological implication as a certain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphic"
P19-1382,Q16-1035,0,0.0272503,"e the presence of one feature strictly implies the presence of another. Universals are important to investigate as they offer insight into the inner workings of language and define the space of plausible languages. Universals can aid cognitive scientists examining the underlying processes of language, as there arguably is a cognitive reason for why, e.g., languages with OV ordering are postpositional (Greenberg, 1963). In the context of natural language processing (NLP), when creating synthetic data for multilingual NLP, one should consider universals to maintain the plausibility of the data (Wang and Eisner, 2016). Computational typology can furthermore be used to induce language representations, useful in, e.g., lan¨ guage modelling (Ostling and Tiedemann, 2017) and syntactic parsing (de Lhoneux et al., 2018). In this paper, we argue that the deterministic Greenbergian view of implications (Greenberg, 1963) is outdated. Instead, we suggest that a probabilistic view of implications is more suitable, and define the notion of a probabilistic typological implication as a certain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features,"
S16-1063,D15-1075,0,0.0530468,"Missing"
S16-1063,N16-1138,1,0.893114,"Missing"
S16-1063,I13-1191,0,0.00781955,"Missing"
S16-1063,L16-1729,1,0.859973,"Missing"
S16-1063,P15-1107,0,0.00921714,"000. Each index i in input dim[i] corresponds to a word in the vocabulary, input dim[i] is 1 if the tweet contains the corresponding word in the vocabulary and 0 otherwise. During autoencoder training, an encoder, i.e. embedding function is learned which maps input of size input dim to an embedding of size output dim, as well as a decoder which reconstructs the input. We apply the encoder to the training and test data to obtain features of size output dim for supervised learning and disregard the decoder. While it would be possible to train an encoder which preserves word order, i.e. an LSTM (Li et al., 2015), we opt for a simpler bag-of-word autoencoder here, following Glorot et al. (2011). The architecture of the autoencoder is as follows: input dim is 50000, it has one hidden layer of di1 http://scikit-learn.org/stable/ modules/generated/sklearn.linear_ model.LogisticRegression.html 2 http://github.com/sheffieldnlp/ stance-semeval2016 mensionality 100, and output dim is of size 100. A dropout of 0.1 is added to the hidden layer (Srivastava et al., 2014). The autoencoder is trained with Adam (Kingma and Ba, 2014), using the learning rate 0.1, for 2600 iterations. In each iteration, 500 training"
S16-1063,D13-1171,0,0.0158216,"Missing"
S16-1063,S16-1003,0,0.0869902,"Missing"
S16-1063,strapparava-valitutti-2004-wordnet,0,0.0546914,"e concatenated with the tweet features • Aut-twe*tar: the autoencoder is applied to the tweet and the target, and the outer product of the tweet and target features is used • InTwe: A boolean “targetInTweet” feature We evaluate the impact of traditional sentiment analysis gazetteer features, extracted by assessing appearance of each word of the tweet in the gazetteers: • Emo: emoticon recognition7 One gazetteer/binary feature for each of: happy, sad, happy+sad, not available • Aff: WordNet Affect gazetteer features, one binary feature for each of: anger, disgust, fear, joy, sadness, surprise (Strapparava and Valitutti, 2004)8 6 https://radimrehurek.com/gensim/models/ phrases.html 7 https://github.com/brendano/tweetmotif/ blob/master/emoticons.py 8 http://wndomains.fbk.eu/wnaffect.html 391 Aut-twe+inTwe+Aff Stance FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro P 0.1587 0.5544 R 0.1709 0.4020 0.2278 0.5545 0.1538 0.5700 0.2647 0.5179 0.0769 0.2570 FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro 0.1538 0.5680 0.1538 0.7328 0.1652 0.5503 0.1624 0.6539 0.0000 0.5712 0.0000 1.0000 0.2388 0.5709 0.1368 0.7684 0.1731 0.5487 0.0769 0.7888 FAVOR AGAINST Macro"
S16-1063,N12-1072,0,0.00890815,"Missing"
S16-1063,D15-1073,0,0.0194744,"Missing"
S17-2083,D16-1084,1,0.910464,"Missing"
S17-2083,S16-1003,0,0.205816,"Missing"
S17-2083,D11-1147,0,0.523447,"ant step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A. 1 2 Related Work Single Tweet Stance Classification Stance classification for rumours was pioneered by Qazvinian et al. (2011) as a binary classification task (support/denial). Zeng et al. (2016) perform stance classification for rumours emerging during crises. Both works use tweets related to the same rumour during training and testing. A model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval2016 task 6 dataset (Augenstein et al., 2016). However the RumourEval task is different as it addresses conversation threads. Introduction In stance classification one is concerned with determining the attitude of the author of a text towards a target"
S17-2083,W13-4008,0,0.0272014,"training and testing. A model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval2016 task 6 dataset (Augenstein et al., 2016). However the RumourEval task is different as it addresses conversation threads. Introduction In stance classification one is concerned with determining the attitude of the author of a text towards a target (Mohammad et al., 2016). Targets can range from abstract ideas, to concrete entities and events. Stance classification is an active research area that has been studied in different domains (Ranade et al., 2013; Chuang and Hsieh, 2015). Here we focus on stance classification of tweets towards the truthfulness of rumours circulating in Twitter conversations in the context of breaking news. Each conversation is defined by a tweet that initiates the conversation and a set of nested replies to it that form a conversation thread. The goal is to classify each of the tweets in the Sequential Stance Classification Lukasik et al. (2016) and Zubiaga et al. (2016a) consider the sequential nature of tweet threads in their works. Lukasik et al. (2016) employ Hawkes processes to classify temporal sequences of twe"
S17-2083,C16-1230,1,0.878808,"stract conversation thread as either supporting, denying, querying or commenting (SDQC) on the rumour initiated by the source tweet. Being able to detect stance automatically is very useful in the context of events provoking public resonance and associated rumours, as a first step towards verification of early reports (Zhao et al., 2015). For instance, it has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b). Here we focus on exploiting the conversational structure of social media threads for stance classification and introduce a novel LSTM-based approach to harness conversations. This paper describes team Turing’s submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour ve"
S17-2083,P16-2064,0,\N,Missing
S17-2091,P14-1119,0,0.153552,"uction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and informat"
S17-2091,P17-2054,1,0.878568,"pplication domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities. 1 Introduction Empirical"
S17-2091,W09-3611,0,0.0304788,"yphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym r"
S17-2091,D15-1086,1,0.830685,"ion would also receive articles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 e"
S17-2091,W10-2924,0,0.0334212,"on extraction, and through hypernym prediction would also receive articles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task"
S17-2091,S17-2168,0,0.107723,"Missing"
S17-2091,S17-2167,0,0.0368744,"Missing"
S17-2091,S17-2173,0,0.0252701,"Missing"
S17-2091,C10-1065,0,0.403289,"owledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base po"
S17-2091,S10-1004,0,0.794457,"owledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base po"
S17-2091,D15-1235,0,0.0152438,"om/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task"
S17-2091,W16-5904,0,0.0307461,"ticles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We"
S17-2091,S17-2166,0,0.0299819,"and only for the Natural Language Processing domain. The ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016) consists of 300 ACL Anthology abstracts annotated on mention-level with seven different types of keyphrases. Unlike our dataset, it does not contain relation annotations. Note that this corpus was created at the same time as the one SemEval 2017 Task 10 dataset and thus we did not have the chance to build on it. A more in-depth comparison between the two datasets as well as keyphrase identification and classification methods evaluated on them can be found in Augenstein and Søgaard (2017). Existing Resources As part of the FUSE project with IARPA, we created a small annotated corpus of 100 noun phrases generated from the titles and abstracts derived from the Web Of Science corpora9 of the domains Physics, Computer Science, Chemistry and Computer Science. These corpora cannot be distributed publicly and were made available by the IARPA funding agency. Annotation was performed by 3 annotators using 14 fine-grained types, including PROCESS. We measured inter-annotator agreement among the three annotators for the 14 categories using Fleiss’ Kappa. The k value was found to be 0.28"
S17-2091,Q15-1010,0,0.0248553,"cscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task organisers’ collab"
S17-2091,D15-1057,0,0.0405374,"ion and classification of keyphrases, e.g. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be"
S17-2091,I11-1001,0,0.680204,"led here is mention-level identification and classification of keyphrases, e.g. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific"
S17-2091,S17-2169,0,0.0534923,"Missing"
S17-2091,P09-1113,0,0.0777004,"elines were refined.8 Annotation Process Mention-level annotation is very time-consuming, and only a handful of semantic relations such as hypernymy and synonymy can be found in each publication. We therefore only annotate paragraphs of publications likely to contain relations. We originally intended to identify suitable documents by automatically extracting a knowledge graph of relations from a large scientific dataset using Hearst-style patterns (Hearst, 1991; Snow et al., 2005), then using those to find potential relations in a distinct set of documents, similar to the distant supervision (Mintz et al., 2009; Snow et al., 2005) heuristic. Documents containing a high number of such potential relations would then be selected. However, this requires automatically learning to identify keyphrases between which those potential relations hold, and requires relations to appear several times in a dataset for such a knowledge graph to be useful. In the end, this strategy was not feasible due to the difficulty of learning to detect keyphrases automatically and only a small overlap between relations in different documents. Instead, keyphrasedense paragraphs were detected automatically using a coarse unsuperv"
S17-2091,C14-1002,0,0.0452535,"Missing"
S17-2091,S17-2172,0,0.092929,"Missing"
S17-2091,L16-1294,0,0.0874962,"In contrast to what we propose, the annotations are more fine-grained and annotations are only available for abstracts. Gupta and Manning (2011) studied keyphrase extraction from ACL Anthology articles, applying a pattern-based bootstrapping approach based on 15 016 documents and assigning the types FOCUS, TECHNIQUE and DOMAIN. Performance was evaluated on 30 manually annotated documents. Although the latter corpus is related to what we propose, manual annotation is only available for a small number of documents and only for the Natural Language Processing domain. The ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016) consists of 300 ACL Anthology abstracts annotated on mention-level with seven different types of keyphrases. Unlike our dataset, it does not contain relation annotations. Note that this corpus was created at the same time as the one SemEval 2017 Task 10 dataset and thus we did not have the chance to build on it. A more in-depth comparison between the two datasets as well as keyphrase identification and classification methods evaluated on them can be found in Augenstein and Søgaard (2017). Existing Resources As part of the FUSE project with IARPA, we created a small annotated corpus of 100 nou"
S17-2091,S17-2161,0,0.0445569,"Missing"
S17-2091,W12-3201,0,0.0249455,".co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publication"
S17-2091,R09-1086,0,0.0762443,"Missing"
S17-2091,D15-1175,0,0.0291535,"nition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impac"
S17-2091,D16-1198,0,0.0680123,"as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communi"
S17-2091,tateisi-etal-2014-annotation,0,\N,Missing
S17-2091,S17-2170,0,\N,Missing
S18-1058,P17-2054,1,0.902217,"Missing"
S18-1058,E17-2026,0,0.0431706,"Missing"
S18-1058,W17-0225,1,0.887854,"place-holder instead of their actual values. Since the model used both word and character representations, the characters are read in separately, although the same basic principle is followed. Every character is represented by an embedded representation, which is initialised randomly prior to training. Introduction We consider the task of identifying affect in tweets, as described in Mohammad et al. (2018). Given a tweet, the task is to predict the emotions and their corresponding intensities which the tweet portrays. Previous approaches to this task are outlined in Mohammad and Bravo-Marquez (2017). The winning team of the SemEval EmoInt 2017, presented in Goel et al. (2017), tackled a similar task as the regression task presented in this year’s SemEval Task 1. The winning system utilised an ensemble approach consisting of 5 sub-models and using a weighted average of these models to come up with the final result. This model is utilising most of the different approaches mentioned in the literature and combining them into one and with great success. Our work bears resemblance to the runner up in the SemEval EmoInt 2017, K¨oper et al. (2017), who used a comparatively simple model consistin"
S18-1058,C16-1333,1,0.897597,"Missing"
S18-1058,P15-1166,0,0.0840695,"Missing"
S18-1058,W15-4322,0,0.0657959,"Missing"
S18-1058,W17-5207,0,0.0371953,"both word and character representations, the characters are read in separately, although the same basic principle is followed. Every character is represented by an embedded representation, which is initialised randomly prior to training. Introduction We consider the task of identifying affect in tweets, as described in Mohammad et al. (2018). Given a tweet, the task is to predict the emotions and their corresponding intensities which the tweet portrays. Previous approaches to this task are outlined in Mohammad and Bravo-Marquez (2017). The winning team of the SemEval EmoInt 2017, presented in Goel et al. (2017), tackled a similar task as the regression task presented in this year’s SemEval Task 1. The winning system utilised an ensemble approach consisting of 5 sub-models and using a weighted average of these models to come up with the final result. This model is utilising most of the different approaches mentioned in the literature and combining them into one and with great success. Our work bears resemblance to the runner up in the SemEval EmoInt 2017, K¨oper et al. (2017), who used a comparatively simple model consisting of a CNN-LSTM neural network. The difference between the models presented in"
S18-1058,W17-5206,0,0.0629953,"Missing"
S18-1058,E17-1005,0,0.048576,"Missing"
S18-1058,W17-5205,0,0.0483094,"Missing"
S18-1058,S18-1001,0,0.044417,"Missing"
S18-1058,P16-2038,0,0.0793817,"Missing"
W14-6203,P12-2011,0,0.0123886,"use a different knowledge base, YAGO (Suchanek et al., 2008). They use a Wikipedia-based NERC, which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO is a knowledge based derived from Wikipedia. A few strategies for seed selection for distant supervision have already been investigated: At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013), 22 hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based on the idea that “if two entities particpate in a relation, at least one sentence that mentions these two entities might express that relation”. While positive results have been reported for those models, Riedel et al. (Riedel et al., 2010) argues that it is challenging to train those models because they are quite complex. Hierarchica"
W14-6203,D11-1142,0,0.0292043,"proposed. For supervised approaches, training and test data is annotated manually by one or several annotators. While this approach results in a high-quality corpus, it is very expensive and time-consuming. As a consequence, the corpora used tend to be small and biased towards a certain domain or type of text. Unsupervised approaches do not need annotated data for training; they instead cluster similar word sequences and generalise them to relations. Although unsupervised aproaches can process very large amounts of data, resulting relations are hard to map to particular schemas. In addition, Fader et al. (2011) observe that these approaches often produce uninformative or incoherent extractions. Semi-supervised methods are methods that only require a small number of seed instances. Hand-crafted seeds are used to extract patterns from a corpus, which are then used to extract more instances and those again to extract new patterns in an iterative way. However, since many iterations are needed, these methods are prone to semantic drift, i.e. an unwanted shift of meaning. As a consequence these methods require a certain amount of human effort - to create seeds initially and also to help keep systems ‘on t"
W14-6203,C92-2082,0,0.461025,"Missing"
W14-6203,P11-1055,0,0.0329557,"d their approach is not class-based. Nguyen et al. (2011)’s approach is very similar to that of Mintz et al. (2009), except that they use a different knowledge base, YAGO (Suchanek et al., 2008). They use a Wikipedia-based NERC, which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO is a knowledge based derived from Wikipedia. A few strategies for seed selection for distant supervision have already been investigated: At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013), 22 hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based on the idea that “if two entities particpate in a relation, at least one sentence that mentions these two entities might express that relation”. While positive results have been reported for those mode"
W14-6203,N13-1095,0,0.0166503,"to that of Mintz et al. (2009), except that they use a different knowledge base, YAGO (Suchanek et al., 2008). They use a Wikipedia-based NERC, which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO is a knowledge based derived from Wikipedia. A few strategies for seed selection for distant supervision have already been investigated: At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013), 22 hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based on the idea that “if two entities particpate in a relation, at least one sentence that mentions these two entities might express that relation”. While positive results have been reported for those models, Riedel et al. (Riedel et al., 2010) argues that it is challenging to train th"
W14-6203,P09-1113,0,0.816205,"re instances and those again to extract new patterns in an iterative way. However, since many iterations are needed, these methods are prone to semantic drift, i.e. an unwanted shift of meaning. As a consequence these methods require a certain amount of human effort - to create seeds initially and also to help keep systems ‘on track’. A fourth group of approaches, distant supervision or self-supervised approaches, exploit big knowledge bases such as Freebase (2008) to automatically label entities in text and use the annotated text to extract features and train a classifier (Wu and Weld, 2007; Mintz et al., 2009). Unlike supervised systems, they do not require manual effort to label data and can be applied to large corpora. Since they extract relations which are defined by schemas, these approaches also do not produce informative or incoherent relations. Distant supervision approaches are based on the following assumption (Mintz et al., 2009): “If two entities participate in a relation, any sentence that contains those two entities might express that relation.” In practice, if the information that two entities participate in a relation is contained in the knowledge base, whenever they appear in the sa"
W14-6203,P11-2048,0,0.0608894,"Missing"
W14-6203,D13-1003,0,0.0798243,"e base, YAGO (Suchanek et al., 2008). They use a Wikipedia-based NERC, which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO is a knowledge based derived from Wikipedia. A few strategies for seed selection for distant supervision have already been investigated: At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013), 22 hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based on the idea that “if two entities particpate in a relation, at least one sentence that mentions these two entities might express that relation”. While positive results have been reported for those models, Riedel et al. (Riedel et al., 2010) argues that it is challenging to train those models because they are quite complex. Hierarchical topic models (Alfonsec"
W14-6203,D12-1042,0,0.0194173,"class-based. Nguyen et al. (2011)’s approach is very similar to that of Mintz et al. (2009), except that they use a different knowledge base, YAGO (Suchanek et al., 2008). They use a Wikipedia-based NERC, which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO is a knowledge based derived from Wikipedia. A few strategies for seed selection for distant supervision have already been investigated: At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013), 22 hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based on the idea that “if two entities particpate in a relation, at least one sentence that mentions these two entities might express that relation”. While positive results have been reported for those models, Riedel et al. (Ried"
W14-6203,P12-1076,0,0.013932,"a Wikipedia-based NERC, which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO is a knowledge based derived from Wikipedia. A few strategies for seed selection for distant supervision have already been investigated: At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013), 22 hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based on the idea that “if two entities particpate in a relation, at least one sentence that mentions these two entities might express that relation”. While positive results have been reported for those models, Riedel et al. (Riedel et al., 2010) argues that it is challenging to train those models because they are quite complex. Hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013) assume t"
W14-6203,P13-2117,0,0.0338825,"Missing"
W14-6203,D10-1099,0,0.0240947,"h is very similar to that of Mintz et al. (2009), except that they use a different knowledge base, YAGO (Suchanek et al., 2008). They use a Wikipedia-based NERC, which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO is a knowledge based derived from Wikipedia. A few strategies for seed selection for distant supervision have already been investigated: At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013), 22 hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based on the idea that “if two entities particpate in a relation, at least one sentence that mentions these two entities might express that relation”. While positive results have been reported for those models, Riedel et al. (Riedel et al., 2010) argues that it is chal"
W14-6203,N07-4013,0,0.0803169,"Missing"
W15-4306,J92-4003,0,0.384216,"Missing"
W15-4306,R13-1015,1,0.89064,"Missing"
W15-4306,N15-1075,0,0.11961,"CRF L-BFGS provided the best performance on our dataset for the ten-types task. 3.5 Training Data In our final system, we included the dev 2015 data, to combat drift present in the corpus. We anticipated that the test set would be from 2015. The original dataset was harvested in 2010, long enough ago to be demonstrably disadvantaged when compared with modern data (Fromreide et al., 2014), and so it was critical to include something more. The compensate for the size imbalance – the dev 2015 data is 0.175 the size of the 2010 data – we weighted down the older dataset to by 0.7, as suggested by (Cherry and Guo, 2015), implemented by uniformly scaling individual feature values on older instances. This successfully reduced the negative impact of the inevitable drift. 4 5 5.1 Analysis Features In terms of features, we looked at the strongestweighted observations in the notypes model, to see what the general indicators are of named entities in tweets. The largest of these are shown in Table 3. Of note is that features indicating URLs, hashtags and usernames indicate against an entity; lowercase words including punctuation, or comprising only punctuation, are not entities; being proceeded by at indicates being"
W15-4306,P02-1022,1,0.731761,"ained on more consistent, longer documents, such as newswire, mostly impotent (Derczynski et al., 2015b). Suffering from a sustained dearth of annotated Twitter datasets, it may be useful to understand what makes this genre tick, and how our existing techniques and resources can be generalised better to fit such a challenging text source. This paper has focused on introducing our Named Entity Recognition (NER) entry to the WNUT evaluation challenge (Baldwin et al., 2015), which builds on our earlier experiments with Twitter and news NER (Derczynski and Bontcheva, 2014; Bontcheva et al., 2013; Cunningham et al., 2002). In particular, we push data sources and representations, using what is know about Twitter so far to construct a model that informs our choices. Specifically, we attempt to compensate for entity drift; to harness unsupervised word clustering in a principled fashion; to bring in large-scale gazetteers; to attenuate the impact of terms frequent in this text type; and to pick and choose targeted gazetteers for specific entity types. 3 Method The WNUT Twitter NER task required us to address many data sparsity challenges. Firstly, the datasets involved are simply very small, making it hard to gene"
W15-4306,N13-1037,0,0.0143652,"ge-scale gazetteers; to attenuate the impact of terms frequent in this text type; and to pick and choose targeted gazetteers for specific entity types. 3 Method The WNUT Twitter NER task required us to address many data sparsity challenges. Firstly, the datasets involved are simply very small, making it hard to generalise in supervised learning, and meaning that effect sizes cannot be reliably measured. Secondly, Twitter language is arguably one of the noisiest and idiosyncratic text genres, which manifests as a large number of word types, and very large vocabularies due to lexical variation (Eisenstein, 2013). Thirdly, the language and especially entities found in tweets change over time, which is commonly referred to as drift. The majority of the WNUT training data is from 2010, and only a small amount from 2015, leading to a sparsity in examples of modern language. Therefore, in our machine learning approach, many of the features we introduce are there to combat sparsity. 48 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 48–53, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics 3.1 Unsupervised Clustering NE type company We use an unsupervis"
W15-4306,fromreide-etal-2014-crowdsourcing,0,0.0851378,"h structured learning. Out of CRF using L-BFGS updates, CRF with passive-aggressive updates to combat Twitter noise (Derczynski and Bontcheva, 2014), and structured perceptron (also useful on Twitter noise (Johannsen et al., 2014)), CRF L-BFGS provided the best performance on our dataset for the ten-types task. 3.5 Training Data In our final system, we included the dev 2015 data, to combat drift present in the corpus. We anticipated that the test set would be from 2015. The original dataset was harvested in 2010, long enough ago to be demonstrably disadvantaged when compared with modern data (Fromreide et al., 2014), and so it was critical to include something more. The compensate for the size imbalance – the dev 2015 data is 0.175 the size of the 2010 data – we weighted down the older dataset to by 0.7, as suggested by (Cherry and Guo, 2015), implemented by uniformly scaling individual feature values on older instances. This successfully reduced the negative impact of the inevitable drift. 4 5 5.1 Analysis Features In terms of features, we looked at the strongestweighted observations in the notypes model, to see what the general indicators are of named entities in tweets. The largest of these are shown"
W15-4306,S14-1001,0,0.0267027,"Missing"
W15-4306,P08-1068,0,0.0672496,"the text type. 250 million tweets from 20102012 were used to generate 2,000 word classes using Brown clustering (Brown et al., 1992). Typically 1,000 or fewer are used; the larger number of classes was chosen because it helpfully increased the expressivity of the representation (Derczynski et al., 2015a), while retaining a useful sparsity reduction. These hierarchical classes were represented using bit depths of 3-10 inclusive, and then 12, 14, 16, 18 and 20, one feature per depth. The typical levels are 4, 6, 10 and 20, though selection of bit depths to use often yields brittle feature sets (Koo et al., 2008), and so we leave it to the classifier to decide which ones are useful. These choices are examined in our post-exercise investigations into the model, Section 5.1, and the clusters provided with this paper. Finally, we also include the Brown class paths for the previous token. To aid in filtering out common tokens and reducing the impact they may have as e.g. spurious gazetteer matches, we incorporate a term frequency from our language model. This is applied to terms that are in the top 50,000 found in our garden hose sample, and represented as a feature having a value scaled in proportion to"
W15-4306,P12-3005,0,0.0355868,"Missing"
W15-4306,D11-1141,0,0.712406,"ly corresponding types. To build gazetteers, we therefore retrieved all Freebase types for all entities in the training corpus and selected the most prominent Freebase types per entity type in the gold standard. The list of Freebase types corresponding to each entity type in the gold standard is listed in Table 1. For each Freebase type, separate gazetteers were created for entity names and alternative names (aliases), since the latter tend to be of lower quality. There were several other gazetteer sources that we tried but which did not work very well: IMDb dumps,2 Ritter’s LabeledLDA lists (Ritter et al., 2011) (duplicated in the baseline system), and ANNIE’s other Morpho-Syntactic Features To model context, we used reasonably conventional features: the token itself, the uni- and bigrams in a [−2, 2] offset window from the current token, and both wordshape (e.g. London becomes Xxxxxx) and reduced wordshape (London to Xx) features. We also included a part-of-speech tag for each token. These were automatically generated by a custom tweet PoS tagger using an extension of the PTB tagset (Derczynski et al., 2013b). To capture orthographic information, we take suffix and prefix features of length [1..3]."
W15-4306,W06-2918,0,0.213294,"nal: the all-types and multiple-types tasks are effectively similar when contrasted with the single-types task, in that they require the recognition of many different kinds of named entity. Finally, we found that other gazetteer types were not helpful to performance; taking for example all of the ANNIE gazetteers, gazetteers from IMDb dumps, entity names extracted from other Twitter NER corpora, or entities generated through LLDA (Ritter et al., 2011) all decreased performance. We suspect this is due to their swamping already-small input dataset with too great a profusion of information, c.f. Smith and Osborne (2006). In addition, we tried generating semi-supervised data using vote-constrained bootstrapping, but this was not helpful either – presumably due to the initially low performance of machine-learning based tools on Twitter NER making it hard to develop semi-supervised bootstrapped training data, no matter how stringent the filtering of autogenerated examples. For the final run, we were faced with a decision about fitting. We could either choose a configuration that minimised training loss on all the available training data (train + dev + dev 2015), but risked overfitting to it. Alternatively, we c"
W15-4306,W15-0211,1,0.807824,"r B-company B-geo-loc B-person B-facility B-facility B-sportsteam B-tvshow B-person B-product B-other B-geo-loc B-person B-geo-loc B-person B-company Terms -0.571505 -0.585369 -0.604976 -0.620909 -0.655420 0.699101 0.699101 0.709865 0.714127 -0.717037 0.747492 0.774895 0.804635 -0.894333 0.895203 0.950866 1.044984 5.2 Gold standard When developing the system, we encountered several problems and inconsistencies in the gold standard. These issues are partly a general problem of developing gold standards, i.e. the more complicated the task is, the more humans tend to disagree on correct answers (Tissot et al., 2015). For Twitter NERC with 10 types, some of the tokens are very difficult to label because the context window is very small (140 characters), which then also leads to acronyms being used very frequently to save space, and because world knowledge about sports, music etc. is required. In particular, the following groups of problems in the gold standard training corpus were identified: Table 4: Largest-weighted Brown cluster features in 10-types task numbers rarely start entities; and being matched by an entry in the video games gazetteer suggests being an entity. One cluster prefix was indicative"
W15-4306,E14-4014,1,\N,Missing
W15-4306,R13-1026,1,\N,Missing
W15-4306,R13-1011,1,\N,Missing
W16-6208,D16-1084,1,0.840423,"Missing"
W16-6208,L16-1626,0,0.506339,"ing the emoji embedding space. 2 Related Work There has been little work in distributional embeddings of emoji. The first research done in this direction was an informal blog post by the Instagram Data Team in 2015 (Dimson, 2015). They generated vector embeddings for emoji similar to skip-gram-based vectors by training on the entire corpus of Instagram posts. Their research gave valuable insight into the usage of emoji on Instagram, and showed that distributed representations can help understanding emoji semantics in everyday usage. The second contribution, closest to ours, was introduced by (Barbieri et al., 2016). They trained emoji embeddings from a large Twitter dataset of over 100 million English tweets using the skip-gram method (Mikolov et al., 2013a). These pre-trained emoji representations led to increased accuracy on a similarity task, and a meaningful clustering of the emoji embedding space. While this method is able to learn robust representations for frequently-used emoji, representations of less frequent emoji are estimated rather poorly or not available at all. In fact, only around 700 emoji can be found in Barbieri et al. (2016)’s corpus, while there is support of over 1600 emoji in the"
W16-6208,Q16-1002,0,0.0181034,"spects. First, since we are estimating the representation of emoji directly from their description, we obtain robust representations for all supported emoji symbols — even the long tail of infrequently used ones. Sec49 Figure 1: Example description of U+1F574. We also use business, man and suit keywords for training. ondly, our method works with much less data. Instead of training on millions of tweets, our representations are trained on only a few thousand descriptions. Still, we obtain higher accuracy results on a Twitter sentiment analysis task. In addition, our work relates to the work of Hill et al. (2016) who built word representations for words and concepts based on their description in a dictionary. Similarly to their approach, we build representations for emoji based on their descriptions and keyword phrases. Some of the limitations of our work are evident in the work of Park et al. (2013) who showed that different cultural phenomena and languages may coopt conventional emoji sentiment. Since we train only on English-language definitions and ignore temporal definitions of emoji, our training method might not capture the full semantic characteristics of an emoji. 3 Method Our method maps emo"
W16-6208,W14-1618,0,0.0553784,"Missing"
W16-6208,N13-1090,0,0.399893,"social media rely on representation learning and word embeddings (Tang et al., 2014; Dong et al., 2014; Dhingra et al., 2016; Augenstein et al., 2 http://www.unicode.org/emoji/charts/ full-emoji-list.html See https://twitter.com/Kyle_MacLachlan/ status/765390472604971009 for an extreme example. 48 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 48–54, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016). Such systems often rely on pre-trained word embeddings that can for instance be obtained from word2vec (Mikolov et al., 2013a) or GloVe (Pennington et al., 2014). Yet, neither resource contain a complete set of Unicode emoji representations, which suggests that many social NLP applications could be improved by the addition of robust emoji representations. In this paper we release emoji2vec, embeddings for emoji Unicode symbols learned from their description in the Unicode emoji standard. We demonstrate the usefulness of emoji representations trained in this way by evaluating on a Twitter sentiment analysis task. Furthermore, we provide a qualitative analysis by investigating emoji analogy examples and visualizing t"
W16-6208,D14-1162,0,0.0991354,"on learning and word embeddings (Tang et al., 2014; Dong et al., 2014; Dhingra et al., 2016; Augenstein et al., 2 http://www.unicode.org/emoji/charts/ full-emoji-list.html See https://twitter.com/Kyle_MacLachlan/ status/765390472604971009 for an extreme example. 48 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 48–54, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016). Such systems often rely on pre-trained word embeddings that can for instance be obtained from word2vec (Mikolov et al., 2013a) or GloVe (Pennington et al., 2014). Yet, neither resource contain a complete set of Unicode emoji representations, which suggests that many social NLP applications could be improved by the addition of robust emoji representations. In this paper we release emoji2vec, embeddings for emoji Unicode symbols learned from their description in the Unicode emoji standard. We demonstrate the usefulness of emoji representations trained in this way by evaluating on a Twitter sentiment analysis task. Furthermore, we provide a qualitative analysis by investigating emoji analogy examples and visualizing the emoji embedding space. 2 Related W"
W16-6208,D11-1141,0,0.109255,"Missing"
W16-6208,S15-2078,0,0.0395286,"Missing"
W16-6208,P14-1146,0,0.110515,"Missing"
W18-0207,Q16-1031,0,0.036842,"estigate the usefulness of explicitly modelling similarities between languages in deep neural networks using language embeddings. To do so, we view NLP tasks for multiple Uralic languages as different aspects of the same problem and model them in one model using multilingual transfer in a multi-task learning model. Multilingual models frequently follow a hard parameter sharing regime, where all hidden layers of a neural network are shared between languages, with the language either being implicitly coded in the input string (Johnson et al., 2017), given as a language ID in a one-hot encoding (Ammar et al., 2016), or as a language embedding (Östling and Tiedemann, 2017). In this paper, we both explore multilingual modelling of Uralic languages, and probe the language embeddings obtained from such modelling in order to gain novel insights about typological traits of Uralic languages. We aim to answer the following three research questions (RQs). RQ 1 To what extent is model transfer between Uralic languages for PoS tagging mutually beneficial? RQ 2 Are distributed language representations useful for model transfer between Uralic languages? RQ 3 Can we observe any explicit typological properties encoded"
W18-0207,D17-1011,0,0.127381,"s have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological pr"
W18-0207,W16-4116,1,0.840006,"putational Linguistics 1 Introduction For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Ös"
W18-0207,P17-1109,0,0.106631,"n two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017)."
W18-0207,P16-1038,0,0.0473904,"troduction For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 20"
W18-0207,D17-1268,0,0.423471,"ith respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological properties of language, re"
W18-0207,L16-1247,0,0.0134878,"se the language embeddings to encode features of language. Other work has explored learning language embeddings in the context of neural machine translation (Malaviya et al., 2017). In this work, we explore the embeddings trained by Östling and Tiedemann (2017), both in their original state, and by further tuning them for PoS tagging. 2.2 Part-of-speech tagging We use PoS annotations from version 2 of the Universal Dependencies (Nivre et al., 2016). We focus on the four Uralic languages present in the UD, namely Finnish (based on the Turku Dependency Treebank, Pyysalo et al., 2015), Estonian (Muischnek et al., 2016), Hungarian (based on the Hungarian Dependency Treebank, Vincze et al., 2010), and North Sámi (Sheyanova and Tyers, 2017). As we are mainly interested in observing the language embeddings, we down-sample all training sets to 1500 sentences (approximate number of sentences in the Hungarian data), so as to minimise any size-based effects. 2.3 Typological data In the experiments for RQ3, we attempt to predict typological features. We extract the features we aim to predict from WALS (Dryer and Haspelmath, 2013). We consider features which are encoded for all four Uralic languages in our sample. 3"
W18-0207,L16-1262,0,0.0866901,"Missing"
W18-0207,P15-2034,0,0.117523,"ciation for Computational Linguistics 1 Introduction For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings"
W18-0207,E17-2102,0,0.213918,"16; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological properties of language, reminiscent of the sparse features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993; Chomsky and Lasnik, 1993; Chomsky, 2014). In this paper, we investigate the usefulness of explicitly modelling similarities between languages in deep neural networks using language embeddings. To do so, we view NLP tasks for multiple Uralic languages as different aspects of the same problem and model them in one model using multilingual transfer in a multi-task learning m"
W18-0207,W17-5403,0,0.0880875,"Missing"
W18-0207,P16-2067,0,0.0290186,"we are mainly interested in observing the language embeddings, we down-sample all training sets to 1500 sentences (approximate number of sentences in the Hungarian data), so as to minimise any size-based effects. 2.3 Typological data In the experiments for RQ3, we attempt to predict typological features. We extract the features we aim to predict from WALS (Dryer and Haspelmath, 2013). We consider features which are encoded for all four Uralic languages in our sample. 3 Method and experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, based on Plank et al. (2016). The system is implemented using DyNet (Neubig et al., 2017). We train using the Adam optimisation algorithm (Kingma and Ba, 2014) over a maximum of 10 epochs, using early stopping. We make two modifications to the bi-LSTM architecture of Plank et al. (2016). First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations. This choice was made so as to encourage the model not to rely on languagespecific vocabulary. Additionally, we concatenate a pre-trained language embedding to each word representation. That is to say, in the ori"
W18-0207,W17-0607,0,0.0141672,"context of neural machine translation (Malaviya et al., 2017). In this work, we explore the embeddings trained by Östling and Tiedemann (2017), both in their original state, and by further tuning them for PoS tagging. 2.2 Part-of-speech tagging We use PoS annotations from version 2 of the Universal Dependencies (Nivre et al., 2016). We focus on the four Uralic languages present in the UD, namely Finnish (based on the Turku Dependency Treebank, Pyysalo et al., 2015), Estonian (Muischnek et al., 2016), Hungarian (based on the Hungarian Dependency Treebank, Vincze et al., 2010), and North Sámi (Sheyanova and Tyers, 2017). As we are mainly interested in observing the language embeddings, we down-sample all training sets to 1500 sentences (approximate number of sentences in the Hungarian data), so as to minimise any size-based effects. 2.3 Typological data In the experiments for RQ3, we attempt to predict typological features. We extract the features we aim to predict from WALS (Dryer and Haspelmath, 2013). We consider features which are encoded for all four Uralic languages in our sample. 3 Method and experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, base"
W18-0207,N16-1161,0,0.342322,"Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological properties of language, reminiscent of the sparse features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993; Chomsky and Lasnik, 1993; Chomsky, 2014). In this paper, we investigate the usefulness of explicitly modelling similarities between languages in deep neural networks using language embeddings. To do so, we view NLP tasks for multiple Uralic languages as different aspects of the same problem and model them in one model using multilingual transf"
W18-3401,D15-1041,0,0.028899,"with a character-based sequence-to-sequence model. Furthermore, we experiment with different choices of external resources and corresponding auxiliary tasks and show that autoencoding can be as efficient as an auxiliary task for low-resource POS tagging as lemmatization. Finally, we evaluate our models on 34 typologically diverse languages. 2 Figure 1: Our multi-task architecture, consisting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We be"
W18-3401,E17-2026,1,0.947527,".6776(.00) .6226( - ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,W17-0225,1,0.850592,"or results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-reso"
W18-3401,C16-1333,1,0.850199,"owever, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevan"
W18-3401,D11-1005,0,0.0784096,"Missing"
W18-3401,K17-2001,0,0.389323,"- ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,P15-2044,1,0.928316,"Missing"
W18-3401,N16-1077,0,0.0302103,"Lemmatization is a task from the area of inflectional morphology. In particular, it is a special case of morphological inflection. Its goal is to map a given inflected word form to its lemma, e.g., sue˜no 7→ so˜nar. (6) Word autoencoding. For the word autoencoding task, we use the inflected forms from the SIGMORPHON 2017 shared task dataset for each respective setting. Due to identical forms for different slot in the morphological paradigm of some lemmas, we might have duplicate examples in those datasets. Sequence-to-sequence models have shown strong performances on morphological inflection (Aharoni et al., 2016; Kann and Sch¨utze, 2016; Makarov et al., 2017). Therefore, when morphological dictionaries are available, we can easily combine a neural model for lemmatization with a POS tagger, using our architecture. Our intuition for this auxiliary task is that it should be possible to include morphological information into our character-based word representations. Formally, the task can be described as follows. Let AL be a discrete alphabet for language L and let TL be a set of morphological tags for L. The morphological paradigm π of a lemma w in L is a set of pairs n o π(w) = fk [w], tk (7) Random s"
W18-3401,P15-1166,0,0.0831235,"Missing"
W18-3401,P07-1094,0,0.133713,"Missing"
W18-3401,P17-2054,1,0.900977,"Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to mention some important work here. Cross-lingual approaches have been used for a large variety of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Ts"
W18-3401,N18-1172,1,0.839691,"nt training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to me"
W18-3401,P16-2067,1,0.917337,"ting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We believe that using subword-level auxiliary tasks to regularize the character-level encoding in hierarchical LSTMs is a flexible and efficient way to get the best of both worlds: such a model is still able to make predictions about unknown words, but the subword-level auxiliary task should prevent it from overfitting. 2.1 vc,i = conc(LSTMc,f (c1:m ), (1) LSTMc,b (cm:1 )) Second, a cont"
W18-3401,P17-1194,0,0.0202508,". Furthermore, their model is also a multi-task model, being trained jointly on predicting the POS and the log-frequency of a word. Their architecture obtained state-of-the-art results for POS tagging in several languages. Hence, in the low-resource setting considered here, we build upon the architecture developed by Plank et al. (2016), and extend it to a multi-task architecture involving sequenceto-sequence learning. Note though that in contrast to our setup, their tasks are both sequence-labeling tasks and using the same input for both tasks. The same holds true for the multi-task model by Rei (2017), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in S"
W18-3401,P17-1182,1,0.881428,"Missing"
W18-3401,P16-2090,1,0.82551,"Missing"
W18-3401,P11-2120,1,0.816951,"Missing"
W18-3401,P16-2038,1,0.707545,"o obtain better performance on a sequence classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lin"
W18-3401,Q16-1023,0,0.0435006,"individual languages. Here, we notice that autoencoders often outperform lemmatization for agglutinative languages. An explanation for this might be that agglutinative morphology is harder to learn, and the chance of overfitting on a small sample is therefore higher. Hyperparameters For all networks, we use 300-dimensional character embeddings, 64-dimensional word embeddings and 100-dimensional LSTM hidden states. Encoder and decoder LSTMs have 1 hidden layer each. For training, we use ADAM (Kingma and Ba, 2014), as well as word dropout and character dropout, each with a coefficient of 0.25 (Kiperwasser and Goldberg, 2016). Gaussian noise is added to the concatenation of the last states of the character LSTMs for POS tagging. All models are trained using early stopping, with a minimum number of 75 (single-task and low), 30 (medium) or 20 (high) epochs and a maximum number of 300 epochs, which is never reached. We stop training if we obtain no improvement for 10 consecutive epochs. The best model on the development set is used for testing. 5 Results The test results for all languages and settings are presented in Table 1. Our first observation is that using 100 words of auxiliary task data seems to be sufficient"
W18-3401,N06-1012,0,0.0581584,"), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in Sutton et al. (2006). Sutton et al. (2006) use a group lasso regularizer to prevent feature swamping. In the same way, we could also detect distributionally similar characters and use a group lasso regularizer to prevent covariate characters to swamp each other. However, this effect can potentially also hurt performance if done in an uninformed way. We intuit that this makes it also impossible for the model to learn useful similarities between characters (random string autoencod7 4.5 4.5 POS, main task POS+AE-Random, main task POS+AE-Random, aux. task POS+Lemmatization, main task POS+Lemmatization, aux. task POS+"
W18-3401,Q13-1001,0,0.0766196,"Missing"
W18-3401,D12-1127,0,0.148688,"Missing"
W18-3401,N16-1161,0,0.0651137,"Missing"
W18-3401,E17-1005,1,0.841116,"ce classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we cons"
W18-3401,Q14-1005,0,0.0607788,"Missing"
W18-3401,D13-1032,0,0.0818875,"Missing"
W18-3401,P12-1066,0,0.0687448,"Missing"
W18-3401,H01-1035,0,0.151424,"Missing"
W18-3401,N16-1004,0,0.0425229,"Missing"
W18-5404,W08-1301,0,0.0438607,"Missing"
W18-5404,P15-1111,0,0.128509,"re uniquely labeled by a single dependency label. Perturbation maps Since dots consistently attach to the root token of the sentence, and commas attach to their left neighbour or to the root token, we can remove and inject additional punctuation in a sentence without affecting the rest of its syntactic structure and without violating the wellformedness of dependency trees. Note, however, that injecting a root-dominated dot or comma may lead to crossing edges, i.e., turn a projective dependency tree into a non-projective one. This may lead to cascading errors for projective dependency parsers (Ng and Curran, 2015). In our experiments, arc-eager M ALT PARSER and S TAN FORD are the only projective parsers. We therefore propose two perturbation maps (Jo and Bengio, 2017): (a) simply removing punctuation, and (b) a simple injection scheme with two parameters χ and δ. Let a dependency structure be an ordered tree with n nodes decorated with words w1 , . . . , wn . At any node 1 ≤ i ≤ n, we (a) inject a comma at position i with probability χ and move nodes i ≤ j ≤ n to positions j + 1, increasing the size of the graph by 1; and (b) inject a dot at position i+1 with probability δ and move nodes i < j ≤ n to p"
W18-5404,D14-1082,0,0.0264032,"Missing"
W18-5404,W16-0518,0,0.0259517,"ovember 1, 2018. 2018 Association for Computational Linguistics words, and many Asian languages, e.g., Thai and Lao, still do not. A period is typically used to mark the end of a grammatical sentence, and commas are often used to separate clauses. Therefore, punctuation also correlates strongly with properties of syntactic structures and is therefore very predictive of dependency structures. Variation in punctuation is often observed in informal texts, but variation may also be the result of errors. Punctuation errors are by far the most frequent error type in scientific writing, for example (Remse et al., 2016). Modern parsers should be robust to such variation, just like humans are (Baldwin and Coady, 1978). nsubj amod punct John , punct dobj punct 27 , likes jazz . Figure 1: Punctuation in Stanford dependencies We show that (a) projective parsers are, unsurprisingly, more sensitive to punctuation injection than non-projective ones, since punctuation injection may introduce crossing edges, and (b) neural parsers are more sensitive than vintage parsers. The latter is our main contribution, but we also show that training a neural parser without punctuation outperforms all parsers trained in a regular"
W18-5404,W11-0303,0,0.0115619,"PARSER is much more robust than the other parsers. That said, it still does much worse than the UUPARSER trained without punctuation. Evaluation on informal text with non-standard punctuation We also evaluate the models on sentences with non-standard punctuation in the development sections in the Google Web Treebank with informal text (from Yahoo Answers and user reviews). Specifically, we evaluate the models on sentences with more than one dot. Again, we show that the neural dependency parser trained without punctuation is superior to the other parsers. 5 Related work Punctuation in parsing Spitkovsky et al. (2011) introduced the idea of splitting sentences at punctuation and imposing parsing restrictions over the fragments and observed significant improvements in the context of unsupervised parsing. Ng and Curran (2015) aim to prevent cascading errors by enforcing correct punctuation arcs. They restrict themselves to projective dependency parsing; erroneous punctuation arcs do not lead to cascading errors in non-projective dependency parsing. Ma et al. (2014), motivated by the same observation, treat punctuation marks as properties of their neighboring words rather than as individual tokens, showing im"
W18-5404,W17-5401,0,0.0438876,"Missing"
W18-5404,P15-1147,0,0.0266262,"Missing"
W18-5404,D17-1215,0,0.0640031,"Missing"
W18-5404,Q16-1023,0,0.0623643,"Missing"
W18-5404,K17-3022,1,0.885964,"Missing"
W18-5404,W17-6314,1,0.894343,"Missing"
W18-5404,P14-2128,0,\N,Missing
